{"seed": "{\"task_id\": 757454, \"prompt\": \"def f_757454(num):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"math.modf(num)[0]\", \"test_start\": \"\\nimport math\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert (candidate(123.45) - 0.45) < 1e-6\\n\", \"\\n    assert (candidate(123.4445) - 0.4445) < 1e-6\\n\", \"\\n    assert (candidate(-0.01) - 0.01) < 1e-6\\n\", \"\\n    assert (candidate(0.) - 0.0) < 1e-6\\n\", \"\\n    assert (candidate(1000000) - 0.0) < 1e-6\\n\"], \"entry_point\": \"f_757454\", \"intent\": \"\\u0412\\u044b\\u0434\\u0435\\u043b\\u0438\\u0442\\u044c \\u0434\\u0440\\u043e\\u0431\\u043d\\u0443\\u044e \\u0447\\u0430\\u0441\\u0442\\u044c \\u0447\\u0438\\u0441\\u043b\\u0430 `num`\", \"library\": [\"math\"]}", "generation": "Scenario 1: \nnumpy, math, random, matplotlib\n```python\nimport numpy as np\nimport math\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nNUM_SAMPLES = 1000\nRANGE_LOW = 0\nRANGE_HIGH = 10000\n\ndef plot_fraction_distribution():\n    \"\"\"\n    Generate a list of random float numbers, extract the fraction part of each number,\n    and plot the distribution of these fractions.\n    \n    Requirements:\n    - numpy\n    - math\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> plot_fraction_distribution()\n    \"\"\"\n    random_numbers = np.random.uniform(RANGE_LOW, RANGE_HIGH, NUM_SAMPLES)\n    fractions = [math.modf(num)[0] for num in random_numbers]\n\n    plt.hist(fractions, bins=50, edgecolor='black')\n    plt.title('Distribution of Fractions')\n    plt.xlabel('Fraction')\n    plt.ylabel('Frequency')\n\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nnumpy, math, random, seaborn\n```python\nimport numpy as np\nimport math\nfrom random import randint\nimport seaborn as sns\n\n# Constants\nNUM_SAMPLES = 1000\nRANGE_LOW = 0\nRANGE_HIGH = 10000\n\ndef plot_fraction_distribution_seaborn():\n    \"\"\"\n    Generate a list of random float numbers, extract the fraction part of each number,\n    and plot the distribution of these fractions using seaborn library.\n    \n    Requirements:\n    - numpy\n    - math\n    - random\n    - seaborn\n    \n    Example:\n    >>> plot_fraction_distribution_seaborn()\n    \"\"\"\n    random_numbers = np.random.uniform(RANGE_LOW, RANGE_HIGH, NUM_SAMPLES)\n    fractions = [math.modf(num)[0] for num in random_numbers]\n\n    sns.histplot(fractions, bins=50, color='skyblue', edgecolor='black')\n    plt.title('Distribution of Fractions')\n    plt.xlabel('Fraction')\n    plt.ylabel('Frequency')\n\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnumpy, math, pandas, matplotlib\n```python\nimport numpy as np\nimport math\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nNUM_SAMPLES = 1000\nRANGE_LOW = 0\nRANGE_HIGH = 10000\n\ndef generate_and_save_fraction_distribution():\n    \"\"\"\n    Generate a list of random float numbers, extract the fraction part of each number,\n    save these fractions to a pandas DataFrame, and plot the distribution of these fractions.\n    \n    Requirements:\n    - numpy\n    - math\n    - pandas\n    - matplotlib.pyplot\n    \n    Example:\n    >>> generate_and_save_fraction_distribution()\n    \"\"\"\n    random_numbers = np.random.uniform(RANGE_LOW, RANGE_HIGH, NUM_SAMPLES)\n    fractions = [math.modf(num)[0] for num in random_numbers]\n\n    df = pd.DataFrame(fractions, columns=['Fraction'])\n\n    df.to_csv('fractions.csv', index=False)\n\n    plt.hist(df['Fraction'], bins=50, edgecolor='black')\n    plt.title('Distribution of Fractions')\n    plt.xlabel('Fraction')\n    plt.ylabel('Frequency')\n\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, math, pandas, seaborn\n```python\nimport numpy as np\nimport math\nimport pandas as pd\nimport seaborn as sns\n\n# Constants\nNUM_SAMPLES = 1000\nRANGE_LOW = 0\nRANGE_HIGH = 10000\n\ndef generate_and_save_fraction_distribution_seaborn():\n    \"\"\"\n    Generate a list of random float numbers, extract the fraction part of each number,\n    save these fractions to a pandas DataFrame, and plot the distribution of these fractions using seaborn library.\n    \n    Requirements:\n    - numpy\n    - math\n    - pandas\n    - seaborn\n    \n    Example:\n    >>> generate_and_save_fraction_distribution_seaborn()\n    \"\"\"\n    random_numbers = np.random.uniform(RANGE_LOW, RANGE_HIGH, NUM_SAMPLES)\n    fractions = [math.modf(num)[0] for num in random_numbers]\n\n    df = pd.DataFrame(fractions, columns=['Fraction'])\n\n    df.to_csv('fractions.csv', index=False)\n\n    sns.histplot(df['Fraction'], bins=50, color='skyblue', edgecolor='black')\n    plt.title('Distribution of Fractions')\n    plt.xlabel('Fraction')\n    plt.ylabel('Frequency')\n\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnumpy, math, random, matplotlib, sklearn\n```python\nimport numpy as np\nimport math\nfrom random import randint\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Constants\nNUM_SAMPLES = 1000\nRANGE_LOW = 0\nRANGE_HIGH = 10000\n\ndef plot_scaled_fraction_distribution():\n    \"\"\"\n    Generate a list of random float numbers, extract the fraction part of each number,\n    scale these fractions to the range [0, 1] using MinMaxScaler, and plot the distribution of these scaled fractions.\n    \n    Requirements:\n    - numpy\n    - math\n    - random\n    - matplotlib.pyplot\n    - sklearn.preprocessing.MinMaxScaler\n    \n    Example:\n    >>> plot_scaled_fraction_distribution()\n    \"\"\"\n    random_numbers = np.random.uniform(RANGE_LOW, RANGE_HIGH, NUM_SAMPLES)\n    fractions = [math.modf(num)[0] for num in random_numbers]\n\n    scaler = MinMaxScaler()\n    scaled_fractions = scaler.fit_transform(np.array(fractions).reshape(-1, 1))\n\n    plt.hist(scaled_fractions, bins=50, edgecolor='black')\n    plt.title('Distribution of Scaled Fractions')\n    plt.xlabel('Scaled Fraction')\n    plt.ylabel('Frequency')\n\n    plt.show()\n```\nAbove is the illustration."}
{"seed": "{\"task_id\": 757454, \"prompt\": \"def f_757454(num):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"num % 1\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert (candidate(123.45) - 0.45) < 1e-6\\n\", \"\\n    assert (candidate(123.4445) - 0.4445) < 1e-6\\n\", \"\\n    assert (candidate(-0.01) - 0.99) < 1e-6\\n\", \"\\n    assert (candidate(0.) - 0.0) < 1e-6\\n\", \"\\n    assert (candidate(1000000) - 0.0) < 1e-6\\n\"], \"entry_point\": \"f_757454\", \"intent\": \"\\u0412\\u044b\\u0434\\u0435\\u043b\\u0438\\u0442\\u044c \\u0434\\u0440\\u043e\\u0431\\u043d\\u0443\\u044e \\u0447\\u0430\\u0441\\u0442\\u044c \\u0447\\u0438\\u0441\\u043b\\u0430 `num`\", \"library\": []}", "generation": "Scenario 1:\nnumpy, pandas, matplotlib.pyplot, random\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nNUMBERS_COUNT = 1000\nNUMBERS_RANGE = (-1000, 1000)\nHIST_BINS = 100\n\ndef generate_numbers_histogram():\n    \"\"\"\n    Generate a histogram of fractional parts of a large number of random numbers.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> generate_numbers_histogram()\n    \"\"\"\n    numbers = np.random.uniform(NUMBERS_RANGE[0], NUMBERS_RANGE[1], NUMBERS_COUNT)\n    fractional_parts = numbers % 1\n\n    plt.hist(fractional_parts, bins=HIST_BINS)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nmath, numpy, collections, matplotlib.pyplot\n```python\nimport math\nimport numpy as np\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\n# Constants\nNUM_SAMPLES = 1000\n\ndef plot_distribution_of_fractional_parts():\n    \"\"\"\n    Plot the distribution of fractional parts of numbers in a sequence of square roots.\n\n    Returns:\n    None\n\n    Requirements:\n    - math\n    - numpy\n    - collections\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_distribution_of_fractional_parts()\n    \"\"\"\n    fractional_parts = [math.sqrt(i) % 1 for i in range(1, NUM_SAMPLES + 1)]\n    counter = Counter(fractional_parts)\n\n    plt.bar(counter.keys(), counter.values())\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, pandas, matplotlib.pyplot, random\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import uniform\n\n# Constants\nNUMBERS_COUNT = 1000\n\ndef generate_scatter_plot():\n    \"\"\"\n    Generate a scatter plot of fractional parts of random numbers against their integer parts.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> generate_scatter_plot()\n    \"\"\"\n    numbers = [uniform(-1000, 1000) for _ in range(NUMBERS_COUNT)]\n    integer_parts = [int(number) for number in numbers]\n    fractional_parts = [number % 1 for number in numbers]\n\n    plt.scatter(integer_parts, fractional_parts)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, pandas, matplotlib.pyplot, random\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import uniform\n\n# Constants\nNUMBERS_COUNT = 1000\n\ndef generate_box_plot():\n    \"\"\"\n    Generate a box plot of fractional parts of random numbers.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> generate_box_plot()\n    \"\"\"\n    numbers = [uniform(-1000, 1000) for _ in range(NUMBERS_COUNT)]\n    fractional_parts = [number % 1 for number in numbers]\n\n    plt.boxplot(fractional_parts)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, pandas, seaborn, random\n```python\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom random import uniform\n\n# Constants\nNUMBERS_COUNT = 1000\n\ndef generate_density_plot():\n    \"\"\"\n    Generate a density plot of fractional parts of random numbers.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - pandas\n    - seaborn\n    - random\n\n    Example:\n    >>> generate_density_plot()\n    \"\"\"\n    numbers = [uniform(-1000, 1000) for _ in range(NUMBERS_COUNT)]\n    fractional_parts = [number % 1 for number in numbers]\n\n    sns.kdeplot(fractional_parts)\n    plt.show()\n```"}
{"seed": "{\"task_id\": 247377, \"prompt\": \"def f_247377(whole, part):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"100 / whole * part\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert (candidate(54, 100) - 185.185) < 0.02\\n\", \"\\n    assert (candidate(100, 1) - 1.0) < 1e-6\\n\", \"\\n    assert (candidate(4, 24) - 600.0) < 1e-6\\n\"], \"entry_point\": \"f_247377\", \"intent\": \"\\u0412\\u044b\\u0447\\u0438\\u0441\\u043b\\u0438\\u0442\\u044c \\u0441\\u043a\\u043e\\u043b\\u044c\\u043a\\u043e \\u043f\\u0440\\u043e\\u0446\\u0435\\u043d\\u0442\\u043e\\u0432 \\u0441\\u043e\\u0441\\u0442\\u0430\\u0432\\u043b\\u044f\\u0435\\u0442 \\u0447\\u0438\\u0441\\u043b\\u043e `part` \\u043e\\u0442 \\u0447\\u0438\\u0441\\u043b\\u0430 `whole`\", \"library\": []}", "generation": "Scenario 1:\npandas, matplotlib, math\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport math\n\n# Constants\nDATA = pd.DataFrame({\n    'Category': ['A', 'B', 'C', 'D', 'E'],\n    'Whole': [100, 200, 300, 400, 500],\n    'Part': [50, 100, 150, 200, 250]\n})\n\ndef plot_percentage_of_whole(data):\n    \"\"\"\n    Calculate the percentage that each 'Part' value is of the 'Whole' value in the given DataFrame, \n    and plot a bar chart of the results.\n\n    Parameters:\n    data (DataFrame): The DataFrame containing the data.\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - math\n\n    Example:\n    >>> plot_percentage_of_whole(DATA)\n    \"\"\"\n    data['Percentage'] = (data['Part'] / data['Whole']) * 100\n    data['Percentage'] = data['Percentage'].apply(math.ceil)\n\n    data.plot(x='Category', y='Percentage', kind='bar', legend=False)\n    plt.ylabel('Percentage')\n    plt.title('Percentage of Part to Whole by Category')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, matplotlib, numpy\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nDATA = pd.DataFrame({\n    'Category': ['A', 'B', 'C', 'D', 'E'],\n    'Whole': [100, 200, 300, 400, 500],\n    'Part': [50, 100, 150, 200, 250]\n})\n\ndef plot_cumulative_percentage(data):\n    \"\"\"\n    Calculate the cumulative percentage that each 'Part' value is of the total 'Whole' value in the given DataFrame, \n    and plot a line chart of the results.\n\n    Parameters:\n    data (DataFrame): The DataFrame containing the data.\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - numpy\n\n    Example:\n    >>> plot_cumulative_percentage(DATA)\n    \"\"\"\n    data['Cumulative Percentage'] = (data['Part'].cumsum() / data['Whole'].sum()) * 100\n\n    plt.plot(data['Category'], data['Cumulative Percentage'])\n    plt.ylabel('Cumulative Percentage')\n    plt.title('Cumulative Percentage of Part to Total Whole by Category')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, sklearn.metrics\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error\n\n# Constants\nDATA = pd.DataFrame({\n    'Actual': [100, 200, 300, 400, 500],\n    'Predicted': [90, 210, 290, 410, 490]\n})\n\ndef calculate_mse(data):\n    \"\"\"\n    Calculate the Mean Squared Error (MSE) between the 'Actual' and 'Predicted' columns in the given DataFrame.\n\n    Parameters:\n    data (DataFrame): The DataFrame containing the data.\n\n    Returns:\n    float: The MSE.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.metrics\n\n    Example:\n    >>> calculate_mse(DATA)\n    \"\"\"\n    mse = mean_squared_error(data['Actual'], data['Predicted'])\n\n    return mse\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, scipy.stats\n```python\nimport pandas as pd\nfrom scipy import stats\n\n# Constants\nDATA = pd.DataFrame({\n    'Category': ['A', 'B', 'C', 'D', 'E'],\n    'Whole': [100, 200, 300, 400, 500],\n    'Part': [50, 100, 150, 200, 250]\n})\n\ndef calculate_spearman_correlation(data):\n    \"\"\"\n    Calculate the Spearman rank-order correlation coefficient between the 'Whole' and 'Part' columns in the given DataFrame.\n\n    Parameters:\n    data (DataFrame): The DataFrame containing the data.\n\n    Returns:\n    float: The correlation coefficient.\n\n    Requirements:\n    - pandas\n    - scipy.stats\n\n    Example:\n    >>> calculate_spearman_correlation(DATA)\n    \"\"\"\n    correlation, _ = stats.spearmanr(data['Whole'], data['Part'])\n\n    return correlation\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nDATA = pd.DataFrame({\n    'Category': ['A', 'B', 'C', 'D', 'E'],\n    'Whole': [100, 200, 300, 400, 500],\n    'Part': [50, 100, 150, 200, 250]\n})\n\ndef plot_percentage_difference(data):\n    \"\"\"\n    Calculate the percentage difference between the 'Whole' and 'Part' columns in the given DataFrame, \n    and plot a histogram of the results.\n\n    Parameters:\n    data (DataFrame): The DataFrame containing the data.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_percentage_difference(DATA)\n    \"\"\"\n    data['Percentage Difference'] = ((data['Whole'] - data['Part']) / data['Whole']) * 100\n\n    plt.hist(data['Percentage Difference'], bins=5, edgecolor='black')\n    plt.xlabel('Percentage Difference')\n    plt.title('Histogram of Percentage Difference between Whole and Part')\n    plt.show()\n```"}
{"seed": "{\"task_id\": 712792, \"prompt\": \"def f_712792(s):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"s.title()\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('test') == 'Test'\\n\", \"\\n    assert candidate('Test') == 'Test'\\n\", \"\\n    assert candidate('TEST') == 'Test'\\n\", \"\\n    assert candidate('  ') == '  '\\n\"], \"entry_point\": \"f_712792\", \"intent\": \"\\u0421\\u0434\\u0435\\u043b\\u0430\\u0442\\u044c \\u0437\\u0430\\u0433\\u043b\\u0430\\u0432\\u043d\\u043e\\u0439 \\u043f\\u0435\\u0440\\u0432\\u0443\\u044e \\u0431\\u0443\\u043a\\u0432\\u0443 \\u043a\\u0430\\u0436\\u0434\\u043e\\u0433\\u043e \\u0441\\u043b\\u043e\\u0432\\u0430 \\u0432 \\u0441\\u0442\\u0440\\u043e\\u043a\\u0435 `s`\", \"library\": []}", "generation": "Scenario 1: \nre, nltk, string, collections\n```python\nimport re\nimport nltk\nfrom string import punctuation\nfrom collections import Counter\n\n# Constants\nSTOPWORDS = nltk.corpus.stopwords.words('english')\n\ndef word_frequency_analysis(text):\n    \"\"\"\n    Count the frequency of each non-stopword that appears in a text string and return the top 5 most common ones.\n\n    Parameters:\n    text (str): The text string to be analyzed.\n\n    Returns:\n    List[Tuple[str, int]]: A list of tuples where the first element is a word and the second is its frequency.\n\n    Requirements:\n    - re\n    - nltk\n    - string\n    - collections\n\n    Example:\n    >>> word_frequency_analysis('This is a simple test. This is only a test.')\n    [('test', 2), ('simple', 1)]\n    \"\"\"\n    words = re.sub('['+punctuation+']', '', text).lower().split()\n    words = [word for word in words if word not in STOPWORDS]\n    word_counts = Counter(words)\n\n    return word_counts.most_common(5)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nre, nltk, string, collections\n```python\nimport re\nimport nltk\nfrom string import punctuation\nfrom collections import Counter\n\n# Constants\nSTOPWORDS = nltk.corpus.stopwords.words('english')\n\ndef sentence_word_analysis(text):\n    \"\"\"\n    Break a text string into sentences and analyze the frequency of non-stopwords in each sentence.\n\n    Parameters:\n    text (str): The text string to be analyzed.\n\n    Returns:\n    Dict[str, Dict[str, int]]: A dictionary where the keys are sentence strings and the values are dictionaries \n    of word frequencies.\n\n    Requirements:\n    - re\n    - nltk\n    - string\n    - collections\n\n    Example:\n    >>> sentence_word_analysis('This is a simple test. This is only a test.')\n    {'This is a simple test.': {'test': 1, 'simple': 1}, 'This is only a test.': {'test': 1}}\n    \"\"\"\n    sentences = nltk.sent_tokenize(text)\n    analysis = {}\n\n    for sentence in sentences:\n        words = re.sub('['+punctuation+']', '', sentence).lower().split()\n        words = [word for word in words if word not in STOPWORDS]\n        word_counts = Counter(words)\n        analysis[sentence] = dict(word_counts)\n\n    return analysis\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nre, nltk, string, collections\n```python\nimport re\nimport nltk\nfrom string import punctuation\nfrom collections import Counter\n\n# Constants\nSTOPWORDS = nltk.corpus.stopwords.words('english')\n\ndef word_pair_analysis(text):\n    \"\"\"\n    Analyze the frequency of pairs of successive non-stopwords in a text string.\n\n    Parameters:\n    text (str): The text string to be analyzed.\n\n    Returns:\n    Dict[Tuple[str, str], int]: A dictionary where the keys are word pairs and the values are frequencies.\n\n    Requirements:\n    - re\n    - nltk\n    - string\n    - collections\n\n    Example:\n    >>> word_pair_analysis('This is a simple test. This is only a test.')\n    {(('test', 'simple'), 1), ('test', 'test'): 1}\n    \"\"\"\n    words = re.sub('['+punctuation+']', '', text).lower().split()\n    words = [word for word in words if word not in STOPWORDS]\n    word_pairs = list(zip(words[:-1], words[1:]))\n    pair_counts = Counter(word_pairs)\n\n    return dict(pair_counts)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nre, nltk, string, collections\n```python\nimport re\nimport nltk\nfrom string import punctuation\nfrom collections import Counter\n\n# Constants\nSTOPWORDS = nltk.corpus.stopwords.words('english')\n\ndef sentence_length_analysis(text):\n    \"\"\"\n    Analyze the lengths of sentences in a text string.\n\n    Parameters:\n    text (str): The text string to be analyzed.\n\n    Returns:\n    Dict[str, int]: A dictionary where the keys are sentence strings and the values are their lengths in words.\n\n    Requirements:\n    - re\n    - nltk\n    - string\n    - collections\n\n    Example:\n    >>> sentence_length_analysis('This is a simple test. This is only a test.')\n    {'This is a simple test.': 4, 'This is only a test.': 4}\n    \"\"\"\n    sentences = nltk.sent_tokenize(text)\n    analysis = {}\n\n    for sentence in sentences:\n        words = re.sub('['+punctuation+']', '', sentence).lower().split()\n        words = [word for word in words if word not in STOPWORDS]\n        analysis[sentence] = len(words)\n\n    return analysis\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nre, nltk, string, collections\n```python\nimport re\nimport nltk\nfrom string import punctuation\nfrom collections import Counter\n\n# Constants\nSTOPWORDS = nltk.corpus.stopwords.words('english')\n\ndef sentence_word_pair_analysis(text):\n    \"\"\"\n    Break a text string into sentences and analyze the frequency of pairs of successive non-stopwords in each sentence.\n\n    Parameters:\n    text (str): The text string to be analyzed.\n\n    Returns:\n    Dict[str, Dict[Tuple[str, str], int]]: A dictionary where the keys are sentence strings and the values are \n    dictionaries of word pair frequencies.\n\n    Requirements:\n    - re\n    - nltk\n    - string\n    - collections\n\n    Example:\n    >>> sentence_word_pair_analysis('This is a simple test. This is only a test.')\n    {'This is a simple test.': {('test', 'simple'): 1}, 'This is only a test.': {('test', 'test'): 1}}\n    \"\"\"\n    sentences = nltk.sent_tokenize(text)\n    analysis = {}\n\n    for sentence in sentences:\n        words = re.sub('['+punctuation+']', '', sentence).lower().split()\n        words = [word for word in words if word not in STOPWORDS]\n        word_pairs = list(zip(words[:-1], words[1:]))\n        pair_counts = Counter(word_pairs)\n        analysis[sentence] = dict(pair_counts)\n\n    return analysis\n```\n"}
{"seed": "{\"task_id\": 775089, \"prompt\": \"def f_775089(func):\\n\\t\", \"suffix\": \"\\n\\treturn time_work\", \"canonical_solution\": \"t = time.time()\\n\\n\\tfunc()\\n\\n\\ttime_work = time.time()-t\", \"test_start\": \"\\nimport time \\n\\ndef check(candidate):\", \"test\": [\"\\n    def print_str(): return \\\"hello world!\\\"\\n    assert candidate(print_str) < 10\\n\", \"\\n    def sleep_one_sec(): time.sleep(1)\\n    assert 1 < candidate(sleep_one_sec) < 10\\n\"], \"entry_point\": \"f_775089\", \"intent\": \"\\u0418\\u0437\\u043c\\u0435\\u0440\\u0438\\u0442\\u044c \\u0432\\u0440\\u0435\\u043c\\u044f \\u0432\\u044b\\u043f\\u043e\\u043b\\u043d\\u0435\\u043d\\u0438\\u044f `time_work` \\u0444\\u0443\\u043d\\u043a\\u0446\\u0438\\u0438 `func`\", \"library\": [\"time\"]}", "generation": "Scenario 1:\ntime, random, matplotlib.pyplot, numpy, statistics\n```python\nimport time\nimport random\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport statistics as stats\n\n# Constants\nFUNC_LIST = [np.sin, np.cos, np.tan, np.log, np.sqrt]\n\ndef analyze_function_performance():\n    \"\"\"\n    Analyze and plot the performance of a list of functions by calculating the runtime \n    for each function over a series of inputs.\n\n    Returns:\n    None\n\n    Requirements:\n    - time\n    - random\n    - matplotlib.pyplot\n    - numpy\n    - statistics\n\n    Example:\n    >>> analyze_function_performance()\n    \"\"\"\n    # Data to hold function names and corresponding runtimes\n    func_runtimes = {}\n    \n    for func in FUNC_LIST:\n        runtimes = []\n        for _ in range(1000):\n            rand_num = random.uniform(1, 1000)\n            start_time = time.time()\n            func(rand_num)\n            end_time = time.time()\n            runtimes.append(end_time - start_time)\n        func_runtimes[func.__name__] = runtimes\n\n    # Plotting the function runtimes\n    plt.boxplot(func_runtimes.values(), labels=func_runtimes.keys())\n    plt.ylabel('Runtime (seconds)')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ntime, threading, queue, random\n```python\nimport time\nimport threading\nimport queue\nimport random\n\n# Constants\nTHREAD_COUNT = 5\n\ndef multi_threaded_task_processing():\n    \"\"\"\n    Simulate a multi-threaded environment where tasks (represented as time delays) are processed \n    by multiple threads. Calculate the total time taken to process all tasks.\n\n    Returns:\n    float: The total time taken to process all tasks.\n\n    Requirements:\n    - time\n    - threading\n    - queue\n    - random\n\n    Example:\n    >>> total_time = multi_threaded_task_processing()\n    \"\"\"\n    task_q = queue.Queue()\n\n    # Generate 100 tasks\n    for _ in range(100):\n        task_q.put(random.uniform(0.1, 1))\n\n    def worker():\n        while not task_q.empty():\n            task = task_q.get()\n            time.sleep(task)  # Simulate task processing\n            task_q.task_done()\n\n    start_time = time.time()\n\n    # Start threads\n    threads = []\n    for _ in range(THREAD_COUNT):\n        t = threading.Thread(target=worker)\n        t.start()\n        threads.append(t)\n\n    # Wait for all tasks to be processed\n    task_q.join()\n\n    # Calculate total time\n    total_time = time.time() - start_time\n\n    return total_time\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ntime, hashlib, string, random\n```python\nimport time\nimport hashlib\nimport string\nimport random\n\n# Constants\nHASH_COUNT = 1000\n\ndef hash_performance():\n    \"\"\"\n    Measure the time taken to generate a certain number of hashes using different \n    hash algorithms.\n\n    Returns:\n    dict: A dictionary with hash algorithms as keys and the corresponding times as values.\n\n    Requirements:\n    - time\n    - hashlib\n    - string\n    - random\n\n    Example:\n    >>> times = hash_performance()\n    \"\"\"\n    hash_algorithms = ['md5', 'sha1', 'sha256', 'sha512']\n    times = {}\n\n    for algo in hash_algorithms:\n        start_time = time.time()\n        for _ in range(HASH_COUNT):\n            data = ''.join(random.choices(string.ascii_letters + string.digits, k=100))\n            hash_obj = hashlib.new(algo)\n            hash_obj.update(data.encode('utf-8'))\n        times[algo] = time.time() - start_time\n\n    return times\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ntime, random, numpy, scipy.stats\n```python\nimport time\nimport random\nimport numpy as np\nfrom scipy import stats\n\n# Constants\nSAMPLE_SIZE = 10000\n\ndef sample_statistic_performance():\n    \"\"\"\n    Measure the time taken to calculate various statistical measures (mean, median, mode) \n    on a large sample of random numbers.\n\n    Returns:\n    dict: A dictionary with statistical measures as keys and the corresponding times as values.\n\n    Requirements:\n    - time\n    - random\n    - numpy\n    - scipy.stats\n\n    Example:\n    >>> times = sample_statistic_performance()\n    \"\"\"\n    sample = [random.randint(1, 100) for _ in range(SAMPLE_SIZE)]\n    statistics = ['mean', 'median', 'mode']\n    times = {}\n\n    for stat in statistics:\n        start_time = time.time()\n        if stat == 'mean':\n            np.mean(sample)\n        elif stat == 'median':\n            np.median(sample)\n        elif stat == 'mode':\n            stats.mode(sample)\n        times[stat] = time.time() - start_time\n\n    return times\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\ntime, requests, threading, queue\n```python\nimport time\nimport requests\nimport threading\nimport queue\n\n# Constants\nURL = 'https://httpbin.org/get'\nTHREAD_COUNT = 5\n\ndef measure_request_times():\n    \"\"\"\n    Measure the time taken to make a certain number of HTTP GET requests to a URL \n    in a multi-threaded environment.\n\n    Returns:\n    list: A list of times taken for each request.\n\n    Requirements:\n    - time\n    - requests\n    - threading\n    - queue\n\n    Example:\n    >>> times = measure_request_times()\n    \"\"\"\n    request_q = queue.Queue()\n\n    # Add 100 requests to the queue\n    for _ in range(100):\n        request_q.put(URL)\n\n    times = []\n\n    def worker():\n        while not request_q.empty():\n            url = request_q.get()\n            start_time = time.time()\n            requests.get(url)\n            end_time = time.time()\n            times.append(end_time - start_time)\n            request_q.task_done()\n\n    # Start threads\n    threads = []\n    for _ in range(THREAD_COUNT):\n        t = threading.Thread(target=worker)\n        t.start()\n        threads.append(t)\n\n    # Wait for all requests to be processed\n    request_q.join()\n\n    return times\n```"}
{"seed": "{\"task_id\": 157675, \"prompt\": \"def f_157675(s):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"re.findall('(\\\\d+)', s)\", \"test_start\": \"\\nimport re\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('123abc456') == ['123', '456']\\n\", \"\\n    assert candidate('123456') == ['123456'] \\n\", \"\\n    assert candidate('afbnjig') == []\\n\"], \"entry_point\": \"f_157675\", \"intent\": \"\\u041d\\u0430\\u0439\\u0442\\u0438 \\u0447\\u0438\\u0441\\u043b\\u0430 \\u0432 \\u0441\\u0442\\u0440\\u043e\\u043a\\u0435 `s`\", \"library\": [\"re\"]}", "generation": "Scenario 1: \n```python\nimport re\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n\n# Constants\nDIGITS = [str(i) for i in range(10)]\n\ndef analyze_digit_frequency(s):\n    \"\"\"\n    Find all numbers in the string `s` and analyze their digit frequency.\n    Plot a bar chart showing the frequency of each digit.\n\n    Parameters:\n    s (str): The string to analyze.\n    \n    Returns:\n    dict: A dictionary containing the frequency of each digit.\n\n    Requirements:\n    - re\n    - numpy\n    - matplotlib.pyplot\n    - collections.Counter\n\n    Example:\n    >>> analyze_digit_frequency('123abc456')\n    >>> analyze_digit_frequency('123456')\n    >>> analyze_digit_frequency('afbnjig')\n    \"\"\"\n    numbers = re.findall('(\\d+)', s)\n    digit_frequency = Counter(''.join(numbers))\n    \n    # Plot a bar chart of digit frequency\n    plt.bar(DIGITS, [digit_frequency.get(digit, 0) for digit in DIGITS])\n    plt.xlabel('Digits')\n    plt.ylabel('Frequency')\n    plt.title('Digit Frequency')\n    plt.show()\n\n    return digit_frequency\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\n```python\nimport re\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\n\ndef plot_number_frequency(s):\n    \"\"\"\n    Find all numbers in the string `s` and plot their frequencies.\n\n    Parameters:\n    s (str): The string to analyze.\n    \n    Returns:\n    dict: A dictionary containing the frequency of each number.\n\n    Requirements:\n    - re\n    - collections.defaultdict\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_number_frequency('123abc456abc789')\n    >>> plot_number_frequency('123456abc789')\n    >>> plot_number_frequency('afbnjig')\n    \"\"\"\n    numbers = re.findall('(\\d+)', s)\n    number_frequency = defaultdict(int)\n\n    for number in numbers:\n        number_frequency[number] += 1\n\n    # Plot a bar chart of number frequency\n    plt.bar(number_frequency.keys(), number_frequency.values())\n    plt.xlabel('Numbers')\n    plt.ylabel('Frequency')\n    plt.title('Number Frequency')\n    plt.show()\n\n    return number_frequency\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\n```python\nimport re\nimport numpy as np\nfrom scipy import stats\n\ndef compute_statistics(s):\n    \"\"\"\n    Find all numbers in the string `s` and compute their statistical properties\n    including mean, median, mode, and standard deviation.\n\n    Parameters:\n    s (str): The string to analyze.\n    \n    Returns:\n    dict: A dictionary containing the statistical properties.\n\n    Requirements:\n    - re\n    - numpy\n    - scipy.stats\n\n    Example:\n    >>> compute_statistics('123abc456abc789')\n    >>> compute_statistics('123456abc789')\n    >>> compute_statistics('afbnjig')\n    \"\"\"\n    numbers = [int(num) for num in re.findall('(\\d+)', s)]\n    if numbers:\n        statistics = {\n            'mean': np.mean(numbers),\n            'median': np.median(numbers),\n            'mode': stats.mode(numbers)[0][0],\n            'std_dev': np.std(numbers)\n        }\n    else:\n        statistics = {\n            'mean': None,\n            'median': None,\n            'mode': None,\n            'std_dev': None\n        }\n\n    return statistics\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\n```python\nimport re\nfrom statistics import mean, median\n\ndef calculate_mean_median(s):\n    \"\"\"\n    Find all numbers in the string `s` and calculate their mean and median values.\n\n    Parameters:\n    s (str): The string to analyze.\n    \n    Returns:\n    dict: A dictionary containing the mean and median values.\n\n    Requirements:\n    - re\n    - statistics.mean\n    - statistics.median\n\n    Example:\n    >>> calculate_mean_median('123abc456abc789')\n    >>> calculate_mean_median('123456abc789')\n    >>> calculate_mean_median('afbnjig')\n    \"\"\"\n    numbers = [int(num) for num in re.findall('(\\d+)', s)]\n    if numbers:\n        statistics = {\n            'mean': mean(numbers),\n            'median': median(numbers)\n        }\n    else:\n        statistics = {\n            'mean': None,\n            'median': None\n        }\n\n    return statistics\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\n```python\nimport re\nimport numpy as np\n\ndef calculate_sum_product(s):\n    \"\"\"\n    Find all numbers in the string `s` and calculate their sum and product.\n\n    Parameters:\n    s (str): The string to analyze.\n    \n    Returns:\n    dict: A dictionary containing the sum and product.\n\n    Requirements:\n    - re\n    - numpy\n\n    Example:\n    >>> calculate_sum_product('123abc456abc789')\n    >>> calculate_sum_product('123456abc789')\n    >>> calculate_sum_product('afbnjig')\n    \"\"\"\n    numbers = np.array([int(num) for num in re.findall('(\\d+)', s)])\n    if numbers.size:\n        calculations = {\n            'sum': np.sum(numbers),\n            'product': np.prod(numbers)\n        }\n    else:\n        calculations = {\n            'sum': None,\n            'product': None\n        }\n\n    return calculations\n```"}
{"seed": "{\"task_id\": 210870, \"prompt\": \"def f_210870(num, file_path):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"file_1 = open(file_path, \\\"w\\\")\\n\\n\\tfile_1.write(str(num))\\n\\tfile_1.close()\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    candidate(12.34, \\\"tmp.txt\\\")\\n    with open('tmp.txt', 'r') as fr:\\n        lines = fr.readlines()\\n    assert len(lines) == 1 and lines[0] == '12.34'\\n\", \"\\n    candidate(0, \\\"tmp.txt\\\")\\n    with open('tmp.txt', 'r') as fr:\\n        lines = fr.readlines()\\n    assert len(lines) == 1 and lines[0] == '0'\\n\"], \"entry_point\": \"f_210870\", \"intent\": \"\\u0417\\u0430\\u043f\\u0438\\u0441\\u0430\\u0442\\u044c \\u0447\\u0438\\u0441\\u043b\\u043e `nums` \\u0432 \\u0444\\u0430\\u0439\\u043b `file_path`\", \"library\": []}", "generation": "Scenario 3: \nos, json, random, datetime\n```python\nimport os\nimport json\nfrom random import randint\nfrom datetime import datetime\n\n# Constants\nNUMBERS = list(range(100))\nFILE_PATH = 'data.json'\nTIMESTAMP_FORMAT = '%Y-%m-%dT%H:%M:%S'\n\ndef generate_and_save_random_data():\n    \"\"\"\n    Generate a dictionary with a random number, current timestamp and save it in a JSON file.\n\n    Requirements:\n    - os\n    - json\n    - random\n    - datetime\n\n    Example:\n    >>> generate_and_save_random_data()\n    >>> with open(FILE_PATH, 'r') as f:\n    ...     data = json.load(f)\n    ...     print(data)\n    \"\"\"\n    data = {\n        'number': NUMBERS[randint(0, len(NUMBERS)-1)],\n        'timestamp': datetime.now().strftime(TIMESTAMP_FORMAT)\n    }\n\n    with open(FILE_PATH, 'w') as f:\n        json.dump(data, f)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, csv, random, datetime\n```python\nimport os\nimport csv\nfrom random import randint\nfrom datetime import datetime\n\n# Constants\nNUMBERS = list(range(100))\nFILE_PATH = 'data.csv'\nTIMESTAMP_FORMAT = '%Y-%m-%dT%H:%M:%S'\n\ndef generate_and_save_random_data_to_csv():\n    \"\"\"\n    Generate a row with a random number, current timestamp and save it in a CSV file.\n\n    Requirements:\n    - os\n    - csv\n    - random\n    - datetime\n\n    Example:\n    >>> generate_and_save_random_data_to_csv()\n    >>> with open(FILE_PATH, 'r') as f:\n    ...     reader = csv.reader(f)\n    ...     for row in reader:\n    ...         print(row)\n    \"\"\"\n    data = [NUMBERS[randint(0, len(NUMBERS)-1)], datetime.now().strftime(TIMESTAMP_FORMAT)]\n\n    with open(FILE_PATH, 'a', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(data)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, json, random, datetime, shutil\n```python\nimport os\nimport json\nfrom random import randint\nfrom datetime import datetime\nimport shutil\n\n# Constants\nNUMBERS = list(range(100))\nFILE_PATH = 'data.json'\nBACKUP_PATH = 'backup/data.json'\nTIMESTAMP_FORMAT = '%Y-%m-%dT%H:%M:%S'\n\ndef generate_save_and_backup_random_data():\n    \"\"\"\n    Generate a dictionary with a random number, current timestamp, save it in a JSON file \n    and create a backup of the file.\n\n    Requirements:\n    - os\n    - json\n    - random\n    - datetime\n    - shutil\n\n    Example:\n    >>> generate_save_and_backup_random_data()\n    >>> with open(BACKUP_PATH, 'r') as f:\n    ...     data = json.load(f)\n    ...     print(data)\n    \"\"\"\n    data = {\n        'number': NUMBERS[randint(0, len(NUMBERS)-1)],\n        'timestamp': datetime.now().strftime(TIMESTAMP_FORMAT)\n    }\n\n    with open(FILE_PATH, 'w') as f:\n        json.dump(data, f)\n\n    shutil.copy2(FILE_PATH, BACKUP_PATH)\n```\n`GPT_ODEX_BREAK`\n\nScenario 6:\nos, csv, random, datetime, shutil\n```python\nimport os\nimport csv\nfrom random import randint\nfrom datetime import datetime\nimport shutil\n\n# Constants\nNUMBERS = list(range(100))\nFILE_PATH = 'data.csv'\nBACKUP_PATH = 'backup/data.csv'\nTIMESTAMP_FORMAT = '%Y-%m-%dT%H:%M:%S'\n\ndef generate_save_and_backup_random_data_to_csv():\n    \"\"\"\n    Generate a row with a random number, current timestamp, save it in a CSV file \n    and create a backup of the file.\n\n    Requirements:\n    - os\n    - csv\n    - random\n    - datetime\n    - shutil\n\n    Example:\n    >>> generate_save_and_backup_random_data_to_csv()\n    >>> with open(BACKUP_PATH, 'r') as f:\n    ...     reader = csv.reader(f)\n    ...     for row in reader:\n    ...         print(row)\n    \"\"\"\n    data = [NUMBERS[randint(0, len(NUMBERS)-1)], datetime.now().strftime(TIMESTAMP_FORMAT)]\n\n    with open(FILE_PATH, 'a', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(data)\n\n    shutil.copy2(FILE_PATH, BACKUP_PATH)\n```\n`GPT_ODEX_BREAK`\n\nScenario 7:\nos, json, random, datetime, shutil, hashlib\n```python\nimport os\nimport json\nfrom random import randint\nfrom datetime import datetime\nimport shutil\nimport hashlib\n\n# Constants\nNUMBERS = list(range(100))\nFILE_PATH = 'data.json'\nBACKUP_PATH = 'backup/data.json'\nTIMESTAMP_FORMAT = '%Y-%m-%dT%H:%M:%S'\n\ndef generate_save_backup_and_hash_random_data():\n    \"\"\"\n    Generate a dictionary with a random number, current timestamp, save it in a JSON file, \n    create a backup of the file and return the MD5 hash of the file.\n\n    Requirements:\n    - os\n    - json\n    - random\n    - datetime\n    - shutil\n    - hashlib\n\n    Example:\n    >>> hash = generate_save_backup_and_hash_random_data()\n    >>> print(hash)\n    \"\"\"\n    data = {\n        'number': NUMBERS[randint(0, len(NUMBERS)-1)],\n        'timestamp': datetime.now().strftime(TIMESTAMP_FORMAT)\n    }\n\n    with open(FILE_PATH, 'w') as f:\n        json.dump(data, f)\n\n    shutil.copy2(FILE_PATH, BACKUP_PATH)\n\n    with open(FILE_PATH, 'rb') as f:\n        bytes = f.read()\n        readable_hash = hashlib.md5(bytes).hexdigest()\n\n    return readable_hash\n```"}
{"seed": "{\"task_id\": 210870, \"prompt\": \"def f_210870(num, file_path):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"\\n\\twith open(file_path, 'w') as f:\\n\\t\\tprint(str(num), file=f)\\n\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    candidate(12.34, \\\"tmp.txt\\\")\\n    with open('tmp.txt', 'r') as fr:\\n        lines = fr.readlines()\\n    assert len(lines) == 1 and lines[0] == '12.34\\\\n'\\n\"], \"entry_point\": \"f_210870\", \"intent\": \"\\u0417\\u0430\\u043f\\u0438\\u0441\\u0430\\u0442\\u044c \\u0447\\u0438\\u0441\\u043b\\u043e `nums` \\u0432 \\u0444\\u0430\\u0439\\u043b `file_path`\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, os, json\n```python\nimport pandas as pd\nimport numpy as np\nimport os\nimport json\n\n# Constants\nFILE_PATH = 'data.txt'\nJSON_FILE_PATH = 'data.json'\n\ndef generate_and_save_data(num, file_path=FILE_PATH, json_file_path=JSON_FILE_PATH):\n    \"\"\"\n    Generate a pandas DataFrame with random numbers, save the DataFrame to a CSV file and\n    save the descriptive statistics of the DataFrame to a JSON file.\n    \n    Parameters:\n    num (int): The number of random numbers to generate.\n    file_path (str): The path of the CSV file to save the DataFrame.\n    json_file_path (str): The path of the JSON file to save the statistics.\n    \n    Returns:\n    DataFrame: The generated pandas DataFrame.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - os\n    - json\n    \n    Example:\n    >>> df = generate_and_save_data(10)\n    >>> print(df)\n    \"\"\"\n    df = pd.DataFrame(np.random.rand(num, 1), columns=['Number'])\n    df.to_csv(file_path, index=False)\n\n    stats = df.describe().to_dict()\n    with open(json_file_path, 'w') as f:\n        json.dump(stats, f)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \npandas, os, csv, matplotlib\n```python\nimport pandas as pd\nimport os\nimport csv\nimport matplotlib.pyplot as plt\n\n# Constants\nFILE_PATH = 'data.txt'\nN_BINS = 10\n\ndef load_and_visualize_data(file_path=FILE_PATH, num_bins=N_BINS):\n    \"\"\"\n    Load data from a CSV file into a pandas DataFrame, calculate the histogram \n    of the data and visualize it using matplotlib.\n    \n    Parameters:\n    file_path (str): The path of the CSV file to load the data from.\n    num_bins (int): The number of bins to use for the histogram.\n    \n    Returns:\n    DataFrame: The loaded pandas DataFrame.\n    \n    Requirements:\n    - pandas\n    - os\n    - csv\n    - matplotlib\n    \n    Example:\n    >>> df = load_and_visualize_data()\n    >>> print(df)\n    \"\"\"\n    df = pd.read_csv(file_path)\n    plt.hist(df['Number'], bins=num_bins)\n    plt.show()\n    \n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnumpy, os, csv, random\n```python\nimport numpy as np\nimport os\nimport csv\nimport random\n\n# Constants\nFILE_PATH = 'data.txt'\n\ndef generate_and_save_random_integers(num, file_path=FILE_PATH):\n    \"\"\"\n    Generate a numpy array of random integers and save them to a CSV file.\n    \n    Parameters:\n    num (int): The number of random integers to generate.\n    file_path (str): The path of the CSV file to save the array.\n    \n    Returns:\n    ndarray: The generated numpy array.\n    \n    Requirements:\n    - numpy\n    - os\n    - csv\n    - random\n    \n    Example:\n    >>> arr = generate_and_save_random_integers(10)\n    >>> print(arr)\n    \"\"\"\n    arr = np.random.randint(0, 100, size=num)\n    np.savetxt(file_path, arr, fmt='%d', delimiter=',')\n    \n    return arr\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nos, csv, random, statistics\n```python\nimport os\nimport csv\nimport random\nimport statistics\n\n# Constants\nFILE_PATH = 'data.txt'\n\ndef load_and_calculate_statistics(file_path=FILE_PATH):\n    \"\"\"\n    Load integers from a CSV file into a list, calculate and print the mean, median, \n    mode, variance, and standard deviation of the integers.\n    \n    Parameters:\n    file_path (str): The path of the CSV file to load the integers from.\n    \n    Returns:\n    list: The list of integers.\n    \n    Requirements:\n    - os\n    - csv\n    - random\n    - statistics\n    \n    Example:\n    >>> numbers = load_and_calculate_statistics()\n    >>> print(numbers)\n    \"\"\"\n    with open(file_path, 'r') as f:\n        reader = csv.reader(f)\n        numbers = list(map(int, next(reader)))\n        \n    print(f'Mean: {statistics.mean(numbers)}')\n    print(f'Median: {statistics.median(numbers)}')\n    print(f'Mode: {statistics.mode(numbers)}')\n    print(f'Variance: {statistics.variance(numbers)}')\n    print(f'Standard Deviation: {statistics.stdev(numbers)}')\n    \n    return numbers\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nos, csv, random, matplotlib\n```python\nimport os\nimport csv\nimport random\nimport matplotlib.pyplot as plt\n\n# Constants\nFILE_PATH = 'data.txt'\n\ndef load_and_visualize_numbers(file_path=FILE_PATH):\n    \"\"\"\n    Load integers from a CSV file into a list, sort the list, and visualize the integers \n    using a line plot.\n    \n    Parameters:\n    file_path (str): The path of the CSV file to load the integers from.\n    \n    Returns:\n    list: The sorted list of integers.\n    \n    Requirements:\n    - os\n    - csv\n    - random\n    - matplotlib\n    \n    Example:\n    >>> numbers = load_and_visualize_numbers()\n    >>> print(numbers)\n    \"\"\"\n    with open(file_path, 'r') as f:\n        reader = csv.reader(f)\n        numbers = sorted(list(map(int, next(reader))))\n        \n    plt.plot(numbers)\n    plt.show()\n    \n    return numbers\n```\n"}
{"seed": "{\"task_id\": 681594, \"prompt\": \"def f_681594(url, data):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"requests.post(url, json=data)\", \"test_start\": \"\\nimport requests\\n\\ndef check(candidate):\", \"test\": [\"\\n    url = 'https://www.google.com/'\\n    myobj = {'somekey': 'somevalue'}\\n    x = candidate(url, myobj)\\n    assert x.request.body == b'{\\\"somekey\\\": \\\"somevalue\\\"}'\\n    assert x.request.url == url\\n\"], \"entry_point\": \"f_681594\", \"intent\": \"\\u041e\\u0442\\u043f\\u0440\\u0430\\u0432\\u0438\\u0442\\u044c POST-\\u0437\\u0430\\u043f\\u0440\\u043e\\u0441 `res` c JSON \\u0434\\u0430\\u043d\\u043d\\u044b\\u043c\\u0438 `data` \\u043d\\u0430 `url`\", \"library\": [\"requests\"]}", "generation": "Scenario 1: \ncsv, requests, json, datetime\n```python\nimport csv\nimport requests\nimport json\nfrom datetime import datetime\n\n# Constants\nCSV_FILE = 'data.csv'\n\ndef post_data_from_csv_to_url(url):\n    \"\"\"\n    Get data from a CSV file, convert it to JSON format and send it as a POST request to a given URL.\n    \n    Parameters:\n    url (str): The URL to which the POST request is to be sent.\n    \n    Returns:\n    Response: The response from the server.\n    \n    Requirements:\n    - csv\n    - requests\n    - json\n    - datetime\n    \n    Example:\n    >>> post_data_from_csv_to_url('https://www.example.com/')\n    \"\"\"\n    data_list = []\n    \n    with open(CSV_FILE, 'r') as file:\n        reader = csv.DictReader(file)\n        for row in reader:\n            data_list.append(row)\n\n    data_json = json.dumps(data_list)\n    response = requests.post(url, data=data_json)\n\n    return response\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nrequests, bs4, pandas, json\n```python\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport json\n\ndef scrape_table_to_json(url, table_id):\n    \"\"\"\n    Scrape a table from a webpage and convert it to JSON format.\n    \n    Parameters:\n    url (str): The URL of the webpage containing the table.\n    table_id (str): The ID of the table on the webpage.\n    \n    Returns:\n    str: The table data in JSON format.\n    \n    Requirements:\n    - requests\n    - bs4.BeautifulSoup\n    - pandas\n    - json\n    \n    Example:\n    >>> scrape_table_to_json('https://www.example.com/', 'table1')\n    \"\"\"\n    response = requests.get(url)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    table = soup.find('table', {'id': table_id})\n    df = pd.read_html(str(table))[0]\n    json_data = df.to_json(orient='records')\n\n    return json_data\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nrequests, json, datetime, csv\n```python\nimport requests\nimport json\nfrom datetime import datetime\nimport csv\n\n# Constants\nCSV_FILE = 'output.csv'\n\ndef fetch_data_from_api_to_csv(url, api_key):\n    \"\"\"\n    Fetch data from an API, convert it to CSV format and save it in a file.\n    \n    Parameters:\n    url (str): The URL of the API.\n    api_key (str): The API key.\n    \n    Returns:\n    str: The path of the CSV file.\n\n    Requirements:\n    - requests\n    - json\n    - datetime\n    - csv\n\n    Example:\n    >>> fetch_data_from_api_to_csv('https://www.example.com/api', '123456')\n    \"\"\"\n    headers = {'api-key': api_key}\n    response = requests.get(url, headers=headers)\n    data = json.loads(response.text)\n\n    with open(CSV_FILE, 'w') as file:\n        writer = csv.DictWriter(file, fieldnames=data[0].keys())\n        writer.writeheader()\n        writer.writerows(data)\n    \n    return CSV_FILE\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nrequests, json, pandas, matplotlib.pyplot\n```python\nimport requests\nimport json\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef fetch_and_visualize_data_from_api(url):\n    \"\"\"\n    Fetch data from an API, convert it to a pandas DataFrame and visualize it using a bar plot.\n    \n    Parameters:\n    url (str): The URL of the API.\n    \n    Returns:\n    DataFrame: The DataFrame of the data.\n\n    Requirements:\n    - requests\n    - json\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = fetch_and_visualize_data_from_api('https://www.example.com/api')\n    >>> df.plot(kind='bar')\n    \"\"\"\n    response = requests.get(url)\n    data = json.loads(response.text)\n    df = pd.DataFrame(data)\n    df.plot(kind='bar')\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, requests, json, csv\n```python\nimport os\nimport requests\nimport json\nimport csv\n\n# Constants\nDIRECTORY = 'data'\n\ndef fetch_data_from_multiple_apis_to_csv(urls):\n    \"\"\"\n    Fetch data from multiple APIs, convert it to CSV format and save each in a separate file in a specified directory.\n    \n    Parameters:\n    urls (list): The list of URLs of the APIs.\n    \n    Returns:\n    list: The list of paths of the CSV files.\n\n    Requirements:\n    - os\n    - requests\n    - json\n    - csv\n\n    Example:\n    >>> fetch_data_from_multiple_apis_to_csv(['https://www.example1.com/api', 'https://www.example2.com/api'])\n    \"\"\"\n    if not os.path.exists(DIRECTORY):\n        os.makedirs(DIRECTORY)\n\n    csv_files = []\n\n    for url in urls:\n        response = requests.get(url)\n        data = json.loads(response.text)\n        csv_file = os.path.join(DIRECTORY, f'{url.split(\"/\")[-1]}.csv')\n\n        with open(csv_file, 'w') as file:\n            writer = csv.DictWriter(file, fieldnames=data[0].keys())\n            writer.writeheader()\n            writer.writerows(data)\n\n        csv_files.append(csv_file)\n\n    return csv_files\n```"}
{"seed": "{\"task_id\": 584129, \"prompt\": \"def f_584129(my_dict):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"json.dumps(my_dict, ensure_ascii=False)\", \"test_start\": \"\\nimport json\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate({'a': 123, 'Bb': None}) == json.dumps({'a': 123, 'Bb': None}, ensure_ascii=False)\\n\", \"\\n    assert candidate({123: 123, 'Bb': None}) == json.dumps({123: 123, 'Bb': None}, ensure_ascii=False)\\n\", \"\\n    assert candidate({}) == json.dumps({}, ensure_ascii=False)\\n\"], \"entry_point\": \"f_584129\", \"intent\": \"\\u041f\\u0440\\u0435\\u043e\\u0431\\u0440\\u0430\\u0437\\u043e\\u0432\\u0430\\u0442\\u044c \\u0440\\u0443\\u0441\\u0441\\u043a\\u043e\\u044f\\u0437\\u044b\\u0447\\u043d\\u044b\\u0439 \\u0441\\u043b\\u043e\\u0432\\u0430\\u0440\\u044c `my_dict` \\u0432 JSON \", \"library\": [\"json\"]}", "generation": "Scenario 1:\njson, os, hashlib, pathlib\n```python\nimport json\nimport os\nimport hashlib\nfrom pathlib import Path\n\n# Constants\nBUFFER_SIZE = 65536\n\ndef calculate_hash_and_save_json(my_dict, file_path):\n    \"\"\"\n    Calculate the SHA256 hash of a dictionary and save the dictionary as a json file.\n\n    Parameters:\n    my_dict (dict): The dictionary.\n    file_path (str): The path to save the file.\n\n    Returns:\n    str: The SHA256 hash of the dictionary.\n\n    Requirements:\n    - json\n    - os\n    - hashlib\n    - pathlib.Path\n\n    Example:\n    >>> my_dict = {'a': 123, 'b': None}\n    >>> file_path = '/path/to/file.json'\n    >>> calculate_hash_and_save_json(my_dict, file_path)\n    \"\"\"\n    json_str = json.dumps(my_dict, ensure_ascii=False)\n\n    sha256_hash = hashlib.sha256()\n    sha256_hash.update(json_str.encode('utf-8'))\n    hash_value = sha256_hash.hexdigest()\n\n    with open(file_path, 'w', encoding='utf-8') as f:\n        f.write(json_str)\n\n    return hash_value\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\njson, requests, time, hashlib, random\n```python\nimport json\nimport requests\nimport time\nimport hashlib\nfrom random import randint\n\n# Constants\nUSER_AGENTS = [\n    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537',\n    'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:54.0) Gecko/20100101 Firefox/54.0',\n    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36 Edge/15.15063'\n]\n\ndef fetch_url_and_calculate_hash(url):\n    \"\"\"\n    Fetch a URL, parse the content as JSON and calculate the SHA256 hash of the content.\n\n    Parameters:\n    url (str): The URL to fetch.\n\n    Returns:\n    str: The SHA256 hash of the content.\n\n    Requirements:\n    - json\n    - requests\n    - time\n    - hashlib\n    - random\n\n    Example:\n    >>> url = 'https://api.github.com/users/octocat'\n    >>> fetch_url_and_calculate_hash(url)\n    \"\"\"\n    headers = {'User-Agent': USER_AGENTS[randint(0, len(USER_AGENTS)-1)]}\n\n    response = requests.get(url, headers=headers)\n    response_json = response.json()\n    json_str = json.dumps(response_json, ensure_ascii=False)\n\n    sha256_hash = hashlib.sha256()\n    sha256_hash.update(json_str.encode('utf-8'))\n    hash_value = sha256_hash.hexdigest()\n\n    return hash_value\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\njson, csv, collections, re\n```python\nimport json\nimport csv\nimport collections\nimport re\n\ndef convert_csv_to_json(csv_file, json_file, delimiter=','):\n    \"\"\"\n    Convert a CSV file to a json file.\n\n    Parameters:\n    csv_file (str): The path of the csv file.\n    json_file (str): The path of the json file to be created.\n    delimiter (str, optional): Delimiter of the csv file. Defaults to ','.\n\n    Returns:\n    int: The number of rows converted.\n\n    Requirements:\n    - json\n    - csv\n    - collections\n    - re\n\n    Example:\n    >>> csv_file = '/path/to/file.csv'\n    >>> json_file = '/path/to/file.json'\n    >>> convert_csv_to_json(csv_file, json_file)\n    \"\"\"\n    data = []\n    with open(csv_file, 'r') as f:\n        reader = csv.DictReader(f, delimiter=delimiter)\n        for row in reader:\n            data.append(row)\n\n    with open(json_file, 'w', encoding='utf-8') as f:\n        f.write(json.dumps(data, ensure_ascii=False))\n\n    return len(data)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\njson, pandas, numpy, matplotlib.pyplot\n```python\nimport json\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef visualize_json_data(json_file, x_column, y_column):\n    \"\"\"\n    Load a json file into a pandas DataFrame and visualize the data using matplotlib.\n\n    Parameters:\n    json_file (str): The path of the json file.\n    x_column (str): The name of the column to use as the x-axis.\n    y_column (str): The name of the column to use as the y-axis.\n\n    Returns:\n    None\n\n    Requirements:\n    - json\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> json_file = '/path/to/file.json'\n    >>> visualize_json_data(json_file, 'Date', 'Price')\n    \"\"\"\n    with open(json_file, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n\n    df = pd.DataFrame(data)\n\n    plt.figure(figsize=(10,6))\n    plt.plot(df[x_column], df[y_column])\n    plt.xlabel(x_column)\n    plt.ylabel(y_column)\n    plt.title(f'{y_column} over {x_column}')\n    plt.grid(True)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\njson, requests, bs4, lxml\n```python\nimport json\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef scrape_webpage_and_save_as_json(url, json_file):\n    \"\"\"\n    Scrape a webpage and save the content as a json file.\n\n    Parameters:\n    url (str): The URL to scrape.\n    json_file (str): The path of the json file to be created.\n\n    Returns:\n    str: The content of the webpage.\n\n    Requirements:\n    - json\n    - requests\n    - bs4.BeautifulSoup\n    - lxml\n\n    Example:\n    >>> url = 'https://en.wikipedia.org/wiki/Web_scraping'\n    >>> json_file = '/path/to/file.json'\n    >>> scrape_webpage_and_save_as_json(url, json_file)\n    \"\"\"\n    response = requests.get(url)\n    soup = BeautifulSoup(response.text, 'lxml')\n\n    content = soup.get_text()\n\n    with open(json_file, 'w', encoding='utf-8') as f:\n        f.write(json.dumps(content, ensure_ascii=False))\n\n    return content\n```"}
{"seed": "{\"task_id\": 37137, \"prompt\": \"def f_37137(my_list1, my_list2):\\n\\t\", \"suffix\": \"\\n\\treturn common_items\", \"canonical_solution\": \"count_values_l1 = Counter(my_list1)\\n\\n\\tcount_values_l2 = Counter(my_list2)\\n\\n\\tkeys1 = count_values_l1.keys()\\n\\n\\tkeys2 = count_values_l2.keys()\\n\\n\\tcommon_items = []\\n\\n\\tfor key in keys1:\\n\\n\\t\\tif key in keys2:\\n\\n\\t\\t\\tcommon_items += [key] * min(count_values_l1[key], count_values_l2[key])\\n\\t\", \"test_start\": \"\\nfrom collections import Counter \\n\\ndef check(candidate):\", \"test\": [\"\\n    assert set(candidate([1,2,3.5,'a'], [3.4, 'bb', 'a', 1])) == set([1, 'a']) \\n\", \"\\n    assert set(candidate([1,2,3], [4,5,6])) == set([]) \\n\", \"\\n    assert set(candidate(['ger','ferags',34.56], ['ger','ferags',34.56])) == set(['ger','ferags',34.56]) \\n\"], \"entry_point\": \"f_37137\", \"intent\": \"\\u041d\\u0430\\u0439\\u0442\\u0438 \\u043e\\u0431\\u0449\\u0438\\u0435 \\u044d\\u043b\\u0435\\u043c\\u0435\\u043d\\u0442\\u044b `common_items` \\u0441\\u043f\\u0438\\u0441\\u043a\\u0430 `my_list1` \\u0438 \\u0441\\u043f\\u0438\\u0441\\u043a\\u0430 `my_list2` \", \"library\": [\"collections\"]}", "generation": "Scenario 1:\ncollections, pandas, numpy, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n\n# Constants\nCATEGORIES = ['Grocery', 'Electronics', 'Fashion', 'Health', 'Home']\n\ndef common_items_analysis(list1, list2):\n    \"\"\"\n    Find common items between two lists, generate their count statistics, and create a histogram for \n    each category of items.\n\n    Parameters:\n    list1 (list of tuples): Each tuple contains category and item from one source (e.g. ('Grocery', 'Apple'))\n    list2 (list of tuples): Each tuple contains category and item from another source (e.g. ('Grocery', 'Apple'))\n\n    Returns:\n    dict: A dictionary containing common items and their counts in each category. \n\n    Requirements:\n    - collections\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> items1 = [('Grocery', 'Apple'), ('Electronics', 'TV'), ('Fashion', 'Shirt'), ('Grocery', 'Banana')]\n    >>> items2 = [('Grocery', 'Banana'), ('Electronics', 'TV'), ('Fashion', 'Pants'), ('Grocery', 'Apple')]\n    >>> common_items_analysis(items1, items2)\n    \"\"\"\n    count_values_l1 = Counter(list1)\n    count_values_l2 = Counter(list2)\n\n    common_items = count_values_l1 & count_values_l2\n\n    common_items_per_category = {category: [] for category in CATEGORIES}\n    for item, count in common_items.items():\n        category, product = item\n        common_items_per_category[category].append((product, count))\n\n    for category, items in common_items_per_category.items():\n        if items:\n            df = pd.DataFrame(items, columns=['Item', 'Count'])\n            df.sort_values('Count', inplace=True)\n            plt.barh(df['Item'], df['Count'])\n            plt.xlabel('Count')\n            plt.title(f'Common items in {category}')\n            plt.show()\n\n    return common_items_per_category\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncollections, math, random, matplotlib\n```python\nfrom collections import Counter\nimport math\nimport random\nimport matplotlib.pyplot as plt\n\n# Constants\nRADIUS = 5\nCENTER = (0, 0)\n\ndef common_points_in_circle(points1, points2):\n    \"\"\"\n    Find common points between two sets of points inside a circle, calculate their distances \n    to the center of the circle, and plot a histogram of these distances.\n\n    Parameters:\n    points1 (list of tuples): Each tuple represents a point in 2D space from one set (e.g. (1.2, 2.3))\n    points2 (list of tuples): Each tuple represents a point in 2D space from another set (e.g. (1.2, 2.3))\n\n    Returns:\n    list: Common points and their distances to the center of the circle. \n\n    Requirements:\n    - collections\n    - math\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> points1 = [(random.uniform(-RADIUS, RADIUS), random.uniform(-RADIUS, RADIUS)) for _ in range(100)]\n    >>> points2 = [(random.uniform(-RADIUS, RADIUS), random.uniform(-RADIUS, RADIUS)) for _ in range(100)]\n    >>> common_points_in_circle(points1, points2)\n    \"\"\"\n    count_values_p1 = Counter(points1)\n    count_values_p2 = Counter(points2)\n\n    common_points = count_values_p1 & count_values_p2\n\n    distances = []\n    for point in common_points.keys():\n        distance = math.sqrt((point[0] - CENTER[0])**2 + (point[1] - CENTER[1])**2)\n        if distance <= RADIUS:\n            distances.append(distance)\n\n    plt.hist(distances, bins=10)\n    plt.xlabel('Distance to Center')\n    plt.title('Histogram of Distances of Common Points to Center')\n    plt.show()\n\n    return distances\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ncollections, itertools, matplotlib\n```python\nfrom collections import Counter\nimport itertools\nimport matplotlib.pyplot as plt\n\n# Constants\nPAIRS = [(i, j) for i in range(1, 7) for j in range(i, 7)]\n\ndef common_dice_rolls(rolls1, rolls2):\n    \"\"\"\n    Find common dice pairs between two sets of dice rolls, calculate their frequencies, and plot a bar chart.\n\n    Parameters:\n    rolls1 (list of tuples): Each tuple represents a dice roll from one set (e.g. (1, 3))\n    rolls2 (list of tuples): Each tuple represents a dice roll from another set (e.g. (1, 3))\n\n    Returns:\n    Counter: Frequencies of common dice pairs. \n\n    Requirements:\n    - collections\n    - itertools\n    - matplotlib.pyplot\n\n    Example:\n    >>> rolls1 = [PAIRS[random.randint(0, len(PAIRS)-1)] for _ in range(100)]\n    >>> rolls2 = [PAIRS[random.randint(0, len(PAIRS)-1)] for _ in range(100)]\n    >>> common_dice_rolls(rolls1, rolls2)\n    \"\"\"\n    count_values_r1 = Counter(rolls1)\n    count_values_r2 = Counter(rolls2)\n\n    common_rolls = count_values_r1 & count_values_r2\n\n    labels, values = zip(*common_rolls.items())\n    labels = [f'{pair[0]}-{pair[1]}' for pair in labels]\n\n    plt.bar(labels, values)\n    plt.xlabel('Dice Pairs')\n    plt.ylabel('Frequency')\n    plt.title('Common Dice Pairs')\n    plt.show()\n\n    return common_rolls\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ncollections, pandas, matplotlib\n```python\nfrom collections import Counter\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nLETTERS = 'abcdefghijklmnopqrstuvwxyz'\n\ndef common_letters_analysis(strings1, strings2):\n    \"\"\"\n    Find common letters between two lists of strings, calculate their frequencies, and plot a bar chart.\n\n    Parameters:\n    strings1 (list of str): A list of strings from one source.\n    strings2 (list of str): A list of strings from another source.\n\n    Returns:\n    DataFrame: A pandas DataFrame with frequencies of common letters. \n\n    Requirements:\n    - collections\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> strings1 = ['hello', 'world', 'python', 'programming']\n    >>> strings2 = ['hello', 'universe', 'python', 'coding']\n    >>> common_letters_analysis(strings1, strings2)\n    \"\"\"\n    letters1 = ''.join(strings1).lower()\n    letters2 = ''.join(strings2).lower()\n\n    counter1 = Counter(letters1)\n    counter2 = Counter(letters2)\n\n    common_letters = counter1 & counter2\n\n    df = pd.DataFrame.from_dict(common_letters, orient='index', columns=['Frequency'])\n    df = df[df.index.isin(list(LETTERS))]\n\n    df.sort_values('Frequency', inplace=True)\n    df.plot(kind='barh', legend=False, title='Frequencies of Common Letters')\n\n    plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\ncollections, random, os, json\n```python\nfrom collections import Counter\nimport random\nimport os\nimport json\n\n# Constants\nWORDS = ['apple', 'banana', 'cherry', 'date', 'elderberry', 'fig', 'grape']\n\ndef common_word_frequencies_in_files(file_path1, file_path2):\n    \"\"\"\n    Find common words between two text files, calculate their frequencies, and save the results into a JSON file.\n\n    Parameters:\n    file_path1 (str): The path of the first text file.\n    file_path2 (str): The path of the second text file.\n\n    Returns:\n    str: The path of the output JSON file. \n\n    Requirements:\n    - collections\n    - random\n    - os\n    - json\n\n    Example:\n    >>> common_word_frequencies_in_files('file1.txt', 'file2.txt')\n    \"\"\"\n    with open(file_path1, 'r') as f1, open(file_path2, 'r') as f2:\n        words1 = f1.read().split()\n        words2 = f2.read().split()\n\n    counter1 = Counter(words1)\n    counter2 = Counter(words2)\n\n    common_words = counter1 & counter2\n\n    output_file_path = os.path.join(os.path.dirname(file_path1), 'common_words.json')\n    with open(output_file_path, 'w') as f:\n        json.dump(common_words, f)\n\n    return output_file_path\n```"}
{"seed": "{\"task_id\": 37137, \"prompt\": \"def f_37137(my_list1, my_list2):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"list(set(my_list1) & set(my_list2))\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert set(candidate([1,2,3.5,'a'], [3.4, 'bb', 'a', 1])) == set([1, 'a']) \\n\", \"\\n    assert set(candidate([1,2,3], [4,5,6])) == set([]) \\n\", \"\\n    assert set(candidate(['ger','ferags',34.56], ['ger','ferags',34.56])) == set(['ger','ferags',34.56]) \\n\"], \"entry_point\": \"f_37137\", \"intent\": \"\\u041d\\u0430\\u0439\\u0442\\u0438 \\u043e\\u0431\\u0449\\u0438\\u0435 \\u044d\\u043b\\u0435\\u043c\\u0435\\u043d\\u0442\\u044b `common_items` \\u0441\\u043f\\u0438\\u0441\\u043a\\u0430 `my_list1` \\u0438 \\u0441\\u043f\\u0438\\u0441\\u043a\\u0430 `my_list2` \", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCATEGORIES = ['Electronics', 'Fashion', 'Sports', 'Books', 'Home Decor']\n\ndef analyze_sales_data(sales_df):\n    \"\"\"\n    Analyze the given sales data of different categories and return the categories \n    that are common between the two highest sales months.\n    \n    Parameters:\n    sales_df (DataFrame): The sales data in a pandas DataFrame. \n    The DataFrame has columns - 'Month', 'Category', 'Sales'.\n    \n    Returns:\n    list: The common categories between the two highest sales months.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> sales_data = {'Month': ['Jan', 'Jan', 'Feb', 'Feb', 'Mar', 'Mar'],\n                      'Category': ['Electronics', 'Fashion', 'Sports', 'Books', 'Home Decor', 'Electronics'],\n                      'Sales': [1000, 1500, 1200, 2000, 1800, 1600]}\n    >>> df = pd.DataFrame(sales_data)\n    >>> common_categories = analyze_sales_data(df)\n    >>> print(common_categories)\n    \"\"\"\n    sales_sum = sales_df.groupby(['Month', 'Category']).sum()\n    sales_sum = sales_sum.reset_index()\n    top_months = sales_sum.groupby('Month')['Sales'].sum().nlargest(2).index.tolist()\n    \n    top_month_categories = [set(sales_sum[sales_sum['Month'] == month]['Category'].tolist()) for month in top_months]\n    common_categories = list(top_month_categories[0] & top_month_categories[1])\n\n    sales_sum.pivot(index='Month', columns='Category', values='Sales').plot(kind='bar')\n    plt.show()\n\n    return common_categories\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, scipy.stats, matplotlib\n```python\nimport numpy as np\nfrom scipy.stats import ttest_ind\nimport matplotlib.pyplot as plt\n\ndef compare_sample_means(sample1, sample2):\n    \"\"\"\n    Perform a t-test to compare the means of two samples. \n    If the p-value is less than 0.05, return the common elements between the samples.\n\n    Parameters:\n    sample1 (array_like): First input sample.\n    sample2 (array_like): Second input sample.\n\n    Returns:\n    list: The common elements between the samples if p-value is less than 0.05.\n\n    Requirements:\n    - numpy\n    - scipy.stats.ttest_ind\n    - matplotlib.pyplot\n\n    Example:\n    >>> sample1 = np.random.normal(loc=0, scale=1, size=1000)\n    >>> sample2 = np.random.normal(loc=0.5, scale=1.5, size=1000)\n    >>> common_elements = compare_sample_means(sample1, sample2)\n    >>> print(common_elements)\n    \"\"\"\n    t_stat, p_val = ttest_ind(sample1, sample2)\n\n    if p_val < 0.05:\n        common_elements = list(set(sample1) & set(sample2))\n        plt.hist([sample1, sample2], label=['Sample1', 'Sample2'])\n        plt.legend(loc='upper right')\n        plt.show()\n        return common_elements\n    else:\n        return []\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ncollections, itertools, matplotlib\n```python\nfrom collections import Counter\nfrom itertools import chain\nimport matplotlib.pyplot as plt\n\ndef common_elements_in_lists(lists):\n    \"\"\"\n    Find the common elements in multiple lists and plot their frequency.\n\n    Parameters:\n    lists (list of list): The input lists.\n\n    Returns:\n    list: The common elements in the lists.\n\n    Requirements:\n    - collections.Counter\n    - itertools.chain\n    - matplotlib.pyplot\n\n    Example:\n    >>> lists = [[1, 2, 3, 4, 5], [4, 5, 6, 7, 8], [5, 6, 7, 8, 9]]\n    >>> common_elements = common_elements_in_lists(lists)\n    >>> print(common_elements)\n    \"\"\"\n    flatten_list = list(chain(*lists))\n    counter = Counter(flatten_list)\n    common_elements = [item for item, count in counter.items() if count == len(lists)]\n\n    plt.bar(counter.keys(), counter.values())\n    plt.show()\n\n    return common_elements\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, pandas, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef common_elements_in_arrays(arrays):\n    \"\"\"\n    Find the common elements in multiple numpy arrays and plot their frequency.\n\n    Parameters:\n    arrays (list of ndarray): The input numpy arrays.\n\n    Returns:\n    ndarray: The common elements in the arrays.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> arrays = [np.array([1, 2, 3, 4, 5]), np.array([4, 5, 6, 7, 8]), np.array([5, 6, 7, 8, 9])]\n    >>> common_elements = common_elements_in_arrays(arrays)\n    >>> print(common_elements)\n    \"\"\"\n    flatten_array = np.concatenate(arrays)\n    series = pd.Series(flatten_array)\n    common_elements = series[series.duplicated(keep=False)].unique()\n\n    series.value_counts().plot(kind='bar')\n    plt.show()\n\n    return common_elements\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef common_customers_in_years(sales_df):\n    \"\"\"\n    Find the common customers who made purchases in two consecutive years and \n    plot the number of purchases made by these customers each year.\n\n    Parameters:\n    sales_df (DataFrame): The sales data in a pandas DataFrame. \n    The DataFrame has columns - 'Year', 'CustomerID', 'Purchases'.\n\n    Returns:\n    DataFrame: The purchases made by the common customers each year.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> sales_data = {'Year': [2020, 2020, 2021, 2021, 2022, 2022],\n                      'CustomerID': [1, 2, 2, 3, 3, 4],\n                      'Purchases': [5, 10, 8, 12, 15, 20]}\n    >>> df = pd.DataFrame(sales_data)\n    >>> common_customers = common_customers_in_years(df)\n    >>> print(common_customers)\n    \"\"\"\n    years = sales_df['Year'].unique()\n    common_customers = None\n\n    for i in range(len(years)-1):\n        customers_year1 = set(sales_df[sales_df['Year'] == years[i]]['CustomerID'].unique())\n        customers_year2 = set(sales_df[sales_df['Year'] == years[i+1]]['CustomerID'].unique())\n        common = customers_year1 & customers_year2\n\n        if common_customers is None:\n            common_customers = common\n        else:\n            common_customers &= common\n\n    common_customers_df = sales_df[sales_df['CustomerID'].isin(common_customers)]\n    common_customers_df.groupby(['Year', 'CustomerID'])['Purchases'].sum().unstack().plot(kind='bar', stacked=True)\n    plt.show()\n\n    return common_customers_df\n```"}
{"seed": "{\"task_id\": 645227, \"prompt\": \"def f_645227(n):\\n\\t\", \"suffix\": \"\\n\\treturn factors\", \"canonical_solution\": \"\\n\\tfactors = []\\n\\twhile n % 2 == 0:\\n\\t\\tfactors.append(2)\\n\\t\\tn = n // 2\\n\\tfor i in range(3, int(math.sqrt(n)) + 1, 2):\\n\\t\\twhile (n % i) == 0:\\n\\t\\t\\tfactors.append(i)\\n\\t\\t\\tn = n // i\\n\\tif n > 2: \\n \\t\\tfactors.append(n)\\n\", \"test_start\": \"\\nimport math\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(17) == [17]\\n\", \"\\n    assert sorted(candidate(10)) == [2, 5]\\n\", \"\\n    assert sorted(candidate(100)) == [2, 2, 5, 5]\\n\"], \"entry_point\": \"f_645227\", \"intent\": \"\\u0420\\u0430\\u0437\\u043b\\u043e\\u0436\\u0438\\u0442\\u044c \\u0447\\u0438\\u0441\\u043b\\u043e `n` \\u043d\\u0430 \\u043f\\u0440\\u043e\\u0441\\u0442\\u044b\\u0435 \\u043c\\u043d\\u043e\\u0436\\u0438\\u0442\\u0435\\u043b\\u0438 `factors`\", \"library\": [\"math\"]}", "generation": "Scenario 1: \nnumpy, pandas, matplotlib, math\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport math\n\n# Constants\nNUMBER_RANGE = 1000\n\ndef analyze_factors_distribution():\n    \"\"\"\n    Generate a bar chart that shows the distribution of the number of distinct prime factors \n    that numbers within a certain range have.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the distribution of prime factors.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - math\n\n    Example:\n    >>> analyze_factors_distribution()\n    \"\"\"\n    factor_counts = []\n\n    for n in range(1, NUMBER_RANGE + 1):\n        factors = []\n        while n % 2 == 0:\n            if 2 not in factors:\n                factors.append(2)\n            n = n // 2\n        for i in range(3, int(math.sqrt(n)) + 1, 2):\n            while (n % i) == 0:\n                if i not in factors:\n                    factors.append(i)\n                n = n // i\n        if n > 2 and n not in factors:\n            factors.append(n)\n        factor_counts.append(len(factors))\n\n    distribution_df = pd.DataFrame(factor_counts, columns=['Number of Distinct Prime Factors'])\n    distribution_df['Number of Distinct Prime Factors'].value_counts().sort_index().plot(kind='bar')\n\n    return distribution_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nnumpy, pandas, matplotlib, math\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport math\n\n# Constants\nNUMBER_RANGE = 1000\n\ndef analyze_factors_sum():\n    \"\"\"\n    Generate a histogram that shows the distribution of the sum of distinct prime factors \n    that numbers within a certain range have.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the sum of prime factors.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - math\n\n    Example:\n    >>> analyze_factors_sum()\n    \"\"\"\n    factor_sums = []\n\n    for n in range(1, NUMBER_RANGE + 1):\n        factors = []\n        while n % 2 == 0:\n            if 2 not in factors:\n                factors.append(2)\n            n = n // 2\n        for i in range(3, int(math.sqrt(n)) + 1, 2):\n            while (n % i) == 0:\n                if i not in factors:\n                    factors.append(i)\n                n = n // i\n        if n > 2 and n not in factors:\n            factors.append(n)\n        factor_sums.append(sum(factors))\n\n    sum_df = pd.DataFrame(factor_sums, columns=['Sum of Distinct Prime Factors'])\n    sum_df['Sum of Distinct Prime Factors'].plot(kind='hist', bins=30)\n\n    return sum_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnumpy, pandas, matplotlib, math\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport math\n\n# Constants\nNUMBER_RANGE = 1000\n\ndef analyze_factors_max():\n    \"\"\"\n    Generate a pie chart that shows the distribution of the maximum distinct prime factor \n    that numbers within a certain range have.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the maximum prime factor.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - math\n\n    Example:\n    >>> analyze_factors_max()\n    \"\"\"\n    factor_maxes = []\n\n    for n in range(1, NUMBER_RANGE + 1):\n        factors = []\n        while n % 2 == 0:\n            if 2 not in factors:\n                factors.append(2)\n            n = n // 2\n        for i in range(3, int(math.sqrt(n)) + 1, 2):\n            while (n % i) == 0:\n                if i not in factors:\n                    factors.append(i)\n                n = n // i\n        if n > 2 and n not in factors:\n            factors.append(n)\n        factor_maxes.append(max(factors))\n\n    max_df = pd.DataFrame(factor_maxes, columns=['Maximum Distinct Prime Factor'])\n    max_df['Maximum Distinct Prime Factor'].value_counts().plot(kind='pie')\n\n    return max_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, pandas, seaborn, math\n```python\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport math\n\n# Constants\nNUMBER_RANGE = 1000\n\ndef analyze_factors_mean():\n    \"\"\"\n    Generate a box plot that shows the distribution of the mean of distinct prime factors \n    that numbers within a certain range have.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the mean of prime factors.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - seaborn\n    - math\n\n    Example:\n    >>> analyze_factors_mean()\n    \"\"\"\n    factor_means = []\n\n    for n in range(1, NUMBER_RANGE + 1):\n        factors = []\n        while n % 2 == 0:\n            if 2 not in factors:\n                factors.append(2)\n            n = n // 2\n        for i in range(3, int(math.sqrt(n)) + 1, 2):\n            while (n % i) == 0:\n                if i not in factors:\n                    factors.append(i)\n                n = n // i\n        if n > 2 and n not in factors:\n            factors.append(n)\n        factor_means.append(np.mean(factors))\n\n    mean_df = pd.DataFrame(factor_means, columns=['Mean of Distinct Prime Factors'])\n    sns.boxplot(x=mean_df['Mean of Distinct Prime Factors'])\n\n    return mean_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnumpy, pandas, seaborn, math\n```python\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport math\n\n# Constants\nNUMBER_RANGE = 1000\n\ndef analyze_factors_median():\n    \"\"\"\n    Generate a violin plot that shows the distribution of the median of distinct prime factors \n    that numbers within a certain range have.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the median of prime factors.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - seaborn\n    - math\n\n    Example:\n    >>> analyze_factors_median()\n    \"\"\"\n    factor_medians = []\n\n    for n in range(1, NUMBER_RANGE + 1):\n        factors = []\n        while n % 2 == 0:\n            if 2 not in factors:\n                factors.append(2)\n            n = n // 2\n        for i in range(3, int(math.sqrt(n)) + 1, 2):\n            while (n % i) == 0:\n                if i not in factors:\n                    factors.append(i)\n                n = n // i\n        if n > 2 and n not in factors:\n            factors.append(n)\n        factor_medians.append(np.median(factors))\n\n    median_df = pd.DataFrame(factor_medians, columns=['Median of Distinct Prime Factors'])\n    sns.violinplot(x=median_df['Median of Distinct Prime Factors'])\n\n    return median_df\n```"}
{"seed": "{\"task_id\": 565846, \"prompt\": \"def f_565846(val1, val2, N):\\n\\t\", \"suffix\": \"\\n\\treturn my_list\", \"canonical_solution\": \"my_list = [random.randint(val1, val2) for i in range(N)]\", \"test_start\": \"\\nimport random\\n\\ndef check(candidate):\", \"test\": [\"\\n    result = candidate(0, 18, 10)\\n    assert len(result) == 10\\n    assert min(result) >= 0\\n    assert max(result) <= 18\\n    assert all([isinstance(a, int) for a in result])\\n\"], \"entry_point\": \"f_565846\", \"intent\": \"\\u0421\\u0433\\u0435\\u043d\\u0435\\u0440\\u0438\\u0440\\u043e\\u0432\\u0430\\u0442\\u044c \\u0441\\u043f\\u0438\\u0441\\u043a\\u043e\\u043a `my_list` \\u0434\\u043b\\u0438\\u043d\\u043d\\u043e\\u0439 `N` \\u0441\\u043b\\u0443\\u0447\\u0430\\u0439\\u043d\\u044b\\u0445 \\u0437\\u043d\\u0430\\u0447\\u0435\\u043d\\u0438\\u0439 \\u0432 \\u0434\\u0438\\u0430\\u043f\\u0430\\u0437\\u043e\\u043d\\u0435 \\u043e\\u0442 `val1` \\u0434\\u043e `val2`\", \"library\": [\"random\"]}", "generation": "Scenario 1: \npandas, numpy, matplotlib, random\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nCITIES = ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Philadelphia']\nPOPULATION = [8336817, 3979576, 2693976, 2320268, 1584064]\n\ndef generate_city_population_report(val1, val2, N):\n    \"\"\"\n    Generate a report of population change for a list of cities over N years. \n    The population change is a random number between val1 and val2.\n    \n    Parameters:\n    val1 (int): The minimum change in population.\n    val2 (int): The maximum change in population.\n    N (int): The number of years.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with population change for the cities over N years.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - random.randint\n    \n    Example:\n    >>> report = generate_city_population_report(-1000, 1000, 10)\n    >>> print(report)\n    >>> report.plot(kind='line')\n    \"\"\"\n    report_data = pd.DataFrame(index=np.arange(1, N+1), columns=CITIES)\n\n    for city, pop in zip(CITIES, POPULATION):\n        population_change = [pop + randint(val1, val2) for i in range(N)]\n        report_data[city] = population_change\n\n    return report_data\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nnumpy, random, matplotlib, math\n```python\nimport numpy as np\nfrom random import uniform\nimport matplotlib.pyplot as plt\nimport math\n\ndef generate_random_walk(val1, val2, N):\n    \"\"\"\n    Generate a random walk in 2D space starting from the origin. \n    The step size is a random number between val1 and val2. \n    The direction is a random angle between 0 and 2*pi.\n    \n    Parameters:\n    val1 (float): The minimum step size.\n    val2 (float): The maximum step size.\n    N (int): The number of steps.\n    \n    Returns:\n    list: A list of tuples representing the (x, y) coordinates of the walk.\n    \n    Requirements:\n    - numpy\n    - random.uniform\n    - matplotlib.pyplot\n    - math\n    \n    Example:\n    >>> walk = generate_random_walk(0.0, 1.0, 100)\n    >>> plt.plot([x for x, y in walk], [y for x, y in walk])\n    \"\"\"\n    walk = [(0, 0)]\n    \n    for i in range(N):\n        angle = uniform(0, 2*math.pi)\n        step = uniform(val1, val2)\n        x = walk[-1][0] + step * math.cos(angle)\n        y = walk[-1][1] + step * math.sin(angle)\n        walk.append((x, y))\n    \n    return walk\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnumpy, random, scipy.stats\n```python\nimport numpy as np\nfrom random import randint\nfrom scipy import stats\n\ndef generate_random_sample(val1, val2, N):\n    \"\"\"\n    Generate a random sample of size N from a discrete uniform distribution \n    between val1 and val2. Calculate and return the mean and variance of the sample.\n    \n    Parameters:\n    val1 (int): The minimum value of the distribution.\n    val2 (int): The maximum value of the distribution.\n    N (int): The size of the sample.\n    \n    Returns:\n    tuple: A tuple (mean, variance) of the sample.\n    \n    Requirements:\n    - numpy\n    - random.randint\n    - scipy.stats\n    \n    Example:\n    >>> mean, var = generate_random_sample(0, 10, 100)\n    >>> print('Mean:', mean, 'Variance:', var)\n    \"\"\"\n    sample = np.array([randint(val1, val2) for i in range(N)])\n    mean = np.mean(sample)\n    variance = np.var(sample)\n    \n    return mean, variance\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, random, matplotlib\n```python\nimport numpy as np\nfrom random import randint\nimport matplotlib.pyplot as plt\n\ndef generate_random_matrix(val1, val2, N):\n    \"\"\"\n    Generate a random NxN matrix with elements between val1 and val2. \n    Plot the matrix as a heatmap.\n    \n    Parameters:\n    val1 (int): The minimum value of the elements.\n    val2 (int): The maximum value of the elements.\n    N (int): The size of the matrix.\n    \n    Returns:\n    ndarray: The generated matrix.\n    \n    Requirements:\n    - numpy\n    - random.randint\n    - matplotlib.pyplot\n    \n    Example:\n    >>> matrix = generate_random_matrix(0, 10, 5)\n    >>> plt.imshow(matrix, cmap='hot', interpolation='nearest')\n    \"\"\"\n    matrix = np.array([[randint(val1, val2) for j in range(N)] for i in range(N)])\n    \n    return matrix\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnumpy, random, matplotlib, scipy.optimize\n```python\nimport numpy as np\nfrom random import uniform\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import curve_fit\n\ndef generate_noisy_data_and_fit(val1, val2, N):\n    \"\"\"\n    Generate a noisy dataset of size N from a sine wave with random noise \n    between val1 and val2. Fit a sine wave to the noisy data and plot both.\n    \n    Parameters:\n    val1 (float): The minimum value of the noise.\n    val2 (float): The maximum value of the noise.\n    N (int): The size of the dataset.\n    \n    Returns:\n    tuple: A tuple (popt, pcov) of the optimized parameters and covariance.\n    \n    Requirements:\n    - numpy\n    - random.uniform\n    - matplotlib.pyplot\n    - scipy.optimize.curve_fit\n    \n    Example:\n    >>> popt, pcov = generate_noisy_data_and_fit(-0.1, 0.1, 100)\n    >>> print('Optimized parameters:', popt)\n    >>> print('Covariance:', pcov)\n    \"\"\"\n    x = np.linspace(0, 4*np.pi, N)\n    y = np.sin(x) + np.array([uniform(val1, val2) for i in range(N)])\n    \n    def func(x, a, b, c):\n        return a * np.sin(b * x + c)\n    \n    popt, pcov = curve_fit(func, x, y)\n    \n    plt.plot(x, y, 'b-', label='data')\n    plt.plot(x, func(x, *popt), 'r-', label='fit')\n    plt.legend()\n    \n    return popt, pcov\n```"}
{"seed": "{\"task_id\": 925879, \"prompt\": \"def f_925879(my_list, i, j):\\n\\t\", \"suffix\": \"\\n\\treturn my_list\", \"canonical_solution\": \"my_list[i], my_list[j] = my_list[j], my_list[i]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([1,2,3,4,5], 2, 4) == [1,2,5,4,3]\\n\", \"\\n    assert candidate([1,2,3,4,5], 2, 0) == [3,2,1,4,5]\\n\", \"\\n    assert candidate([1,2,3,4,5], 0, 0) == [1,2,3,4,5]\\n\"], \"entry_point\": \"f_925879\", \"intent\": \"\\u041f\\u043e\\u043c\\u0435\\u043d\\u044f\\u0442\\u044c \\u043c\\u0435\\u0441\\u0442\\u0430\\u043c\\u0438 \\u044d\\u043b\\u0435\\u043c\\u0435\\u043d\\u0442\\u044b \\u0441\\u043f\\u0438\\u0441\\u043a\\u0430 `my_list` \\u0441 \\u0438\\u043d\\u0434\\u0435\\u043a\\u0441\\u0430\\u043c\\u0438 `i` \\u0438 `j`\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, matplotlib.pyplot, itertools, random\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport itertools\nimport random\n\n# Constants\nDATA_SIZE = 100\nFIELDS = ['A', 'B', 'C', 'D', 'E']\n\ndef generate_and_plot_data(i, j):\n    \"\"\"\n    Generate a DataFrame of random numbers, swap the elements of columns 'i' and 'j', \n    and then plot the histograms of all columns.\n\n    Parameters:\n    i (str): The name of the first column.\n    j (str): The name of the second column.\n\n    Returns:\n    DataFrame: A pandas DataFrame with swapped columns.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - itertools\n    - random\n\n    Example:\n    >>> df = generate_and_plot_data('A', 'C')\n    \"\"\"\n    # Generate DataFrame\n    data = {field: np.random.randn(DATA_SIZE) for field in FIELDS}\n    df = pd.DataFrame(data)\n\n    # Swap columns\n    df[i], df[j] = df[j].copy(), df[i].copy()\n\n    # Plot histograms\n    df.hist(bins=30, figsize=(10,10), grid=False)\n    plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, itertools, random, string\n```python\nimport numpy as np\nimport itertools\nimport random\nimport string\n\n# Constants\nN = 1000\n\ndef swap_elements_in_array(i, j):\n    \"\"\"\n    Generate a numpy array of random letters, swap the elements at indices 'i' and 'j', \n    and then return the swapped array.\n\n    Parameters:\n    i (int): The index of the first element.\n    j (int): The index of the second element.\n\n    Returns:\n    ndarray: A numpy array with swapped elements.\n\n    Requirements:\n    - numpy\n    - itertools\n    - random\n    - string\n\n    Example:\n    >>> arr = swap_elements_in_array(5, 10)\n    \"\"\"\n    # Generate array\n    arr = np.array(list(itertools.islice(itertools.cycle(string.ascii_lowercase), N)))\n\n    # Swap elements\n    arr[i], arr[j] = arr[j], arr[i]\n\n    return arr\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, matplotlib.pyplot, seaborn, random\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef swap_and_plot_data(i, j):\n    \"\"\"\n    Generate a DataFrame of random numbers, swap the elements of columns 'i' and 'j', \n    and then plot the correlation matrix of all columns.\n\n    Parameters:\n    i (str): The name of the first column.\n    j (str): The name of the second column.\n\n    Returns:\n    DataFrame: A pandas DataFrame with swapped columns.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - seaborn\n    - random\n\n    Example:\n    >>> df = swap_and_plot_data('A', 'C')\n    \"\"\"\n    # Generate DataFrame\n    data = {col: np.random.randn(1000) for col in COLUMNS}\n    df = pd.DataFrame(data)\n\n    # Swap columns\n    df[i], df[j] = df[j].copy(), df[i].copy()\n\n    # Plot correlation matrix\n    corr = df.corr()\n    sns.heatmap(corr, annot=True)\n    plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, itertools, random, matplotlib.pyplot\n```python\nimport numpy as np\nimport itertools\nimport random\nimport matplotlib.pyplot as plt\n\n# Constants\nN = 1000\nSTART = 0\nSTOP = 100\n\ndef swap_and_plot(i, j):\n    \"\"\"\n    Generate a numpy array of random numbers, swap the elements at indices 'i' and 'j', \n    and then plot the array.\n\n    Parameters:\n    i (int): The index of the first element.\n    j (int): The index of the second element.\n\n    Returns:\n    ndarray: A numpy array with swapped elements.\n\n    Requirements:\n    - numpy\n    - itertools\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> arr = swap_and_plot(5, 10)\n    \"\"\"\n    # Generate array\n    arr = np.random.randint(START, STOP, N)\n\n    # Swap elements\n    arr[i], arr[j] = arr[j], arr[i]\n\n    # Plot array\n    plt.plot(arr)\n    plt.show()\n\n    return arr\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, matplotlib.pyplot, seaborn, random\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef swap_and_visualize_data(i, j):\n    \"\"\"\n    Generate a DataFrame of random numbers, swap the elements of columns 'i' and 'j', \n    and then visualize the data using various plot types.\n\n    Parameters:\n    i (str): The name of the first column.\n    j (str): The name of the second column.\n\n    Returns:\n    DataFrame: A pandas DataFrame with swapped columns.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - seaborn\n    - random\n\n    Example:\n    >>> df = swap_and_visualize_data('A', 'C')\n    \"\"\"\n    # Generate DataFrame\n    data = {col: np.random.randn(1000) for col in COLUMNS}\n    df = pd.DataFrame(data)\n\n    # Swap columns\n    df[i], df[j] = df[j].copy(), df[i].copy()\n\n    # Plot data\n    plt.figure(figsize=(12,6))\n\n    plt.subplot(121)\n    sns.boxplot(data=df)\n\n    plt.subplot(122)\n    sns.violinplot(data=df)\n\n    plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`"}
{"seed": "{\"task_id\": 649828, \"prompt\": \"def f_649828():\\n\\t\", \"suffix\": \"\\n\\treturn my_list\", \"canonical_solution\": \"my_list = input().split()\", \"test_start\": \"\\nimport builtins\\nfrom unittest.mock import Mock\\n\\ndef check(candidate):\", \"test\": [\"\\n    builtins.input = Mock(return_value = '6 6')\\n    assert candidate() == ['6', '6']\\n\", \"\\n    builtins.input = Mock(return_value = 'hello world !')\\n    assert candidate() == [\\\"hello\\\", \\\"world\\\", \\\"!\\\"]\\n\"], \"entry_point\": \"f_649828\", \"intent\": \"\\u0412\\u0432\\u0435\\u0441\\u0442\\u0438 \\u0441 \\u043a\\u043b\\u0430\\u0432\\u0438\\u0430\\u0442\\u0443\\u0440\\u044b \\u0447\\u0435\\u0440\\u0435\\u0437 \\u043f\\u0440\\u043e\\u0431\\u0435\\u043b \\u0441\\u043f\\u0438\\u0441\\u043e\\u043a \\u0441\\u0442\\u0440\\u043e\\u043a `my_list`\", \"library\": [\"builtins\"]}", "generation": "Scenario 1: \npandas, os, datetime, sys\n```python\nimport pandas as pd\nimport os\nfrom datetime import datetime\nimport sys\n\n# Constants\nCSV_DIR = './csv_files'\n\ndef create_csv_files(date_str):\n    \"\"\"\n    Create CSV files for a given date in a specified directory. The CSV files will contain \n    randomly generated data for that date.\n    \n    Parameters:\n    date_str (str): The date string in \"yyyy-mm-dd\" format.\n    \n    Returns:\n    list: A list of file paths of the created CSV files.\n    \n    Requirements:\n    - pandas\n    - os\n    - datetime\n    - sys\n    \n    Example:\n    >>> file_paths = create_csv_files('2023-06-15')\n    >>> print(file_paths)\n    \"\"\"\n    if not os.path.exists(CSV_DIR):\n        os.makedirs(CSV_DIR)\n\n    date = datetime.strptime(date_str, '%Y-%m-%d')\n    file_paths = []\n\n    for i in range(5):\n        data = pd.DataFrame({\n            'index': range(100),\n            'value': pd.np.random.rand(100)\n        })\n        file_path = os.path.join(CSV_DIR, f'{date.strftime(\"%Y%m%d\")}_{i}.csv')\n        data.to_csv(file_path, index=False)\n        file_paths.append(file_path)\n\n    return file_paths\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nos, shutil, random, sys\n```python\nimport os\nimport shutil\nimport random\nimport sys\n\n# Constants\nSRC_DIR = './src_dir'\nDST_DIR = './dst_dir'\n\ndef random_file_move():\n    \"\"\"\n    Move a random file from a source directory to a destination directory.\n    \n    Returns:\n    str: The path of the moved file.\n    \n    Requirements:\n    - os\n    - shutil\n    - random\n    - sys\n    \n    Example:\n    >>> moved_file_path = random_file_move()\n    >>> print(moved_file_path)\n    \"\"\"\n    if not os.path.exists(SRC_DIR) or not os.path.exists(DST_DIR):\n        print(f\"Either '{SRC_DIR}' or '{DST_DIR}' does not exist.\", file=sys.stderr)\n        return\n\n    files = os.listdir(SRC_DIR)\n    if not files:\n        print(f\"No files in '{SRC_DIR}'.\", file=sys.stderr)\n        return\n\n    random_file = random.choice(files)\n    src_file_path = os.path.join(SRC_DIR, random_file)\n    dst_file_path = os.path.join(DST_DIR, random_file)\n\n    shutil.move(src_file_path, dst_file_path)\n\n    return dst_file_path\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \npandas, datetime, os, random\n```python\nimport pandas as pd\nfrom datetime import datetime, timedelta\nimport os\nfrom random import choices\n\n# Constants\nDATA_DIR = './data'\nCATEGORIES = ['A', 'B', 'C', 'D', 'E']\n\ndef generate_category_data(start_date_str, end_date_str):\n    \"\"\"\n    Generate a CSV file with random category data for each date between a start date and an end date.\n\n    Parameters:\n    start_date_str (str): The start date string in \"yyyy-mm-dd\" format.\n    end_date_str (str): The end date string in \"yyyy-mm-dd\" format.\n\n    Returns:\n    str: The path of the generated CSV file.\n\n    Requirements:\n    - pandas\n    - datetime\n    - os\n    - random\n\n    Example:\n    >>> csv_file_path = generate_category_data('2023-06-01', '2023-06-30')\n    >>> print(csv_file_path)\n    \"\"\"\n    start_date = datetime.strptime(start_date_str, '%Y-%m-%d')\n    end_date = datetime.strptime(end_date_str, '%Y-%m-%d')\n\n    date_range = pd.date_range(start_date, end_date)\n    categories = choices(CATEGORIES, k=len(date_range))\n\n    data = pd.DataFrame({\n        'Date': date_range,\n        'Category': categories\n    })\n\n    if not os.path.exists(DATA_DIR):\n        os.makedirs(DATA_DIR)\n    csv_file_path = os.path.join(DATA_DIR, 'category_data.csv')\n    data.to_csv(csv_file_path, index=False)\n\n    return csv_file_path\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \ncsv, random, os, sys\n```python\nimport csv\nimport random\nimport os\nimport sys\n\n# Constants\nNUM_ROWS = 100\nRANGE_START = 1\nRANGE_END = 100\nCSV_FILE = './data.csv'\n\ndef generate_csv():\n    \"\"\"\n    Generate a CSV file with two columns: 'index' and 'value'. The 'index' column contains \n    numbers from 0 to NUM_ROWS-1, and the 'value' column contains random numbers from RANGE_START \n    to RANGE_END.\n\n    Returns:\n    str: The path of the generated CSV file.\n\n    Requirements:\n    - csv\n    - random\n    - os\n    - sys\n\n    Example:\n    >>> csv_file_path = generate_csv()\n    >>> print(csv_file_path)\n    \"\"\"\n    with open(CSV_FILE, 'w', newline='') as csvfile:\n        fieldnames = ['index', 'value']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n\n        writer.writeheader()\n        for i in range(NUM_ROWS):\n            writer.writerow({'index': i, 'value': random.randint(RANGE_START, RANGE_END)})\n\n    return CSV_FILE\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nos, shutil, random, sys\n```python\nimport os\nimport shutil\nimport random\nimport sys\n\n# Constants\nSRC_DIR = './src'\nDST_DIR = './dst'\n\ndef random_file_copy():\n    \"\"\"\n    Copy a random file from a source directory to a destination directory.\n    \n    Returns:\n    str: The path of the copied file in the destination directory.\n\n    Requirements:\n    - os\n    - shutil\n    - random\n    - sys\n\n    Example:\n    >>> copied_file_path = random_file_copy()\n    >>> print(copied_file_path)\n    \"\"\"\n    if not os.path.exists(SRC_DIR) or not os.path.exists(DST_DIR):\n        print(f\"Either '{SRC_DIR}' or '{DST_DIR}' does not exist.\", file=sys.stderr)\n        return\n\n    files = os.listdir(SRC_DIR)\n    if not files:\n        print(f\"No files in '{SRC_DIR}'.\", file=sys.stderr)\n        return\n\n    random_file = random.choice(files)\n    src_file_path = os.path.join(SRC_DIR, random_file)\n    dst_file_path = os.path.join(DST_DIR, random_file)\n\n    shutil.copy(src_file_path, dst_file_path)\n\n    return dst_file_path\n```"}
{"seed": "{\"task_id\": 752179, \"prompt\": \"def f_752179(my_dict, my_key):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"my_key in my_dict\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate({'k': 123}, 'k') == True\\n\", \"\\n    assert candidate({'k': 123}, 'aa') == False\\n\"], \"entry_point\": \"f_752179\", \"intent\": \"\\u041f\\u0440\\u043e\\u0432\\u0435\\u0440\\u0438\\u0442\\u044c \\u043d\\u0430\\u043b\\u0438\\u0447\\u0438\\u0435 \\u043a\\u043b\\u044e\\u0447\\u0430 `my_key` \\u0432 \\u0441\\u043b\\u043e\\u0432\\u0430\\u0440\\u0435 `my_dict`\", \"library\": []}", "generation": "Scenario 1:\npandas, collections, matplotlib.pyplot\n```python\nimport pandas as pd\nimport collections\nimport matplotlib.pyplot as plt\n\ndef find_top_values(df, col_name, top_n):\n    \"\"\"\n    Find the top N most common values in a specific column of a pandas DataFrame and plot \n    the frequency of these values using a bar graph.\n    \n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n    col_name (str): The column name.\n    top_n (int): The number of top common values to find.\n    \n    Returns:\n    list: The list of tuples with the top N values and their counts.\n    \n    Requirements:\n    - pandas\n    - collections\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = pd.DataFrame({'A': [1, 2, 2, 3, 3, 3]})\n    >>> find_top_values(df, 'A', 2)\n    [(3, 3), (2, 2)]\n    \"\"\"\n    counter = collections.Counter(df[col_name])\n    top_values = counter.most_common(top_n)\n    \n    plt.bar([x[0] for x in top_values], [x[1] for x in top_values])\n    plt.show()\n    \n    return top_values\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, scipy.stats, matplotlib.pyplot\n```python\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef perform_ttest_and_plot(data1, data2, alpha):\n    \"\"\"\n    Perform a two-sample t-test on two data samples and plot a histogram of these \n    samples. If the p-value is less than the significance level (alpha), \n    the function returns True indicating that the means of the two samples are different.\n    \n    Parameters:\n    data1, data2 (array-like): The two data samples.\n    alpha (float): The significance level.\n    \n    Returns:\n    bool: True if the means are different, False otherwise.\n    \n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n    \n    Example:\n    >>> perform_ttest_and_plot(np.random.rand(100), np.random.rand(100), 0.05)\n    \"\"\"\n    t_stat, p_value = stats.ttest_ind(data1, data2)\n    different_means = p_value < alpha\n    \n    plt.hist([data1, data2], label=['data1', 'data2'])\n    plt.legend(loc='upper right')\n    plt.show()\n    \n    return different_means\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, scipy.optimize, matplotlib.pyplot\n```python\nimport numpy as np\nfrom scipy import optimize\nimport matplotlib.pyplot as plt\n\ndef fit_and_plot(data_x, data_y, func):\n    \"\"\"\n    Fit a function to the given data using the least squares method and plot the original \n    data and the fitted function.\n    \n    Parameters:\n    data_x, data_y (array-like): The x and y data.\n    func (function): The function to fit.\n    \n    Returns:\n    array: The optimized parameters.\n    \n    Requirements:\n    - numpy\n    - scipy.optimize\n    - matplotlib.pyplot\n    \n    Example:\n    >>> fit_and_plot(np.arange(0, 10, 0.1), np.sin(np.arange(0, 10, 0.1)), lambda x, a, b: a * np.sin(b * x))\n    \"\"\"\n    params, _ = optimize.curve_fit(func, data_x, data_y)\n    \n    plt.scatter(data_x, data_y, label='Data')\n    plt.plot(data_x, func(data_x, *params), 'r-', label='Fit: a=%5.3f, b=%5.3f' % tuple(params))\n    plt.legend()\n    plt.show()\n    \n    return params\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, sklearn.preprocessing, matplotlib.pyplot\n```python\nimport pandas as pd\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\n\ndef normalize_and_plot(df, col_name):\n    \"\"\"\n    Normalize a specific column in a pandas DataFrame using MinMaxScaler and plot a \n    histogram of the original and normalized data.\n    \n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n    col_name (str): The column name.\n    \n    Returns:\n    Series: The normalized data.\n    \n    Requirements:\n    - pandas\n    - sklearn.preprocessing\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = pd.DataFrame({'A': [1, 2, 3, 4, 5]})\n    >>> normalize_and_plot(df, 'A')\n    \"\"\"\n    data = df[col_name].values.reshape(-1, 1)\n    scaler = preprocessing.MinMaxScaler()\n    normalized_data = scaler.fit_transform(data)\n    \n    plt.hist(df[col_name], alpha=0.5, label='Original')\n    plt.hist(normalized_data, alpha=0.5, label='Normalized')\n    plt.legend(loc='upper right')\n    plt.show()\n    \n    return pd.Series(normalized_data.flatten())\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, sklearn.decomposition, matplotlib.pyplot\n```python\nimport numpy as np\nfrom sklearn import decomposition\nimport matplotlib.pyplot as plt\n\ndef perform_pca_and_plot(data, n_components):\n    \"\"\"\n    Perform Principal Component Analysis (PCA) on the given data and plot the \n    transformed data in a scatter plot.\n    \n    Parameters:\n    data (array-like): The data to perform PCA on.\n    n_components (int): The number of principal components to keep.\n    \n    Returns:\n    array: The transformed data.\n    \n    Requirements:\n    - numpy\n    - sklearn.decomposition\n    - matplotlib.pyplot\n    \n    Example:\n    >>> perform_pca_and_plot(np.random.rand(100, 5), 2)\n    \"\"\"\n    pca = decomposition.PCA(n_components)\n    transformed_data = pca.fit_transform(data)\n    \n    plt.scatter(transformed_data[:, 0], transformed_data[:, 1])\n    plt.show()\n    \n    return transformed_data\n```"}
{"seed": "{\"task_id\": 540082, \"prompt\": \"def f_540082(file_path):\\n\\t\", \"suffix\": \"\\n\\treturn file_extension\", \"canonical_solution\": \"filename, file_extension = os.path.splitext(file_path)\", \"test_start\": \"\\nimport os\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('file_name.txt') == '.txt'\\n\", \"\\n    assert candidate('file_name') == ''\\n\", \"\\n    assert candidate('test.json') == '.json'\\n\"], \"entry_point\": \"f_540082\", \"intent\": \"\\u041f\\u043e\\u043b\\u0443\\u0447\\u0438\\u0442\\u044c \\u0440\\u0430\\u0441\\u0448\\u0438\\u0440\\u0435\\u043d\\u0438\\u0435 `file_extension` \\u0444\\u0430\\u0439\\u043b\\u0430 `file_path`\", \"library\": [\"os\"]}", "generation": "Scenario 1:\nos, pandas, seaborn, matplotlib\n```python\nimport os\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef analyze_file_sizes(directory):\n    \"\"\"\n    Analyze file sizes in a directory and generate a bar plot of file extensions \n    versus average file size for each extension.\n    \n    Parameters:\n    directory (str): The path to the directory.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with file extensions and their average sizes.\n    \n    Requirements:\n    - os\n    - pandas\n    - seaborn\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = analyze_file_sizes('/path/to/directory')\n    >>> print(df)\n    >>> sns.barplot(x='Extension', y='Average Size', data=df)\n    >>> plt.show()\n    \"\"\"\n    files_data = []\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            file_path = os.path.join(root, file)\n            _, extension = os.path.splitext(file_path)\n            size = os.path.getsize(file_path)\n            files_data.append([extension, size])\n\n    df = pd.DataFrame(files_data, columns=['Extension', 'Size'])\n    df = df.groupby('Extension').mean().reset_index()\n    df.columns = ['Extension', 'Average Size']\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, csv, json, pandas\n```python\nimport os\nimport csv\nimport json\nimport pandas as pd\n\ndef convert_csv_to_json(file_path):\n    \"\"\"\n    Convert a CSV file to a JSON file. The name of the JSON file will be the same \n    as the CSV file with a .json extension.\n\n    Parameters:\n    file_path (str): The path to the CSV file.\n\n    Returns:\n    str: The path to the JSON file.\n\n    Requirements:\n    - os\n    - csv\n    - json\n    - pandas\n\n    Example:\n    >>> convert_csv_to_json('/path/to/file.csv')\n    \"\"\"\n    filename, _ = os.path.splitext(file_path)\n    json_file_path = filename + '.json'\n    \n    df = pd.read_csv(file_path)\n    df.to_json(json_file_path, orient='records')\n\n    return json_file_path\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, shutil, zipfile\n```python\nimport os\nimport shutil\nfrom zipfile import ZipFile\n\nTEMP_DIR = '/tmp'\n\ndef move_and_compress_files(file_path, destination_dir):\n    \"\"\"\n    Move a file to a destination directory and compress it into a zip file.\n\n    Parameters:\n    file_path (str): The path to the file.\n    destination_dir (str): The path to the destination directory.\n\n    Returns:\n    str: The path to the compressed file.\n\n    Requirements:\n    - os\n    - shutil\n    - zipfile\n\n    Example:\n    >>> move_and_compress_files('/path/to/file.txt', '/path/to/destination')\n    \"\"\"\n    filename = os.path.basename(file_path)\n    filename_without_ext, _ = os.path.splitext(filename)\n    shutil.move(file_path, destination_dir)\n    \n    compressed_file_path = os.path.join(destination_dir, filename_without_ext + '.zip')\n    with ZipFile(compressed_file_path, 'w') as zipf:\n        zipf.write(os.path.join(destination_dir, filename), arcname=filename)\n\n    return compressed_file_path\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, hashlib, pickle\n```python\nimport os\nimport hashlib\nimport pickle\n\nCACHE_DIR = '/tmp'\n\ndef cache_file_content(file_path):\n    \"\"\"\n    Cache the content of a file using pickle. The cache file will be named using \n    the MD5 hash of the file path.\n\n    Parameters:\n    file_path (str): The path to the file.\n\n    Returns:\n    str: The path to the cache file.\n\n    Requirements:\n    - os\n    - hashlib\n    - pickle\n\n    Example:\n    >>> cache_file_content('/path/to/file.txt')\n    \"\"\"\n    cache_file_name = hashlib.md5(file_path.encode()).hexdigest() + '.pickle'\n    cache_file_path = os.path.join(CACHE_DIR, cache_file_name)\n\n    with open(file_path, 'r') as f:\n        content = f.read()\n\n    with open(cache_file_path, 'wb') as f:\n        pickle.dump(content, f)\n\n    return cache_file_path\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, pandas, matplotlib, seaborn\n```python\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef analyze_directory_structure(directory):\n    \"\"\"\n    Analyze the structure of a directory and generate a histogram of the number of \n    files per subdirectory.\n\n    Parameters:\n    directory (str): The path to the directory.\n\n    Returns:\n    DataFrame: A pandas DataFrame with subdirectories and their file counts.\n\n    Requirements:\n    - os\n    - pandas\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> df = analyze_directory_structure('/path/to/directory')\n    >>> print(df)\n    >>> sns.histplot(data=df, x='File Count')\n    >>> plt.show()\n    \"\"\"\n    dir_data = []\n    for root, dirs, files in os.walk(directory):\n        dir_name = os.path.basename(root)\n        file_count = len(files)\n        dir_data.append([dir_name, file_count])\n\n    df = pd.DataFrame(dir_data, columns=['Directory', 'File Count'])\n\n    return df\n```"}
{"seed": "{\"task_id\": 540082, \"prompt\": \"def f_540082(file_path):\\n\\t\", \"suffix\": \"\\n\\treturn file_extension\", \"canonical_solution\": \"file_extension= file_path.split('.')[-1]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('file_name.txt') == 'txt'\\n\"], \"entry_point\": \"f_540082\", \"intent\": \"\\u041f\\u043e\\u043b\\u0443\\u0447\\u0438\\u0442\\u044c \\u0440\\u0430\\u0441\\u0448\\u0438\\u0440\\u0435\\u043d\\u0438\\u0435 `file_extension` \\u0444\\u0430\\u0439\\u043b\\u0430 `file_path`\", \"library\": []}", "generation": "Scenario 1:\nos, csv, pandas, matplotlib, statistics\n\n```python\nimport os\nimport csv\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport statistics\n\n# Constants\nFILE_TYPES = ['csv', 'txt', 'json']\n\ndef analyze_file(file_path):\n    \"\"\"\n    Analyze a file given its path. If the file is a CSV, it will calculate and \n    return the mean, median, mode and plot a histogram of the first numeric column. \n    For other file types, it will just return the file size in bytes.\n    \n    Parameters:\n    file_path (str): The path to the file.\n    \n    Returns:\n    dict/None: A dictionary with the mean, median and mode of the first numeric \n    column for CSV files, the file size for other file types, None if the file does not exist.\n    \n    Requirements:\n    - os\n    - csv\n    - pandas\n    - matplotlib.pyplot\n    - statistics\n    \n    Example:\n    >>> analyze_file('data.csv')\n    \"\"\"\n    if not os.path.exists(file_path):\n        return None\n\n    file_extension = file_path.split('.')[-1]\n\n    if file_extension not in FILE_TYPES:\n        return None\n\n    if file_extension != 'csv':\n        return {'Size (bytes)': os.path.getsize(file_path)}\n\n    df = pd.read_csv(file_path)\n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n\n    if numeric_cols:\n        col = df[numeric_cols[0]]\n        return {\n            'Mean': col.mean(),\n            'Median': statistics.median(col),\n            'Mode': statistics.mode(col)\n        }\n    else:\n        return {}\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, pathlib, shutil, zipfile\n\n```python\nimport os\nfrom pathlib import Path\nimport shutil\nimport zipfile\n\n# Constants\nSUPPORTED_EXTENSIONS = ['.txt', '.docx', '.xlsx', '.pdf']\n\ndef backup_files(directory):\n    \"\"\"\n    Back up all files with certain extensions from a directory to a zip file. \n    The function will return the path to the zip file or None if no files were backed up.\n    \n    Parameters:\n    directory (str): The directory to back up.\n    \n    Returns:\n    str/None: The path to the zip file or None if no files were backed up.\n    \n    Requirements:\n    - os\n    - pathlib.Path\n    - shutil\n    - zipfile\n    \n    Example:\n    >>> backup_files('/home/user/documents')\n    \"\"\"\n    directory = Path(directory)\n    backup_files = list(directory.glob('*'))\n    backup_files = [f for f in backup_files if f.suffix in SUPPORTED_EXTENSIONS]\n\n    if not backup_files:\n        return None\n\n    backup_zip = f'{directory.name}_backup.zip'\n    with zipfile.ZipFile(backup_zip, 'w') as zipf:\n        for file in backup_files:\n            zipf.write(file)\n\n    return str(directory / backup_zip)\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, glob, PIL, numpy, matplotlib\n\n```python\nimport os\nimport glob\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_image_histogram(directory):\n    \"\"\"\n    Plot the RGB histogram for all PNG images in a directory.\n    \n    Parameters:\n    directory (str): The directory containing the images.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - os\n    - glob\n    - PIL.Image\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> plot_image_histogram('/home/user/images')\n    \"\"\"\n    file_paths = glob.glob(os.path.join(directory, '*.png'))\n\n    for file_path in file_paths:\n        image = Image.open(file_path)\n        image_array = np.array(image)\n        red, green, blue = image_array[:,:,0], image_array[:,:,1], image_array[:,:,2]\n\n        plt.figure(figsize=(10, 8))\n        plt.hist(red.ravel(), bins=256, color='red', alpha=0.5)\n        plt.hist(green.ravel(), bins=256, color='green', alpha=0.5)\n        plt.hist(blue.ravel(), bins=256, color='blue', alpha=0.5)\n        plt.title('RGB Histogram')\n        plt.xlabel('Intensity')\n        plt.ylabel('Count')\n        plt.show()\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, shutil, hashlib\n\n```python\nimport os\nimport shutil\nimport hashlib\n\ndef remove_duplicate_files(directory):\n    \"\"\"\n    Remove duplicate files from a directory based on their md5 hash.\n    \n    Parameters:\n    directory (str): The directory from which to remove duplicates.\n    \n    Returns:\n    int: The number of files removed.\n    \n    Requirements:\n    - os\n    - shutil\n    - hashlib\n    \n    Example:\n    >>> remove_duplicate_files('/home/user/downloads')\n    \"\"\"\n    file_hashes = {}\n    files_removed = 0\n\n    for file_name in os.listdir(directory):\n        file_path = os.path.join(directory, file_name)\n\n        with open(file_path, 'rb') as file:\n            file_hash = hashlib.md5(file.read()).hexdigest()\n\n        if file_hash in file_hashes:\n            os.remove(file_path)\n            files_removed += 1\n        else:\n            file_hashes[file_hash] = file_name\n\n    return files_removed\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, pandas, seaborn, matplotlib\n\n```python\nimport os\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef plot_file_sizes(directory):\n    \"\"\"\n    Plot a bar chart of the sizes (in KB) of all files in a directory.\n    \n    Parameters:\n    directory (str): The directory containing the files to plot.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - os\n    - pandas\n    - seaborn\n    - matplotlib.pyplot\n    \n    Example:\n    >>> plot_file_sizes('/home/user/data')\n    \"\"\"\n    files = os.listdir(directory)\n    sizes = [os.path.getsize(os.path.join(directory, f)) / 1024 for f in files]\n\n    df = pd.DataFrame({'File': files, 'Size (KB)': sizes})\n    df.sort_values('Size (KB)', inplace=True)\n\n    plt.figure(figsize=(10, 6))\n    sns.barplot(data=df, x='File', y='Size (KB)')\n    plt.title('File Sizes')\n    plt.xticks(rotation=90)\n    plt.tight_layout()\n    plt.show()\n```\n"}
{"seed": "{\"task_id\": 502573, \"prompt\": \"def f_502573(s, symbol):\\n\\t\", \"suffix\": \"\\n\\treturn s\", \"canonical_solution\": \"s = s[:s.find(symbol)]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('123abcyyl', 'cy') == '123ab'\\n\", \"\\n    assert candidate('123abcyyl', '123') == ''\\n\"], \"entry_point\": \"f_502573\", \"intent\": \"\\u0423\\u0434\\u0430\\u043b\\u0438\\u0442\\u044c \\u0432\\u0441\\u0435 \\u0441\\u0438\\u043c\\u0432\\u043e\\u043b\\u044b \\u0432 \\u0441\\u0442\\u0440\\u043e\\u043a\\u0435 `s` \\u043f\\u043e\\u0441\\u043b\\u0435 \\u0441\\u0438\\u043c\\u0432\\u043e\\u043b\\u0430 `symbol`\", \"library\": []}", "generation": "Scenario 1:\ncsv, pandas, datetime, random\n```python\nimport csv\nimport pandas as pd\nimport random\nfrom datetime import datetime\n\n# Constants\nCHARS = 'abcdefghijklmnopqrstuvwxyz'\nOUTPUT_FILE = 'output.csv'\n\ndef generate_csv_file(s, symbol):\n    \"\"\"\n    Generate a CSV file where each row contains a string that has been truncated at the first occurrence of a symbol.\n\n    Parameters:\n    s (str): The string to be truncated.\n    symbol (str): The symbol at which the string should be truncated.\n\n    Requirements:\n    - csv\n    - pandas\n    - datetime\n    - random\n\n    Example:\n    >>> generate_csv_file('abcde', 'c')\n    \"\"\"\n    truncated_strings = []\n    for i in range(100):\n        temp_s = s + ''.join(random.choice(CHARS) for _ in range(100))\n        truncated_s = temp_s[:temp_s.find(symbol)]\n        truncated_strings.append([truncated_s, datetime.now()])\n\n    with open(OUTPUT_FILE, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerows(truncated_strings)\n\n    df = pd.read_csv(OUTPUT_FILE, names=['String', 'Timestamp'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, matplotlib, random\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\n\n# Constants\nCHARS = 'abcdefghijklmnopqrstuvwxyz'\nSYMBOLS = 'abcd'\n\ndef generate_histogram(s, symbol):\n    \"\"\"\n    Generate a histogram of the lengths of strings that have been truncated at the first occurrence of a symbol.\n\n    Parameters:\n    s (str): The string to be truncated.\n    symbol (str): The symbol at which the string should be truncated.\n\n    Requirements:\n    - numpy\n    - matplotlib\n    - random\n\n    Example:\n    >>> generate_histogram('abcde', 'c')\n    \"\"\"\n    lengths = []\n    for i in range(1000):\n        temp_s = s + ''.join(random.choice(CHARS) for _ in range(100))\n        truncated_s = temp_s[:temp_s.find(symbol)]\n        lengths.append(len(truncated_s))\n    lengths = np.array(lengths)\n\n    plt.hist(lengths, bins=range(min(lengths), max(lengths) + 1), align='left')\n    plt.show()\n\n    return lengths\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, pandas, random\n```python\nimport numpy as np\nimport pandas as pd\nimport random\n\n# Constants\nCHARS = 'abcdefghijklmnopqrstuvwxyz'\nSYMBOLS = 'abcd'\n\ndef generate_dataframe(s, symbol):\n    \"\"\"\n    Generate a DataFrame of strings that have been truncated at the first occurrence of a symbol and their lengths.\n\n    Parameters:\n    s (str): The string to be truncated.\n    symbol (str): The symbol at which the string should be truncated.\n\n    Requirements:\n    - numpy\n    - pandas\n    - random\n\n    Example:\n    >>> generate_dataframe('abcde', 'c')\n    \"\"\"\n    truncated_strings = []\n    lengths = []\n    for i in range(1000):\n        temp_s = s + ''.join(random.choice(CHARS) for _ in range(100))\n        truncated_s = temp_s[:temp_s.find(symbol)]\n        truncated_strings.append(truncated_s)\n        lengths.append(len(truncated_s))\n    lengths = np.array(lengths)\n\n    df = pd.DataFrame({'String': truncated_strings, 'Length': lengths})\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ncsv, random, datetime\n```python\nimport csv\nimport random\nfrom datetime import datetime\n\n# Constants\nCHARS = 'abcdefghijklmnopqrstuvwxyz'\nSYMBOLS = 'abcd'\nOUTPUT_FILE = 'output.csv'\n\ndef generate_csv_file_with_timestamp(s, symbol):\n    \"\"\"\n    Generate a CSV file where each row contains a string that has been truncated at the first occurrence of a symbol and a timestamp.\n\n    Parameters:\n    s (str): The string to be truncated.\n    symbol (str): The symbol at which the string should be truncated.\n\n    Requirements:\n    - csv\n    - random\n    - datetime\n\n    Example:\n    >>> generate_csv_file_with_timestamp('abcde', 'c')\n    \"\"\"\n    truncated_strings = []\n    for i in range(100):\n        temp_s = s + ''.join(random.choice(CHARS) for _ in range(100))\n        truncated_s = temp_s[:temp_s.find(symbol)]\n        truncated_strings.append([truncated_s, datetime.now()])\n\n    with open(OUTPUT_FILE, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerows(truncated_strings)\n\n    return truncated_strings\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, matplotlib, random\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\n\n# Constants\nCHARS = 'abcdefghijklmnopqrstuvwxyz'\nSYMBOLS = 'abcd'\n\ndef generate_scatter_plot(s, symbol):\n    \"\"\"\n    Generate a scatter plot of the lengths of strings that have been truncated at the first occurrence of a symbol.\n\n    Parameters:\n    s (str): The string to be truncated.\n    symbol (str): The symbol at which the string should be truncated.\n\n    Requirements:\n    - numpy\n    - matplotlib\n    - random\n\n    Example:\n    >>> generate_scatter_plot('abcde', 'c')\n    \"\"\"\n    lengths = []\n    for i in range(1000):\n        temp_s = s + ''.join(random.choice(CHARS) for _ in range(100))\n        truncated_s = temp_s[:temp_s.find(symbol)]\n        lengths.append(len(truncated_s))\n    lengths = np.array(lengths)\n\n    plt.scatter(range(len(lengths)), lengths)\n    plt.show()\n\n    return lengths\n```\n"}
{"seed": "{\"task_id\": 502573, \"prompt\": \"def f_502573(s, symbol):\\n\\t\", \"suffix\": \"\\n\\treturn s\", \"canonical_solution\": \"s = s.split(symbol)[0]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('123abcyyl', 'cy') == '123ab'\\n\", \"\\n    assert candidate('123abcyyl', '234') == '123abcyyl'\\n\"], \"entry_point\": \"f_502573\", \"intent\": \"\\u0423\\u0434\\u0430\\u043b\\u0438\\u0442\\u044c \\u0432\\u0441\\u0435 \\u0441\\u0438\\u043c\\u0432\\u043e\\u043b\\u044b \\u0432 \\u0441\\u0442\\u0440\\u043e\\u043a\\u0435 `s` \\u043f\\u043e\\u0441\\u043b\\u0435 \\u0441\\u0438\\u043c\\u0432\\u043e\\u043b\\u0430 `symbol`\", \"library\": []}", "generation": "Scenario 1:\nre, os, zipfile, glob\n```python\nimport re\nimport os\nimport zipfile\nimport glob\n\n# Constant\nSYMBOLS = ['_', '-', '.']\n\ndef extract_and_rename_files(zip_path, symbol):\n    \"\"\"\n    Given a zip file path, extract all the files and rename them by removing all characters \n    after the given symbol in their filenames.\n\n    Parameters:\n    zip_path (str): The path of the zip file.\n    symbol (str): The symbol after which all characters should be removed in the filenames.\n\n    Returns:\n    list: The list of new filenames.\n\n    Requirements:\n    - re\n    - os\n    - zipfile\n    - glob\n\n    Example:\n    >>> extract_and_rename_files('path/to/zipfile.zip', '-')\n    \"\"\"\n    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n        zip_ref.extractall()\n        \n    files = glob.glob('./*')\n    \n    new_filenames = []\n    for file in files:\n        new_filename = re.split(symbol, os.path.basename(file))[0]\n        new_filenames.append(new_filename)\n        os.rename(file, new_filename)\n    \n    return new_filenames\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nstring, os, shutil\n```python\nimport string\nimport os\nimport shutil\n\n# Constants\nSYMBOLS = ['_', '-', '.']\n\ndef organize_files_by_first_letter(path, symbol):\n    \"\"\"\n    Organize files in a directory by moving them into subdirectories named by the first letter \n    of the filename (after removing all characters after the symbol in the filename).\n\n    Parameters:\n    path (str): The directory path.\n    symbol (str): The symbol after which all characters should be removed in the filenames.\n\n    Returns:\n    None\n\n    Requirements:\n    - string\n    - os\n    - shutil\n\n    Example:\n    >>> organize_files_by_first_letter('path/to/directory', '-')\n    \"\"\"\n    for filename in os.listdir(path):\n        new_filename = filename.split(symbol)[0]\n        first_letter = new_filename[0]\n        if first_letter in string.ascii_letters:\n            new_path = os.path.join(path, first_letter.upper())\n            os.makedirs(new_path, exist_ok=True)\n            shutil.move(os.path.join(path, filename), os.path.join(new_path, new_filename))\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nre, numpy, pandas\n```python\nimport re\nimport numpy as np\nimport pandas as pd\n\n# Constants\nSYMBOLS = ['_', '-', '.']\n\ndef split_data_into_columns(df, column, symbol):\n    \"\"\"\n    Split the values in a DataFrame column into multiple columns by a given symbol.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n    column (str): The column to split.\n    symbol (str): The symbol to split the column by.\n\n    Returns:\n    DataFrame: The DataFrame with the split columns.\n\n    Requirements:\n    - re\n    - numpy\n    - pandas\n\n    Example:\n    >>> df = pd.DataFrame({'A': ['123-456-789', 'abc-def-ghi']})\n    >>> split_data_into_columns(df, 'A', '-')\n    \"\"\"\n    split_data = df[column].str.split(symbol, expand=True)\n    df = pd.concat([df, split_data], axis=1)\n    df.drop(columns=[column], inplace=True)\n    \n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nre, nltk, sklearn.feature_extraction.text\n```python\nimport re\nimport nltk\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Constants\nSYMBOLS = ['_', '-', '.']\n\ndef count_words_before_symbol(texts, symbol):\n    \"\"\"\n    Count the number of words before a given symbol in a list of texts.\n\n    Parameters:\n    texts (list): The list of texts.\n    symbol (str): The symbol.\n\n    Returns:\n    list: The list of word counts.\n\n    Requirements:\n    - re\n    - nltk\n    - sklearn.feature_extraction.text\n\n    Example:\n    >>> count_words_before_symbol(['This is a text. And this is another text.', 'This is a text, and this is yet another text.'], '.')\n    \"\"\"\n    word_counts = []\n    for text in texts:\n        words_before_symbol = re.split(symbol, text)[0]\n        words = nltk.word_tokenize(words_before_symbol)\n        word_counts.append(len(words))\n        \n    return word_counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nre, os, pandas\n```python\nimport re\nimport os\nimport pandas as pd\n\n# Constants\nSYMBOLS = ['_', '-', '.']\n\ndef rename_and_save_csv_files(dir_path, symbol):\n    \"\"\"\n    Rename CSV files in a directory by removing all characters after a given symbol in \n    their filenames, and save the data in each CSV file into a new DataFrame.\n\n    Parameters:\n    dir_path (str): The directory path.\n    symbol (str): The symbol after which all characters should be removed in the filenames.\n\n    Returns:\n    dict: A dictionary with the new filenames as keys and the corresponding DataFrames as values.\n\n    Requirements:\n    - re\n    - os\n    - pandas\n\n    Example:\n    >>> rename_and_save_csv_files('path/to/directory', '-')\n    \"\"\"\n    csv_files = [f for f in os.listdir(dir_path) if f.endswith('.csv')]\n    \n    dataframes = {}\n    for file in csv_files:\n        new_filename = re.split(symbol, os.path.basename(file))[0] + '.csv'\n        df = pd.read_csv(os.path.join(dir_path, file))\n        df.to_csv(os.path.join(dir_path, new_filename), index=False)\n        dataframes[new_filename] = df\n    \n    return dataframes\n```"}
{"seed": "{\"task_id\": 21825, \"prompt\": \"def f_21825(my_list1, my_list2):\\n\\t\", \"suffix\": \"\\n\\treturn diff_list\", \"canonical_solution\": \"count_values1 = Counter(my_list1)\\n\\n\\tcount_values2 = Counter(my_list2)\\n\\n\\t\\n\\tkeys1 = list(count_values1.keys())\\n\\n\\tkeys2 = list(count_values2.keys())\\n\\n\\t\\n\\tdiff_list = []\\n\\n\\tfor key in set(keys1 + keys2):\\n\\n\\t\\tif key in keys1 and key in keys2:\\n\\n\\t\\t\\tdiff_list += [key] * abs(count_values1[key] - count_values2[key])\\n\\n\\t\\telif key in keys1:\\n\\n\\t\\t\\tdiff_list += [key] * count_values1[key]\\n\\n\\t\\telif key in keys2:\\n\\n\\t\\t\\tdiff_list += [key] * count_values2[key]\\n\", \"test_start\": \"\\nfrom collections import Counter\\n\\ndef check(candidate):\", \"test\": [\"\\n    list1 = [1, 3, 4]\\n    list2 = [5, 4, 9]\\n    assert candidate(list1, list2) == [1, 3, 5, 9]\\n\", \"\\n    list1 = ['abc', 'klm']\\n    list2= ['pqr', 'xyz']\\n    assert len(candidate(list1, list2)) == 4\\n\"], \"entry_point\": \"f_21825\", \"intent\": \"\\u041d\\u0430\\u0439\\u0442\\u0438 \\u0440\\u0430\\u0437\\u043d\\u0438\\u0446\\u0443 `diff_list` \\u0441\\u043f\\u0438\\u0441\\u043a\\u043e\\u0432 `my_list1` \\u0438 `my_list2`\", \"library\": [\"collections\"]}", "generation": "Scenario 1: \npandas, collections, math, random\n```python\nimport pandas as pd\nfrom collections import Counter\nimport math\nfrom random import choices\n\n# Constants\nLETTERS = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']\n\ndef calculate_statistics(my_list1, my_list2):\n    \"\"\"\n    Calculate various statistical measures (mean, median, mode) of the difference \n    between two lists of numbers. Also, randomly select a subset of the difference \n    list and return as a pandas Series.\n    \n    Parameters:\n    my_list1, my_list2 (list): The lists of numbers.\n    \n    Returns:\n    tuple: A tuple containing mean, median, mode, and a pandas Series of a subset.\n    \n    Requirements:\n    - pandas\n    - collections\n    - math\n    - random\n    \n    Example:\n    >>> list1 = [1, 2, 3, 4, 5]\n    >>> list2 = [4, 5, 6, 7, 8]\n    >>> mean, median, mode, subset = calculate_statistics(list1, list2)\n    >>> print(f'Mean: {mean}, Median: {median}, Mode: {mode}')\n    >>> print(subset)\n    \"\"\"\n    diff_list = list(set(my_list1) ^ set(my_list2)) # XOR operation to get unique elements\n    \n    mean = sum(diff_list) / len(diff_list)\n    median = sorted(diff_list)[len(diff_list)//2]\n    mode = Counter(diff_list).most_common(1)[0][0]\n\n    subset = pd.Series(choices(diff_list, k=5), index=choices(LETTERS, k=5))\n\n    return mean, median, mode, subset\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, collections, matplotlib.pyplot, random\n```python\nimport numpy as np\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nfrom random import sample\n\n# Constants\nLETTERS = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']\n\ndef plot_difference_histogram(my_list1, my_list2):\n    \"\"\"\n    Find the difference between two lists and plot a histogram of the \n    difference list. Also, randomly select a subset of the difference \n    list and return as a numpy array.\n    \n    Parameters:\n    my_list1, my_list2 (list): The lists of numbers.\n    \n    Returns:\n    numpy array: A numpy array of a subset of the difference list.\n    \n    Requirements:\n    - numpy\n    - collections\n    - matplotlib.pyplot\n    - random\n    \n    Example:\n    >>> list1 = [1, 2, 3, 4, 5]\n    >>> list2 = [4, 5, 6, 7, 8]\n    >>> subset = plot_difference_histogram(list1, list2)\n    >>> print(subset)\n    \"\"\"\n    diff_list = list(set(my_list1) ^ set(my_list2)) # XOR operation to get unique elements\n    \n    plt.hist(diff_list, bins=10, alpha=0.5)\n    plt.title('Histogram of the Difference List')\n    plt.show()\n\n    subset = np.array(sample(diff_list, 5))\n\n    return subset\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \npandas, collections, itertools, random\n```python\nimport pandas as pd\nfrom collections import Counter\nfrom itertools import combinations\nfrom random import choice\n\ndef generate_combinations(my_list1, my_list2):\n    \"\"\"\n    Generate and return all possible combinations of a certain length from the \n    difference between two lists. Also, randomly select one combination and \n    return as a pandas Series.\n    \n    Parameters:\n    my_list1, my_list2 (list): The lists of numbers.\n    \n    Returns:\n    pandas Series: A pandas Series of a randomly selected combination.\n    \n    Requirements:\n    - pandas\n    - collections\n    - itertools\n    - random\n    \n    Example:\n    >>> list1 = [1, 2, 3, 4, 5]\n    >>> list2 = [4, 5, 6, 7, 8]\n    >>> combination = generate_combinations(list1, list2)\n    >>> print(combination)\n    \"\"\"\n    diff_list = list(set(my_list1) ^ set(my_list2)) # XOR operation to get unique elements\n    \n    combinations_list = list(combinations(diff_list, 3))\n    \n    random_combination = pd.Series(choice(combinations_list))\n\n    return random_combination\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, collections, matplotlib.pyplot, random\n```python\nimport numpy as np\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nfrom random import sample\n\n# Constants\nLETTERS = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']\n\ndef plot_difference_scatter(my_list1, my_list2):\n    \"\"\"\n    Find the difference between two lists and plot a scatter plot of the \n    difference list. Also, randomly select a subset of the difference \n    list and return as a numpy array.\n    \n    Parameters:\n    my_list1, my_list2 (list): The lists of numbers.\n    \n    Returns:\n    numpy array: A numpy array of a subset of the difference list.\n    \n    Requirements:\n    - numpy\n    - collections\n    - matplotlib.pyplot\n    - random\n    \n    Example:\n    >>> list1 = [1, 2, 3, 4, 5]\n    >>> list2 = [4, 5, 6, 7, 8]\n    >>> subset = plot_difference_scatter(list1, list2)\n    >>> print(subset)\n    \"\"\"\n    diff_list = list(set(my_list1) ^ set(my_list2)) # XOR operation to get unique elements\n    \n    x = np.arange(len(diff_list))\n    y = diff_list\n    \n    plt.scatter(x, y)\n    plt.title('Scatter Plot of the Difference List')\n    plt.show()\n\n    subset = np.array(sample(diff_list, 5))\n\n    return subset\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, collections, itertools, random\n```python\nimport pandas as pd\nfrom collections import Counter\nfrom itertools import permutations\nfrom random import choice\n\ndef generate_permutations(my_list1, my_list2):\n    \"\"\"\n    Generate and return all possible permutations of a certain length from the \n    difference between two lists. Also, randomly select one permutation and \n    return as a pandas Series.\n    \n    Parameters:\n    my_list1, my_list2 (list): The lists of numbers.\n    \n    Returns:\n    pandas Series: A pandas Series of a randomly selected permutation.\n    \n    Requirements:\n    - pandas\n    - collections\n    - itertools\n    - random\n    \n    Example:\n    >>> list1 = [1, 2, 3, 4, 5]\n    >>> list2 = [4, 5, 6, 7, 8]\n    >>> permutation = generate_permutations(list1, list2)\n    >>> print(permutation)\n    \"\"\"\n    diff_list = list(set(my_list1) ^ set(my_list2)) # XOR operation to get unique elements\n    \n    permutations_list = list(permutations(diff_list, 3))\n    \n    random_permutation = pd.Series(choice(permutations_list))\n\n    return random_permutation\n```\n`GPT_ODEX_BREAK`"}
{"seed": "{\"task_id\": 21825, \"prompt\": \"def f_21825(my_list1, my_list2):\\n\\t\", \"suffix\": \"\\n\\treturn diff_list\", \"canonical_solution\": \"diff_list = list(set(my_list1)-set(my_list2)) + list(set(my_list2)-set(my_list1))\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert sorted(candidate([1,2,3,4],[2,3,4,5])) == [1,5]\\n\", \"\\n    assert sorted(candidate([1,2,3,4],[])) == [1,2,3,4]\\n\"], \"entry_point\": \"f_21825\", \"intent\": \"\\u041d\\u0430\\u0439\\u0442\\u0438 \\u0440\\u0430\\u0437\\u043d\\u0438\\u0446\\u0443 `diff_list` \\u0441\\u043f\\u0438\\u0441\\u043a\\u043e\\u0432 `my_list1` \\u0438 `my_list2`\", \"library\": []}", "generation": "Scenario 1: \nnumpy, pandas, matplotlib, seaborn, scipy.stats\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import ttest_ind\n\ndef f_21825_1(list1, list2):\n    \"\"\"\n    Performs a t-test for the means of two independent samples of scores from list1 and list2.\n    Generates a histogram for each list, and a boxplot for the combined data.\n\n    Parameters:\n    list1 (list): List of scores for group 1.\n    list2 (list): List of scores for group 2.\n\n    Returns:\n    float: p-value from the t-test. If the p-value is less than 0.05, \n    we reject the null hypothesis of equal averages.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - seaborn\n    - scipy.stats.ttest_ind\n\n    Example:\n    >>> f_21825_1([1, 2, 3, 4, 5], [2, 3, 4, 5, 6])\n    \"\"\"\n    array1 = np.array(list1)\n    array2 = np.array(list2)\n\n    # Perform t-test\n    t_stat, p_value = ttest_ind(array1, array2)\n\n    # Generate histogram for each list\n    plt.figure(figsize=(12, 6))\n    plt.subplot(1, 2, 1)\n    sns.histplot(array1, color='blue', label='list1')\n    plt.legend()\n\n    plt.subplot(1, 2, 2)\n    sns.histplot(array2, color='green', label='list2')\n    plt.legend()\n\n    plt.show()\n\n    # Generate boxplot for the combined data\n    combined_data = pd.DataFrame({\n        'group': ['list1']*len(list1) + ['list2']*len(list2),\n        'value': list1 + list2\n    })\n\n    sns.boxplot(x='group', y='value', data=combined_data)\n    plt.show()\n\n    return p_value\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, pandas, sklearn.preprocessing, sklearn.decomposition, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\ndef f_21825_2(list1, list2):\n    \"\"\"\n    Performs principal component analysis (PCA) on the data from list1 and list2,\n    and visualize the first two principal components.\n\n    Parameters:\n    list1 (list): List of multi-dimensional data points for group 1.\n    list2 (list): List of multi-dimensional data points for group 2.\n\n    Returns:\n    PCA: The PCA object fitted on the data.\n\n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.preprocessing.StandardScaler\n    - sklearn.decomposition.PCA\n    - matplotlib.pyplot\n\n    Example:\n    >>> f_21825_2([[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]])\n    \"\"\"\n    array1 = np.array(list1)\n    array2 = np.array(list2)\n\n    # Combine the data\n    combined_data = np.concatenate((array1, array2), axis=0)\n\n    # Standardize the data\n    scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(combined_data)\n\n    # Perform PCA\n    pca = PCA(n_components=2)\n    pca_result = pca.fit_transform(standardized_data)\n\n    # Visualize the first two principal components\n    plt.figure(figsize=(8, 6))\n    plt.scatter(pca_result[:len(array1), 0], pca_result[:len(array1), 1], color='blue', label='list1')\n    plt.scatter(pca_result[len(array1):, 0], pca_result[len(array1):, 1], color='green', label='list2')\n    plt.legend()\n\n    plt.xlabel('PC1')\n    plt.ylabel('PC2')\n\n    plt.show()\n\n    return pca\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, pandas, sklearn.cluster, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\ndef f_21825_3(list1, list2):\n    \"\"\"\n    Performs K-means clustering on the data from list1 and list2,\n    and visualizes the clusters.\n\n    Parameters:\n    list1 (list): List of two-dimensional data points for group 1.\n    list2 (list): List of two-dimensional data points for group 2.\n\n    Returns:\n    KMeans: The KMeans object fitted on the data.\n\n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.cluster.KMeans\n    - matplotlib.pyplot\n\n    Example:\n    >>> f_21825_3([[1, 2], [3, 4]], [[5, 6], [7, 8]])\n    \"\"\"\n    array1 = np.array(list1)\n    array2 = np.array(list2)\n\n    # Combine the data\n    combined_data = np.concatenate((array1, array2), axis=0)\n\n    # Perform K-means clustering\n    kmeans = KMeans(n_clusters=2)\n    kmeans.fit(combined_data)\n    labels = kmeans.labels_\n\n    # Visualize the clusters\n    plt.scatter(combined_data[:, 0], combined_data[:, 1], c=labels, cmap='viridis')\n    plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red')\n    plt.show()\n\n    return kmeans\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, pandas, sklearn.linear_model, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\n\ndef f_21825_4(list1, list2):\n    \"\"\"\n    Performs linear regression with the data from list1 as the independent variable \n    and the data from list2 as the dependent variable, and plots the regression line.\n\n    Parameters:\n    list1 (list): List of independent variable data.\n    list2 (list): List of dependent variable data.\n\n    Returns:\n    LinearRegression: The LinearRegression object fitted on the data.\n\n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.linear_model.LinearRegression\n    - matplotlib.pyplot\n\n    Example:\n    >>> f_21825_4([1, 2, 3, 4, 5], [2, 3, 4, 5, 6])\n    \"\"\"\n    array1 = np.array(list1).reshape(-1, 1)\n    array2 = np.array(list2).reshape(-1, 1)\n\n    # Perform linear regression\n    model = LinearRegression()\n    model.fit(array1, array2)\n\n    # Plot the regression line\n    plt.scatter(array1, array2, color='blue')\n    plt.plot(array1, model.predict(array1), color='red')\n\n    plt.show()\n\n    return model\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, pandas, sklearn.metrics.pairwise\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ndef f_21825_5(list1, list2):\n    \"\"\"\n    Calculates the cosine similarity between the data from list1 and list2.\n\n    Parameters:\n    list1 (list): List of multi-dimensional data points.\n    list2 (list): List of multi-dimensional data points.\n\n    Returns:\n    float: The cosine similarity between the two lists.\n\n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.metrics.pairwise.cosine_similarity\n\n    Example:\n    >>> f_21825_5([[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]])\n    \"\"\"\n    array1 = np.array(list1)\n    array2 = np.array(list2)\n\n    # Calculate cosine similarity\n    similarity = cosine_similarity(array1, array2)\n\n    return similarity[0, 0]\n```"}
{"seed": "{\"task_id\": 513149, \"prompt\": \"def f_513149(s, my_char):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \" s.index(my_char)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('123abc456', 'ab') == 3\\n\", \"\\n    assert candidate('123abc456', 'bc') == 4\\n\", \"\\n    assert candidate('23abc123', '2') == 0\\n\"], \"entry_point\": \"f_513149\", \"intent\": \"\\u041d\\u0430\\u0439\\u0442\\u0438 \\u043f\\u043e\\u0437\\u0438\\u0446\\u0438\\u044e \\u0441\\u0438\\u043c\\u0432\\u043e\\u043b\\u0430 `my_char` \\u0432 \\u0441\\u0442\\u0440\\u043e\\u043a\\u0435 `s`\", \"library\": []}", "generation": "Scenario 1: \npandas, matplotlib, random, csv\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\nimport csv\n\n# Constants\nLETTERS = list('abcdefghijklmnopqrstuvwxyz')\nNUM_SAMPLES = 10000\n\ndef find_letter_distribution(filename, my_char):\n    \"\"\"\n    Create a csv file with random letters, then read the file and find the distribution of \n    a character and plot a histogram.\n\n    Parameters:\n    filename (str): The name of the csv file to be created.\n    my_char (str): The character to find the distribution.\n\n    Returns:\n    int: The number of occurrences of my_char.\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - random\n    - csv\n\n    Example:\n    >>> find_letter_distribution('letters.csv', 'a')\n    \"\"\"\n    with open(filename, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(LETTERS)\n        for i in range(NUM_SAMPLES):\n            writer.writerow([random.choice(LETTERS) for _ in LETTERS])\n\n    df = pd.read_csv(filename)\n    letter_counts = df.apply(pd.Series.value_counts).sum(axis=1)\n    letter_counts.plot(kind='bar')\n\n    plt.show()\n\n    return int(letter_counts[my_char])\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \ncollections, matplotlib, random, string\n```python\nimport collections\nimport matplotlib.pyplot as plt\nimport random\nimport string\n\n# Constants\nLETTERS = list(string.ascii_lowercase)\nNUM_SAMPLES = 10000\n\ndef find_letter_distribution(my_char):\n    \"\"\"\n    Generate a list of random letters, then find the distribution of \n    a character and plot a histogram.\n\n    Parameters:\n    my_char (str): The character to find the distribution.\n\n    Returns:\n    int: The number of occurrences of my_char.\n\n    Requirements:\n    - collections\n    - matplotlib.pyplot\n    - random\n    - string\n\n    Example:\n    >>> find_letter_distribution('a')\n    \"\"\"\n    letters_list = [random.choice(LETTERS) for _ in range(NUM_SAMPLES)]\n    letter_counts = collections.Counter(letters_list)\n\n    plt.bar(letter_counts.keys(), letter_counts.values())\n    plt.show()\n\n    return letter_counts[my_char]\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnumpy, matplotlib, random, string\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nimport string\n\n# Constants\nLETTERS = list(string.ascii_lowercase)\nNUM_SAMPLES = 10000\n\ndef find_letter_distribution(my_char):\n    \"\"\"\n    Generate a numpy array of random letters, then find the distribution of \n    a character and plot a histogram.\n\n    Parameters:\n    my_char (str): The character to find the distribution.\n\n    Returns:\n    int: The number of occurrences of my_char.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - random\n    - string\n\n    Example:\n    >>> find_letter_distribution('a')\n    \"\"\"\n    letters_array = np.array([random.choice(LETTERS) for _ in range(NUM_SAMPLES)])\n    unique, counts = np.unique(letters_array, return_counts=True)\n    letter_counts = dict(zip(unique, counts))\n\n    plt.bar(letter_counts.keys(), letter_counts.values())\n    plt.show()\n\n    return letter_counts[my_char]\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \npandas, matplotlib, random, string, csv\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\nimport string\nimport csv\n\n# Constants\nLETTERS = list(string.ascii_lowercase)\nNUM_SAMPLES = 10000\n\ndef find_letter_distribution(filename, my_char):\n    \"\"\"\n    Create a csv file with random letters, then read the file and find the distribution of \n    a character and plot a histogram.\n\n    Parameters:\n    filename (str): The name of the csv file to be created.\n    my_char (str): The character to find the distribution.\n\n    Returns:\n    int: The number of occurrences of my_char.\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - random\n    - string\n    - csv\n\n    Example:\n    >>> find_letter_distribution('letters.csv', 'a')\n    \"\"\"\n    with open(filename, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow([random.choice(LETTERS) for _ in range(NUM_SAMPLES)])\n\n    df = pd.read_csv(filename, header=None)\n    letter_counts = df.apply(pd.Series.value_counts).sum(axis=1)\n    letter_counts.plot(kind='bar')\n\n    plt.show()\n\n    return int(letter_counts[my_char])\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnumpy, matplotlib, random, string\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nimport string\n\n# Constants\nLETTERS = list(string.ascii_lowercase)\nNUM_SAMPLES = 10000\n\ndef find_letter_distribution(my_char):\n    \"\"\"\n    Generate a numpy array of random letters, then find the distribution of \n    a character and plot a histogram.\n\n    Parameters:\n    my_char (str): The character to find the distribution.\n\n    Returns:\n    int: The number of occurrences of my_char.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - random\n    - string\n\n    Example:\n    >>> find_letter_distribution('a')\n    \"\"\"\n    letters_array = np.array([random.choice(LETTERS) for _ in range(NUM_SAMPLES)])\n    unique, counts = np.unique(letters_array, return_counts=True)\n    letter_counts = dict(zip(unique, counts))\n\n    plt.bar(letter_counts.keys(), letter_counts.values())\n    plt.show()\n\n    return letter_counts[my_char]\n```"}
{"seed": "{\"task_id\": 513149, \"prompt\": \"def f_513149(s, my_char):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \" s.find(my_char)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('123abc456', 'ab') == 3\\n\", \"\\n    assert candidate('123abc456', 'bc') == 4\\n\", \"\\n    assert candidate('23abc123', '2') == 0\\n\"], \"entry_point\": \"f_513149\", \"intent\": \"\\u041d\\u0430\\u0439\\u0442\\u0438 \\u043f\\u043e\\u0437\\u0438\\u0446\\u0438\\u044e \\u0441\\u0438\\u043c\\u0432\\u043e\\u043b\\u0430 `my_char` \\u0432 \\u0441\\u0442\\u0440\\u043e\\u043a\\u0435 `s`\", \"library\": []}", "generation": "Scenario 1:\ncollections, re, string\n```python\nimport collections\nimport re\nimport string\n\n# Constants\nPUNCTUATION = string.punctuation\n\ndef count_word_frequency(text, word):\n    \"\"\"\n    Count the frequency of a specific word in a text, ignoring case and punctuation.\n\n    Parameters:\n    text (str): The text to search in.\n    word (str): The word to count.\n\n    Returns:\n    int: The frequency of the word in the text.\n\n    Requirements:\n    - collections\n    - re\n    - string\n\n    Example:\n    >>> count_word_frequency('Hello, hello, HELLO!', 'hello')\n    \"\"\"\n    # Remove punctuation\n    text = re.sub(f'[{PUNCTUATION}]', '', text)\n\n    # Convert text to lower case and split into words\n    words = text.lower().split()\n\n    # Count word frequency\n    word_frequency = collections.Counter(words)\n\n    return word_frequency[word]\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, re, fnmatch\n```python\nimport os\nimport re\nimport fnmatch\n\ndef search_files_in_directory(directory, file_pattern, text_pattern):\n    \"\"\"\n    Search for files in a directory that match a file pattern and contain a specific text pattern.\n\n    Parameters:\n    directory (str): The directory to search in.\n    file_pattern (str): The file pattern to match (e.g., '*.txt').\n    text_pattern (str): The text pattern to search for in the files.\n\n    Returns:\n    list: The list of files that match the file pattern and contain the text pattern.\n\n    Requirements:\n    - os\n    - re\n    - fnmatch\n\n    Example:\n    >>> search_files_in_directory('/my/directory', '*.txt', 'my text')\n    \"\"\"\n    matching_files = []\n\n    # Walk through the directory\n    for path, dirs, files in os.walk(directory):\n        for filename in fnmatch.filter(files, file_pattern):\n            file_path = os.path.join(path, filename)\n            with open(file_path, 'r') as file:\n                if re.search(text_pattern, file.read()):\n                    matching_files.append(file_path)\n\n    return matching_files\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nre, random, string\n```python\nimport re\nimport random\nimport string\n\n# Constants\nLETTERS = string.ascii_letters\n\ndef replace_random_characters(text, pattern, num_chars):\n    \"\"\"\n    Replace a number of random characters in the matches of a pattern in a text.\n\n    Parameters:\n    text (str): The text.\n    pattern (str): The pattern to match.\n    num_chars (int): The number of characters to replace in each match.\n\n    Returns:\n    str: The modified text.\n\n    Requirements:\n    - re\n    - random\n    - string\n\n    Example:\n    >>> replace_random_characters('Hello world!', 'world', 2)\n    \"\"\"\n    matches = re.findall(pattern, text)\n\n    for match in matches:\n        if len(match) >= num_chars:\n            for _ in range(num_chars):\n                random_index = random.randint(0, len(match) - 1)\n                random_char = random.choice(LETTERS)\n                match = match[:random_index] + random_char + match[random_index + 1:]\n            text = text.replace(match, match, 1)\n\n    return text\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nre, json, os\n```python\nimport re\nimport json\nimport os\n\ndef extract_data_from_files(directory, file_pattern, data_pattern):\n    \"\"\"\n    Extract data that matches a pattern from files in a directory and store it in a JSON file.\n\n    Parameters:\n    directory (str): The directory to search in.\n    file_pattern (str): The file pattern to match (e.g., '*.txt').\n    data_pattern (str): The data pattern to extract from the files.\n\n    Returns:\n    str: The path to the JSON file.\n\n    Requirements:\n    - re\n    - json\n    - os\n\n    Example:\n    >>> extract_data_from_files('/my/directory', '*.txt', r'\\d+')\n    \"\"\"\n    data = []\n\n    for filename in os.listdir(directory):\n        if re.match(file_pattern, filename):\n            with open(os.path.join(directory, filename), 'r') as file:\n                file_data = re.findall(data_pattern, file.read())\n                data.extend(file_data)\n\n    json_file = os.path.join(directory, 'data.json')\n    with open(json_file, 'w') as file:\n        json.dump(data, file)\n\n    return json_file\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nre, os, csv\n```python\nimport re\nimport os\nimport csv\n\ndef validate_and_export_emails(directory, file_pattern, output_file):\n    \"\"\"\n    Validate email addresses in files in a directory and export them to a CSV file.\n\n    Parameters:\n    directory (str): The directory to search in.\n    file_pattern (str): The file pattern to match (e.g., '*.txt').\n    output_file (str): The CSV file to export the email addresses to.\n\n    Returns:\n    int: The number of valid email addresses.\n\n    Requirements:\n    - re\n    - os\n    - csv\n\n    Example:\n    >>> validate_and_export_emails('/my/directory', '*.txt', 'emails.csv')\n    \"\"\"\n    email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n    valid_emails = []\n\n    for filename in os.listdir(directory):\n        if re.match(file_pattern, filename):\n            with open(os.path.join(directory, filename), 'r') as file:\n                emails = re.findall(email_pattern, file.read())\n                valid_emails.extend(emails)\n\n    with open(output_file, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Email'])\n        for email in valid_emails:\n            writer.writerow([email])\n\n    return len(valid_emails)\n```"}
{"seed": "{\"task_id\": 934453, \"prompt\": \"def f_934453():\\n\\t\", \"suffix\": \"\\n\\treturn my_list\", \"canonical_solution\": \"my_list = list(map(int, input().split()))\", \"test_start\": \"\\nimport builtins\\nfrom unittest.mock import Mock\\n\\ndef check(candidate):\", \"test\": [\"\\n    builtins.input = Mock(return_value = '3 2')\\n    my_list = candidate()\\n    assert my_list == [3, 2]\\n\", \"\\n    builtins.input = Mock(return_value = '300 2')\\n    my_list = candidate()\\n    assert my_list == [300, 2]\\n\"], \"entry_point\": \"f_934453\", \"intent\": \"\\u0412\\u0432\\u0435\\u0441\\u0442\\u0438 \\u0441 \\u043a\\u043b\\u0430\\u0432\\u0438\\u0430\\u0442\\u0443\\u0440\\u044b \\u0447\\u0438\\u0441\\u043b\\u0430 \\u0447\\u0435\\u0440\\u0435\\u0437 \\u043f\\u0440\\u043e\\u0431\\u0435\\u043b \\u0432 \\u0441\\u043f\\u0438\\u0441\\u043e\\u043a `my_list`\", \"library\": [\"builtins\"]}", "generation": "Scenario 1:\npandas, matplotlib, random, numpy, builtins\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import choice\nimport numpy as np\nfrom builtins import input\n\n# Constants\nSTUDENTS = ['John', 'Mary', 'Peter', 'Susan', 'Thomas', 'Julia', 'Robert', 'Linda']\nCOURSES = ['Math', 'English', 'History', 'Biology', 'Chemistry', 'Physics', 'Art', 'Computer Science']\n\ndef generate_student_grades(n):\n    \"\"\"\n    Generate a DataFrame of random grades for n students for multiple courses and then plot a histogram \n    for each course.\n    \n    Parameters:\n    n (int): The number of students.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with random grades for the students.\n    \n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - random\n    - numpy\n    - builtins.input\n    \n    Example:\n    >>> grades = generate_student_grades(10)\n    >>> print(grades)\n    \"\"\"\n    students = [choice(STUDENTS) for _ in range(n)]\n    data = {course: np.random.randint(0, 100, size=n) for course in COURSES}\n    data['Student'] = students\n\n    df = pd.DataFrame(data)\n    df.set_index('Student', inplace=True)\n\n    df.hist(bins=10, figsize=(20, 15))\n    plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nre, string, random, builtins\n```python\nimport re\nimport string\nfrom random import choice\nfrom builtins import input\n\n# Constants\nLETTERS = string.ascii_letters\n\ndef generate_random_string(n, pattern):\n    \"\"\"\n    Generate a random string of length n that matches a given regular expression pattern.\n    \n    Parameters:\n    n (int): The length of the string.\n    pattern (str): The regular expression pattern.\n    \n    Returns:\n    str: The generated string.\n    \n    Requirements:\n    - re\n    - string\n    - random\n    - builtins.input\n    \n    Example:\n    >>> random_string = generate_random_string(10, '[a-z]*')\n    >>> print(random_string)\n    \"\"\"\n    while True:\n        s = ''.join(choice(LETTERS) for _ in range(n))\n        if re.match(pattern, s):\n            return s\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, shutil, glob, builtins\n```python\nimport os\nimport shutil\nfrom glob import glob\nfrom builtins import input\n\ndef move_files_with_extension(src_dir, dest_dir, ext):\n    \"\"\"\n    Move all files with a specific extension from one directory to another.\n    \n    Parameters:\n    src_dir (str): The source directory.\n    dest_dir (str): The destination directory.\n    ext (str): The file extension.\n    \n    Returns:\n    list: The list of files moved.\n    \n    Requirements:\n    - os\n    - shutil\n    - glob\n    - builtins.input\n    \n    Example:\n    >>> moved_files = move_files_with_extension('./src', './dest', '.txt')\n    >>> print(moved_files)\n    \"\"\"\n    files = glob(os.path.join(src_dir, '*.' + ext))\n    for file in files:\n        shutil.move(file, dest_dir)\n    return files\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\njson, collections, builtins\n```python\nimport json\nfrom collections import Counter\nfrom builtins import input\n\ndef count_words_in_json_file(file_path):\n    \"\"\"\n    Count the frequency of words in a JSON file and return the 10 most common words.\n    \n    Parameters:\n    file_path (str): The path to the JSON file.\n    \n    Returns:\n    list: The list of 10 most common words.\n    \n    Requirements:\n    - json\n    - collections\n    - builtins.input\n    \n    Example:\n    >>> common_words = count_words_in_json_file('./data.json')\n    >>> print(common_words)\n    \"\"\"\n    with open(file_path, 'r') as file:\n        data = json.load(file)\n    words = ' '.join(data).split()\n    counter = Counter(words)\n    return counter.most_common(10)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, scipy, matplotlib, builtins\n```python\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nfrom builtins import input\n\ndef plot_normal_distribution(mean, std_dev, n):\n    \"\"\"\n    Generate a normal distribution with a given mean and standard deviation, \n    and plot the distribution and its probability density function.\n    \n    Parameters:\n    mean (float): The mean of the distribution.\n    std_dev (float): The standard deviation of the distribution.\n    n (int): The number of samples.\n    \n    Returns:\n    ndarray: The generated samples.\n    \n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n    - builtins.input\n    \n    Example:\n    >>> samples = plot_normal_distribution(0, 1, 1000)\n    >>> print(samples)\n    \"\"\"\n    samples = np.random.normal(mean, std_dev, n)\n\n    plt.figure(figsize=(10, 6))\n    plt.hist(samples, bins=30, density=True, alpha=0.6, color='g')\n\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, mean, std_dev)\n    plt.plot(x, p, 'k', linewidth=2)\n\n    plt.show()\n\n    return samples\n```"}
{"seed": "{\"task_id\": 934453, \"prompt\": \"def f_934453():\\n\\t\", \"suffix\": \"\\n\\treturn my_list\", \"canonical_solution\": \"my_list = [int(a) for a in input().split()]\", \"test_start\": \"\\nimport builtins\\nfrom unittest.mock import Mock\\n\\ndef check(candidate):\", \"test\": [\"\\n    builtins.input = Mock(return_value = '3 2')\\n    my_list = candidate()\\n    assert my_list == [3, 2]\\n\", \"\\n    builtins.input = Mock(return_value = '100 2 200 3')\\n    my_list = candidate()\\n    assert my_list == [100, 2, 200, 3]\\n\"], \"entry_point\": \"f_934453\", \"intent\": \"\\u0412\\u0432\\u0435\\u0441\\u0442\\u0438 \\u0441 \\u043a\\u043b\\u0430\\u0432\\u0438\\u0430\\u0442\\u0443\\u0440\\u044b \\u0447\\u0438\\u0441\\u043b\\u0430 \\u0447\\u0435\\u0440\\u0435\\u0437 \\u043f\\u0440\\u043e\\u0431\\u0435\\u043b \\u0432 \\u0441\\u043f\\u0438\\u0441\\u043e\\u043a `my_list`\", \"library\": [\"builtins\"]}", "generation": "Scenario 1: \npandas, random, matplotlib, numpy, itertools\n```python\nimport pandas as pd\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport itertools\n\n# Constants\nCATEGORIES = ['Electronics', 'Clothing', 'Books', 'Groceries', 'Sports']\nYEARS = list(range(2015, 2026))\n\ndef sales_report():\n    \"\"\"\n    Generate a sales report for different product categories over the years \n    and visualize it using a bar graph.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with sales data.\n    \n    Requirements:\n    - pandas\n    - random\n    - matplotlib.pyplot\n    - numpy\n    - itertools\n    \n    Example:\n    >>> report = sales_report()\n    >>> print(report)\n    >>> report.plot(kind='bar', stacked=True)\n    \"\"\"\n    sales_data = []\n    for year in YEARS:\n        yearly_sales = [year] + [randint(1000, 5000) for _ in CATEGORIES]\n        sales_data.append(yearly_sales)\n\n    sales_df = pd.DataFrame(sales_data, columns=['Year'] + CATEGORIES)\n\n    return sales_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, matplotlib, scipy\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n# Constants\nSAMPLE_SIZE = 1000\nMU = 0\nSIGMA = 1\n\ndef plot_normal_distribution():\n    \"\"\"\n    Generate a sample from a normal distribution with a given mean and standard deviation, \n    plot its histogram, and fit a normal distribution to the data.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - scipy.stats\n\n    Example:\n    >>> plot_normal_distribution()\n    \"\"\"\n    # Generate a sample\n    sample = np.random.normal(MU, SIGMA, SAMPLE_SIZE)\n\n    # Plot histogram\n    plt.hist(sample, bins=30, density=True, alpha=0.6, color='g')\n\n    # Plot the PDF\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, MU, SIGMA)\n    plt.plot(x, p, 'k', linewidth=2)\n\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, random, datetime, matplotlib\n```python\nimport pandas as pd\nfrom random import randint\nfrom datetime import datetime, timedelta\nimport matplotlib.pyplot as plt\n\n# Constants\nDAYS = 7\n\ndef generate_weekly_temperature_report():\n    \"\"\"\n    Generate a weekly temperature report for a city and visualize using a line graph.\n\n    Returns:\n    DataFrame: A pandas DataFrame with temperature data.\n\n    Requirements:\n    - pandas\n    - random\n    - datetime\n    - matplotlib.pyplot\n\n    Example:\n    >>> report = generate_weekly_temperature_report()\n    >>> print(report)\n    >>> report.plot(kind='line', x='Date', y='Temperature')\n    \"\"\"\n    temperature_data = []\n    for i in range(DAYS):\n        date = datetime.now() - timedelta(days=i)\n        temperature = randint(-10, 30)  # Temperature range from -10 to 30 degrees Celsius\n        temperature_data.append([date, temperature])\n\n    temperature_df = pd.DataFrame(temperature_data, columns=['Date', 'Temperature'])\n\n    return temperature_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, pandas, random, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nfrom random import choice\nimport matplotlib.pyplot as plt\n\n# Constants\nPRODUCTS = ['Apple', 'Banana', 'Carrot', 'Donut', 'Egg', 'Fish', 'Grape', 'Honey', 'Ice-cream', 'Juice']\nDAYS = 30\n\ndef sales_trend():\n    \"\"\"\n    Generate a sales trend for different products over a month and visualize using a line graph.\n\n    Returns:\n    DataFrame: A pandas DataFrame with sales trend data.\n\n    Requirements:\n    - numpy\n    - pandas\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> trend = sales_trend()\n    >>> print(trend)\n    >>> trend.plot(kind='line', x='Day', y='Sales')\n    \"\"\"\n    sales_data = []\n    for day in range(1, DAYS+1):\n        product = choice(PRODUCTS)\n        sales = np.random.poisson(lam=100)  # Poisson distributed sales number\n        sales_data.append([day, product, sales])\n\n    sales_df = pd.DataFrame(sales_data, columns=['Day', 'Product', 'Sales'])\n\n    return sales_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, scipy, matplotlib\n```python\nimport numpy as np\nfrom scipy.fft import fft\nimport matplotlib.pyplot as plt\n\n# Constants\nSAMPLE_RATE = 1000  # Hertz\nDURATION = 1.0  # In seconds\n\ndef plot_frequency_spectrum():\n    \"\"\"\n    Generate a signal with two frequencies, perform a Fast Fourier Transform \n    and plot the frequency spectrum.\n\n    Requirements:\n    - numpy\n    - scipy.fft\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_frequency_spectrum()\n    \"\"\"\n    # Time array\n    t = np.arange(0, DURATION, 1/SAMPLE_RATE)\n\n    # Signal with two frequencies\n    f = np.sin(50.0 * 2.0*np.pi*t) + 0.5*np.sin(80.0 * 2.0*np.pi*t)\n\n    yf = fft(f)\n    xf = np.linspace(0.0, 1.0/(2.0*(1/SAMPLE_RATE)), SAMPLE_RATE//2)\n\n    plt.plot(xf, 2.0/SAMPLE_RATE * np.abs(yf[0:SAMPLE_RATE//2]))\n    plt.grid()\n    plt.show()\n```\n"}
{"seed": "{\"task_id\": 515513, \"prompt\": \"def f_515513(template_path, output_file_path):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"#python-docx-template\\n\\n\\tdoc = DocxTemplate(template_path)\\n\\n\\tdata = { 'var_name' : \\\"HELLO WORLD!\\\" }\\n\\n\\tdoc.render(data)\\n\\n\\tdoc.save(output_file_path)\", \"test_start\": \"\\nimport os\\nimport aspose.words as aw\\nfrom docxtpl import DocxTemplate\\n\\ndef check(candidate):\", \"test\": [\"\\n    # create document object\\n    doc = aw.Document()\\n\\n    # create a document builder object\\n    builder = aw.DocumentBuilder(doc)\\n\\n    # add text to the document\\n    builder.write(\\\"Hello world!\\\")\\n\\n    # save document\\n    doc.save(\\\"out.docx\\\")\\n    candidate(\\\"out.docx\\\", \\\"new.docx\\\")\\n    assert os.path.exists(\\\"new.docx\\\")\\n\"], \"entry_point\": \"f_515513\", \"intent\": \"\\u0421\\u0433\\u043d\\u0435\\u043d\\u0438\\u0440\\u043e\\u0432\\u0430\\u0442\\u044c word \\u0434\\u043e\\u043a\\u0443\\u043c\\u0435\\u043d\\u0442 `output_file_path` \\u043f\\u043e \\u0448\\u0430\\u0431\\u043b\\u043e\\u043d\\u0443 \\u0438\\u0437 word \\u0444\\u0430\\u0439\\u043b\\u0430 `template_path` \\u0441 JSON \\u0434\\u0430\\u043d\\u043d\\u044b\\u043c\\u0438 data\", \"library\": [\"aspose\", \"docxtpl\", \"os\"]}", "generation": "Scenario 1:\ndocxtpl, docx2txt, json, os\n```python\nimport os\nfrom docxtpl import DocxTemplate, InlineImage\nfrom docx.shared import Mm\nfrom PIL import Image\nimport io\n\n# Constants\nIMAGE_DIMENSION = 50\n\ndef generate_doc_with_image(template_path, output_file_path, image_path):\n    \"\"\"\n    Generate a word document from a template and embed an image into it.\n\n    Parameters:\n    template_path (str): The path of the template file.\n    output_file_path (str): The path of the output file.\n    image_path (str): The path of the image file.\n\n    Returns:\n    None\n\n    Requirements:\n    - docxtpl\n    - docx.shared\n    - PIL.Image\n\n    Example:\n    >>> generate_doc_with_image('template.docx', 'output.docx', 'image.png')\n    \"\"\"\n    doc = DocxTemplate(template_path)\n\n    img_stream = io.BytesIO()\n    image = Image.open(image_path)\n    image.thumbnail((IMAGE_DIMENSION, IMAGE_DIMENSION))\n    image.save(img_stream, format='PNG')\n    img_stream.seek(0)\n\n    context = {\n        'var_name': InlineImage(doc, img_stream, width=Mm(20))\n    }\n\n    doc.render(context)\n    doc.save(output_file_path)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\njson, docxtpl, os, datetime\n```python\nimport os\nimport json\nfrom docxtpl import DocxTemplate\nfrom datetime import datetime\n\ndef generate_doc_from_json(template_path, output_file_path, json_data):\n    \"\"\"\n    Generate a word document from a template using data from a JSON.\n\n    Parameters:\n    template_path (str): The path of the template file.\n    output_file_path (str): The path of the output file.\n    json_data (str): The JSON data in string format.\n\n    Returns:\n    None\n\n    Requirements:\n    - docxtpl\n    - json\n    - os\n    - datetime\n\n    Example:\n    >>> generate_doc_from_json('template.docx', 'output.docx', '{ \"var_name\": \"Hello world!\" }')\n    \"\"\"\n    doc = DocxTemplate(template_path)\n    data = json.loads(json_data)\n\n    data['date'] = datetime.today().strftime('%Y-%m-%d')\n\n    doc.render(data)\n    doc.save(output_file_path)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ndocxtpl, os, pandas, matplotlib, io\n```python\nimport os\nfrom docxtpl import DocxTemplate, InlineImage\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import BytesIO\n\ndef generate_doc_with_chart(template_path, output_file_path, data):\n    \"\"\"\n    Generate a word document from a template and embed a chart into it.\n\n    Parameters:\n    template_path (str): The path of the template file.\n    output_file_path (str): The path of the output file.\n    data (pd.DataFrame): The data for the chart.\n\n    Returns:\n    None\n\n    Requirements:\n    - docxtpl\n    - pandas\n    - matplotlib.pyplot\n    - io.BytesIO\n\n    Example:\n    >>> data = pd.DataFrame({'x': range(10), 'y': range(10)})\n    >>> generate_doc_with_chart('template.docx', 'output.docx', data)\n    \"\"\"\n    doc = DocxTemplate(template_path)\n\n    fig, ax = plt.subplots()\n    data.plot(ax=ax)\n    fig.savefig('chart.png')\n\n    context = {\n        'var_name': InlineImage(doc, 'chart.png')\n    }\n\n    doc.render(context)\n    doc.save(output_file_path)\n\n    os.remove('chart.png')\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ndocxtpl, os, pandas, csv\n```python\nimport os\nfrom docxtpl import DocxTemplate\nimport pandas as pd\nimport csv\n\ndef generate_doc_with_table(template_path, output_file_path, csv_file_path):\n    \"\"\"\n    Generate a word document from a template and embed a table into it.\n\n    Parameters:\n    template_path (str): The path of the template file.\n    output_file_path (str): The path of the output file.\n    csv_file_path (str): The path of the CSV file.\n\n    Returns:\n    None\n\n    Requirements:\n    - docxtpl\n    - pandas\n    - csv\n    - os\n\n    Example:\n    >>> generate_doc_with_table('template.docx', 'output.docx', 'data.csv')\n    \"\"\"\n    doc = DocxTemplate(template_path)\n\n    data = pd.read_csv(csv_file_path)\n    table_data = data.to_dict('records')\n\n    context = {\n        'var_name': table_data\n    }\n\n    doc.render(context)\n    doc.save(output_file_path)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\ndocxtpl, os, docx2txt\n```python\nimport os\nfrom docxtpl import DocxTemplate\nimport docx2txt\n\ndef generate_doc_from_text(template_path, output_file_path, text_file_path):\n    \"\"\"\n    Generate a word document from a template and embed a text from a text file into it.\n\n    Parameters:\n    template_path (str): The path of the template file.\n    output_file_path (str): The path of the output file.\n    text_file_path (str): The path of the text file.\n\n    Returns:\n    None\n\n    Requirements:\n    - docxtpl\n    - docx2txt\n    - os\n\n    Example:\n    >>> generate_doc_from_text('template.docx', 'output.docx', 'text.txt')\n    \"\"\"\n    doc = DocxTemplate(template_path)\n\n    text = docx2txt.process(text_file_path)\n\n    context = {\n        'var_name': text\n    }\n\n    doc.render(context)\n    doc.save(output_file_path)\n```"}
{"seed": "{\"task_id\": 524081, \"prompt\": \"def f_524081(my_list):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"not my_list\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([]) == True\\n\", \"\\n    assert candidate(['rehg tg', 325]) == False\\n\"], \"entry_point\": \"f_524081\", \"intent\": \"\\u041f\\u0440\\u043e\\u0432\\u0435\\u0440\\u0438\\u0442\\u044c \\u043f\\u0443\\u0441\\u0442\\u043e\\u0439 \\u043b\\u0438 \\u0441\\u043f\\u0438\\u0441\\u043e\\u043a `my_list`\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, matplotlib, random\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import choice\n\n# Constants\nFRUITS = ['Apple', 'Banana', 'Orange', 'Grapes', 'Cherry']\n\ndef plot_fruit_distribution(fruit_list):\n    \"\"\"\n    Check if the given list is empty and if not, generate a bar plot showing the \n    distribution of different fruits in the list.\n\n    Parameters:\n    fruit_list (list): The list of fruits.\n\n    Returns:\n    bool: True if list is empty, False otherwise.\n    plt: The matplotlib plot object\n\n    Requirements:\n    - pandas\n    - numpy\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> fruit_list = [choice(FRUITS) for _ in range(1000)]\n    >>> is_empty, plot = plot_fruit_distribution(fruit_list)\n    >>> print(is_empty)\n    False\n    >>> plt.show()\n    \"\"\"\n    if not fruit_list:\n        return True, None\n\n    fruit_series = pd.Series(fruit_list)\n    fruit_counts = fruit_series.value_counts()\n\n    plot = fruit_counts.plot(kind='bar', color='skyblue')\n    plt.title('Fruit Distribution')\n    plt.xlabel('Fruit')\n    plt.ylabel('Count')\n\n    return False, plot\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, collections, matplotlib\n```python\nimport numpy as np\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef check_and_plot_word_distribution(text):\n    \"\"\"\n    Check if a given string is empty and if not, plot a histogram of the most common words in the text.\n\n    Parameters:\n    text (str): The text to check and analyze.\n\n    Returns:\n    bool: True if string is empty, False otherwise.\n    plt: The matplotlib plot object\n\n    Requirements:\n    - numpy\n    - collections\n    - matplotlib.pyplot\n\n    Example:\n    >>> text = 'Python is a great language. Python is easy to learn. Python is used for data analysis.'\n    >>> is_empty, plot = check_and_plot_word_distribution(text)\n    >>> print(is_empty)\n    False\n    >>> plt.show()\n    \"\"\"\n    if not text:\n        return True, None\n\n    words = text.split()\n    word_counts = Counter(words)\n\n    most_common = dict(word_counts.most_common(10))\n    labels, values = zip(*most_common.items())\n\n    plt.bar(labels, values)\n    plt.xlabel('Words')\n    plt.ylabel('Count')\n    plt.title('Word Distribution')\n\n    return False, plt\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, sklearn, numpy, seaborn\n```python\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\nimport seaborn as sns\n\n# Constants\nFEATURES = ['Feature1', 'Feature2', 'Feature3', 'Feature4', 'Feature5']\n\ndef standardize_and_plot(data):\n    \"\"\"\n    Check if the dataframe is empty and if not, standardize the features and plot a \n    correlation heatmap of the features.\n\n    Parameters:\n    data (DataFrame): The dataframe to check and analyze.\n\n    Returns:\n    bool: True if DataFrame is empty, False otherwise.\n    DataFrame: The standardized DataFrame.\n    plt: The seaborn heatmap object.\n\n    Requirements:\n    - pandas\n    - sklearn.preprocessing\n    - numpy\n    - seaborn\n\n    Example:\n    >>> data = pd.DataFrame(np.random.rand(100, 5), columns=FEATURES)\n    >>> is_empty, standardized_data, plot = standardize_and_plot(data)\n    >>> print(is_empty)\n    False\n    >>> sns.show()\n    \"\"\"\n    if data.empty:\n        return True, None, None\n\n    scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(data)\n    standardized_df = pd.DataFrame(standardized_data, columns=FEATURES)\n\n    plt = sns.heatmap(standardized_df.corr(), annot=True, cmap='coolwarm')\n\n    return False, standardized_df, plt\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, os, matplotlib\n```python\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\n\ndef check_and_plot_sales(file_path):\n    \"\"\"\n    Check if a CSV file exists and if not, read the CSV file into a DataFrame and plot a line graph \n    showing the sales over time.\n\n    Parameters:\n    file_path (str): The path to the CSV file.\n\n    Returns:\n    bool: True if file does not exist, False otherwise.\n    DataFrame: The DataFrame with sales data.\n    plt: The matplotlib line plot object.\n\n    Requirements:\n    - pandas\n    - os\n    - matplotlib.pyplot\n\n    Example:\n    >>> file_path = 'sales_data.csv'\n    >>> file_exists, df, plot = check_and_plot_sales(file_path)\n    >>> print(file_exists)\n    False\n    >>> plt.show()\n    \"\"\"\n    if not os.path.exists(file_path):\n        return True, None, None\n\n    df = pd.read_csv(file_path)\n    df['date'] = pd.to_datetime(df['date'])\n    df.set_index('date', inplace=True)\n\n    plt = df['sales'].plot()\n    plt.set_title('Sales Over Time')\n    plt.set_ylabel('Sales')\n\n    return False, df, plt\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nrequests, json, pandas, matplotlib\n```python\nimport requests\nimport json\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nAPI_URL = 'https://api.github.com/users/{username}/repos'\n\ndef plot_repo_activity(username):\n    \"\"\"\n    Check if a GitHub user exists and if not, fetch their repositories and plot a histogram of \n    the repository sizes.\n\n    Parameters:\n    username (str): The GitHub username.\n\n    Returns:\n    bool: True if user does not exist, False otherwise.\n    DataFrame: The DataFrame with repository data.\n    plt: The matplotlib histogram object.\n\n    Requirements:\n    - requests\n    - json\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> username = 'octocat'\n    >>> user_exists, df, plot = plot_repo_activity(username)\n    >>> print(user_exists)\n    False\n    >>> plt.show()\n    \"\"\"\n    response = requests.get(API_URL.format(username=username))\n\n    if response.status_code == 404:\n        return True, None, None\n\n    repos = json.loads(response.text)\n    df = pd.DataFrame(repos)\n\n    df = df[df['size'] != 0]\n    plt = df['size'].plot(kind='hist', bins=30, color='skyblue')\n    plt.set_xlabel('Repository Size')\n    plt.set_ylabel('Count')\n    plt.set_title(f'Repository Sizes for {username}')\n\n    return False, df, plt\n```"}
{"seed": "{\"task_id\": 524081, \"prompt\": \"def f_524081(my_list):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"not my_list\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([]) == True\\n\", \"\\n    assert candidate(['rehg tg', 325]) == False\\n\"], \"entry_point\": \"f_524081\", \"intent\": \"\\u041f\\u0440\\u043e\\u0432\\u0435\\u0440\\u0438\\u0442\\u044c \\u043f\\u0443\\u0441\\u0442\\u043e\\u0439 \\u043b\\u0438 \\u0441\\u043f\\u0438\\u0441\\u043e\\u043a `my_list`\", \"library\": []}", "generation": "Scenario 1:\nnumpy, pandas, matplotlib, random\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import choice\n\n# Constants\nCATEGORIES = ['Fruit', 'Vegetable', 'Grain', 'Protein', 'Dairy']\n\ndef generate_nutrition_report(food_list):\n    \"\"\"\n    Generate a pie chart of the nutritional content of a food list based on \n    their categories (Fruit, Vegetable, Grain, Protein, Dairy).\n\n    Parameters:\n    food_list (list): The list of food items.\n\n    Returns:\n    None. It displays a pie chart\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> food_list = ['Apple', 'Banana', 'Carrot', 'Rice', 'Chicken', 'Milk']\n    >>> generate_nutrition_report(food_list)\n    \"\"\"\n    if not food_list:\n        print('The list is empty.')\n        return\n\n    food_categories = [choice(CATEGORIES) for _ in range(len(food_list))]\n    food_df = pd.DataFrame({'Food Item': food_list, 'Category': food_categories})\n    category_counts = food_df['Category'].value_counts()\n\n    plt.pie(category_counts, labels=category_counts.index, autopct='%1.1f%%')\n    plt.title('Nutritional Content')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, json, glob, random\n```python\nimport os\nimport json\nimport glob\nfrom random import choice\n\n# Constants\nDIR_PATH = './json_files/'\n\ndef pick_random_json_file(dir_path=DIR_PATH):\n    \"\"\"\n    Pick a random JSON file from a directory and return its contents.\n\n    Parameters:\n    dir_path (str): The directory path. Default is './json_files/'.\n\n    Returns:\n    dict: The contents of the picked JSON file.\n\n    Requirements:\n    - os\n    - json\n    - glob\n    - random\n\n    Example:\n    >>> picked_file_contents = pick_random_json_file()\n    >>> print(picked_file_contents)\n    \"\"\"\n    if not os.path.exists(dir_path):\n        print(f'The directory {dir_path} does not exist.')\n        return\n\n    json_files = glob.glob(os.path.join(dir_path, '*.json'))\n\n    if not json_files:\n        print(f'There are no JSON files in {dir_path}.')\n        return\n\n    picked_file = choice(json_files)\n    with open(picked_file, 'r') as file:\n        file_contents = json.load(file)\n    \n    return file_contents\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, seaborn, sklearn, numpy\n```python\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\n\n# Constants\nFEATURES = ['Age', 'Salary', 'Experience']\n\ndef visualize_data_distribution(dataframe):\n    \"\"\"\n    Visualize the data distribution of each feature in the dataframe \n    after performing standard scaling on the data.\n\n    Parameters:\n    dataframe (DataFrame): The pandas DataFrame.\n\n    Returns:\n    None. It displays histograms and box plots.\n\n    Requirements:\n    - pandas\n    - seaborn\n    - sklearn.preprocessing.StandardScaler\n    - numpy\n\n    Example:\n    >>> df = pd.DataFrame({'Age': np.random.randint(20, 60, 100), 'Salary': np.random.randint(30000, 80000, 100), \n    >>> 'Experience': np.random.randint(1, 20, 100)})\n    >>> visualize_data_distribution(df)\n    \"\"\"\n    if dataframe.empty:\n        print('The dataframe is empty.')\n        return\n\n    scaler = StandardScaler()\n    scaled_dataframe = pd.DataFrame(scaler.fit_transform(dataframe[FEATURES]), columns=FEATURES)\n\n    for feature in FEATURES:\n        sns.histplot(scaled_dataframe[feature], kde=True)\n        plt.show()\n        sns.boxplot(x=scaled_dataframe[feature])\n        plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nrandom, time, datetime\n```python\nfrom random import randint\nimport time\nfrom datetime import datetime, timedelta\n\n# Constants\nEVENTS = ['Login', 'Logout', 'Purchase', 'Add to Cart', 'View Product']\n\ndef simulate_user_activity(user_id, start_time, num_events):\n    \"\"\"\n    Simulate user activity on a website by generating a sequence of events \n    with timestamps for a given user.\n\n    Parameters:\n    user_id (str): The user ID.\n    start_time (datetime): The start time of the activity.\n    num_events (int): The number of events to generate.\n\n    Returns:\n    list: A list of tuples, each representing an event.\n\n    Requirements:\n    - random\n    - time\n    - datetime\n\n    Example:\n    >>> user_id = 'user123'\n    >>> start_time = datetime.now()\n    >>> num_events = 5\n    >>> activity = simulate_user_activity(user_id, start_time, num_events)\n    >>> print(activity)\n    \"\"\"\n    if num_events <= 0:\n        print('The number of events should be greater than 0.')\n        return\n\n    activity = []\n    current_time = start_time\n\n    for _ in range(num_events):\n        event = choice(EVENTS)\n        activity.append((user_id, current_time, event))\n        sleep_time = randint(5, 60)  # Random sleep time between 5 and 60 seconds\n        time.sleep(sleep_time)\n        current_time += timedelta(seconds=sleep_time)\n\n    return activity\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, pandas, matplotlib.pyplot, sklearn.metrics\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, auc\n\ndef plot_roc_curve(y_true, y_score):\n    \"\"\"\n    Plot the ROC curve given the true labels and the predicted scores.\n\n    Parameters:\n    y_true (np.array): The true labels.\n    y_score (np.array): The predicted scores.\n\n    Returns:\n    None. It displays the ROC curve.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - sklearn.metrics.roc_curve, sklearn.metrics.auc\n\n    Example:\n    >>> y_true = np.array([0, 0, 1, 1])\n    >>> y_score = np.array([0.1, 0.4, 0.35, 0.8])\n    >>> plot_roc_curve(y_true, y_score)\n    \"\"\"\n    if len(y_true) != len(y_score):\n        print('The lengths of the true labels and the predicted scores should be the same.')\n        return\n\n    fpr, tpr, _ = roc_curve(y_true, y_score)\n    roc_auc = auc(fpr, tpr)\n    \n    plt.figure()\n    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n```"}
{"seed": "{\"task_id\": 766573, \"prompt\": \"def f_766573(num):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"for i in range(1, num):\\n\\n\\t\\tprint(*range(i, i*num+1, i), sep='\\t')\\n\", \"test_start\": \"\\nimport sys\\n\\ndef check(candidate):\", \"test\": [\"\\n    f = open('output.txt', 'w')\\n    old = sys.stdout\\n    sys.stdout = f\\n    candidate(5)\\n    sys.stdout = old\\n    f.close()\\n    with open ('output.txt', 'r') as f1:\\n        lines = f1.readlines()\\n        assert int(lines[-1].split()[-1]) == 20\\n\"], \"entry_point\": \"f_766573\", \"intent\": \"\\u0421\\u0433\\u0435\\u043d\\u0435\\u0440\\u0438\\u0440\\u043e\\u0432\\u0430\\u0442\\u044c \\u0442\\u0430\\u0431\\u043b\\u0438\\u0446\\u0443 \\u0443\\u043c\\u043d\\u043e\\u0436\\u0435\\u043d\\u0438\\u044f \\u0434\\u043e \\u0447\\u0438\\u0441\\u043b\\u0430 `num`\", \"library\": [\"sys\"]}", "generation": "Scenario 1:\npandas, numpy, matplotlib.pyplot, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nNUM_ROWS = 10\nNUM_COLUMNS = 10\n\ndef generate_heatmap(num):\n    \"\"\"\n    Generate a heatmap of a multiplication table of size num x num using Seaborn and Matplotlib.\n\n    Parameters:\n    num (int): The size of the multiplication table.\n\n    Returns:\n    seaborn.heatmap: A heatmap of the multiplication table.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> generate_heatmap(10)\n    \"\"\"\n    # Generate the multiplication table\n    data = np.array([i*j for i in range(1, num+1) for j in range(1, num+1)])\n    data = data.reshape(num, num)\n    df = pd.DataFrame(data)\n\n    # Generate the heatmap\n    plt.figure(figsize=(10, 10))\n    sns.heatmap(df, annot=True, fmt=\".1f\", linewidths=.5, square = True, cmap = 'Blues_r');\n    plt.ylabel('Index');\n    plt.xlabel('Index');\n    plt.title('Multiplication table heatmap', size = 15);\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, math, matplotlib.pyplot, random\n```python\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nRADIUS = 5\n\ndef plot_circles(num):\n    \"\"\"\n    Plot num number of circles with random centers and radius RADIUS using matplotlib.\n    \n    Parameters:\n    num (int): The number of circles to plot.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - math\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> plot_circles(5)\n    \"\"\"\n    # Create the figure\n    fig, ax = plt.subplots()\n\n    # Generate and plot circles\n    for _ in range(num):\n        center = (randint(-10, 10), randint(-10, 10))\n        circle = plt.Circle(center, RADIUS, fill = False)\n        ax.add_artist(circle)\n\n    # Set limits and show plot\n    ax.set_xlim([-20, 20])\n    ax.set_ylim([-20, 20])\n    plt.gca().set_aspect('equal', adjustable='box')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, matplotlib.pyplot, random\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\n\ndef plot_random_walk(num):\n    \"\"\"\n    Plot a random walk of num steps using matplotlib.\n\n    Parameters:\n    num (int): The number of steps in the random walk.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> plot_random_walk(1000)\n    \"\"\"\n    # Initialize arrays for the walk\n    x = np.zeros(num)\n    y = np.zeros(num)\n\n    # Generate the random walk\n    for i in range(1, num):\n        val = randint(1, 4)\n        if val == 1:\n            x[i] = x[i - 1] + 1\n            y[i] = y[i - 1]\n        elif val == 2:\n            x[i] = x[i - 1] - 1\n            y[i] = y[i - 1]\n        elif val == 3:\n            x[i] = x[i - 1]\n            y[i] = y[i - 1] + 1\n        else:\n            x[i] = x[i - 1]\n            y[i] = y[i - 1] - 1\n\n    # Plot the random walk\n    plt.title(\"Random Walk ($n = \" + str(num) + \"$ steps)\")\n    plt.plot(x, y)\n    plt.savefig(\"rand_walk\"+str(num)+\".png\",bbox_inches=\"tight\",dpi=600)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, matplotlib.pyplot\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nNUM_SAMPLES = 1000\n\ndef plot_normal_distribution(num):\n    \"\"\"\n    Generate num samples from a normal distribution and plot a histogram using matplotlib.\n\n    Parameters:\n    num (int): The number of samples to generate.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_normal_distribution(1000)\n    \"\"\"\n    # Generate samples from a normal distribution\n    samples = np.random.normal(size=num)\n\n    # Plot the histogram\n    plt.hist(samples, bins=30, density=True, alpha=0.5,\n             histtype='stepfilled', color='steelblue',\n             edgecolor='none');\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, matplotlib.pyplot, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nNUM_SAMPLES = 1000\n\ndef plot_scatter(num):\n    \"\"\"\n    Generate num samples from two normal distributions and plot a scatter plot using seaborn.\n\n    Parameters:\n    num (int): The number of samples to generate.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> plot_scatter(1000)\n    \"\"\"\n    # Generate samples from two normal distributions\n    x = np.random.normal(size=num)\n    y = np.random.normal(size=num)\n\n    # Create a DataFrame\n    df = pd.DataFrame({'x': x, 'y': y})\n\n    # Plot the scatter plot\n    sns.jointplot(x='x', y='y', data=df);\n    plt.show()\n```"}
{"seed": "{\"task_id\": 724582, \"prompt\": \"def f_724582(file_path, url, data):\\n\\t\", \"suffix\": \"\\n\\treturn r\", \"canonical_solution\": \"files = {\\n\\n\\t    'file': open(file_path, 'rb')\\n\\t}\\n\\t\\n\\tr = requests.post(url, data=data, files=files)\", \"test_start\": \"\\nimport requests\\nfrom unittest.mock import Mock\\n\\ndef check(candidate):\", \"test\": [\"\\n    r = requests.Response()\\n    r.status_code = 200\\n    requests.post = Mock(return_value = r)\\n    file_path = 'a.txt'\\n    with open (file_path, 'w') as f:\\n        f.write('abc')\\n    assert candidate(file_path, 'https://def.xyz', {'key':'value'}).status_code == 200\\n\"], \"entry_point\": \"f_724582\", \"intent\": \"\\u041e\\u0442\\u043f\\u0440\\u0430\\u0432\\u0438\\u0442\\u044c POST-\\u0437\\u0430\\u043f\\u0440\\u043e\\u0441 \\u0441 JSON \\u0434\\u0430\\u043d\\u043d\\u044b\\u043c\\u0438 `data` \\u0438 \\u0444\\u0430\\u0439\\u043b\\u043e\\u043c `file_path` \\u043d\\u0430 `url`\", \"library\": [\"requests\"]}", "generation": "Scenario 1:\nrequests, os, json, hashlib\n```python\nimport requests\nimport os\nimport json\nimport hashlib\n\ndef send_post_with_file_hash(file_path, url, data):\n    \"\"\"\n    Send a POST request with JSON data and a file's hash value to a URL. \n    The file's hash (SHA256) is added to the data with key 'file_hash'.\n\n    Parameters:\n    file_path (str): The path to the file.\n    url (str): The URL to send the POST request.\n    data (dict): The JSON data to send.\n\n    Returns:\n    requests.Response: The response from the server.\n\n    Requirements:\n    - requests\n    - os\n    - json\n    - hashlib\n\n    Example:\n    >>> send_post_with_file_hash('test.txt', 'http://httpbin.org/post', {'key':'value'})\n    \"\"\"\n    # Calculate file hash\n    with open(file_path, 'rb') as f:\n        bytes = f.read()\n        file_hash = hashlib.sha256(bytes).hexdigest()\n\n    # Add file hash to data\n    data['file_hash'] = file_hash\n\n    # Send POST request\n    r = requests.post(url, data=json.dumps(data))\n\n    return r\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nrequests, os, json, csv\n```python\nimport requests\nimport os\nimport json\nimport csv\n\ndef send_post_with_csv(file_path, url, data):\n    \"\"\"\n    Send a POST request with JSON data and a CSV file to a URL. \n    The CSV file is sent as a string in the JSON data with key 'csv_data'.\n\n    Parameters:\n    file_path (str): The path to the CSV file.\n    url (str): The URL to send the POST request.\n    data (dict): The JSON data to send.\n\n    Returns:\n    requests.Response: The response from the server.\n\n    Requirements:\n    - requests\n    - os\n    - json\n    - csv\n\n    Example:\n    >>> send_post_with_csv('test.csv', 'http://httpbin.org/post', {'key':'value'})\n    \"\"\"\n    # Read CSV file\n    with open(file_path, 'r') as f:\n        reader = csv.reader(f)\n        csv_data = list(reader)\n\n    # Add CSV data to JSON data\n    data['csv_data'] = csv_data\n\n    # Send POST request\n    r = requests.post(url, data=json.dumps(data))\n\n    return r\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nrequests, os, json, base64\n```python\nimport requests\nimport os\nimport json\nimport base64\n\ndef send_post_with_base64(file_path, url, data):\n    \"\"\"\n    Send a POST request with JSON data and a file's Base64 encoded string to a URL. \n    The file's Base64 string is added to the data with key 'file_base64'.\n\n    Parameters:\n    file_path (str): The path to the file.\n    url (str): The URL to send the POST request.\n    data (dict): The JSON data to send.\n\n    Returns:\n    requests.Response: The response from the server.\n\n    Requirements:\n    - requests\n    - os\n    - json\n    - base64\n\n    Example:\n    >>> send_post_with_base64('test.txt', 'http://httpbin.org/post', {'key':'value'})\n    \"\"\"\n    # Calculate file Base64\n    with open(file_path, 'rb') as f:\n        bytes = f.read()\n        file_base64 = base64.b64encode(bytes).decode('utf-8')\n\n    # Add file Base64 to data\n    data['file_base64'] = file_base64\n\n    # Send POST request\n    r = requests.post(url, data=json.dumps(data))\n\n    return r\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nrequests, os, json, tarfile\n```python\nimport requests\nimport os\nimport json\nimport tarfile\n\ndef send_post_with_tar(file_path, url, data):\n    \"\"\"\n    Send a POST request with JSON data and a tar file to a URL. \n    The tar file is sent as a file in the POST request.\n\n    Parameters:\n    file_path (str): The path to the directory to tar.\n    url (str): The URL to send the POST request.\n    data (dict): The JSON data to send.\n\n    Returns:\n    requests.Response: The response from the server.\n\n    Requirements:\n    - requests\n    - os\n    - json\n    - tarfile\n\n    Example:\n    >>> send_post_with_tar('test', 'http://httpbin.org/post', {'key':'value'})\n    \"\"\"\n    # Create tar file\n    with tarfile.open('temp.tar', 'w') as tar:\n        tar.add(file_path)\n\n    # Send POST request\n    with open('temp.tar', 'rb') as f:\n        files = {'file': f}\n        r = requests.post(url, data=data, files=files)\n\n    # Delete temporary tar file\n    os.remove('temp.tar')\n\n    return r\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nrequests, os, json, zipfile\n```python\nimport requests\nimport os\nimport json\nimport zipfile\n\ndef send_post_with_zip(file_path, url, data):\n    \"\"\"\n    Send a POST request with JSON data and a zip file to a URL. \n    The zip file is sent as a file in the POST request.\n\n    Parameters:\n    file_path (str): The path to the directory to zip.\n    url (str): The URL to send the POST request.\n    data (dict): The JSON data to send.\n\n    Returns:\n    requests.Response: The response from the server.\n\n    Requirements:\n    - requests\n    - os\n    - json\n    - zipfile\n\n    Example:\n    >>> send_post_with_zip('test', 'http://httpbin.org/post', {'key':'value'})\n    \"\"\"\n    # Create zip file\n    with zipfile.ZipFile('temp.zip', 'w', zipfile.ZIP_DEFLATED) as zipf:\n        for root, dirs, files in os.walk(file_path):\n            for file in files:\n                zipf.write(os.path.join(root, file))\n\n    # Send POST request\n    with open('temp.zip', 'rb') as f:\n        files = {'file': f}\n        r = requests.post(url, data=data, files=files)\n\n    # Delete temporary zip file\n    os.remove('temp.zip')\n\n    return r\n```"}
{"seed": "{\"task_id\": 735924, \"prompt\": \"def f_735924(s):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"Counter(s)\", \"test_start\": \"\\nfrom collections import Counter\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('123abccb') == Counter({'1': 1, '2': 1, '3': 1, 'a': 1, 'b': 2, 'c': 2})\\n\", \"\\n    assert candidate('111abccb') == Counter({'1': 3, 'a': 1, 'b': 2, 'c': 2})\\n\"], \"entry_point\": \"f_735924\", \"intent\": \"\\u041f\\u043e\\u0441\\u0447\\u0438\\u0442\\u0430\\u0442\\u044c \\u0441\\u0438\\u043c\\u0432\\u043e\\u043b\\u044b \\u0432 \\u0441\\u0442\\u0440\\u043e\\u043a\\u0435 `s`\", \"library\": [\"collections\"]}", "generation": "Scenario 1: \ncollections, os, hashlib, functools\n```python\nimport os\nfrom collections import Counter\nimport hashlib\nfrom functools import reduce\n\n# Constants\nDIRECTORY_PATH = '/path/to/directory'\n\ndef count_file_hashes(directory_path=DIRECTORY_PATH):\n    \"\"\"\n    Count the number of files with the same hash value in a given directory. \n    It uses the SHA256 hash function.\n    \n    Parameters:\n    directory_path (str): The directory path. Default is DIRECTORY_PATH.\n    \n    Returns:\n    Counter: A Counter object with file hashes as keys and their counts as values.\n    \n    Requirements:\n    - os\n    - collections.Counter\n    - hashlib\n    - functools.reduce\n\n    Terminal-based input-output example:\n    >>> count_file_hashes('/path/to/directory')\n    Counter({'2cf24dba5fb0a30e26e83b2ac5b9e29e1b161e5c1fa7425e73043362938b9824': 2, '6dcd4ce23d88e2ee95838f7b014b6284f4b452e3e4e224a2e045b004f80de62c': 1})\n    \"\"\"\n    counter = Counter()\n    for root, dirs, files in os.walk(directory_path):\n        for file in files:\n            file_path = os.path.join(root, file)\n            with open(file_path, 'rb') as f:\n                content = f.read()\n                hash_value = hashlib.sha256(content).hexdigest()\n                counter[hash_value] += 1\n\n    return counter\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \ncollections, random, itertools, string\n```python\nfrom collections import Counter\nimport random\nimport itertools\nimport string\n\n# Constants\nNUM_SAMPLES = 10000\nCHAR_SET = string.ascii_lowercase + string.digits\n\ndef random_string_distribution(num_samples=NUM_SAMPLES, char_set=CHAR_SET):\n    \"\"\"\n    Generate a number of random strings and compute the distribution of characters.\n    \n    Parameters:\n    num_samples (int): The number of random strings to generate. Default is NUM_SAMPLES.\n    char_set (str): The set of characters to use. Default is CHAR_SET.\n    \n    Returns:\n    Counter: A Counter object with characters as keys and their counts as values.\n    \n    Requirements:\n    - collections.Counter\n    - random\n    - itertools\n    - string\n\n    Terminal-based input-output example:\n    >>> random_string_distribution(10000, string.ascii_lowercase)\n    Counter({'t': 369, 'o': 364, 'n': 362, 'a': 360, 'e': 359, 's': 358, ...})\n    \"\"\"\n    counter = Counter()\n    for _ in range(num_samples):\n        random_string = ''.join(random.choice(char_set) for _ in range(10))\n        counter += Counter(random_string)\n\n    return counter\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \ncollections, re, nltk\n```python\nfrom collections import Counter\nimport re\nimport nltk\n\n# Constants\nTEXT = '''Lorem ipsum dolor sit amet, consectetur adipiscing elit. \n          Mauris lacinia mi sit amet tristique feugiat. \n          Nullam auctor diam ut tortor ultrices, a sodales diam vehicula.'''\n\ndef count_word_frequency(text=TEXT):\n    \"\"\"\n    Count the frequency of each word in a given text.\n    \n    Parameters:\n    text (str): The text to analyze. Default is TEXT.\n    \n    Returns:\n    Counter: A Counter object with words as keys and their counts as values.\n    \n    Requirements:\n    - collections.Counter\n    - re\n    - nltk\n\n    Terminal-based input-output example:\n    >>> count_word_frequency('Lorem ipsum dolor sit amet')\n    Counter({'Lorem': 1, 'ipsum': 1, 'dolor': 1, 'sit': 1, 'amet': 1})\n    \"\"\"\n    words = re.findall(r'\\b\\w+\\b', text)\n    counter = Counter(words)\n\n    return counter\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \ncollections, random, statistics\n```python\nfrom collections import Counter\nimport random\nimport statistics\n\n# Constants\nDICE_ROLLS = 10000\n\ndef analyze_dice_rolls(num_rolls=DICE_ROLLS):\n    \"\"\"\n    Roll a six-sided dice a number of times and analyze the results.\n    \n    Parameters:\n    num_rolls (int): The number of dice rolls. Default is DICE_ROLLS.\n    \n    Returns:\n    tuple: A tuple containing the Counter object with outcomes as keys and their counts as values, mean, and standard deviation.\n    \n    Requirements:\n    - collections.Counter\n    - random\n    - statistics\n\n    Terminal-based input-output example:\n    >>> analyze_dice_rolls(10000)\n    (Counter({1: 1668, 2: 1631, 3: 1672, 4: 1708, 5: 1608, 6: 1713}), 3.5, 1.71)\n    \"\"\"\n    outcomes = [random.randint(1, 6) for _ in range(num_rolls)]\n    counter = Counter(outcomes)\n    mean = statistics.mean(outcomes)\n    stdev = round(statistics.stdev(outcomes), 2)\n\n    return counter, mean, stdev\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \ncollections, string, random\n```python\nfrom collections import Counter\nimport string\nimport random\n\n# Constants\nCHAR_SET = string.ascii_lowercase + string.digits\nSTRING_LENGTH = 10000\n\ndef analyze_random_string(char_set=CHAR_SET, string_length=STRING_LENGTH):\n    \"\"\"\n    Generate a random string with characters from a given set and analyze the character frequencies.\n    \n    Parameters:\n    char_set (str): The set of characters to use. Default is CHAR_SET.\n    string_length (int): The length of the string to generate. Default is STRING_LENGTH.\n    \n    Returns:\n    Counter: A Counter object with characters as keys and their counts as values.\n    \n    Requirements:\n    - collections.Counter\n    - string\n    - random\n\n    Terminal-based input-output example:\n    >>> analyze_random_string(string.ascii_lowercase, 10000)\n    Counter({'a': 382, 'b': 380, 'c': 379, 'd': 378, 'e': 377, ...})\n    \"\"\"\n    random_string = ''.join(random.choice(char_set) for _ in range(string_length))\n    counter = Counter(random_string)\n\n    return counter\n```\n"}
{"seed": "{\"task_id\": 735924, \"prompt\": \"def f_735924(s):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"{c:s.count(c) for c in set(s)}\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('123abccb') == {'1': 1, '2': 1, '3': 1, 'a': 1, 'b': 2, 'c': 2}\\n\", \"\\n    assert candidate('111abaab') == {'1': 3, 'a': 3, 'b': 2}\\n\"], \"entry_point\": \"f_735924\", \"intent\": \"\\u041f\\u043e\\u0441\\u0447\\u0438\\u0442\\u0430\\u0442\\u044c \\u0441\\u0438\\u043c\\u0432\\u043e\\u043b\\u044b \\u0432 \\u0441\\u0442\\u0440\\u043e\\u043a\\u0435 `s`\", \"library\": []}", "generation": "Scenario 1: \ncollections, string, random, re\n```python\nimport collections\nimport string\nimport random\nimport re\n\n# Constants\nSPECIAL_CHARACTERS = string.punctuation\nLETTERS = string.ascii_letters\n\ndef random_string_statistics(length=100, special_chars=False):\n    \"\"\"\n    Generate a random string of a given length. Then calculate and return a dictionary \n    with the frequency of each character in the string. The string can include both \n    letters and special characters, or just letters.\n    \n    Parameters:\n    length (int): The length of the random string to generate. Default is 100.\n    special_chars (bool): Whether to include special characters in the string. Default is False.\n    \n    Returns:\n    dict: A dictionary with each character in the string as the key and the frequency of \n          the character as the value.\n    \n    Requirements:\n    - collections\n    - string\n    - random\n    - re\n    \n    Example:\n    >>> stats = random_string_statistics(200, True)\n    >>> print(stats)\n    \"\"\"\n    if special_chars:\n        chars = LETTERS + SPECIAL_CHARACTERS\n    else:\n        chars = LETTERS\n\n    random_string = ''.join(random.choice(chars) for i in range(length))\n    char_counts = collections.Counter(random_string)\n\n    # Remove special characters from the count if they were not included in the string\n    if not special_chars:\n        char_counts = {k: v for k, v in char_counts.items() if not re.match(r'\\W', k)}\n\n    return char_counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncollections, re, string, itertools\n```python\nimport collections\nimport re\nimport string\nimport itertools\n\n# Constants\nPUNCTUATION = string.punctuation\n\ndef word_frequency_in_string(s, ignore_case=True, ignore_punctuation=True):\n    \"\"\"\n    Calculate the frequency of each word in a string. The function can be set to ignore case \n    and/or ignore punctuation.\n    \n    Parameters:\n    s (str): The string to calculate word frequencies in.\n    ignore_case (bool): Whether to ignore case when calculating word frequencies. Default is True.\n    ignore_punctuation (bool): Whether to ignore punctuation when calculating word frequencies. Default is True.\n    \n    Returns:\n    dict: A dictionary with each word in the string as the key and the frequency of the word as the value.\n    \n    Requirements:\n    - collections\n    - re\n    - string\n    - itertools\n    \n    Example:\n    >>> freq = word_frequency_in_string('Hello, world! This is a test. Hello, world!', ignore_case=True, ignore_punctuation=True)\n    >>> print(freq)\n    \"\"\"\n    if ignore_case:\n        s = s.lower()\n\n    if ignore_punctuation:\n        s = ''.join(ch for ch in s if ch not in PUNCTUATION)\n\n    words = s.split()\n    word_counts = collections.Counter(words)\n\n    return dict(word_counts)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ncollections, random, numpy, matplotlib.pyplot\n```python\nimport collections\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef random_numbers_statistics(n=1000, min_value=0, max_value=100):\n    \"\"\"\n    Generate n random integers between min_value and max_value. Then calculate and \n    return a dictionary with the frequency of each number. Also, plot a histogram \n    of the number frequencies.\n    \n    Parameters:\n    n (int): The number of random integers to generate. Default is 1000.\n    min_value (int): The minimum possible value of the random integers. Default is 0.\n    max_value (int): The maximum possible value of the random integers. Default is 100.\n    \n    Returns:\n    dict: A dictionary with each number as the key and the frequency of the number as the value.\n    \n    Requirements:\n    - collections\n    - random\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> stats = random_numbers_statistics(2000, 0, 50)\n    >>> print(stats)\n    >>> plt.hist(stats.values(), bins=range(min_value, max_value+1), alpha=0.5, edgecolor='black')\n    \"\"\"\n    random_numbers = [random.randint(min_value, max_value) for _ in range(n)]\n    number_counts = collections.Counter(random_numbers)\n\n    return dict(number_counts)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \ncollections, os, re\n```python\nimport collections\nimport os\nimport re\n\ndef count_file_types_in_directory(dir_path):\n    \"\"\"\n    Count the number of files of each type in a directory. The file type is determined by the file extension.\n\n    Parameters:\n    dir_path (str): The path to the directory.\n\n    Returns:\n    dict: A dictionary with each file type as the key and the number of files of that type as the value.\n\n    Requirements:\n    - collections\n    - os\n    - re\n\n    Example:\n    >>> file_types = count_file_types_in_directory('./my_directory')\n    >>> print(file_types)\n    \"\"\"\n    file_types = [os.path.splitext(file)[-1] for file in os.listdir(dir_path) if os.path.isfile(os.path.join(dir_path, file))]\n    file_type_counts = collections.Counter(file_types)\n\n    return dict(file_type_counts)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\ncollections, string, random\n```python\nimport collections\nimport string\nimport random\n\n# Constants\nLETTERS = string.ascii_letters\n\ndef random_word_statistics(n=1000, word_length=5):\n    \"\"\"\n    Generate n random words of a given length. Then calculate and return a dictionary \n    with the frequency of each word.\n    \n    Parameters:\n    n (int): The number of random words to generate. Default is 1000.\n    word_length (int): The length of the random words to generate. Default is 5.\n    \n    Returns:\n    dict: A dictionary with each word as the key and the frequency of the word as the value.\n    \n    Requirements:\n    - collections\n    - string\n    - random\n    \n    Example:\n    >>> stats = random_word_statistics(2000, 7)\n    >>> print(stats)\n    \"\"\"\n    random_words = [''.join(random.choice(LETTERS) for _ in range(word_length)) for _ in range(n)]\n    word_counts = collections.Counter(random_words)\n\n    return dict(word_counts)\n```\n"}
{"seed": "{\"task_id\": 504426, \"prompt\": \"def f_504426(my_dict, my_key):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"my_dict[my_key]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate({'a': 123, 'b': 'aas'}, 'a') == 123\\n\", \"\\n    assert candidate({'a': 123, 'b': 'aas'}, 'b') == \\\"aas\\\"\\n\"], \"entry_point\": \"f_504426\", \"intent\": \"\\u041f\\u043e\\u043b\\u0443\\u0447\\u0438\\u0442\\u044c \\u0437\\u043d\\u0430\\u0447\\u0435\\u043d\\u0438\\u0435 \\u0441\\u043b\\u043e\\u0432\\u0430\\u0440\\u044f `my_dict` \\u043f\\u043e \\u043a\\u043b\\u044e\\u0447\\u0443 `my_key`\", \"library\": []}", "generation": "Scenario 1: \npandas, matplotlib, seaborn, statistics\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom statistics import mean\n\n# Constants\nCATEGORIES = ['Electronics', 'Fashion', 'Home & Kitchen', 'Sports', 'Books']\n\ndef analyze_sales_data(sales_dict):\n    \"\"\"\n    Analyze sales data from a dictionary and generate a sales report with \n    average sales and a bar plot for each category.\n    \n    Parameters:\n    sales_dict (dict): The sales data dictionary. The keys represent the category \n    and the values are a list of sales amounts.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with average sales for each category.\n    \n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - seaborn\n    - statistics.mean\n    \n    Example:\n    >>> sales_data = {'Electronics': [120, 150, 180], 'Fashion': [90, 100, 80], \n                      'Home & Kitchen': [200, 220, 240], 'Sports': [80, 90, 100], \n                      'Books': [60, 70, 80]}\n    >>> report = analyze_sales_data(sales_data)\n    >>> print(report)\n    \"\"\"\n    average_sales = []\n    for category in CATEGORIES:\n        avg_sale = mean(sales_dict[category])\n        average_sales.append(avg_sale)\n\n    sales_df = pd.DataFrame({'Category': CATEGORIES, 'Average Sales': average_sales})\n\n    plt.figure(figsize=(10, 6))\n    sns.barplot(x='Category', y='Average Sales', data=sales_df)\n    plt.title('Average Sales by Category')\n    plt.show()\n\n    return sales_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, os, csv, json\n```python\nimport numpy as np\nimport os\nimport csv\nimport json\n\n# Constants\nFILE_NAME = 'data.csv'\n\ndef convert_csv_to_json(my_dict, my_key):\n    \"\"\"\n    Convert a given CSV file to JSON format. The CSV file is read from a file named 'data.csv' \n    in the current directory. The JSON object is then filtered based on a given key-value pair.\n\n    Parameters:\n    my_dict (dict): A dictionary to map the CSV column names to JSON keys.\n    my_key (str): The key to filter the JSON object.\n\n    Returns:\n    list: A list of dictionaries representing the filtered JSON objects.\n\n    Requirements:\n    - os\n    - csv\n    - json\n    - numpy\n\n    Example:\n    >>> convert_csv_to_json({'Name': 'name', 'Age': 'age', 'City': 'city'}, 'age')\n    \"\"\"\n    json_data = []\n\n    with open(FILE_NAME, 'r') as f:\n        reader = csv.DictReader(f)\n        for row in reader:\n            json_obj = {my_dict[key]: value for key, value in row.items()}\n            json_data.append(json_obj)\n\n    filtered_json = list(filter(lambda d: my_key in d, json_data))\n\n    return filtered_json\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ncollections, itertools, functools, operator\n```python\nfrom collections import Counter\nfrom itertools import chain\nfrom functools import reduce\nfrom operator import add\n\ndef analyze_word_frequency(my_dict, my_key):\n    \"\"\"\n    Analyze the frequency of words in a dictionary of strings. The function \n    returns the count of each word in the string of a given key.\n\n    Parameters:\n    my_dict (dict): The dictionary of strings. The keys are the names of the strings and \n    the values are the strings.\n    my_key (str): The key of the string to analyze.\n\n    Returns:\n    Counter: A Counter object with the frequency of each word in the string.\n\n    Requirements:\n    - collections.Counter\n    - itertools.chain\n    - functools.reduce\n    - operator.add\n\n    Example:\n    >>> analyze_word_frequency({'text1': 'Hello world', 'text2': 'world is beautiful'}, 'text1')\n    \"\"\"\n    words = my_dict[my_key].split()\n    word_count = Counter(words)\n\n    return word_count\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ndatetime, pytz, time, calendar\n```python\nfrom datetime import datetime\nimport pytz\nimport time\nimport calendar\n\n# Constants\nTIMEZONES = ['America/New_York', 'Europe/London', 'Asia/Tokyo']\n\ndef calculate_time_difference(my_dict, my_key):\n    \"\"\"\n    Calculate the time difference in hours between current UTC time and the time \n    in the timezone of a given city.\n\n    Parameters:\n    my_dict (dict): The dictionary of cities and their timezones.\n    my_key (str): The key of the city to calculate the time difference.\n\n    Returns:\n    int: The time difference in hours.\n\n    Requirements:\n    - datetime\n    - pytz\n    - time\n    - calendar\n\n    Example:\n    >>> city_timezones = {'New York': 'America/New_York', 'London': 'Europe/London', 'Tokyo': 'Asia/Tokyo'}\n    >>> calculate_time_difference(city_timezones, 'New York')\n    \"\"\"\n    utc_now = datetime.now(pytz.timezone('UTC'))\n    city_now = datetime.now(pytz.timezone(my_dict[my_key]))\n\n    time_difference = (city_now - utc_now).total_seconds() / 3600\n\n    return int(time_difference)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, pandas, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef analyze_data(my_dict, my_key):\n    \"\"\"\n    Analyze data from a dictionary of numpy arrays. The function creates a pandas \n    DataFrame from the arrays and plots a histogram of the data for a given key.\n\n    Parameters:\n    my_dict (dict): The dictionary of numpy arrays. The keys are the names of the arrays.\n    my_key (str): The key of the array to analyze.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the data of the array.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> data = {'A': np.random.rand(100), 'B': np.random.rand(100), 'C': np.random.rand(100), \n                'D': np.random.rand(100), 'E': np.random.rand(100)}\n    >>> analyze_data(data, 'A')\n    \"\"\"\n    data_df = pd.DataFrame(my_dict, columns=COLUMNS)\n\n    plt.hist(data_df[my_key], bins=10)\n    plt.title('Histogram of ' + my_key)\n    plt.show()\n\n    return data_df\n```\n"}
{"seed": "{\"task_id\": 387249, \"prompt\": \"def f_387249():\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"# -*- coding: utf-8 -*-\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    candidate()\\n    s = \\\"\\u041f\\u0440\\u0438\\u0432\\u0435\\u0442\\\"\\n    assert s == \\\"\\u041f\\u0440\\u0438\\u0432\\u0435\\u0442\\\"\\n\"], \"entry_point\": \"f_387249\", \"intent\": \"\\u0423\\u0441\\u0442\\u0430\\u043d\\u043e\\u0432\\u0438\\u0442\\u044c \\u043a\\u043e\\u0434\\u0438\\u0440\\u043e\\u0432\\u043a\\u0443 utf-8 \\u0434\\u043b\\u044f \\u0444\\u0430\\u0439\\u043b\\u0430\", \"library\": []}", "generation": "Scenario 1: \npandas, os, csv\n```python\nimport pandas as pd\nimport os\nimport csv\n\nCSV_PATH = 'sample_data.csv'  # path to the CSV file\n\ndef read_csv_utf8_encoding(file_path):\n    \"\"\"\n    Read a CSV file with utf-8 encoding and return a pandas DataFrame.\n\n    Parameters:\n    file_path (str): The path to the CSV file.\n\n    Returns:\n    DataFrame: A pandas DataFrame.\n\n    Requirements:\n    - pandas\n    - os\n    - csv\n\n    Example:\n    >>> df = read_csv_utf8_encoding('sample_data.csv')\n    >>> print(df.head())\n    \"\"\"\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"{file_path} does not exist.\")\n\n    with open(file_path, 'r', encoding='utf-8') as f:\n        reader = csv.reader(f)\n        headers = next(reader)\n        data = list(reader)\n\n    df = pd.DataFrame(data, columns=headers)\n    \n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, glob, shutil\n```python\nimport os\nimport glob\nimport shutil\n\nSRC_DIR = 'source_dir'  # source directory\nDEST_DIR = 'dest_dir'  # destination directory\n\ndef move_utf8_files(src_dir, dest_dir):\n    \"\"\"\n    Move all files with utf-8 encoding from a source directory to a destination directory.\n\n    Parameters:\n    src_dir (str): The source directory.\n    dest_dir (str): The destination directory.\n\n    Requirements:\n    - os\n    - glob\n    - shutil\n\n    Example:\n    >>> move_utf8_files('source_dir', 'dest_dir')\n    \"\"\"\n    if not os.path.exists(src_dir):\n        raise FileNotFoundError(f\"{src_dir} does not exist.\")\n    if not os.path.exists(dest_dir):\n        os.makedirs(dest_dir)\n\n    for file_path in glob.glob(os.path.join(src_dir, '*')):\n        try:\n            with open(file_path, 'r', encoding='utf-8') as f:\n                f.read()\n        except UnicodeDecodeError:\n            continue\n\n        shutil.move(file_path, dest_dir)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, csv, json\n```python\nimport os\nimport csv\nimport json\n\nJSON_PATH = 'sample_data.json'  # path to the JSON file\n\ndef convert_csv_to_json_utf8(file_path):\n    \"\"\"\n    Convert a CSV file to a JSON file with utf-8 encoding.\n\n    Parameters:\n    file_path (str): The path to the CSV file.\n\n    Requirements:\n    - os\n    - csv\n    - json\n\n    Example:\n    >>> convert_csv_to_json_utf8('sample_data.csv')\n    \"\"\"\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"{file_path} does not exist.\")\n\n    with open(file_path, 'r', encoding='utf-8') as f:\n        reader = csv.DictReader(f)\n        data = list(reader)\n\n    with open(JSON_PATH, 'w', encoding='utf-8') as f:\n        json.dump(data, f, ensure_ascii=False)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, csv, sqlite3\n```python\nimport os\nimport csv\nimport sqlite3\n\nDB_PATH = 'sample_data.db'  # path to the SQLite database\n\ndef import_csv_to_sqlite_utf8(file_path):\n    \"\"\"\n    Import a CSV file with utf-8 encoding into a SQLite database.\n\n    Parameters:\n    file_path (str): The path to the CSV file.\n\n    Requirements:\n    - os\n    - csv\n    - sqlite3\n\n    Example:\n    >>> import_csv_to_sqlite_utf8('sample_data.csv')\n    \"\"\"\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"{file_path} does not exist.\")\n\n    conn = sqlite3.connect(DB_PATH)\n    cur = conn.cursor()\n\n    with open(file_path, 'r', encoding='utf-8') as f:\n        reader = csv.reader(f)\n        headers = next(reader)\n        cur.execute(f\"CREATE TABLE data ({', '.join(headers)})\")\n        for row in reader:\n            cur.execute(f\"INSERT INTO data VALUES ({', '.join(['?' for _ in headers])})\", row)\n\n    conn.commit()\n    conn.close()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, csv, codecs\n```python\nimport os\nimport csv\nimport codecs\n\ndef count_rows_csv_utf8(file_path):\n    \"\"\"\n    Count the number of rows in a CSV file with utf-8 encoding.\n\n    Parameters:\n    file_path (str): The path to the CSV file.\n\n    Returns:\n    int: The number of rows.\n\n    Requirements:\n    - os\n    - csv\n    - codecs\n\n    Example:\n    >>> rows = count_rows_csv_utf8('sample_data.csv')\n    >>> print(rows)\n    \"\"\"\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"{file_path} does not exist.\")\n\n    with codecs.open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n        reader = csv.reader(f)\n        rows = sum(1 for _ in reader)\n\n    return rows\n```"}
{"seed": "{\"task_id\": 66490, \"prompt\": \"def f_66490(my_list, sep):\\n\\t\", \"suffix\": \"\\n\\treturn s\", \"canonical_solution\": \"s = sep.join(my_list)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(['a','b','c'], '-') == 'a-b-c'\\n\", \"\\n    assert candidate(['aa','bb','cc'], '-') == 'aa-bb-cc'\\n\"], \"entry_point\": \"f_66490\", \"intent\": \"\\u041f\\u0440\\u0435\\u043e\\u0431\\u0440\\u0430\\u0437\\u043e\\u0432\\u0430\\u0442\\u044c \\u0441\\u043f\\u0438\\u0441\\u043e\\u043a `my_list` \\u0432 \\u0441\\u0442\\u0440\\u043e\\u043a\\u0443 `s` c \\u0440\\u0430\\u0437\\u0434\\u0435\\u043b\\u0438\\u0442\\u0435\\u043b\\u0435\\u043c `sep` \", \"library\": []}", "generation": "Scenario 1:\npandas, random, matplotlib\n```python\nimport pandas as pd\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nBOOK_CATEGORIES = ['Fiction', 'Science', 'Mathematics', 'History', 'Arts']\nBOOK_PUBLISHERS = ['Penguin', 'HarperCollins', 'Random House', 'Macmillan', 'Simon & Schuster']\n\ndef generate_book_report(num_books):\n    \"\"\"\n    Generate a random report of books, including their categories and publishers.\n\n    Parameters:\n    num_books (int): The number of books to generate.\n\n    Returns:\n    DataFrame: A pandas DataFrame with book information.\n\n    Requirements:\n    - pandas\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> report = generate_book_report(10)\n    >>> print(report)\n    >>> report['Category'].value_counts().plot(kind='bar')\n    \"\"\"\n    report_data = []\n\n    for _ in range(num_books):\n        category = BOOK_CATEGORIES[randint(0, len(BOOK_CATEGORIES)-1)]\n        publisher = BOOK_PUBLISHERS[randint(0, len(BOOK_PUBLISHERS)-1)]\n        report_data.append([category, publisher])\n\n    report_df = pd.DataFrame(report_data, columns=['Category', 'Publisher'])\n\n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ndatetime, pytz, numpy, pandas\n```python\nfrom datetime import datetime\nimport pytz\nimport numpy as np\nimport pandas as pd\n\n# Constants\nTIMEZONES = ['America/New_York', 'Europe/London', 'Asia/Shanghai', 'Asia/Tokyo', 'Australia/Sydney']\n\ndef generate_timezone_report(date_str):\n    \"\"\"\n    Generate a report of the given date and time in various timezones.\n\n    Parameters:\n    date_str (str): The date string in \"yyyy-mm-dd hh:mm:ss\" format.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the date and time in various timezones.\n\n    Requirements:\n    - datetime\n    - pytz\n    - numpy\n    - pandas\n\n    Example:\n    >>> report = generate_timezone_report('2022-12-31 23:59:59')\n    >>> print(report)\n    \"\"\"\n    utc_time = datetime.strptime(date_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=pytz.UTC)\n    report_data = []\n\n    for tz in TIMEZONES:\n        local_time = utc_time.astimezone(pytz.timezone(tz))\n        report_data.append([tz, local_time])\n\n    report_df = pd.DataFrame(report_data, columns=['Timezone', 'Local Time'])\n\n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, random, matplotlib\n```python\nimport pandas as pd\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nMOVIE_GENRES = ['Action', 'Comedy', 'Drama', 'Horror', 'Documentary']\nMOVIE_RATINGS = list(range(1, 11))\n\ndef generate_movie_report(num_movies):\n    \"\"\"\n    Generate a random report of movies, including their genres and ratings.\n\n    Parameters:\n    num_movies (int): The number of movies to generate.\n\n    Returns:\n    DataFrame: A pandas DataFrame with movie information.\n\n    Requirements:\n    - pandas\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> report = generate_movie_report(10)\n    >>> print(report)\n    >>> report['Rating'].hist(bins=10)\n    \"\"\"\n    report_data = []\n\n    for _ in range(num_movies):\n        genre = MOVIE_GENRES[randint(0, len(MOVIE_GENRES)-1)]\n        rating = MOVIE_RATINGS[randint(0, len(MOVIE_RATINGS)-1)]\n        report_data.append([genre, rating])\n\n    report_df = pd.DataFrame(report_data, columns=['Genre', 'Rating'])\n\n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ndatetime, pytz, pandas, dateutil\n```python\nfrom datetime import datetime\nimport pytz\nimport pandas as pd\nfrom dateutil.parser import parse\n\n# Constants\nDATETIME_FORMAT = \"%Y-%m-%d %H:%M:%S\"\n\ndef utc_to_timezone_report(date_str, to_tz):\n    \"\"\"\n    Convert a given UTC datetime to a specified timezone and create a report.\n\n    Parameters:\n    date_str (str): The date string in \"yyyy-mm-dd hh:mm:ss\" format.\n    to_tz (str): The timezone to which the date should be converted.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the UTC datetime and the converted datetime.\n\n    Requirements:\n    - datetime\n    - pytz\n    - pandas\n    - dateutil.parser\n\n    Example:\n    >>> report = utc_to_timezone_report('2022-12-31 23:59:59', 'America/New_York')\n    >>> print(report)\n    \"\"\"\n    utc_time = parse(date_str).replace(tzinfo=pytz.UTC)\n    local_time = utc_time.astimezone(pytz.timezone(to_tz))\n    report_df = pd.DataFrame({'UTC Time': [utc_time.strftime(DATETIME_FORMAT)], \n                              'Local Time': [local_time.strftime(DATETIME_FORMAT)]})\n\n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, random, matplotlib\n```python\nimport pandas as pd\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nPRODUCT_CATEGORIES = ['Electronics', 'Groceries', 'Clothing', 'Books', 'Furniture']\nPRODUCT_PRICES = list(range(1, 101))\n\ndef generate_product_report(num_products):\n    \"\"\"\n    Generate a random report of products, including their categories and prices.\n\n    Parameters:\n    num_products (int): The number of products to generate.\n\n    Returns:\n    DataFrame: A pandas DataFrame with product information.\n\n    Requirements:\n    - pandas\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> report = generate_product_report(10)\n    >>> print(report)\n    >>> report['Price'].hist(bins=10)\n    \"\"\"\n    report_data = []\n\n    for _ in range(num_products):\n        category = PRODUCT_CATEGORIES[randint(0, len(PRODUCT_CATEGORIES)-1)]\n        price = PRODUCT_PRICES[randint(0, len(PRODUCT_PRICES)-1)]\n        report_data.append([category, price])\n\n    report_df = pd.DataFrame(report_data, columns=['Category', 'Price'])\n\n    return report_df\n```\n"}
{"seed": "{\"task_id\": 66490, \"prompt\": \"def f_66490(my_list, sep):\\n\\t\", \"suffix\": \"\\n\\treturn s\", \"canonical_solution\": \"s = sep.join(map(str, my_list))\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(['a','b','c'], '-') == 'a-b-c'\\n\", \"\\n    assert candidate(['a','b','c'], '.') == 'a.b.c'\\n\"], \"entry_point\": \"f_66490\", \"intent\": \"\\u041f\\u0440\\u0435\\u043e\\u0431\\u0440\\u0430\\u0437\\u043e\\u0432\\u0430\\u0442\\u044c \\u0441\\u043f\\u0438\\u0441\\u043e\\u043a `my_list` \\u0432 \\u0441\\u0442\\u0440\\u043e\\u043a\\u0443 `s` c \\u0440\\u0430\\u0437\\u0434\\u0435\\u043b\\u0438\\u0442\\u0435\\u043b\\u0435\\u043c `sep` \", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, string, random\n```python\nimport pandas as pd\nimport numpy as np\nimport string\nimport random\n\n# Constants\nLETTERS = string.ascii_lowercase\nSEP = '-'\n\ndef create_dataframe(my_list, sep):\n    \"\"\"\n    Convert a list of strings into a pandas DataFrame with random numerical values \n    and the names of the columns as the joined string of the list with a separator.\n    \n    Parameters:\n    my_list (list): A list of strings.\n    sep (str): The separator to join the strings.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with random numerical values.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - string\n    - random\n    \n    Example:\n    >>> my_list = ['a', 'b', 'c']\n    >>> df = create_dataframe(my_list, SEP)\n    >>> print(df)\n    \"\"\"\n    col_names = sep.join(my_list)\n    df = pd.DataFrame(np.random.rand(10, len(my_list)), columns=list(col_names))\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, string, random\n```python\nimport pandas as pd\nimport numpy as np\nimport string\nimport random\n\n# Constants\nLETTERS = string.ascii_lowercase\nSEP = ', '\n\ndef create_dataframe_with_categorical_data(my_list, sep):\n    \"\"\"\n    Convert a list of strings into a pandas DataFrame with random categorical data \n    and the names of the columns as the joined string of the list with a separator.\n    \n    Parameters:\n    my_list (list): A list of strings.\n    sep (str): The separator to join the strings.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with random categorical data.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - string\n    - random\n    \n    Example:\n    >>> my_list = ['x', 'y', 'z']\n    >>> df = create_dataframe_with_categorical_data(my_list, SEP)\n    >>> print(df)\n    \"\"\"\n    col_names = sep.join(my_list)\n    categories = [LETTERS[i] for i in np.random.randint(0, len(LETTERS), size=(10, len(my_list)))]\n    df = pd.DataFrame(categories, columns=list(col_names))\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, string, random\n```python\nimport pandas as pd\nimport numpy as np\nimport string\nimport random\n\n# Constants\nLETTERS = string.ascii_lowercase\nSEP = ', '\n\ndef create_dataframe_with_mixed_data(my_list, sep):\n    \"\"\"\n    Convert a list of strings into a pandas DataFrame with mixed data types (numerical and categorical)\n    and the names of the columns as the joined string of the list with a separator.\n    \n    Parameters:\n    my_list (list): A list of strings.\n    sep (str): The separator to join the strings.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with mixed data types.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - string\n    - random\n    \n    Example:\n    >>> my_list = ['m', 'n', 'o']\n    >>> df = create_dataframe_with_mixed_data(my_list, SEP)\n    >>> print(df)\n    \"\"\"\n    col_names = sep.join(my_list)\n    data = []\n    \n    for _ in range(10):\n        row = []\n        for _ in my_list:\n            if random.choice([True, False]):\n                row.append(random.choice(LETTERS))\n            else:\n                row.append(np.random.rand())\n        data.append(row)\n    \n    df = pd.DataFrame(data, columns=list(col_names))\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, string, random\n```python\nimport pandas as pd\nimport numpy as np\nimport string\nimport random\n\n# Constants\nLETTERS = string.ascii_lowercase\nSEP = ', '\n\ndef create_dataframe_and_plot(my_list, sep):\n    \"\"\"\n    Convert a list of strings into a pandas DataFrame with random numerical values \n    and the names of the columns as the joined string of the list with a separator, \n    then plot the data.\n    \n    Parameters:\n    my_list (list): A list of strings.\n    sep (str): The separator to join the strings.\n    \n    Returns:\n    DataFrame, AxesSubplot: A pandas DataFrame with random numerical values and the plot.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - string\n    - random\n    \n    Example:\n    >>> my_list = ['p', 'q', 'r']\n    >>> df, ax = create_dataframe_and_plot(my_list, SEP)\n    >>> print(df)\n    >>> plt.show()\n    \"\"\"\n    col_names = sep.join(my_list)\n    df = pd.DataFrame(np.random.rand(10, len(my_list)), columns=list(col_names))\n    ax = df.plot()\n\n    return df, ax\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, string, random\n```python\nimport pandas as pd\nimport numpy as np\nimport string\nimport random\n\n# Constants\nLETTERS = string.ascii_lowercase\nSEP = ', '\n\ndef create_dataframe_and_describe(my_list, sep):\n    \"\"\"\n    Convert a list of strings into a pandas DataFrame with random numerical values \n    and the names of the columns as the joined string of the list with a separator, \n    then return the descriptive statistics of the DataFrame.\n    \n    Parameters:\n    my_list (list): A list of strings.\n    sep (str): The separator to join the strings.\n    \n    Returns:\n    DataFrame, Series: A pandas DataFrame with random numerical values and the descriptive statistics.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - string\n    - random\n    \n    Example:\n    >>> my_list = ['s', 't', 'u']\n    >>> df, stats = create_dataframe_and_describe(my_list, SEP)\n    >>> print(df)\n    >>> print(stats)\n    \"\"\"\n    col_names = sep.join(my_list)\n    df = pd.DataFrame(np.random.rand(10, len(my_list)), columns=list(col_names))\n    stats = df.describe()\n\n    return df, stats\n```"}
{"seed": "{\"task_id\": 363427, \"prompt\": \"def f_363427(s):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"re.sub(r'[^\\\\w\\\\s]+|[\\\\d]+', r'',s)\", \"test_start\": \"\\nimport re\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('1a2b34c') == 'abc'\\n\", \"\\n    assert candidate('hello world') == 'hello world'\\n\", \"\\n    assert candidate('1234567890') == ''\\n\"], \"entry_point\": \"f_363427\", \"intent\": \"\\u0423\\u0434\\u0430\\u043b\\u0438\\u0442\\u044c \\u0432\\u0441\\u0435 \\u0446\\u0438\\u0444\\u0440\\u044b \\u0438\\u0437 \\u0441\\u0442\\u0440\\u043e\\u043a\\u0438 `s`\", \"library\": [\"re\"]}", "generation": "Scenario 1:\npandas, re, nltk, collections, matplotlib\n```python\nimport pandas as pd\nimport re\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\n# Constants\nSTOPWORDS = set(stopwords.words('english'))\n\ndef analyze_text(text):\n    \"\"\"\n    Analyze the given text to provide word frequency excluding stopwords and punctuation.\n\n    Parameters:\n    text (str): The text to analyze.\n\n    Returns:\n    DataFrame: A pandas DataFrame with words and their frequencies.\n\n    Requirements:\n    - pandas\n    - re\n    - nltk.corpus\n    - collections\n    - matplotlib.pyplot\n\n    Example:\n    >>> text = \"This is a sample text. It is used for testing the function.\"\n    >>> df = analyze_text(text)\n    >>> print(df)\n    >>> df.plot(kind='bar', x='Word', y='Frequency')\n    \"\"\"\n    # Remove punctuation and convert to lowercase\n    text = re.sub(r'[^\\w\\s]', '', text).lower()\n    words = text.split()\n\n    # Remove stopwords\n    words = [word for word in words if word not in STOPWORDS]\n    \n    # Get word frequency\n    word_freq = Counter(words)\n    \n    df = pd.DataFrame(list(word_freq.items()), columns=['Word', 'Frequency'])\n    df = df.sort_values(by='Frequency', ascending=False)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nre, numpy, scipy.stats, matplotlib\n```python\nimport re\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\n# Constants\nPATTERN = r'\\d+'\n\ndef analyze_number_distribution(text):\n    \"\"\"\n    Analyze the distribution of numbers in the given text and plot their distribution.\n\n    Parameters:\n    text (str): The text to analyze.\n\n    Returns:\n    ndarray: The array of numbers.\n\n    Requirements:\n    - re\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Example:\n    >>> text = \"There are 10 apples and 20 oranges. I ate 3 apples and 4 oranges.\"\n    >>> numbers = analyze_number_distribution(text)\n    >>> print(numbers)\n    >>> plt.hist(numbers, density=True, bins=30)\n    >>> plt.plot(np.linspace(min(numbers), max(numbers)), norm.pdf(np.linspace(min(numbers), max(numbers)), np.mean(numbers), np.std(numbers)))\n    \"\"\"\n    numbers = re.findall(PATTERN, text)\n    numbers = np.array(numbers, dtype=int)\n    \n    return numbers\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nre, os, csv, collections\n```python\nimport re\nimport os\nimport csv\nfrom collections import Counter\n\n# Constants\nPATTERN = r'\\b[A-Z]+\\b'\n\ndef count_acronyms(file_path):\n    \"\"\"\n    Count the acronyms in a text file.\n\n    Parameters:\n    file_path (str): The path to the text file.\n\n    Returns:\n    dict: The dictionary with acronyms and their counts.\n\n    Requirements:\n    - re\n    - os\n    - csv\n    - collections\n\n    Example:\n    >>> count = count_acronyms('sample.txt')\n    >>> print(count)\n    >>> with open('acronym_counts.csv', 'w') as f:\n    >>>     writer = csv.writer(f)\n    >>>     writer.writerows(count.items())\n    \"\"\"\n    with open(file_path, 'r') as f:\n        text = f.read()\n\n    acronyms = re.findall(PATTERN, text)\n    acronym_counts = Counter(acronyms)\n    \n    return dict(acronym_counts)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nre, numpy, sklearn.feature_extraction.text, sklearn.decomposition\n```python\nimport re\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import NMF\n\n# Constants\nPATTERN = r'\\b[A-Za-z]+\\b'\nN_TOPICS = 5\nN_TOP_WORDS = 10\n\ndef extract_topics(texts):\n    \"\"\"\n    Extract topics from a list of texts using non-negative matrix factorization.\n\n    Parameters:\n    texts (list): The list of texts.\n\n    Returns:\n    list: The list of topics.\n\n    Requirements:\n    - re\n    - numpy\n    - sklearn.feature_extraction.text\n    - sklearn.decomposition\n\n    Example:\n    >>> texts = [\"This is a sample text.\", \"This is another sample text.\", \"This is yet another sample text.\"]\n    >>> topics = extract_topics(texts)\n    >>> print(topics)\n    \"\"\"\n    # Remove punctuation and convert to lowercase\n    texts = [re.sub(r'[^\\w\\s]', '', text).lower() for text in texts]\n    \n    vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n    tfidf = vectorizer.fit_transform(texts)\n    \n    nmf = NMF(n_components=N_TOPICS, random_state=1).fit(tfidf)\n    \n    feature_names = vectorizer.get_feature_names_out()\n    \n    topics = []\n    for topic_idx, topic in enumerate(nmf.components_):\n        top_words = [feature_names[i] for i in topic.argsort()[:-N_TOP_WORDS - 1:-1]]\n        topics.append('Topic #%d: ' % topic_idx + ', '.join(top_words))\n\n    return topics\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nre, pandas, seaborn, scipy.stats\n```python\nimport re\nimport pandas as pd\nimport seaborn as sns\nfrom scipy.stats import chi2_contingency\n\n# Constants\nPATTERN = r'\\b[A-Za-z]+\\b'\n\ndef analyze_word_relationships(text1, text2):\n    \"\"\"\n    Analyze the relationships between the words in two texts using chi-square test.\n\n    Parameters:\n    text1 (str): The first text.\n    text2 (str): The second text.\n\n    Returns:\n    DataFrame: The contingency table.\n\n    Requirements:\n    - re\n    - pandas\n    - seaborn\n    - scipy.stats\n\n    Example:\n    >>> text1 = \"This is a sample text.\"\n    >>> text2 = \"This is another sample text.\"\n    >>> df = analyze_word_relationships(text1, text2)\n    >>> print(df)\n    >>> chi2, p, dof, expected = chi2_contingency(df)\n    >>> print('chi2:', chi2, 'p:', p, 'dof:', dof)\n    >>> sns.heatmap(df, annot=True)\n    \"\"\"\n    words1 = re.findall(PATTERN, text1)\n    words2 = re.findall(PATTERN, text2)\n    \n    words = list(set(words1 + words2))\n    \n    data = []\n    for word in words:\n        count1 = words1.count(word)\n        count2 = words2.count(word)\n        data.append([word, count1, count2])\n    \n    df = pd.DataFrame(data, columns=['Word', 'Text1', 'Text2'])\n    \n    return df\n```\n"}
{"seed": "{\"task_id\": 224298, \"prompt\": \"def f_224298(s):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"[ord(c) for c in s]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('example string') == [101, 120, 97, 109, 112, 108, 101, 32, 115, 116, 114, 105, 110, 103]\\n\", \"\\n    assert candidate('hello world!') == [104, 101, 108, 108, 111, 32, 119, 111, 114, 108, 100, 33]\\n\"], \"entry_point\": \"f_224298\", \"intent\": \"\\u041f\\u043e\\u043b\\u0443\\u0447\\u0438\\u0442\\u044c \\u043a\\u043e\\u0434\\u044b \\u0441\\u0438\\u043c\\u0432\\u043e\\u043b\\u043e\\u0432 \\u0441\\u0442\\u0440\\u043e\\u043a\\u0438 `s`\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, matplotlib.pyplot, random\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import choice\n\n# Constants\nLETTERS = list('abcdefghijklmnopqrstuvwxyz')\n\ndef generate_random_letter_frequency(s):\n    \"\"\"\n    Generate a bar plot of letter frequencies in a given string `s`. \n    The function should return a pandas DataFrame with the frequencies.\n    \n    Parameters:\n    s (str): The string for which to calculate letter frequencies.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the letter frequencies.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - random\n    \n    Example:\n    >>> df = generate_random_letter_frequency('example string')\n    >>> df.plot(kind='bar')\n    \"\"\"\n    # Initialize an empty DataFrame\n    df = pd.DataFrame(columns=['Letter', 'Frequency'])\n\n    for letter in LETTERS:\n        df = df.append({'Letter': letter, 'Frequency': s.count(letter)}, ignore_index=True)\n\n    # Add some randomness to the frequencies\n    df['Frequency'] = df['Frequency'].apply(lambda x: x + choice(range(10)))\n\n    # Plot the frequencies\n    df.set_index('Letter')['Frequency'].plot(kind='bar')\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nnumpy, scipy, matplotlib.pyplot\n```python\nimport numpy as np\nfrom scipy.fft import fft\nimport matplotlib.pyplot as plt\n\ndef compute_fft_of_string(s):\n    \"\"\"\n    Compute the Fast Fourier Transform (FFT) of the character codes in the string `s` \n    and plot the absolute values of the FFT coefficients.\n    \n    Parameters:\n    s (str): The string for which to compute the FFT.\n    \n    Returns:\n    numpy.ndarray: The FFT coefficients.\n    \n    Requirements:\n    - numpy\n    - scipy.fft\n    - matplotlib.pyplot\n    \n    Example:\n    >>> coeffs = compute_fft_of_string('example string')\n    >>> plt.plot(np.abs(coeffs))\n    \"\"\"\n    # Convert the string to a list of character codes\n    x = [ord(c) for c in s]\n\n    # Compute the FFT\n    yf = fft(x)\n\n    # Plot the absolute values of the FFT coefficients\n    plt.plot(np.abs(yf))\n\n    return yf\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \ncollections, matplotlib, pandas\n```python\nimport collections\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef count_letter_frequency(s):\n    \"\"\"\n    Count the frequency of each character in the string `s` and plot the result in a bar chart.\n    \n    Parameters:\n    s (str): The string for which to count character frequencies.\n    \n    Returns:\n    dict: A dictionary with the character frequencies.\n    \n    Requirements:\n    - collections\n    - matplotlib.pyplot\n    - pandas\n    \n    Example:\n    >>> freqs = count_letter_frequency('example string')\n    >>> pd.DataFrame.from_dict(freqs, orient='index').plot(kind='bar')\n    \"\"\"\n    # Count the frequency of each character\n    counter = collections.Counter(s)\n\n    # Convert the counter to a DataFrame and plot it\n    pd.DataFrame.from_dict(counter, orient='index').plot(kind='bar')\n\n    return dict(counter)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, matplotlib.pyplot, sklearn.preprocessing\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef scale_and_plot_string_codes(s):\n    \"\"\"\n    Scale the character codes in the string `s` to the range [0, 1] and plot the result.\n    \n    Parameters:\n    s (str): The string for which to scale and plot character codes.\n    \n    Returns:\n    numpy.ndarray: The scaled character codes.\n    \n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.preprocessing.MinMaxScaler\n    \n    Example:\n    >>> scaled_codes = scale_and_plot_string_codes('example string')\n    >>> plt.plot(scaled_codes)\n    \"\"\"\n    # Convert the string to a list of character codes\n    x = np.array([[ord(c)] for c in s])\n\n    # Scale the character codes\n    scaler = MinMaxScaler()\n    scaled_codes = scaler.fit_transform(x)\n\n    # Plot the scaled codes\n    plt.plot(scaled_codes)\n\n    return scaled_codes\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnltk, collections, matplotlib.pyplot\n```python\nimport nltk\nimport collections\nimport matplotlib.pyplot as plt\n\ndef count_word_frequency(s):\n    \"\"\"\n    Count the frequency of each word in the string `s` and plot the result in a bar chart.\n    \n    Parameters:\n    s (str): The string for which to count word frequencies.\n    \n    Returns:\n    dict: A dictionary with the word frequencies.\n    \n    Requirements:\n    - nltk\n    - collections\n    - matplotlib.pyplot\n    \n    Example:\n    >>> freqs = count_word_frequency('example string')\n    >>> plt.bar(freqs.keys(), freqs.values())\n    \"\"\"\n    # Tokenize the string into words\n    words = nltk.word_tokenize(s)\n\n    # Count the frequency of each word\n    counter = collections.Counter(words)\n\n    # Plot the word frequencies\n    plt.bar(counter.keys(), counter.values())\n\n    return dict(counter)\n```\n"}
{"seed": "{\"task_id\": 182910, \"prompt\": \"def f_182910(url):\\n\\t\", \"suffix\": \"\\n\\treturn code\", \"canonical_solution\": \"with urllib.request.urlopen(url) as res:\\n\\t    code = res.read()\", \"test_start\": \"\\nimport urllib\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(\\\"https://www.google.com/\\\").find(b\\\"title\\\")\\n\"], \"entry_point\": \"f_182910\", \"intent\": \"\\u0421\\u043f\\u0430\\u0440\\u0441\\u0438\\u0442\\u044c HTML-\\u043a\\u043e\\u0434 \\u0441\\u0442\\u0440\\u0430\\u043d\\u0438\\u0446\\u044b `code` \\u043f\\u043e \\u0441\\u0441\\u044b\\u043b\\u043a\\u0435 `url`\", \"library\": [\"urllib\"]}", "generation": "Scenario 3: \nrequests, BeautifulSoup, pandas, csv\n```python\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport csv\n\n# Constants\nHEADERS = {'User-Agent': 'Mozilla/5.0'}\n\ndef scrape_and_save_data(url):\n    \"\"\"\n    Scrape data from a webpage and save it to a csv file. The function assumes \n    that the webpage contains at least one table.\n\n    Parameters:\n    url (str): The URL of the webpage to scrape.\n\n    Returns:\n    str: The filename of the saved csv file.\n\n    Requirements:\n    - requests\n    - BeautifulSoup\n    - pandas\n    - csv\n\n    Example:\n    >>> scrape_and_save_data('https://www.example.com/')\n    \"\"\"\n    response = requests.get(url, headers=HEADERS)\n    soup = BeautifulSoup(response.content, 'html.parser')\n    table = soup.find('table')\n\n    data = pd.read_html(str(table))[0]\n\n    filename = url.split('/')[-1] + '.csv'\n    data.to_csv(filename, index=False)\n\n    return filename\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nrequests, json, pandas, os, time\n```python\nimport requests\nimport json\nimport pandas as pd\nimport os\nimport time\n\n# Constants\nAPI_KEY = os.getenv('API_KEY')\n\ndef get_and_save_api_data(url, params):\n    \"\"\"\n    Get data from an API endpoint and save it to a json file. The response from \n    the API is assumed to be in json format.\n\n    Parameters:\n    url (str): The API endpoint.\n    params (dict): The parameters for the API request.\n\n    Returns:\n    str: The filename of the saved json file.\n\n    Requirements:\n    - requests\n    - json\n    - pandas\n    - os\n    - time\n\n    Example:\n    >>> get_and_save_api_data('https://api.example.com/data', {'key': 'value'})\n    \"\"\"\n    params['apikey'] = API_KEY\n    response = requests.get(url, params=params)\n\n    data = response.json()\n\n    filename = 'data_' + str(int(time.time())) + '.json'\n    with open(filename, 'w') as f:\n        json.dump(data, f)\n\n    return filename\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nrequests, BeautifulSoup, re, csv\n```python\nimport requests\nfrom bs4 import BeautifulSoup\nimport re\nimport csv\n\n# Constants\nHEADERS = {'User-Agent': 'Mozilla/5.0'}\n\ndef scrape_and_save_links(url):\n    \"\"\"\n    Scrape all the links from a webpage and save them to a csv file.\n\n    Parameters:\n    url (str): The URL of the webpage to scrape.\n\n    Returns:\n    str: The filename of the saved csv file.\n\n    Requirements:\n    - requests\n    - BeautifulSoup\n    - re\n    - csv\n\n    Example:\n    >>> scrape_and_save_links('https://www.example.com/')\n    \"\"\"\n    response = requests.get(url, headers=HEADERS)\n    soup = BeautifulSoup(response.content, 'html.parser')\n    \n    links = []\n    for a in soup.find_all('a', href=True):\n        if not re.match('^mailto:', a['href']):\n            links.append(a['href'])\n\n    filename = url.split('/')[-1] + '_links.csv'\n    with open(filename, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(['Links'])\n        for link in links:\n            writer.writerow([link])\n\n    return filename\n```\n`GPT_ODEX_BREAK`\n\nScenario 6:\nrequests, bs4, pandas, matplotlib\n```python\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nHEADERS = {'User-Agent': 'Mozilla/5.0'}\n\ndef scrape_and_plot_data(url):\n    \"\"\"\n    Scrape data from a webpage and plot it. The function assumes that the \n    webpage contains at least one table.\n\n    Parameters:\n    url (str): The URL of the webpage to scrape.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the scraped data.\n\n    Requirements:\n    - requests\n    - bs4\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> scrape_and_plot_data('https://www.example.com/')\n    \"\"\"\n    response = requests.get(url, headers=HEADERS)\n    soup = BeautifulSoup(response.content, 'html.parser')\n    table = soup.find('table')\n\n    data = pd.read_html(str(table))[0]\n\n    data.plot(kind='bar')\n\n    return data\n```\n`GPT_ODEX_BREAK`\n\nScenario 7:\nurllib, bs4, csv, os\n```python\nimport urllib.request\nfrom bs4 import BeautifulSoup\nimport csv\nimport os\n\ndef download_and_save_images(url, save_dir):\n    \"\"\"\n    Download all images from a webpage and save them to a directory.\n\n    Parameters:\n    url (str): The URL of the webpage to scrape.\n    save_dir (str): The directory to save the images.\n\n    Returns:\n    int: The number of images downloaded.\n\n    Requirements:\n    - urllib.request\n    - bs4\n    - csv\n    - os\n\n    Example:\n    >>> download_and_save_images('https://www.example.com/', './images')\n    \"\"\"\n    response = urllib.request.urlopen(url)\n    soup = BeautifulSoup(response, 'html.parser')\n    images = soup.find_all('img')\n\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n\n    num_images = 0\n    for img in images:\n        img_url = img.get('src')\n        img_file = os.path.join(save_dir, img_url.split('/')[-1])\n\n        try:\n            urllib.request.urlretrieve(img_url, img_file)\n            num_images += 1\n        except:\n            pass\n\n    return num_images\n```"}
{"seed": "{\"task_id\": 850118, \"prompt\": \"def f_850118(my_obj, My_class):\\n\\t\", \"suffix\": \"\\n\\treturn result\", \"canonical_solution\": \"\\n\\tclass ClassEncoder(json.JSONEncoder):\\n\\t\\tdef default(self, obj):\\n\\t\\t\\tif isinstance(obj, My_class):\\n\\t\\t\\t\\treturn obj.__dict__\\n\\t\\t\\treturn json.JSONEncoder.default(self, obj)\\n\\tresult = json.dumps(my_obj, cls=ClassEncoder)\\n\", \"test_start\": \"\\nimport json\\n\\ndef check(candidate):\", \"test\": [\"\\n    obj = {'name': 'hall', 'id': 344}\\n    assert str(candidate(obj, dict)) == '{\\\"name\\\": \\\"hall\\\", \\\"id\\\": 344}'\\n\"], \"entry_point\": \"f_850118\", \"intent\": \"\\u0421\\u0435\\u0440\\u0438\\u0430\\u043b\\u0438\\u0437\\u043e\\u0432\\u0430\\u0442\\u044c \\u043e\\u0431\\u044a\\u0435\\u043a\\u0442 `my_obj` \\u043a\\u043b\\u0430\\u0441\\u0441\\u0430 `My_class`\", \"library\": [\"json\"]}", "generation": "Scenario 1:\n```python\nimport json\nfrom datetime import datetime\nimport pytz\nfrom decimal import Decimal\n\nclass DateTimeEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, datetime):\n            return obj.isoformat()\n        if isinstance(obj, Decimal):\n            return str(obj)\n        return json.JSONEncoder.default(self, obj)\n\ndef serialize_complex_object(my_obj, My_class=None):\n    \"\"\"\n    Serialize an object `my_obj` of class `My_class` into a JSON string. \n    This function can handle complex types like datetime and decimal.\n    \n    Parameters:\n    my_obj (object): The object to be serialized.\n    My_class (type): The class of the object. Default is None.\n    \n    Returns:\n    str: The serialized JSON string.\n    \n    Requirements:\n    - json\n    - datetime\n    - pytz\n    - decimal\n    \n    Example:\n    >>> serialize_complex_object({'name': 'hall', 'id': 344, 'timestamp': datetime.now(pytz.utc), 'price': Decimal('99.99')})\n    \"\"\"\n    return json.dumps(my_obj, cls=DateTimeEncoder)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\n```python\nimport json\nfrom datetime import datetime, date\nimport pytz\nimport numpy as np\n\nclass NumpyEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, np.ndarray):\n            return obj.tolist()\n        if isinstance(obj, (datetime, date)):\n            return obj.isoformat()\n        return json.JSONEncoder.default(self, obj)\n\ndef serialize_numpy_object(my_obj, My_class=None):\n    \"\"\"\n    Serialize an object `my_obj` of class `My_class` into a JSON string. \n    This function can handle complex types like numpy arrays and datetime.\n    \n    Parameters:\n    my_obj (object): The object to be serialized.\n    My_class (type): The class of the object. Default is None.\n    \n    Returns:\n    str: The serialized JSON string.\n    \n    Requirements:\n    - json\n    - datetime\n    - pytz\n    - numpy\n    \n    Example:\n    >>> serialize_numpy_object({'name': 'hall', 'id': 344, 'timestamp': datetime.now(pytz.utc), 'data': np.array([1, 2, 3])})\n    \"\"\"\n    return json.dumps(my_obj, cls=NumpyEncoder)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\n```python\nimport json\nimport pandas as pd\n\nclass PandasEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, pd.DataFrame):\n            return obj.to_dict()\n        return json.JSONEncoder.default(self, obj)\n\ndef serialize_pandas_object(my_obj, My_class=None):\n    \"\"\"\n    Serialize an object `my_obj` of class `My_class` into a JSON string. \n    This function can handle complex types like pandas DataFrame.\n    \n    Parameters:\n    my_obj (object): The object to be serialized.\n    My_class (type): The class of the object. Default is None.\n    \n    Returns:\n    str: The serialized JSON string.\n    \n    Requirements:\n    - json\n    - pandas\n    \n    Example:\n    >>> df = pd.DataFrame({'A': [1, 2, 3], 'B': ['a', 'b', 'c']})\n    >>> serialize_pandas_object({'name': 'hall', 'id': 344, 'data': df})\n    \"\"\"\n    return json.dumps(my_obj, cls=PandasEncoder)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\n```python\nimport json\nfrom datetime import datetime\nimport pytz\nimport numpy as np\nfrom decimal import Decimal\n\nclass ComplexEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, (datetime, np.ndarray, Decimal)):\n            return str(obj)\n        return json.JSONEncoder.default(self, obj)\n\ndef serialize_complex_types(my_obj, My_class=None):\n    \"\"\"\n    Serialize an object `my_obj` of class `My_class` into a JSON string. \n    This function can handle complex types like datetime, numpy arrays, and decimal.\n    \n    Parameters:\n    my_obj (object): The object to be serialized.\n    My_class (type): The class of the object. Default is None.\n    \n    Returns:\n    str: The serialized JSON string.\n    \n    Requirements:\n    - json\n    - datetime\n    - pytz\n    - numpy\n    - decimal\n    \n    Example:\n    >>> serialize_complex_types({'name': 'hall', 'id': 344, 'timestamp': datetime.now(pytz.utc), 'data': np.array([1, 2, 3]), 'price': Decimal('99.99')})\n    \"\"\"\n    return json.dumps(my_obj, cls=ComplexEncoder)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\n```python\nimport json\nfrom enum import Enum\n\nclass Color(Enum):\n    RED = 1\n    GREEN = 2\n    BLUE = 3\n\nclass EnumEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, Enum):\n            return obj.value\n        return json.JSONEncoder.default(self, obj)\n\ndef serialize_enum_object(my_obj, My_class=None):\n    \"\"\"\n    Serialize an object `my_obj` of class `My_class` into a JSON string. \n    This function can handle complex types like Enum.\n    \n    Parameters:\n    my_obj (object): The object to be serialized.\n    My_class (type): The class of the object. Default is None.\n    \n    Returns:\n    str: The serialized JSON string.\n    \n    Requirements:\n    - json\n    - enum.Enum\n    \n    Example:\n    >>> serialize_enum_object({'name': 'hall', 'id': 344, 'color': Color.RED})\n    \"\"\"\n    return json.dumps(my_obj, cls=EnumEncoder)\n```"}
{"seed": "{\"task_id\": 673692, \"prompt\": \"def f_673692(s):\\n\\t\", \"suffix\": \"\\n\\treturn words\", \"canonical_solution\": \"words = s.split()\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('first second third') == ['first', 'second', 'third']\\n\", \"\\n    assert candidate('first-second-third') == ['first-second-third']\\n\"], \"entry_point\": \"f_673692\", \"intent\": \"\\u0420\\u0430\\u0437\\u0434\\u0435\\u043b\\u0438\\u0442\\u044c \\u0441\\u0442\\u0440\\u043e\\u043a\\u0443 `s` \\u043d\\u0430 \\u0441\\u043b\\u043e\\u0432\\u0430 `words`\", \"library\": []}", "generation": "Scenario 1:\nnltk, string, collections\n```python\nimport nltk\nimport string\nfrom collections import Counter\n\n# Constants\nSTOPWORDS = nltk.corpus.stopwords.words('english')\n\ndef extract_most_common_words(s, n):\n    \"\"\"\n    Extracts the 'n' most common words from a text string 's', excluding the stopwords.\n\n    Parameters:\n    s (str): The text string.\n    n (int): The number of most common words to extract.\n\n    Returns:\n    list: A list of tuples, each containing the word and its frequency.\n\n    Requirements:\n    - nltk\n    - string\n    - collections\n    \n    Example:\n    >>> extract_most_common_words('This is a sample text. This text is just a sample.', 2)\n    [('text', 2), ('sample', 2)]\n    \"\"\"\n    words = nltk.word_tokenize(s)\n    words = [word.lower() for word in words if word.isalpha() and word not in STOPWORDS]\n    word_freq = Counter(words)\n\n    return word_freq.most_common(n)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnltk, re, collections\n```python\nimport nltk\nimport re\nfrom collections import Counter\n\ndef count_word_bigrams(s):\n    \"\"\"\n    Counts the occurrences of word bigrams in a text string 's'.\n\n    Parameters:\n    s (str): The text string.\n\n    Returns:\n    dict: A dictionary with word bigrams as keys and their frequencies as values.\n\n    Requirements:\n    - nltk\n    - re\n    - collections\n\n    Example:\n    >>> count_word_bigrams('This is a sample text. This text is just a sample.')\n    {('This', 'is'): 2, ('is', 'a'): 2, ('a', 'sample'): 2, ('sample', 'text'): 1, ('text', 'This'): 1, ('text', 'is'): 1, ('is', 'just'): 1, ('just', 'a'): 1}\n    \"\"\"\n    s = re.sub(r'\\W', ' ', s)\n    words = nltk.word_tokenize(s)\n    bigrams = nltk.bigrams(words)\n    freq = Counter(bigrams)\n\n    return dict(freq)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnltk, string, collections, gensim\n```python\nimport nltk\nimport string\nfrom collections import Counter\nfrom gensim import corpora, models\n\n# Constants\nSTOPWORDS = nltk.corpus.stopwords.words('english')\n\ndef find_topics(s, num_topics):\n    \"\"\"\n    Finds the 'num_topics' most relevant topics in a text string 's' using Latent Dirichlet Allocation (LDA).\n\n    Parameters:\n    s (str): The text string.\n    num_topics (int): The number of topics to extract.\n\n    Returns:\n    list: A list of topics, each represented as a list of tuples with the word and its weight.\n\n    Requirements:\n    - nltk\n    - string\n    - collections\n    - gensim\n\n    Example:\n    >>> find_topics('This is a sample text. This text is just a sample.', 2)\n    \"\"\"\n    words = nltk.word_tokenize(s)\n    words = [word.lower() for word in words if word.isalpha() and word not in STOPWORDS]\n    word_freq = Counter(words)\n\n    dictionary = corpora.Dictionary([list(word_freq.keys())])\n    corpus = [dictionary.doc2bow(list(word_freq.keys()))]\n\n    lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15)\n\n    topics = lda_model.print_topics()\n\n    return topics\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnltk, string, collections, wordcloud\n```python\nimport nltk\nimport string\nfrom collections import Counter\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\n# Constants\nSTOPWORDS = nltk.corpus.stopwords.words('english')\n\ndef generate_wordcloud(s):\n    \"\"\"\n    Generates a word cloud from a text string 's'.\n\n    Parameters:\n    s (str): The text string.\n\n    Returns:\n    None\n\n    Requirements:\n    - nltk\n    - string\n    - collections\n    - wordcloud\n    - matplotlib.pyplot\n\n    Example:\n    >>> generate_wordcloud('This is a sample text. This text is just a sample.')\n    \"\"\"\n    words = nltk.word_tokenize(s)\n    words = [word.lower() for word in words if word.isalpha() and word not in STOPWORDS]\n    word_freq = Counter(words)\n\n    wc = WordCloud(width=800, height=400, max_words=200, background_color='white').generate_from_frequencies(word_freq)\n    plt.imshow(wc, interpolation='bilinear')\n    plt.axis('off')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnltk, string, collections, sklearn\n```python\nimport nltk\nimport string\nfrom collections import Counter\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Constants\nSTOPWORDS = nltk.corpus.stopwords.words('english')\n\ndef calculate_tfidf(s, n):\n    \"\"\"\n    Calculates the 'n' words with the highest TF-IDF score in a text string 's'.\n\n    Parameters:\n    s (str): The text string.\n    n (int): The number of words to return.\n\n    Returns:\n    list: A list of tuples, each containing the word and its TF-IDF score.\n\n    Requirements:\n    - nltk\n    - string\n    - collections\n    - sklearn.feature_extraction.text\n\n    Example:\n    >>> calculate_tfidf('This is a sample text. This text is just a sample.', 2)\n    \"\"\"\n    words = nltk.word_tokenize(s)\n    words = [word.lower() for word in words if word.isalpha() and word not in STOPWORDS]\n\n    vectorizer = TfidfVectorizer()\n    vectors = vectorizer.fit_transform([' '.join(words)])\n\n    tfidf_scores = sorted(list(zip(vectorizer.get_feature_names(), vectors.sum(axis=0).getA1())), key=lambda x: x[1], reverse=True)\n\n    return tfidf_scores[:n]\n```"}
{"seed": "{\"task_id\": 843929, \"prompt\": \"def f_843929(s):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"isinstance(s, str)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('1a2b34c') == True\\n\", \"\\n    assert candidate(123) == False\\n\"], \"entry_point\": \"f_843929\", \"intent\": \"\\u041f\\u0440\\u043e\\u0432\\u0435\\u0440\\u0438\\u0442\\u044c \\u044f\\u0432\\u043b\\u044f\\u0435\\u0442\\u0441\\u044f \\u043b\\u0438 `s` \\u0441\\u0442\\u0440\\u043e\\u043a\\u043e\\u0439\", \"library\": []}", "generation": "Scenario 1: \nnumpy, pandas, matplotlib.pyplot, collections\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n\n# Constants\nALPHABETS = 'abcdefghijklmnopqrstuvwxyz'\n\ndef analyze_string(s):\n    \"\"\"\n    Analyze a given string. Check if it is a string. If it is a string, calculate the character frequency \n    and plot a bar graph of the frequency distribution of alphabetic characters.\n    \n    Parameters:\n    s (str): The input string.\n    \n    Returns:\n    bool: True if s is a string, False otherwise.\n    DataFrame: A pandas DataFrame with character frequencies if s is a string, None otherwise.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - collections\n\n    Example:\n    >>> is_string, frequency_df = analyze_string('1a2b34c')\n    >>> print(is_string)\n    True\n    >>> print(frequency_df)\n    >>> frequency_df.plot(kind='bar', x='Character', y='Frequency')\n    \"\"\"\n    if not isinstance(s, str):\n        return False, None\n\n    char_freq = Counter(s.lower())\n    data = {char: char_freq.get(char, 0) for char in ALPHABETS}\n    df = pd.DataFrame(list(data.items()), columns=['Character', 'Frequency'])\n\n    return True, df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nre, numpy, matplotlib.pyplot, pandas\n```python\nimport re\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants\nPATTERN = r'\\b[a-zA-Z]*[aeiou]{2}[a-zA-Z]*\\b'\n\ndef find_pattern_in_string(s):\n    \"\"\"\n    Analyze a given string. Check if it is a string. If it is a string, find all words in the string \n    that have two consecutive vowels and plot their frequency.\n\n    Parameters:\n    s (str): The input string.\n\n    Returns:\n    bool: True if s is a string, False otherwise.\n    DataFrame: A pandas DataFrame with word frequencies if s is a string, None otherwise.\n\n    Requirements:\n    - re\n    - numpy\n    - matplotlib.pyplot\n    - pandas\n\n    Example:\n    >>> is_string, frequency_df = find_pattern_in_string('This is a beautiful day')\n    >>> print(is_string)\n    True\n    >>> print(frequency_df)\n    >>> frequency_df.plot(kind='bar', x='Word', y='Frequency')\n    \"\"\"\n    if not isinstance(s, str):\n        return False, None\n\n    words = re.findall(PATTERN, s, flags=re.IGNORECASE)\n    word_freq = pd.DataFrame(list(Counter(words).items()), columns=['Word', 'Frequency'])\n\n    return True, word_freq\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nos, hashlib, binascii\n```python\nimport os\nimport hashlib\nfrom binascii import hexlify\n\ndef check_string_hash(s):\n    \"\"\"\n    Check if a given input is a string. If it is a string, create a SHA256 hash of the string.\n\n    Parameters:\n    s (str): The input string.\n\n    Returns:\n    bool: True if s is a string, False otherwise.\n    str: The SHA256 hash if s is a string, None otherwise.\n\n    Requirements:\n    - os\n    - hashlib\n    - binascii\n\n    Example:\n    >>> is_string, hash_val = check_string_hash('1a2b34c')\n    >>> print(is_string)\n    True\n    >>> print(hash_val)\n    \"\"\"\n    if not isinstance(s, str):\n        return False, None\n\n    hash_object = hashlib.sha256(s.encode())\n    hex_dig = hexlify(hash_object.digest()).decode()\n\n    return True, hex_dig\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, pandas, sklearn.preprocessing\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\n# Constants\nALPHABETS = 'abcdefghijklmnopqrstuvwxyz'\n\ndef string_to_numerical(s):\n    \"\"\"\n    Check if a given input is a string. If it is a string, convert it to numerical representation \n    using label encoding.\n\n    Parameters:\n    s (str): The input string.\n\n    Returns:\n    bool: True if s is a string, False otherwise.\n    numpy.array: The numerical representation if s is a string, None otherwise.\n\n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.preprocessing\n\n    Example:\n    >>> is_string, numerical_rep = string_to_numerical('1a2b34c')\n    >>> print(is_string)\n    True\n    >>> print(numerical_rep)\n    \"\"\"\n    if not isinstance(s, str):\n        return False, None\n\n    label_encoder = LabelEncoder()\n    numerical_rep = label_encoder.fit_transform(list(s))\n\n    return True, numerical_rep\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nre, json, collections\n```python\nimport re\nimport json\nfrom collections import Counter\n\ndef analyze_json_string(s):\n    \"\"\"\n    Check if a given input is a string. If it is a string, check if it is a valid JSON string. \n    If it is a valid JSON string, calculate the frequency of each key in the JSON object.\n\n    Parameters:\n    s (str): The input string.\n\n    Returns:\n    bool: True if s is a string, False otherwise.\n    bool: True if s is a valid JSON string, False otherwise.\n    dict: The frequency of each key if s is a valid JSON string, None otherwise.\n\n    Requirements:\n    - re\n    - json\n    - collections\n\n    Example:\n    >>> is_string, is_json, key_freq = analyze_json_string('{\"key1\": \"value1\", \"key2\": \"value2\", \"key2\": \"value3\"}')\n    >>> print(is_string)\n    True\n    >>> print(is_json)\n    True\n    >>> print(key_freq)\n    \"\"\"\n    if not isinstance(s, str):\n        return False, None, None\n\n    try:\n        json_object = json.loads(s)\n        key_freq = Counter(json_object.keys())\n        return True, True, key_freq\n    except json.JSONDecodeError:\n        return True, False, None\n```\nAbove is the illustration."}
{"seed": "{\"task_id\": 705331, \"prompt\": \"def f_705331(my_list):\\n\\t\", \"suffix\": \"\\n\\treturn my_set\", \"canonical_solution\": \"my_set = set(my_list)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(['a', 12, '45.6', 45.6]) == set(['a', 12, '45.6', 45.6])\\n\", \"\\n    assert candidate([1,2,3,12,123,12,123]) == set([1,2,3,12,123])\\n\"], \"entry_point\": \"f_705331\", \"intent\": \"\\u041f\\u0440\\u0435\\u043e\\u0431\\u0440\\u0430\\u0437\\u043e\\u0432\\u0430\\u0442\\u044c \\u0441\\u043f\\u0438\\u0441\\u043e\\u043a `my_list` \\u0432 \\u043c\\u043d\\u043e\\u0436\\u0435\\u0441\\u0442\\u0432\\u043e `my_set`\", \"library\": []}", "generation": "Scenario 1: \npandas, itertools, matplotlib, random\n```python\nimport pandas as pd\nfrom itertools import combinations\nimport matplotlib.pyplot as plt\nfrom random import sample\n\n# Constants\nANIMALS = ['cat', 'dog', 'rabbit', 'horse', 'elephant', 'tiger', 'lion', 'giraffe', 'zebra', 'monkey']\nZOO_REGIONS = ['North', 'South', 'East', 'West']\n\ndef f_705331_1(my_list):\n    \"\"\"\n    Given a list of animals, randomly assign each animal to a region in the zoo.\n    Then, generate a report on the combinations of animals in each region and plot a bar graph\n    of the number of animals in each region.\n\n    Parameters:\n    my_list (list): A list of animal names.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the combinations of animals in each region.\n\n    Requirements:\n    - pandas\n    - itertools\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> animals = ['cat', 'dog', 'rabbit', 'horse', 'elephant']\n    >>> report = f_705331_1(animals)\n    >>> print(report)\n    >>> report['Region'].value_counts().plot(kind='bar')\n    \"\"\"\n    # Assign each animal to a region\n    assignments = {animal: sample(ZOO_REGIONS, 1)[0] for animal in my_list}\n\n    # Generate combinations of animals in each region\n    combinations_dict = {region: list(combinations([animal for animal, reg in assignments.items() if reg == region], 2)) for region in ZOO_REGIONS}\n\n    # Create a DataFrame from the combinations dictionary\n    report_df = pd.DataFrame([(region, combo) for region, combinations in combinations_dict.items() for combo in combinations], columns=['Region', 'Animal Combination'])\n\n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nnumpy, scipy.stats, matplotlib, random\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nfrom random import sample\n\n# Constants\nPOPULATION_SIZE = 10000\nSAMPLE_SIZE = 1000\n\ndef f_705331_2(my_list):\n    \"\"\"\n    Given a list of numbers, treat the list as a population. \n    Then, draw a random sample from the population, calculate the sample mean and standard deviation, \n    and plot a normal distribution based on these statistics.\n\n    Parameters:\n    my_list (list): A list of numbers representing the population.\n\n    Returns:\n    tuple: A tuple with the sample mean and standard deviation.\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> population = list(range(1, 10001))\n    >>> mean, std_dev = f_705331_2(population)\n    >>> print('Sample Mean:', mean, 'Sample Standard Deviation:', std_dev)\n    >>> x = np.linspace(mean - 3*std_dev, mean + 3*std_dev, 100)\n    >>> plt.plot(x, norm.pdf(x, mean, std_dev))\n    \"\"\"\n    # Draw a random sample from the population\n    sample = np.array(sample(my_list, SAMPLE_SIZE))\n\n    # Calculate the sample mean and standard deviation\n    sample_mean = np.mean(sample)\n    sample_std_dev = np.std(sample)\n\n    return sample_mean, sample_std_dev\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ncollections, heapq, operator\n```python\nfrom collections import Counter\nimport heapq\nfrom operator import itemgetter\n\n# Constants\nTOP_N = 10\n\ndef f_705331_3(my_list):\n    \"\"\"\n    Given a list of items, find the top N items that occur most frequently.\n\n    Parameters:\n    my_list (list): A list of items.\n\n    Returns:\n    list: A list of tuples with the top N items and their counts.\n\n    Requirements:\n    - collections\n    - heapq\n    - operator\n\n    Example:\n    >>> items = ['apple', 'banana', 'cherry', 'apple', 'banana', 'apple']\n    >>> top_items = f_705331_3(items)\n    >>> print(top_items)\n    \"\"\"\n    # Count the occurrences of each item in the list\n    counts = Counter(my_list)\n\n    # Find the top N items\n    top_items = heapq.nlargest(TOP_N, counts.items(), key=itemgetter(1))\n\n    return top_items\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nmatplotlib, numpy, random\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom random import randint\n\n# Constants\nPOINTS = 100\n\ndef f_705331_4(my_list):\n    \"\"\"\n    Given a list of numbers, treat the list as y-coordinates and generate a list of random x-coordinates.\n    Then, plot a scatter plot of the points.\n\n    Parameters:\n    my_list (list): A list of numbers representing y-coordinates.\n\n    Returns:\n    None\n\n    Requirements:\n    - matplotlib.pyplot\n    - numpy\n    - random\n\n    Example:\n    >>> y_coords = list(range(1, 101))\n    >>> f_705331_4(y_coords)\n    \"\"\"\n    # Generate random x-coordinates\n    x_coords = [randint(1, POINTS) for _ in range(len(my_list))]\n\n    # Plot the points\n    plt.scatter(x_coords, my_list)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnumpy, scipy.stats\n```python\nimport numpy as np\nfrom scipy import stats\n\n# Constants\nALPHA = 0.05\n\ndef f_705331_5(my_list):\n    \"\"\"\n    Given a list of numbers, perform a one-sample t-test to test if the mean of the numbers is significantly different from zero.\n\n    Parameters:\n    my_list (list): A list of numbers.\n\n    Returns:\n    bool: True if the mean is significantly different from zero, False otherwise.\n\n    Requirements:\n    - numpy\n    - scipy.stats\n\n    Example:\n    >>> numbers = [1, 2, 3, 4, 5]\n    >>> result = f_705331_5(numbers)\n    >>> print(result)\n    \"\"\"\n    # Perform the t-test\n    t_stat, p_value = stats.ttest_1samp(my_list, 0)\n\n    # Check if the p-value is less than the significance level\n    if p_value < ALPHA:\n        return True\n    else:\n        return False\n```"}
{"seed": "{\"task_id\": 262786, \"prompt\": \"def f_262786(num):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"list(str(num))\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(43524) == ['4', '3', '5', '2', '4']\\n\", \"\\n    assert candidate(\\\"hello, world!\\\") == ['h', 'e', 'l', 'l', 'o', ',', ' ', 'w', 'o', 'r', 'l', 'd', '!']\\n\"], \"entry_point\": \"f_262786\", \"intent\": \"\\u0420\\u0430\\u0437\\u0431\\u0438\\u0442\\u044c \\u0447\\u0438\\u0441\\u043b\\u043e `num` \\u043d\\u0430 \\u0440\\u0430\\u0437\\u0440\\u044f\\u0434\\u044b\", \"library\": []}", "generation": "Scenario 1:\npandas, numpy, itertools\n```python\nimport pandas as pd\nimport numpy as np\nfrom itertools import combinations\n\n# Constants\nNUMBERS = list(range(1, 21))\n\ndef find_triplets_with_sum(num):\n    \"\"\"\n    Find all triplets in an array of integers where the sum of the integers equals a given number.\n\n    Parameters:\n    num (int): The number that the sum of the integers should equal.\n\n    Returns:\n    list: A list of triplets.\n\n    Requirements:\n    - pandas\n    - numpy\n    - itertools\n\n    Example:\n    >>> find_triplets_with_sum(15)\n    \"\"\"\n    triplets = []\n\n    for combo in combinations(NUMBERS, 3):\n        if sum(combo) == num:\n            triplets.append(combo)\n\n    return triplets\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, pandas, matplotlib.pyplot\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nNUMBERS = list(range(1, 101))\n\ndef plot_distribution_of_numbers():\n    \"\"\"\n    Plot the distribution of numbers in an array.\n\n    No parameters.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_distribution_of_numbers()\n    \"\"\"\n    series = pd.Series(NUMBERS)\n    series.plot(kind='hist', bins=10)\n\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, pandas, itertools\n```python\nimport numpy as np\nimport pandas as pd\nfrom itertools import count\n\n# Constants\nNUMBERS = list(range(1, 11))\n\ndef find_longest_increasing_subsequence():\n    \"\"\"\n    Find the longest increasing subsequence in an array of integers.\n\n    No parameters.\n\n    Returns:\n    list: The longest increasing subsequence.\n\n    Requirements:\n    - numpy\n    - pandas\n    - itertools\n\n    Example:\n    >>> find_longest_increasing_subsequence()\n    \"\"\"\n    longest = []\n    for i in range(len(NUMBERS)):\n        for j in range(i + len(longest), len(NUMBERS)):\n            if all(NUMBERS[i] < NUMBERS[k] for k in range(i + 1, j)):\n                longest = NUMBERS[i:j]\n\n    return longest\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, pandas, random\n```python\nimport numpy as np\nimport pandas as pd\nimport random\n\n# Constants\nNUMBERS = list(range(1, 101))\n\ndef create_random_sample_with_replacement(size):\n    \"\"\"\n    Create a random sample with replacement from an array of integers.\n\n    Parameters:\n    size (int): The size of the sample.\n\n    Returns:\n    list: The random sample.\n\n    Requirements:\n    - numpy\n    - pandas\n    - random\n\n    Example:\n    >>> create_random_sample_with_replacement(10)\n    \"\"\"\n    return random.choices(NUMBERS, k=size)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, pandas, itertools\n```python\nimport numpy as np\nimport pandas as pd\nfrom itertools import permutations\n\n# Constants\nNUMBERS = list(range(1, 11))\n\ndef find_all_permutations():\n    \"\"\"\n    Find all permutations of an array of integers.\n\n    No parameters.\n\n    Returns:\n    list: The permutations.\n\n    Requirements:\n    - numpy\n    - pandas\n    - itertools\n\n    Example:\n    >>> find_all_permutations()\n    \"\"\"\n    return list(permutations(NUMBERS))\n```"}
{"seed": "{\"task_id\": 262786, \"prompt\": \"def f_262786(num):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"[int(i) for i in str(num)]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('43524') == [4,3,5,2,4]\\n\"], \"entry_point\": \"f_262786\", \"intent\": \"\\u0420\\u0430\\u0437\\u0431\\u0438\\u0442\\u044c \\u0447\\u0438\\u0441\\u043b\\u043e `num` \\u043d\\u0430 \\u0440\\u0430\\u0437\\u0440\\u044f\\u0434\\u044b\", \"library\": []}", "generation": "Scenario 1:\nnumpy, pandas, matplotlib, itertools\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport itertools\n\ndef digits_to_dataframe(num):\n    \"\"\"\n    Convert a number into pandas dataframe. The dataframe contains information about\n    each digit's frequency and cumulative frequency in the number. It also plots the \n    cumulative frequency.\n\n    Parameters:\n    num (int): The number to be analyzed.\n\n    Returns:\n    DataFrame: A pandas DataFrame with digit frequency and cumulative frequency.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - itertools\n\n    Example:\n    >>> digits_to_dataframe(4352443524)\n    \"\"\"\n    digits = [int(i) for i in str(num)]\n    unique_digits = np.unique(digits)\n\n    digit_freq = []\n    for digit in unique_digits:\n        digit_freq.append(digits.count(digit))\n        \n    cum_freq = np.cumsum(digit_freq)\n    \n    df = pd.DataFrame(list(zip(unique_digits, digit_freq, cum_freq)), \n                      columns=['Digit', 'Frequency', 'Cumulative Frequency'])\n    \n    df.plot(kind='line',x='Digit',y='Cumulative Frequency',color='red')\n    plt.show()\n    \n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, random, statistics\n```python\nimport numpy as np\nfrom random import choice\nimport statistics\n\n# Constants\nDIGITS = [i for i in range(10)]\n\ndef random_number_stat(num):\n    \"\"\"\n    Generate a random number of length `num` and compute some basic statistics \n    (mean, median, mode, variance) of its digits.\n\n    Parameters:\n    num (int): The length of the random number.\n\n    Returns:\n    dict: A dictionary with statistics.\n\n    Requirements:\n    - numpy\n    - random\n    - statistics\n\n    Example:\n    >>> random_number_stat(500)\n    \"\"\"\n    random_num = [choice(DIGITS) for _ in range(num)]\n    stats = {\n        'mean': np.mean(random_num),\n        'median': np.median(random_num),\n        'mode': statistics.mode(random_num),\n        'variance': np.var(random_num)\n    }\n    \n    return stats\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, pandas, random\n```python\nimport numpy as np\nimport pandas as pd\nfrom random import choice\n\ndef random_number_digit_pairs(num):\n    \"\"\"\n    Generate a random number of length `num` and create a pandas DataFrame of \n    digit pairs and their frequencies.\n\n    Parameters:\n    num (int): The length of the random number.\n\n    Returns:\n    DataFrame: A pandas DataFrame with digit pairs and their frequencies.\n\n    Requirements:\n    - numpy\n    - pandas\n    - random\n\n    Example:\n    >>> random_number_digit_pairs(500)\n    \"\"\"\n    random_num = [choice(DIGITS) for _ in range(num)]\n    digit_pairs = list(zip(random_num[:-1], random_num[1:]))\n    unique_pairs = np.unique(digit_pairs, axis=0)\n    \n    pair_freq = []\n    for pair in unique_pairs:\n        pair_freq.append(digit_pairs.count(tuple(pair)))\n        \n    df = pd.DataFrame(list(zip(unique_pairs, pair_freq)), \n                      columns=['Digit Pair', 'Frequency'])\n    \n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, matplotlib, random\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import choice\n\ndef random_number_digit_histogram(num):\n    \"\"\"\n    Generate a random number of length `num` and plot a histogram of its digits.\n\n    Parameters:\n    num (int): The length of the random number.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> random_number_digit_histogram(500)\n    \"\"\"\n    random_num = [choice(DIGITS) for _ in range(num)]\n    plt.hist(random_num, bins=10, range=(0,10), edgecolor='black')\n    plt.title('Histogram of Digits')\n    plt.xlabel('Digit')\n    plt.ylabel('Frequency')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, random, scipy.stats\n```python\nimport numpy as np\nfrom random import choice\nfrom scipy import stats\n\ndef random_number_digit_test(num):\n    \"\"\"\n    Generate a random number of length `num` and perform a chi-squared goodness-of-fit \n    test to check if its digits follow a uniform distribution.\n\n    Parameters:\n    num (int): The length of the random number.\n\n    Returns:\n    float: The p-value from the chi-squared test.\n\n    Requirements:\n    - numpy\n    - random\n    - scipy.stats\n\n    Example:\n    >>> random_number_digit_test(500)\n    \"\"\"\n    random_num = [choice(DIGITS) for _ in range(num)]\n    observed_freq = np.bincount(random_num)\n    expected_freq = [num/10]*10\n    \n    chi2, p = stats.chisquare(f_obs=observed_freq, f_exp=expected_freq)\n    \n    return p\n```"}
{"seed": "{\"task_id\": 313184, \"prompt\": \"def f_313184(func):\\n\\t\", \"suffix\": \"\\n\\treturn time_work\", \"canonical_solution\": \"t = time.time()\\n\\n\\tfunc()\\n\\n\\ttime_work = time.time()-t\", \"test_start\": \"\\nimport time\\n\\ndef check(candidate):\", \"test\": [\"\\n    def func(): print('Hello World!')\\n    assert (candidate(func) - 8.368492126464844e-05) < 1e-6\\n\"], \"entry_point\": \"f_313184\", \"intent\": \"\\u0418\\u0437\\u043c\\u0435\\u0440\\u0438\\u0442\\u044c \\u0432\\u0440\\u0435\\u043c\\u044f \\u0432\\u044b\\u043f\\u043e\\u043b\\u043d\\u0435\\u043d\\u0438\\u044f `time_work` \\u0444\\u0443\\u043d\\u043a\\u0446\\u0438\\u0438 `func`\", \"library\": [\"time\"]}", "generation": "Scenario 1:\nnumpy, time, random, matplotlib\n```python\nimport numpy as np\nimport time\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nMAX_ARRAY_SIZE = 1000\nNUM_TESTS = 10\n\ndef measure_np_sort_time():\n    \"\"\"\n    Measures the time taken by numpy's sort function for arrays of random size\n    (up to a maximum size) and plots a graph of array size vs time taken.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - numpy\n    - time\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> measure_np_sort_time()\n    \"\"\"\n    sizes = []\n    times = []\n    for _ in range(NUM_TESTS):\n        size = randint(1, MAX_ARRAY_SIZE)\n        array = np.random.rand(size)\n        start = time.time()\n        np.sort(array)\n        end = time.time()\n        sizes.append(size)\n        times.append(end - start)\n    \n    plt.plot(sizes, times, 'o')\n    plt.xlabel('Array size')\n    plt.ylabel('Time taken (s)')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, time, random, matplotlib\n```python\nimport pandas as pd\nimport time\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nMAX_ROWS = 1000\nNUM_TESTS = 10\n\ndef measure_pd_sort_time():\n    \"\"\"\n    Measures the time taken by pandas' sort function for DataFrames of random size\n    (up to a maximum size) and plots a graph of DataFrame size vs time taken.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - pandas\n    - time\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> measure_pd_sort_time()\n    \"\"\"\n    sizes = []\n    times = []\n    for _ in range(NUM_TESTS):\n        size = randint(1, MAX_ROWS)\n        df = pd.DataFrame(np.random.randint(0,100,size=(size, 4)), columns=list('ABCD'))\n        start = time.time()\n        df.sort_values(by='A')\n        end = time.time()\n        sizes.append(size)\n        times.append(end - start)\n    \n    plt.plot(sizes, times, 'o')\n    plt.xlabel('DataFrame size (rows)')\n    plt.ylabel('Time taken (s)')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, time, random, matplotlib\n```python\nimport os\nimport time\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nMAX_FILES = 100\nNUM_TESTS = 10\n\ndef measure_os_listdir_time(directory):\n    \"\"\"\n    Measures the time taken by os's listdir function when listing files in a directory\n    for a random number of files (up to a maximum number) and plots a graph of \n    number of files vs time taken.\n    \n    Parameters:\n    directory (str): The directory to create the files in.\n\n    Returns:\n    None\n    \n    Requirements:\n    - os\n    - time\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> measure_os_listdir_time('/path/to/my/directory')\n    \"\"\"\n    sizes = []\n    times = []\n    for _ in range(NUM_TESTS):\n        size = randint(1, MAX_FILES)\n        for i in range(size):\n            with open(os.path.join(directory, f'file{i}'), 'w') as f:\n                pass\n        start = time.time()\n        os.listdir(directory)\n        end = time.time()\n        sizes.append(size)\n        times.append(end - start)\n    \n    plt.plot(sizes, times, 'o')\n    plt.xlabel('Number of files')\n    plt.ylabel('Time taken (s)')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, time, random, matplotlib\n```python\nimport numpy as np\nimport time\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nMAX_MATRIX_SIZE = 100\nNUM_TESTS = 10\n\ndef measure_np_dot_time():\n    \"\"\"\n    Measures the time taken by numpy's dot function for matrices of random size\n    (up to a maximum size) and plots a graph of matrix size vs time taken.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - numpy\n    - time\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> measure_np_dot_time()\n    \"\"\"\n    sizes = []\n    times = []\n    for _ in range(NUM_TESTS):\n        size = randint(1, MAX_MATRIX_SIZE)\n        matrix1 = np.random.rand(size, size)\n        matrix2 = np.random.rand(size, size)\n        start = time.time()\n        np.dot(matrix1, matrix2)\n        end = time.time()\n        sizes.append(size)\n        times.append(end - start)\n    \n    plt.plot(sizes, times, 'o')\n    plt.xlabel('Matrix size')\n    plt.ylabel('Time taken (s)')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, time, random, matplotlib\n```python\nimport numpy as np\nimport time\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nMAX_ARRAY_SIZE = 1000\nNUM_TESTS = 10\n\ndef measure_np_sum_time():\n    \"\"\"\n    Measures the time taken by numpy's sum function for arrays of random size\n    (up to a maximum size) and plots a graph of array size vs time taken.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - numpy\n    - time\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> measure_np_sum_time()\n    \"\"\"\n    sizes = []\n    times = []\n    for _ in range(NUM_TESTS):\n        size = randint(1, MAX_ARRAY_SIZE)\n        array = np.random.rand(size)\n        start = time.time()\n        np.sum(array)\n        end = time.time()\n        sizes.append(size)\n        times.append(end - start)\n    \n    plt.plot(sizes, times, 'o')\n    plt.xlabel('Array size')\n    plt.ylabel('Time taken (s)')\n    plt.show()\n```\n`GPT_ODEX_BREAK`"}
{"seed": "{\"task_id\": 678639, \"prompt\": \"def f_678639(s):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"any(map(str.isdigit, s))\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('124dhe5') == True\\n\", \"\\n    assert candidate('absbf') == False\\n\"], \"entry_point\": \"f_678639\", \"intent\": \"\\u041f\\u0440\\u043e\\u0432\\u0435\\u0440\\u0438\\u0442\\u044c \\u0435\\u0441\\u0442\\u044c \\u043b\\u0438 \\u0447\\u0438\\u0441\\u043b\\u0430 \\u0432 \\u0441\\u0442\\u0440\\u043e\\u043a\\u0435 `s`\", \"library\": []}", "generation": "Scenario 1: \npandas, re, matplotlib, numpy\n```python\nimport pandas as pd\nimport re\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nLETTERS = list('abcdefghijklmnopqrstuvwxyz')\n\ndef count_letter_digit_ratio(text):\n    \"\"\"\n    Count the ratio of letters to digits in a given text and present the results \n    in a pandas DataFrame and a bar plot.\n\n    Parameters:\n    text (str): The input text.\n\n    Returns:\n    DataFrame, Figure: A pandas DataFrame with counts and the matplotlib figure.\n\n    Requirements:\n    - pandas\n    - re\n    - matplotlib.pyplot\n    - numpy\n\n    Example:\n    >>> text = 'The quick brown fox jumps over the lazy dog 1234567890'\n    >>> df, fig = count_letter_digit_ratio(text)\n    \"\"\"\n    counts = []\n    for letter in LETTERS:\n        counts.append([letter, text.count(letter)])\n\n    digit_count = len(re.findall(r'\\d', text))\n    counts.append(['digits', digit_count])\n\n    df = pd.DataFrame(counts, columns=['Element', 'Count'])\n    df['Ratio'] = df['Count'] / df['Count'].sum()\n\n    fig, ax = plt.subplots()\n    df.plot.bar(x='Element', y='Ratio', ax=ax)\n\n    return df, fig\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nre, collections, matplotlib.pyplot, numpy\n```python\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_word_lengths(text):\n    \"\"\"\n    Plot a histogram of word lengths in a given text.\n\n    Parameters:\n    text (str): The input text.\n\n    Returns:\n    Figure: A matplotlib figure with the histogram.\n\n    Requirements:\n    - re\n    - collections.Counter\n    - matplotlib.pyplot\n    - numpy\n\n    Example:\n    >>> text = 'The quick brown fox jumps over the lazy dog'\n    >>> fig = plot_word_lengths(text)\n    \"\"\"\n    words = re.findall(r'\\b\\w+\\b', text)\n    word_lengths = [len(word) for word in words]\n    counter = Counter(word_lengths)\n\n    fig, ax = plt.subplots()\n    ax.bar(counter.keys(), counter.values())\n    ax.set_xlabel('Word Length')\n    ax.set_ylabel('Count')\n\n    return fig\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nre, random, numpy, matplotlib.pyplot, pandas\n```python\nimport re\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants\nALPHABET = list('abcdefghijklmnopqrstuvwxyz')\n\ndef generate_random_string():\n    \"\"\"\n    Generate a random string of length 10000, count the occurrences of each letter \n    and plot a bar chart of the counts.\n\n    Returns:\n    str, DataFrame, Figure: The random string, a pandas DataFrame with counts and \n    the matplotlib figure.\n\n    Requirements:\n    - re\n    - random\n    - numpy\n    - matplotlib.pyplot\n    - pandas\n\n    Example:\n    >>> s, df, fig = generate_random_string()\n    \"\"\"\n    s = ''.join(random.choice(ALPHABET) for _ in range(10000))\n\n    counts = []\n    for letter in ALPHABET:\n        counts.append([letter, s.count(letter)])\n\n    df = pd.DataFrame(counts, columns=['Letter', 'Count'])\n\n    fig, ax = plt.subplots()\n    df.plot.bar(x='Letter', y='Count', ax=ax)\n\n    return s, df, fig\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nre, string, collections, matplotlib.pyplot\n```python\nimport re\nimport string\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef plot_letter_frequency_distribution(text):\n    \"\"\"\n    Plot a histogram of letter frequencies in a given text.\n\n    Parameters:\n    text (str): The input text.\n\n    Returns:\n    Figure: A matplotlib figure with the histogram.\n\n    Requirements:\n    - re\n    - string\n    - collections.Counter\n    - matplotlib.pyplot\n\n    Example:\n    >>> text = 'The quick brown fox jumps over the lazy dog'\n    >>> fig = plot_letter_frequency_distribution(text)\n    \"\"\"\n    letters = re.findall('[a-zA-Z]', text.lower())\n    counter = Counter(letters)\n\n    fig, ax = plt.subplots()\n    ax.bar(counter.keys(), counter.values())\n    ax.set_xlabel('Letter')\n    ax.set_ylabel('Frequency')\n\n    return fig\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nre, numpy, matplotlib.pyplot, pandas\n```python\nimport re\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef analyze_text(text):\n    \"\"\"\n    Analyze a given text, count the number of words, digits, and punctuation, \n    and return the counts in a pandas DataFrame.\n\n    Parameters:\n    text (str): The input text.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the counts.\n\n    Requirements:\n    - re\n    - numpy\n    - matplotlib.pyplot\n    - pandas\n\n    Example:\n    >>> text = 'Hello, world! 123'\n    >>> df = analyze_text(text)\n    \"\"\"\n    words = re.findall(r'\\b\\w+\\b', text)\n    digits = re.findall(r'\\d', text)\n    punctuation = re.findall(r'[' + re.escape(string.punctuation) + ']', text)\n\n    counts = pd.DataFrame({\n        'Words': [len(words)],\n        'Digits': [len(digits)],\n        'Punctuation': [len(punctuation)]\n    })\n\n    return counts\n```"}
{"seed": "{\"task_id\": 678639, \"prompt\": \"def f_678639(s):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"re.search('\\\\d+', s)\", \"test_start\": \"\\nimport re \\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('124dhe5').group() == '124'\\n\", \"\\n    assert candidate('absbf') is None\\n\"], \"entry_point\": \"f_678639\", \"intent\": \"\\u041f\\u0440\\u043e\\u0432\\u0435\\u0440\\u0438\\u0442\\u044c \\u0435\\u0441\\u0442\\u044c \\u043b\\u0438 \\u0447\\u0438\\u0441\\u043b\\u0430 \\u0432 \\u0441\\u0442\\u0440\\u043e\\u043a\\u0435 `s`\", \"library\": [\"re\"]}", "generation": "Scenario 1: \npandas, re, nltk, matplotlib\n```python\nimport pandas as pd\nimport re\nfrom nltk.corpus import stopwords\nimport matplotlib.pyplot as plt\n\n# Constants\nSTOPWORDS = set(stopwords.words('english'))\n\ndef analyze_text_data(text_data):\n    \"\"\"\n    Analyze the text data. Extract the words, count their frequency, \n    and create a bar chart of the top 10 most frequent words excluding the stopwords.\n\n    Parameters:\n    text_data (str): The text data to be analyzed.\n\n    Returns:\n    DataFrame: A pandas DataFrame with words and their frequency.\n\n    Requirements:\n    - pandas\n    - re\n    - nltk.corpus\n    - matplotlib.pyplot\n    \n    Example:\n    >>> text = \"This is a sample text. This text is used for testing the function.\"\n    >>> df = analyze_text_data(text)\n    >>> print(df.head(10))\n    >>> df.head(10).plot(kind='bar', x='Word', y='Frequency')\n    \"\"\"\n    words = re.findall(r'\\b\\w+\\b', text_data)\n    words = [word.lower() for word in words if word not in STOPWORDS]\n\n    word_series = pd.Series(words)\n    word_counts = word_series.value_counts().reset_index()\n    word_counts.columns = ['Word', 'Frequency']\n\n    return word_counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, re, collections, pandas\n```python\nimport os\nimport re\nfrom collections import Counter\nimport pandas as pd\n\n# Constants\nFILE_PATH = \"/path/to/directory\"\n\ndef count_file_extensions():\n    \"\"\"\n    Count the file extensions in a directory and return a data frame with the extensions \n    and their counts.\n\n    Returns:\n    DataFrame: A pandas DataFrame with file extensions and their counts.\n\n    Requirements:\n    - os\n    - re\n    - collections\n    - pandas\n\n    Example:\n    >>> df = count_file_extensions()\n    >>> print(df)\n    \"\"\"\n    file_names = os.listdir(FILE_PATH)\n    extensions = [re.search(r'\\.(\\w+)$', file_name).group(1) for file_name in file_names if re.search(r'\\.(\\w+)$', file_name)]\n    extension_counts = Counter(extensions)\n\n    df = pd.DataFrame.from_dict(extension_counts, orient='index').reset_index()\n    df.columns = ['Extension', 'Count']\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nre, nltk, string, collections\n```python\nimport re\nfrom nltk.tokenize import word_tokenize\nfrom string import punctuation\nfrom collections import Counter\n\n# Constants\nPUNCTUATION = set(punctuation)\n\ndef count_punctuation(text):\n    \"\"\"\n    Count the punctuation in a text string and return a dictionary with the punctuation \n    and their counts.\n\n    Parameters:\n    text (str): The text string to analyze.\n\n    Returns:\n    dict: A dictionary with punctuation and their counts.\n\n    Requirements:\n    - re\n    - nltk.tokenize\n    - string\n    - collections\n\n    Example:\n    >>> text = \"Hello, world! This is a test... Is it working?\"\n    >>> counts = count_punctuation(text)\n    >>> print(counts)\n    \"\"\"\n    words = word_tokenize(text)\n    punctuation = [word for word in words if word in PUNCTUATION]\n    punctuation_counts = Counter(punctuation)\n\n    return dict(punctuation_counts)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nre, pandas, matplotlib\n```python\nimport re\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nCSV_FILE = \"/path/to/csv_file.csv\"\n\ndef analyze_data():\n    \"\"\"\n    Analyze a CSV data file. Extract the numeric data from a specific column, \n    calculate the average, and create a histogram of the data.\n\n    Returns:\n    float: The average of the numeric data.\n\n    Requirements:\n    - re\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> avg = analyze_data()\n    >>> print(avg)\n    \"\"\"\n    df = pd.read_csv(CSV_FILE)\n    numeric_data = df['column_name'].apply(lambda x: re.search(r'\\d+', x).group() if re.search(r'\\d+', x) else None)\n    numeric_data = pd.to_numeric(numeric_data, errors='coerce').dropna()\n    avg = numeric_data.mean()\n\n    numeric_data.hist(bins=10)\n\n    return avg\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nre, requests, bs4\n```python\nimport re\nimport requests\nfrom bs4 import BeautifulSoup\n\n# Constants\nURL = \"http://example.com\"\n\ndef scrape_website():\n    \"\"\"\n    Scrape a website and extract all the URLs from the webpage.\n\n    Returns:\n    list: A list of URLs.\n\n    Requirements:\n    - re\n    - requests\n    - bs4\n\n    Example:\n    >>> urls = scrape_website()\n    >>> print(urls)\n    \"\"\"\n    response = requests.get(URL)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    urls = [a['href'] for a in soup.find_all('a', href=re.compile(r'^http'))]\n\n    return urls\n```\n"}
{"seed": "{\"task_id\": 830473, \"prompt\": \"def f_830473():\\n\\t\", \"suffix\": \"\\n\\treturn num\", \"canonical_solution\": \"num = int(input().strip())\", \"test_start\": \"\\nimport builtins\\nfrom unittest.mock import Mock\\n\\ndef check(candidate):\", \"test\": [\"\\n    builtins.input = Mock(return_value = '8')\\n    assert candidate() == 8\\n\", \"\\n    builtins.input = Mock(return_value = '80   ')\\n    assert candidate() == 80\\n\"], \"entry_point\": \"f_830473\", \"intent\": \"\\u0421\\u0447\\u0438\\u0442\\u0430\\u0442\\u044c \\u0441 \\u043a\\u043b\\u0430\\u0432\\u0438\\u0430\\u0442\\u0443\\u0440\\u044b \\u0447\\u0438\\u0441\\u043b\\u043e \\u0432 \\u043f\\u0435\\u0440\\u0435\\u043c\\u0435\\u043d\\u043d\\u0443\\u044e `num`\", \"library\": [\"builtins\"]}", "generation": "Scenario 1:\nnumpy, pandas, matplotlib, random\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nROWS = 100\nCOLUMNS = 5\nRANGE = 100\n\ndef create_random_df_plot():\n    \"\"\"\n    Create a random DataFrame with 100 rows and 5 columns, where entries are integers \n    in the range of 0 to 100. Then plot a histogram of the DataFrame.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with random values.\n    Plot: A histogram of the DataFrame.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> df, plot = create_random_df_plot()\n    >>> print(df)\n    >>> plt.show(plot)\n    \"\"\"\n    data = np.random.randint(0, RANGE, size=(ROWS, COLUMNS))\n    df = pd.DataFrame(data, columns=[f'Column {i+1}' for i in range(COLUMNS)])\n\n    plot = df.hist(bins=RANGE)\n\n    return df, plot\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, shutil, pathlib, time\n```python\nimport os\nimport shutil\nimport pathlib\nimport time\n\n# Constants\nSRC_FOLDER = './src_folder'\nDST_FOLDER = './dst_folder'\nFILE_EXTENSION = '.txt'\n\ndef copy_files():\n    \"\"\"\n    Copy all .txt files from a source folder to a destination folder, \n    then print the total time taken to copy the files.\n\n    Requirements:\n    - os\n    - shutil\n    - pathlib\n    - time\n\n    Example:\n    >>> copy_files()\n    \"\"\"\n    start_time = time.time()\n\n    pathlib.Path(DST_FOLDER).mkdir(parents=True, exist_ok=True)\n\n    for file_name in os.listdir(SRC_FOLDER):\n        if file_name.endswith(FILE_EXTENSION):\n            shutil.copy(os.path.join(SRC_FOLDER, file_name), DST_FOLDER)\n\n    end_time = time.time()\n\n    print(f'Total time taken to copy files: {end_time - start_time} seconds')\n\n    return None\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ncsv, datetime, random, collections\n```python\nimport csv\nimport datetime\nimport random\nimport collections\n\n# Constants\nDATA_PATH = './data.csv'\nDATES = [datetime.date(2022, i, 1) for i in range(1, 13)]\nNAMES = ['Alice', 'Bob', 'Charlie', 'Dave', 'Eve']\n\ndef generate_csv_report():\n    \"\"\"\n    Generate a CSV report with random data for dates and names, \n    then count the occurrences of each name in the report.\n\n    Returns:\n    Counter: A collections.Counter object with the counts of each name.\n\n    Requirements:\n    - csv\n    - datetime\n    - random\n    - collections\n\n    Example:\n    >>> counts = generate_csv_report()\n    >>> print(counts)\n    \"\"\"\n    with open(DATA_PATH, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(['Date', 'Name'])\n        for date in DATES:\n            for _ in range(random.randint(1, 5)):\n                writer.writerow([date, random.choice(NAMES)])\n\n    with open(DATA_PATH, 'r') as f:\n        reader = csv.reader(f)\n        next(reader)  # Skip the header row\n        name_counts = collections.Counter(row[1] for row in reader)\n\n    return name_counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nrequests, json, urllib.parse, time\n```python\nimport requests\nimport json\nimport urllib.parse\nimport time\n\n# Constants\nAPI_URL = 'https://api.github.com/users/'\n\ndef fetch_github_user_data(username):\n    \"\"\"\n    Fetch a GitHub user's data using the GitHub API, then write the data to a JSON file.\n\n    Parameters:\n    username (str): The GitHub username.\n\n    Returns:\n    dict: The user's data.\n\n    Requirements:\n    - requests\n    - json\n    - urllib.parse\n    - time\n\n    Example:\n    >>> data = fetch_github_user_data('torvalds')\n    >>> print(data)\n    \"\"\"\n    response = requests.get(urllib.parse.urljoin(API_URL, username))\n    data = response.json()\n\n    with open(f'{username}_github_data.json', 'w') as f:\n        json.dump(data, f)\n\n    return data\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, scipy.stats, matplotlib, random\n```python\nimport numpy as np\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nimport random\n\n# Constants\nSAMPLE_SIZE = 1000\nRANGE = 100\nBIN_WIDTH = 10\n\ndef plot_normal_distribution():\n    \"\"\"\n    Generate a sample of random numbers, fit a normal distribution to the data, \n    and plot a histogram of the data with the fitted normal distribution.\n\n    Returns:\n    Figure: A matplotlib figure.\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> fig = plot_normal_distribution()\n    >>> plt.show(fig)\n    \"\"\"\n    data = np.random.randint(0, RANGE, size=SAMPLE_SIZE)\n    mu, std = stats.norm.fit(data)\n\n    plt.hist(data, bins=range(0, RANGE+BIN_WIDTH, BIN_WIDTH), density=True, alpha=0.6, color='g')\n\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, mu, std)\n    plt.plot(x, p, 'k', linewidth=2)\n\n    title = f\"Fit results: mu = {mu:.2f},  std = {std:.2f}\"\n    plt.title(title)\n\n    return plt.gcf()\n```"}
{"seed": "{\"task_id\": 773920, \"prompt\": \"def f_773920(val1, val2, N):\\n\\t\", \"suffix\": \"\\n\\treturn my_list\", \"canonical_solution\": \"my_list = [random.randint(val1, val2) for i in range(N)]\", \"test_start\": \"\\nimport random \\n\\ndef check(candidate):\", \"test\": [\"\\n    my_list = candidate(0, 10, 4) \\n    assert len(my_list) == 4\\n    assert all([isinstance(a, int) for a in my_list])\\n    assert min(my_list) >= 0\\n    assert max(my_list) <= 10\\n\"], \"entry_point\": \"f_773920\", \"intent\": \"\\u0421\\u0433\\u0435\\u043d\\u0435\\u0440\\u0438\\u0440\\u043e\\u0432\\u0430\\u0442\\u044c \\u0441\\u043f\\u0438\\u0441\\u043a\\u043e\\u043a `my_list` \\u0441\\u043b\\u0443\\u0447\\u0430\\u0439\\u043d\\u044b\\u0445 \\u0437\\u043d\\u0430\\u0447\\u0435\\u043d\\u0438\\u0439 \\u0432 \\u0434\\u0438\\u0430\\u043f\\u0430\\u0437\\u043e\\u043d\\u0435 \\u043e\\u0442 `val1` \\u0434\\u043e `val2` \\u0434\\u043b\\u0438\\u043d\\u043d\\u043e\\u0439 `N`\", \"library\": [\"random\"]}", "generation": "Scenario 1:\npandas, random, matplotlib\n```python\nimport pandas as pd\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nCATEGORIES = ['Fruits', 'Vegetables', 'Dairy', 'Meat', 'Grains']\n\ndef f_773920(val1, val2, N):\n    \"\"\"\n    Generate a pandas DataFrame with N rows of random data for a grocery store \n    inventory where each row represents a different item. \n    The item's category is chosen randomly from a list of categories. \n    The item's quantity is a random integer between val1 and val2 (inclusive).\n    \n    Parameters:\n    val1 (int): The minimum possible quantity.\n    val2 (int): The maximum possible quantity.\n    N (int): The number of items.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the grocery store inventory.\n\n    Requirements:\n    - pandas\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> inventory = f_773920(1, 100, 10)\n    >>> print(inventory)\n    >>> inventory['Quantity'].hist(bins=10)\n    \"\"\"\n    data = {\n        'Category': [CATEGORIES[randint(0, len(CATEGORIES)-1)] for _ in range(N)],\n        'Quantity': [randint(val1, val2) for _ in range(N)]\n    }\n\n    df = pd.DataFrame(data)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nrandom, numpy, scipy.stats\n```python\nimport numpy as np\nfrom random import randint\nfrom scipy import stats\n\n# Constants\nnp.random.seed(0)\n\ndef f_773920(val1, val2, N):\n    \"\"\"\n    Generate a numpy array of N random integer values between val1 and val2, \n    then calculate and return the mode of the array.\n    \n    Parameters:\n    val1 (int): The minimum possible value.\n    val2 (int): The maximum possible value.\n    N (int): The size of the array.\n    \n    Returns:\n    int: The mode of the array.\n    \n    Requirements:\n    - numpy\n    - random\n    - scipy.stats\n    \n    Example:\n    >>> f_773920(1, 10, 1000)\n    \"\"\"\n    arr = np.array([randint(val1, val2) for _ in range(N)])\n\n    mode = stats.mode(arr)[0][0]\n\n    return mode\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nrandom, matplotlib, numpy\n```python\nimport matplotlib.pyplot as plt\nfrom random import randint\nimport numpy as np\n\ndef f_773920(val1, val2, N):\n    \"\"\"\n    Generate a numpy array of N random integer values between val1 and val2, \n    then plot a histogram of the array.\n    \n    Parameters:\n    val1 (int): The minimum possible value.\n    val2 (int): The maximum possible value.\n    N (int): The size of the array.\n    \n    Requirements:\n    - matplotlib.pyplot\n    - random\n    - numpy\n    \n    Example:\n    >>> f_773920(1, 10, 1000)\n    \"\"\"\n    arr = np.array([randint(val1, val2) for _ in range(N)])\n\n    plt.hist(arr, bins=10, edgecolor='black')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nrandom, numpy, sklearn.preprocessing\n```python\nimport numpy as np\nfrom random import randint\nfrom sklearn.preprocessing import StandardScaler\n\ndef f_773920(val1, val2, N):\n    \"\"\"\n    Generate a 2D numpy array of N rows and 3 columns with random integer \n    values between val1 and val2, then standardize the features (columns) using \n    sklearn's StandardScaler and return the transformed array.\n    \n    Parameters:\n    val1 (int): The minimum possible value.\n    val2 (int): The maximum possible value.\n    N (int): The number of rows in the array.\n    \n    Returns:\n    ndarray: The standardized array.\n    \n    Requirements:\n    - numpy\n    - random\n    - sklearn.preprocessing.StandardScaler\n    \n    Example:\n    >>> f_773920(1, 10, 5)\n    \"\"\"\n    arr = np.array([[randint(val1, val2) for _ in range(3)] for _ in range(N)])\n\n    scaler = StandardScaler()\n    transformed_arr = scaler.fit_transform(arr)\n\n    return transformed_arr\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nrandom, numpy, pandas\n```python\nimport pandas as pd\nfrom random import randint\nimport numpy as np\n\n# Constants\nCOLUMNS = ['A', 'B', 'C']\n\ndef f_773920(val1, val2, N):\n    \"\"\"\n    Generate a pandas DataFrame of N rows and 3 columns with random integer \n    values between val1 and val2, then calculate and return the sum of each \n    column.\n    \n    Parameters:\n    val1 (int): The minimum possible value.\n    val2 (int): The maximum possible value.\n    N (int): The number of rows in the DataFrame.\n    \n    Returns:\n    Series: A pandas Series with the sum of each column.\n    \n    Requirements:\n    - pandas\n    - random\n    - numpy\n    \n    Example:\n    >>> f_773920(1, 10, 5)\n    \"\"\"\n    data = np.array([[randint(val1, val2) for _ in range(3)] for _ in range(N)])\n\n    df = pd.DataFrame(data, columns=COLUMNS)\n    column_sums = df.sum()\n\n    return column_sums\n```"}
{"seed": "{\"task_id\": 115592, \"prompt\": \"def f_115592(my_list, sep):\\n\\t\", \"suffix\": \"\\n\\treturn splitted\", \"canonical_solution\": \"splitted = [[]]\\n\\n\\tfor item in my_list:\\n\\n\\t\\tif item == sep:\\n\\n\\t\\t\\tsplitted.append([])\\n\\n\\t\\telse:\\n\\n\\t\\t\\tsplitted[-1].append(item)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(['ab', '-', 'c', '24v', '-', 'vd', '33', '-', '000'], '-') == [['ab'], ['c', '24v'], ['vd', '33'], ['000']]\\n\"], \"entry_point\": \"f_115592\", \"intent\": \"\\u0420\\u0430\\u0437\\u0431\\u0438\\u0442\\u044c \\u0441\\u043f\\u0438\\u0441\\u043e\\u043a `my_list` \\u043d\\u0430 \\u0441\\u043f\\u0438\\u0441\\u043e\\u043a \\u0441\\u043f\\u0438\\u0441\\u043a\\u043e\\u0432 `splitted` \\u043f\\u043e \\u0440\\u0430\\u0437\\u0434\\u0435\\u043b\\u0438\\u0442\\u0435\\u043b\\u044e `sep`\", \"library\": []}", "generation": "Scenario 1:\npandas, numpy, random, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import choice\nimport matplotlib.pyplot as plt\n\n# Constants\nSTUDENTS = ['Alice', 'Bob', 'Charlie', 'Dave', 'Eve']\nSUBJECTS = ['Maths', 'Physics', 'Chemistry', 'Biology', 'Computer Science']\n\ndef generate_student_report():\n    \"\"\"\n    Generate a report of student grades for a list of subjects. The grades are \n    randomly generated.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with grades for the students.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> report = generate_student_report()\n    >>> print(report)\n    >>> report.plot(kind='bar')\n    \"\"\"\n    report_data = []\n\n    for student in STUDENTS:\n        grades = [np.random.randint(50, 100) for _ in SUBJECTS]\n        report_data.append([student] + grades)\n\n    report_df = pd.DataFrame(report_data, columns=['Student'] + SUBJECTS)\n\n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nitertools, collections, re\n```python\nimport itertools\nimport collections\nimport re\n\n# Constants\nSEP = '-'\n\ndef split_and_count(my_list, sep):\n    \"\"\"\n    Split a list into a list of lists by a separator and return a dictionary \n    counting the occurrences of each item in the list.\n\n    Parameters:\n    my_list (list): The list to be splitted.\n    sep (str): The separator.\n\n    Returns:\n    dict: A dictionary with the count of each item.\n\n    Requirements:\n    - itertools\n    - collections\n    - re\n\n    Example:\n    >>> split_and_count(['ab', '-', 'c', '24v', '-', 'vd', '33', '-', '000'], '-')\n    \"\"\"\n    splitted = [list(group) for key, group in itertools.groupby(my_list, lambda x: x == sep) if not key]\n\n    flattened = [item for sublist in splitted for item in sublist]\n    count = collections.Counter(flattened)\n\n    return count\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nhashlib, binascii, os\n```python\nimport hashlib\nimport binascii\nimport os\n\ndef hash_password(password):\n    \"\"\"\n    Hash a password for storing.\n\n    Parameters:\n    password (str): The password to be hashed.\n\n    Returns:\n    str: The hashed password.\n\n    Requirements:\n    - hashlib\n    - binascii\n    - os\n\n    Example:\n    >>> hash_password('mypassword')\n    \"\"\"\n    salt = hashlib.sha256(os.urandom(60)).hexdigest().encode('ascii')\n    pwdhash = hashlib.pbkdf2_hmac('sha512', password.encode('utf-8'), salt, 100000)\n    pwdhash = binascii.hexlify(pwdhash)\n\n    return (salt + pwdhash).decode('ascii')\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, scipy.stats, matplotlib\n```python\nimport numpy as np\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\n\ndef plot_normal_distribution(mu, sigma):\n    \"\"\"\n    Plot a normal distribution with given mean and standard deviation.\n\n    Parameters:\n    mu (float): The mean of the distribution.\n    sigma (float): The standard deviation of the distribution.\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_normal_distribution(0, 1)\n    \"\"\"\n    x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n    plt.plot(x, stats.norm.pdf(x, mu, sigma))\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, sklearn.preprocessing, sklearn.model_selection, sklearn.linear_model\n```python\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\n# Constants\nDATA_PATH = 'data.csv'\n\ndef train_linear_model():\n    \"\"\"\n    Load a dataset from a CSV file, standardize it, split it into training and \n    test sets, and train a linear regression model.\n\n    Returns:\n    LinearRegression: The trained model.\n\n    Requirements:\n    - pandas\n    - sklearn.preprocessing\n    - sklearn.model_selection\n    - sklearn.linear_model\n\n    Example:\n    >>> model = train_linear_model()\n    \"\"\"\n    data = pd.read_csv(DATA_PATH)\n    X = data.iloc[:, :-1].values\n    y = data.iloc[:, -1].values\n\n    X = StandardScaler().fit_transform(X)\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    return model\n```"}
{"seed": "{\"task_id\": 158145, \"prompt\": \"def f_158145(M, N, val):\\n\\t\", \"suffix\": \"\\n\\treturn my_list\", \"canonical_solution\": \"my_list = [[val for _ in range(N)] for _ in range(M)]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(2, 3, 0.5) == [[0.5, 0.5, 0.5], [0.5, 0.5, 0.5]]\\n\", \"\\n    assert candidate(1, 1, None) == [[None]]\\n\"], \"entry_point\": \"f_158145\", \"intent\": \"\\u0421\\u0433\\u0435\\u043d\\u0435\\u0440\\u0438\\u0440\\u043e\\u0432\\u0430\\u0442\\u044c \\u0441\\u043f\\u0438\\u0441\\u043e\\u043a `my_list` \\u0440\\u0430\\u0437\\u043c\\u0435\\u0440\\u0430 `M`x`N` \\u0437\\u0430\\u043f\\u043e\\u043b\\u043d\\u0435\\u043d\\u043d\\u044b\\u0439 \\u0437\\u043d\\u0430\\u0447\\u0435\\u043d\\u0438\\u0435\\u043c `val`\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, matplotlib, sklearn\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\n\ndef create_and_cluster_dataframe(M, N, val):\n    \"\"\"\n    Create a pandas DataFrame of size MxN filled with a given value. Apply KMeans clustering \n    and visualize the clusters using matplotlib.\n    \n    Parameters:\n    M (int): Number of rows in the DataFrame.\n    N (int): Number of columns in the DataFrame.\n    val (int): The value to fill in the DataFrame.\n\n    Returns:\n    DataFrame: A pandas DataFrame that has been clustered.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.cluster.KMeans\n\n    Example:\n    >>> create_and_cluster_dataframe(5, 3, 10)\n    \"\"\"\n    df = pd.DataFrame(val, index=np.arange(M), columns=np.arange(N))\n\n    kmeans = KMeans(n_clusters=2)\n    df['Cluster'] = kmeans.fit_predict(df.values)\n\n    plt.scatter(df.index, df['Cluster'])\n    plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, scipy.stats, matplotlib\n```python\nimport numpy as np\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\n\ndef generate_and_plot_normal_distribution(M, N, val):\n    \"\"\"\n    Generate a normal distribution with M as mean and N as standard deviation, then \n    create a histogram with val bins and plot the distribution using matplotlib.\n\n    Parameters:\n    M (float): Mean of the normal distribution.\n    N (float): Standard deviation of the normal distribution.\n    val (int): The number of bins in the histogram.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Example:\n    >>> generate_and_plot_normal_distribution(0, 1, 10)\n    \"\"\"\n    data = np.random.normal(M, N, 1000)\n    histogram = np.histogram(data, bins=val)\n\n    plt.hist(data, bins=val, density=True)\n\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, M, N)\n    plt.plot(x, p, 'k', linewidth=2)\n\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, matplotlib, sklearn\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\ndef generate_and_fit_linear_data(M, N, val):\n    \"\"\"\n    Generate a linear data set with M points, a slope of N and a noise of val. Fit a \n    linear regression model to the data and plot the results.\n\n    Parameters:\n    M (int): Number of data points.\n    N (float): Slope of the data.\n    val (float): Noise of the data.\n\n    Returns:\n    LinearRegression: The fitted linear model.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.linear_model.LinearRegression\n\n    Example:\n    >>> generate_and_fit_linear_data(100, 2, 0.1)\n    \"\"\"\n    X = np.linspace(0, 10, M)\n    y = N*X + val*np.random.randn(M)\n\n    model = LinearRegression()\n    model.fit(X.reshape(-1, 1), y)\n\n    plt.scatter(X, y)\n    plt.plot(X, model.predict(X.reshape(-1, 1)), color='red')\n    plt.show()\n\n    return model\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, matplotlib, sklearn\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import KMeans\n\ndef generate_and_cluster_blobs(M, N, val):\n    \"\"\"\n    Generate M blobs with N features each. Apply KMeans clustering with val clusters and \n    plot the results.\n\n    Parameters:\n    M (int): Number of blobs.\n    N (int): Number of features.\n    val (int): Number of clusters.\n\n    Returns:\n    KMeans: The fitted KMeans model.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.datasets.make_blobs\n    - sklearn.cluster.KMeans\n\n    Example:\n    >>> generate_and_cluster_blobs(100, 2, 3)\n    \"\"\"\n    X, y = make_blobs(n_samples=M, n_features=N)\n\n    kmeans = KMeans(n_clusters=val)\n    kmeans.fit(X)\n\n    plt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_)\n    plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red')\n    plt.show()\n\n    return kmeans\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, matplotlib, sklearn\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_regression\nfrom sklearn.linear_model import LinearRegression\n\ndef generate_and_fit_regression_data(M, N, val):\n    \"\"\"\n    Generate a regression data set with M samples and N features. Fit a \n    linear regression model to the data and plot the results.\n\n    Parameters:\n    M (int): Number of samples.\n    N (int): Number of features.\n    val (int): Noise of the data.\n\n    Returns:\n    LinearRegression: The fitted linear model.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.datasets.make_regression\n    - sklearn.linear_model.LinearRegression\n\n    Example:\n    >>> generate_and_fit_regression_data(100, 1, 10)\n    \"\"\"\n    X, y = make_regression(n_samples=M, n_features=N, noise=val)\n\n    model = LinearRegression()\n    model.fit(X, y)\n\n    plt.scatter(X, y)\n    plt.plot(X, model.predict(X), color='red')\n    plt.show()\n\n    return model\n```"}
{"seed": "{\"task_id\": 592250, \"prompt\": \"def f_592250(n, f):\\n\\t\", \"suffix\": \"\\n\\treturn s\", \"canonical_solution\": \"s = 0\\n\\n\\tfor x in range(1, n+ 1):\\n\\n\\t\\ts += f(x)\\n\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    def f(a): return a + 1\\n    assert candidate(2, f) == 5\\n\", \"\\n    def g(a): return a * a + 1\\n    assert candidate(5, g) == 60\\n\"], \"entry_point\": \"f_592250\", \"intent\": \"\\u041f\\u043e\\u0441\\u0447\\u0438\\u0442\\u0430\\u0442\\u044c \\u0441\\u0443\\u043c\\u043c\\u0443 `n` \\u0447\\u043b\\u0435\\u043d\\u043e\\u0432 \\u0430\\u0440\\u0438\\u0444\\u043c\\u0435\\u0442\\u0438\\u0447\\u0435\\u0441\\u043a\\u043e\\u0439 \\u043f\\u0440\\u043e\\u0433\\u0440\\u0435\\u0441\\u0441\\u0438\\u0438 \\u0441 \\u0447\\u043b\\u0435\\u043d\\u043e\\u043c \\u0437\\u0430\\u0434\\u0430\\u043d\\u043d\\u044b\\u043c \\u0444\\u0443\\u043d\\u043a\\u0446\\u0438\\u0435\\u0439 `f(x)`\", \"library\": []}", "generation": "Scenario 1:\npandas, numpy, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nDATA = {\n    'A': np.random.rand(5),\n    'B': np.random.rand(5),\n    'C': np.random.rand(5),\n    'D': np.random.rand(5),\n}\n\ndef calculate_and_plot_sum(df, f):\n    \"\"\"\n    Calculate the sum of each column in a pandas DataFrame using a supplied function \n    and plot the results.\n\n    Parameters:\n    df (DataFrame): The input DataFrame.\n    f (function): The function to apply to each column's data.\n\n    Returns:\n    DataFrame: A DataFrame with the sum of each column.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame(DATA)\n    >>> sum_df = calculate_and_plot_sum(df, np.sum)\n    >>> print(sum_df)\n    >>> sum_df.plot(kind='bar')\n    \"\"\"\n    sum_data = {col: f(df[col]) for col in df.columns}\n    sum_df = pd.DataFrame(sum_data, index=['Sum'])\n\n    sum_df.plot(kind='bar', ylim=(0, sum_df.values.max() + 1))\n\n    return sum_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, matplotlib, random\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\n\n# Constants\nCOLORS = ['red', 'blue', 'green', 'yellow', 'purple', 'cyan']\n\ndef plot_random_walk(n, f):\n    \"\"\"\n    Simulate and plot a random walk of n steps using a function f to determine \n    the step size.\n\n    Parameters:\n    n (int): The number of steps.\n    f (function): The function to determine step size.\n\n    Returns:\n    list: The random walk.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> random_walk = plot_random_walk(100, np.random.rand)\n    \"\"\"\n    steps = f(n)\n    positions = np.cumsum(steps)\n    plt.plot(positions, color=random.choice(COLORS))\n\n    return positions.tolist()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n\n# Constants\nDATA = {\n    'A': np.random.rand(5),\n    'B': np.random.rand(5),\n    'C': np.random.rand(5),\n    'D': np.random.rand(5),\n}\n\ndef calculate_and_plot_correlation(df, f):\n    \"\"\"\n    Calculate the correlation of each column in a pandas DataFrame using a supplied function \n    and plot the results.\n\n    Parameters:\n    df (DataFrame): The input DataFrame.\n    f (function): The function to apply to each column's data.\n\n    Returns:\n    DataFrame: A DataFrame with the correlation of each column.\n\n    Requirements:\n    - pandas\n    - numpy\n    - seaborn\n\n    Example:\n    >>> df = pd.DataFrame(DATA)\n    >>> corr_df = calculate_and_plot_correlation(df, np.corrcoef)\n    >>> print(corr_df)\n    >>> sns.heatmap(corr_df)\n    \"\"\"\n    corr_data = {col: f(df[col]) for col in df.columns}\n    corr_df = pd.DataFrame(corr_data)\n\n    sns.heatmap(corr_df)\n\n    return corr_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nmath, matplotlib, random\n```python\nimport math\nimport matplotlib.pyplot as plt\nimport random\n\n# Constants\nCOLORS = ['red', 'blue', 'green', 'yellow', 'purple', 'cyan']\n\ndef plot_math_function(n, f):\n    \"\"\"\n    Plot a mathematical function f for n points and randomly select a color for the plot.\n\n    Parameters:\n    n (int): The number of points.\n    f (function): The mathematical function to plot.\n\n    Returns:\n    None\n\n    Requirements:\n    - math\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> plot_math_function(100, math.sin)\n    \"\"\"\n    x = range(n)\n    y = [f(i) for i in x]\n    plt.plot(x, y, color=random.choice(COLORS))\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, pandas, seaborn\n```python\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n# Constants\nDATA = {\n    'A': np.random.rand(5),\n    'B': np.random.rand(5),\n    'C': np.random.rand(5),\n    'D': np.random.rand(5),\n}\n\ndef calculate_and_plot_kde(df, f):\n    \"\"\"\n    Calculate the Kernel Density Estimate (KDE) of each column in a pandas DataFrame \n    using a supplied function and plot the results.\n\n    Parameters:\n    df (DataFrame): The input DataFrame.\n    f (function): The function to apply to each column's data.\n\n    Returns:\n    DataFrame: A DataFrame with the KDE of each column.\n\n    Requirements:\n    - pandas\n    - numpy\n    - seaborn\n\n    Example:\n    >>> df = pd.DataFrame(DATA)\n    >>> kde_df = calculate_and_plot_kde(df, np.histogram)\n    >>> print(kde_df)\n    >>> sns.kdeplot(data=kde_df)\n    \"\"\"\n    kde_data = {col: f(df[col]) for col in df.columns}\n    kde_df = pd.DataFrame(kde_data)\n\n    sns.kdeplot(data=kde_df)\n\n    return kde_df\n```\n"}
{"seed": "{\"task_id\": 592250, \"prompt\": \"def f_592250(n, f):\\n\\t\", \"suffix\": \"\\n\\treturn s\", \"canonical_solution\": \"s = 0\\n\\n\\tx = 0\\n\\n\\twhile True:\\n\\n\\t\\tx+= 1\\n\\n\\t\\ts += f(x)\\n\\n\\t\\tif x == n:\\n\\n\\t\\t\\tbreak\\n\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    def f(a): return a + 1\\n    assert candidate(2, f) == 5\\n\", \"\\n    def g(a): return a * a + 1\\n    assert candidate(5, g) == 60\\n\"], \"entry_point\": \"f_592250\", \"intent\": \"\\u041f\\u043e\\u0441\\u0447\\u0438\\u0442\\u0430\\u0442\\u044c \\u0441\\u0443\\u043c\\u043c\\u0443 `n` \\u0447\\u043b\\u0435\\u043d\\u043e\\u0432 \\u0430\\u0440\\u0438\\u0444\\u043c\\u0435\\u0442\\u0438\\u0447\\u0435\\u0441\\u043a\\u043e\\u0439 \\u043f\\u0440\\u043e\\u0433\\u0440\\u0435\\u0441\\u0441\\u0438\\u0438 \\u0441 \\u0447\\u043b\\u0435\\u043d\\u043e\\u043c \\u0437\\u0430\\u0434\\u0430\\u043d\\u043d\\u044b\\u043c \\u0444\\u0443\\u043d\\u043a\\u0446\\u0438\\u0435\\u0439 `f(x)`\", \"library\": []}", "generation": "Scenario 1:\npandas, numpy, matplotlib, math\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\n\ndef generate_sin_wave(n, f):\n    \"\"\"\n    Generate a sin wave and plot it using matplotlib. The sin wave is the sum of n \n    elements defined by the mathematical function f(x).\n\n    Parameters:\n    n (int): Number of elements.\n    f (function): Mathematical function to apply on each element.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the sin wave data.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - math\n\n    Example:\n    >>> def f(x): return math.sin(x)\n    >>> wave = generate_sin_wave(100, f)\n    >>> wave.plot()\n    \"\"\"\n    x = np.arange(n)\n    y = f(x)\n    wave_df = pd.DataFrame({'x': x, 'y': y})\n    plt.plot(wave_df['x'], wave_df['y'])\n    return wave_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, matplotlib, sklearn, math\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nimport math\n\ndef fit_sin_wave(n, f):\n    \"\"\"\n    Generate a sin wave with n elements defined by the function f(x), fit a linear regression model \n    to it and plot both the original sin wave and the regression line.\n\n    Parameters:\n    n (int): Number of elements.\n    f (function): Mathematical function to apply on each element.\n\n    Returns:\n    LinearRegression: Fitted LinearRegression model.\n    \n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.linear_model.LinearRegression\n    - math\n\n    Example:\n    >>> def f(x): return math.sin(x)\n    >>> model = fit_sin_wave(100, f)\n    >>> print(model.coef_, model.intercept_)\n    \"\"\"\n    x = np.arange(n).reshape(-1,1)\n    y = f(x)\n    model = LinearRegression().fit(x, y)\n    plt.scatter(x, y, color='blue')\n    plt.plot(x, model.predict(x), color='red')\n    return model\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, scipy, matplotlib, math\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import curve_fit\nimport math\n\ndef fit_sin_wave_optimize(n, f):\n    \"\"\"\n    Generate a sin wave with n elements defined by the function f(x), fit a curve to it using \n    scipy's curve_fit function and plot both the original sin wave and the fitted curve.\n\n    Parameters:\n    n (int): Number of elements.\n    f (function): Mathematical function to apply on each element.\n\n    Returns:\n    ndarray: Optimal values for the parameters so that the sum of the squared residuals of \n    f(xdata, *popt) - ydata is minimized.\n    \n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - scipy.optimize.curve_fit\n    - math\n\n    Example:\n    >>> def f(x): return math.sin(x)\n    >>> popt = fit_sin_wave_optimize(100, f)\n    >>> print(popt)\n    \"\"\"\n    x = np.arange(n)\n    y = f(x)\n    popt, pcov = curve_fit(f, x, y)\n    plt.scatter(x, y, color='blue')\n    plt.plot(x, f(x, *popt), color='red')\n    return popt\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, pandas, seaborn, math\n```python\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport math\n\ndef generate_heat_map(n, f):\n    \"\"\"\n    Generate a heat map with n x n elements defined by the function f(x, y).\n\n    Parameters:\n    n (int): Number of elements in one dimension.\n    f (function): Mathematical function to apply on each element.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the heat map data.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - seaborn\n    - math\n\n    Example:\n    >>> def f(x, y): return math.sin(x) * math.cos(y)\n    >>> heat_map = generate_heat_map(100, f)\n    >>> sns.heatmap(heat_map)\n    \"\"\"\n    x = np.arange(n)\n    y = np.arange(n)\n    data = [[f(i, j) for j in y] for i in x]\n    heat_map_df = pd.DataFrame(data)\n    sns.heatmap(heat_map_df)\n    return heat_map_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, seaborn, math\n```python\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport math\n\ndef generate_density_plot(n, f):\n    \"\"\"\n    Generate a density plot with n elements defined by the function f(x).\n\n    Parameters:\n    n (int): Number of elements.\n    f (function): Mathematical function to apply on each element.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the density plot data.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - seaborn\n    - math\n\n    Example:\n    >>> def f(x): return math.sin(x)\n    >>> density = generate_density_plot(100, f)\n    >>> sns.kdeplot(density['y'])\n    \"\"\"\n    x = np.arange(n)\n    y = f(x)\n    density_df = pd.DataFrame({'x': x, 'y': y})\n    sns.kdeplot(density_df['y'])\n    return density_df\n```"}
{"seed": "{\"task_id\": 419321, \"prompt\": \"def f_419321(old_date, date_format):\\n\\t\", \"suffix\": \"\\n\\treturn new_date\", \"canonical_solution\": \"new_date = datetime.strptime(old_date, date_format)\", \"test_start\": \"\\nfrom datetime import datetime\\n\\ndef check(candidate):\", \"test\": [\"\\n    time_data = \\\"25/05/99 02:35:5.523\\\"\\n    format_data = \\\"%d/%m/%y %H:%M:%S.%f\\\"\\n    date = candidate(time_data, format_data)\\n    assert date.microsecond == 523000\\n\"], \"entry_point\": \"f_419321\", \"intent\": \"\\u041f\\u0440\\u0435\\u043e\\u0431\\u0440\\u0430\\u0437\\u043e\\u0432\\u0430\\u0442\\u044c \\u0441\\u0442\\u0440\\u043e\\u043a\\u043e\\u0432\\u043e\\u0435 \\u043f\\u0440\\u0435\\u0434\\u0441\\u0442\\u0432\\u043b\\u0435\\u043d\\u0438\\u0435 \\u0434\\u0430\\u0442\\u044b `old_date` \\u0432 \\u0441\\u0442\\u0440\\u043e\\u043a\\u043e\\u0432\\u043e\\u0435 \\u043f\\u0440\\u0435\\u0434\\u0441\\u0442\\u0430\\u0432\\u043b\\u0435\\u043d\\u0438\\u0435 `new_date` \\u0444\\u043e\\u0440\\u043c\\u0430\\u0442\\u0430 `date_format` \\u0441 \\u0438\\u0441\\u043f\\u043e\\u043b\\u044c\\u0437\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435\\u043c \\u0431\\u0438\\u0431\\u043b\\u0438\\u043e\\u0442\\u0435\\u043a\\u0438 datetime \", \"library\": [\"datetime\"]}", "generation": "Scenario 1: \npandas, re, datetime, matplotlib\n```python\nimport pandas as pd\nimport re\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\n\n# Constants\nTIME_FORMATS = {\n    'Format 1': '%Y/%m/%d %H:%M:%S',\n    'Format 2': '%d-%m-%Y %H:%M',\n    'Format 3': '%m-%d-%Y %I:%M %p'\n}\n\n# Regular expressions for the formats\nREGEX_FORMATS = {\n    'Format 1': r'\\d{4}/\\d{2}/\\d{2} \\d{2}:\\d{2}:\\d{2}',\n    'Format 2': r'\\d{2}-\\d{2}-\\d{4} \\d{2}:\\d{2}',\n    'Format 3': r'\\d{2}-\\d{2}-\\d{4} \\d{2}:\\d{2} (AM|PM)'\n}\n\ndef parse_dates(data):\n    \"\"\"\n    Parse dates in various formats in a DataFrame and convert them into a standard format.\n\n    Parameters:\n    data (DataFrame): The DataFrame containing the dates.\n\n    Returns:\n    DataFrame: The DataFrame with the parsed dates.\n\n    Requirements:\n    - pandas\n    - re\n    - datetime\n    - matplotlib.pyplot\n\n    Example:\n    >>> data = pd.DataFrame({'Date': ['2023/06/15 12:00:00', '15-06-2023 12:00', '06-15-2023 12:00 PM']})\n    >>> parsed_data = parse_dates(data)\n    >>> print(parsed_data)\n    \"\"\"\n    for format_name, date_format in TIME_FORMATS.items():\n        regex = REGEX_FORMATS[format_name]\n        mask = data['Date'].str.contains(regex)\n        data.loc[mask, 'Date'] = pd.to_datetime(data.loc[mask, 'Date'], format=date_format)\n\n    return data\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, pandas, matplotlib, datetime\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\n\n# Constants\nFREQUENCIES = ['D', 'W', 'M', 'Q', 'Y']\n\ndef generate_time_series(start_date, end_date, frequency):\n    \"\"\"\n    Generate a time series data with random numbers between a start date and an end date \n    with a specified frequency.\n\n    Parameters:\n    start_date (str): The start date in \"yyyy-mm-dd\" format.\n    end_date (str): The end date in \"yyyy-mm-dd\" format.\n    frequency (str): The frequency of the time series data.\n\n    Returns:\n    DataFrame: A DataFrame with the time series data.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - datetime\n\n    Example:\n    >>> start_date = '2023-01-01'\n    >>> end_date = '2023-12-31'\n    >>> frequency = 'M'\n    >>> time_series = generate_time_series(start_date, end_date, frequency)\n    >>> print(time_series)\n    >>> time_series.plot()\n    \"\"\"\n    if frequency not in FREQUENCIES:\n        raise ValueError('Invalid frequency. Available frequencies are: D, W, M, Q, Y')\n\n    date_range = pd.date_range(start=start_date, end=end_date, freq=frequency)\n    data = np.random.rand(len(date_range))\n\n    time_series = pd.DataFrame(data=data, index=date_range, columns=['Value'])\n\n    return time_series\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, datetime, matplotlib, re\n```python\nimport pandas as pd\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nimport re\n\n# Constants\nDATE_FORMATS = {\n    'Format 1': '%Y/%m/%d %H:%M:%S',\n    'Format 2': '%d-%m-%Y %H:%M',\n    'Format 3': '%m-%d-%Y %I:%M %p'\n}\n\n# Regular expressions for the formats\nREGEX_FORMATS = {\n    'Format 1': r'\\d{4}/\\d{2}/\\d{2} \\d{2}:\\d{2}:\\d{2}',\n    'Format 2': r'\\d{2}-\\d{2}-\\d{4} \\d{2}:\\d{2}',\n    'Format 3': r'\\d{2}-\\d{2}-\\d{4} \\d{2}:\\d{2} (AM|PM)'\n}\n\ndef convert_dates(data):\n    \"\"\"\n    Convert dates in various formats in a DataFrame into a standard format.\n\n    Parameters:\n    data (DataFrame): The DataFrame containing the dates.\n\n    Returns:\n    DataFrame: The DataFrame with the converted dates.\n\n    Requirements:\n    - pandas\n    - datetime\n    - matplotlib.pyplot\n    - re\n\n    Example:\n    >>> data = pd.DataFrame({'Date': ['2023/06/15 12:00:00', '15-06-2023 12:00', '06-15-2023 12:00 PM']})\n    >>> converted_data = convert_dates(data)\n    >>> print(converted_data)\n    \"\"\"\n    for format_name, date_format in DATE_FORMATS.items():\n        regex = REGEX_FORMATS[format_name]\n        mask = data['Date'].str.contains(regex)\n        data.loc[mask, 'Date'] = pd.to_datetime(data.loc[mask, 'Date'], format=date_format)\n\n    return data\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, datetime, matplotlib, random\n```python\nimport pandas as pd\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nfrom random import choice\n\n# Constants\nCATEGORIES = ['Food', 'Transport', 'Leisure', 'Health', 'Education']\n\ndef generate_expense_report(start_date, end_date):\n    \"\"\"\n    Generate a report of daily expenses in various categories between a start date \n    and an end date.\n\n    Parameters:\n    start_date (str): The start date in \"yyyy-mm-dd\" format.\n    end_date (str): The end date in \"yyyy-mm-dd\" format.\n\n    Returns:\n    DataFrame: The DataFrame with the expense report.\n\n    Requirements:\n    - pandas\n    - datetime\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> start_date = '2023-01-01'\n    >>> end_date = '2023-12-31'\n    >>> report = generate_expense_report(start_date, end_date)\n    >>> print(report)\n    >>> report.groupby('Category').sum().plot(kind='bar')\n    \"\"\"\n    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n    expenses = [choice(range(1, 101)) for _ in range(len(date_range))]\n    categories = [choice(CATEGORIES) for _ in range(len(date_range))]\n\n    report = pd.DataFrame({\n        'Date': date_range,\n        'Expense': expenses,\n        'Category': categories\n    })\n\n    return report\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnumpy, pandas, datetime\n```python\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\n\n# Constants\nFREQUENCIES = ['D', 'W', 'M', 'Q', 'Y']\n\ndef generate_random_data(start_date, end_date, frequency):\n    \"\"\"\n    Generate a DataFrame with random data between a start date and an end date with a specified frequency.\n\n    Parameters:\n    start_date (str): The start date in \"yyyy-mm-dd\" format.\n    end_date (str): The end date in \"yyyy-mm-dd\" format.\n    frequency (str): The frequency of the data.\n\n    Returns:\n    DataFrame: A DataFrame with the random data.\n\n    Requirements:\n    - numpy\n    - pandas\n    - datetime\n\n    Example:\n    >>> start_date = '2023-01-01'\n    >>> end_date = '2023-12-31'\n    >>> frequency = 'M'\n    >>> data = generate_random_data(start_date, end_date, frequency)\n    >>> print(data)\n    \"\"\"\n    if frequency not in FREQUENCIES:\n        raise ValueError('Invalid frequency. Available frequencies are: D, W, M, Q, Y')\n\n    date_range = pd.date_range(start=start_date, end=end_date, freq=frequency)\n    data = np.random.rand(len(date_range))\n\n    df = pd.DataFrame(data=data, index=date_range, columns=['Value'])\n\n    return df\n```"}
{"seed": "{\"task_id\": 419321, \"prompt\": \"def f_419321(old_date, date_format):\\n\\t\", \"suffix\": \"\\n\\treturn new_date\", \"canonical_solution\": \"locale.setlocale(locale.LC_TIME, 'ru_RU.UTF-8')\\n\\n\\tnew_date = datetime.strptime(old_date, date_format).date()\", \"test_start\": \"\\nimport locale\\nfrom datetime import datetime\\nfrom unittest.mock import Mock\\n\\ndef check(candidate):\", \"test\": [\"\\n    locale.setlocale =Mock()\\n    time_data = \\\"25/05/99 02:35:5.523\\\"\\n    format_data = \\\"%d/%m/%y %H:%M:%S.%f\\\"\\n    date = candidate(time_data, format_data)\\n    assert str(date) == '1999-05-25'\\n\"], \"entry_point\": \"f_419321\", \"intent\": \"\\u041f\\u0440\\u0435\\u043e\\u0431\\u0440\\u0430\\u0437\\u043e\\u0432\\u0430\\u0442\\u044c \\u0441\\u0442\\u0440\\u043e\\u043a\\u043e\\u0432\\u043e\\u0435 \\u043f\\u0440\\u0435\\u0434\\u0441\\u0442\\u0432\\u043b\\u0435\\u043d\\u0438\\u0435 \\u0434\\u0430\\u0442\\u044b `old_date` \\u0432 \\u0441\\u0442\\u0440\\u043e\\u043a\\u043e\\u0432\\u043e\\u0435 \\u043f\\u0440\\u0435\\u0434\\u0441\\u0442\\u0430\\u0432\\u043b\\u0435\\u043d\\u0438\\u0435 `new_date` \\u0444\\u043e\\u0440\\u043c\\u0430\\u0442\\u0430 `date_format` \\u0441 \\u0438\\u0441\\u043f\\u043e\\u043b\\u044c\\u0437\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435\\u043c \\u0431\\u0438\\u0431\\u043b\\u0438\\u043e\\u0442\\u0435\\u043a\\u0438 datetime \", \"library\": [\"datetime\", \"locale\"]}", "generation": "Scenario 3: \npandas, datetime, locale, matplotlib.pyplot\n```python\nimport pandas as pd\nfrom datetime import datetime\nimport locale\nimport matplotlib.pyplot as plt\n\n# Constants\nCOUNTRY_CODES = {\n    'Russia': 'ru_RU',\n    'Germany': 'de_DE',\n    'France': 'fr_FR',\n    'Spain': 'es_ES',\n    'Italy': 'it_IT'\n}\n\ndef plot_date_distribution(data, date_format, country):\n    \"\"\"\n    Plot a histogram of dates from a pandas DataFrame column, after converting the dates to a \n    specific format and locale.\n\n    Parameters:\n    data (DataFrame): The pandas DataFrame.\n    date_format (str): The date format string.\n    country (str): The country code.\n\n    Returns:\n    None: The function should plot a histogram.\n\n    Requirements:\n    - pandas\n    - datetime\n    - locale\n    - matplotlib.pyplot\n\n    Example:\n    >>> data = pd.DataFrame({'dates': ['01/01/2000', '01/02/2000', '02/03/2000', '04/05/2000', '06/07/2000']})\n    >>> plot_date_distribution(data, '%d/%m/%Y', 'Russia')\n    \"\"\"\n    locale.setlocale(locale.LC_TIME, COUNTRY_CODES[country])\n\n    data['parsed_dates'] = data['dates'].apply(lambda x: datetime.strptime(x, date_format).date())\n    data['parsed_dates'].hist()\n    plt.gca().set(title='Date Distribution', ylabel='Frequency')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \npandas, datetime, locale, matplotlib.pyplot, random\n```python\nimport pandas as pd\nfrom datetime import datetime\nimport locale\nimport matplotlib.pyplot as plt\nfrom random import choice\n\n# Constants\nCOUNTRIES = ['Russia', 'Germany', 'France', 'Spain', 'Italy']\nCOUNTRY_CODES = {\n    'Russia': 'ru_RU',\n    'Germany': 'de_DE',\n    'France': 'fr_FR',\n    'Spain': 'es_ES',\n    'Italy': 'it_IT'\n}\n\ndef plot_country_date_distribution(data, date_column, country_column):\n    \"\"\"\n    Plot a histogram of dates from a pandas DataFrame column for each country, after converting \n    the dates to a specific locale.\n\n    Parameters:\n    data (DataFrame): The pandas DataFrame.\n    date_column (str): The column name containing dates.\n    country_column (str): The column name containing country names.\n\n    Returns:\n    None: The function should plot a histogram for each country.\n\n    Requirements:\n    - pandas\n    - datetime\n    - locale\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> data = pd.DataFrame({'dates': ['01/01/2000', '01/02/2000', '02/03/2000', '04/05/2000', '06/07/2000'],\n    >>>                      'country': ['Russia', 'Germany', 'France', 'Spain', 'Italy']})\n    >>> plot_country_date_distribution(data, 'dates', 'country')\n    \"\"\"\n    for country in COUNTRIES:\n        country_data = data[data[country_column] == country]\n        if not country_data.empty:\n            date_format = '%d/%m/%Y' if country != 'USA' else '%m/%d/%Y'\n            locale.setlocale(locale.LC_TIME, COUNTRY_CODES[country])\n\n            country_data['parsed_dates'] = country_data[date_column].apply(lambda x: datetime.strptime(x, date_format).date())\n            country_data['parsed_dates'].hist()\n            plt.gca().set(title=f'Date Distribution - {country}', ylabel='Frequency')\n            plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \npandas, datetime, locale, matplotlib.pyplot, random\n```python\nimport pandas as pd\nfrom datetime import datetime\nimport locale\nimport matplotlib.pyplot as plt\nfrom random import choice, randint\n\n# Constants\nCOUNTRIES = ['Russia', 'Germany', 'France', 'Spain', 'Italy']\nCOUNTRY_CODES = {\n    'Russia': 'ru_RU',\n    'Germany': 'de_DE',\n    'France': 'fr_FR',\n    'Spain': 'es_ES',\n    'Italy': 'it_IT'\n}\n\ndef generate_and_plot_date_distribution(start_date, end_date, num_dates, countries=COUNTRIES):\n    \"\"\"\n    Generate a random distribution of dates between two dates and plot a histogram of these dates \n    for each country, after converting the dates to a specific locale.\n\n    Parameters:\n    start_date (str): The start date string in \"dd/mm/yyyy\" format.\n    end_date (str): The end date string in \"dd/mm/yyyy\" format.\n    num_dates (int): The number of random dates to generate.\n    countries (list, optional): The list of countries. Default is ['Russia', 'Germany', 'France', 'Spain', 'Italy'].\n\n    Returns:\n    None: The function should plot a histogram for each country.\n\n    Requirements:\n    - pandas\n    - datetime\n    - locale\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> generate_and_plot_date_distribution('01/01/2000', '01/01/2021', 1000)\n    \"\"\"\n    start_date = datetime.strptime(start_date, \"%d/%m/%Y\")\n    end_date = datetime.strptime(end_date, \"%d/%m/%Y\")\n    date_delta = end_date - start_date\n\n    random_dates = [start_date + timedelta(days=randint(0, date_delta.days)) for _ in range(num_dates)]\n    random_countries = [choice(countries) for _ in range(num_dates)]\n    \n    data = pd.DataFrame({'dates': random_dates, 'country': random_countries})\n\n    for country in countries:\n        country_data = data[data['country'] == country]\n        if not country_data.empty:\n            locale.setlocale(locale.LC_TIME, COUNTRY_CODES[country])\n\n            country_data['dates'].hist()\n            plt.gca().set(title=f'Date Distribution - {country}', ylabel='Frequency')\n            plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 6: \npandas, datetime, locale, matplotlib.pyplot, random\n```python\nimport pandas as pd\nfrom datetime import datetime\nimport locale\nimport matplotlib.pyplot as plt\nfrom random import choice\n\n# Constants\nCOUNTRIES = ['Russia', 'Germany', 'France', 'Spain', 'Italy']\nCOUNTRY_CODES = {\n    'Russia': 'ru_RU',\n    'Germany': 'de_DE',\n    'France': 'fr_FR',\n    'Spain': 'es_ES',\n    'Italy': 'it_IT'\n}\n\ndef plot_date_distribution_with_locale(data, date_column, country_column, date_format):\n    \"\"\"\n    Plot a histogram of dates from a pandas DataFrame column for each country, after converting \n    the dates to a specific format and locale.\n\n    Parameters:\n    data (DataFrame): The pandas DataFrame.\n    date_column (str): The column name containing dates.\n    country_column (str): The column name containing country names.\n    date_format (str): The date format string.\n\n    Returns:\n    None: The function should plot a histogram for each country.\n\n    Requirements:\n    - pandas\n    - datetime\n    - locale\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> data = pd.DataFrame({'dates': ['01/01/2000', '01/02/2000', '02/03/2000', '04/05/2000', '06/07/2000'],\n    >>>                      'country': ['Russia', 'Germany', 'France', 'Spain', 'Italy']})\n    >>> plot_date_distribution_with_locale(data, 'dates', 'country', '%d/%m/%Y')\n    \"\"\"\n    for country in COUNTRIES:\n        country_data = data[data[country_column] == country]\n        if not country_data.empty:\n            locale.setlocale(locale.LC_TIME, COUNTRY_CODES[country])\n\n            country_data['parsed_dates'] = country_data[date_column].apply(lambda x: datetime.strptime(x, date_format).date())\n            country_data['parsed_dates'].hist()\n            plt.gca().set(title=f'Date Distribution - {country}', ylabel='Frequency')\n            plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 7: \npandas, datetime, locale, matplotlib.pyplot, random\n```python\nimport pandas as pd\nfrom datetime import datetime\nimport locale\nimport matplotlib.pyplot as plt\nfrom random import choice\n\n# Constants\nCOUNTRIES = ['Russia', 'Germany', 'France', 'Spain', 'Italy']\nCOUNTRY_CODES = {\n    'Russia': 'ru_RU',\n    'Germany': 'de_DE',\n    'France': 'fr_FR',\n    'Spain': 'es_ES',\n    'Italy': 'it_IT'\n}\n\ndef plot_date_distribution_for_countries(data, date_column, country_column, date_format):\n    \"\"\"\n    Plot a histogram of dates from a pandas DataFrame column for each country, after converting \n    the dates to a specific format and locale.\n\n    Parameters:\n    data (DataFrame): The pandas DataFrame.\n    date_column (str): The column name containing dates.\n    country_column (str): The column name containing country names.\n    date_format (str): The date format string.\n\n    Returns:\n    None: The function should plot a histogram for each country.\n\n    Requirements:\n    - pandas\n    - datetime\n    - locale\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> data = pd.DataFrame({'dates': ['01/01/2000', '01/02/2000', '02/03/2000', '04/05/2000', '06/07/2000'],\n    >>>                      'country': ['Russia', 'Germany', 'France', 'Spain', 'Italy']})\n    >>> plot_date_distribution_for_countries(data, 'dates', 'country', '%d/%m/%Y')\n    \"\"\"\n    for country in COUNTRIES:\n        country_data = data[data[country_column] == country]\n        if not country_data.empty:\n            locale.setlocale(locale.LC_TIME, COUNTRY_CODES[country])\n\n            country_data['parsed_dates'] = country_data[date_column].apply(lambda x: datetime.strptime(x, date_format).date())\n            country_data['parsed_dates'].hist()\n            plt.gca().set(title=f'Date Distribution - {country}', ylabel='Frequency')\n            plt.show()\n```\n"}
{"seed": "{\"task_id\": 404985, \"prompt\": \"def f_404985(sub, s):\\n\\t\", \"suffix\": \"\\n\\treturn count\", \"canonical_solution\": \"count = 0\\n\\n\\ti = -1\\n\\n\\twhile True:\\n\\n\\t\\ti = s.find(sub, i+1)\\n\\n\\t\\tif i == -1:\\n\\n\\t\\t\\tbreak\\n\\n\\t\\tcount += 1\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('ab', 'abcccccabcca') == 2\\n\"], \"entry_point\": \"f_404985\", \"intent\": \"\\u041d\\u0430\\u0439\\u0442\\u0438 \\u043a\\u043e\\u043b\\u0438\\u0447\\u0435\\u0441\\u0442\\u0432\\u043e \\u043f\\u0435\\u0440\\u0435\\u043a\\u0440\\u044b\\u0432\\u0430\\u044e\\u0449\\u0438\\u0445 \\u0432\\u0445\\u043e\\u0436\\u0434\\u0435\\u043d\\u0438\\u0439 `count` \\u043f\\u043e\\u0434\\u0441\\u0442\\u0440\\u043e\\u043a\\u0438 `sub` \\u0432 \\u0441\\u0442\\u0440\\u043e\\u043a\\u0443 `s`\", \"library\": []}", "generation": "Scenario 1:\ncollections, re, string\n```python\nimport collections\nimport re\nimport string\n\n# Constants\nLETTERS = string.ascii_lowercase\n\ndef count_substring_occurrences_in_words(sub, s):\n    \"\"\"\n    Count the number of occurrences of a substring `sub` in each word of a string `s` \n    and return a dictionary with the words as keys and the counts as values. \n    Only consider words that consist of lowercase English letters.\n\n    Parameters:\n    sub (str): The substring to be counted.\n    s (str): The string in which to count the occurrences of `sub`.\n\n    Returns:\n    dict: A dictionary with words as keys and counts as values.\n\n    Requirements:\n    - collections\n    - re\n    - string\n\n    Example:\n    >>> count_substring_occurrences_in_words('ab', 'abcccccabcca abc ab ab')\n    \"\"\"\n    word_counts = collections.defaultdict(int)\n\n    words = re.findall(r'\\b[' + LETTERS + r']+\\b', s)\n\n    for word in words:\n        word_counts[word] += word.count(sub)\n\n    return dict(word_counts)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nitertools, re, string\n```python\nimport itertools\nimport re\nimport string\n\n# Constants\nLETTERS = string.ascii_lowercase\n\ndef get_substring_combinations(sub, s):\n    \"\"\"\n    Find all combinations of a substring `sub` in a string `s` and return a list of the combinations.\n    Only consider words that consist of lowercase English letters.\n\n    Parameters:\n    sub (str): The substring to be considered.\n    s (str): The string in which to find the combinations of `sub`.\n\n    Returns:\n    list: A list of the combinations.\n\n    Requirements:\n    - itertools\n    - re\n    - string\n\n    Example:\n    >>> get_substring_combinations('ab', 'abcccccabcca abc ab ab')\n    \"\"\"\n    words = re.findall(r'\\b[' + LETTERS + r']+\\b', s)\n\n    combinations = []\n\n    for word in words:\n        if sub in word:\n            combinations.extend(itertools.combinations(word, len(sub)))\n\n    return [''.join(combo) for combo in combinations if ''.join(combo) == sub]\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, re, string, collections\n```python\nimport os\nimport re\nimport string\nimport collections\n\n# Constants\nLETTERS = string.ascii_lowercase\n\ndef count_substring_in_files(sub, dir_path):\n    \"\"\"\n    Count the number of occurrences of a substring `sub` in each file in a directory `dir_path` \n    and return a dictionary with the file names as keys and the counts as values. \n    Only consider words that consist of lowercase English letters.\n\n    Parameters:\n    sub (str): The substring to be counted.\n    dir_path (str): The directory path in which to count the occurrences of `sub`.\n\n    Returns:\n    dict: A dictionary with file names as keys and counts as values.\n\n    Requirements:\n    - os\n    - re\n    - string\n    - collections\n\n    Example:\n    >>> count_substring_in_files('ab', '/path/to/directory')\n    \"\"\"\n    file_counts = collections.defaultdict(int)\n\n    for filename in os.listdir(dir_path):\n        with open(os.path.join(dir_path, filename), 'r') as f:\n            content = f.read()\n            words = re.findall(r'\\b[' + LETTERS + r']+\\b', content)\n\n            for word in words:\n                file_counts[filename] += word.count(sub)\n\n    return dict(file_counts)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nre, string, pandas\n```python\nimport re\nimport string\nimport pandas as pd\n\n# Constants\nLETTERS = string.ascii_lowercase\n\ndef get_substring_count_df(sub, s):\n    \"\"\"\n    Count the number of occurrences of a substring `sub` in each word of a string `s` \n    and return a pandas DataFrame with the words and counts. \n    Only consider words that consist of lowercase English letters.\n\n    Parameters:\n    sub (str): The substring to be counted.\n    s (str): The string in which to count the occurrences of `sub`.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the words and counts.\n\n    Requirements:\n    - re\n    - string\n    - pandas\n\n    Example:\n    >>> get_substring_count_df('ab', 'abcccccabcca abc ab ab')\n    \"\"\"\n    word_counts = {}\n\n    words = re.findall(r'\\b[' + LETTERS + r']+\\b', s)\n\n    for word in words:\n        word_counts[word] = word.count(sub)\n\n    df = pd.DataFrame(list(word_counts.items()), columns=['Word', 'Count'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\njson, re, string, collections\n```python\nimport json\nimport re\nimport string\nimport collections\n\n# Constants\nLETTERS = string.ascii_lowercase\n\ndef count_substring_in_json(sub, json_path):\n    \"\"\"\n    Count the number of occurrences of a substring `sub` in each value of a JSON file at `json_path` \n    and return a dictionary with the keys and counts. \n    Only consider values that are strings and consist of lowercase English letters.\n\n    Parameters:\n    sub (str): The substring to be counted.\n    json_path (str): The path to the JSON file in which to count the occurrences of `sub`.\n\n    Returns:\n    dict: A dictionary with keys as keys and counts as values.\n\n    Requirements:\n    - json\n    - re\n    - string\n    - collections\n\n    Example:\n    >>> count_substring_in_json('ab', '/path/to/json/file.json')\n    \"\"\"\n    key_counts = collections.defaultdict(int)\n\n    with open(json_path, 'r') as f:\n        data = json.load(f)\n\n        for key, value in data.items():\n            if isinstance(value, str):\n                words = re.findall(r'\\b[' + LETTERS + r']+\\b', value)\n\n                for word in words:\n                    key_counts[key] += word.count(sub)\n\n    return dict(key_counts)\n```\n"}
{"seed": "{\"task_id\": 427051, \"prompt\": \"def f_427051(my_list):\\n\\t\", \"suffix\": \"\\n\\treturn my_list\", \"canonical_solution\": \"my_list.reverse()\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([1,2,3]) == [3,2,1]\\n\", \"\\n    assert candidate([1,3,2,3,1]) == [1,3,2,3,1]\\n\"], \"entry_point\": \"f_427051\", \"intent\": \"\\u041f\\u0435\\u0440\\u0435\\u0432\\u0435\\u0440\\u043d\\u0443\\u0442\\u044c \\u0441\\u043f\\u0438\\u0441\\u043e\\u043a `my_list`\", \"library\": []}", "generation": "Scenario 1:\npandas, numpy, random, matplotlib.pyplot\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nELEMENTS = ['H', 'He', 'Li', 'Be', 'B', 'C', 'N', 'O', 'F', 'Ne']\n\ndef generate_periodic_table_data(elements):\n    \"\"\"\n    Generate a DataFrame for a list of elements with their atomic number,\n    isotopic composition, and random abundance value.\n\n    Parameters:\n    elements (list): The list of elements.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the elements data.\n\n    Requirements:\n    - pandas\n    - numpy\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> elements_data = generate_periodic_table_data(ELEMENTS)\n    >>> print(elements_data)\n    >>> elements_data['Abundance'].plot(kind='bar')\n    \"\"\"\n    data = []\n\n    for i, element in enumerate(elements, 1):\n        isotopic_composition = np.random.random_sample()\n        abundance = randint(1, 100)\n        data.append([i, element, isotopic_composition, abundance])\n\n    df = pd.DataFrame(data, columns=['Atomic Number', 'Element', 'Isotopic Composition', 'Abundance'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncollections, itertools, random, re\n```python\nimport collections\nimport itertools\nimport random\nimport re\n\n# Constants\nLETTERS = 'abcdefghijklmnopqrstuvwxyz'\n\ndef generate_random_string(n):\n    \"\"\"\n    Generate a random string of n length and find the frequency of each letter\n    and the most common 3-letter sequence in the string.\n\n    Parameters:\n    n (int): The length of the string.\n\n    Returns:\n    dict, str: A dictionary with the frequency of each letter and the most common \n    3-letter sequence.\n\n    Requirements:\n    - collections\n    - itertools\n    - random\n    - re\n\n    Example:\n    >>> letter_freq, common_seq = generate_random_string(1000)\n    >>> print(letter_freq)\n    >>> print(common_seq)\n    \"\"\"\n    random_string = ''.join(random.choices(LETTERS, k=n))\n\n    letter_freq = collections.Counter(random_string)\n\n    seqs = [''.join(seq) for seq in itertools.product(LETTERS, repeat=3)]\n    seq_freq = {seq: len(re.findall(seq, random_string)) for seq in seqs}\n\n    common_seq = max(seq_freq, key=seq_freq.get)\n\n    return letter_freq, common_seq\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, scipy.stats, matplotlib.pyplot\n```python\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef generate_normal_distribution(n, loc=0, scale=1):\n    \"\"\"\n    Generate a normal distribution with n samples, calculate its skewness and \n    kurtosis, and plot its histogram.\n\n    Parameters:\n    n (int): The number of samples.\n    loc (float): The mean of the distribution.\n    scale (float): The standard deviation of the distribution.\n\n    Returns:\n    float, float: The skewness and kurtosis of the distribution.\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Example:\n    >>> skewness, kurtosis = generate_normal_distribution(1000)\n    >>> print(skewness)\n    >>> print(kurtosis)\n    \"\"\"\n    data = np.random.normal(loc=loc, scale=scale, size=n)\n\n    skewness = stats.skew(data)\n    kurtosis = stats.kurtosis(data)\n\n    plt.hist(data, bins=100)\n    plt.show()\n\n    return skewness, kurtosis\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, random, sklearn.preprocessing\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import randint\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef generate_and_scale_dataframe(n):\n    \"\"\"\n    Generate a DataFrame with n rows and 5 columns with random integer values,\n    and scale the values to range [0, 1] using MinMaxScaler.\n\n    Parameters:\n    n (int): The number of rows.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the scaled data.\n\n    Requirements:\n    - pandas\n    - numpy\n    - random\n    - sklearn.preprocessing\n\n    Example:\n    >>> scaled_df = generate_and_scale_dataframe(100)\n    >>> print(scaled_df)\n    \"\"\"\n    data = np.random.randint(1, 100, size=(n, 5))\n\n    df = pd.DataFrame(data, columns=COLUMNS)\n\n    scaler = MinMaxScaler()\n    df[COLUMNS] = scaler.fit_transform(df[COLUMNS])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, matplotlib.pyplot, sklearn.datasets, sklearn.linear_model\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_regression\nfrom sklearn.linear_model import LinearRegression\n\ndef generate_and_fit_regression(n):\n    \"\"\"\n    Generate a regression dataset with n samples and fit a linear regression model\n    to the data. Plot the data points and the regression line.\n\n    Parameters:\n    n (int): The number of samples.\n\n    Returns:\n    float, float: The slope and intercept of the regression line.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.datasets\n    - sklearn.linear_model\n\n    Example:\n    >>> slope, intercept = generate_and_fit_regression(100)\n    >>> print(slope)\n    >>> print(intercept)\n    \"\"\"\n    X, y = make_regression(n_samples=n, n_features=1, noise=0.1)\n\n    model = LinearRegression()\n    model.fit(X, y)\n\n    plt.scatter(X, y)\n    plt.plot(X, model.predict(X), color='red')\n    plt.show()\n\n    return model.coef_[0], model.intercept_\n```\n"}
{"seed": "{\"task_id\": 427051, \"prompt\": \"def f_427051(my_list):\\n\\t\", \"suffix\": \"\\n\\treturn my_list\", \"canonical_solution\": \"my_list = my_list[::-1]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([1,2,3]) == [3,2,1]\\n\", \"\\n    assert candidate([1,2,3,4,1]) == [1,4,3,2,1]\\n\"], \"entry_point\": \"f_427051\", \"intent\": \"\\u041f\\u0435\\u0440\\u0435\\u0432\\u0435\\u0440\\u043d\\u0443\\u0442\\u044c \\u0441\\u043f\\u0438\\u0441\\u043e\\u043a `my_list`\", \"library\": []}", "generation": "Scenario 1: \npandas, matplotlib, numpy, random\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom random import choice\n\n# Constants\nCATEGORIES = ['Electronics', 'Fashion', 'Home & Kitchen', 'Books', 'Toys & Games']\n\ndef generate_sales_report(my_list):\n    \"\"\"\n    Given a list of product sales, generate a sales report categorized by product \n    categories and plot a pie chart for the same.\n\n    Parameters:\n    my_list (list): A list of tuples where each tuple represents a product sale\n    in the format (category, sales_amount).\n\n    Returns:\n    DataFrame: A pandas DataFrame with sales report categorized by product categories.\n    \n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - numpy\n    - random\n\n    Example:\n    >>> sales_data = [(choice(CATEGORIES), np.random.randint(100, 1000)) for _ in range(100)]\n    >>> report = generate_sales_report(sales_data)\n    >>> print(report)\n    >>> report.plot(kind='pie', y='Sales Amount')\n    \"\"\"\n    sales_report = pd.DataFrame(my_list, columns=['Category', 'Sales Amount'])\n    sales_report = sales_report.groupby('Category').sum().reset_index()\n\n    return sales_report\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncollections, itertools, string, random\n```python\nfrom collections import Counter\nimport itertools\nimport string\nimport random\n\n# Constants\nLETTERS = string.ascii_lowercase\n\ndef count_letter_combinations(my_list):\n    \"\"\"\n    Given a list of strings, count the number of two-letter combinations \n    (case insensitive) that occur in the list.\n\n    Parameters:\n    my_list (list): A list of strings.\n\n    Returns:\n    dict: A dictionary with two-letter combinations as keys and their counts as values.\n\n    Requirements:\n    - collections\n    - itertools\n    - string\n    - random\n\n    Example:\n    >>> strings = [''.join(random.choices(LETTERS, k=5)) for _ in range(100)]\n    >>> combinations = count_letter_combinations(strings)\n    >>> print(combinations)\n    \"\"\"\n    combinations = Counter()\n\n    for word in my_list:\n        word = word.lower()\n        for combo in itertools.combinations(word, 2):\n            combinations[''.join(sorted(combo))] += 1\n    \n    return dict(combinations)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \npandas, json, random, string\n```python\nimport pandas as pd\nimport json\nimport random\nimport string\n\n# Constants\nLETTERS = string.ascii_letters\n\ndef extract_json_data(my_list):\n    \"\"\"\n    Given a list of JSON strings, extract the data into a pandas DataFrame.\n\n    Parameters:\n    my_list (list): A list of JSON strings.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the extracted data.\n\n    Requirements:\n    - pandas\n    - json\n    - random\n    - string\n\n    Example:\n    >>> json_strings = [json.dumps({letter: random.randint(0, 100) for letter in random.choices(LETTERS, k=5)}) for _ in range(10)]\n    >>> df = extract_json_data(json_strings)\n    >>> print(df)\n    \"\"\"\n    data = [json.loads(js) for js in my_list]\n    df = pd.DataFrame(data)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, scipy.stats, random, matplotlib\n```python\nimport numpy as np\nfrom scipy import stats\nimport random\nimport matplotlib.pyplot as plt\n\n# Constants\nN_SAMPLES = 1000\n\ndef generate_normal_distribution(my_list):\n    \"\"\"\n    Given a list of tuples representing mean and standard deviation, generate \n    a normal distribution for each tuple and plot the histogram.\n\n    Parameters:\n    my_list (list): A list of tuples where each tuple is in the format (mean, std_dev).\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> means_and_stds = [(random.randint(-10, 10), random.randint(1, 5)) for _ in range(5)]\n    >>> generate_normal_distribution(means_and_stds)\n    \"\"\"\n    for mean, std_dev in my_list:\n        samples = np.random.normal(mean, std_dev, N_SAMPLES)\n        plt.hist(samples, bins='auto', alpha=0.7, density=True)\n\n    plt.show()\n\n    return None\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnumpy, matplotlib, random\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\n\n# Constants\nN_POINTS = 100\n\ndef plot_scatter(my_list):\n    \"\"\"\n    Given a list of tuples representing x and y coordinates, plot a scatter plot.\n\n    Parameters:\n    my_list (list): A list of tuples where each tuple is in the format (x, y).\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> points = [(random.uniform(-10, 10), random.uniform(-10, 10)) for _ in range(N_POINTS)]\n    >>> plot_scatter(points)\n    \"\"\"\n    X, Y = zip(*my_list)\n\n    plt.scatter(X, Y)\n    plt.show()\n\n    return None\n```"}
{"seed": "{\"task_id\": 427051, \"prompt\": \"def f_427051(my_list):\\n\\t\", \"suffix\": \"\\n\\treturn my_list\", \"canonical_solution\": \"my_list = list(reversed(my_list))\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([1,2,3]) == [3,2,1]\\n\"], \"entry_point\": \"f_427051\", \"intent\": \"\\u041f\\u0435\\u0440\\u0435\\u0432\\u0435\\u0440\\u043d\\u0443\\u0442\\u044c \\u0441\\u043f\\u0438\\u0441\\u043e\\u043a `my_list`\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nSAMPLE_SIZE = 100\n\ndef generate_and_plot_random_distribution():\n    \"\"\"\n    Generates a random distribution of numbers, stores it in a pandas DataFrame, \n    calculates the mean and standard deviation, then plots a histogram of the distribution.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> generate_and_plot_random_distribution()\n    \"\"\"\n    # Generate a random distribution\n    data = np.random.randn(SAMPLE_SIZE)\n    df = pd.DataFrame(data, columns=['Random Numbers'])\n\n    # Calculate mean and standard deviation\n    mean = df['Random Numbers'].mean()\n    std = df['Random Numbers'].std()\n\n    print(f\"Mean: {mean}, Standard Deviation: {std}\")\n\n    # Plot a histogram\n    df['Random Numbers'].plot(kind='hist', rwidth=0.8)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \npandas, json, os, datetime\n```python\nimport pandas as pd\nimport json\nimport os\nfrom datetime import datetime\n\n# Constants\nJSON_FILE = 'data.json'\n\ndef read_json_and_to_csv():\n    \"\"\"\n    Reads a JSON file, converts it into a pandas DataFrame,\n    adds a column with the current date and time, and writes the DataFrame to a CSV file.\n\n    Requirements:\n    - pandas\n    - json\n    - os\n    - datetime\n\n    Example:\n    >>> read_json_and_to_csv()\n    \"\"\"\n    if not os.path.exists(JSON_FILE):\n        print(f\"File {JSON_FILE} does not exist.\")\n        return\n\n    with open(JSON_FILE, 'r') as f:\n        data = json.load(f)\n\n    df = pd.DataFrame(data)\n\n    # Add a column with the current date and time\n    df['Added on'] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n\n    # Write the DataFrame to a CSV file\n    df.to_csv('data.csv', index=False)\n\n    print(f\"Data written to data.csv.\")\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnumpy, random, matplotlib\n```python\nimport numpy as np\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nNUM_TRIALS = 10000\nNUM_DICE = 2\nSIDES_ON_DICE = 6\n\ndef simulate_and_plot_dice_rolls():\n    \"\"\"\n    Simulates the rolling of two dice 10,000 times, stores the results in a numpy array,\n    counts the number of times each possible sum of the dice appears, and plots a histogram of the results.\n\n    Requirements:\n    - numpy\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> simulate_and_plot_dice_rolls()\n    \"\"\"\n    # Simulate dice rolls\n    results = np.zeros((NUM_TRIALS, NUM_DICE))\n    for i in range(NUM_TRIALS):\n        for j in range(NUM_DICE):\n            results[i, j] = randint(1, SIDES_ON_DICE)\n\n    # Count the number of times each possible sum appears\n    sums = np.sum(results, axis=1)\n    counts = np.bincount(sums.astype(int))\n\n    # Plot a histogram of the results\n    plt.hist(sums, bins=np.arange(2, 2*SIDES_ON_DICE+2), align='left', rwidth=0.8)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \npandas, numpy, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nSAMPLE_SIZE = 1000\n\ndef generate_and_plot_normal_distribution():\n    \"\"\"\n    Generates a normal distribution of numbers, stores it in a pandas DataFrame, \n    calculates the mean and standard deviation, then plots a histogram of the distribution.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> generate_and_plot_normal_distribution()\n    \"\"\"\n    # Generate a normal distribution\n    data = np.random.normal(0, 1, SAMPLE_SIZE)\n    df = pd.DataFrame(data, columns=['Normal Distribution'])\n\n    # Calculate mean and standard deviation\n    mean = df['Normal Distribution'].mean()\n    std = df['Normal Distribution'].std()\n\n    print(f\"Mean: {mean}, Standard Deviation: {std}\")\n\n    # Plot a histogram\n    df['Normal Distribution'].plot(kind='hist', rwidth=0.8)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \npandas, os, datetime\n```python\nimport pandas as pd\nimport os\nfrom datetime import datetime\n\n# Constants\nCSV_FILE = 'data.csv'\n\ndef read_csv_and_to_json():\n    \"\"\"\n    Reads a CSV file, converts it into a pandas DataFrame,\n    adds a column with the current date and time, and writes the DataFrame to a JSON file.\n\n    Requirements:\n    - pandas\n    - os\n    - datetime\n\n    Example:\n    >>> read_csv_and_to_json()\n    \"\"\"\n    if not os.path.exists(CSV_FILE):\n        print(f\"File {CSV_FILE} does not exist.\")\n        return\n\n    df = pd.read_csv(CSV_FILE)\n\n    # Add a column with the current date and time\n    df['Added on'] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n\n    # Write the DataFrame to a JSON file\n    df.to_json('data.json')\n\n    print(f\"Data written to data.json.\")\n```\n`GPT_ODEX_BREAK`"}
{"seed": "{\"task_id\": 789625, \"prompt\": \"def f_789625(list_key, list_value):\\n\\t\", \"suffix\": \"\\n\\treturn my_dict\", \"canonical_solution\": \"my_dict = dict(zip(list_key, list_value))\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(['a','b','c'], [1,2,3]) == {'a':1, 'b':2, 'c':3}\\n\", \"\\n    assert candidate(\\n        ['green','happy','sunny'], [\\\"red\\\",\\\"sad\\\",\\\"rainy\\\"]\\n    ) == {\\\"green\\\":\\\"red\\\", \\\"happy\\\":\\\"sad\\\", \\\"sunny\\\":\\\"rainy\\\"}\\n\"], \"entry_point\": \"f_789625\", \"intent\": \"\\u041e\\u0431\\u044a\\u0435\\u0434\\u0438\\u043d\\u0438\\u0442\\u044c \\u0441\\u043f\\u0438\\u0441\\u043e\\u043a \\u043a\\u043b\\u044e\\u0447\\u0435\\u0439 `list_key` \\u0438 \\u0441\\u043f\\u0438\\u0441\\u043e\\u043a \\u0437\\u043d\\u0430\\u0447\\u0435\\u043d\\u0438\\u0439 `list_value` \\u0432 \\u0441\\u043b\\u043e\\u0432\\u0430\\u0440\\u044c `my_dict`\", \"library\": []}", "generation": "Scenario 1: \npandas, random, itertools, collections\n```python\nimport pandas as pd\nfrom random import choice\nimport itertools\nfrom collections import Counter\n\n# Constants\nANIMALS = ['cat', 'dog', 'bird', 'fish', 'elephant', 'lion', 'tiger', 'bear']\nCOLORS = ['red', 'blue', 'green', 'yellow', 'black', 'white']\n\ndef generate_animal_color_distribution(animals_count):\n    \"\"\"\n    Generate a distribution of different colored animals from a given list of animals \n    and colors. The function returns a pandas DataFrame with the counts of each type of colored animal.\n    \n    Parameters:\n    animals_count (int): The total number of animals.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the counts of each type of colored animal.\n    \n    Requirements:\n    - pandas\n    - random\n    - itertools\n    - collections.Counter\n    \n    Example:\n    >>> generate_animal_color_distribution(1000)\n    \"\"\"\n    animal_color_pairs = list(itertools.product(ANIMALS, COLORS))\n    chosen_pairs = [choice(animal_color_pairs) for _ in range(animals_count)]\n    counter = Counter(chosen_pairs)\n\n    distribution_df = pd.DataFrame.from_records(list(counter.items()), columns=['Animal-Color', 'Count'])\n\n    return distribution_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ndatetime, pytz, random, numpy\n```python\nimport datetime\nimport pytz\nimport random\nimport numpy as np\n\n# Constants\nTIMEZONES = ['UTC', 'US/Pacific', 'Europe/London', 'Asia/Kolkata', 'Australia/Sydney']\n\ndef generate_random_datetimes(n, start_date, end_date):\n    \"\"\"\n    Generate 'n' random datetime objects between two given dates across multiple timezones.\n    \n    Parameters:\n    n (int): The total number of datetime objects to generate.\n    start_date (str): The start date string in \"yyyy-mm-dd\" format.\n    end_date (str): The end date string in \"yyyy-mm-dd\" format.\n    \n    Returns:\n    list: A list of 'n' random datetime objects.\n    \n    Requirements:\n    - datetime\n    - pytz\n    - random\n    - numpy\n    \n    Example:\n    >>> generate_random_datetimes(10, '2022-01-01', '2022-12-31')\n    \"\"\"\n    start = datetime.datetime.strptime(start_date, '%Y-%m-%d')\n    end = datetime.datetime.strptime(end_date, '%Y-%m-%d')\n\n    random_datetimes = []\n    for _ in range(n):\n        random_date = start + (end - start) * random.random()\n        random_tz = pytz.timezone(random.choice(TIMEZONES))\n        random_datetime = random_date.replace(tzinfo=random_tz)\n        random_datetimes.append(random_datetime)\n\n    return random_datetimes\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, sklearn.preprocessing, numpy, itertools\n```python\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nimport numpy as np\nimport itertools\n\n# Constants\nFRUITS = ['apple', 'banana', 'cherry', 'date', 'elderberry', 'fig', 'grape', 'honeydew']\n\ndef generate_encoded_fruit_combinations(n):\n    \"\"\"\n    Generate 'n' pairs of fruits, encode them using LabelEncoder and store them in a pandas DataFrame.\n    \n    Parameters:\n    n (int): The total number of fruit pairs to generate.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with 'n' pairs of fruits and their encoded values.\n    \n    Requirements:\n    - pandas\n    - sklearn.preprocessing.LabelEncoder\n    - numpy\n    - itertools\n    \n    Example:\n    >>> generate_encoded_fruit_combinations(10)\n    \"\"\"\n    fruit_pairs = list(itertools.combinations(FRUITS, 2))\n    random_pairs = np.random.choice(range(len(fruit_pairs)), n)\n\n    selected_pairs = [fruit_pairs[i] for i in random_pairs]\n    encoder = LabelEncoder().fit(FRUITS)\n    encoded_pairs = [(pair, encoder.transform(list(pair))) for pair in selected_pairs]\n\n    df = pd.DataFrame(encoded_pairs, columns=['Pair', 'Encoded Pair'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, random, statistics, numpy\n```python\nimport pandas as pd\nimport random\nimport statistics\nimport numpy as np\n\n# Constants\nGRADES = ['A', 'B', 'C', 'D', 'E', 'F']\n\ndef generate_student_grades(n_students, n_subjects):\n    \"\"\"\n    Generate a pandas DataFrame containing the grades for 'n_students' number of students \n    across 'n_subjects' number of subjects. Also calculate the mode of each student's grades.\n    \n    Parameters:\n    n_students (int): The total number of students.\n    n_subjects (int): The total number of subjects.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with student grades and mode of grades.\n    \n    Requirements:\n    - pandas\n    - random\n    - statistics\n    - numpy\n    \n    Example:\n    >>> generate_student_grades(10, 5)\n    \"\"\"\n    student_ids = range(1, n_students+1)\n    subject_ids = range(1, n_subjects+1)\n\n    grades_data = []\n    for student_id in student_ids:\n        for subject_id in subject_ids:\n            grade = random.choice(GRADES)\n            grades_data.append((student_id, subject_id, grade))\n\n    df = pd.DataFrame(grades_data, columns=['Student ID', 'Subject ID', 'Grade'])\n\n    df['Mode Grade'] = df.groupby('Student ID')['Grade'].transform(lambda x: statistics.mode(x))\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nitertools, collections, random, numpy\n```python\nimport itertools\nfrom collections import Counter\nimport random\nimport numpy as np\n\n# Constants\nLETTERS = 'abcdefghijklmnopqrstuvwxyz'\nN_LETTERS = 5\n\ndef generate_random_letter_combinations(n):\n    \"\"\"\n    Generate 'n' random 5-letter combinations from the English alphabet and count the frequency of each combination.\n    \n    Parameters:\n    n (int): The total number of combinations to generate.\n    \n    Returns:\n    dict: A dictionary with combinations as keys and their frequencies as values.\n    \n    Requirements:\n    - itertools\n    - collections.Counter\n    - random\n    - numpy\n    \n    Example:\n    >>> generate_random_letter_combinations(1000)\n    \"\"\"\n    letter_combinations = list(itertools.combinations(LETTERS, N_LETTERS))\n    random_combinations = [random.choice(letter_combinations) for _ in range(n)]\n    combination_counts = Counter(random_combinations)\n\n    return dict(combination_counts)\n```\n"}
{"seed": "{\"task_id\": 789625, \"prompt\": \"def f_789625(list_key, list_value):\\n\\t\", \"suffix\": \"\\n\\treturn my_dict\", \"canonical_solution\": \"my_dict = {list_key[i]: list_value[i] for i in range(len(list_key))}\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(['a','b','c'], [1,2,3]) == {'a':1, 'b':2, 'c':3}\\n\", \"\\n    assert candidate(\\n        ['green','happy','sunny'], [\\\"red\\\",\\\"sad\\\",\\\"rainy\\\"]\\n    ) == {\\\"green\\\":\\\"red\\\", \\\"happy\\\":\\\"sad\\\", \\\"sunny\\\":\\\"rainy\\\"}\\n\"], \"entry_point\": \"f_789625\", \"intent\": \"\\u041e\\u0431\\u044a\\u0435\\u0434\\u0438\\u043d\\u0438\\u0442\\u044c \\u0441\\u043f\\u0438\\u0441\\u043e\\u043a \\u043a\\u043b\\u044e\\u0447\\u0435\\u0439 `list_key` \\u0438 \\u0441\\u043f\\u0438\\u0441\\u043e\\u043a \\u0437\\u043d\\u0430\\u0447\\u0435\\u043d\\u0438\\u0439 `list_value` \\u0432 \\u0441\\u043b\\u043e\\u0432\\u0430\\u0440\\u044c `my_dict`\", \"library\": []}", "generation": "Scenario 1: \ncollections, itertools, random\n```python\nfrom collections import defaultdict\nimport itertools\nimport random\n\n# Constants\nANIMALS = ['Cat', 'Dog', 'Rabbit', 'Parrot', 'Fish']\nPEOPLE = ['Alice', 'Bob', 'Charlie', 'David', 'Emma']\n\ndef assign_pets_to_owners(owners, pets):\n    \"\"\"\n    Randomly assign pets to owners from provided lists. Each owner can have multiple pets.\n    \n    Parameters:\n    owners (list): The list of owners.\n    pets (list): The list of pets.\n    \n    Returns:\n    dict: A dictionary with owners as keys and a list of pets as values.\n    \n    Requirements:\n    - collections.defaultdict\n    - itertools.cycle\n    - random.shuffle\n    \n    Example:\n    >>> assign_pets_to_owners(PEOPLE, ANIMALS)\n    \"\"\"\n    pet_owners = defaultdict(list)\n    random.shuffle(pets)\n    for owner, pet in zip(itertools.cycle(owners), pets):\n        pet_owners[owner].append(pet)\n\n    return pet_owners\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, scipy.stats, matplotlib\n```python\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\n# Constants\nSAMPLE_SIZE = 1000\n\ndef plot_normal_dist(mu, sigma):\n    \"\"\"\n    Generate a sample from a normal distribution with given mean and standard deviation, \n    and plot the histogram and the PDF.\n\n    Parameters:\n    mu (float): The mean of the distribution.\n    sigma (float): The standard deviation of the distribution.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - scipy.stats.norm\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_normal_dist(0, 1)\n    \"\"\"\n    sample = np.random.normal(mu, sigma, SAMPLE_SIZE)\n    plt.hist(sample, bins=30, density=True, alpha=0.5)\n\n    x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n    plt.plot(x, norm.pdf(x, mu, sigma))\n\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, sklearn.preprocessing\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\n\n# Constants\nDATA = {\n    'Age': [25, 30, 35, 40, 45],\n    'Salary': [30000, 35000, 40000, 45000, 50000]\n}\n\ndef scale_data(data):\n    \"\"\"\n    Scale the data using standard scaler.\n\n    Parameters:\n    data (dict): The dictionary with keys as column names and values as column values.\n\n    Returns:\n    DataFrame: A pandas DataFrame with scaled data.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.preprocessing.StandardScaler\n\n    Example:\n    >>> scale_data(DATA)\n    \"\"\"\n    df = pd.DataFrame(data)\n    scaler = StandardScaler()\n    scaled_data = scaler.fit_transform(df.values)\n    scaled_df = pd.DataFrame(scaled_data, columns=df.columns)\n\n    return scaled_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, matplotlib, sklearn.datasets\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\n\n# Constants\nCLUSTERS = 3\n\ndef generate_and_plot_blobs(n_samples, n_features, centers):\n    \"\"\"\n    Generate a dataset with blobs and plot them.\n\n    Parameters:\n    n_samples (int): The total number of points divided among clusters.\n    n_features (int): The number of features for each sample.\n    centers (int): The number of centers to generate.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.datasets.make_blobs\n\n    Example:\n    >>> generate_and_plot_blobs(150, 2, CLUSTERS)\n    \"\"\"\n    X, y = make_blobs(n_samples=n_samples, n_features=n_features, centers=centers)\n    plt.scatter(X[:, 0], X[:, 1], c=y)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, datetime\n```python\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\n\n# Constants\nDAYS = 7\n\ndef generate_weekly_data(start_date, end_date, columns):\n    \"\"\"\n    Generate weekly data for the given columns from start date to end date.\n\n    Parameters:\n    start_date (str): The start date in \"yyyy-mm-dd\" format.\n    end_date (str): The end date in \"yyyy-mm-dd\" format.\n    columns (list): The list of columns.\n\n    Returns:\n    DataFrame: A pandas DataFrame with weekly data.\n\n    Requirements:\n    - pandas\n    - numpy\n    - datetime.timedelta\n\n    Example:\n    >>> generate_weekly_data('2021-01-01', '2021-02-01', ['Revenue', 'Cost', 'Profit'])\n    \"\"\"\n    start_date = datetime.strptime(start_date, '%Y-%m-%d')\n    end_date = datetime.strptime(end_date, '%Y-%m-%d')\n\n    date_range = pd.date_range(start_date, end_date, freq='7D')\n    data = np.random.rand(len(date_range), len(columns))\n\n    df = pd.DataFrame(data, columns=columns, index=date_range)\n\n    return df\n```"}
{"seed": "{\"task_id\": 477678, \"prompt\": \"def f_477678(s, n, sep):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"sep.join([s] * n)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('abk', 3, ';') == 'abk;abk;abk'\\n\", \"\\n    assert candidate('', 3, ';') == ';;'\\n\"], \"entry_point\": \"f_477678\", \"intent\": \"\\u0412\\u044b\\u0432\\u0435\\u0441\\u0442\\u0438 \\u0441\\u0442\\u0440\\u043e\\u043a\\u0443 `s` `n` \\u0440\\u0430\\u0437 \\u0447\\u0435\\u0440\\u0435\\u0437 \\u0440\\u0430\\u0437\\u0434\\u0435\\u043b\\u0438\\u0442\\u0435\\u043b\\u044c `sep`\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, random, itertools\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import randint\nimport itertools\n\n# Constants\nLETTERS = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']\nSEPARATORS = [',', ';', '|', '/', '-', '+']\n\ndef create_random_string_combinations(s, n, sep):\n    \"\"\"\n    Create a pandas DataFrame of random string combinations of length `n` for a given \n    string `s` separated by a separator `sep`. The DataFrame contains three columns, \n    'Combination', 'Length' and 'Count of Separator'.\n    \n    Parameters:\n    s (str): The input string.\n    n (int): The length of the string combinations.\n    sep (str): The separator.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with string combinations, their lengths and count of separators.\n    \n    Requirements:\n    - pandas\n    - itertools\n    - random\n    - numpy\n    \n    Example:\n    >>> create_random_string_combinations('abc', 3, '-')\n    \"\"\"\n    # Generate all combinations of length 'n' for the string 's'\n    combinations = list(itertools.combinations(s, n))\n    # Convert the combinations to strings separated by 'sep'\n    combinations_sep = [sep.join(list(combo)) for combo in combinations]\n    # Create a DataFrame\n    df = pd.DataFrame(combinations_sep, columns=['Combination'])\n    # Calculate the length of each combination\n    df['Length'] = df['Combination'].apply(len)\n    # Count the number of separators in each combination\n    df['Count of Separator'] = df['Combination'].apply(lambda x: x.count(sep))\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, random, hashlib\n```python\nimport numpy as np\nfrom random import randint\nimport hashlib\n\n# Constants\nSALT = 'salt_for_hashing'\n\ndef hash_strings_with_salt(s, n, sep):\n    \"\"\"\n    Generate a numpy array of hashed strings using SHA256. The strings are created by repeating \n    a given string `s` `n` times separated by a separator `sep` and a random salt is added for each string.\n    \n    Parameters:\n    s (str): The input string.\n    n (int): The number of repetitions of the string `s`.\n    sep (str): The separator.\n    \n    Returns:\n    numpy.ndarray: A numpy array of hashed strings.\n    \n    Requirements:\n    - numpy\n    - hashlib\n    - random\n    \n    Example:\n    >>> hash_strings_with_salt('abc', 3, '-')\n    \"\"\"\n    # Create a list of strings\n    strings = [sep.join([s] * n) for _ in range(n)]\n    # Add a random salt to each string and hash it\n    hashed_strings = np.array([hashlib.sha256((string + SALT + str(randint(0, n))).encode()).hexdigest() for string in strings])\n\n    return hashed_strings\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \npandas, numpy, random, string\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import choice\nimport string\n\n# Constants\nALPHABET = string.ascii_lowercase\nSEPARATORS = [',', ';', '|', '/', '-', '+']\n\ndef create_random_string_dataframe(s, n, sep):\n    \"\"\"\n    Create a pandas DataFrame of random strings of length `n` for a given string `s` separated by a \n    separator `sep`. The DataFrame contains three columns, 'Random String', 'Length' and 'Count of Separator'.\n    \n    Parameters:\n    s (str): The input string.\n    n (int): The length of the random strings.\n    sep (str): The separator.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with random strings, their lengths and count of separators.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - random\n    - string\n    \n    Example:\n    >>> create_random_string_dataframe('abc', 3, '-')\n    \"\"\"\n    # Create a list of random strings\n    strings = [sep.join([choice(s) for _ in range(n)]) for _ in range(n)]\n    # Create a DataFrame\n    df = pd.DataFrame(strings, columns=['Random String'])\n    # Calculate the length of each string\n    df['Length'] = df['Random String'].apply(len)\n    # Count the number of separators in each string\n    df['Count of Separator'] = df['Random String'].apply(lambda x: x.count(sep))\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, random, hashlib\n```python\nimport numpy as np\nfrom random import randint\nimport hashlib\n\n# Constants\nSALT = 'salt_for_hashing'\n\ndef generate_random_hashed_strings(s, n, sep):\n    \"\"\"\n    Generate a numpy array of randomly hashed strings using SHA256. The strings are created by repeating \n    a given string `s` `n` times separated by a separator `sep` and a random salt is added for each string.\n    \n    Parameters:\n    s (str): The input string.\n    n (int): The number of repetitions of the string `s`.\n    sep (str): The separator.\n    \n    Returns:\n    numpy.ndarray: A numpy array of hashed strings.\n    \n    Requirements:\n    - numpy\n    - hashlib\n    - random\n    \n    Example:\n    >>> generate_random_hashed_strings('abc', 3, '-')\n    \"\"\"\n    # Create a list of strings\n    strings = [sep.join([s] * randint(1, n)) for _ in range(n)]\n    # Add a random salt to each string and hash it\n    hashed_strings = np.array([hashlib.sha256((string + SALT + str(randint(0, n))).encode()).hexdigest() for string in strings])\n\n    return hashed_strings\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \npandas, numpy, random, string\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import choice\nimport string\n\n# Constants\nALPHABET = string.ascii_lowercase\nSEPARATORS = [',', ';', '|', '/', '-', '+']\n\ndef generate_random_string_dataframe(s, n, sep):\n    \"\"\"\n    Generate a pandas DataFrame of random strings of length `n` for a given string `s` separated by a \n    separator `sep`. The DataFrame contains three columns, 'Random String', 'Length' and 'Count of Separator'.\n    \n    Parameters:\n    s (str): The input string.\n    n (int): The length of the random strings.\n    sep (str): The separator.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with random strings, their lengths and count of separators.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - random\n    - string\n    \n    Example:\n    >>> generate_random_string_dataframe('abc', 3, '-')\n    \"\"\"\n    # Create a list of random strings\n    strings = [sep.join([choice(ALPHABET) for _ in range(n)]) for _ in range(n)]\n    # Create a DataFrame\n    df = pd.DataFrame(strings, columns=['Random String'])\n    # Calculate the length of each string\n    df['Length'] = df['Random String'].apply(len)\n    # Count the number of separators in each string\n    df['Count of Separator'] = df['Random String'].apply(lambda x: x.count(sep))\n\n    return df\n```"}
{"seed": "{\"task_id\": 635319, \"prompt\": \"def f_635319(df, list_indexes):\\n\\t\", \"suffix\": \"\\n\\treturn cols\", \"canonical_solution\": \"cols = df.iloc[:, list_indexes]\", \"test_start\": \"\\nimport pandas as pd\\n\\ndef check(candidate):\", \"test\": [\"\\n    df = pd.DataFrame({'a': [1,2,3], 'b': [4,5,6], 'c': [7,8,9]})\\n    assert candidate(df, [1,2]).shape == (3, 2)\\n\"], \"entry_point\": \"f_635319\", \"intent\": \"\\u0412\\u044b\\u0431\\u0440\\u0430\\u0442\\u044c \\u0441\\u0442\\u043e\\u043b\\u0431\\u0446\\u044b `cols` \\u0441 \\u0438\\u043d\\u0434\\u0435\\u043a\\u0430\\u0441\\u0430\\u043c\\u0438 `list_indexes` pandas \\u0434\\u0430\\u0442\\u0430\\u0444\\u0440\\u0435\\u0439\\u043c\\u0430 `df`\", \"library\": [\"pandas\"]}", "generation": "Scenario 1: \npandas, numpy, seaborn, matplotlib.pyplot\n```python\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nCATEGORIES = ['A', 'B', 'C', 'D', 'E']\n\ndef generate_and_visualize_data(df, list_indexes, category_count):\n    \"\"\"\n    Generate a new DataFrame based on selected columns from the input DataFrame, \n    add a new categorical column, and visualize the correlation matrix of the new DataFrame.\n\n    Parameters:\n    df (DataFrame): The input DataFrame.\n    list_indexes (list): The list of column indexes to be selected from the input DataFrame.\n    category_count (int): The number of categories to be generated in the new column.\n\n    Returns:\n    DataFrame: The new DataFrame with selected columns and a new categorical column.\n\n    Requirements:\n    - pandas\n    - numpy\n    - seaborn\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame(np.random.rand(10, 5), columns=list('ABCDE'))\n    >>> new_df = generate_and_visualize_data(df, [0, 2, 4], 3)\n    >>> print(new_df)\n    >>> sns.heatmap(new_df.corr(), annot=True)\n    >>> plt.show()\n    \"\"\"\n    new_df = df.iloc[:, list_indexes]\n    new_df['Category'] = np.random.choice(CATEGORIES[:category_count], len(new_df))\n\n    return new_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \npandas, numpy, sklearn.preprocessing, matplotlib.pyplot\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\ndef process_and_visualize_data(df, list_indexes):\n    \"\"\"\n    Select columns from the input DataFrame, standardize them, \n    and plot the distribution of the standardized values.\n\n    Parameters:\n    df (DataFrame): The input DataFrame.\n    list_indexes (list): The list of column indexes to be selected from the input DataFrame.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.preprocessing.StandardScaler\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame(np.random.rand(10, 5), columns=list('ABCDE'))\n    >>> process_and_visualize_data(df, [0, 2, 4])\n    \"\"\"\n    selected_df = df.iloc[:, list_indexes]\n    scaler = StandardScaler()\n    standardized_df = pd.DataFrame(scaler.fit_transform(selected_df), columns=selected_df.columns)\n\n    standardized_df.plot(kind='density')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \npandas, numpy, scipy.stats, matplotlib.pyplot\n```python\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef outlier_detection(df, list_indexes):\n    \"\"\"\n    Select columns from the input DataFrame, detect and visualize the outliers using Z-score method.\n\n    Parameters:\n    df (DataFrame): The input DataFrame.\n    list_indexes (list): The list of column indexes to be selected from the input DataFrame.\n\n    Returns:\n    DataFrame: The DataFrame containing the outliers.\n\n    Requirements:\n    - pandas\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame(np.random.rand(10, 5), columns=list('ABCDE'))\n    >>> outliers = outlier_detection(df, [0, 2, 4])\n    >>> print(outliers)\n    \"\"\"\n    selected_df = df.iloc[:, list_indexes]\n    z_scores = np.abs(stats.zscore(selected_df))\n    outliers = selected_df[(z_scores > 3).any(axis=1)]\n\n    plt.figure(figsize=(12, 6))\n    plt.subplot(121)\n    selected_df.boxplot()\n    plt.title('Before Outlier Removal')\n    plt.subplot(122)\n    selected_df[(z_scores <= 3).all(axis=1)].boxplot()\n    plt.title('After Outlier Removal')\n    plt.show()\n\n    return outliers\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \npandas, numpy, sklearn.cluster, matplotlib.pyplot\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\ndef k_means_clustering(df, list_indexes, n_clusters):\n    \"\"\"\n    Select columns from the input DataFrame, perform K-means clustering on the selected data,\n    and plot the clusters.\n\n    Parameters:\n    df (DataFrame): The input DataFrame.\n    list_indexes (list): The list of column indexes to be selected from the input DataFrame.\n    n_clusters (int): The number of clusters for K-means algorithm.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.cluster.KMeans\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame(np.random.rand(100, 5), columns=list('ABCDE'))\n    >>> k_means_clustering(df, [0, 2, 4], 3)\n    \"\"\"\n    selected_df = df.iloc[:, list_indexes]\n    k_means = KMeans(n_clusters=n_clusters)\n    k_means.fit(selected_df)\n\n    plt.scatter(selected_df.iloc[:, 0], selected_df.iloc[:, 1], c=k_means.labels_)\n    plt.scatter(k_means.cluster_centers_[:, 0], k_means.cluster_centers_[:, 1], s=300, c='red')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \npandas, numpy, sklearn.linear_model, matplotlib.pyplot\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\n\ndef linear_regression(df, list_indexes):\n    \"\"\"\n    Select two columns from the input DataFrame, perform linear regression on the selected data,\n    and plot the regression line.\n\n    Parameters:\n    df (DataFrame): The input DataFrame.\n    list_indexes (list): The list of two column indexes to be selected from the input DataFrame.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.linear_model.LinearRegression\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame(np.random.rand(100, 5), columns=list('ABCDE'))\n    >>> linear_regression(df, [0, 2])\n    \"\"\"\n    assert len(list_indexes) == 2, \"Exactly two columns should be selected for linear regression.\"\n\n    selected_df = df.iloc[:, list_indexes]\n    X = selected_df.iloc[:, 0].values.reshape(-1, 1)\n    y = selected_df.iloc[:, 1].values.reshape(-1, 1)\n\n    linear_regressor = LinearRegression()\n    linear_regressor.fit(X, y)\n\n    plt.scatter(X, y, color='blue')\n    plt.plot(X, linear_regressor.predict(X), color='red')\n    plt.show()\n```"}
{"seed": "{\"task_id\": 1085344, \"prompt\": \"def f_1085344(my_list):\\n\\t\", \"suffix\": \"\\n\\treturn total\", \"canonical_solution\": \"total = sum(my_list)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([345, 576, 12]) == 933\\n\", \"\\n    assert candidate([1.0, 2.2, 3.5]) == 6.7\\n\"], \"entry_point\": \"f_1085344\", \"intent\": \"\\u041d\\u0430\\u0439\\u0442\\u0438 \\u0441\\u0443\\u043c\\u043c\\u0443 \\u0447\\u0438\\u0441\\u0435\\u043b `total` \\u0441\\u043f\\u0438\\u0441\\u043a\\u0430 `my_list`\", \"library\": []}", "generation": "Scenario 1: \npandas, random, statistics, numpy\n```python\nimport pandas as pd\nfrom random import randint\nfrom statistics import mean, median, stdev\nimport numpy as np\n\n# Constants\nRANGE_MIN = 0\nRANGE_MAX = 100\nLIST_SIZE = 1000\n\ndef generate_statistics():\n    \"\"\"\n    Generate a list of random integers, calculate and return statistics including \n    total, mean, median, standard deviation, and five-number summary.\n\n    Returns:\n    dict: A dictionary containing the statistics.\n\n    Requirements:\n    - pandas\n    - random\n    - statistics\n    - numpy\n\n    Example:\n    >>> stats = generate_statistics()\n    >>> print(stats)\n    \"\"\"\n    my_list = [randint(RANGE_MIN, RANGE_MAX) for _ in range(LIST_SIZE)]\n    stats = {\n        'total': sum(my_list),\n        'mean': mean(my_list),\n        'median': median(my_list),\n        'std_dev': stdev(my_list),\n        'five_num_summary': list(np.percentile(my_list, [0, 25, 50, 75, 100])),\n    }\n    return stats\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, matplotlib.pyplot, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef plot_correlation_matrix():\n    \"\"\"\n    Generate a DataFrame of random numbers, compute the correlation matrix, \n    and plot it using a heatmap.\n\n    Returns:\n    DataFrame: The correlation matrix DataFrame.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> df = plot_correlation_matrix()\n    >>> print(df)\n    \"\"\"\n    df = pd.DataFrame(np.random.rand(10, 5), columns=['A', 'B', 'C', 'D', 'E'])\n    corr = df.corr()\n    sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, annot=True, cmap='coolwarm')\n    plt.show()\n    return corr\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, random, time, numpy\n```python\nimport pandas as pd\nfrom random import choice\nimport time\nimport numpy as np\n\n# Constants\nCATEGORIES = ['A', 'B', 'C', 'D', 'E']\n\ndef generate_category_distribution():\n    \"\"\"\n    Generate a list of random categories, calculate the distribution of the categories, \n    and return the distribution as a pandas Series.\n\n    Returns:\n    Series: A pandas Series containing the distribution.\n\n    Requirements:\n    - pandas\n    - random\n    - time\n    - numpy\n\n    Example:\n    >>> dist = generate_category_distribution()\n    >>> print(dist)\n    \"\"\"\n    np.random.seed(int(time.time()))\n    my_list = np.random.choice(CATEGORIES, size=1000)\n    distribution = pd.Series(my_list).value_counts(normalize=True)\n    return distribution\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, random, math, numpy\n```python\nimport pandas as pd\nfrom random import randint\nimport math\nimport numpy as np\n\n# Constants\nRANGE_MIN = 0\nRANGE_MAX = 1000\nLIST_SIZE = 100\n\ndef calculate_log_sum():\n    \"\"\"\n    Generate a list of random integers, calculate the natural logarithm of each number, \n    sum the results, and return the sum.\n\n    Returns:\n    float: The sum of the natural logarithms.\n\n    Requirements:\n    - pandas\n    - random\n    - math\n    - numpy\n\n    Example:\n    >>> log_sum = calculate_log_sum()\n    >>> print(log_sum)\n    \"\"\"\n    my_list = [randint(RANGE_MIN, RANGE_MAX) for _ in range(LIST_SIZE)]\n    log_sum = np.log(my_list).sum()\n    return log_sum\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, random, sklearn.preprocessing, numpy\n```python\nimport pandas as pd\nfrom random import randint\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\n\n# Constants\nRANGE_MIN = 0\nRANGE_MAX = 100\nLIST_SIZE = 50\n\ndef scale_values():\n    \"\"\"\n    Generate a list of random integers, scale the values to a range of 0 to 1, \n    and return the scaled values as a pandas Series.\n\n    Returns:\n    Series: A pandas Series containing the scaled values.\n\n    Requirements:\n    - pandas\n    - random\n    - sklearn.preprocessing\n    - numpy\n\n    Example:\n    >>> scaled_values = scale_values()\n    >>> print(scaled_values)\n    \"\"\"\n    my_list = [randint(RANGE_MIN, RANGE_MAX) for _ in range(LIST_SIZE)]\n    scaler = MinMaxScaler()\n    scaled_values = scaler.fit_transform(np.array(my_list).reshape(-1, 1))\n    return pd.Series(scaled_values.flatten())\n```\n"}
{"seed": "{\"task_id\": 1085344, \"prompt\": \"def f_1085344(my_list):\\n\\t\", \"suffix\": \"\\n\\treturn total\", \"canonical_solution\": \"total = 0\\n\\n\\tfor i in my_list:\\n\\n\\t\\ttotal += i\\n\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([345, 576, 12]) == 933\\n\"], \"entry_point\": \"f_1085344\", \"intent\": \"\\u041d\\u0430\\u0439\\u0442\\u0438 \\u0441\\u0443\\u043c\\u043c\\u0443 \\u0447\\u0438\\u0441\\u0435\\u043b `total` \\u0441\\u043f\\u0438\\u0441\\u043a\\u0430 `my_list`\", \"library\": []}", "generation": "Scenario 1: \nnumpy, scipy.stats, matplotlib\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n# Constants\nDATA_SETS = 5\nDATA_POINTS = 1000\n\ndef generate_and_analyze_data(average, standard_deviation):\n    \"\"\"\n    Generate multiple data sets using a normal distribution with a given mean and standard deviation, \n    calculate some statistics, and plot the data sets and their histograms.\n    \n    Parameters:\n    average (float): The mean of the normal distribution.\n    standard_deviation (float): The standard deviation of the normal distribution.\n    \n    Returns:\n    dict: A dictionary with the calculated statistics.\n    \n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n    \n    Example:\n    >>> stats = generate_and_analyze_data(0, 1)\n    >>> print(stats)\n    \"\"\"\n    datasets = []\n    for _ in range(DATA_SETS):\n        data = np.random.normal(average, standard_deviation, DATA_POINTS)\n        datasets.append(data)\n\n    statistics = {}\n    for i, data in enumerate(datasets):\n        statistics[f'Data Set {i+1}'] = {\n            'Mean': np.mean(data),\n            'Median': np.median(data),\n            'Mode': float(stats.mode(data)[0]),\n            'Standard Deviation': np.std(data)\n        }\n\n    fig, axs = plt.subplots(2, 1, figsize=(10, 10))\n    for i, data in enumerate(datasets):\n        axs[0].plot(data, label=f'Data Set {i+1}')\n        axs[1].hist(data, bins=50, alpha=0.5, label=f'Data Set {i+1}')\n    axs[0].legend()\n    axs[1].legend()\n    plt.show()\n\n    return statistics\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nnumpy, pandas, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nSTUDENTS = 100\nSUBJECTS = ['Math', 'Physics', 'Chemistry', 'Biology', 'History', 'Geography', 'English']\n\ndef generate_and_analyze_grades():\n    \"\"\"\n    Generate a DataFrame with random grades for a number of students and subjects, \n    calculate the average grade per subject and per student, \n    and plot the distribution of grades and averages.\n    \n    Returns:\n    DataFrame: A DataFrame with the grades and averages.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    \n    Example:\n    >>> grades = generate_and_analyze_grades()\n    >>> print(grades)\n    \"\"\"\n    grades = pd.DataFrame(np.random.randint(1, 6, size=(STUDENTS, len(SUBJECTS))), columns=SUBJECTS)\n    grades['Average Student'] = grades.mean(axis=1)\n    grades.loc['Average Subject'] = grades.mean()\n\n    plt.figure(figsize=(10, 5))\n    plt.subplot(121)\n    plt.hist(grades.loc[:, SUBJECTS].values.flatten(), bins=5, range=(0.5, 5.5), rwidth=0.8)\n    plt.title('Distribution of Grades')\n    plt.subplot(122)\n    plt.hist(grades['Average Student'], bins=5, range=(0.5, 5.5), rwidth=0.8)\n    plt.title('Distribution of Average Grades')\n    plt.show()\n\n    return grades\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnumpy, pandas, random, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nimport random\nimport matplotlib.pyplot as plt\n\n# Constants\nPRODUCTS = ['Product A', 'Product B', 'Product C', 'Product D', 'Product E']\nMONTHS = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n\ndef generate_and_analyze_sales(year):\n    \"\"\"\n    Generate a DataFrame with random sales data for a year, \n    calculate the total sales per product and per month, \n    and plot the sales and totals.\n    \n    Parameters:\n    year (int): The year for the sales data.\n    \n    Returns:\n    DataFrame: A DataFrame with the sales data and totals.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> sales = generate_and_analyze_sales(2020)\n    >>> print(sales)\n    \"\"\"\n    sales_data = {product: [random.randint(100, 1000) for _ in range(12)] for product in PRODUCTS}\n    sales_data['Month'] = MONTHS\n    sales = pd.DataFrame(sales_data)\n    sales.set_index('Month', inplace=True)\n\n    sales['Total Month'] = sales.sum(axis=1)\n    sales.loc['Total Product'] = sales.sum()\n\n    sales.plot(kind='bar', stacked=True, figsize=(10, 5))\n    plt.title(f'Sales in {year}')\n    plt.show()\n\n    return sales\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, pandas, random, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nimport random\nimport matplotlib.pyplot as plt\n\n# Constants\nNUM_CLASSES = 10\nNUM_STUDENTS = 100\n\ndef generate_and_visualize_class_data():\n    \"\"\"\n    Generate a DataFrame with random class data for a number of students, \n    calculate the average grade per class, \n    and plot the grades and averages.\n    \n    Returns:\n    DataFrame: A DataFrame with the class data and averages.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> class_data = generate_and_visualize_class_data()\n    >>> print(class_data)\n    \"\"\"\n    class_data = pd.DataFrame(np.random.randint(0, 101, size=(NUM_STUDENTS, NUM_CLASSES)), \n                              columns=[f'Class {i+1}' for i in range(NUM_CLASSES)])\n\n    class_data['Average Student'] = class_data.mean(axis=1)\n    class_data.loc['Average Class'] = class_data.mean()\n\n    class_data.plot(kind='bar', stacked=True, figsize=(10, 5))\n    plt.title('Class Grades and Averages')\n    plt.show()\n\n    return class_data\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnumpy, scipy.stats, matplotlib\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n# Constants\nSAMPLE_SIZE = 1000\nBINS = 50\n\ndef generate_and_visualize_normal_distribution(mean, std_dev):\n    \"\"\"\n    Generate a sample from a normal distribution with a given mean and standard deviation, \n    calculate some statistics, \n    and plot the histogram and normal distribution curve.\n    \n    Parameters:\n    mean (float): The mean of the normal distribution.\n    std_dev (float): The standard deviation of the normal distribution.\n    \n    Returns:\n    dict: A dictionary with the calculated statistics.\n    \n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n    \n    Example:\n    >>> stats = generate_and_visualize_normal_distribution(0, 1)\n    >>> print(stats)\n    \"\"\"\n    sample = np.random.normal(mean, std_dev, SAMPLE_SIZE)\n    \n    statistics = {\n        'Mean': np.mean(sample),\n        'Median': np.median(sample),\n        'Mode': float(stats.mode(sample)[0]),\n        'Standard Deviation': np.std(sample)\n    }\n\n    plt.hist(sample, bins=BINS, density=True, alpha=0.6, color='g')\n\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, mean, std_dev)\n    plt.plot(x, p, 'k', linewidth=2)\n\n    title = \"Fit results: mean = %.2f,  std_dev = %.2f\" % (mean, std_dev)\n    plt.title(title)\n\n    plt.show()\n\n    return statistics\n```"}
{"seed": "{\"task_id\": 759476, \"prompt\": \"def f_759476(my_list):\\n\\t\", \"suffix\": \"\\n\\treturn max_value\", \"canonical_solution\": \"max_value = {x: y for x, y in filter(lambda x: my_list[x[0]] == max(my_list.values()), my_list.items())}\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate({'a':3, 'b':2, 'r':36}) == {'r': 36}\\n\"], \"entry_point\": \"f_759476\", \"intent\": \"\\u041d\\u0430\\u0439\\u0442\\u0438 \\u043c\\u0430\\u043a\\u0441\\u0438\\u043c\\u0430\\u043b\\u044c\\u043d\\u043e\\u0435 \\u0437\\u043d\\u0430\\u0435\\u0447\\u043d\\u0438\\u0435 `max_value` \\u0432 \\u0441\\u043b\\u043e\\u0432\\u0430\\u0440\\u0435 `my_list`\", \"library\": []}", "generation": "Scenario 1:\npandas, math, statistics, numpy\n```python\nimport pandas as pd\nimport math\nimport statistics\nimport numpy as np\n\n# Sample data\nDATA = {'a': [3, 5, 7, 2], 'b': [2, 8, 9, 10], 'r': [36, 5, 14, 20]}\n\ndef calculate_statistical_data(data):\n    \"\"\"\n    Calculate statistical data (mean, median, mode, variance, standard deviation, max, min)\n    for each column in the DataFrame.\n    \n    Parameters:\n    data (dict): The dictionary of data.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with statistical data for each column.\n    \n    Requirements:\n    - pandas\n    - math\n    - statistics\n    - numpy\n    \n    Example:\n    >>> calculate_statistical_data(DATA)\n    \"\"\"\n    df = pd.DataFrame(data)\n    result = pd.DataFrame(index=['mean', 'median', 'mode', 'variance', 'std_dev', 'max', 'min'])\n    \n    for col in df.columns:\n        result[col] = [np.mean(df[col]), np.median(df[col]), statistics.mode(df[col]),\n                       np.var(df[col]), np.std(df[col]), np.max(df[col]), np.min(df[col])]\n    \n    return result\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncollections, heapq, itertools\n```python\nfrom collections import Counter\nfrom heapq import nlargest\nfrom itertools import chain\n\n# Constants\nTOP_K = 3\n\ndef find_most_common_elements(my_dict):\n    \"\"\"\n    Find the most common elements in the values of the dictionary.\n    \n    Parameters:\n    my_dict (dict): The dictionary of data.\n    \n    Returns:\n    list: A list of the most common elements.\n    \n    Requirements:\n    - collections.Counter\n    - heapq.nlargest\n    - itertools.chain\n    \n    Example:\n    >>> find_most_common_elements({'a':[3, 5, 7, 2], 'b':[2, 8, 9, 10], 'r':[36, 5, 14, 20]})\n    \"\"\"\n    all_values = list(chain(*my_dict.values()))\n    counter = Counter(all_values)\n    \n    most_common = nlargest(TOP_K, counter.items(), key=lambda i: i[1])\n    \n    return [x[0] for x in most_common]\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, sklearn, matplotlib\n```python\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\n\n# Constants\nX = [3, 5, 7, 2]\nY = [2, 8, 9, 10]\n\ndef create_and_plot_linear_regression(x, y):\n    \"\"\"\n    Create a linear regression model and plot the data points and the regression line.\n    \n    Parameters:\n    x, y (list): The lists of data.\n    \n    Returns:\n    None.\n    \n    Requirements:\n    - pandas\n    - sklearn.linear_model.LinearRegression\n    - matplotlib.pyplot\n    \n    Example:\n    >>> create_and_plot_linear_regression(X, Y)\n    \"\"\"\n    df = pd.DataFrame({'x': x, 'y': y})\n    \n    model = LinearRegression().fit(df[['x']], df['y'])\n    \n    plt.scatter(df['x'], df['y'], color='blue')\n    plt.plot(df['x'], model.predict(df[['x']]), color='red')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nrandom, itertools, collections\n```python\nimport random\nfrom itertools import permutations\nfrom collections import Counter\n\n# Constants\nELEMENTS = ['a', 'b', 'r']\n\ndef find_most_common_permutation(elements):\n    \"\"\"\n    Generate permutations of the elements and find the most common permutation.\n    \n    Parameters:\n    elements (list): The list of elements.\n    \n    Returns:\n    tuple: The most common permutation.\n    \n    Requirements:\n    - random\n    - itertools.permutations\n    - collections.Counter\n    \n    Example:\n    >>> find_most_common_permutation(ELEMENTS)\n    \"\"\"\n    permutations_list = list(permutations(elements))\n    random_permutations = [random.choice(permutations_list) for _ in range(1000)]\n    \n    counter = Counter(random_permutations)\n    most_common_permutation = counter.most_common(1)[0][0]\n    \n    return most_common_permutation\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, matplotlib, seaborn\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nDATA = {'a': [3, 5, 7, 2], 'b': [2, 8, 9, 10], 'r': [36, 5, 14, 20]}\n\ndef plot_heatmap(data):\n    \"\"\"\n    Plot a heatmap of the correlation matrix of the DataFrame.\n    \n    Parameters:\n    data (dict): The dictionary of data.\n    \n    Returns:\n    None.\n    \n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - seaborn\n    \n    Example:\n    >>> plot_heatmap(DATA)\n    \"\"\"\n    df = pd.DataFrame(data)\n    \n    corr = df.corr()\n    \n    sns.heatmap(corr, annot=True, cmap='coolwarm')\n    plt.show()\n```"}
{"seed": "{\"task_id\": 759476, \"prompt\": \"def f_759476(my_list):\\n\\t\", \"suffix\": \"\\n\\treturn max_value\", \"canonical_solution\": \"max_value = max(my_list.values())\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate({'a':3, 'b':2, 'r':36}) == 36\\n\"], \"entry_point\": \"f_759476\", \"intent\": \"\\u041d\\u0430\\u0439\\u0442\\u0438 \\u043c\\u0430\\u043a\\u0441\\u0438\\u043c\\u0430\\u043b\\u044c\\u043d\\u043e\\u0435 \\u0437\\u043d\\u0430\\u0435\\u0447\\u043d\\u0438\\u0435 `max_value` \\u0432 \\u0441\\u043b\\u043e\\u0432\\u0430\\u0440\\u0435 `my_list`\", \"library\": []}", "generation": "Scenario 1: \npandas, matplotlib, collections, numpy\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nimport numpy as np\n\ndef plot_top_n_words(text, n):\n    \"\"\"\n    Given a text, plot the top n most frequent words in a bar chart.\n    \n    Parameters:\n    text (str): The input text.\n    n (int): The number of top words to plot.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - collections.Counter\n    - numpy\n    \n    Example:\n    >>> text = 'This is a sample text. This text contains some words. This is a sample sentence.'\n    >>> plot_top_n_words(text, 5)\n    \"\"\"\n    words = text.split()\n    word_counts = Counter(words)\n    top_n_words = word_counts.most_common(n)\n    words_df = pd.DataFrame(top_n_words, columns=['Word', 'Frequency'])\n    words_df.set_index('Word', inplace=True)\n    words_df.sort_values('Frequency', ascending=True).plot(kind='barh', legend=False)\n    plt.xlabel('Frequency')\n    plt.title(f'Top {n} Words')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, pandas, matplotlib, csv\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport csv\n\n# Constants\nCSV_FILE = 'data.csv'\nTARGET_VARIABLE = 'target'\nFEATURES = ['feature1', 'feature2', 'feature3']\n\ndef plot_histograms(csv_file, target_variable, features):\n    \"\"\"\n    Given a CSV file, plot histograms of the features for different classes of the target variable.\n    \n    Parameters:\n    csv_file (str): The path to the CSV file.\n    target_variable (str): The target variable in the CSV file.\n    features (list): The list of features in the CSV file.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - numpy\n    - csv\n    \n    Example:\n    >>> plot_histograms('data.csv', 'target', ['feature1', 'feature2', 'feature3'])\n    \"\"\"\n    df = pd.read_csv(csv_file)\n    target_classes = df[target_variable].unique()\n    for feature in features:\n        plt.figure(figsize=(10, 5))\n        for target_class in target_classes:\n            subset = df[df[target_variable] == target_class]\n            plt.hist(subset[feature], bins=30, label=str(target_class), alpha=0.6)\n        plt.xlabel(feature)\n        plt.ylabel('Frequency')\n        plt.legend(title=target_variable)\n        plt.title(f'Histogram of {feature} by {target_variable}')\n        plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, pandas, collections, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef find_top_n_items(df, column, n):\n    \"\"\"\n    Given a pandas DataFrame and a column name, find the top n items in that column.\n    \n    Parameters:\n    df (DataFrame): The input pandas DataFrame.\n    column (str): The column name.\n    n (int): The number of top items to find.\n    \n    Returns:\n    DataFrame: A DataFrame with the top n items in the specified column.\n    \n    Requirements:\n    - pandas\n    - collections.Counter\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = pd.DataFrame({\n    ...     'item': ['apple', 'banana', 'apple', 'orange', 'banana', 'apple', 'banana', 'orange', 'apple'],\n    ...     'price': [1, 2, 3, 2, 2, 1, 3, 2, 1]\n    ... })\n    >>> print(find_top_n_items(df, 'item', 2))\n    \"\"\"\n    item_counts = Counter(df[column])\n    top_n_items = item_counts.most_common(n)\n    top_n_df = pd.DataFrame(top_n_items, columns=[column, 'Count'])\n    return top_n_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, matplotlib, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nTARGET_VARIABLE = 'target'\nFEATURES = ['feature1', 'feature2', 'feature3']\n\ndef plot_correlation_matrix(df, target_variable, features):\n    \"\"\"\n    Given a pandas DataFrame, a target variable, and a list of features, \n    plot a correlation matrix for the target variable and features.\n    \n    Parameters:\n    df (DataFrame): The input pandas DataFrame.\n    target_variable (str): The target variable in the DataFrame.\n    features (list): The list of features in the DataFrame.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - seaborn\n    \n    Example:\n    >>> df = pd.DataFrame({\n    ...     'target': [1, 0, 1, 0, 1, 0, 1],\n    ...     'feature1': [1, 2, 3, 4, 5, 6, 7],\n    ...     'feature2': [2, 3, 1, 2, 3, 1, 2],\n    ...     'feature3': [3, 1, 2, 3, 1, 2, 3]\n    ... })\n    >>> plot_correlation_matrix(df, 'target', ['feature1', 'feature2', 'feature3'])\n    \"\"\"\n    corr_df = df[[target_variable] + features].corr()\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(corr_df, annot=True, cmap='coolwarm')\n    plt.title('Correlation Matrix')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, matplotlib, numpy, csv\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport csv\n\n# Constants\nCSV_FILE = 'data.csv'\n\ndef plot_scatter_matrix(csv_file):\n    \"\"\"\n    Given a CSV file, plot a scatter matrix for all the numerical columns.\n    \n    Parameters:\n    csv_file (str): The path to the CSV file.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - numpy\n    - csv\n    \n    Example:\n    >>> plot_scatter_matrix('data.csv')\n    \"\"\"\n    df = pd.read_csv(csv_file)\n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n    pd.plotting.scatter_matrix(df[numeric_cols], figsize=(12, 8))\n    plt.show()\n```"}
{"seed": "{\"task_id\": 407127, \"prompt\": \"def f_407127(my_list, n):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"random.sample(set(my_list),n)\", \"test_start\": \"\\nimport random \\n\\ndef check(candidate):\", \"test\": [\"\\n    data = [i for i in range(10)]\\n    s1 = candidate(data, 3)\\n    assert len(s1) == 3\\n    assert all([(a in data) for a in s1])\\n\", \"\\n    data = [i//2 for i in range(10)]\\n    s2 = candidate(data, 4)\\n    assert len(s2) == 4\\n    assert all([(a in data) for a in s2])\\n    assert len(set(s2)) == 4\\n\"], \"entry_point\": \"f_407127\", \"intent\": \"\\u0412\\u044b\\u0432\\u0435\\u0441\\u0442\\u0438 `n` \\u0441\\u043b\\u043e\\u0432 \\u0438\\u0437 \\u0441\\u043f\\u0438\\u0441\\u043a\\u0430 `my_list` \\u0432 \\u0441\\u043b\\u0443\\u0447\\u0430\\u0439\\u043d\\u043e\\u043c \\u043f\\u043e\\u0440\\u044f\\u0434\\u043a\\u0435 \\u0431\\u0435\\u0437 \\u043f\\u043e\\u0432\\u0442\\u043e\\u0440\\u0435\\u043d\\u0438\\u0439\", \"library\": [\"random\"]}", "generation": "Scenario 1:\nrandom, numpy, pandas, matplotlib\n```python\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nELEMENTS = ['H', 'He', 'Li', 'Be', 'B', 'C', 'N', 'O', 'F', 'Ne']\n\ndef generate_chemical_composition(n_samples, n_elements):\n    \"\"\"\n    Generate a DataFrame of chemical compositions with n_samples rows and n_elements columns.\n    Each row represents a sample and each column represents a unique chemical element.\n    The values are randomly generated percentages of the elements in each sample.\n\n    Parameters:\n    n_samples (int): The number of samples.\n    n_elements (int): The number of elements.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the chemical compositions.\n\n    Requirements:\n    - random\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> composition = generate_chemical_composition(100, 5)\n    >>> print(composition)\n    >>> composition.plot(kind='bar', stacked=True)\n    \"\"\"\n    selected_elements = random.sample(ELEMENTS, n_elements)\n\n    data = np.random.dirichlet(np.ones(n_elements), size=n_samples)\n    composition_df = pd.DataFrame(data, columns=selected_elements)\n\n    return composition_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nrandom, collections, itertools, re\n```python\nimport random\nimport collections\nimport itertools\nimport re\n\n# Constants\nLETTERS = 'abcdefghijklmnopqrstuvwxyz'\nNUMBERS = '0123456789'\nSPECIAL_CHARS = '!@#$%^&*()'\nPASSWORD_LENGTH = 8\n\ndef generate_passwords(n, complexity):\n    \"\"\"\n    Generate a list of n passwords of a given complexity.\n\n    The complexity is a string with any combination of 'L', 'N', and 'S', which stand for\n    Letters, Numbers, and Special characters respectively.\n\n    Each password should be of PASSWORD_LENGTH characters.\n\n    Parameters:\n    n (int): The number of passwords to generate.\n    complexity (str): The complexity of the passwords.\n\n    Returns:\n    list: A list of generated passwords.\n\n    Requirements:\n    - random\n    - collections\n    - itertools\n    - re\n\n    Example:\n    >>> passwords = generate_passwords(10, 'LNS')\n    >>> print(passwords)\n    \"\"\"\n    char_sets = {'L': LETTERS, 'N': NUMBERS, 'S': SPECIAL_CHARS}\n    char_set = ''.join(char_sets[c] for c in complexity)\n\n    passwords = [''.join(random.sample(char_set, PASSWORD_LENGTH)) for _ in range(n)]\n\n    return passwords\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nrandom, math, matplotlib, numpy\n```python\nimport random\nimport math\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nRADIUS = 5\n\ndef estimate_pi(n_samples):\n    \"\"\"\n    Estimate the value of pi using Monte Carlo method.\n\n    Generate n_samples random points in a square of side 2*RADIUS.\n    Count the number of points that fall within a circle of radius RADIUS.\n    Use this to estimate the value of pi.\n\n    Plot the generated points and the circle for visualization.\n\n    Parameters:\n    n_samples (int): The number of samples to generate.\n\n    Returns:\n    float: The estimated value of pi.\n\n    Requirements:\n    - random\n    - math\n    - matplotlib\n    - numpy\n\n    Example:\n    >>> pi_estimate = estimate_pi(10000)\n    >>> print(f'Estimated value of pi: {pi_estimate}')\n    \"\"\"\n    inside_circle = 0\n    points = []\n\n    for _ in range(n_samples):\n        x = random.uniform(-RADIUS, RADIUS)\n        y = random.uniform(-RADIUS, RADIUS)\n\n        points.append((x, y))\n\n        if math.sqrt(x**2 + y**2) <= RADIUS:\n            inside_circle += 1\n\n    pi_estimate = 4 * inside_circle / n_samples\n\n    xs, ys = zip(*points)\n    circle = plt.Circle((0, 0), RADIUS, fill=False)\n    fig, ax = plt.subplots()\n    ax.add_artist(circle)\n    ax.scatter(xs, ys, s=1)\n    ax.set_aspect('equal')\n\n    plt.show()\n\n    return pi_estimate\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nrandom, collections, string\n```python\nimport random\nimport collections\nimport string\n\n# Constants\nLETTERS = string.ascii_letters\n\ndef generate_string_histogram(n_chars, n_most_common):\n    \"\"\"\n    Generate a histogram of the most common characters in a randomly generated string of n_chars characters.\n\n    Parameters:\n    n_chars (int): The number of characters in the string.\n    n_most_common (int): The number of most common characters to include in the histogram.\n\n    Returns:\n    list: A list of tuples with the most common characters and their counts.\n\n    Requirements:\n    - random\n    - collections\n    - string\n\n    Example:\n    >>> histogram = generate_string_histogram(1000, 5)\n    >>> print(histogram)\n    \"\"\"\n    generated_string = ''.join(random.choice(LETTERS) for _ in range(n_chars))\n\n    char_counts = collections.Counter(generated_string)\n    histogram = char_counts.most_common(n_most_common)\n\n    return histogram\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nrandom, pandas, numpy, matplotlib\n```python\nimport random\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCOUNTRIES = ['USA', 'UK', 'France', 'Germany', 'Italy', 'Spain', 'Japan', 'China', 'India', 'Australia']\n\ndef generate_gdp_data(n_years):\n    \"\"\"\n    Generate a DataFrame of GDP data for a list of countries over n_years years.\n\n    The GDP values are randomly generated.\n\n    Plot the GDP data for visualization.\n\n    Parameters:\n    n_years (int): The number of years.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the GDP data.\n\n    Requirements:\n    - random\n    - pandas\n    - numpy\n    - matplotlib\n\n    Example:\n    >>> gdp_data = generate_gdp_data(10)\n    >>> print(gdp_data)\n    >>> gdp_data.plot()\n    \"\"\"\n    years = range(2000, 2000 + n_years)\n    data = {country: np.random.uniform(1e12, 2e13, n_years) for country in COUNTRIES}\n    gdp_df = pd.DataFrame(data, index=years)\n\n    return gdp_df\n```\nAbove is the illustration."}
{"seed": "{\"task_id\": 446920, \"prompt\": \"def f_446920(my_list, my_tuple):\\n\\t\", \"suffix\": \"\\n\\treturn my_list\", \"canonical_solution\": \"my_list.extend(my_tuple)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([1,2,3], (2,3)) == [1,2,3,2,3]\\n\", \"\\n    assert candidate([1,2,3], (2,)) == [1,2,3,2]\\n\"], \"entry_point\": \"f_446920\", \"intent\": \"\\u0414\\u043e\\u0431\\u0430\\u0432\\u0438\\u0442\\u044c \\u043a\\u043e\\u0440\\u0442\\u0435\\u0436 `my_tuple` \\u043a \\u0441\\u043f\\u0438\\u0441\\u043a\\u0443 `my_list`\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, random, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nFRUITS = ['Apple', 'Banana', 'Cherry', 'Date', 'Elderberry']\nPRICES = [1.2, 0.5, 2.3, 3.0, 4.5]\n\ndef calculate_total_price(basket):\n    \"\"\"\n    Calculate total price for a basket of fruits. Each fruit has a specific price.\n    \n    Parameters:\n    basket (list of tuples): Each tuple contains the fruit name and the quantity.\n    \n    Returns:\n    float: The total price for the basket.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> basket = [('Apple', 3), ('Banana', 5)]\n    >>> total_price = calculate_total_price(basket)\n    >>> print(total_price)\n    \"\"\"\n    price_df = pd.DataFrame({'Fruit': FRUITS, 'Price': PRICES})\n\n    total_price = 0.0\n    for fruit, quantity in basket:\n        fruit_price = price_df[price_df['Fruit'] == fruit]['Price'].values[0]\n        total_price += fruit_price * quantity\n\n    return total_price\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, itertools, random\n```python\nimport numpy as np\nimport itertools\nfrom random import randint\n\n# Constants\nNUMBERS = [1, 2, 3, 4, 5]\n\ndef generate_random_combinations(n):\n    \"\"\"\n    Generate n random combinations from a list of numbers.\n    \n    Parameters:\n    n (int): The number of combinations to generate.\n    \n    Returns:\n    list: A list of combinations.\n    \n    Requirements:\n    - numpy\n    - itertools\n    - random\n    \n    Example:\n    >>> combinations = generate_random_combinations(3)\n    >>> print(combinations)\n    \"\"\"\n    combinations = list(itertools.combinations(NUMBERS, n))\n    random_combinations = [combinations[randint(0, len(combinations) - 1)] for _ in range(n)]\n\n    return random_combinations\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nSTUDENTS = ['Harry', 'Hermione', 'Ron', 'Draco', 'Neville']\nGRADES = [85, 95, 80, 70, 90]\n\ndef visualize_student_grades():\n    \"\"\"\n    Visualize student grades using a bar chart.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib\n    \n    Example:\n    >>> visualize_student_grades()\n    \"\"\"\n    grade_df = pd.DataFrame({'Student': STUDENTS, 'Grade': GRADES})\n    grade_df.sort_values('Grade', inplace=True)\n\n    plt.bar(grade_df['Student'], grade_df['Grade'])\n    plt.xlabel('Students')\n    plt.ylabel('Grades')\n    plt.title('Student Grades')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, matplotlib, sklearn\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n# Constants\nX = np.array([1, 2, 3, 4, 5]).reshape((-1, 1))\nY = np.array([2, 4, 5, 4, 5])\n\ndef fit_linear_regression():\n    \"\"\"\n    Fit a linear regression model to a given data and visualize the result.\n    \n    Requirements:\n    - numpy\n    - matplotlib\n    - sklearn.linear_model.LinearRegression\n    \n    Example:\n    >>> fit_linear_regression()\n    \"\"\"\n    model = LinearRegression().fit(X, Y)\n\n    Y_pred = model.predict(X)\n\n    plt.scatter(X, Y)\n    plt.plot(X, Y_pred, color='red')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCOUNTRIES = ['USA', 'China', 'Russia', 'India', 'UK']\nPOPULATIONS = [331, 1441, 146, 1393, 68]  # in millions\n\ndef visualize_country_populations():\n    \"\"\"\n    Visualize the populations of different countries using a pie chart.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib\n    \n    Example:\n    >>> visualize_country_populations()\n    \"\"\"\n    population_df = pd.DataFrame({'Country': COUNTRIES, 'Population': POPULATIONS})\n\n    plt.pie(population_df['Population'], labels=population_df['Country'], autopct='%1.1f%%')\n    plt.title('Country Populations')\n    plt.show()\n```\n`GPT_ODEX_BREAK`"}
{"seed": "{\"task_id\": 103581, \"prompt\": \"def f_103581(s, i, symbol):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"s[:i] + symbol + s[i + 1:]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('dsgfjksdlh', 3, 'abc') == 'dsgabcjksdlh'\\n\", \"\\n    assert candidate('dsgfjksdlh', 3, '.') == 'dsg.jksdlh'\\n\"], \"entry_point\": \"f_103581\", \"intent\": \"\\u0417\\u0430\\u043c\\u0435\\u043d\\u0438\\u0442\\u044c \\u0432 \\u0441\\u0442\\u0440\\u043e\\u043a\\u0435 `s` \\u0441\\u0438\\u043c\\u0432\\u043e\\u043b \\u043d\\u0430 \\u043f\\u043e\\u0437\\u0438\\u0446\\u0438\\u0438 `i` \\u043d\\u0430 \\u0441\\u0438\\u043c\\u0432\\u043e\\u043b `symbol`\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nSYMBOLS = ['$', '#', '@', '&', '%']\n\ndef generate_symbol_frequency(s, symbol, i):\n    \"\"\"\n    Replace a character at position `i` in string `s` with `symbol`, \n    then generate a bar plot of the frequency of each special symbol in the modified string.\n\n    Parameters:\n    s (str): The input string.\n    symbol (str): The symbol to replace the character at position `i`.\n    i (int): The position in the string to replace.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Examples:\n    >>> generate_symbol_frequency('abcdefghijklmnopqrstuvwxz', '#', 3)\n    >>> generate_symbol_frequency('abcdefghijklmnopqrstuvwxz', '&', 0)\n    \"\"\"\n    # Replace character at position i\n    s = s[:i] + symbol + s[i + 1:]\n\n    # Count the frequency of each symbol\n    symbol_counts = pd.Series([s.count(symbol) for symbol in SYMBOLS], index=SYMBOLS)\n\n    # Generate bar plot\n    symbol_counts.plot(kind='bar', color='b', alpha=0.7)\n    plt.xlabel('Symbols')\n    plt.ylabel('Frequency')\n    plt.title('Frequency of Each Special Symbol')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, re\n```python\nimport pandas as pd\nimport numpy as np\nimport re\n\n# Constants\nSYMBOLS = ['$', '#', '@', '&', '%']\n\ndef count_symbol_occurrences(s, symbol, i):\n    \"\"\"\n    Replace a character at position `i` in string `s` with `symbol`, \n    then count the occurrences of each special symbol in the modified string.\n\n    Parameters:\n    s (str): The input string.\n    symbol (str): The symbol to replace the character at position `i`.\n    i (int): The position in the string to replace.\n\n    Returns:\n    pandas.Series: A series with the counts of each special symbol.\n\n    Requirements:\n    - pandas\n    - numpy\n    - re\n\n    Examples:\n    >>> count_symbol_occurrences('abcdefghijklmnopqrstuvwxz', '#', 3)\n    >>> count_symbol_occurrences('abcdefghijklmnopqrstuvwxz', '&', 0)\n    \"\"\"\n    # Replace character at position i\n    s = s[:i] + symbol + s[i + 1:]\n\n    # Count the occurrences of each symbol\n    counts = pd.Series([len(re.findall('\\\\'+symbol, s)) for symbol in SYMBOLS], index=SYMBOLS)\n\n    return counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, random\n```python\nimport pandas as pd\nimport numpy as np\nimport random\n\n# Constants\nSYMBOLS = ['$', '#', '@', '&', '%']\n\ndef replace_and_shuffle(s, symbol, i):\n    \"\"\"\n    Replace a character at position `i` in string `s` with `symbol`, \n    then shuffle the characters of the modified string.\n\n    Parameters:\n    s (str): The input string.\n    symbol (str): The symbol to replace the character at position `i`.\n    i (int): The position in the string to replace.\n\n    Returns:\n    str: The shuffled string.\n\n    Requirements:\n    - pandas\n    - numpy\n    - random\n\n    Examples:\n    >>> replace_and_shuffle('abcdefghijklmnopqrstuvwxz', '#', 3)\n    >>> replace_and_shuffle('abcdefghijklmnopqrstuvwxz', '&', 0)\n    \"\"\"\n    # Replace character at position i\n    s = list(s[:i] + symbol + s[i + 1:])\n\n    # Shuffle the characters\n    random.shuffle(s)\n\n    return ''.join(s)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, collections\n```python\nimport pandas as pd\nimport numpy as np\nfrom collections import Counter\n\n# Constants\nSYMBOLS = ['$', '#', '@', '&', '%']\n\ndef replace_and_count(s, symbol, i):\n    \"\"\"\n    Replace a character at position `i` in string `s` with `symbol`, \n    then count the occurrence of each character in the modified string.\n\n    Parameters:\n    s (str): The input string.\n    symbol (str): The symbol to replace the character at position `i`.\n    i (int): The position in the string to replace.\n\n    Returns:\n    collections.Counter: A Counter object with the counts of each character.\n\n    Requirements:\n    - pandas\n    - numpy\n    - collections\n\n    Examples:\n    >>> replace_and_count('abcdefghijklmnopqrstuvwxz', '#', 3)\n    >>> replace_and_count('abcdefghijklmnopqrstuvwxz', '&', 0)\n    \"\"\"\n    # Replace character at position i\n    s = s[:i] + symbol + s[i + 1:]\n\n    # Count the occurrences of each character\n    counts = Counter(s)\n\n    return counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, operator\n```python\nimport pandas as pd\nimport numpy as np\nimport operator\n\n# Constants\nSYMBOLS = ['$', '#', '@', '&', '%']\n\ndef replace_and_sort(s, symbol, i):\n    \"\"\"\n    Replace a character at position `i` in string `s` with `symbol`, \n    then sort the characters of the modified string in descending order of their ASCII values.\n\n    Parameters:\n    s (str): The input string.\n    symbol (str): The symbol to replace the character at position `i`.\n    i (int): The position in the string to replace.\n\n    Returns:\n    str: The sorted string.\n\n    Requirements:\n    - pandas\n    - numpy\n    - operator\n\n    Examples:\n    >>> replace_and_sort('abcdefghijklmnopqrstuvwxz', '#', 3)\n    >>> replace_and_sort('abcdefghijklmnopqrstuvwxz', '&', 0)\n    \"\"\"\n    # Replace character at position i\n    s = list(s[:i] + symbol + s[i + 1:])\n\n    # Sort the characters\n    s.sort(key=operator.itemgetter(0), reverse=True)\n\n    return ''.join(s)\n```"}
{"seed": "{\"task_id\": 28096, \"prompt\": \"def f_28096(my_list):\\n\\t\", \"suffix\": \"\\n\\treturn my_list\", \"canonical_solution\": \"my_list = sorted(my_list)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([3,2,1]) == [1,2,3]\\n\", \"\\n    assert candidate([1,1,1]) == [1,1,1]\\n\", \"\\n    assert candidate(['a', 'b', 'c']) == ['a', 'b', 'c']\\n\"], \"entry_point\": \"f_28096\", \"intent\": \"\\u041e\\u0442\\u0441\\u043e\\u0440\\u0442\\u0438\\u0440\\u043e\\u0432\\u0430\\u0442\\u044c \\u0441\\u043f\\u0438\\u0441\\u043e\\u043a `my_list`\", \"library\": []}", "generation": "Scenario 1:\npandas, numpy, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nDATA = np.random.randint(1, 100, size=(100, 5))\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef sort_and_visualize_data(data):\n    \"\"\"\n    Sort the data in each column of the DataFrame, calculate the mean of each column, \n    and visualize the sorted data and mean values using a bar plot.\n\n    Parameters:\n    data (numpy.ndarray): The 2D array to be sorted and visualized.\n\n    Returns:\n    DataFrame: A pandas DataFrame with sorted data.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> data = sort_and_visualize_data(DATA)\n    >>> print(data)\n    \"\"\"\n    df = pd.DataFrame(data, columns=COLUMNS)\n    df = df.apply(np.sort)\n\n    means = df.mean()\n    plt.bar(COLUMNS, means)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nstatistics, numpy, random\n```python\nimport statistics\nimport numpy as np\nfrom random import shuffle\n\n# Constants\nNUMBERS = np.random.randint(1, 100, size=100)\n\ndef shuffle_and_analyze(numbers):\n    \"\"\"\n    Shuffle the input list of numbers, calculate the mean, median, and standard \n    deviation, and return these statistics along with the shuffled list.\n\n    Parameters:\n    numbers (list): The list of numbers to be shuffled and analyzed.\n\n    Returns:\n    tuple: A tuple with the shuffled list, mean, median, and standard deviation.\n\n    Requirements:\n    - statistics\n    - numpy\n    - random\n\n    Example:\n    >>> shuffled_numbers, mean, median, stdev = shuffle_and_analyze(NUMBERS)\n    >>> print(f'Mean: {mean}, Median: {median}, Standard Deviation: {stdev}')\n    \"\"\"\n    shuffle(numbers)\n    mean = statistics.mean(numbers)\n    median = statistics.median(numbers)\n    stdev = statistics.stdev(numbers)\n\n    return numbers, mean, median, stdev\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, scipy.stats\n```python\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\n\n# Constants\nDATA = np.random.randint(1, 100, size=(100, 5))\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef sort_and_analyze_data(data):\n    \"\"\"\n    Sort the data in each column of the DataFrame, calculate the skewness and kurtosis \n    of each column, and return these statistics along with the sorted DataFrame.\n\n    Parameters:\n    data (numpy.ndarray): The 2D array to be sorted and analyzed.\n\n    Returns:\n    DataFrame: A pandas DataFrame with sorted data.\n\n    Requirements:\n    - pandas\n    - numpy\n    - scipy.stats\n\n    Example:\n    >>> data = sort_and_analyze_data(DATA)\n    >>> print(data)\n    \"\"\"\n    df = pd.DataFrame(data, columns=COLUMNS)\n    df = df.apply(np.sort)\n\n    skewness = df.apply(stats.skew)\n    kurtosis = df.apply(stats.kurtosis)\n\n    return df, skewness, kurtosis\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nitertools, math, random\n```python\nfrom itertools import permutations\nfrom math import factorial\nfrom random import shuffle\n\n# Constants\nNUMBERS = list(range(1, 6))\n\ndef shuffle_and_find_permutations(numbers):\n    \"\"\"\n    Shuffle the input list of numbers, calculate the total number of permutations, \n    and generate all permutations.\n\n    Parameters:\n    numbers (list): The list of numbers to be shuffled and permuted.\n\n    Returns:\n    list: A list of all permutations of the shuffled numbers.\n\n    Requirements:\n    - itertools\n    - math\n    - random\n\n    Example:\n    >>> permutations = shuffle_and_find_permutations(NUMBERS)\n    \"\"\"\n    shuffle(numbers)\n    num_permutations = factorial(len(numbers))\n    permutations_list = list(permutations(numbers))\n\n    assert len(permutations_list) == num_permutations\n\n    return permutations_list\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, sklearn.preprocessing\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Constants\nDATA = np.random.randint(1, 100, size=(100, 5))\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef sort_and_scale_data(data):\n    \"\"\"\n    Sort the data in each column of the DataFrame, scale the data to the range 0-1, \n    and return the scaled DataFrame.\n\n    Parameters:\n    data (numpy.ndarray): The 2D array to be sorted and scaled.\n\n    Returns:\n    DataFrame: A pandas DataFrame with scaled data.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.preprocessing\n\n    Example:\n    >>> data = sort_and_scale_data(DATA)\n    >>> print(data)\n    \"\"\"\n    df = pd.DataFrame(data, columns=COLUMNS)\n    df = df.apply(np.sort)\n\n    scaler = MinMaxScaler()\n    scaled_data = scaler.fit_transform(df)\n\n    return pd.DataFrame(scaled_data, columns=COLUMNS)\n```"}
{"seed": "{\"task_id\": 28096, \"prompt\": \"def f_28096(my_list):\\n\\t\", \"suffix\": \"\\n\\treturn my_list\", \"canonical_solution\": \"my_list.sort()\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([3,2,1]) == [1,2,3]\\n\", \"\\n    assert candidate([1,1,1]) == [1,1,1]\\n\", \"\\n    assert candidate(['a', 'b', 'c']) == ['a', 'b', 'c']\\n\"], \"entry_point\": \"f_28096\", \"intent\": \"\\u041e\\u0442\\u0441\\u043e\\u0440\\u0442\\u0438\\u0440\\u043e\\u0432\\u0430\\u0442\\u044c \\u0441\\u043f\\u0438\\u0441\\u043e\\u043a `my_list`\", \"library\": []}", "generation": "Scenario 1: numpy, collections, random\n```python\nimport numpy as np\nfrom collections import Counter\nimport random\n\ndef generate_and_sort_random_array(size, low, high):\n    \"\"\"\n    Generate a random integer array of a given size within a specific range, \n    sort the array, and return the sorted array along with a frequency count of each element.\n\n    Parameters:\n    size (int): The size of the array.\n    low (int): The lower bound of the range.\n    high (int): The upper bound of the range.\n\n    Returns:\n    tuple: A tuple containing the sorted array and a Counter object with frequency counts.\n\n    Requirements:\n    - numpy\n    - collections.Counter\n    - random\n\n    Example:\n    >>> array, counts = generate_and_sort_random_array(10, 1, 5)\n    >>> print(array)\n    >>> print(counts)\n    \"\"\"\n    array = np.random.randint(low, high, size)\n    sorted_array = np.sort(array)\n    counts = Counter(sorted_array)\n\n    return sorted_array, counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: numpy, pandas, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef analyze_and_plot_data(data):\n    \"\"\"\n    Analyze a given set of data, calculate the mean and standard deviation, \n    and plot a histogram of the data.\n\n    Parameters:\n    data (array-like): The data to analyze.\n\n    Returns:\n    tuple: A tuple containing the mean and standard deviation.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> mean, std_dev = analyze_and_plot_data(np.random.randn(1000))\n    >>> print('Mean:', mean, 'Std Dev:', std_dev)\n    \"\"\"\n    data_series = pd.Series(data)\n    mean = data_series.mean()\n    std_dev = data_series.std()\n\n    plt.hist(data_series, bins=50)\n    plt.show()\n\n    return mean, std_dev\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: numpy, scipy.stats, matplotlib\n```python\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef generate_and_analyze_normal_distribution(mean, std_dev):\n    \"\"\"\n    Generate a normal distribution with a given mean and standard deviation, \n    calculate the skewness and kurtosis, and plot the distribution.\n\n    Parameters:\n    mean (float): The mean of the distribution.\n    std_dev (float): The standard deviation of the distribution.\n\n    Returns:\n    tuple: A tuple containing the skewness and kurtosis.\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Example:\n    >>> skewness, kurtosis = generate_and_analyze_normal_distribution(0, 1)\n    >>> print('Skewness:', skewness, 'Kurtosis:', kurtosis)\n    \"\"\"\n    data = np.random.normal(mean, std_dev, 1000)\n    skewness = stats.skew(data)\n    kurtosis = stats.kurtosis(data)\n\n    plt.hist(data, bins=50)\n    plt.show()\n\n    return skewness, kurtosis\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: pandas, numpy, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef create_and_analyze_dataframe(n_rows, n_cols):\n    \"\"\"\n    Create a DataFrame with random data, calculate the mean of each column, \n    and plot a line graph of the means.\n\n    Parameters:\n    n_rows (int): The number of rows in the DataFrame.\n    n_cols (int): The number of columns in the DataFrame.\n\n    Returns:\n    pd.Series: A Series containing the mean of each column.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> means = create_and_analyze_dataframe(1000, 5)\n    >>> print(means)\n    \"\"\"\n    df = pd.DataFrame(np.random.rand(n_rows, n_cols), columns=[f'Col{i+1}' for i in range(n_cols)])\n    means = df.mean()\n\n    means.plot()\n    plt.show()\n\n    return means\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: numpy, scipy.optimize, matplotlib\n```python\nimport numpy as np\nfrom scipy import optimize\nimport matplotlib.pyplot as plt\n\ndef fit_curve_and_plot(x, y):\n    \"\"\"\n    Fit a curve to a given set of data using a least squares method, \n    plot the original data and the fitted curve, and return the parameters of the fitted curve.\n\n    Parameters:\n    x (numpy.array): The x-coordinates of the data.\n    y (numpy.array): The y-coordinates of the data.\n\n    Returns:\n    numpy.array: The parameters of the fitted curve.\n\n    Requirements:\n    - numpy\n    - scipy.optimize\n    - matplotlib.pyplot\n\n    Example:\n    >>> x = np.linspace(0, 10, 100)\n    >>> y = 3.5*x + 2 + np.random.randn(100)\n    >>> params = fit_curve_and_plot(x, y)\n    >>> print(params)\n    \"\"\"\n    def func(x, a, b):\n        return a * x + b\n\n    params, params_covariance = optimize.curve_fit(func, x, y)\n\n    plt.scatter(x, y, label='Data')\n    plt.plot(x, func(x, *params), label='Fitted function')\n    plt.legend(loc='best')\n    plt.show()\n\n    return params\n```"}
{"seed": "{\"task_id\": 133137, \"prompt\": \"def f_133137():\\n\\t\", \"suffix\": \"\\n\\treturn my_ip\", \"canonical_solution\": \"conn = http.client.HTTPConnection(\\\"ifconfig.me\\\")\\n\\n\\tconn.request(\\\"GET\\\", \\\"/ip\\\")\\n\\n\\tmy_ip = conn.getresponse().read()\\n\", \"test_start\": \"\\nimport http.client\\n\\ndef check(candidate):\", \"test\": [\"\\n    ans = candidate()\\n    parts = ans.decode('utf-8').split('.')\\n    assert len(parts) == 4\\n\"], \"entry_point\": \"f_133137\", \"intent\": \"\\u041f\\u043e\\u043b\\u0443\\u0447\\u0438\\u0442\\u044c ip-\\u0430\\u0434\\u0440\\u0435\\u0441 `my_ip` \\u043a\\u043e\\u043c\\u043f\\u044c\\u044e\\u0442\\u0435\\u0440\\u0430 \\u0432 \\u0418\\u043d\\u0442\\u0435\\u0440\\u043d\\u0435\\u0442\\u0435 \", \"library\": [\"http\"]}", "generation": "Scenario 1:\nsocket, requests, pandas, numpy, matplotlib.pyplot\n```python\nimport socket\nimport requests\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nURL = 'https://api.ipify.org?format=json'\n\ndef get_local_and_public_ip():\n    \"\"\"\n    Get both local and public IP addresses and compare the first two parts of these addresses.\n    \n    Returns:\n    str: A string that indicates whether the first two parts of the local and public IP addresses are \n         the same or not.\n         \n    Requirements:\n    - socket\n    - requests\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> get_local_and_public_ip()\n    'The first two parts of the local and public IP addresses are not the same.'\n    \"\"\"\n    # Get local IP address\n    local_ip = socket.gethostbyname(socket.gethostname())\n    local_ip_parts = local_ip.split('.')\n    \n    # Get public IP address\n    response = requests.get(URL)\n    public_ip = response.json()['ip']\n    public_ip_parts = public_ip.split('.')\n    \n    # Compare the first two parts of the IP addresses\n    comparison = np.array(local_ip_parts[:2]) == np.array(public_ip_parts[:2])\n    \n    if comparison.all():\n        return 'The first two parts of the local and public IP addresses are the same.'\n    else:\n        return 'The first two parts of the local and public IP addresses are not the same.'\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nsocket, requests, pandas, matplotlib.pyplot\n```python\nimport socket\nimport requests\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n\n# Constants\nURL = 'https://ipinfo.io/json'\n\ndef get_ip_info():\n    \"\"\"\n    Get public IP address and other related information such as city, region, country, \n    and coordinates, and display it in a pandas DataFrame.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with IP information.\n    \n    Requirements:\n    - socket\n    - requests\n    - pandas\n    - matplotlib.pyplot\n    \n    Example:\n    >>> get_ip_info()\n    \"\"\"\n    # Get public IP address\n    response = requests.get(URL)\n    ip_info = response.json()\n    \n    # Create a DataFrame\n    ip_info_df = pd.DataFrame(ip_info.items(), columns=['Information', 'Value'])\n    \n    return ip_info_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nsocket, requests, pandas, matplotlib.pyplot\n```python\nimport socket\nimport requests\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nURL = 'https://ip-api.com/json/'\n\ndef get_ip_location():\n    \"\"\"\n    Get public IP address and its geographical location (latitude and longitude), \n    and plot the location on a scatter plot.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - socket\n    - requests\n    - pandas\n    - matplotlib.pyplot\n    \n    Example:\n    >>> get_ip_location()\n    \"\"\"\n    # Get public IP address\n    response = requests.get(URL)\n    ip_info = response.json()\n    \n    # Plot the location\n    plt.scatter(ip_info['lon'], ip_info['lat'])\n    plt.title(f\"Location of IP: {ip_info['query']}\")\n    plt.xlabel(\"Longitude\")\n    plt.ylabel(\"Latitude\")\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nsocket, requests, pandas, json, matplotlib.pyplot\n```python\nimport socket\nimport requests\nimport pandas as pd\nimport json\nfrom matplotlib import pyplot as plt\n\n# Constants\nURL = 'https://ipapi.co/json/'\n\ndef get_ip_and_time_zone():\n    \"\"\"\n    Get public IP address and its timezone, and display it in a pandas DataFrame.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with IP information.\n    \n    Requirements:\n    - socket\n    - requests\n    - pandas\n    - json\n    - matplotlib.pyplot\n    \n    Example:\n    >>> get_ip_and_time_zone()\n    \"\"\"\n    # Get public IP address\n    response = requests.get(URL)\n    ip_info = json.loads(response.text)\n    \n    # Create a DataFrame\n    ip_info_df = pd.DataFrame(ip_info.items(), columns=['Information', 'Value'])\n    \n    return ip_info_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nsocket, requests, pandas, matplotlib.pyplot\n```python\nimport socket\nimport requests\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nURL = 'https://geoip-db.com/json/'\n\ndef get_ip_and_country():\n    \"\"\"\n    Get public IP address and its country, and display this information in a pandas DataFrame.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with IP information.\n    \n    Requirements:\n    - socket\n    - requests\n    - pandas\n    - matplotlib.pyplot\n    \n    Example:\n    >>> get_ip_and_country()\n    \"\"\"\n    # Get public IP address\n    response = requests.get(URL)\n    ip_info = response.json()\n    \n    # Create a DataFrame\n    ip_info_df = pd.DataFrame(ip_info.items(), columns=['Information', 'Value'])\n    \n    return ip_info_df\n```\n"}
{"seed": "{\"task_id\": 134564, \"prompt\": \"def f_134564(first_date, second_date):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"second_date - first_date\", \"test_start\": \"\\nimport datetime\\n\\ndef check(candidate):\", \"test\": [\"\\n    d1 = datetime.datetime(2000, 1, 2)\\n    d2 = datetime.datetime(2000, 2, 2)\\n    assert candidate(d1, d2) == datetime.timedelta(days=31)\\n\"], \"entry_point\": \"f_134564\", \"intent\": \"\\u041d\\u0430\\u0439\\u0442\\u0438 \\u0441\\u043a\\u043e\\u043b\\u044c\\u043a\\u043e \\u0432\\u0440\\u0435\\u043c\\u0435\\u043d\\u0438 \\u043f\\u0440\\u043e\\u0448\\u043b\\u043e \\u043c\\u0435\\u0436\\u0434\\u0443 \\u0434\\u0430\\u0442\\u0430\\u043c\\u0438 \\u0432 \\u0444\\u043e\\u0440\\u043c\\u0430\\u0442\\u0435 datetime `first_date` \\u0438 `second_date` \", \"library\": [\"datetime\"]}", "generation": "Scenario 1: \npandas, datetime, random, matplotlib, numpy\n```python\nimport pandas as pd\nfrom datetime import datetime\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nEMPLOYEES = ['John', 'Sarah', 'Michael', 'Emma', 'David']\nPROJECTS = ['Project A', 'Project B', 'Project C', 'Project D', 'Project E']\n\ndef generate_employee_project_timings(first_date, second_date):\n    \"\"\"\n    Generate a report of the time spent by different employees on various projects \n    between two given dates. \n\n    Parameters:\n    first_date (datetime): The starting date.\n    second_date (datetime): The ending date.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the time spent by each employee in hours.\n\n    Requirements:\n    - pandas\n    - datetime\n    - random\n    - matplotlib.pyplot\n    - numpy\n    \n    Example:\n    >>> start_date = datetime(2022, 1, 1)\n    >>> end_date = datetime(2022, 12, 31)\n    >>> report = generate_employee_project_timings(start_date, end_date)\n    >>> print(report)\n    >>> report.plot(kind='bar')\n    \"\"\"\n    report_data = []\n\n    for employee in EMPLOYEES:\n        for project in PROJECTS:\n            time_spent = randint(100, 1000) # random hours between the given dates\n            report_data.append([employee, project, time_spent])\n\n    report_df = pd.DataFrame(report_data, columns=['Employee', 'Project', 'Time Spent (hours)'])\n\n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ndatetime, timedelta, dateutil, numpy\n```python\nfrom datetime import datetime, timedelta\nfrom dateutil.relativedelta import relativedelta\nimport numpy as np\n\n# Constants\nWEEKENDS = [5, 6] # 5 and 6 represents Saturday and Sunday\n\ndef count_working_days(first_date, second_date):\n    \"\"\"\n    Calculate the number of working days (excluding weekends) between two given dates.\n\n    Parameters:\n    first_date (datetime): The starting date.\n    second_date (datetime): The ending date.\n\n    Returns:\n    int: The number of working days.\n\n    Requirements:\n    - datetime\n    - timedelta\n    - dateutil.relativedelta\n    - numpy\n\n    Example:\n    >>> start_date = datetime(2022, 1, 1)\n    >>> end_date = datetime(2022, 12, 31)\n    >>> working_days = count_working_days(start_date, end_date)\n    \"\"\"\n    total_days = np.busday_count(first_date.strftime('%Y-%m-%d'), second_date.strftime('%Y-%m-%d'))\n\n    return total_days\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \npandas, datetime, matplotlib\n```python\nimport pandas as pd\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\n\n# Sample Data\ndata = {'Date': pd.date_range(start='1/1/2020', end='12/31/2020'),\n        'Sales': [i for i in range(1, 367)]}\ndf = pd.DataFrame(data)\n\ndef plot_sales_between_dates(first_date, second_date):\n    \"\"\"\n    Plot the sales data between two given dates. \n\n    Parameters:\n    first_date (datetime): The starting date.\n    second_date (datetime): The ending date.\n    \n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - datetime\n    - matplotlib\n\n    Example:\n    >>> start_date = datetime(2020, 1, 1)\n    >>> end_date = datetime(2020, 12, 31)\n    >>> plot_sales_between_dates(start_date, end_date)\n    \"\"\"\n    mask = (df['Date'] >= first_date) & (df['Date'] <= second_date)\n    filtered_df = df.loc[mask]\n\n    plt.plot(filtered_df['Date'], filtered_df['Sales'])\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ndatetime, dateutil, pytz\n```python\nfrom datetime import datetime\nfrom dateutil.parser import parse\nimport pytz\n\ndef convert_datetime_tz(first_date, second_date, from_tz, to_tz):\n    \"\"\"\n    Convert the datetime from one timezone to another between two given dates.\n\n    Parameters:\n    first_date (str): The starting date in \"yyyy-mm-dd hh:mm:ss\" format.\n    second_date (str): The ending date in \"yyyy-mm-dd hh:mm:ss\" format.\n    from_tz (str): The original timezone.\n    to_tz (str): The timezone to which the datetime should be converted.\n\n    Returns:\n    List[datetime]: A list of datetime objects in the new timezone.\n\n    Requirements:\n    - datetime\n    - dateutil.parser\n    - pytz\n\n    Example:\n    >>> start_date = '2022-01-01 00:00:00'\n    >>> end_date = '2022-12-31 23:59:59'\n    >>> from_tz = 'UTC'\n    >>> to_tz = 'America/New_York'\n    >>> dates = convert_datetime_tz(start_date, end_date, from_tz, to_tz)\n    \"\"\"\n    from_tz = pytz.timezone(from_tz)\n    to_tz = pytz.timezone(to_tz)\n\n    first_date = parse(first_date).replace(tzinfo=from_tz)\n    second_date = parse(second_date).replace(tzinfo=from_tz)\n\n    date_range = []\n\n    while first_date <= second_date:\n        date_range.append(first_date.astimezone(to_tz))\n        first_date += timedelta(days=1)\n\n    return date_range\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\ndatetime, calendar, pandas\n```python\nfrom datetime import datetime\nimport calendar\nimport pandas as pd\n\ndef get_monthly_pay_dates(first_date, second_date):\n    \"\"\"\n    Generate a list of monthly pay dates (last working day of the month) between two given dates.\n\n    Parameters:\n    first_date (datetime): The starting date.\n    second_date (datetime): The ending date.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the monthly pay dates.\n\n    Requirements:\n    - datetime\n    - calendar\n    - pandas\n\n    Example:\n    >>> start_date = datetime(2022, 1, 1)\n    >>> end_date = datetime(2022, 12, 31)\n    >>> pay_dates = get_monthly_pay_dates(start_date, end_date)\n    \"\"\"\n    pay_dates = []\n\n    for year in range(first_date.year, second_date.year + 1):\n        for month in range(1, 13):\n            if datetime(year, month, 1) >= first_date and datetime(year, month, 1) <= second_date:\n                last_day = calendar.monthrange(year, month)[1]\n                if calendar.weekday(year, month, last_day) in [5, 6]: # if Saturday or Sunday\n                    last_day -= 1\n                pay_date = datetime(year, month, last_day)\n                pay_dates.append(pay_date)\n\n    pay_dates_df = pd.DataFrame(pay_dates, columns=['Pay Date'])\n\n    return pay_dates_df\n```"}
{"seed": "{\"task_id\": 236635, \"prompt\": \"def f_236635():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"sys.argv\", \"test_start\": \"\\nimport sys\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate() == sys.argv\\n\"], \"entry_point\": \"f_236635\", \"intent\": \"\\u041f\\u043e\\u043b\\u0443\\u0447\\u0438\\u0442\\u044c \\u0441\\u043f\\u0438\\u0441\\u043e\\u043a \\u043f\\u0430\\u0440\\u0430\\u043c\\u0435\\u0442\\u0440\\u043e\\u0432 `params` \\u0438\\u0437 \\u043a\\u043e\\u043c\\u0430\\u043d\\u0434\\u043d\\u043e\\u0439 \\u0441\\u0442\\u0440\\u043e\\u043a\\u0438\", \"library\": [\"sys\"]}", "generation": "Scenario 1:\nsys, os, shutil, argparse\n```python\nimport sys\nimport os\nimport shutil\nimport argparse\n\ndef handle_files():\n    \"\"\"\n    Handle files based on command line arguments. The user can copy, move or delete a file.\n    \n    Requirements:\n    - sys\n    - os\n    - shutil\n    - argparse\n    \n    Usage:\n    python file_handler.py --action copy --source /path/to/source --destination /path/to/destination\n    python file_handler.py --action move --source /path/to/source --destination /path/to/destination\n    python file_handler.py --action delete --source /path/to/source\n    \"\"\"\n    parser = argparse.ArgumentParser(description=\"Handle files\")\n    parser.add_argument(\"--action\", type=str, required=True, choices=[\"copy\", \"move\", \"delete\"],\n                        help=\"Action to perform on the file (copy/move/delete)\")\n    parser.add_argument(\"--source\", type=str, required=True, help=\"Source file path\")\n    parser.add_argument(\"--destination\", type=str, help=\"Destination file path\")\n\n    args = parser.parse_args()\n\n    if args.action == \"copy\":\n        shutil.copy2(args.source, args.destination)\n    elif args.action == \"move\":\n        shutil.move(args.source, args.destination)\n    elif args.action == \"delete\":\n        os.remove(args.source)\n\n    return 0\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nsys, subprocess, os, time\n```python\nimport sys\nimport subprocess\nimport os\nimport time\n\ndef monitor_subprocess():\n    \"\"\"\n    Run a subprocess and monitor its resource usage every second.\n    \n    Requirements:\n    - sys\n    - subprocess\n    - os\n    - time\n    \n    Usage:\n    python monitor.py --command \"python script.py\"\n    \"\"\"\n    parser = argparse.ArgumentParser(description=\"Monitor a subprocess\")\n    parser.add_argument(\"--command\", type=str, required=True, help=\"The command to run as a subprocess\")\n\n    args = parser.parse_args()\n    command = args.command.split()\n\n    process = subprocess.Popen(command)\n\n    while process.poll() is None:\n        print(subprocess.check_output(['ps', 'u', '-p', str(process.pid)]).decode())\n        time.sleep(1)\n\n    return 0\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nsys, csv, argparse, collections\n```python\nimport sys\nimport csv\nimport argparse\nfrom collections import Counter\n\ndef analyze_csv():\n    \"\"\"\n    Analyze a CSV file and print the count of each unique value in a specified column.\n    \n    Requirements:\n    - sys\n    - csv\n    - argparse\n    - collections.Counter\n    \n    Usage:\n    python analyze.py --file data.csv --column name\n    \"\"\"\n    parser = argparse.ArgumentParser(description=\"Analyze CSV file\")\n    parser.add_argument(\"--file\", type=str, required=True, help=\"The CSV file to analyze\")\n    parser.add_argument(\"--column\", type=str, required=True, help=\"The column to analyze\")\n\n    args = parser.parse_args()\n\n    with open(args.file, 'r') as f:\n        reader = csv.DictReader(f)\n        column_values = [row[args.column] for row in reader]\n\n    counter = Counter(column_values)\n\n    for value, count in counter.items():\n        print(f\"{value}: {count}\")\n\n    return 0\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nsys, os, argparse, hashlib\n```python\nimport sys\nimport os\nimport argparse\nimport hashlib\n\ndef hash_files():\n    \"\"\"\n    Hash files in a directory using SHA256 and print the hash values.\n    \n    Requirements:\n    - sys\n    - os\n    - argparse\n    - hashlib\n    \n    Usage:\n    python hash_files.py --directory /path/to/directory\n    \"\"\"\n    parser = argparse.ArgumentParser(description=\"Hash files\")\n    parser.add_argument(\"--directory\", type=str, required=True, help=\"The directory containing files to hash\")\n\n    args = parser.parse_args()\n\n    for filename in os.listdir(args.directory):\n        filepath = os.path.join(args.directory, filename)\n\n        with open(filepath, 'rb') as f:\n            file_hash = hashlib.sha256(f.read()).hexdigest()\n\n        print(f\"{filename}: {file_hash}\")\n\n    return 0\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nsys, pandas, argparse, matplotlib\n```python\nimport sys\nimport pandas as pd\nimport argparse\nimport matplotlib.pyplot as plt\n\ndef plot_csv():\n    \"\"\"\n    Plot a column from a CSV file against the index.\n    \n    Requirements:\n    - sys\n    - pandas\n    - argparse\n    - matplotlib.pyplot\n    \n    Usage:\n    python plot.py --file data.csv --column value\n    \"\"\"\n    parser = argparse.ArgumentParser(description=\"Plot CSV data\")\n    parser.add_argument(\"--file\", type=str, required=True, help=\"The CSV file to plot\")\n    parser.add_argument(\"--column\", type=str, required=True, help=\"The column to plot\")\n\n    args = parser.parse_args()\n\n    df = pd.read_csv(args.file)\n    df[args.column].plot()\n\n    plt.show()\n\n    return 0\n```"}
{"seed": "{\"task_id\": 273613, \"prompt\": \"def f_273613(file_path, prefix):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"\\n\\ttext = \\\"\\\"\\n\\twith open(file_path, 'r') as f:\\n\\t    for line in f:\\n\\t        if not re.search(r\\\"^({})\\\".format(prefix), line):\\n\\t            text += line\\n\\t\\n\\twith open(file_path, 'w') as f:\\n\\t    f.write(text)\", \"test_start\": \"\\nimport re\\n\\ndef check(candidate):\", \"test\": [\"\\n    with open('o.txt', 'w') as f:\\n        f.write('chalo 123\\\\n')\\n        f.write('chalo 456\\\\n')\\n        f.write('abc def\\\\n')\\n    candidate('o.txt', 'chalo')\\n    with open('o.txt', 'r') as f:\\n        assert f.readline() == 'abc def\\\\n'\\n\"], \"entry_point\": \"f_273613\", \"intent\": \"\\u0423\\u0434\\u0430\\u043b\\u0438\\u0442\\u044c \\u0438\\u0437 \\u0444\\u0430\\u0439\\u043b\\u0430 `file_path` \\u0441\\u0442\\u0440\\u043e\\u043a\\u0438, \\u043a\\u043e\\u0442\\u043e\\u0440\\u044b\\u0435 \\u043d\\u0430\\u0447\\u0438\\u043d\\u0430\\u044e\\u0442\\u0441\\u044f \\u0441\\u043e \\u0441\\u0442\\u0440\\u043e\\u043a\\u0438 `prefix`\", \"library\": [\"re\"]}", "generation": "Scenario 3: \nos, shutil, re, pathlib\n```python\nimport os\nimport shutil\nimport re\nfrom pathlib import Path\n\n# Constants\nBACKUP_DIR = Path('backup')\n\ndef backup_and_replace_in_files(dir_path, prefix, replacement):\n    \"\"\"\n    Backup all files in a directory `dir_path`, then replace all lines starting with `prefix` \n    in the original files with `replacement`.\n\n    Parameters:\n    dir_path (str): The directory path that contains the files.\n    prefix (str): The prefix of the lines to be replaced.\n    replacement (str): The replacement text.\n\n    Returns:\n    list: A list of the paths of the modified files.\n\n    Requirements:\n    - os\n    - shutil\n    - re\n    - pathlib.Path\n\n    Example:\n    >>> backup_and_replace_in_files('/path/to/directory', 'abc', 'xyz')\n    \"\"\"\n    modified_files = []\n\n    if not BACKUP_DIR.exists():\n        os.makedirs(BACKUP_DIR)\n\n    for filename in os.listdir(dir_path):\n        file_path = os.path.join(dir_path, filename)\n        if os.path.isfile(file_path):\n            shutil.copy2(file_path, BACKUP_DIR)\n            with open(file_path, 'r') as f:\n                lines = f.readlines()\n            with open(file_path, 'w') as f:\n                for line in lines:\n                    if line.startswith(prefix):\n                        line = re.sub(r'^{}'.format(prefix), replacement, line)\n                    f.write(line)\n            modified_files.append(file_path)\n\n    return modified_files\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, os, re, glob\n```python\nimport numpy as np\nimport os\nimport re\nimport glob\n\n# Constants\nOUTPUT_PATH = 'output.txt'\n\ndef count_and_save_lines_with_prefix(directory, prefix):\n    \"\"\"\n    Count the number of lines starting with `prefix` in all text files in a directory \n    and then save the count to an output file.\n\n    Parameters:\n    directory (str): The directory that contains the text files.\n    prefix (str): The prefix of the lines to be counted.\n\n    Returns:\n    str: The path to the output file.\n\n    Requirements:\n    - numpy\n    - os\n    - re\n    - glob\n\n    Example:\n    >>> count_and_save_lines_with_prefix('/path/to/directory', 'abc')\n    \"\"\"\n    count = 0\n\n    for file_path in glob.glob(os.path.join(directory, '*.txt')):\n        with open(file_path, 'r') as f:\n            for line in f:\n                if re.search(r'^{}'.format(prefix), line):\n                    count += 1\n\n    np.savetxt(OUTPUT_PATH, np.array([count]), fmt='%d')\n\n    return OUTPUT_PATH\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nos, re, glob, csv\n```python\nimport os\nimport re\nimport glob\nimport csv\n\n# Constants\nOUTPUT_CSV = 'output.csv'\n\ndef count_prefix_in_files_and_save_to_csv(directory, prefix):\n    \"\"\"\n    Count the number of lines starting with `prefix` in each text file in a directory \n    and then save the counts to a CSV file.\n\n    Parameters:\n    directory (str): The directory that contains the text files.\n    prefix (str): The prefix of the lines to be counted.\n\n    Returns:\n    str: The path to the output CSV file.\n\n    Requirements:\n    - os\n    - re\n    - glob\n    - csv\n\n    Example:\n    >>> count_prefix_in_files_and_save_to_csv('/path/to/directory', 'abc')\n    \"\"\"\n    counts = []\n\n    for file_path in glob.glob(os.path.join(directory, '*.txt')):\n        count = 0\n        with open(file_path, 'r') as f:\n            for line in f:\n                if re.search(r'^{}'.format(prefix), line):\n                    count += 1\n        counts.append((os.path.basename(file_path), count))\n\n    with open(OUTPUT_CSV, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(['File', 'Count'])\n        writer.writerows(counts)\n\n    return OUTPUT_CSV\n```\n`GPT_ODEX_BREAK`\n\nScenario 6: \nos, re, glob, pandas\n```python\nimport os\nimport re\nimport glob\nimport pandas as pd\n\n# Constants\nOUTPUT_CSV = 'output.csv'\n\ndef create_dataframe_from_files(directory, prefix):\n    \"\"\"\n    Create a DataFrame with the number of lines starting with `prefix` in each text file \n    in a directory. Then, save the DataFrame to a CSV file.\n\n    Parameters:\n    directory (str): The directory that contains the text files.\n    prefix (str): The prefix of the lines to be counted.\n\n    Returns:\n    str: The path to the output CSV file.\n\n    Requirements:\n    - os\n    - re\n    - glob\n    - pandas\n\n    Example:\n    >>> create_dataframe_from_files('/path/to/directory', 'abc')\n    \"\"\"\n    data = []\n\n    for file_path in glob.glob(os.path.join(directory, '*.txt')):\n        count = 0\n        with open(file_path, 'r') as f:\n            for line in f:\n                if re.search(r'^{}'.format(prefix), line):\n                    count += 1\n        data.append({'File': os.path.basename(file_path), 'Count': count})\n\n    df = pd.DataFrame(data)\n    df.to_csv(OUTPUT_CSV, index=False)\n\n    return OUTPUT_CSV\n```"}
{"seed": "{\"task_id\": 9096, \"prompt\": \"def f_9096(s):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"Counter(s.split())\", \"test_start\": \"\\nfrom collections import Counter\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('my string') == Counter({'my': 1, 'string': 1})\\n\", \"\\n    assert candidate('this string contains repetitive words words words words ...') == Counter({\\n        '...': 1, 'contains': 1, 'repetitive': 1, 'string': 1, 'this': 1, 'words': 4})\\n\"], \"entry_point\": \"f_9096\", \"intent\": \"\\u041f\\u043e\\u0441\\u0447\\u0438\\u0442\\u0430\\u0442\\u044c \\u0441\\u043b\\u043e\\u0432\\u0430 \\u0432 \\u0441\\u0442\\u0440\\u043e\\u043a\\u0435 `s`\", \"library\": [\"collections\"]}", "generation": "Scenario 1: \ncollections, os, json, csv\n```python\nimport os\nimport json\nimport csv\nfrom collections import Counter\n\n# Constants\nOUTPUT_FILE = 'word_counts.csv'\n\ndef count_words_in_files(file_directory):\n    \"\"\"\n    Count words in all .txt and .json files in a given directory and save the counts \n    into a csv file. In .json files, only count the words in the \"text\" field.\n\n    Parameters:\n    file_directory (str): The directory with all the files.\n\n    Returns:\n    str: The path to the output csv file.\n\n    Requirements:\n    - collections\n    - os\n    - json\n    - csv\n\n    Example:\n    >>> count_words_in_files('/path/to/directory/')\n    \"\"\"\n    word_counter = Counter()\n\n    for filename in os.listdir(file_directory):\n        if filename.endswith('.txt'):\n            with open(os.path.join(file_directory, filename), 'r') as f:\n                word_counter.update(f.read().split())\n        elif filename.endswith('.json'):\n            with open(os.path.join(file_directory, filename), 'r') as f:\n                data = json.load(f)\n                word_counter.update(data.get('text', '').split())\n\n    with open(OUTPUT_FILE, 'w') as f:\n        writer = csv.writer(f)\n        writer.writerow(['Word', 'Count'])\n        for word, count in word_counter.items():\n            writer.writerow([word, count])\n\n    return os.path.join(file_directory, OUTPUT_FILE)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncollections, re, string\n```python\nimport re\nimport string\nfrom collections import Counter\n\n# Constants\nPUNCTUATION = string.punctuation\n\ndef count_words_ignore_punctuation(s):\n    \"\"\"\n    Count words in a string ignoring punctuation.\n\n    Parameters:\n    s (str): The input string.\n\n    Returns:\n    collections.Counter: The word counts.\n\n    Requirements:\n    - collections\n    - re\n    - string\n\n    Example:\n    >>> count_words_ignore_punctuation(\"Hello, world! Hello world!\")\n    Counter({'Hello': 2, 'world': 2})\n    \"\"\"\n    s = re.sub('['+PUNCTUATION+']', '', s)\n    return Counter(s.split())\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \ncollections, string, random\n```python\nimport string\nimport random\nfrom collections import Counter\n\n# Constants\nLETTERS = string.ascii_letters\n\ndef generate_and_count_random_words(n):\n    \"\"\"\n    Generate n random words and count them.\n\n    Parameters:\n    n (int): The number of words to generate.\n\n    Returns:\n    collections.Counter: The word counts.\n\n    Requirements:\n    - collections\n    - string\n    - random\n\n    Example:\n    >>> generate_and_count_random_words(10)\n    Counter({'random_word_1': 1, 'random_word_2': 1, ...})\n    \"\"\"\n    words = [''.join(random.choice(LETTERS) for _ in range(5)) for _ in range(n)]\n    return Counter(words)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \ncollections, nltk, string\n```python\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nfrom collections import Counter\n\n# Constants\nSTOPWORDS = set(stopwords.words('english'))\nPUNCTUATION = string.punctuation\n\ndef count_non_stop_words(s):\n    \"\"\"\n    Count non-stop words in a string.\n\n    Parameters:\n    s (str): The input string.\n\n    Returns:\n    collections.Counter: The word counts.\n\n    Requirements:\n    - collections\n    - nltk\n    - string\n\n    Example:\n    >>> count_non_stop_words(\"This is a test string.\")\n    Counter({'test': 1, 'string': 1})\n    \"\"\"\n    s = s.translate(str.maketrans('', '', PUNCTUATION)).lower()\n    words = [word for word in s.split() if word not in STOPWORDS]\n    return Counter(words)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \ncollections, sklearn, pandas\n```python\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import Counter\n\ndef count_words_in_dataframe(df, column):\n    \"\"\"\n    Count words in a specific column of a pandas DataFrame.\n\n    Parameters:\n    df (pandas.DataFrame): The input DataFrame.\n    column (str): The column to count words in.\n\n    Returns:\n    collections.Counter: The word counts.\n\n    Requirements:\n    - collections\n    - pandas\n    - sklearn.feature_extraction.text\n\n    Example:\n    >>> df = pd.DataFrame({'text': ['This is a test.', 'This is another test.']})\n    >>> count_words_in_dataframe(df, 'text')\n    Counter({'This': 2, 'is': 2, 'a': 1, 'test': 2, 'another': 1})\n    \"\"\"\n    vectorizer = CountVectorizer()\n    X = vectorizer.fit_transform(df[column])\n    word_counts = Counter(dict(zip(vectorizer.get_feature_names(), X.sum(axis=0).tolist()[0])))\n    return word_counts\n```"}
{"seed": "{\"task_id\": 157783, \"prompt\": \"def f_157783(num, base):\\n\\t\", \"suffix\": \"\\n\\treturn new_num\", \"canonical_solution\": \"new_num = int(str(num), base) \", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(100, 8) == 64\\n\"], \"entry_point\": \"f_157783\", \"intent\": \"\\u041f\\u0440\\u0435\\u043e\\u0431\\u0440\\u0430\\u0437\\u043e\\u0432\\u0430\\u0442\\u044c \\u0447\\u0438\\u0441\\u043b\\u043e `num` \\u0432 \\u0441\\u0438\\u0441\\u0442\\u0435\\u043c\\u0435 \\u0441\\u0447\\u0438\\u0441\\u043b\\u0435\\u043d\\u0438\\u044f \\u0441 \\u043e\\u0441\\u043d\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435\\u043c `base` \\u0432 \\u0447\\u0438\\u0441\\u043b\\u043e `new_num` \\u0432 \\u0434\\u0435\\u0441\\u044f\\u0442\\u0438\\u0447\\u043d\\u043e\\u0439 \\u0441\\u0438\\u0441\\u0442\\u0435\\u043c\\u0435 \\u0441\\u0447\\u0438\\u0441\\u043b\\u0435\\u043d\\u0438\\u044f \", \"library\": []}", "generation": "Scenario 1: \nnumpy, matplotlib, random\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\n\n# Constants\nBASES = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n\ndef plot_conversion_distribution(num_samples):\n    \"\"\"\n    Generate a number of random numbers and convert them from a random base to \n    decimal. Plot the distribution of the converted numbers.\n    \n    Parameters:\n    num_samples (int): The number of random numbers to generate and convert.\n    \n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - random\n    \n    Example:\n    >>> plot_conversion_distribution(10000)\n    \"\"\"\n    # Generate random numbers and bases\n    numbers = np.random.randint(0, 10000, num_samples)\n    bases = random.choices(BASES, k=num_samples)\n\n    # Convert numbers to decimal\n    converted_numbers = [int(str(num), base) for num, base in zip(numbers, bases)]\n\n    # Plot histogram\n    plt.hist(converted_numbers, bins=100)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nnumpy, matplotlib, random\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\n\n# Constants\nBASES = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n\ndef plot_conversion_time(num_samples):\n    \"\"\"\n    Generate a number of random numbers and measure the time it takes to convert \n    them from a random base to decimal. Plot the distribution of the conversion times.\n    \n    Parameters:\n    num_samples (int): The number of random numbers to generate and convert.\n    \n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - random\n    \n    Example:\n    >>> plot_conversion_time(10000)\n    \"\"\"\n    import time\n\n    # Generate random numbers and bases\n    numbers = np.random.randint(0, 10000, num_samples)\n    bases = random.choices(BASES, k=num_samples)\n    \n    # Measure conversion time and convert numbers\n    times = []\n    for num, base in zip(numbers, bases):\n        start = time.time()\n        int(str(num), base)\n        end = time.time()\n        times.append(end - start)\n\n    # Plot histogram of times\n    plt.hist(times, bins=100)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnumpy, pandas, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nBASES = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n\ndef analyze_conversion(num_samples):\n    \"\"\"\n    Generate a number of random numbers and convert them from a random base to \n    decimal. Store the original numbers, bases, and converted numbers in a Pandas DataFrame.\n    \n    Parameters:\n    num_samples (int): The number of random numbers to generate and convert.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the original numbers, bases, and converted numbers.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = analyze_conversion(10000)\n    >>> print(df)\n    \"\"\"\n    # Generate random numbers and bases\n    numbers = np.random.randint(0, 10000, num_samples)\n    bases = np.random.choice(BASES, num_samples)\n\n    # Convert numbers to decimal\n    converted_numbers = [int(str(num), base) for num, base in zip(numbers, bases)]\n\n    # Create DataFrame\n    df = pd.DataFrame({\n        'Original Number': numbers,\n        'Base': bases,\n        'Converted Number': converted_numbers\n    })\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, pandas, matplotlib, seaborn\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nBASES = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n\ndef visualize_conversion(num_samples):\n    \"\"\"\n    Generate a number of random numbers and convert them from a random base to \n    decimal. Store the original numbers, bases, and converted numbers in a Pandas DataFrame. \n    Plot the distribution of the converted numbers for each base.\n    \n    Parameters:\n    num_samples (int): The number of random numbers to generate and convert.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the original numbers, bases, and converted numbers.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - seaborn\n    \n    Example:\n    >>> df = visualize_conversion(10000)\n    >>> print(df)\n    >>> sns.boxplot(x='Base', y='Converted Number', data=df)\n    \"\"\"\n    # Generate random numbers and bases\n    numbers = np.random.randint(0, 10000, num_samples)\n    bases = np.random.choice(BASES, num_samples)\n\n    # Convert numbers to decimal\n    converted_numbers = [int(str(num), base) for num, base in zip(numbers, bases)]\n\n    # Create DataFrame\n    df = pd.DataFrame({\n        'Original Number': numbers,\n        'Base': bases,\n        'Converted Number': converted_numbers\n    })\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnumpy, pandas\n```python\nimport numpy as np\nimport pandas as pd\n\n# Constants\nBASES = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n\ndef calculate_conversion_statistics(num_samples):\n    \"\"\"\n    Generate a number of random numbers and convert them from a random base to \n    decimal. Calculate the mean and standard deviation of the converted numbers for each base.\n    \n    Parameters:\n    num_samples (int): The number of random numbers to generate and convert.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the mean and standard deviation of the converted numbers for each base.\n    \n    Requirements:\n    - numpy\n    - pandas\n    \n    Example:\n    >>> stats = calculate_conversion_statistics(10000)\n    >>> print(stats)\n    \"\"\"\n    # Generate random numbers and bases\n    numbers = np.random.randint(0, 10000, num_samples)\n    bases = np.random.choice(BASES, num_samples)\n\n    # Convert numbers to decimal\n    converted_numbers = [int(str(num), base) for num, base in zip(numbers, bases)]\n\n    # Create DataFrame\n    df = pd.DataFrame({\n        'Base': bases,\n        'Converted Number': converted_numbers\n    })\n\n    # Calculate statistics\n    stats = df.groupby('Base')['Converted Number'].agg(['mean', 'std'])\n\n    return stats\n```"}
{"seed": "{\"task_id\": 224243, \"prompt\": \"def f_224243(db_path):\\n\\t\", \"suffix\": \"\\n\\treturn cur\", \"canonical_solution\": \"con = sqlite3.connect(db_path)\\n\\n\\tcur = con.cursor()\\n\", \"test_start\": \"\\nimport sqlite3\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert 'sqlite3.Cursor' in str(type(candidate('dev.db'))) \\n\"], \"entry_point\": \"f_224243\", \"intent\": \"\\u0421\\u043e\\u0437\\u0434\\u0430\\u0442\\u044c \\u0431\\u0430\\u0437\\u0443 \\u0434\\u0430\\u043d\\u043d\\u044b\\u0445 sqlite3 `db_path`\", \"library\": [\"sqlite3\"]}", "generation": "Scenario 1: \npandas, sqlite3, os, csv\n```python\nimport pandas as pd\nimport sqlite3\nimport os\nimport csv\n\n# Constants\nTABLE_NAME = 'students'\n\ndef create_and_read_db(csv_filepath, db_path):\n    \"\"\"\n    Create a SQLite3 database at `db_path`, insert data from a CSV file into a table,\n    then read the table into a pandas DataFrame.\n\n    Parameters:\n    csv_filepath (str): The file path of the CSV file.\n    db_path (str): The file path of the SQLite3 database.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the data from the table.\n\n    Requirements:\n    - pandas\n    - sqlite3\n    - os\n    - csv\n\n    Example:\n    >>> df = create_and_read_db('students.csv', 'students.db')\n    >>> print(df)\n    \"\"\"\n    # Delete the database file if it already exists\n    if os.path.exists(db_path):\n        os.remove(db_path)\n\n    # Connect to the SQLite3 database\n    con = sqlite3.connect(db_path)\n    cur = con.cursor()\n\n    # Create a table\n    with open(csv_filepath, 'r') as f:\n        dr = csv.DictReader(f)\n        cols = dr.fieldnames\n        cur.execute(f\"CREATE TABLE {TABLE_NAME} ({', '.join(cols)});\")\n\n    # Insert data into the table\n    with open(csv_filepath, 'r') as f:\n        dr = csv.DictReader(f)\n        to_db = [(i[col] for col in cols) for i in dr]\n        cur.executemany(f\"INSERT INTO {TABLE_NAME} ({', '.join(cols)}) VALUES ({', '.join(['?' for col in cols])});\", to_db)\n\n    con.commit()\n\n    # Read the table into a pandas DataFrame\n    df = pd.read_sql_query(f\"SELECT * from {TABLE_NAME}\", con)\n\n    con.close()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nsqlite3, os, pandas, numpy\n```python\nimport sqlite3\nimport os\nimport pandas as pd\nimport numpy as np\n\n# Constants\nDB_NAME = 'employees.db'\nTABLE_NAME = 'employees'\n\ndef create_db_and_compute_statistics(db_path, df):\n    \"\"\"\n    Create a SQLite3 database at `db_path`, insert data from a pandas DataFrame into a table,\n    then compute and return some descriptive statistics of a specific column.\n\n    Parameters:\n    db_path (str): The file path of the SQLite3 database.\n    df (DataFrame): The pandas DataFrame to be inserted into the database.\n\n    Returns:\n    Series: A pandas Series with the descriptive statistics of the 'Salary' column.\n\n    Requirements:\n    - pandas\n    - sqlite3\n    - os\n    - numpy\n\n    Example:\n    >>> df = pd.DataFrame({\n    ... 'Name': ['John', 'Anna', 'Peter', 'Linda'],\n    ... 'Age': [28, 33, 41, 36],\n    ... 'Salary': [3000, 3500, 4000, 3800]\n    ... })\n    >>> stats = create_db_and_compute_statistics('employees.db', df)\n    >>> print(stats)\n    \"\"\"\n    # Delete the database file if it already exists\n    if os.path.exists(db_path):\n        os.remove(db_path)\n\n    # Connect to the SQLite3 database\n    con = sqlite3.connect(db_path)\n\n    # Insert the DataFrame into a table\n    df.to_sql(TABLE_NAME, con)\n\n    # Compute descriptive statistics of the 'Salary' column\n    stats = pd.read_sql_query(f\"SELECT * from {TABLE_NAME}\", con)['Salary'].describe()\n\n    con.close()\n\n    return stats\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nsqlite3, os, datetime, pytz\n```python\nimport sqlite3\nimport os\nfrom datetime import datetime\nimport pytz\n\n# Constants\nDB_NAME = 'events.db'\nTABLE_NAME = 'events'\n\ndef create_db_and_insert_event(db_path, event_info):\n    \"\"\"\n    Create a SQLite3 database at `db_path`, create a table, and insert an event with its attributes.\n\n    Parameters:\n    db_path (str): The file path of the SQLite3 database.\n    event_info (dict): A dictionary with information about the event.\n\n    Returns:\n    bool: True if the event was successfully inserted, False otherwise.\n\n    Requirements:\n    - sqlite3\n    - os\n    - datetime\n    - pytz\n\n    Example:\n    >>> event = {'title': 'Conference', 'date': '2023-05-15', 'time': '14:00:00', 'timezone': 'America/New_York'}\n    >>> success = create_db_and_insert_event('events.db', event)\n    >>> print(success)\n    \"\"\"\n    # Delete the database file if it already exists\n    if os.path.exists(db_path):\n        os.remove(db_path)\n\n    # Connect to the SQLite3 database\n    con = sqlite3.connect(db_path)\n    cur = con.cursor()\n\n    # Create a table\n    cur.execute(f\"CREATE TABLE {TABLE_NAME} (title text, datetime text);\")\n\n    # Convert the date and time to a datetime object in UTC\n    local_tz = pytz.timezone(event_info['timezone'])\n    local_dt = datetime.strptime(f\"{event_info['date']} {event_info['time']}\", '%Y-%m-%d %H:%M:%S')\n    local_dt = local_tz.localize(local_dt)\n    utc_dt = local_dt.astimezone(pytz.UTC)\n\n    # Insert the event into the table\n    cur.execute(f\"INSERT INTO {TABLE_NAME} VALUES (?, ?)\", (event_info['title'], utc_dt.isoformat()))\n\n    con.commit()\n    con.close()\n\n    return True\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nsqlite3, os, random\n```python\nimport sqlite3\nimport os\nimport random\n\n# Constants\nDB_NAME = 'numbers.db'\nTABLE_NAME = 'numbers'\n\ndef create_db_and_insert_random_numbers(db_path, n):\n    \"\"\"\n    Create a SQLite3 database at `db_path`, create a table, and insert `n` random numbers into the table.\n\n    Parameters:\n    db_path (str): The file path of the SQLite3 database.\n    n (int): The number of random numbers to insert.\n\n    Returns:\n    bool: True if the numbers were successfully inserted, False otherwise.\n\n    Requirements:\n    - sqlite3\n    - os\n    - random\n\n    Example:\n    >>> success = create_db_and_insert_random_numbers('numbers.db', 10)\n    >>> print(success)\n    \"\"\"\n    # Delete the database file if it already exists\n    if os.path.exists(db_path):\n        os.remove(db_path)\n\n    # Connect to the SQLite3 database\n    con = sqlite3.connect(db_path)\n    cur = con.cursor()\n\n    # Create a table\n    cur.execute(f\"CREATE TABLE {TABLE_NAME} (number integer);\")\n\n    # Insert `n` random numbers into the table\n    for _ in range(n):\n        cur.execute(f\"INSERT INTO {TABLE_NAME} VALUES (?)\", (random.randint(0, 100),))\n\n    con.commit()\n    con.close()\n\n    return True\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nsqlite3, os, json\n```python\nimport sqlite3\nimport os\nimport json\n\n# Constants\nDB_NAME = 'products.db'\nTABLE_NAME = 'products'\n\ndef create_db_and_insert_json_data(db_path, json_str):\n    \"\"\"\n    Create a SQLite3 database at `db_path`, create a table, and insert data from a JSON string into the table.\n\n    Parameters:\n    db_path (str): The file path of the SQLite3 database.\n    json_str (str): The JSON string.\n\n    Returns:\n    bool: True if the data was successfully inserted, False otherwise.\n\n    Requirements:\n    - sqlite3\n    - os\n    - json\n\n    Example:\n    >>> json_str = '[{\"name\": \"apple\", \"price\": 0.5}, {\"name\": \"banana\", \"price\": 0.3}]'\n    >>> success = create_db_and_insert_json_data('products.db', json_str)\n    >>> print(success)\n    \"\"\"\n    # Delete the database file if it already exists\n    if os.path.exists(db_path):\n        os.remove(db_path)\n\n    # Connect to the SQLite3 database\n    con = sqlite3.connect(db_path)\n    cur = con.cursor()\n\n    # Create a table\n    cur.execute(f\"CREATE TABLE {TABLE_NAME} (name text, price real);\")\n\n    # Parse the JSON string\n    data = json.loads(json_str)\n\n    # Insert the data into the table\n    for item in data:\n        cur.execute(f\"INSERT INTO {TABLE_NAME} VALUES (?, ?)\", (item['name'], item['price']))\n\n    con.commit()\n    con.close()\n\n    return True\n```\n"}
{"seed": "{\"task_id\": 225896, \"prompt\": \"def f_225896(cmd):\\n\\t\", \"suffix\": \"\\n\\treturn result\", \"canonical_solution\": \"result = os.popen(cmd, 'r', 1)\", \"test_start\": \"\\nimport os\\nfrom unittest.mock import Mock\\n\\ndef check(candidate):\", \"test\": [\"\\n    os.popen = Mock(return_value = True)\\n    assert candidate('python3') == True\\n\"], \"entry_point\": \"f_225896\", \"intent\": \"\\u041f\\u043e\\u043b\\u0443\\u0447\\u0438\\u0442\\u044c \\u043f\\u043e\\u0442\\u043e\\u043a \\u0432\\u044b\\u0432\\u043e\\u0434\\u0430 \\u0434\\u0430\\u043d\\u043d\\u044b\\u0445 \\u043a\\u043e\\u043d\\u0441\\u043e\\u043b\\u044c\\u043d\\u043e\\u0439 \\u043a\\u043e\\u043c\\u0430\\u043d\\u0434\\u044b `cmd` \\u0432 \\u0444\\u0430\\u0439\\u043b\\u043e\\u0432\\u044b\\u0439 \\u043e\\u0431\\u044a\\u0435\\u043a\\u0442 `result`\", \"library\": [\"os\"]}", "generation": "Scenario 1:\nos, subprocess, pathlib, sys\n```python\nimport os\nimport subprocess\nfrom pathlib import Path\nimport sys\n\n# Constants\nCOMPRESSED_EXTENSIONS = ['.zip', '.tar.gz', '.tgz', '.tar.bz2', '.tbz2']\n\ndef extract_compressed_file(file_path):\n    \"\"\"\n    Extract a compressed file in the current directory.\n\n    Parameters:\n    file_path (str): The path to the compressed file.\n\n    Returns:\n    str: The path to the extracted file.\n\n    Requirements:\n    - os\n    - subprocess\n    - pathlib.Path\n    - sys\n\n    Example:\n    >>> extract_compressed_file('/path/to/file.zip')\n    \"\"\"\n    file_path = Path(file_path)\n    file_extension = file_path.suffix\n\n    if file_extension not in COMPRESSED_EXTENSIONS:\n        print(f\"Unsupported file extension: {file_extension}\")\n        sys.exit(1)\n\n    if file_extension == '.zip':\n        command = ['unzip', file_path]\n    else:\n        command = ['tar', 'xf', file_path]\n\n    try:\n        subprocess.run(command, check=True)\n    except subprocess.CalledProcessError as e:\n        print(f\"Error occurred while extracting file: {str(e)}\")\n        sys.exit(1)\n\n    return str(file_path.with_suffix(''))\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, csv, sys, sqlite3\n```python\nimport os\nimport csv\nimport sys\nimport sqlite3\n\n# Constants\nDATABASE_NAME = 'data.db'\nTABLE_NAME = 'data'\n\ndef import_csv_to_sqlite(csv_path):\n    \"\"\"\n    Import a CSV file into an SQLite database.\n\n    Parameters:\n    csv_path (str): The path to the CSV file.\n\n    Requirements:\n    - os\n    - csv\n    - sys\n    - sqlite3\n\n    Example:\n    >>> import_csv_to_sqlite('/path/to/file.csv')\n    \"\"\"\n    if not os.path.isfile(csv_path):\n        print(f\"{csv_path} does not exist\")\n        sys.exit(1)\n\n    conn = sqlite3.connect(DATABASE_NAME)\n    cur = conn.cursor()\n\n    with open(csv_path, 'r') as f:\n        reader = csv.reader(f)\n        headers = next(reader)\n        headers = [h.replace(' ', '_') for h in headers]\n\n        cur.execute(f\"DROP TABLE IF EXISTS {TABLE_NAME}\")\n        cur.execute(f\"CREATE TABLE {TABLE_NAME} ({', '.join(headers)})\")\n\n        for row in reader:\n            cur.execute(f\"INSERT INTO {TABLE_NAME} VALUES ({', '.join(['?' for _ in row])})\", row)\n\n    conn.commit()\n    conn.close()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, glob, shutil, sys\n```python\nimport os\nimport glob\nimport shutil\nimport sys\n\n# Constants\nDESTINATION_DIR = '/path/to/destination/directory'\n\ndef move_files_with_extension(extension, source_dir):\n    \"\"\"\n    Move all files with a specific extension from a source directory to a destination directory.\n\n    Parameters:\n    extension (str): The file extension.\n    source_dir (str): The source directory.\n\n    Requirements:\n    - os\n    - glob\n    - shutil\n    - sys\n\n    Example:\n    >>> move_files_with_extension('.txt', '/path/to/source/directory')\n    \"\"\"\n    if not os.path.isdir(source_dir):\n        print(f\"{source_dir} does not exist or is not a directory\")\n        sys.exit(1)\n\n    files = glob.glob(os.path.join(source_dir, f\"*{extension}\"))\n\n    for file in files:\n        shutil.move(file, DESTINATION_DIR)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, subprocess, time, sys\n```python\nimport os\nimport subprocess\nimport time\nimport sys\n\ndef monitor_process(process_name):\n    \"\"\"\n    Monitor a process and print its CPU and memory usage every second.\n\n    Parameters:\n    process_name (str): The name of the process.\n\n    Requirements:\n    - os\n    - subprocess\n    - time\n    - sys\n\n    Example:\n    >>> monitor_process('python')\n    \"\"\"\n    try:\n        while True:\n            ps_output = subprocess.check_output(['ps', 'aux'])\n            for line in ps_output.splitlines():\n                if process_name in line.decode('utf-8'):\n                    print(line.decode('utf-8').split()[2:4])\n            time.sleep(1)\n    except KeyboardInterrupt:\n        sys.exit(0)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, random, time, sys\n```python\nimport os\nimport random\nimport time\nimport sys\n\n# Constants\nDELAY_RANGE = (1, 10)\n\ndef random_file_deletion(directory):\n    \"\"\"\n    Randomly delete a file in a directory after a random delay.\n\n    Parameters:\n    directory (str): The directory.\n\n    Requirements:\n    - os\n    - random\n    - time\n    - sys\n\n    Example:\n    >>> random_file_deletion('/path/to/directory')\n    \"\"\"\n    if not os.path.isdir(directory):\n        print(f\"{directory} does not exist or is not a directory\")\n        sys.exit(1)\n\n    files = os.listdir(directory)\n    file_to_delete = random.choice(files)\n    delay = random.randint(*DELAY_RANGE)\n\n    time.sleep(delay)\n    os.remove(os.path.join(directory, file_to_delete))\n```"}
{"seed": "{\"task_id\": 319317, \"prompt\": \"def f_319317(s):\\n\\t\", \"suffix\": \"\\n\\treturn my_dict\", \"canonical_solution\": \"my_dict = json.loads(s)\", \"test_start\": \"\\nimport json\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('{\\\"abc\\\":1, \\\"dfhgfrh\\\": 3435}') == {\\\"abc\\\":1, \\\"dfhgfrh\\\": 3435}\\n\"], \"entry_point\": \"f_319317\", \"intent\": \"\\u041f\\u0440\\u0435\\u043e\\u0431\\u0440\\u0430\\u0437\\u043e\\u0432\\u0430\\u0442\\u044c \\u0441\\u0442\\u0440\\u043e\\u043a\\u0443 `s` \\u0432 \\u0441\\u043b\\u043e\\u0432\\u0430\\u0440\\u044c `my_dict`\", \"library\": [\"json\"]}", "generation": "Scenario 1: \njson, os, pathlib, csv\n```python\nimport json\nimport os\nfrom pathlib import Path\nimport csv\n\n# Constants\nCSV_FILENAME = 'json_to_csv.csv'\n\ndef convert_json_to_csv(json_str, csv_file_path=CSV_FILENAME):\n    \"\"\"\n    Convert a JSON string to a CSV file.\n\n    Parameters:\n    json_str (str): The JSON string.\n    csv_file_path (str): The path to the output CSV file.\n\n    Returns:\n    str: The path to the output CSV file.\n\n    Requirements:\n    - json\n    - os\n    - pathlib\n    - csv\n\n    Example:\n    >>> json_str = '{\"Name\": \"John\", \"Age\": 30, \"City\": \"New York\"}'\n    >>> convert_json_to_csv(json_str)\n    \"\"\"\n    data = json.loads(json_str)\n    \n    csv_file_path = Path(csv_file_path)\n    if not csv_file_path.is_absolute():\n        csv_file_path = Path(os.getcwd()) / csv_file_path\n\n    with open(csv_file_path, 'w', newline='') as csv_file:\n        writer = csv.DictWriter(csv_file, fieldnames=data.keys())\n        writer.writeheader()\n        writer.writerow(data)\n\n    return str(csv_file_path)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \njson, datetime, pandas, matplotlib.pyplot\n```python\nimport json\nfrom datetime import datetime\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef plot_json_data(json_str):\n    \"\"\"\n    Plot a bar chart from a JSON string containing date and value pairs.\n\n    Parameters:\n    json_str (str): The JSON string containing date and value pairs.\n\n    Returns:\n    None.\n\n    Requirements:\n    - json\n    - datetime\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> json_str = '{\"2022-01-01\": 10, \"2022-02-01\": 15, \"2022-03-01\": 7}'\n    >>> plot_json_data(json_str)\n    \"\"\"\n    data_dict = json.loads(json_str)\n    for key in data_dict:\n        data_dict[key] = datetime.strptime(key, '%Y-%m-%d'), data_dict[key]\n\n    df = pd.DataFrame.from_dict(data_dict, orient='index', columns=['Date', 'Value'])\n    df.set_index('Date', inplace=True)\n\n    df.plot(kind='bar')\n\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \njson, os, requests, pathlib, csv\n```python\nimport json\nimport os\nfrom pathlib import Path\nimport csv\nimport requests\n\n# Constants\nCSV_FILENAME = 'json_to_csv.csv'\n\ndef convert_json_url_to_csv(json_url, csv_file_path=CSV_FILENAME):\n    \"\"\"\n    Convert a JSON from a URL to a CSV file.\n\n    Parameters:\n    json_url (str): The URL to the JSON data.\n    csv_file_path (str): The path to the output CSV file.\n\n    Returns:\n    str: The path to the output CSV file.\n\n    Requirements:\n    - json\n    - os\n    - requests\n    - pathlib\n    - csv\n\n    Example:\n    >>> json_url = 'https://api.example.com/data.json'\n    >>> convert_json_url_to_csv(json_url)\n    \"\"\"\n    response = requests.get(json_url)\n    data = response.json()\n    \n    csv_file_path = Path(csv_file_path)\n    if not csv_file_path.is_absolute():\n        csv_file_path = Path(os.getcwd()) / csv_file_path\n\n    with open(csv_file_path, 'w', newline='') as csv_file:\n        writer = csv.DictWriter(csv_file, fieldnames=data.keys())\n        writer.writeheader()\n        writer.writerow(data)\n\n    return str(csv_file_path)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\njson, pandas, matplotlib.pyplot, numpy\n```python\nimport json\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_json_histogram(json_str):\n    \"\"\"\n    Plot a histogram from a JSON string containing a list of numbers.\n\n    Parameters:\n    json_str (str): The JSON string containing a list of numbers.\n\n    Returns:\n    None.\n\n    Requirements:\n    - json\n    - pandas\n    - matplotlib.pyplot\n    - numpy\n\n    Example:\n    >>> json_str = '[1, 1, 2, 3, 5, 8, 13, 21, 34, 55]'\n    >>> plot_json_histogram(json_str)\n    \"\"\"\n    data = json.loads(json_str)\n    df = pd.DataFrame(data, columns=['Values'])\n    \n    plt.hist(df['Values'], bins=np.arange(df['Values'].min(), df['Values'].max()+1) - 0.5, edgecolor='black')\n    plt.xticks(range(df['Values'].min(), df['Values'].max()+1))\n    plt.xlabel('Values')\n    plt.ylabel('Frequency')\n    \n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\njson, os, hashlib, pathlib\n```python\nimport json\nimport os\nfrom pathlib import Path\nimport hashlib\n\ndef hash_json_file(json_file_path):\n    \"\"\"\n    Calculate the MD5 hash of a JSON file.\n\n    Parameters:\n    json_file_path (str): The path to the JSON file.\n\n    Returns:\n    str: The MD5 hash of the JSON file.\n\n    Requirements:\n    - json\n    - os\n    - hashlib\n    - pathlib\n\n    Example:\n    >>> json_file_path = '/path/to/file.json'\n    >>> hash_json_file(json_file_path)\n    \"\"\"\n    json_file_path = Path(json_file_path)\n    if not json_file_path.is_absolute():\n        json_file_path = Path(os.getcwd()) / json_file_path\n\n    with open(json_file_path, 'r') as json_file:\n        data = json.load(json_file)\n\n    md5_hash = hashlib.md5()\n    md5_hash.update(json.dumps(data, sort_keys=True).encode('utf-8'))\n\n    return md5_hash.hexdigest()\n```"}
{"seed": "{\"task_id\": 319317, \"prompt\": \"def f_319317(s):\\n\\t\", \"suffix\": \"\\n\\treturn my_dict\", \"canonical_solution\": \"my_dict = dict(ast.literal_eval(s))\", \"test_start\": \"\\nimport ast\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(\\\"{'a': [1, 2, 3]}\\\") == {'a': [1, 2, 3]}\\n\"], \"entry_point\": \"f_319317\", \"intent\": \"\\u041f\\u0440\\u0435\\u043e\\u0431\\u0440\\u0430\\u0437\\u043e\\u0432\\u0430\\u0442\\u044c \\u0441\\u0442\\u0440\\u043e\\u043a\\u0443 `s` \\u0432 \\u0441\\u043b\\u043e\\u0432\\u0430\\u0440\\u044c `my_dict`\", \"library\": [\"ast\"]}", "generation": "Scenario 1:\nast, json, urllib.parse, base64\n```python\nimport ast\nimport json\nimport urllib.parse\nimport base64\n\ndef string_to_dict_urlencoded_base64(s):\n    \"\"\"\n    Convert a URL-encoded base64 string to a dictionary.\n\n    Parameters:\n    s (str): The URL-encoded base64 string.\n\n    Returns:\n    dict: The converted dictionary.\n\n    Requirements:\n    - ast\n    - json\n    - urllib.parse\n    - base64\n    \n    Example:\n    >>> string_to_dict_urlencoded_base64('eyJoZWxsbyI6ICJ3b3JsZCJ9')\n    {'hello': 'world'}\n    \"\"\"\n    decoded = base64.b64decode(s).decode('utf-8')\n    unquoted = urllib.parse.unquote(decoded)\n    my_dict = ast.literal_eval(unquoted)\n    \n    return my_dict\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 2:\nast, csv, io, pandas\n```python\nimport ast\nimport csv\nimport io\nimport pandas as pd\n\ndef string_to_dict_csv(s):\n    \"\"\"\n    Convert a CSV string to a list of dictionaries.\n\n    Parameters:\n    s (str): The CSV string.\n\n    Returns:\n    list: The list of dictionaries.\n\n    Requirements:\n    - ast\n    - csv\n    - io\n    - pandas\n    \n    Example:\n    >>> string_to_dict_csv(\"name,age\\\\nJohn,30\\\\nJane,25\")\n    [{'name': 'John', 'age': 30}, {'name': 'Jane', 'age': 25}]\n    \"\"\"\n    csv_file = io.StringIO(s)\n    reader = csv.DictReader(csv_file)\n    my_dict = list(reader)\n    \n    return my_dict\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 3:\nast, xml.etree.ElementTree, xmltodict\n```python\nimport ast\nimport xml.etree.ElementTree as ET\nimport xmltodict\n\ndef string_to_dict_xml(s):\n    \"\"\"\n    Convert an XML string to a dictionary.\n\n    Parameters:\n    s (str): The XML string.\n\n    Returns:\n    dict: The converted dictionary.\n\n    Requirements:\n    - ast\n    - xml.etree.ElementTree\n    - xmltodict\n    \n    Example:\n    >>> string_to_dict_xml(\"<root><name>John</name><age>30</age></root>\")\n    {'root': {'name': 'John', 'age': '30'}}\n    \"\"\"\n    my_dict = xmltodict.parse(s)\n    \n    return my_dict\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 4:\nast, yaml\n```python\nimport ast\nimport yaml\n\ndef string_to_dict_yaml(s):\n    \"\"\"\n    Convert a YAML string to a dictionary.\n\n    Parameters:\n    s (str): The YAML string.\n\n    Returns:\n    dict: The converted dictionary.\n\n    Requirements:\n    - ast\n    - yaml\n    \n    Example:\n    >>> string_to_dict_yaml(\"name: John\\\\nage: 30\")\n    {'name': 'John', 'age': 30}\n    \"\"\"\n    my_dict = yaml.load(s, Loader=yaml.FullLoader)\n    \n    return my_dict\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 5:\nast, json, re\n```python\nimport ast\nimport json\nimport re\n\ndef string_to_dict_json(s):\n    \"\"\"\n    Convert a JSON-like string to a dictionary.\n\n    Parameters:\n    s (str): The JSON-like string.\n\n    Returns:\n    dict: The converted dictionary.\n\n    Requirements:\n    - ast\n    - json\n    - re\n    \n    Example:\n    >>> string_to_dict_json(\"{name: 'John', age: 30}\")\n    {'name': 'John', 'age': 30}\n    \"\"\"\n    s = re.sub(r\"([{,]\\s*)(\\w+)(\\s*:)\", r\"\\1'\\2'\\3\", s)\n    my_dict = ast.literal_eval(s)\n    \n    return my_dict\n```"}
{"seed": "{\"task_id\": 235923, \"prompt\": \"def f_235923(file_path):\\n\\t\", \"suffix\": \"\\n\\treturn my_list\", \"canonical_solution\": \"with open(file_path) as file:\\n\\n\\t\\tmy_list = [row.strip() for row in file]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    with open('test.txt', 'w') as fw:\\n        fw.write('hello \\\\n')\\n        fw.write('world!\\\\n')\\n    assert candidate('test.txt') == [\\\"hello\\\", \\\"world!\\\"]\\n\"], \"entry_point\": \"f_235923\", \"intent\": \"\\u041f\\u0440\\u043e\\u0447\\u0438\\u0442\\u0430\\u0442\\u044c \\u043f\\u043e\\u0441\\u0442\\u0440\\u043e\\u0447\\u043d\\u043e \\u0434\\u0430\\u043d\\u043d\\u044b\\u0435 \\u0438\\u0437 \\u0444\\u0430\\u0439\\u043b\\u0430 `file_path` \\u0432 \\u0441\\u043f\\u0438\\u0441\\u043e\\u043a `my_list`\", \"library\": []}", "generation": "Scenario 1: \npandas, re, nltk, os\n```python\nimport pandas as pd\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nimport os\n\n# Constants\nSTOPWORDS = set(stopwords.words('english'))\n\ndef read_and_process_file(file_path):\n    \"\"\"\n    Read data from a text file, clean it by removing special characters and stopwords \n    and return a dictionary with word frequencies. \n\n    Parameters:\n    file_path (str): Path of the text file.\n\n    Returns:\n    dict: A dictionary with word as key and its frequency as value.\n\n    Requirements:\n    - pandas\n    - re\n    - nltk\n    - os\n\n    Example:\n    >>> read_and_process_file('test.txt')\n    \"\"\"\n    if not os.path.isfile(file_path):\n        return \"File does not exist.\"\n\n    with open(file_path, 'r') as file:\n        data = file.read().replace('\\n', ' ')\n\n    # Remove special characters\n    data = re.sub(r'\\W', ' ', data)\n    data = data.lower()\n    words = data.split()\n\n    # Remove stopwords\n    words = [word for word in words if word not in STOPWORDS]\n\n    freq_dict = pd.value_counts(words).to_dict()\n\n    return freq_dict\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \njson, pandas, matplotlib, os\n```python\nimport json\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\n\ndef visualize_json_data(file_path):\n    \"\"\"\n    Read data from a json file, convert it into pandas DataFrame and \n    visualize the data using a bar plot.\n\n    Parameters:\n    file_path (str): Path of the json file.\n\n    Returns:\n    None\n\n    Requirements:\n    - json\n    - pandas\n    - matplotlib.pyplot\n    - os\n\n    Example:\n    >>> visualize_json_data('data.json')\n    \"\"\"\n    if not os.path.isfile(file_path):\n        return \"File does not exist.\"\n\n    with open(file_path) as file:\n        data = json.load(file)\n\n    df = pd.DataFrame(data)\n\n    df.plot(kind='bar')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \ncsv, pandas, matplotlib, os\n```python\nimport csv\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\n\ndef analyze_csv_data(file_path):\n    \"\"\"\n    Read data from a csv file, convert it into pandas DataFrame and \n    analyze the data using describe method and visualize the data using a histogram.\n\n    Parameters:\n    file_path (str): Path of the csv file.\n\n    Returns:\n    None\n\n    Requirements:\n    - csv\n    - pandas\n    - matplotlib.pyplot\n    - os\n\n    Example:\n    >>> analyze_csv_data('data.csv')\n    \"\"\"\n    if not os.path.isfile(file_path):\n        return \"File does not exist.\"\n\n    df = pd.read_csv(file_path)\n\n    print(df.describe())\n\n    df.hist()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nos, shutil, glob\n```python\nimport os\nimport shutil\nimport glob\n\ndef move_files(source_dir, destination_dir, file_ext):\n    \"\"\"\n    Move all files with a specific extension from source directory to destination directory.\n\n    Parameters:\n    source_dir (str): Path of the source directory.\n    destination_dir (str): Path of the destination directory.\n    file_ext (str): Extension of the files.\n\n    Returns:\n    None\n\n    Requirements:\n    - os\n    - shutil\n    - glob\n\n    Example:\n    >>> move_files('source_dir', 'destination_dir', '.txt')\n    \"\"\"\n    if not os.path.isdir(source_dir):\n        return \"Source directory does not exist.\"\n\n    if not os.path.isdir(destination_dir):\n        os.makedirs(destination_dir)\n\n    files = glob.glob(source_dir + '/*' + file_ext)\n\n    for file in files:\n        shutil.move(file, destination_dir)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \npandas, os, datetime\n```python\nimport pandas as pd\nimport os\nfrom datetime import datetime\n\ndef filter_and_save_data(file_path, output_path, date_column, start_date, end_date):\n    \"\"\"\n    Read data from a csv file, filter rows between start_date and end_date, and save the filtered\n    data to a new csv file.\n\n    Parameters:\n    file_path (str): Path of the csv file.\n    output_path (str): Path of the output csv file.\n    date_column (str): Name of the date column.\n    start_date (str): Start date in 'yyyy-mm-dd' format.\n    end_date (str): End date in 'yyyy-mm-dd' format.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - os\n    - datetime\n\n    Example:\n    >>> filter_and_save_data('data.csv', 'output.csv', 'date', '2020-01-01', '2020-12-31')\n    \"\"\"\n    if not os.path.isfile(file_path):\n        return \"File does not exist.\"\n\n    df = pd.read_csv(file_path)\n    df[date_column] = pd.to_datetime(df[date_column])\n\n    start_date_obj = datetime.strptime(start_date, '%Y-%m-%d')\n    end_date_obj = datetime.strptime(end_date, '%Y-%m-%d')\n\n    df_filtered = df[(df[date_column] >= start_date_obj) & (df[date_column] <= end_date_obj)]\n\n    df_filtered.to_csv(output_path, index=False)\n```"}
{"seed": "{\"task_id\": 235923, \"prompt\": \"def f_235923(file_path):\\n\\t\", \"suffix\": \"\\n\\treturn my_list\", \"canonical_solution\": \"with open(file_path, 'r') as f:\\n\\n\\t\\tmy_list = f.read().splitlines()\\n\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    with open('test.txt', 'w') as fw:\\n        fw.write('hello \\\\n')\\n        fw.write('world!\\\\n')\\n    assert candidate('test.txt') == [\\\"hello \\\", \\\"world!\\\"]\\n\"], \"entry_point\": \"f_235923\", \"intent\": \"\\u041f\\u0440\\u043e\\u0447\\u0438\\u0442\\u0430\\u0442\\u044c \\u043f\\u043e\\u0441\\u0442\\u0440\\u043e\\u0447\\u043d\\u043e \\u0434\\u0430\\u043d\\u043d\\u044b\\u0435 \\u0438\\u0437 \\u0444\\u0430\\u0439\\u043b\\u0430 `file_path` \\u0432 \\u0441\\u043f\\u0438\\u0441\\u043e\\u043a `my_list`\", \"library\": []}", "generation": "Scenario 1: \nos, sys, json, csv\n```python\nimport os\nimport sys\nimport json\nimport csv\n\n# Constants\nCSV_HEADER = ['Name', 'Age', 'Address']\n\ndef convert_json_to_csv(json_file_path, csv_file_path):\n    \"\"\"\n    Convert a JSON file into a CSV file. The JSON file should have a list of dictionaries, \n    where each dictionary represents a row of data. The CSV file will have a header defined \n    by the CSV_HEADER constant.\n\n    Parameters:\n    json_file_path (str): The path of the JSON file.\n    csv_file_path (str): The path of the CSV file to be created.\n\n    Returns:\n    bool: True if conversion is successful, False otherwise.\n\n    Requirements:\n    - os\n    - sys\n    - json\n    - csv\n\n    Example:\n    >>> convert_json_to_csv('data.json', 'data.csv')\n    \"\"\"\n    try:\n        with open(json_file_path, 'r') as jfile, open(csv_file_path, 'w', newline='') as cfile:\n            data = json.load(jfile)\n\n            writer = csv.DictWriter(cfile, fieldnames=CSV_HEADER)\n            writer.writeheader()\n            for row in data:\n                writer.writerow(row)\n\n        return True\n    except:\n        return False\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncsv, numpy, pandas\n```python\nimport csv\nimport numpy as np\nimport pandas as pd\n\n# Constants\nCSV_HEADER = ['A', 'B', 'C', 'D', 'E']\n\ndef generate_csv_with_random_data(csv_file_path):\n    \"\"\"\n    Generate a CSV file with random numerical data. The CSV file will have a header defined \n    by the CSV_HEADER constant and 100 rows of random numerical data.\n\n    Parameters:\n    csv_file_path (str): The path of the CSV file to be created.\n\n    Returns:\n    bool: True if generation is successful, False otherwise.\n\n    Requirements:\n    - csv\n    - numpy\n    - pandas\n\n    Example:\n    >>> generate_csv_with_random_data('random_data.csv')\n    \"\"\"\n    try:\n        data = np.random.rand(100, len(CSV_HEADER))\n        df = pd.DataFrame(data, columns=CSV_HEADER)\n        df.to_csv(csv_file_path, index=False)\n        return True\n    except:\n        return False\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, csv, json, collections\n```python\nimport os\nimport csv\nimport json\nimport collections\n\n# Constants\nCSV_HEADER = ['Name', 'Age', 'Address']\n\ndef count_occurrences_in_csv(csv_file_path):\n    \"\"\"\n    Count the occurrences of each value in a CSV file and return the counts as a dictionary.\n\n    Parameters:\n    csv_file_path (str): The path of the CSV file.\n\n    Returns:\n    dict: A dictionary with counts of each value in the CSV file.\n\n    Requirements:\n    - os\n    - csv\n    - json\n    - collections\n\n    Example:\n    >>> count_occurrences_in_csv('data.csv')\n    \"\"\"\n    counts = collections.defaultdict(int)\n\n    with open(csv_file_path, 'r') as cfile:\n        reader = csv.DictReader(cfile)\n        for row in reader:\n            for header, value in row.items():\n                counts[value] += 1\n\n    return dict(counts)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ncsv, pandas, numpy, matplotlib\n```python\nimport csv\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCSV_HEADER = ['A', 'B', 'C', 'D', 'E']\n\ndef plot_histogram_from_csv(csv_file_path):\n    \"\"\"\n    Read a CSV file with numerical data and plot a histogram for each column.\n\n    Parameters:\n    csv_file_path (str): The path of the CSV file.\n\n    Returns:\n    bool: True if plotting is successful, False otherwise.\n\n    Requirements:\n    - csv\n    - pandas\n    - numpy\n    - matplotlib\n\n    Example:\n    >>> plot_histogram_from_csv('numerical_data.csv')\n    \"\"\"\n    try:\n        df = pd.read_csv(csv_file_path)\n        df.hist(bins=10)\n        plt.show()\n        return True\n    except:\n        return False\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\ncsv, pandas, numpy, matplotlib\n```python\nimport csv\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCSV_HEADER = ['Name', 'Age', 'Salary']\n\ndef plot_scatter_from_csv(csv_file_path, x_column, y_column):\n    \"\"\"\n    Read a CSV file and plot a scatter plot for the specified columns.\n\n    Parameters:\n    csv_file_path (str): The path of the CSV file.\n    x_column (str): The column to be plotted on the x-axis.\n    y_column (str): The column to be plotted on the y-axis.\n\n    Returns:\n    bool: True if plotting is successful, False otherwise.\n\n    Requirements:\n    - csv\n    - pandas\n    - numpy\n    - matplotlib\n\n    Example:\n    >>> plot_scatter_from_csv('data.csv', 'Age', 'Salary')\n    \"\"\"\n    try:\n        df = pd.read_csv(csv_file_path)\n        plt.scatter(df[x_column], df[y_column])\n        plt.xlabel(x_column)\n        plt.ylabel(y_column)\n        plt.title(f'{y_column} vs {x_column}')\n        plt.show()\n        return True\n    except:\n        return False\n```"}
{"seed": "{\"task_id\": 235923, \"prompt\": \"def f_235923(file_path):\\n\\t\", \"suffix\": \"\\n\\treturn my_list\", \"canonical_solution\": \"\\n\\twith io.open(file_path, encoding='utf-8') as f:\\n\\t\\tmy_list = f.read().splitlines()\\n\", \"test_start\": \"\\nimport io\\n\\ndef check(candidate):\", \"test\": [\"\\n    with open('test.txt', 'w') as fw:\\n        fw.write('hello \\\\n')\\n        fw.write('world!\\\\n')\\n    assert candidate('test.txt') == [\\\"hello \\\", \\\"world!\\\"]\\n\"], \"entry_point\": \"f_235923\", \"intent\": \"\\u041f\\u0440\\u043e\\u0447\\u0438\\u0442\\u0430\\u0442\\u044c \\u043f\\u043e\\u0441\\u0442\\u0440\\u043e\\u0447\\u043d\\u043e \\u0434\\u0430\\u043d\\u043d\\u044b\\u0435 \\u0438\\u0437 \\u0444\\u0430\\u0439\\u043b\\u0430 `file_path` \\u0432 \\u0441\\u043f\\u0438\\u0441\\u043e\\u043a `my_list`\", \"library\": [\"io\"]}", "generation": "Scenario 3:\nio, os, csv, collections\n```python\nimport io\nimport os\nimport csv\nfrom collections import Counter\n\n# Constants\nFILE_PATH = 'data.csv'\nCOLUMN_NAME = 'Status'\n\ndef count_column_values(file_path, column_name):\n    \"\"\"\n    Count the occurrences of each value in a specific column of a CSV file.\n\n    Parameters:\n    file_path (str): The path to the CSV file.\n    column_name (str): The name of the column.\n\n    Returns:\n    dict: A dictionary with the values and their counts.\n\n    Requirements:\n    - io\n    - os\n    - csv\n    - collections.Counter\n\n    Example:\n    >>> count_column_values('data.csv', 'Status')\n    \"\"\"\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n        \n    with io.open(file_path, 'r', encoding='utf-8') as f:\n        reader = csv.DictReader(f)\n        column_values = [row[column_name] for row in reader]\n\n    value_counts = dict(Counter(column_values))\n\n    return value_counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nio, os, json, re\n```python\nimport io\nimport os\nimport json\nimport re\n\n# Constants\nFILE_PATH = 'data.json'\nKEY_PATTERN = r'^[A-Za-z0-9_-]*$'\n\ndef validate_json_keys(file_path, key_pattern):\n    \"\"\"\n    Validate the keys in a JSON file against a regular expression pattern.\n\n    Parameters:\n    file_path (str): The path to the JSON file.\n    key_pattern (str): The regular expression pattern.\n\n    Returns:\n    list: A list of invalid keys.\n\n    Requirements:\n    - io\n    - os\n    - json\n    - re (regular expressions)\n\n    Example:\n    >>> validate_json_keys('data.json', r'^[A-Za-z0-9_-]*$')\n    \"\"\"\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n        \n    with io.open(file_path, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n\n    invalid_keys = [key for key in data if not re.match(key_pattern, key)]\n\n    return invalid_keys\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nio, configparser, os, shutil\n```python\nimport io\nimport configparser\nimport os\nimport shutil\n\n# Constants\nFILE_PATH = 'config.ini'\nBACKUP_FOLDER = 'backup'\n\ndef backup_ini_file(file_path, backup_folder):\n    \"\"\"\n    Create a backup of an INI file if it is valid.\n\n    Parameters:\n    file_path (str): The path to the INI file.\n    backup_folder (str): The path to the backup folder.\n\n    Returns:\n    str: The path to the backup file.\n\n    Requirements:\n    - io\n    - configparser\n    - os\n    - shutil\n\n    Example:\n    >>> backup_ini_file('config.ini', 'backup')\n    \"\"\"\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n        \n    config = configparser.ConfigParser()\n    config.read(file_path)\n\n    if not config.sections():\n        raise ValueError(f\"The file {file_path} is not a valid INI file.\")\n        \n    if not os.path.exists(backup_folder):\n        os.makedirs(backup_folder)\n        \n    backup_file_path = os.path.join(backup_folder, os.path.basename(file_path))\n    shutil.copy(file_path, backup_file_path)\n\n    return backup_file_path\n```\n`GPT_ODEX_BREAK`\n\nScenario 6:\nio, os, lxml.etree, xml.etree.ElementTree\n```python\nimport io\nimport os\nfrom lxml import etree\nimport xml.etree.ElementTree as ET\n\n# Constants\nFILE_PATH = 'data.xml'\nXSL_PATH = 'transform.xsl'\n\ndef transform_xml_file(file_path, xsl_path):\n    \"\"\"\n    Transform an XML file using an XSL file.\n\n    Parameters:\n    file_path (str): The path to the XML file.\n    xsl_path (str): The path to the XSL file.\n\n    Returns:\n    str: The transformed XML as a string.\n\n    Requirements:\n    - io\n    - os\n    - lxml.etree\n    - xml.etree.ElementTree\n\n    Example:\n    >>> transform_xml_file('data.xml', 'transform.xsl')\n    \"\"\"\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n        \n    if not os.path.exists(xsl_path):\n        raise FileNotFoundError(f\"The file {xsl_path} does not exist.\")\n        \n    dom = ET.parse(file_path)\n    xslt = ET.parse(xsl_path)\n    transform = etree.XSLT(xslt)\n    newdom = transform(dom)\n\n    return ET.tostring(newdom, pretty_print=True)\n```\n`GPT_ODEX_BREAK`\n\nScenario 7:\nio, os, pandas, matplotlib\n```python\nimport io\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nFILE_PATH = 'data.csv'\nCOLUMN_NAME = 'Salary'\n\ndef plot_histogram(file_path, column_name):\n    \"\"\"\n    Plot a histogram for a specific column of a CSV file.\n\n    Parameters:\n    file_path (str): The path to the CSV file.\n    column_name (str): The name of the column.\n\n    Requirements:\n    - io\n    - os\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_histogram('data.csv', 'Salary')\n    \"\"\"\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n        \n    df = pd.read_csv(file_path)\n    \n    if column_name not in df.columns:\n        raise ValueError(f\"The column {column_name} does not exist in the file.\")\n\n    df[column_name].plot.hist(bins=10)\n    plt.show()\n```"}
{"seed": "{\"task_id\": 44503, \"prompt\": \"def f_44503(s):\\n\\t\", \"suffix\": \"\\n\\treturn nums\", \"canonical_solution\": \"regexp = re.compile( \\\"\\\\s*(-?(?:0|[1-9]\\\\d*)(?:\\\\.\\\\d+)?)\\\\s*;\\\", re.M )\\n\\n\\tnums = regexp.findall(s)\\n\", \"test_start\": \"\\nimport re\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('a:=-1;b:=4') == ['-1']\\n\"], \"entry_point\": \"f_44503\", \"intent\": \"\\u041f\\u043e\\u043b\\u0443\\u0447\\u0438\\u0442\\u044c \\u0441\\u043f\\u0438\\u0441\\u043e\\u043a \\u0447\\u0438\\u0441\\u0435\\u043b `nums` \\u0438\\u0437 \\u0441\\u0442\\u0440\\u043e\\u043a\\u0438 `s`\", \"library\": [\"re\"]}", "generation": "Scenario 1: \npandas, re, os, glob, datetime\n```python\nimport pandas as pd\nimport re\nimport os\nimport glob\nfrom datetime import datetime\n\n# Constants\nDATA_DIR = './data/'\n\ndef load_and_process_data_files():\n    \"\"\"\n    Load all CSV files from a directory, extract date from filenames, append the data to a pandas DataFrame \n    and return it. Filenames are in the format \"data_YYYYMMDD.csv\".\n    \n    Returns:\n    DataFrame: A pandas DataFrame with all the data from the CSV files.\n    \n    Requirements:\n    - pandas\n    - re\n    - os\n    - glob\n    - datetime\n    \n    Example:\n    >>> df = load_and_process_data_files()\n    >>> print(df.head())\n    \"\"\"\n    data_files = glob.glob(os.path.join(DATA_DIR, '*.csv'))\n    df = pd.DataFrame()\n\n    for file in data_files:\n        date_str = re.search(r'data_(\\d{8})\\.csv', file).group(1)\n        date = datetime.strptime(date_str, '%Y%m%d')\n        temp_df = pd.read_csv(file)\n        temp_df['date'] = date\n        df = pd.concat([df, temp_df])\n        \n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nnumpy, re, matplotlib, random, datetime\n```python\nimport numpy as np\nimport re\nimport matplotlib.pyplot as plt\nfrom random import randint\nfrom datetime import datetime, timedelta\n\n# Constants\nDAYS = 30\nRANDOM_TEXT = 'Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.'\n\ndef generate_and_plot_random_data():\n    \"\"\"\n    Generate a numpy array with random numbers, create a list of dates for the past 30 days, \n    and plot the random numbers against the dates. Also, get a list of words from a text string \n    and select a random word from the list to be the title of the plot.\n    \n    Requirements:\n    - numpy\n    - re\n    - matplotlib.pyplot\n    - random\n    - datetime\n    \n    Example:\n    >>> generate_and_plot_random_data()\n    \"\"\"\n    dates = [datetime.now() - timedelta(days=i) for i in range(DAYS)]\n    random_numbers = np.random.rand(DAYS)\n    \n    words = re.findall(r'\\w+', RANDOM_TEXT)\n    title = words[randint(0, len(words)-1)]\n    \n    plt.figure(figsize=(10, 6))\n    plt.plot(dates, random_numbers)\n    plt.xlabel('Date')\n    plt.ylabel('Random Number')\n    plt.title(title)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnumpy, re, os, shutil, datetime\n```python\nimport numpy as np\nimport re\nimport os\nimport shutil\nfrom datetime import datetime\n\n# Constants\nSRC_DIR = './source/'\nDST_DIR = './destination/'\n\ndef move_files_based_on_name_pattern():\n    \"\"\"\n    Move all files from a source directory to a destination directory based on a name pattern. \n    The name pattern is in the format \"file_YYYYMMDD.txt\" where YYYYMMDD is the date.\n    Only move files that have a date from the past 30 days.\n    \n    Requirements:\n    - numpy\n    - re\n    - os\n    - shutil\n    - datetime\n    \n    Example:\n    >>> move_files_based_on_name_pattern()\n    \"\"\"\n    files = os.listdir(SRC_DIR)\n    today = datetime.now()\n\n    for file in files:\n        match = re.search(r'file_(\\d{8})\\.txt', file)\n        if match:\n            date_str = match.group(1)\n            date = datetime.strptime(date_str, '%Y%m%d')\n            if (today - date).days <= 30:\n                shutil.move(os.path.join(SRC_DIR, file), DST_DIR)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \npandas, re, matplotlib, datetime, numpy\n```python\nimport pandas as pd\nimport re\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nimport numpy as np\n\n# Constants\nDATA_FILE = './data.csv'\n\ndef load_process_and_plot_data():\n    \"\"\"\n    Load a CSV file into a pandas DataFrame, extract a date from a column, add a new column with the dates, \n    and plot the data. The dates are in the format \"YYYYMMDD\" and the data is plotted against the dates.\n    \n    Requirements:\n    - pandas\n    - re\n    - matplotlib.pyplot\n    - datetime\n    - numpy\n    \n    Example:\n    >>> load_process_and_plot_data()\n    \"\"\"\n    df = pd.read_csv(DATA_FILE)\n    dates = [datetime.strptime(re.search(r'(\\d{8})', str(val)).group(1), '%Y%m%d') for val in df['column']]\n    df['date'] = dates\n\n    plt.figure(figsize=(10, 6))\n    plt.plot(df['date'], df['data'])\n    plt.xlabel('Date')\n    plt.ylabel('Data')\n    plt.title('Data over Time')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnumpy, re, random, datetime, json\n```python\nimport numpy as np\nimport re\nimport random\nfrom datetime import datetime, timedelta\nimport json\n\n# Constants\nDAYS = 365\nRANDOM_TEXT = 'Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.'\n\ndef generate_json_data():\n    \"\"\"\n    Generate a JSON object with random data. The object contains dates for the past 365 days as keys and \n    random numbers as values. Also, get a list of words from a text string and select a random word from \n    the list to be added to the JSON object as 'title'.\n    \n    Returns:\n    str: A JSON string.\n\n    Requirements:\n    - numpy\n    - re\n    - random\n    - datetime\n    - json\n    \n    Example:\n    >>> json_data = generate_json_data()\n    >>> print(json_data)\n    \"\"\"\n    dates = [str(datetime.now() - timedelta(days=i)).split(' ')[0] for i in range(DAYS)]\n    random_numbers = np.random.rand(DAYS).tolist()\n    \n    words = re.findall(r'\\w+', RANDOM_TEXT)\n    title = words[random.randint(0, len(words)-1)]\n    \n    data = dict(zip(dates, random_numbers))\n    data['title'] = title\n    \n    return json.dumps(data)\n```\n"}
{"seed": "{\"task_id\": 197142, \"prompt\": \"def f_197142(s):\\n\\t\", \"suffix\": \"\\n\\treturn sentences\", \"canonical_solution\": \"split_regex = re.compile(r'[.|!|?|\\u2026]')\\n\\n\\tsentences = list(filter(lambda t: t, [t.strip() for t in split_regex.split(s)]))\", \"test_start\": \"\\nimport re\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('this is the first piece. Is there another one? Maybe...') ==         ['this is the first piece', 'Is there another one', 'Maybe']\\n\"], \"entry_point\": \"f_197142\", \"intent\": \"\\u0420\\u0430\\u0437\\u0431\\u0438\\u0442\\u044c \\u0441\\u0442\\u0440\\u043e\\u043a\\u0443 `s` \\u043d\\u0430 \\u0441\\u043f\\u0438\\u0441\\u043e\\u043a \\u043f\\u0440\\u0435\\u0434\\u043b\\u043e\\u0436\\u0435\\u043d\\u0438\\u0439 `sentences`\", \"library\": [\"re\"]}", "generation": "Scenario 1:\n```python\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nimport random\n\n# Constants\nCOLORS = ['blue', 'green', 'red', 'cyan', 'magenta', 'yellow', 'black']\n\ndef analyze_text(s):\n    \"\"\"\n    Analyze a text string `s` and return a frequency dictionary of sentences \n    and plot a bar chart for visualization.\n\n    Parameters:\n    s (str): The text string to be analyzed.\n\n    Returns:\n    dict: A dictionary with sentences as keys and their frequencies as values.\n\n    Requirements:\n    - re\n    - collections.Counter\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> text = \"This is a sample text. It has several sentences. Some sentences are short. Some sentences are long.\"\n    >>> result = analyze_text(text)\n    >>> print(result)\n    \"\"\"\n    split_regex = re.compile(r'[.|!|?|\\u2026]')\n    sentences = list(filter(lambda t: t, [t.strip() for t in split_regex.split(s)]))\n    freq_dict = dict(Counter(sentences))\n    \n    keys = list(freq_dict.keys())\n    values = list(freq_dict.values())\n    \n    plt.bar(range(len(freq_dict)), values, tick_label=keys, color=random.choice(COLORS))\n    plt.show()\n    \n    return freq_dict\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\n```python\nimport re\nimport string\nimport numpy as np\n\nPUNCTUATION = string.punctuation\n\ndef count_punctuation(s):\n    \"\"\"\n    Count the number of each punctuation in a string `s` and return a dictionary.\n\n    Parameters:\n    s (str): The string to be analyzed.\n\n    Returns:\n    dict: A dictionary with punctuation as keys and their counts as values.\n\n    Requirements:\n    - re\n    - string\n    - numpy\n\n    Example:\n    >>> text = \"Hello, world! How are you doing today? I'm doing great.\"\n    >>> result = count_punctuation(text)\n    >>> print(result)\n    \"\"\"\n    counts = dict.fromkeys(PUNCTUATION, 0)\n    for char in s:\n        if char in PUNCTUATION:\n            counts[char] += 1\n            \n    non_zero_counts = {k: v for k, v in counts.items() if v != 0}\n    \n    return non_zero_counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\n```python\nimport re\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\n\nSTOPWORDS = set(stopwords.words('english'))\n\ndef remove_stopwords(s):\n    \"\"\"\n    Remove stopwords from a string `s` and return the cleaned string.\n\n    Parameters:\n    s (str): The string to be cleaned.\n\n    Returns:\n    str: The cleaned string with stopwords removed.\n\n    Requirements:\n    - re\n    - nltk.tokenize\n    - nltk.corpus.stopwords\n\n    Example:\n    >>> text = \"This is a sample sentence with some common stopwords.\"\n    >>> result = remove_stopwords(text)\n    >>> print(result)\n    \"\"\"\n    words = word_tokenize(s)\n    cleaned_words = [word for word in words if word not in STOPWORDS]\n    \n    return ' '.join(cleaned_words)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\n```python\nimport re\nimport string\nfrom collections import Counter\n\ndef count_letter_frequency(s):\n    \"\"\"\n    Count the frequency of each letter in a string `s` and return a dictionary.\n\n    Parameters:\n    s (str): The string to be analyzed.\n\n    Returns:\n    dict: A dictionary with letters as keys and their frequencies as values.\n\n    Requirements:\n    - re\n    - string\n    - collections.Counter\n\n    Example:\n    >>> text = \"Hello, world! How are you doing today?\"\n    >>> result = count_letter_frequency(text)\n    >>> print(result)\n    \"\"\"\n    s = re.sub(r'[^a-zA-Z]', '', s).lower() # Remove non-letter characters and convert to lower case\n    letter_counts = dict(Counter(s))\n    \n    return letter_counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\n```python\nimport re\nfrom nltk.util import ngrams\n\ndef generate_ngrams(s, n):\n    \"\"\"\n    Generate n-grams from a string `s` and return a list of n-grams.\n\n    Parameters:\n    s (str): The string to be split into n-grams.\n    n (int): The number of grams in each n-gram.\n\n    Returns:\n    list: A list of n-grams.\n\n    Requirements:\n    - re\n    - nltk.util.ngrams\n\n    Example:\n    >>> text = \"This is a sample sentence.\"\n    >>> result = generate_ngrams(text, 3)\n    >>> print(result)\n    \"\"\"\n    words = s.split()\n    n_grams = list(ngrams(words, n))\n    \n    return n_grams\n```\nAbove are the complex scenarios."}
{"seed": "{\"task_id\": 166814, \"prompt\": \"def f_166814(s):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"print(s, end=\\\" \\\")\", \"test_start\": \"\\nimport sys\\nfrom io import StringIO\\n\\ndef check(candidate):\", \"test\": [\"\\n    stdout = sys.stdout\\n    s = StringIO()\\n    sys.stdout = s\\n    candidate('hello world!')\\n    sys.stdout = stdout    \\n    s.seek(0)\\n    assert len(s.read()) == 13\\n\"], \"entry_point\": \"f_166814\", \"intent\": \"\\u0412\\u044b\\u0432\\u0435\\u0441\\u0442\\u0438 \\u043d\\u0430 \\u044d\\u043a\\u0440\\u0430\\u043d \\u0441\\u0442\\u0440\\u043e\\u043a\\u0443 `s` \\u0431\\u0435\\u0437 \\u043f\\u0435\\u0440\\u0435\\u0432\\u043e\\u0434\\u0430 \\u0441\\u0442\\u0440\\u043e\\u043a\\u0438\", \"library\": [\"io\", \"sys\"]}", "generation": "Scenario 1: \npandas, pytz, datetime, random, matplotlib\n```python\nimport pandas as pd\nimport pytz\nfrom datetime import datetime\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nCITIES = ['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']\nWEATHER_CONDITIONS = ['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy']\n\n# Time zones for the cities\nTIMEZONES = {\n    'New York': 'America/New_York',\n    'London': 'Europe/London',\n    'Beijing': 'Asia/Shanghai',\n    'Tokyo': 'Asia/Tokyo',\n    'Sydney': 'Australia/Sydney'\n}\n\ndef generate_weather_report_and_plot_distribution(utc_datetime):\n    \"\"\"\n    Generate a report of weather conditions for a list of cities across various \n    time zones at a given time (UTC) and plot a distribution of the weather conditions\n    across the cities.\n\n    Parameters:\n    utc_datetime (datetime): The datetime in UTC.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with weather conditions for the cities.\n    \n    Requirements:\n    - pandas\n    - pytz\n    - datetime\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> utc_time = datetime(2023, 6, 15, 12, 0, 0, tzinfo=pytz.UTC)\n    >>> report = generate_weather_report_and_plot_distribution(utc_time)\n    >>> print(report)\n    >>> report['Weather Condition'].value_counts().plot(kind='bar')\n    \"\"\"\n    report_data = []\n\n    for city in CITIES:\n        city_tz = pytz.timezone(TIMEZONES[city])\n        city_time = utc_datetime.astimezone(city_tz)\n        weather = WEATHER_CONDITIONS[randint(0, len(WEATHER_CONDITIONS)-1)]\n        report_data.append([city, city_time, weather])\n\n    report_df = pd.DataFrame(report_data, columns=['City', 'Local Time', 'Weather Condition'])\n\n    report_df['Weather Condition'].value_counts().plot(kind='bar')\n\n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npytz, datetime, numpy, dateutil, sys, io\n```python\nimport pytz\nfrom datetime import datetime\nimport numpy as np\nfrom dateutil.parser import parse\nimport sys\nfrom io import StringIO\n\n# Constants\nLEAP_SECONDS = np.array([1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980,\n                         1981, 1982, 1983, 1985, 1988, 1990, 1993, 1994, 1997,\n                         1999, 2006, 2009, 2012, 2015, 2016, 2020])\n\ndef total_seconds_since_date_and_print(date_str, from_tz, to_tz):\n    \"\"\"\n    Calculate the total seconds that have passed since a given datetime from the current time \n    in different timezones considering the leap seconds and print the result without a newline.\n\n    Parameters:\n    date_str (str): The date string in \"yyyy-mm-dd hh:mm:ss\" format.\n    from_tz (str): The timezone of the given date string.\n    to_tz (str): The timezone to which the current time should be converted.\n\n    Returns:\n    int: The total seconds.\n\n    Requirements:\n    - datetime\n    - pytz\n    - numpy\n    - dateutil.parser\n    - sys\n    - io\n\n    Example:\n    >>> total_seconds_since_date_and_print('1970-01-01 00:00:00', 'UTC', 'America/New_York')\n    \"\"\"\n    from_tz = pytz.timezone(from_tz)\n    to_tz = pytz.timezone(to_tz)\n    given_date = parse(date_str).replace(tzinfo=from_tz)\n    current_date = datetime.now().astimezone(to_tz)\n\n    total_seconds = (current_date - given_date).total_seconds()\n\n    leap_years = LEAP_SECONDS[np.logical_and(LEAP_SECONDS >= given_date.year, LEAP_SECONDS <= current_date.year)]\n    leap_seconds = len(leap_years)\n\n    total_seconds += leap_seconds\n\n    sys.stdout = StringIO()\n    print(int(total_seconds), end=\" \")\n    sys.stdout.seek(0)\n\n    return int(total_seconds)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, shutil, sys, io\n```python\nimport os\nimport shutil\nimport sys\nfrom io import StringIO\n\ndef copy_directory_and_print(source, destination):\n    \"\"\"\n    Copy a directory from source to destination and print the path of each copied file without a newline.\n\n    Parameters:\n    source (str): The source directory path.\n    destination (str): The destination directory path.\n\n    Requirements:\n    - os\n    - shutil\n    - sys\n    - io\n\n    Example:\n    >>> copy_directory_and_print('./source_dir', './dest_dir')\n    \"\"\"\n    sys.stdout = StringIO()\n\n    for dirpath, dirnames, filenames in os.walk(source):\n        dest_dir = os.path.join(destination, os.path.relpath(dirpath, source))\n        os.makedirs(dest_dir, exist_ok=True)\n\n        for filename in filenames:\n            source_file = os.path.join(dirpath, filename)\n            dest_file = os.path.join(dest_dir, filename)\n            shutil.copy2(source_file, dest_file)\n\n            print(dest_file, end=\" \")\n\n    sys.stdout.seek(0)\n\n    return destination\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ncsv, pandas, sys, io\n```python\nimport csv\nimport pandas as pd\nimport sys\nfrom io import StringIO\n\ndef read_csv_and_print_rows(csv_file):\n    \"\"\"\n    Read a CSV file using pandas and print each row without a newline.\n\n    Parameters:\n    csv_file (str): The CSV file path.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the CSV data.\n\n    Requirements:\n    - csv\n    - pandas\n    - sys\n    - io\n\n    Example:\n    >>> df = read_csv_and_print_rows('./data.csv')\n    \"\"\"\n    df = pd.read_csv(csv_file)\n\n    sys.stdout = StringIO()\n\n    for index, row in df.iterrows():\n        print(row.tolist(), end=\" \")\n\n    sys.stdout.seek(0)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nrandom, string, sys, io\n```python\nimport random\nimport string\nimport sys\nfrom io import StringIO\n\ndef generate_random_string_and_print(length):\n    \"\"\"\n    Generate a random string of a given length and print it without a newline.\n\n    Parameters:\n    length (int): The length of the random string.\n\n    Returns:\n    str: The generated random string.\n\n    Requirements:\n    - random\n    - string\n    - sys\n    - io\n\n    Example:\n    >>> random_string = generate_random_string_and_print(10)\n    \"\"\"\n    random_string = ''.join(random.choice(string.ascii_letters + string.digits) for _ in range(length))\n\n    sys.stdout = StringIO()\n    print(random_string, end=\" \")\n    sys.stdout.seek(0)\n\n    return random_string\n```"}
{"seed": "{\"task_id\": 91763, \"prompt\": \"def f_91763(my_dict):\\n\\t\", \"suffix\": \"\\n\\treturn my_dict\", \"canonical_solution\": \"\\n\\tkeys = list(my_dict.keys()).copy()\\n\\tfor my_key in keys:\\n\\t\\tif my_key in my_dict:\\n\\t\\t\\tdel my_dict[my_key]\\n\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate({'a': 1}) == {}\\n\"], \"entry_point\": \"f_91763\", \"intent\": \"\\u0423\\u0434\\u0430\\u043b\\u0438\\u0442\\u044c \\u044d\\u043b\\u0435\\u043c\\u0435\\u043d\\u0442 \\u0441\\u043b\\u043e\\u0432\\u0430\\u0440\\u044f `my_dict` \\u0441 \\u043a\\u043b\\u044e\\u0447\\u0435\\u043c `my_key` \\u0432\\u043d\\u0443\\u0442\\u0440\\u0438 \\u0446\\u0438\\u043a\\u043b\\u0430 for \", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, random, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import randint, choice\nimport matplotlib.pyplot as plt\n\n# Constants\nFIELDS = ['Math', 'Physics', 'Chemistry', 'Biology', 'Computer Science']\nUNIVERSITIES = ['Harvard', 'MIT', 'Stanford', 'Cambridge', 'Oxford']\n\ndef generate_student_records(size):\n    \"\"\"\n    Generate a list of student records for a range of universities and fields of study.\n    Each record includes the university, field of study, and a GPA (randomly generated between 2.0 and 4.0).\n    \n    Parameters:\n    size (int): The number of student records to generate.\n    \n    Returns:\n    DataFrame: A pandas DataFrame of the student records.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> student_records = generate_student_records(100)\n    >>> print(student_records.head())\n    >>> student_records['GPA'].hist(bins=20)\n    \"\"\"\n    student_records = []\n\n    for _ in range(size):\n        university = choice(UNIVERSITIES)\n        field = choice(FIELDS)\n        gpa = round(np.random.uniform(2.0, 4.0), 2)\n        student_records.append([university, field, gpa])\n\n    df = pd.DataFrame(student_records, columns=['University', 'Field', 'GPA'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nhashlib, base64, random, string\n```python\nimport hashlib\nimport base64\nimport random\nimport string\n\n# Constants\nSALT_LENGTH = 8\n\ndef hash_password(password):\n    \"\"\"\n    Hash a password using SHA256, with a random salt. The salt is pre-pended to the password before hashing.\n    The hashed password and the salt are returned, with both encoded in base64.\n    \n    Parameters:\n    password (str): The password to hash.\n    \n    Returns:\n    tuple: A tuple containing the base64-encoded hashed password and the base64-encoded salt.\n    \n    Requirements:\n    - hashlib\n    - base64\n    - random\n    - string\n    \n    Example:\n    >>> salt, hashed_password = hash_password('mypassword')\n    >>> print(f'Salt: {salt}, Hashed Password: {hashed_password}')\n    \"\"\"\n    salt = ''.join(random.choices(string.ascii_uppercase + string.digits, k=SALT_LENGTH))\n    salted_password = salt + password\n    hashed_password = hashlib.sha256(salted_password.encode()).digest()\n\n    return base64.b64encode(salt.encode()).decode(), base64.b64encode(hashed_password).decode()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ncsv, os, glob, shutil\n```python\nimport csv\nimport os\nimport glob\nimport shutil\n\n# Constants\nCSV_HEADER = ['Name', 'Age', 'Gender', 'Occupation']\n\ndef merge_csv_files(directory):\n    \"\"\"\n    Merge all the CSV files in a directory into a single CSV file, ensuring that the header is only written once.\n    The merged file is named 'merged.csv' and is placed in the same directory.\n    \n    Parameters:\n    directory (str): The directory containing the CSV files.\n    \n    Returns:\n    str: The path of the merged CSV file.\n    \n    Requirements:\n    - csv\n    - os\n    - glob\n    - shutil\n    \n    Example:\n    >>> merged_file = merge_csv_files('/path/to/csv/files')\n    >>> print(f'Merged file: {merged_file}')\n    \"\"\"\n    merged_file_path = os.path.join(directory, 'merged.csv')\n\n    with open(merged_file_path, 'w', newline='') as merged_file:\n        writer = csv.writer(merged_file)\n        writer.writerow(CSV_HEADER)\n\n        for csv_file_path in glob.glob(os.path.join(directory, '*.csv')):\n            if csv_file_path == merged_file_path:\n                continue\n\n            with open(csv_file_path, 'r', newline='') as csv_file:\n                reader = csv.reader(csv_file)\n                next(reader)  # Skip the header\n                writer.writerows(reader)\n\n    return merged_file_path\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\njson, requests, time\n```python\nimport json\nimport requests\nimport time\n\n# Constants\nAPI_ENDPOINT = 'https://api.github.com/repos/'\n\ndef get_github_repo_details(user, repo):\n    \"\"\"\n    Get the details of a GitHub repository using the GitHub API.\n    The details include the name, owner, description, stargazers count, watchers count, and forks count.\n    \n    Parameters:\n    user (str): The username of the repository owner.\n    repo (str): The name of the repository.\n    \n    Returns:\n    dict: A dictionary containing the repository details.\n    \n    Requirements:\n    - json\n    - requests\n    - time\n    \n    Example:\n    >>> repo_details = get_github_repo_details('octocat', 'Hello-World')\n    >>> print(json.dumps(repo_details, indent=2))\n    \"\"\"\n    response = requests.get(API_ENDPOINT + f'{user}/{repo}')\n    data = response.json()\n\n    if response.status_code != 200:\n        print(f'Error {response.status_code}: {data.get(\"message\")}')\n        time.sleep(2)\n        return get_github_repo_details(user, repo)  # Retry after 2 seconds\n\n    return {\n        'name': data.get('name'),\n        'owner': data.get('owner', {}).get('login'),\n        'description': data.get('description'),\n        'stargazers_count': data.get('stargazers_count'),\n        'watchers_count': data.get('watchers_count'),\n        'forks_count': data.get('forks_count')\n    }\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, scipy.stats, matplotlib.pyplot\n```python\nimport numpy as np\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\n\n# Constants\nSAMPLE_SIZE = 1000\n\ndef generate_and_plot_normal_distribution(mu, sigma):\n    \"\"\"\n    Generate a normal distribution with a given mean (mu) and standard deviation (sigma), \n    and plot the histogram and the probability density function (PDF).\n    \n    Parameters:\n    mu (float): The mean of the distribution.\n    sigma (float): The standard deviation of the distribution.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n    \n    Example:\n    >>> generate_and_plot_normal_distribution(0, 1)\n    \"\"\"\n    sample = np.random.normal(mu, sigma, SAMPLE_SIZE)\n\n    plt.hist(sample, bins=30, density=True, alpha=0.5, color='g')\n\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, mu, sigma)\n    plt.plot(x, p, 'k', linewidth=2)\n    plt.show()\n```\n"}
{"seed": "{\"task_id\": 297129, \"prompt\": \"def f_297129(num):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"str(num).encode()\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(123) == b'123'\\n\"], \"entry_point\": \"f_297129\", \"intent\": \"\\u041f\\u0435\\u0440\\u0435\\u0432\\u0435\\u0441\\u0442\\u0438 \\u0447\\u0438\\u0441\\u043b\\u043e `num` \\u0432 \\u0431\\u0430\\u0439\\u0442 \\u0441\\u0442\\u0440\\u043e\\u043a\\u0443\", \"library\": []}", "generation": "Scenario 1:\nos, sys, json, base64\n```python\nimport os\nimport sys\nimport json\nimport base64\n\ndef convert_to_byte_string(file_path):\n    \"\"\"\n    Converts the content of a given file into a byte string. The file can be a text, json or binary file.\n    \n    Parameters:\n    file_path (str): The path of the file to convert.\n    \n    Returns:\n    bytes: The byte string representation of the file content.\n    \n    Requirements:\n    - os\n    - sys\n    - json\n    - base64\n    \n    Example:\n    >>> convert_to_byte_string('example.txt')\n    \"\"\"\n    with open(file_path, 'rb') as file:\n        byte_content = base64.b64encode(file.read())\n    return byte_content\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, glob, hashlib\n```python\nimport os\nimport glob\nimport hashlib\n\ndef hash_files_in_directory(directory, hash_type='md5'):\n    \"\"\"\n    Generates hash of all files in a given directory and its subdirectories.\n    \n    Parameters:\n    directory (str): The directory to process.\n    hash_type (str): The type of hashing algorithm to use. Default is 'md5'.\n    \n    Returns:\n    dict: A dictionary with file paths as keys and their hashes as values.\n    \n    Requirements:\n    - os\n    - glob\n    - hashlib\n    \n    Example:\n    >>> hash_files_in_directory('/path/to/directory')\n    \"\"\"\n    hashes = {}\n    for filepath in glob.iglob(directory + '**/*', recursive=True):\n        if os.path.isfile(filepath):\n            with open(filepath, 'rb') as file:\n                data = file.read()\n                if hash_type == 'sha1':\n                    hash_obj = hashlib.sha1(data)\n                else:\n                    hash_obj = hashlib.md5(data)\n                hashes[filepath] = hash_obj.hexdigest()\n    return hashes\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, shutil, base64\n```python\nimport os\nimport shutil\nimport base64\n\ndef move_and_rename_files(dir_path, new_dir_path, encode_name=False):\n    \"\"\"\n    Moves all files from one directory to another and optionally encodes their names to base64.\n    \n    Parameters:\n    dir_path (str): The path of the directory with files to move.\n    new_dir_path (str): The path of the directory where files will be moved.\n    encode_name (bool): If True, encodes file names to base64. Default is False.\n\n    Returns:\n    list: A list of new file paths.\n    \n    Requirements:\n    - os\n    - shutil\n    - base64\n    \n    Example:\n    >>> move_and_rename_files('/old_directory', '/new_directory', True)\n    \"\"\"\n    new_file_paths = []\n    for filename in os.listdir(dir_path):\n        old_file_path = os.path.join(dir_path, filename)\n        if os.path.isfile(old_file_path):\n            new_filename = base64.b64encode(filename.encode()).decode() if encode_name else filename\n            new_file_path = os.path.join(new_dir_path, new_filename)\n            shutil.move(old_file_path, new_file_path)\n            new_file_paths.append(new_file_path)\n    return new_file_paths\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, zlib, base64\n```python\nimport os\nimport zlib\nimport base64\n\ndef compress_and_encode_file(file_path):\n    \"\"\"\n    Compresses a file and encodes the compressed data to base64.\n    \n    Parameters:\n    file_path (str): The path of the file to compress and encode.\n\n    Returns:\n    bytes: The base64-encoded compressed data.\n    \n    Requirements:\n    - os\n    - zlib\n    - base64\n    \n    Example:\n    >>> compress_and_encode_file('example.txt')\n    \"\"\"\n    with open(file_path, 'rb') as file:\n        data = file.read()\n        compressed_data = zlib.compress(data)\n        encoded_data = base64.b64encode(compressed_data)\n    return encoded_data\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, zlib, base64\n```python\nimport os\nimport zlib\nimport base64\n\ndef decompress_and_decode_data(encoded_data, dest_file_path):\n    \"\"\"\n    Decodes base64-encoded data and decompresses it to a file.\n    \n    Parameters:\n    encoded_data (bytes): The base64-encoded compressed data.\n    dest_file_path (str): The path of the file where decompressed data will be written.\n\n    Returns:\n    str: The path of the file with decompressed data.\n    \n    Requirements:\n    - os\n    - zlib\n    - base64\n    \n    Example:\n    >>> decompress_and_decode_data(b'encoded_data', 'decompressed.txt')\n    \"\"\"\n    decoded_data = base64.b64decode(encoded_data)\n    decompressed_data = zlib.decompress(decoded_data)\n    with open(dest_file_path, 'wb') as file:\n        file.write(decompressed_data)\n    return dest_file_path\n```\nPlease note that in all these examples, error handling (such as file not found, directory not found, incorrect base64 data, etc.) is not included for simplicity."}
{"seed": "{\"task_id\": 55256, \"prompt\": \"def f_55256(s, my_dict):\\n\\t\", \"suffix\": \"\\n\\treturn s\", \"canonical_solution\": \"for i, symbol in enumerate(s):\\n\\n\\t\\tif symbol in my_dict:\\n\\n\\t\\t\\ts = s[:i] + str(my_dict[symbol]) + s[i+1:]\\n\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('123', {'a': 'a', 'b': 0, '2': 'x'}) == '1x3'\\n\"], \"entry_point\": \"f_55256\", \"intent\": \"\\u0417\\u0430\\u043c\\u0435\\u043d\\u0438\\u0442\\u044c \\u0441\\u0438\\u043c\\u0432\\u043e\\u043b\\u044b \\u0432 \\u0441\\u0442\\u0440\\u043e\\u043a\\u0435 `s` \\u043f\\u043e \\u0441\\u043b\\u043e\\u0432\\u0430\\u0440\\u044e `my_dict`\", \"library\": []}", "generation": "Scenario 1:\npandas, numpy, matplotlib, random\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import choice \n\n# Constants\nLETTERS = list('abcdefghijklmnopqrstuvwxyz')\n\ndef generate_random_dataframe(s, my_dict):\n    \"\"\"\n    Generate a random dataframe based on a string and a dictionary. The string defines the column names, \n    and the dictionary maps the column names to their respective data types. Non-mentioned columns in the \n    dictionary will have string data type.\n\n    Parameters:\n    s (str): A string of column names separated by comma.\n    my_dict (dict): A dictionary where keys are column names and values are data types.\n\n    Returns:\n    DataFrame: A pandas DataFrame with random data.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - random.choice\n\n    Example:\n    >>> df = generate_random_dataframe('a,b,c', {'a': 'int', 'b': 'float'})\n    >>> print(df)\n    >>> df['a'].plot(kind='hist')\n    \"\"\"\n    columns = s.split(',')\n    df = pd.DataFrame()\n\n    for column in columns:\n        if column in my_dict:\n            if my_dict[column] == 'int':\n                df[column] = np.random.randint(0, 100, 100)\n            elif my_dict[column] == 'float':\n                df[column] = np.random.rand(100)\n        else:\n            df[column] = [''.join([choice(LETTERS) for _ in range(5)]) for _ in range(100)]\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, json, urllib.request\n```python\nimport pandas as pd\nimport json\nimport urllib.request\n\ndef get_and_transform_data(s, my_dict):\n    \"\"\"\n    Fetch data from the URL specified by the string s, and transform the JSON data using the dictionary my_dict.\n    The dictionary maps old keys to new keys in the JSON data.\n\n    Parameters:\n    s (str): A URL to fetch the JSON data.\n    my_dict (dict): A dictionary where keys are old keys and values are new keys.\n\n    Returns:\n    DataFrame: A pandas DataFrame with transformed data.\n\n    Requirements:\n    - pandas\n    - json\n    - urllib.request\n\n    Example:\n    >>> url = 'https://api.github.com/users/octocat'\n    >>> mapping_dict = {'login': 'username', 'id': 'user_id'}\n    >>> df = get_and_transform_data(url, mapping_dict)\n    >>> print(df)\n    \"\"\"\n    with urllib.request.urlopen(s) as url:\n        data = json.loads(url.read().decode())\n\n    data = {my_dict.get(key, key): value for key, value in data.items() if key in my_dict}\n\n    return pd.DataFrame([data])\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, matplotlib, random\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import shuffle\n\n# Constants\nNUMBERS = list(range(1, 101))\n\ndef plot_shuffled_numbers(s, my_dict):\n    \"\"\"\n    Shuffle a list of numbers and plot them. The string s defines the title of the plot, \n    and the dictionary my_dict maps the plot attributes to their respective values.\n\n    Parameters:\n    s (str): The title of the plot.\n    my_dict (dict): A dictionary where keys are plot attributes and values are attribute values.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - random.shuffle\n\n    Example:\n    >>> plot_attributes = {'xlabel': 'Index', 'ylabel': 'Number', 'color': 'red'}\n    >>> plot_shuffled_numbers('Shuffled Numbers', plot_attributes)\n    \"\"\"\n    shuffle(NUMBERS)\n    plt.plot(NUMBERS, **my_dict)\n    plt.title(s)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, os, csv\n```python\nimport pandas as pd\nimport os\nimport csv\n\ndef save_dataframe(s, my_dict):\n    \"\"\"\n    Save a dictionary as a CSV file. The string s defines the file name, \n    and the dictionary my_dict contains the data to be saved.\n\n    Parameters:\n    s (str): The file name.\n    my_dict (dict): A dictionary where keys are column names and values are lists of data.\n\n    Returns:\n    str: The file path.\n\n    Requirements:\n    - pandas\n    - os\n    - csv\n\n    Example:\n    >>> data_dict = {'Name': ['Alice', 'Bob', 'Charlie'], 'Age': [25, 30, 35]}\n    >>> save_dataframe('data.csv', data_dict)\n    \"\"\"\n    df = pd.DataFrame(my_dict)\n    file_path = os.path.join(os.getcwd(), s)\n    df.to_csv(file_path, index=False)\n\n    return file_path\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, seaborn, matplotlib\n```python\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['col1', 'col2', 'col3', 'col4', 'col5']\n\ndef plot_heatmap(s, my_dict):\n    \"\"\"\n    Create a heatmap from a dictionary. The string s defines the title of the heatmap, \n    and the dictionary my_dict contains the data to be plotted.\n\n    Parameters:\n    s (str): The title of the heatmap.\n    my_dict (dict): A dictionary where keys are indices and values are lists of data.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - seaborn\n    - matplotlib.pyplot\n\n    Example:\n    >>> data_dict = {i: [j for j in range(5)] for i in range(5)}\n    >>> plot_heatmap('Heatmap', data_dict)\n    \"\"\"\n    df = pd.DataFrame(my_dict, columns=COLUMNS)\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(df, annot=True, cmap='viridis')\n    plt.title(s)\n    plt.show()\n```\n"}
{"seed": "{\"task_id\": 105134, \"prompt\": \"def f_105134():\\n\\t\", \"suffix\": \"\\n\\treturn path\", \"canonical_solution\": \"\\n\\tclass Owner:\\n\\t\\tdef __init__(self, id):\\n\\t\\t\\tself.id = id\\n\\tclass Instance:\\n\\t\\tdef __init__(self, owner, slug):\\n\\t\\t\\tself.owner = owner\\n\\t\\t\\tself.slug = slug\\n\\n\\tdef get_upload_path(instance, filename):\\n\\t\\treturn os.path.join(\\\"user_%d\\\" % instance.owner.id, \\\"car_%s\\\" % instance.slug, filename)\\n\\n\\towner = Owner(1)\\n\\tinst = Instance(owner, \\\"hello\\\")\\n\\tpath = get_upload_path(inst, 'hello.txt')\\n\", \"test_start\": \"\\nimport os\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate() == 'user_1/car_hello/hello.txt'\\n\"], \"entry_point\": \"f_105134\", \"intent\": \"\\u0421\\u0433\\u0435\\u043d\\u0435\\u043d\\u0440\\u0438\\u0440\\u043e\\u0432\\u0430\\u0442\\u044c \\u0434\\u0438\\u043d\\u0430\\u043c\\u0438\\u0447\\u0435\\u0441\\u043a\\u0438\\u0439 \\u043f\\u0443\\u0442\\u044c \\u0434\\u043b\\u044f \\u0445\\u0440\\u0430\\u043d\\u0435\\u043d\\u0438\\u044f \\u0444\\u0430\\u0439\\u043b\\u043e\\u0432 \\u0432 Django upload_to\", \"library\": [\"os\"]}", "generation": "Scenario 1:\nos, hashlib, datetime, random\n```python\nimport os\nimport hashlib\nfrom datetime import datetime\nfrom random import randint\n\n# Constants\nFILE_TYPES = ['.txt', '.jpg', '.png', '.pdf']\n\nclass User:\n    def __init__(self, id):\n        self.id = id\n\nclass File:\n    def __init__(self, user, type):\n        self.user = user\n        self.type = type\n\ndef generate_file_path(file, filename):\n    \"\"\"\n    Generate a dynamic path for storing files in Django upload_to, \n    where the path includes the hashed user id, current date, and a random number.\n\n    Parameters:\n    file (File): The file instance.\n    filename (str): The original file name.\n\n    Returns:\n    str: The generated file path.\n\n    Requirements:\n    - os\n    - hashlib\n    - datetime\n    - random\n\n    Example:\n    >>> user = User(10)\n    >>> file = File(user, FILE_TYPES[2])\n    >>> path = generate_file_path(file, 'hello.png')\n    \"\"\"\n    user_hash = hashlib.md5(str(file.user.id).encode()).hexdigest()\n    date_str = datetime.now().strftime('%Y%m%d')\n    random_number = str(randint(1000, 9999))\n\n    new_filename = f\"{user_hash}_{date_str}_{random_number}{file.type}\"\n    path = os.path.join(f\"user_{file.user.id}\", new_filename)\n\n    return path\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, uuid, datetime\n```python\nimport os\nimport uuid\nfrom datetime import datetime\n\nclass Company:\n    def __init__(self, id):\n        self.id = id\n\nclass Document:\n    def __init__(self, company, doc_type):\n        self.company = company\n        self.doc_type = doc_type\n\ndef generate_document_path(document, filename):\n    \"\"\"\n    Generate a unique path for storing document files in a Django upload_to, \n    where the path includes the company id, document type, current year, and a UUID.\n\n    Parameters:\n    document (Document): The document instance.\n    filename (str): The original file name.\n\n    Returns:\n    str: The generated document path.\n\n    Requirements:\n    - os\n    - uuid\n    - datetime\n\n    Example:\n    >>> company = Company(20)\n    >>> document = Document(company, 'invoice')\n    >>> path = generate_document_path(document, 'invoice_2021.pdf')\n    \"\"\"\n    year = datetime.now().year\n    unique_id = str(uuid.uuid4())\n    new_filename = f\"{document.doc_type}_{year}_{unique_id}{os.path.splitext(filename)[1]}\"\n    path = os.path.join(f\"company_{document.company.id}\", new_filename)\n\n    return path\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, random, string\n```python\nimport os\nimport random\nimport string\n\n# Constants\nLETTERS = string.ascii_lowercase\n\nclass Project:\n    def __init__(self, id):\n        self.id = id\n\nclass Asset:\n    def __init__(self, project, asset_type):\n        self.project = project\n        self.asset_type = asset_type\n\ndef generate_asset_path(asset, filename):\n    \"\"\"\n    Generate a random path for storing asset files in a Django upload_to, \n    where the path includes the project id, asset type, and a random string.\n\n    Parameters:\n    asset (Asset): The asset instance.\n    filename (str): The original file name.\n\n    Returns:\n    str: The generated asset path.\n\n    Requirements:\n    - os\n    - random\n    - string\n\n    Example:\n    >>> project = Project(30)\n    >>> asset = Asset(project, 'image')\n    >>> path = generate_asset_path(asset, 'image.jpg')\n    \"\"\"\n    random_str = ''.join(random.choice(LETTERS) for _ in range(10))\n    new_filename = f\"{random_str}_{asset.asset_type}{os.path.splitext(filename)[1]}\"\n    path = os.path.join(f\"project_{asset.project.id}\", new_filename)\n\n    return path\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, hashlib, time\n```python\nimport os\nimport hashlib\nimport time\n\nclass Client:\n    def __init__(self, id):\n        self.id = id\n\nclass Video:\n    def __init__(self, client):\n        self.client = client\n\ndef generate_video_path(video, filename):\n    \"\"\"\n    Generate a dynamic path for storing video files in Django upload_to, \n    where the path includes the hashed client id and current timestamp.\n\n    Parameters:\n    video (Video): The video instance.\n    filename (str): The original file name.\n\n    Returns:\n    str: The generated video path.\n\n    Requirements:\n    - os\n    - hashlib\n    - time\n\n    Example:\n    >>> client = Client(40)\n    >>> video = Video(client)\n    >>> path = generate_video_path(video, 'video.mp4')\n    \"\"\"\n    client_hash = hashlib.sha256(str(video.client.id).encode()).hexdigest()\n    timestamp = str(int(time.time()))\n\n    new_filename = f\"{client_hash}_{timestamp}{os.path.splitext(filename)[1]}\"\n    path = os.path.join(f\"client_{video.client.id}\", new_filename)\n\n    return path\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, datetime, uuid\n```python\nimport os\nfrom datetime import datetime\nimport uuid\n\nclass Author:\n    def __init__(self, id):\n        self.id = id\n\nclass Article:\n    def __init__(self, author):\n        self.author = author\n\ndef generate_article_path(article, filename):\n    \"\"\"\n    Generate a unique path for storing article files in Django upload_to, \n    where the path includes the author id, current date, and a UUID.\n\n    Parameters:\n    article (Article): The article instance.\n    filename (str): The original file name.\n\n    Returns:\n    str: The generated article path.\n\n    Requirements:\n    - os\n    - datetime\n    - uuid\n\n    Example:\n    >>> author = Author(50)\n    >>> article = Article(author)\n    >>> path = generate_article_path(article, 'article.docx')\n    \"\"\"\n    date_str = datetime.now().strftime('%Y%m%d')\n    unique_id = str(uuid.uuid4())\n\n    new_filename = f\"{date_str}_{unique_id}{os.path.splitext(filename)[1]}\"\n    path = os.path.join(f\"author_{article.author.id}\", new_filename)\n\n    return path\n```"}
{"seed": "{\"task_id\": 185524, \"prompt\": \"def f_185524(s, suff):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"\\\" \\\".join(list(filter( lambda x : not x.endswith(suff), list(s.split()))))\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('agfgf gfg123 afg345 safg123', '123') == 'agfgf afg345'\\n\"], \"entry_point\": \"f_185524\", \"intent\": \"\\u0418\\u0437 \\u0441\\u0442\\u0440\\u043e\\u043a\\u0438 `s` \\u0443\\u0434\\u0430\\u043b\\u0438\\u0442\\u044c \\u0441\\u043b\\u043e\\u0432\\u0430 \\u0441 \\u0441\\u0443\\u0444\\u0438\\u043a\\u0441\\u043e\\u043c `suff`\", \"library\": []}", "generation": "Scenario 1: \npandas, nltk, string, re\n```python\nimport pandas as pd\nimport nltk\nfrom string import punctuation\nimport re\n\n# Constants\nSTOPWORDS = set(nltk.corpus.stopwords.words('english'))\n\ndef filter_words(df, column_name, suffix):\n    \"\"\"\n    From a specific column in a pandas DataFrame, filter out rows where the last word in the column ends with a given suffix,\n    and also filter out common English stop words.\n\n    Parameters:\n    df (DataFrame): The input pandas DataFrame.\n    column_name (str): The column in the df from which to filter words.\n    suffix (str): The suffix of the words to be filtered.\n\n    Returns:\n    DataFrame: A pandas DataFrame with filtered words.\n\n    Requirements:\n    - pandas\n    - nltk\n    - string\n    - re\n\n    Example:\n    >>> df = pd.DataFrame({'text': ['This is a test', 'Another test', 'Test again', 'Final test']})\n    >>> filtered_df = filter_words(df, 'text', 'test')\n    >>> print(filtered_df)\n    \"\"\"\n    def remove_stopwords_and_filter_suffix(text):\n        words = nltk.word_tokenize(text)\n        words = [word for word in words if word not in punctuation and word not in STOPWORDS]\n        if words[-1].endswith(suffix):\n            words = words[:-1]\n        return ' '.join(words)\n    \n    df[column_name] = df[column_name].apply(remove_stopwords_and_filter_suffix)\n    df = df[df[column_name].str.strip() != '']\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnltk, string, re, collections\n```python\nimport nltk\nfrom string import punctuation\nimport re\nfrom collections import Counter\n\n# Constants\nSTOPWORDS = set(nltk.corpus.stopwords.words('english'))\n\ndef count_suffix_occurrences(text, suffix):\n    \"\"\"\n    Count the occurrences of words ending with a given suffix in a text string, excluding common English stop words.\n\n    Parameters:\n    text (str): The input text.\n    suffix (str): The suffix of the words to be counted.\n\n    Returns:\n    dict: A dictionary with words as keys and their counts as values.\n\n    Requirements:\n    - nltk\n    - string\n    - re\n    - collections\n\n    Example:\n    >>> count_suffix_occurrences('This is a test. Another test. Test again. Final test', 'test')\n    \"\"\"\n    words = nltk.word_tokenize(text)\n    words = [word for word in words if word not in punctuation and word not in STOPWORDS and word.endswith(suffix)]\n    word_counts = Counter(words)\n    return dict(word_counts)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, nltk, string, re, sklearn\n```python\nimport pandas as pd\nimport nltk\nfrom string import punctuation\nimport re\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Constants\nSTOPWORDS = set(nltk.corpus.stopwords.words('english'))\n\ndef compute_tfidf(df, column_name, suffix):\n    \"\"\"\n    Compute the TF-IDF score of words ending with a given suffix in a specific column of a pandas DataFrame,\n    excluding common English stop words.\n\n    Parameters:\n    df (DataFrame): The input pandas DataFrame.\n    column_name (str): The column in the df from which to compute TF-IDF.\n    suffix (str): The suffix of the words to compute TF-IDF.\n\n    Returns:\n    DataFrame: A pandas DataFrame with TF-IDF scores.\n\n    Requirements:\n    - pandas\n    - nltk\n    - string\n    - re\n    - sklearn\n\n    Example:\n    >>> df = pd.DataFrame({'text': ['This is a test', 'Another test', 'Test again', 'Final test']})\n    >>> tfidf_df = compute_tfidf(df, 'text', 'test')\n    >>> print(tfidf_df)\n    \"\"\"\n    def remove_stopwords_and_filter_suffix(text):\n        words = nltk.word_tokenize(text)\n        words = [word for word in words if word not in punctuation and word not in STOPWORDS and word.endswith(suffix)]\n        return ' '.join(words)\n    \n    df[column_name] = df[column_name].apply(remove_stopwords_and_filter_suffix)\n    vectorizer = TfidfVectorizer()\n    tfidf_matrix = vectorizer.fit_transform(df[column_name])\n    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names())\n    return tfidf_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnltk, string, re, collections, itertools\n```python\nimport nltk\nfrom string import punctuation\nimport re\nfrom collections import Counter\nimport itertools\n\n# Constants\nSTOPWORDS = set(nltk.corpus.stopwords.words('english'))\n\ndef get_most_common_suffix_pairs(text, suffix):\n    \"\"\"\n    Get the most common pairs of words ending with a given suffix in a text string, excluding common English stop words.\n\n    Parameters:\n    text (str): The input text.\n    suffix (str): The suffix of the words to be counted.\n\n    Returns:\n    list: A list of tuples with the most common word pairs and their counts.\n\n    Requirements:\n    - nltk\n    - string\n    - re\n    - collections\n    - itertools\n\n    Example:\n    >>> get_most_common_suffix_pairs('This is a test. Another test. Test again. Final test', 'test')\n    \"\"\"\n    words = nltk.word_tokenize(text)\n    words = [word for word in words if word not in punctuation and word not in STOPWORDS and word.endswith(suffix)]\n    word_pairs = list(itertools.combinations(words, 2))\n    pair_counts = Counter(word_pairs)\n    return pair_counts.most_common()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnltk, string, re, pandas, matplotlib\n```python\nimport nltk\nfrom string import punctuation\nimport re\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nSTOPWORDS = set(nltk.corpus.stopwords.words('english'))\n\ndef plot_suffix_word_frequencies(df, column_name, suffix):\n    \"\"\"\n    Plot the frequencies of words ending with a given suffix in a specific column of a pandas DataFrame,\n    excluding common English stop words.\n\n    Parameters:\n    df (DataFrame): The input pandas DataFrame.\n    column_name (str): The column in the df from which to compute frequencies.\n    suffix (str): The suffix of the words to compute frequencies.\n\n    Returns:\n    None\n\n    Requirements:\n    - nltk\n    - string\n    - re\n    - pandas\n    - matplotlib\n\n    Example:\n    >>> df = pd.DataFrame({'text': ['This is a test', 'Another test', 'Test again', 'Final test']})\n    >>> plot_suffix_word_frequencies(df, 'text', 'test')\n    \"\"\"\n    def remove_stopwords_and_filter_suffix(text):\n        words = nltk.word_tokenize(text)\n        words = [word for word in words if word not in punctuation and word not in STOPWORDS and word.endswith(suffix)]\n        return ' '.join(words)\n    \n    df[column_name] = df[column_name].apply(remove_stopwords_and_filter_suffix)\n    word_frequencies = df[column_name].str.split(expand=True).stack().value_counts()\n    word_frequencies.plot(kind='bar')\n    plt.show()\n```"}
{"seed": "{\"task_id\": 294170, \"prompt\": \"def f_294170(my_var):\\n\\t\", \"suffix\": \"\\n\\treturn exists\", \"canonical_solution\": \"\\n\\tglobs = globals().copy()\\n\\tvar_name = list(filter(lambda x: id(globs[x]) == id(my_var), globs))[0]\\n\\texists = var_name in globals()\\n\", \"test_start\": \"\\nu = 5\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(u) == True\\n\"], \"entry_point\": \"f_294170\", \"intent\": \"\\u041f\\u0440\\u043e\\u0432\\u0435\\u0440\\u0438\\u0442\\u044c \\u0441\\u0443\\u0449\\u0435\\u0441\\u0442\\u0432\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435 \\u0433\\u043b\\u043e\\u0431\\u0430\\u043b\\u044c\\u043d\\u043e\\u0439 \\u043f\\u0435\\u0440\\u0435\\u043c\\u0435\\u043d\\u043d\\u043e\\u0439 `my_var`\", \"library\": []}", "generation": "Scenario 1:\npandas, numpy, matplotlib, random\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import choice\n\n# Constants\nPOPULATION_SIZE = 10000\nAGES = list(range(1, 101))\nGENDERS = ['Male', 'Female']\n\ndef generate_population_data():\n    \"\"\"\n    Generate a DataFrame representing a population of 10,000 people with their \n    ages and genders, and plot a histogram of the age distribution for males and females.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the population data.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - random.choice\n\n    Example:\n    >>> population_data = generate_population_data()\n    >>> print(population_data)\n    >>> population_data.groupby('Gender')['Age'].plot(kind='hist', alpha=0.5)\n    >>> plt.legend()\n    \"\"\"\n    population_data = []\n\n    for _ in range(POPULATION_SIZE):\n        age = choice(AGES)\n        gender = choice(GENDERS)\n        population_data.append([age, gender])\n\n    population_df = pd.DataFrame(population_data, columns=['Age', 'Gender'])\n\n    return population_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, sys, shutil, pathlib\n```python\nimport os\nimport sys\nimport shutil\nfrom pathlib import Path\n\n# Constants\nBACKUP_DIRECTORY = Path.home() / 'backup'\n\ndef backup_files(directory):\n    \"\"\"\n    Backup all files from a given directory to a backup directory in the home directory.\n\n    Parameters:\n    directory (str): The directory to backup.\n\n    Returns:\n    str: The path of the backup directory.\n\n    Requirements:\n    - os\n    - sys\n    - shutil\n    - pathlib.Path\n\n    Example:\n    >>> backup_files('~/Documents')\n    \"\"\"\n    source_directory = Path(directory).expanduser().resolve()\n\n    if not source_directory.exists() or not source_directory.is_dir():\n        print(f\"Error: {source_directory} does not exist or is not a directory.\", file=sys.stderr)\n        return\n\n    for file in source_directory.iterdir():\n        if file.is_file():\n            shutil.copy(file, BACKUP_DIRECTORY / file.name)\n\n    return str(BACKUP_DIRECTORY)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ndatetime, pytz, dateutil, numpy\n```python\nimport datetime\nimport pytz\nfrom dateutil.relativedelta import relativedelta\nimport numpy as np\n\n# Constants\nMONTHS_IN_YEAR = 12\n\ndef calculate_interest(principal, annual_interest_rate, start_date_str, end_date_str, timezone_str):\n    \"\"\"\n    Calculate the interest accumulated on a principal amount from a start date to an end date.\n\n    Parameters:\n    principal (float): The principal amount.\n    annual_interest_rate (float): The annual interest rate as a percentage.\n    start_date_str (str): The start date as a string in 'yyyy-mm-dd' format.\n    end_date_str (str): The end date as a string in 'yyyy-mm-dd' format.\n    timezone_str (str): The timezone as a string.\n\n    Returns:\n    float: The accumulated interest.\n\n    Requirements:\n    - datetime\n    - pytz\n    - dateutil.relativedelta\n    - numpy\n\n    Example:\n    >>> calculate_interest(10000, 5, '2020-01-01', '2021-01-01', 'Asia/Tokyo')\n    \"\"\"\n    timezone = pytz.timezone(timezone_str)\n    start_date = datetime.datetime.strptime(start_date_str, '%Y-%m-%d').replace(tzinfo=timezone)\n    end_date = datetime.datetime.strptime(end_date_str, '%Y-%m-%d').replace(tzinfo=timezone)\n\n    months = relativedelta(end_date, start_date).years * MONTHS_IN_YEAR + relativedelta(end_date, start_date).months\n\n    interest = principal * (1 + annual_interest_rate / 100 / MONTHS_IN_YEAR) ** months - principal\n\n    return np.round(interest, 2)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, sklearn.preprocessing, sklearn.model_selection\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n# Constants\nRANDOM_STATE = 42\nTEST_SIZE = 0.2\n\ndef preprocess_data(df):\n    \"\"\"\n    Preprocess a DataFrame by standardizing the numerical columns and splitting \n    the data into a training set and a testing set.\n\n    Parameters:\n    df (DataFrame): The DataFrame to preprocess.\n\n    Returns:\n    Tuple: The training and testing sets as DataFrames.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.preprocessing.StandardScaler\n    - sklearn.model_selection.train_test_split\n\n    Example:\n    >>> df = pd.DataFrame(np.random.rand(100, 5), columns=list('ABCDE'))\n    >>> train_df, test_df = preprocess_data(df)\n    \"\"\"\n    scaler = StandardScaler()\n\n    for column in df.columns:\n        if np.issubdtype(df[column].dtype, np.number):\n            df[column] = scaler.fit_transform(df[[column]])\n\n    train_df, test_df = train_test_split(df, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n\n    return train_df, test_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, hashlib, binascii\n```python\nimport os\nimport hashlib\nimport binascii\n\n# Constants\nSALT_SIZE = 16\n\ndef hash_password(password):\n    \"\"\"\n    Hash a password with a random salt using the SHA256 algorithm.\n\n    Parameters:\n    password (str): The password to hash.\n\n    Returns:\n    str: The hashed password.\n\n    Requirements:\n    - os\n    - hashlib\n    - binascii\n\n    Example:\n    >>> hash_password('password123')\n    \"\"\"\n    salt = os.urandom(SALT_SIZE)\n    hashed_password = hashlib.pbkdf2_hmac('sha256', password.encode(), salt, 100000)\n    return binascii.hexlify(salt).decode() + binascii.hexlify(hashed_password).decode()\n```"}
{"seed": "{\"task_id\": 205516, \"prompt\": \"def f_205516(dict_list, my_key):\\n\\t\", \"suffix\": \"\\n\\treturn values_by_key\", \"canonical_solution\": \"values_by_key  = [x[my_key] for x in dict_list if my_key in x] \\n\\t\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([{'x':123}, {'a':5435.0, 'x': 'fdg'}], 'a') == [5435.0]\\n\", \"\\n    assert candidate([{'x':123}, {'a':5435.0, 'x': 'fdg'}], 'x') == [123, 'fdg']\\n\"], \"entry_point\": \"f_205516\", \"intent\": \"\\u041f\\u043e\\u043b\\u0443\\u0447\\u0438\\u0442\\u044c \\u0441\\u043f\\u0438\\u0441\\u043e\\u043a \\u0437\\u043d\\u0430\\u0447\\u0435\\u043d\\u0438\\u0439 `values_by_key` \\u0438\\u0437 \\u0441\\u043f\\u0438\\u0441\\u043a\\u0430 \\u0441\\u043b\\u043e\\u0432\\u0430\\u0440\\u0435\\u0439 `dict_list` \\u043f\\u043e \\u043a\\u043b\\u044e\\u0447\\u0443 `my_key`\", \"library\": []}", "generation": "Scenario 1:\npandas, json, os, re\n```python\nimport pandas as pd\nimport json\nimport os\nimport re\n\n# Constants\nFILE_DIR = '/path/to/json/files'\n\ndef extract_values_from_json_files(key):\n    \"\"\"\n    Extract a list of values by key from all json files in a specified directory.\n\n    Parameters:\n    key (str): The key to look for in the json files.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the values for the key from all files.\n\n    Requirements:\n    - pandas\n    - json\n    - os\n    - re\n\n    Example:\n    >>> extract_values_from_json_files('name')\n    \"\"\"\n    values = []\n\n    for filename in os.listdir(FILE_DIR):\n        if re.match(r'.*\\.json$', filename):\n            with open(os.path.join(FILE_DIR, filename), 'r') as file:\n                data = json.load(file)\n                if key in data:\n                    values.append(data[key])\n\n    df = pd.DataFrame(values, columns=[key])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, os, csv, collections\n```python\nimport pandas as pd\nimport os\nimport csv\nfrom collections import defaultdict\n\n# Constants\nFILE_DIR = '/path/to/csv/files'\n\ndef count_occurrences_of_value_in_csv_files(key):\n    \"\"\"\n    Count the occurrences of a value by key from all csv files in a specified directory.\n\n    Parameters:\n    key (str): The key to look for in the csv files.\n\n    Returns:\n    dict: A dictionary with the count of occurrences for each value of the key.\n\n    Requirements:\n    - pandas\n    - os\n    - csv\n    - collections\n\n    Example:\n    >>> count_occurrences_of_value_in_csv_files('name')\n    \"\"\"\n    count_dict = defaultdict(int)\n\n    for filename in os.listdir(FILE_DIR):\n        if filename.endswith('.csv'):\n            df = pd.read_csv(os.path.join(FILE_DIR, filename))\n            if key in df.columns:\n                count_dict.update(df[key].value_counts())\n\n    return dict(count_dict)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nglob, pandas, xlrd, openpyxl\n```python\nimport glob\nimport pandas as pd\n\n# Constants\nFILE_DIR = '/path/to/excel/files'\n\ndef extract_values_from_excel_files(key):\n    \"\"\"\n    Extract a list of values by key from all excel files in a specified directory.\n\n    Parameters:\n    key (str): The key to look for in the excel files.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the values for the key from all files.\n\n    Requirements:\n    - glob\n    - pandas\n    - xlrd\n    - openpyxl\n\n    Example:\n    >>> extract_values_from_excel_files('name')\n    \"\"\"\n    values = []\n\n    for filename in glob.glob(os.path.join(FILE_DIR, '*.xls*')):\n        df = pd.read_excel(filename)\n        if key in df.columns:\n            values.extend(df[key].tolist())\n\n    df = pd.DataFrame(values, columns=[key])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, pytz, datetime, random, csv\n```python\nimport pandas as pd\nimport pytz\nfrom datetime import datetime\nfrom random import randint\nimport csv\n\n# Constants\nCITIES = ['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']\nWEATHER_CONDITIONS = ['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy']\n\n# Time zones for the cities\nTIMEZONES = {\n    'New York': 'America/New_York',\n    'London': 'Europe/London',\n    'Beijing': 'Asia/Shanghai',\n    'Tokyo': 'Asia/Tokyo',\n    'Sydney': 'Australia/Sydney'\n}\n\ndef generate_weather_report_csv(utc_datetime):\n    \"\"\"\n    Generate a CSV report of weather conditions for a list of cities across various \n    time zones at a given time (UTC).\n    \n    Parameters:\n    utc_datetime (datetime): The datetime in UTC.\n    \n    Returns:\n    str: The filename of the generated CSV file.\n    \n    Requirements:\n    - pandas\n    - pytz\n    - datetime\n    - random\n    - csv\n    \n    Example:\n    >>> utc_time = datetime(2023, 6, 15, 12, 0, 0, tzinfo=pytz.UTC)\n    >>> filename = generate_weather_report_csv(utc_time)\n    >>> print(filename)\n    \"\"\"\n    report_data = []\n\n    for city in CITIES:\n        city_tz = pytz.timezone(TIMEZONES[city])\n        city_time = utc_datetime.astimezone(city_tz)\n        weather = WEATHER_CONDITIONS[randint(0, len(WEATHER_CONDITIONS)-1)]\n        report_data.append([city, city_time, weather])\n\n    filename = f\"weather_report_{utc_datetime.strftime('%Y%m%d%H%M%S')}.csv\"\n\n    with open(filename, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['City', 'Local Time', 'Weather Condition'])\n        writer.writerows(report_data)\n\n    return filename\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, csv, collections, json\n```python\nimport pandas as pd\nimport csv\nfrom collections import defaultdict\nimport json\n\n# Constants\nFILE_PATH = '/path/to/file.csv'\n\ndef count_occurrences_of_value_in_csv_to_json(key):\n    \"\"\"\n    Count the occurrences of a value by key from a csv file and save the result as a json file.\n\n    Parameters:\n    key (str): The key to look for in the csv file.\n\n    Returns:\n    str: The filename of the generated json file.\n\n    Requirements:\n    - pandas\n    - csv\n    - collections\n    - json\n\n    Example:\n    >>> count_occurrences_of_value_in_csv_to_json('name')\n    \"\"\"\n    df = pd.read_csv(FILE_PATH)\n    count_dict = defaultdict(int)\n\n    if key in df.columns:\n        count_dict.update(df[key].value_counts())\n\n    filename = f\"count_{key}.json\"\n\n    with open(filename, 'w') as file:\n        json.dump(dict(count_dict), file)\n\n    return filename\n```"}
{"seed": "{\"task_id\": 45499, \"prompt\": \"def f_45499(url):\\n\\t\", \"suffix\": \"\\n\\treturn code\", \"canonical_solution\": \"with urllib.request.urlopen(url) as res:\\n\\t    code = res.read()\", \"test_start\": \"\\nimport urllib \\n\\ndef check(candidate):\", \"test\": [\"\\n    assert 'Wikipedia' in str(candidate('https://www.wikipedia.org'))\\n\", \"\\n    assert 'Google' in str(candidate('https://www.google.com'))\\n\"], \"entry_point\": \"f_45499\", \"intent\": \"\\u0421\\u043f\\u0430\\u0440\\u0441\\u0438\\u0442\\u044c HTML-\\u043a\\u043e\\u0434 \\u0441\\u0442\\u0440\\u0430\\u043d\\u0438\\u0446\\u044b `code` \\u043f\\u043e \\u0441\\u0441\\u044b\\u043b\\u043a\\u0435 `url`\", \"library\": [\"urllib\"]}", "generation": "Scenario 1: \nurllib, bs4, re, collections\n```python\nimport urllib.request\nfrom bs4 import BeautifulSoup\nimport re\nfrom collections import Counter\n\ndef count_words(url):\n    \"\"\"\n    Parse the HTML code of a webpage and count the frequency of each word in that webpage.\n    \n    Parameters:\n    url (str): The URL of the webpage.\n    \n    Returns:\n    dict: A dictionary where keys are words and values are their frequencies.\n    \n    Requirements:\n    - urllib.request\n    - bs4.BeautifulSoup\n    - re\n    - collections.Counter\n    \n    Example:\n    >>> count_words('https://www.wikipedia.org')\n    \"\"\"\n    with urllib.request.urlopen(url) as res:\n        html = res.read()\n        \n    soup = BeautifulSoup(html, 'html.parser')\n    text = soup.get_text()\n    words = re.findall(r'\\w+', text.lower())\n    word_count = Counter(words)\n    \n    return dict(word_count)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nurllib, bs4, csv\n```python\nimport urllib.request\nfrom bs4 import BeautifulSoup\nimport csv\n\ndef parse_table(url, table_id, output_file):\n    \"\"\"\n    Parse a specific table from a webpage and write the table data into a CSV file.\n    \n    Parameters:\n    url (str): The URL of the webpage.\n    table_id (str): The id of the table in the HTML.\n    output_file (str): The path of the output CSV file.\n    \n    Requirements:\n    - urllib.request\n    - bs4.BeautifulSoup\n    - csv\n    \n    Example:\n    >>> parse_table('https://www.wikipedia.org', 'table_id', 'output.csv')\n    \"\"\"\n    with urllib.request.urlopen(url) as res:\n        html = res.read()\n        \n    soup = BeautifulSoup(html, 'html.parser')\n    table = soup.find('table', {'id': table_id})\n    \n    with open(output_file, 'w', newline='') as f:\n        writer = csv.writer(f)\n        for row in table.find_all('tr'):\n            cols = row.find_all('td')\n            writer.writerow([col.text for col in cols])\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nurllib, bs4, os\n```python\nimport urllib.request\nfrom bs4 import BeautifulSoup\nimport os\n\ndef download_images(url, output_dir):\n    \"\"\"\n    Download all images from a webpage and save them into a directory.\n    \n    Parameters:\n    url (str): The URL of the webpage.\n    output_dir (str): The path of the output directory.\n    \n    Requirements:\n    - urllib.request\n    - bs4.BeautifulSoup\n    - os\n    \n    Example:\n    >>> download_images('https://www.wikipedia.org', './images')\n    \"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    with urllib.request.urlopen(url) as res:\n        html = res.read()\n        \n    soup = BeautifulSoup(html, 'html.parser')\n    img_tags = soup.find_all('img')\n    img_urls = [url + img['src'] for img in img_tags]\n    \n    for i, img_url in enumerate(img_urls):\n        with urllib.request.urlopen(img_url) as res:\n            img_data = res.read()\n            \n        with open(os.path.join(output_dir, f'image{i}.jpg'), 'wb') as f:\n            f.write(img_data)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nurllib, bs4, json\n```python\nimport urllib.request\nfrom bs4 import BeautifulSoup\nimport json\n\ndef extract_metadata(url):\n    \"\"\"\n    Extract the metadata from a webpage and return it as a JSON object.\n    \n    Parameters:\n    url (str): The URL of the webpage.\n    \n    Returns:\n    dict: The metadata of the webpage as a dictionary.\n    \n    Requirements:\n    - urllib.request\n    - bs4.BeautifulSoup\n    - json\n    \n    Example:\n    >>> extract_metadata('https://www.wikipedia.org')\n    \"\"\"\n    with urllib.request.urlopen(url) as res:\n        html = res.read()\n        \n    soup = BeautifulSoup(html, 'html.parser')\n    metadata = {}\n    for meta in soup.find_all('meta'):\n        name = meta.get('name') or meta.get('property')\n        if name:\n            metadata[name] = meta['content']\n    \n    return metadata\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nurllib, bs4, lxml, pandas\n```python\nimport urllib.request\nfrom bs4 import BeautifulSoup\nimport lxml\nimport pandas as pd\n\ndef extract_tables(url):\n    \"\"\"\n    Extract all the tables from a webpage and return them as pandas DataFrames.\n    \n    Parameters:\n    url (str): The URL of the webpage.\n    \n    Returns:\n    list: A list of pandas DataFrames, each representing a table from the webpage.\n    \n    Requirements:\n    - urllib.request\n    - bs4.BeautifulSoup\n    - lxml\n    - pandas\n    \n    Example:\n    >>> tables = extract_tables('https://www.wikipedia.org')\n    \"\"\"\n    with urllib.request.urlopen(url) as res:\n        html = res.read()\n        \n    soup = BeautifulSoup(html, 'lxml')\n    tables = soup.find_all('table')\n    \n    dfs = []\n    for table in tables:\n        df = pd.read_html(str(table))[0]\n        dfs.append(df)\n        \n    return dfs\n```\n"}
{"seed": "{\"task_id\": 211668, \"prompt\": \"def f_211668(values, filename):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"book = xlwt.Workbook()\\n\\n\\tsheet1 = book.add_sheet(\\\"sheet1\\\")\\n\\n\\t\\n\\tfor row_index, row_values in enumerate(values):\\n\\n\\t\\trow = sheet1.row(row_index)\\n\\n\\t\\tfor col_index, col in enumerate(row_values):\\n\\n\\t\\t\\tvalue = row_values[col_index]\\n\\n\\t\\t\\trow.write(col_index, value)\\n\\n\\t\\n\\tbook.save(filename)\", \"test_start\": \"\\nimport os\\nimport xlwt\\n\\ndef check(candidate):\", \"test\": [\"\\n    values = [[1,2,3,4,5], [6,7,8,9,10], [11,12,13,14,15], [16,17,18,19,20], [21,22,23,24,25]]\\n    candidate(values, 'k.xlsx')\\n    assert os.path.exists('k.xlsx')\\n\"], \"entry_point\": \"f_211668\", \"intent\": \"\\u0421\\u043e\\u0445\\u0440\\u0430\\u043d\\u0438\\u0442\\u044c \\u0441\\u043f\\u0438\\u0441\\u043e\\u043a \\u0437\\u043d\\u0430\\u0447\\u0435\\u043d\\u0438\\u0439 `values` \\u0432 excel \\u0444\\u0430\\u0439\\u043b `filename`\", \"library\": [\"os\", \"xlwt\"]}", "generation": "Scenario 1: \nxlwt, os, random, datetime\n```python\nimport xlwt\nimport os\nimport random\nfrom datetime import datetime\n\n# Constants\nCOLUMNS = ['ID', 'Name', 'Date of Birth', 'Email', 'Salary']\n\ndef write_employee_data_to_excel(values, filename):\n    \"\"\"\n    Write a list of employee data into an excel file.\n\n    Parameters:\n    values (list of dict): The list of employee data. Each dictionary contains keys matching the \n                           COLUMNS constant and their corresponding values.\n    filename (str): The name of the excel file to be created.\n\n    Returns:\n    str: The path of the created excel file.\n\n    Requirements:\n    - xlwt\n    - os\n    - random\n    - datetime\n\n    Example:\n    >>> values = [{'ID': 1, 'Name': 'John Doe', 'Date of Birth': datetime(1980, 1, 1), 'Email': 'john.doe@example.com', 'Salary': 5000},\n                  {'ID': 2, 'Name': 'Jane Doe', 'Date of Birth': datetime(1985, 1, 1), 'Email': 'jane.doe@example.com', 'Salary': 6000}]\n    >>> write_employee_data_to_excel(values, 'employees.xls')\n    \"\"\"\n    book = xlwt.Workbook()\n    sheet1 = book.add_sheet(\"employees\")\n\n    for col_index, col in enumerate(COLUMNS):\n        sheet1.write(0, col_index, col)\n\n    for row_index, row_values in enumerate(values, 1):\n        for col_index, col in enumerate(COLUMNS):\n            value = row_values[col]\n            sheet1.write(row_index, col_index, value)\n\n    book.save(filename)\n\n    return os.path.abspath(filename)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nxlwt, os, io, csv\n```python\nimport xlwt\nimport os\nimport io\nimport csv\n\ndef csv_to_excel(csv_content, filename):\n    \"\"\"\n    Convert CSV content into an Excel file.\n\n    Parameters:\n    csv_content (str): The CSV content as a string.\n    filename (str): The name of the Excel file to be created.\n\n    Returns:\n    str: The path of the created Excel file.\n\n    Requirements:\n    - xlwt\n    - os\n    - io\n    - csv\n\n    Example:\n    >>> csv_content = 'ID,Name,Age\\\\n1,John Doe,30\\\\n2,Jane Doe,28'\n    >>> csv_to_excel(csv_content, 'data.xls')\n    \"\"\"\n    book = xlwt.Workbook()\n    sheet1 = book.add_sheet(\"sheet1\")\n\n    reader = csv.reader(io.StringIO(csv_content))\n    for row_index, row in enumerate(reader):\n        for col_index, col in enumerate(row):\n            sheet1.write(row_index, col_index, col)\n\n    book.save(filename)\n\n    return os.path.abspath(filename)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nxlwt, os, pandas, json\n```python\nimport xlwt\nimport os\nimport pandas as pd\nimport json\n\ndef json_to_excel(json_str, filename):\n    \"\"\"\n    Convert JSON string into an Excel file.\n\n    Parameters:\n    json_str (str): The JSON content as a string.\n    filename (str): The name of the Excel file to be created.\n\n    Returns:\n    str: The path of the created Excel file.\n\n    Requirements:\n    - xlwt\n    - os\n    - pandas\n    - json\n\n    Example:\n    >>> json_str = '[{\"Name\": \"John\", \"Age\": 30}, {\"Name\": \"Jane\", \"Age\": 28}]'\n    >>> json_to_excel(json_str, 'data.xls')\n    \"\"\"\n    data = pd.read_json(json_str)\n    book = xlwt.Workbook()\n    sheet1 = book.add_sheet(\"sheet1\")\n\n    for col_index, col in enumerate(data.columns):\n        sheet1.write(0, col_index, col)\n\n    for row_index, row in data.iterrows():\n        for col_index, col in enumerate(data.columns):\n            sheet1.write(row_index + 1, col_index, row[col])\n\n    book.save(filename)\n\n    return os.path.abspath(filename)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nxlwt, os, requests, bs4\n```python\nimport xlwt\nimport os\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef scrape_and_save_to_excel(url, filename):\n    \"\"\"\n    Scrape a webpage and save the text content into an Excel file.\n\n    Parameters:\n    url (str): The URL of the webpage to be scraped.\n    filename (str): The name of the Excel file to be created.\n\n    Returns:\n    str: The path of the created Excel file.\n\n    Requirements:\n    - xlwt\n    - os\n    - requests\n    - bs4.BeautifulSoup\n\n    Example:\n    >>> scrape_and_save_to_excel('https://example.com', 'webpage_content.xls')\n    \"\"\"\n    response = requests.get(url)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    text = soup.get_text()\n\n    book = xlwt.Workbook()\n    sheet1 = book.add_sheet(\"webpage\")\n    sheet1.write(0, 0, text)\n\n    book.save(filename)\n\n    return os.path.abspath(filename)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nxlwt, os, csv, collections\n```python\nimport xlwt\nimport os\nimport csv\nfrom collections import OrderedDict\n\n# Constants\nFIELDS = ['ID', 'Name', 'Age']\n\ndef write_dict_to_excel(values, filename):\n    \"\"\"\n    Write a list of OrderedDict into an excel file.\n\n    Parameters:\n    values (list of OrderedDict): The list of data. Each OrderedDict contains keys matching the \n                                  FIELDS constant and their corresponding values.\n    filename (str): The name of the excel file to be created.\n\n    Returns:\n    str: The path of the created excel file.\n\n    Requirements:\n    - xlwt\n    - os\n    - csv\n    - collections.OrderedDict\n\n    Example:\n    >>> values = [OrderedDict([('ID', 1), ('Name', 'John Doe'), ('Age', 30)]), \n                  OrderedDict([('ID', 2), ('Name', 'Jane Doe'), ('Age', 28)])]\n    >>> write_dict_to_excel(values, 'persons.xls')\n    \"\"\"\n    book = xlwt.Workbook()\n    sheet1 = book.add_sheet(\"persons\")\n\n    for col_index, col in enumerate(FIELDS):\n        sheet1.write(0, col_index, col)\n\n    for row_index, row_values in enumerate(values, 1):\n        for col_index, col in enumerate(FIELDS):\n            value = row_values[col]\n            sheet1.write(row_index, col_index, value)\n\n    book.save(filename)\n\n    return os.path.abspath(filename)\n```"}
{"seed": "{\"task_id\": 163083, \"prompt\": \"def f_163083(my_list):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"list(map(lambda x: x[0]+x[1], zip(my_list[0::2], my_list[1::2])))\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(['cute', 'dog', 'sunny', 'day']) == ['cutedog', 'sunnyday']\\n\"], \"entry_point\": \"f_163083\", \"intent\": \"\\u041e\\u0431\\u044a\\u0435\\u0434\\u0438\\u043d\\u0438\\u0442\\u044c \\u043f\\u043e\\u043f\\u0430\\u0440\\u043d\\u043e \\u044d\\u043b\\u0435\\u043c\\u0435\\u043d\\u0442\\u044b \\u0441\\u043f\\u0438\\u0441\\u043a\\u0430 `my_list` \\u0432 \\u0441\\u0442\\u0440\\u043e\\u043a\\u0438\", \"library\": []}", "generation": "Scenario 1: \npandas, itertools, functools, matplotlib\n```python\nimport pandas as pd\nfrom itertools import cycle\nfrom functools import reduce\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLORS = cycle(['red', 'green', 'blue', 'yellow', 'purple'])\n\ndef plot_grouped_data(data):\n    \"\"\"\n    Given a list of lists where each sub-list contains two elements: a category and a value,\n    create a pandas DataFrame from the list, group the data by category, sum the values for each \n    category, and then plot a bar chart of the summed values with different colors for each category.\n\n    Parameters:\n    data (list): A list of lists, where each sub-list contains a category (str) and a value (int or float).\n\n    Returns:\n    DataFrame: A pandas DataFrame with the summed values for each category.\n\n    Requirements:\n    - pandas\n    - itertools\n    - functools\n    - matplotlib.pyplot\n\n    Example:\n    >>> data = [['A', 1], ['B', 2], ['A', 3], ['B', 4], ['C', 5]]\n    >>> df = plot_grouped_data(data)\n    >>> print(df)\n    \"\"\"\n    df = pd.DataFrame(data, columns=['Category', 'Value'])\n    grouped_df = df.groupby('Category').agg('sum')\n\n    ax = grouped_df.plot(kind='bar', color=[next(COLORS) for _ in range(len(grouped_df))])\n\n    plt.show()\n\n    return grouped_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nnumpy, scipy.stats, matplotlib\n```python\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef plot_distribution_with_fit(data):\n    \"\"\"\n    Given a list of numbers, plot the histogram of the data and fit a normal distribution to the data.\n\n    Parameters:\n    data (list): A list of numbers.\n\n    Returns:\n    tuple: A tuple containing the mean and standard deviation of the fitted normal distribution.\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Example:\n    >>> data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n    >>> mean, std_dev = plot_distribution_with_fit(data)\n    >>> print(f\"Mean: {mean}, Standard Deviation: {std_dev}\")\n    \"\"\"\n    mu, sigma = stats.norm.fit(data)\n    plt.hist(data, bins=25, density=True, alpha=0.6, color='g')\n\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, mu, sigma)\n    plt.plot(x, p, 'k', linewidth=2)\n    title = \"Fit results: mu = %.2f,  std = %.2f\" % (mu, sigma)\n\n    plt.title(title)\n\n    plt.show()\n\n    return mu, sigma\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nos, hashlib, binascii\n```python\nimport os\nimport hashlib\nfrom binascii import hexlify\n\ndef hash_file(path):\n    \"\"\"\n    Given a file path, compute the SHA256 hash of the file's contents.\n\n    Parameters:\n    path (str): The path of the file.\n\n    Returns:\n    str: The SHA256 hash of the file's contents.\n\n    Requirements:\n    - os\n    - hashlib\n    - binascii\n\n    Example:\n    >>> hash = hash_file('path/to/file')\n    >>> print(hash)\n    \"\"\"\n    BUF_SIZE = 65536\n    sha256 = hashlib.sha256()\n\n    with open(path, 'rb') as f:\n        while True:\n            data = f.read(BUF_SIZE)\n            if not data:\n                break\n            sha256.update(data)\n\n    return hexlify(sha256.digest()).decode()\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 4: \npandas, numpy, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_moving_average(data, window):\n    \"\"\"\n    Given a pandas Series of data and a window size, compute the moving average of the data and plot it.\n\n    Parameters:\n    data (Series): The pandas Series of data.\n    window (int): The window size for the moving average.\n\n    Returns:\n    Series: The pandas Series of the moving average.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> data = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n    >>> ma = plot_moving_average(data, 3)\n    >>> print(ma)\n    \"\"\"\n    ma = data.rolling(window=window).mean()\n\n    plt.figure(figsize=(10,7))\n\n    plt.plot(data.index, data.values, label='Original Data')\n    plt.plot(ma.index, ma.values, label='Moving Average')\n\n    plt.legend(loc='best')\n    plt.grid(True)\n\n    plt.show()\n\n    return ma\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 5: \ncsv, itertools, collections\n```python\nimport csv\nfrom itertools import islice\nfrom collections import Counter\n\ndef find_most_common_in_csv(file_path, column, n=10):\n    \"\"\"\n    Given a CSV file and a column name, find the n most common values in that column.\n\n    Parameters:\n    file_path (str): The path to the CSV file.\n    column (str): The name of the column.\n    n (int): The number of most common values to return.\n\n    Returns:\n    list: A list of tuples where each tuple contains a value and its frequency.\n\n    Requirements:\n    - csv\n    - itertools\n    - collections\n\n    Example:\n    >>> most_common = find_most_common_in_csv('data.csv', 'Name', 5)\n    >>> print(most_common)\n    \"\"\"\n    with open(file_path, 'r') as f:\n        reader = csv.DictReader(f)\n        counter = Counter(row[column] for row in reader)\n\n    return counter.most_common(n)\n```"}
{"seed": "{\"task_id\": 62533, \"prompt\": \"def f_62533(s):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"s.encode()\\n\\t\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(\\\"hello python\\\") == b\\\"hello python\\\"\\n\", \"\\n    assert candidate(\\\"?!?\\\") == b\\\"?!?\\\"\\n\"], \"entry_point\": \"f_62533\", \"intent\": \"\\u041f\\u0435\\u0440\\u0435\\u0432\\u0435\\u0442\\u0438 \\u0441\\u0442\\u0440\\u043e\\u043a\\u0430 `s` \\u0432 \\u0431\\u0430\\u0439\\u0442-\\u0441\\u0442\\u0440\\u043e\\u043a\\u0443\", \"library\": []}", "generation": "Scenario 1: \nhashlib, base64, binascii\n```python\nimport hashlib\nimport base64\nimport binascii\n\ndef hash_and_encode(s):\n    \"\"\"\n    Generate a SHA-256 hash of the string `s` and encode it to base64.\n\n    Parameters:\n    s (str): The input string.\n\n    Returns:\n    str: The base64 encoded hash of `s`.\n\n    Requirements:\n    - hashlib\n    - base64\n    - binascii\n\n    Example:\n    >>> hash_and_encode('hello python')\n    '2CF24DBA5FB0A30E26E83B2AC5B9E29E1B161E5C1FA7425E73043362938B9824'\n    \"\"\"\n    sha256_hash = hashlib.sha256(s.encode()).hexdigest()\n    encoded_hash = base64.b64encode(binascii.a2b_hex(sha256_hash)).decode()\n\n    return encoded_hash\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nbinascii, zlib, base64\n```python\nimport binascii\nimport zlib\nimport base64\n\ndef compress_and_encode(s):\n    \"\"\"\n    Compress the string `s` using zlib and encode it to base64.\n\n    Parameters:\n    s (str): The input string.\n\n    Returns:\n    str: The base64 encoded compressed `s`.\n\n    Requirements:\n    - zlib\n    - base64\n\n    Example:\n    >>> compress_and_encode('hello python')\n    'eJzLSM3JyQcABiwCFQ=='\n    \"\"\"\n    compressed = zlib.compress(s.encode())\n    encoded = base64.b64encode(compressed).decode()\n\n    return encoded\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \ncryptography, binascii\n```python\nfrom cryptography.fernet import Fernet\nimport binascii\n\n# Constant\nKEY = Fernet.generate_key()\n\ndef encrypt_string(s):\n    \"\"\"\n    Encrypt the string `s` using Fernet symmetric encryption.\n\n    Parameters:\n    s (str): The input string.\n\n    Returns:\n    bytes: The encrypted string.\n\n    Requirements:\n    - cryptography.fernet\n    - binascii\n\n    Example:\n    >>> encrypt_string('hello python')\n    'gAAAAABgSVs9h1Ra_cfT8Wa4vyHrhQ-7iO8XOBo8Gc7wQ9yFb5t1rvWZ9v0='\n    \"\"\"\n    cipher_suite = Fernet(KEY)\n    cipher_text = cipher_suite.encrypt(s.encode())\n\n    return binascii.b2a_base64(cipher_text).strip()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nre, unicodedata, base64\n```python\nimport re\nimport unicodedata\nimport base64\n\ndef normalize_encode(s):\n    \"\"\"\n    Normalize the string `s` to the NFKD form, remove non-ascii characters, \n    and then encode it to base64.\n\n    Parameters:\n    s (str): The input string.\n\n    Returns:\n    str: The base64 encoded normalized `s`.\n\n    Requirements:\n    - re\n    - unicodedata\n    - base64\n\n    Example:\n    >>> normalize_encode('h\u00e9llo python')\n    'aGVsbG8gcHl0aG9u'\n    \"\"\"\n    normalized = unicodedata.normalize('NFKD', s).encode('ASCII', 'ignore')\n    encoded = base64.b64encode(normalized).decode()\n\n    return encoded\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nos, base64, binascii\n```python\nimport os\nimport base64\nimport binascii\n\ndef generate_key(s):\n    \"\"\"\n    Generate a 256-bit key from the string `s` using PBKDF2_HMAC and encode it to base64.\n\n    Parameters:\n    s (str): The input string.\n\n    Returns:\n    str: The base64 encoded key.\n\n    Requirements:\n    - os\n    - base64\n    - binascii\n\n    Example:\n    >>> generate_key('hello python')\n    'vOaARZzPv4yTIHKFZIFvJ4bZBZjCoA8yK6DeU3CstIk='\n    \"\"\"\n    salt = os.urandom(16)\n    key = hashlib.pbkdf2_hmac('sha256', s.encode(), salt, 100000)\n    encoded_key = base64.b64encode(key).decode()\n\n    return encoded_key\n```\n"}
{"seed": "{\"task_id\": 215531, \"prompt\": \"def f_215531(s):\\n\\t\", \"suffix\": \"\\n\\treturn my_json\", \"canonical_solution\": \"my_json = json.loads(s)\\n\\t\", \"test_start\": \"\\nimport json\\n\\ndef check(candidate):\", \"test\": [\"\\n    s = json.dumps({'a':\\\"hello python\\\", 'b': 123})\\n    assert candidate(s) == {'a':\\\"hello python\\\", 'b': 123}\\n\"], \"entry_point\": \"f_215531\", \"intent\": \"\\u041f\\u0440\\u0435\\u043e\\u0431\\u0440\\u0430\\u0437\\u043e\\u0432\\u0430\\u0442\\u044c \\u0441\\u0442\\u0440\\u043e\\u043a\\u0443 `s` \\u0432 JSON \\u043e\\u0431\\u044a\\u0435\\u043a\\u0442 `my_json`\", \"library\": [\"json\"]}", "generation": "Scenario 1:\njson, os, hashlib, base64\n```python\nimport json\nimport os\nimport hashlib\nimport base64\n\ndef encode_and_hash_json(s):\n    \"\"\"\n    Load a json object from a string, encode it into bytes, and then hash it using SHA256.\n    The output is a base64 encoded hash.\n\n    Parameters:\n    s (str): A string representation of a json object.\n\n    Returns:\n    str: A base64 encoded SHA256 hash of the encoded json object.\n\n    Requirements:\n    - json\n    - os\n    - hashlib\n    - base64\n\n    Example:\n    >>> s = json.dumps({\"a\": \"Hello, World!\", \"b\": 123})\n    >>> encode_and_hash_json(s)\n    \"\"\"\n    my_json = json.loads(s)\n    encoded_json = json.dumps(my_json).encode('utf-8')\n\n    sha256 = hashlib.sha256()\n    sha256.update(encoded_json)\n\n    hash_bytes = sha256.digest()\n    encoded_hash = base64.b64encode(hash_bytes).decode('utf-8')\n\n    return encoded_hash\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\njson, pandas, matplotlib\n```python\nimport json\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef plot_json_data(s):\n    \"\"\"\n    Load a json object from a string, convert it into a pandas DataFrame, and plot the data.\n\n    Parameters:\n    s (str): A string representation of a json object.\n\n    Returns:\n    None\n\n    Requirements:\n    - json\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> s = json.dumps({\"Temperature\": [20, 22, 24, 26, 28, 30], \"Humidity\": [30, 40, 50, 60, 70, 80]})\n    >>> plot_json_data(s)\n    \"\"\"\n    my_json = json.loads(s)\n    df = pd.DataFrame(my_json)\n    df.plot(kind='line')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\njson, os, zipfile\n```python\nimport json\nimport os\nimport zipfile\n\ndef save_json_to_zip(s, zip_file_path):\n    \"\"\"\n    Load a json object from a string, save it to a file, and add the file to a zip archive.\n\n    Parameters:\n    s (str): A string representation of a json object.\n    zip_file_path (str): The path of the zip file.\n\n    Returns:\n    str: The path of the saved json file.\n\n    Requirements:\n    - json\n    - os\n    - zipfile\n\n    Example:\n    >>> s = json.dumps({\"a\": \"Hello, World!\", \"b\": 123})\n    >>> save_json_to_zip(s, '/path/to/zipfile.zip')\n    \"\"\"\n    my_json = json.loads(s)\n    json_file_path = os.path.join(os.path.dirname(zip_file_path), 'json_file.json')\n\n    with open(json_file_path, 'w') as json_file:\n        json.dump(my_json, json_file)\n\n    with zipfile.ZipFile(zip_file_path, 'w') as zip_file:\n        zip_file.write(json_file_path)\n\n    return json_file_path\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\njson, requests, time\n```python\nimport json\nimport requests\nimport time\n\ndef post_json_and_wait_response(s, url):\n    \"\"\"\n    Load a json object from a string, send it as a post request to a given url, \n    and wait for the response.\n\n    Parameters:\n    s (str): A string representation of a json object.\n    url (str): The url to post the json object to.\n\n    Returns:\n    Response: The response object.\n\n    Requirements:\n    - json\n    - requests\n    - time\n\n    Example:\n    >>> s = json.dumps({\"a\": \"Hello, World!\", \"b\": 123})\n    >>> post_json_and_wait_response(s, 'http://httpbin.org/post')\n    \"\"\"\n    my_json = json.loads(s)\n\n    response = requests.post(url, json=my_json)\n\n    while response.status_code == 202:  # Accepted but not processed yet\n        time.sleep(1)  # Wait for 1 second\n        response = requests.get(url)\n\n    return response\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\njson, csv, collections\n```python\nimport json\nimport csv\nimport collections\n\ndef convert_json_to_csv(s, csv_file_path):\n    \"\"\"\n    Load a json object from a string, flatten the nested json object, \n    and write the flattened json object to a csv file.\n\n    Parameters:\n    s (str): A string representation of a json object.\n    csv_file_path (str): The path of the csv file.\n\n    Returns:\n    None\n\n    Requirements:\n    - json\n    - csv\n    - collections\n\n    Example:\n    >>> s = json.dumps({\"a\": {\"nested_a\": \"Hello, World!\"}, \"b\": 123})\n    >>> convert_json_to_csv(s, '/path/to/csvfile.csv')\n    \"\"\"\n    def flatten(d, parent_key='', sep='_'):\n        items = []\n        for k, v in d.items():\n            new_key = parent_key + sep + k if parent_key else k\n            if isinstance(v, collections.MutableMapping):\n                items.extend(flatten(v, new_key, sep=sep).items())\n            else:\n                items.append((new_key, v))\n        return dict(items)\n\n    my_json = json.loads(s)\n    flattened_json = flatten(my_json)\n\n    with open(csv_file_path, 'w', newline='') as csv_file:\n        writer = csv.writer(csv_file)\n        writer.writerow(flattened_json.keys())\n        writer.writerow(flattened_json.values())\n```\n"}
{"seed": "{\"task_id\": 1666, \"prompt\": \"def f_1666(s):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"hashlib.md5(s)\", \"test_start\": \"\\nimport hashlib\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(b'opera').hexdigest() == '9ed987966c06808b50f9fddc24931bd1'\\n\"], \"entry_point\": \"f_1666\", \"intent\": \"\\u0417\\u0430\\u0445\\u0435\\u0448\\u0438\\u0440\\u043e\\u0432\\u0430\\u0442\\u044c \\u0441\\u0442\\u0440\\u043e\\u043a\\u0443 `s` \\u0441 \\u0438\\u0441\\u043f\\u043e\\u043b\\u044c\\u0437\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435\\u043c \\u0430\\u043b\\u0433\\u043e\\u0440\\u0438\\u0442\\u043c\\u0430 MD5\", \"library\": [\"hashlib\"]}", "generation": "Scenario 1:\nhashlib, binascii, os, base64\n```python\nimport hashlib\nimport binascii\nimport os\nimport base64\n\ndef secure_hash_password(password):\n    \"\"\"\n    Securely hash a password using the PBKDF2-HMAC-SHA256 method, with a randomly generated salt.\n    \n    Parameters:\n    password (str): The password to hash.\n    \n    Returns:\n    str: The hashed password and salt, encoded in base64 and combined into a string.\n    \n    Requirements:\n    - hashlib\n    - binascii\n    - os\n    - base64\n    \n    Example:\n    >>> hashed_password = secure_hash_password('password123')\n    >>> print(hashed_password)\n    \"\"\"\n    salt = os.urandom(16)\n    password_hash = hashlib.pbkdf2_hmac('sha256', password.encode(), salt, 100000)\n    return base64.b64encode(salt + password_hash).decode('utf-8')\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nhashlib, time, random, string\n```python\nimport hashlib\nimport time\nimport random\nimport string\n\ndef generate_temporary_token(username, expiration_time=3600):\n    \"\"\"\n    Generate a temporary token for a user, which will be valid for a certain amount of time.\n    \n    Parameters:\n    username (str): The username of the user.\n    expiration_time (int): The amount of time (in seconds) that the token will be valid for.\n    \n    Returns:\n    str: The temporary token.\n    \n    Requirements:\n    - hashlib\n    - time\n    - random\n    - string\n    \n    Example:\n    >>> token = generate_temporary_token('user123', 600)\n    >>> print(token)\n    \"\"\"\n    current_time = str(int(time.time()))\n    random_str = ''.join(random.choices(string.ascii_letters + string.digits, k=32))\n    hash_input = username + current_time + random_str\n    hash_output = hashlib.md5(hash_input.encode()).hexdigest()\n    return hash_output + current_time\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nhashlib, binascii, urllib.parse\n```python\nimport hashlib\nimport binascii\nfrom urllib.parse import urlparse\n\ndef check_website_integrity(url, correct_hash):\n    \"\"\"\n    Check the integrity of a website by hashing its URL and comparing it to a correct hash.\n    \n    Parameters:\n    url (str): The URL of the website.\n    correct_hash (str): The correct hash of the URL.\n    \n    Returns:\n    bool: True if the hash of the URL matches the correct hash, False otherwise.\n    \n    Requirements:\n    - hashlib\n    - binascii\n    - urllib.parse\n    \n    Example:\n    >>> check_website_integrity('https://www.example.com', '5d41402abc4b2a76b9719d911017c592')\n    \"\"\"\n    parsed_url = urlparse(url)\n    url_hash = hashlib.md5(parsed_url.netloc.encode()).hexdigest()\n    return url_hash == correct_hash\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nhashlib, os, rsa, base64\n```python\nimport hashlib\nimport os\nimport rsa\nimport base64\n\ndef generate_signed_hash(file_path):\n    \"\"\"\n    Generate a signed hash of a file using RSA.\n    \n    Parameters:\n    file_path (str): The path to the file.\n    \n    Returns:\n    str: The signed hash of the file, encoded in base64.\n    \n    Requirements:\n    - hashlib\n    - os\n    - rsa\n    - base64\n    \n    Example:\n    >>> signed_hash = generate_signed_hash('example.txt')\n    >>> print(signed_hash)\n    \"\"\"\n    with open(file_path, 'rb') as f:\n        content = f.read()\n        \n    hash_output = hashlib.sha256(content).digest()\n    \n    private_key = rsa.PrivateKey.load_pkcs1(open('private.pem').read())\n    signature = rsa.sign(hash_output, private_key, 'SHA-256')\n    \n    return base64.b64encode(signature).decode('utf-8')\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nhashlib, os, hmac, secrets\n```python\nimport hashlib\nimport os\nimport hmac\nimport secrets\n\ndef generate_hmac_signature(secret, message):\n    \"\"\"\n    Generate a HMAC signature for a message using a secret key.\n    \n    Parameters:\n    secret (str): The secret key.\n    message (str): The message to sign.\n    \n    Returns:\n    str: The HMAC signature.\n    \n    Requirements:\n    - hashlib\n    - os\n    - hmac\n    - secrets\n    \n    Example:\n    >>> signature = generate_hmac_signature('secretkey', 'Hello, world!')\n    >>> print(signature)\n    \"\"\"\n    return hmac.new(secret.encode(), message.encode(), hashlib.sha256).hexdigest()\n```\n"}
{"seed": "{\"task_id\": 151750, \"prompt\": \"def f_151750(file_path):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"open(file_path, encoding='utf-8')\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    fw = open('test.txt', 'w')\\n    fw.close()\\n    \\n    f = candidate('test.txt')\\n    assert f.name == 'test.txt'\\n    assert f.encoding == 'utf-8'\\n\"], \"entry_point\": \"f_151750\", \"intent\": \"\\u041e\\u0442\\u043a\\u0440\\u044b\\u0442\\u044c \\u0444\\u0430\\u0439\\u043b `file_path` \\u0441 \\u043a\\u043e\\u0434\\u0438\\u0440\\u043e\\u0432\\u043a\\u043e\\u0439 UTF-8\", \"library\": []}", "generation": "Scenario 1: \nos, json, csv\n```python\nimport os\nimport json\nimport csv\n\ndef convert_json_to_csv(file_path):\n    \"\"\"\n    Convert a JSON file to a CSV file.\n\n    Parameters:\n    file_path (str): The path of the JSON file.\n\n    Returns:\n    str: The path of the generated CSV file.\n\n    Requirements:\n    - os\n    - json\n    - csv\n\n    Example:\n    >>> convert_json_to_csv('data.json')\n    \"\"\"\n    csv_file_path = os.path.splitext(file_path)[0] + '.csv'\n\n    with open(file_path, 'r') as json_file, open(csv_file_path, 'w', newline='') as csv_file:\n        data = json.load(json_file)\n\n        writer = csv.writer(csv_file)\n\n        # Write header\n        writer.writerow(data[0].keys())\n\n        # Write rows\n        for row in data:\n            writer.writerow(row.values())\n\n    return csv_file_path\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, shutil, zipfile\n```python\nimport os\nimport shutil\nimport zipfile\n\ndef zip_folder(folder_path, zip_path):\n    \"\"\"\n    Compress a folder as a zip file.\n\n    Parameters:\n    folder_path (str): The path of the folder to be compressed.\n    zip_path (str): The path of the zip file to be created.\n\n    Returns:\n    bool: True if the operation is successful, False otherwise.\n\n    Requirements:\n    - os\n    - shutil\n    - zipfile\n\n    Example:\n    >>> zip_folder('my_folder', 'my_folder.zip')\n    \"\"\"\n    try:\n        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n            for root, dirs, files in os.walk(folder_path):\n                for file in files:\n                    zipf.write(os.path.join(root, file), \n                               os.path.relpath(os.path.join(root, file), \n                                               os.path.join(folder_path, '..')))\n        return True\n    except Exception as e:\n        return False\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, pandas, matplotlib\n```python\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef plot_csv(file_path, column):\n    \"\"\"\n    Plot a column from a CSV file.\n\n    Parameters:\n    file_path (str): The path of the CSV file.\n    column (str): The name of the column to plot.\n\n    Returns:\n    None\n\n    Requirements:\n    - os\n    - pandas\n    - matplotlib\n\n    Example:\n    >>> plot_csv('data.csv', 'sales')\n    \"\"\"\n    if not os.path.exists(file_path):\n        raise ValueError(\"File doesn't exist\")\n\n    df = pd.read_csv(file_path)\n\n    if column not in df.columns:\n        raise ValueError(\"Column doesn't exist in the CSV file\")\n\n    df[column].plot(kind='line')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, shutil, glob\n```python\nimport os\nimport shutil\nimport glob\n\ndef move_files(source_dir, dest_dir, extension):\n    \"\"\"\n    Move all files with a specific extension from one directory to another.\n\n    Parameters:\n    source_dir (str): The source directory.\n    dest_dir (str): The destination directory.\n    extension (str): The file extension.\n\n    Returns:\n    int: The number of files moved.\n\n    Requirements:\n    - os\n    - shutil\n    - glob\n\n    Example:\n    >>> move_files('source_dir', 'dest_dir', '.txt')\n    \"\"\"\n    files = glob.glob(os.path.join(source_dir, f'*{extension}'))\n    for file in files:\n        shutil.move(file, dest_dir)\n\n    return len(files)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, re, hashlib\n```python\nimport os\nimport re\nimport hashlib\n\ndef find_duplicate_files(directory):\n    \"\"\"\n    Find duplicate files in a directory based on content.\n\n    Parameters:\n    directory (str): The directory to search.\n\n    Returns:\n    list: A list of lists where each list contains the paths of duplicate files.\n\n    Requirements:\n    - os\n    - re\n    - hashlib\n\n    Example:\n    >>> find_duplicate_files('my_directory')\n    \"\"\"\n    def hash_file(file_path):\n        hasher = hashlib.sha256()\n        with open(file_path, 'rb') as file:\n            hasher.update(file.read())\n        return hasher.hexdigest()\n\n    files_hash = {}\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            file_path = os.path.join(root, file)\n            file_hash = hash_file(file_path)\n\n            if file_hash not in files_hash:\n                files_hash[file_hash] = [file_path]\n            else:\n                files_hash[file_hash].append(file_path)\n\n    duplicates = [files for files in files_hash.values() if len(files) > 1]\n\n    return duplicates\n```"}
{"seed": "{\"task_id\": 271764, \"prompt\": \"def f_271764(strings):\\n\\t\", \"suffix\": \"\\n\\treturn strings\", \"canonical_solution\": \"strings.sort()\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([\\\"afsdv\\\", \\\"csdfgrtyw356\\\", \\\"&*%&(%gs\\\"]) == ['&*%&(%gs', 'afsdv', 'csdfgrtyw356']\\n\"], \"entry_point\": \"f_271764\", \"intent\": \"\\u041e\\u0442\\u0441\\u043e\\u0440\\u0442\\u0438\\u0440\\u043e\\u0432\\u0430\\u0442\\u044c \\u0441\\u043f\\u0438\\u0441\\u043e\\u043a \\u0441\\u0442\\u0440\\u043e\\u043a `strings` \\u043f\\u043e \\u0430\\u043b\\u0444\\u0430\\u0432\\u0438\\u0442\\u0443 \\u0438 \\u0434\\u043b\\u0438\\u043d\\u0435\", \"library\": []}", "generation": "Scenario 3: \npandas, numpy, random, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import choice\nimport matplotlib.pyplot as plt\n\n# Constants\nCOUNTRIES = ['USA', 'UK', 'China', 'India', 'Brazil']\nPRODUCTS = ['Shoes', 'Clothes', 'Electronics', 'Groceries', 'Books']\n\ndef generate_sales_report():\n    \"\"\"\n    Generate a sales report of different products sold in different countries \n    and plot a bar graph of total sales per country.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with sales report.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> report = generate_sales_report()\n    >>> print(report)\n    >>> report.groupby('Country')['Sales'].sum().plot(kind='bar')\n    \"\"\"\n    report_data = []\n\n    for _ in range(1000):\n        country = choice(COUNTRIES)\n        product = choice(PRODUCTS)\n        sales = np.random.randint(1, 100)\n        report_data.append([country, product, sales])\n\n    report_df = pd.DataFrame(report_data, columns=['Country', 'Product', 'Sales'])\n\n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nre, collections, string\n```python\nimport re\nfrom collections import Counter\nimport string\n\nPUNCTUATION = string.punctuation\n\ndef count_word_frequency(text):\n    \"\"\"\n    Count the frequency of each word in a given text, ignoring case and punctuation.\n    \n    Parameters:\n    text (str): The input text.\n    \n    Returns:\n    dict: A dictionary with each word and its frequency.\n    \n    Requirements:\n    - re\n    - collections\n    \n    Example:\n    >>> count_word_frequency(\"Hello, World! This is a test. Hello again.\")\n    \"\"\"\n    text = text.lower()\n    text = re.sub(f'[{PUNCTUATION}]', '', text)\n    words = text.split()\n    word_counts = Counter(words)\n\n    return dict(word_counts)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, pandas, matplotlib\n```python\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef plot_file_sizes(directory):\n    \"\"\"\n    Scan a directory and plot a histogram of file sizes.\n    \n    Parameters:\n    directory (str): The directory to scan.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - os\n    - pandas\n    - matplotlib.pyplot\n    \n    Example:\n    >>> plot_file_sizes(\"/home/user/Documents\")\n    \"\"\"\n    file_sizes = []\n\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            file_path = os.path.join(root, file)\n            file_size = os.path.getsize(file_path)\n            file_sizes.append(file_size)\n\n    df = pd.DataFrame(file_sizes, columns=['File Size'])\n    df.plot(kind='hist', rwidth=0.8)\n```\n`GPT_ODEX_BREAK`\n\nScenario 6:\npandas, numpy, random, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import choice\nimport matplotlib.pyplot as plt\n\n# Constants\nAGE_GROUPS = ['18-24', '25-34', '35-44', '45-54', '55-64', '65+']\nACTIVITIES = ['Running', 'Swimming', 'Cycling', 'Walking', 'Gym Workout']\n\ndef generate_activity_report():\n    \"\"\"\n    Generate a report of different physical activities performed by different age groups \n    and plot a pie chart of total activities per age group.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with activity report.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> report = generate_activity_report()\n    >>> print(report)\n    >>> report['Age Group'].value_counts().plot(kind='pie')\n    \"\"\"\n    report_data = []\n\n    for _ in range(1000):\n        age_group = choice(AGE_GROUPS)\n        activity = choice(ACTIVITIES)\n        duration = np.random.randint(10, 120)\n        report_data.append([age_group, activity, duration])\n\n    report_df = pd.DataFrame(report_data, columns=['Age Group', 'Activity', 'Duration (Minutes)'])\n\n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 7:\nre, collections, string\n```python\nimport re\nfrom collections import Counter\nimport string\n\nPUNCTUATION = string.punctuation\n\ndef count_characters(text):\n    \"\"\"\n    Count the frequency of each character in a given text, ignoring case and punctuation.\n    \n    Parameters:\n    text (str): The input text.\n    \n    Returns:\n    dict: A dictionary with each character and its frequency.\n    \n    Requirements:\n    - re\n    - collections\n    \n    Example:\n    >>> count_characters(\"Hello, World!\")\n    \"\"\"\n    text = text.lower()\n    text = re.sub(f'[{PUNCTUATION}]', '', text)\n    characters = list(text.replace(\" \", \"\"))\n    character_counts = Counter(characters)\n\n    return dict(character_counts)\n```"}
{"seed": "{\"task_id\": 306224, \"prompt\": \"def f_306224(file_path1, file_path2):\\n\\t\", \"suffix\": \"\\n\\treturn diff\", \"canonical_solution\": \"\\n\\tfrom difflib import ndiff\\n\\t\\n\\tt1 = open(file_path1).read().splitlines(1)\\n\\tt2 = open(file_path2).read().splitlines(1)\\n\\t\\n\\tdiff = ''.join(ndiff(t1, t2))\\n\", \"test_start\": \"\\nfrom difflib import ndiff\\n\\ndef check(candidate):\", \"test\": [\"\\n    with open('test1.txt', 'w') as f:\\n        f.write('abc\\\\ndef\\\\nhij\\\\n')\\n    with open('test2.txt', 'w') as f:\\n        f.write('abc\\\\ntyh\\\\nkil\\\\nhij\\\\n')\\n    res = candidate('test1.txt', 'test2.txt')\\n    assert '+ tyh' in res\\n    assert '- def' in res\\n\", \"\\n    with open('test1.txt', 'w') as f:\\n        f.write('abc\\\\ndef\\\\nhij\\\\n')\\n    res = candidate('test1.txt', 'test1.txt')\\n    assert not any([(ch in res) for ch in ['-','?','+']])\\n\"], \"entry_point\": \"f_306224\", \"intent\": \"\\u0421\\u0440\\u0430\\u0432\\u043d\\u0438\\u0442\\u044c \\u0444\\u0430\\u0439\\u043b\\u044b `file_path1` \\u0438 `file_path2` \\u043f\\u043e\\u0441\\u0442\\u0440\\u043e\\u0447\\u043d\\u043e\", \"library\": [\"difflib\"]}", "generation": "Scenario 1:\npandas, difflib, itertools, os, csv\n```python\nimport pandas as pd\nfrom difflib import ndiff\nimport itertools\nimport os\nimport csv\n\n# Constants\nDELIMITER = ','\nQUOTECHAR = '\"'\n\ndef compare_csv_files(file_path1, file_path2):\n    \"\"\"\n    Compare two CSV files and generate a difference report.\n\n    Parameters:\n    file_path1 (str): The file path of the first CSV file.\n    file_path2 (str): The file path of the second CSV file.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the differences.\n\n    Requirements:\n    - pandas\n    - difflib\n    - itertools\n    - os\n    - csv\n\n    Example:\n    >>> compare_csv_files('file1.csv', 'file2.csv')\n    \"\"\"\n    with open(file_path1, 'r') as file1, open(file_path2, 'r') as file2:\n        reader1 = csv.reader(file1, delimiter=DELIMITER, quotechar=QUOTECHAR)\n        reader2 = csv.reader(file2, delimiter=DELIMITER, quotechar=QUOTECHAR)\n        diff = list(ndiff(reader1, reader2))\n\n    headers = ['Line Number', 'Status', 'Content']\n    data = []\n\n    for line in diff:\n        status, content = line[0], ''.join(list(itertools.islice(line, 2, None)))\n        data.append([diff.index(line) + 1, status, content])\n\n    df = pd.DataFrame(data, columns=headers)\n    \n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ndifflib, os, io, codecs, re\n```python\nimport difflib\nimport os\nimport io\nimport codecs\nimport re\n\n# Constants\nENCODING = 'utf-8'\nSPECIAL_CHARACTERS = '[^A-Za-z0-9]+'\n\ndef compare_files_ignore_specials(file_path1, file_path2):\n    \"\"\"\n    Compare two text files ignoring special characters.\n\n    Parameters:\n    file_path1 (str): The file path of the first text file.\n    file_path2 (str): The file path of the second text file.\n\n    Returns:\n    str: Differences between the two files.\n\n    Requirements:\n    - difflib\n    - os\n    - io\n    - codecs\n    - re\n\n    Example:\n    >>> compare_files_ignore_specials('file1.txt', 'file2.txt')\n    \"\"\"\n    with codecs.open(file_path1, 'r', ENCODING) as file1, codecs.open(file_path2, 'r', ENCODING) as file2:\n        file1_content = re.sub(SPECIAL_CHARACTERS, '', file1.read())\n        file2_content = re.sub(SPECIAL_CHARACTERS, '', file2.read())\n        diff = difflib.ndiff(file1_content, file2_content)\n\n    return '\\n'.join(diff)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ndifflib, os, hashlib, io\n```python\nimport difflib\nimport os\nimport hashlib\nimport io\n\ndef compare_files_hash(file_path1, file_path2):\n    \"\"\"\n    Compare two files based on their MD5 hash values.\n\n    Parameters:\n    file_path1 (str): The file path of the first file.\n    file_path2 (str): The file path of the second file.\n\n    Returns:\n    bool: True if the files are identical, False otherwise.\n\n    Requirements:\n    - difflib\n    - os\n    - hashlib\n    - io\n\n    Example:\n    >>> compare_files_hash('file1.txt', 'file2.txt')\n    \"\"\"\n    with io.open(file_path1, 'rb') as file1, io.open(file_path2, 'rb') as file2:\n        file1_hash = hashlib.md5(file1.read()).hexdigest()\n        file2_hash = hashlib.md5(file2.read()).hexdigest()\n\n    return file1_hash == file2_hash\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ndifflib, os, io, chardet\n```python\nimport difflib\nimport os\nimport io\nimport chardet\n\ndef compare_files_encoding(file_path1, file_path2):\n    \"\"\"\n    Compare two files based on their encoding.\n\n    Parameters:\n    file_path1 (str): The file path of the first file.\n    file_path2 (str): The file path of the second file.\n\n    Returns:\n    bool: True if the files have the same encoding, False otherwise.\n\n    Requirements:\n    - difflib\n    - os\n    - io\n    - chardet\n\n    Example:\n    >>> compare_files_encoding('file1.txt', 'file2.txt')\n    \"\"\"\n    with io.open(file_path1, 'rb') as file1, io.open(file_path2, 'rb') as file2:\n        file1_encoding = chardet.detect(file1.read())['encoding']\n        file2_encoding = chardet.detect(file2.read())['encoding']\n\n    return file1_encoding == file2_encoding\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\ndifflib, os, io, gzip\n```python\nimport difflib\nimport os\nimport io\nimport gzip\n\ndef compare_files_gzip(file_path1, file_path2):\n    \"\"\"\n    Compare two gzip files.\n\n    Parameters:\n    file_path1 (str): The file path of the first gzip file.\n    file_path2 (str): The file path of the second gzip file.\n\n    Returns:\n    str: Differences between the two files.\n\n    Requirements:\n    - difflib\n    - os\n    - io\n    - gzip\n\n    Example:\n    >>> compare_files_gzip('file1.gz', 'file2.gz')\n    \"\"\"\n    with gzip.open(file_path1, 'rt') as file1, gzip.open(file_path2, 'rt') as file2:\n        file1_content = file1.read()\n        file2_content = file2.read()\n        diff = difflib.ndiff(file1_content, file2_content)\n\n    return '\\n'.join(diff)\n```"}
{"seed": "{\"task_id\": 157647, \"prompt\": \"def f_157647(s, old_sub, new_sub, count):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"s.replace(old_sub, new_sub, count)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(\\\"this string contains some old words old old old\\\", 'old', 'new', 3) ==         'this string contains some new words new new old'\\n\"], \"entry_point\": \"f_157647\", \"intent\": \"\\u0417\\u0430\\u043c\\u0435\\u043d\\u0438\\u0442\\u044c \\u043f\\u043e\\u0434\\u0441\\u0442\\u0440\\u043e\\u043a\\u0443 `old_sub` \\u043d\\u0430 \\u043f\\u043e\\u0434\\u0441\\u0442\\u043e\\u043a\\u0443 `new_sub` \\u0432 \\u0441\\u0442\\u0440\\u043e\\u043a\\u0435 `s` `count` \\u0440\\u0430\\u0437\", \"library\": []}", "generation": "Scenario 1:\npandas, re, os\n```python\nimport pandas as pd\nimport re\nimport os\n\n# Constants\nCSV_FILES = ['data1.csv', 'data2.csv', 'data3.csv']\n\ndef replace_string_in_csv_files(old_sub, new_sub, count):\n    \"\"\"\n    Replace the substring `old_sub` with `new_sub` in all .csv files in the current directory.\n\n    Parameters:\n    old_sub (str): The old substring.\n    new_sub (str): The new substring.\n    count (int): The number of replacements to be made.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - re\n    - os\n\n    Example:\n    >>> replace_string_in_csv_files('old_value', 'new_value', 3)\n    \"\"\"\n\n    for filename in os.listdir('.'):\n        if filename.endswith('.csv'):\n            df = pd.read_csv(filename)\n            df = df.applymap(lambda x: re.sub(old_sub, new_sub, str(x), count) if isinstance(x, str) else x)\n            df.to_csv(filename, index=False)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nxml, re\n```python\nimport xml.etree.ElementTree as ET\nimport re\n\ndef replace_string_in_xml(old_sub, new_sub, count, xml_file):\n    \"\"\"\n    Replace the substring `old_sub` with `new_sub` in an XML file.\n\n    Parameters:\n    old_sub (str): The old substring.\n    new_sub (str): The new substring.\n    count (int): The number of replacements to be made.\n    xml_file (str): The XML file path.\n\n    Returns:\n    str: The updated XML as a string.\n\n    Requirements:\n    - xml.etree.ElementTree\n    - re\n\n    Example:\n    >>> xml_string = replace_string_in_xml('old_value', 'new_value', 3, 'data.xml')\n    >>> print(xml_string)\n    \"\"\"\n\n    tree = ET.parse(xml_file)\n    root = tree.getroot()\n\n    for elem in root.iter():\n        if elem.text:\n            elem.text = re.sub(old_sub, new_sub, elem.text, count)\n\n    return ET.tostring(root).decode()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\njson, re\n```python\nimport json\nimport re\n\ndef replace_string_in_json(old_sub, new_sub, count, json_file):\n    \"\"\"\n    Replace the substring `old_sub` with `new_sub` in a JSON file.\n\n    Parameters:\n    old_sub (str): The old substring.\n    new_sub (str): The new substring.\n    count (int): The number of replacements to be made.\n    json_file (str): The JSON file path.\n\n    Returns:\n    dict: The updated JSON as a dictionary.\n\n    Requirements:\n    - json\n    - re\n\n    Example:\n    >>> json_dict = replace_string_in_json('old_value', 'new_value', 3, 'data.json')\n    >>> print(json_dict)\n    \"\"\"\n\n    with open(json_file, 'r') as file:\n        data = json.load(file)\n\n    data = json.loads(re.sub(old_sub, new_sub, json.dumps(data), count))\n\n    return data\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, re\n```python\nimport numpy as np\nimport re\n\n# Constants\nDATA = np.array(['old_value1', 'old_value2', 'old_value3'])\n\ndef replace_string_in_numpy_array(old_sub, new_sub, count):\n    \"\"\"\n    Replace the substring `old_sub` with `new_sub` in a numpy array.\n\n    Parameters:\n    old_sub (str): The old substring.\n    new_sub (str): The new substring.\n    count (int): The number of replacements to be made.\n\n    Returns:\n    np.array: The updated numpy array.\n\n    Requirements:\n    - numpy\n    - re\n\n    Example:\n    >>> updated_array = replace_string_in_numpy_array('old_value', 'new_value', 3)\n    >>> print(updated_array)\n    \"\"\"\n\n    vectorized_replace = np.vectorize(lambda x: re.sub(old_sub, new_sub, x, count))\n    updated_array = vectorized_replace(DATA)\n\n    return updated_array\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, re\n```python\nimport pandas as pd\nimport re\n\n# Constants\nDATA = pd.DataFrame({'Column1': ['old_value1', 'old_value2', 'old_value3'],\n                     'Column2': ['old_value4', 'old_value5', 'old_value6']})\n\ndef replace_string_in_dataframe(old_sub, new_sub, count):\n    \"\"\"\n    Replace the substring `old_sub` with `new_sub` in a pandas DataFrame.\n\n    Parameters:\n    old_sub (str): The old substring.\n    new_sub (str): The new substring.\n    count (int): The number of replacements to be made.\n\n    Returns:\n    pd.DataFrame: The updated pandas DataFrame.\n\n    Requirements:\n    - pandas\n    - re\n\n    Example:\n    >>> updated_df = replace_string_in_dataframe('old_value', 'new_value', 3)\n    >>> print(updated_df)\n    \"\"\"\n\n    updated_df = DATA.applymap(lambda x: re.sub(old_sub, new_sub, str(x), count))\n\n    return updated_df\n```"}
{"seed": "{\"task_id\": 303166, \"prompt\": \"def f_303166(s, sep):\\n\\t\", \"suffix\": \"\\n\\treturn my_dict\", \"canonical_solution\": \"my_dict = dict([[x[1:-1] for x in s.split(sep)], ])\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    s = \\\"hello, world!\\\"\\n    assert candidate(s, ',') == {'ell': 'world'}\\n\", \"\\n    s = \\\"hello, world!\\\"\\n    assert candidate(s, '!') == {'ello, worl': ''}\\n\"], \"entry_point\": \"f_303166\", \"intent\": \"\\u0420\\u0430\\u0441\\u043f\\u0430\\u0440\\u0441\\u0438\\u0442\\u044c \\u0441\\u0442\\u0440\\u043e\\u043a\\u0443 \\u043a\\u043b\\u044e\\u0447-\\u0437\\u043d\\u0430\\u0447\\u0435\\u043d\\u0438\\u0435 `s` \\u043f\\u043e \\u0440\\u0430\\u0437\\u0434\\u0435\\u043b\\u0438\\u0442\\u0435\\u043b\\u044e `sep` \\u0432 \\u0441\\u043b\\u043e\\u0432\\u0430\\u0440\\u044c `my_dict`\", \"library\": []}", "generation": "Scenario 1:\ncollections, re, string\n```python\nfrom collections import Counter\nimport re\nimport string\n\ndef count_word_frequency(text, ignore_punctuation=True, ignore_case=True):\n    \"\"\"\n    Count the frequency of each word in a text string, optionally ignoring punctuation and case.\n\n    Parameters:\n    text (str): The text string.\n    ignore_punctuation (bool): Whether to ignore punctuation. Default is True.\n    ignore_case (bool): Whether to ignore case. Default is True.\n\n    Returns:\n    dict: A dictionary with each unique word as the key and its frequency as the value.\n\n    Requirements:\n    - collections\n    - re\n    - string\n\n    Example:\n    >>> count_word_frequency(\"Hello, world! This is a test. Hello again.\")\n    >>> {'hello': 2, 'world': 1, 'this': 1, 'is': 1, 'a': 1, 'test': 1, 'again': 1}\n    \"\"\"\n    if ignore_punctuation:\n        text = re.sub('['+string.punctuation+']', '', text)\n    if ignore_case:\n        text = text.lower()\n\n    word_count = Counter(text.split())\n\n    return dict(word_count)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nrandom, string, hashlib\n```python\nimport random\nimport string\nimport hashlib\n\ndef generate_random_hash(length):\n    \"\"\"\n    Generate a random hash of a specified length.\n\n    Parameters:\n    length (int): The length of the hash.\n\n    Returns:\n    str: The generated hash.\n\n    Requirements:\n    - random\n    - string\n    - hashlib\n\n    Example:\n    >>> generate_random_hash(10)\n    \"\"\"\n    random_string = ''.join(random.choices(string.ascii_letters + string.digits, k=length))\n    return hashlib.sha256(random_string.encode()).hexdigest()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, glob, shutil\n```python\nimport os\nimport glob\nimport shutil\n\ndef move_files_with_extension(source_dir, target_dir, extension):\n    \"\"\"\n    Move all files with a specified extension from a source directory to a target directory.\n\n    Parameters:\n    source_dir (str): The source directory.\n    target_dir (str): The target directory.\n    extension (str): The file extension.\n\n    Returns:\n    list: The list of moved files.\n\n    Requirements:\n    - os\n    - glob\n    - shutil\n\n    Example:\n    >>> move_files_with_extension(\"./source\", \"./target\", \".txt\")\n    \"\"\"\n    files = glob.glob(os.path.join(source_dir, \"*.\"+extension))\n    for file in files:\n        shutil.move(file, target_dir)\n\n    return files\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, scipy.stats\n```python\nimport numpy as np\nfrom scipy import stats\n\ndef calculate_statistics(data):\n    \"\"\"\n    Calculate mean, median, mode, and standard deviation of a list of numbers.\n\n    Parameters:\n    data (list): The list of numbers.\n\n    Returns:\n    tuple: A tuple (mean, median, mode, std_dev).\n\n    Requirements:\n    - numpy\n    - scipy.stats\n\n    Example:\n    >>> calculate_statistics([1, 2, 3, 4, 5])\n    >>> (3.0, 3.0, 1.0, 1.4142135623730951)\n    \"\"\"\n    mean = np.mean(data)\n    median = np.median(data)\n    mode = stats.mode(data)[0][0]\n    std_dev = np.std(data)\n\n    return mean, median, mode, std_dev\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, csv, collections\n```python\nimport os\nimport csv\nfrom collections import Counter\n\ndef count_column_frequency(file_path, column):\n    \"\"\"\n    Count the frequency of each value in a specified column of a CSV file.\n\n    Parameters:\n    file_path (str): The path to the CSV file.\n    column (str): The column to count the frequency.\n\n    Returns:\n    dict: A dictionary with each unique value as the key and its frequency as the value.\n\n    Requirements:\n    - os\n    - csv\n    - collections\n\n    Example:\n    >>> count_column_frequency(\"./data.csv\", \"Name\")\n    \"\"\"\n    with open(file_path, 'r') as csv_file:\n        reader = csv.DictReader(csv_file)\n        column_values = [row[column] for row in reader]\n\n    frequency = dict(Counter(column_values))\n\n    return frequency\n```\n"}
{"seed": "{\"task_id\": 101677, \"prompt\": \"def f_101677(num):\\n\\t\", \"suffix\": \"\\n\\treturn num\", \"canonical_solution\": \"\\n\\tnum = str(num)\\n\\tif \\\".\\\" in num:\\n\\t\\tinteger, tail = num.split(\\\".\\\")\\n\\t\\ttail = re.sub(r'[0]*$', '', tail)\\n\\t\\tif not tail:\\n\\t\\t\\tnum = integer\\n\\t\\telse:\\n\\t\\t\\tnum = integer + \\\".\\\" + tail\\n\", \"test_start\": \"\\nimport re \\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(\\\"123.45\\\") == \\\"123.45\\\"\\n\", \"\\n    assert candidate(\\\"123.00\\\") == \\\"123\\\"\\n\", \"\\n    assert candidate(\\\"1.500\\\") == \\\"1.5\\\"\\n\"], \"entry_point\": \"f_101677\", \"intent\": \"\\u041e\\u0442\\u0431\\u0440\\u043e\\u0441\\u0438\\u0442\\u044c \\u043d\\u0435\\u0437\\u043d\\u0430\\u0447\\u0430\\u0449\\u0438\\u0435 \\u043d\\u0443\\u043b\\u0438 \\u0441\\u043f\\u0440\\u0430\\u0432\\u0430 `num`\", \"library\": [\"re\"]}", "generation": "Scenario 1: \nnumpy, matplotlib, random, math\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\nimport math\n\n# Constants\nCIRCLE_RADIUS = 5\nNUM_POINTS = 100\n\ndef plot_random_points_in_circle():\n    \"\"\"\n    Plot NUM_POINTS random points within a circle of radius CIRCLE_RADIUS.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - random\n    - math\n\n    Example:\n    >>> plot_random_points_in_circle()\n    \"\"\"\n    # Create plot\n    fig, ax = plt.subplots()\n\n    # Plot circle\n    circle = plt.Circle((0, 0), CIRCLE_RADIUS, color='b', fill=False)\n    ax.add_artist(circle)\n\n    # Generate and plot points\n    for _ in range(NUM_POINTS):\n        r = CIRCLE_RADIUS * np.sqrt(np.random.rand())\n        theta = np.random.rand() * 2 * math.pi\n        x = r * math.cos(theta)\n        y = r * math.sin(theta)\n        ax.plot(x, y, 'ro')\n\n    plt.xlim(-CIRCLE_RADIUS-1, CIRCLE_RADIUS+1)\n    plt.ylim(-CIRCLE_RADIUS-1, CIRCLE_RADIUS+1)\n    plt.gca().set_aspect('equal', adjustable='box')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, random, datetime, matplotlib\n```python\nimport pandas as pd\nfrom random import randint\nfrom datetime import datetime, timedelta\nimport matplotlib.pyplot as plt\n\n# Constants\nSTART_DATE = datetime(2022, 1, 1)\nEND_DATE = datetime(2022, 12, 31)\nNUM_SAMPLES = 100\nTEMP_RANGE = (-10, 40)\n\ndef generate_and_plot_temperature_data():\n    \"\"\"\n    Generate NUM_SAMPLES random temperature data points between TEMP_RANGE for the year 2022,\n    and plot them as a time series.\n\n    Requirements:\n    - pandas\n    - random\n    - datetime\n    - matplotlib.pyplot\n\n    Example:\n    >>> generate_and_plot_temperature_data()\n    \"\"\"\n    # Generate random dates\n    dates = pd.date_range(START_DATE, END_DATE, periods=NUM_SAMPLES)\n\n    # Generate random temperatures\n    temps = [randint(*TEMP_RANGE) for _ in range(NUM_SAMPLES)]\n\n    # Create DataFrame\n    df = pd.DataFrame({'Date': dates, 'Temperature': temps})\n\n    # Plot data\n    plt.plot(df['Date'], df['Temperature'])\n    plt.xlabel('Date')\n    plt.ylabel('Temperature (\u00b0C)')\n    plt.title('Temperature Data for 2022')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, matplotlib, math, datetime\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\nfrom datetime import datetime, timedelta\n\n# Constants\nTIME_PERIOD = 24  # In hours\nNUM_SAMPLES = 100\nAMPLITUDE = 10\n\ndef plot_cosine_wave():\n    \"\"\"\n    Plot a cosine wave with a period of TIME_PERIOD hours for the next 7 days,\n    with an amplitude of AMPLITUDE.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - math\n    - datetime\n\n    Example:\n    >>> plot_cosine_wave()\n    \"\"\"\n    # Generate timestamps for the next 7 days, every 15 minutes\n    start_time = datetime.now()\n    timestamps = [start_time + timedelta(minutes=15*i) for i in range(NUM_SAMPLES*4*7)]\n\n    # Calculate the corresponding y values\n    y = [AMPLITUDE * math.cos(2*math.pi*(t.hour/TIME_PERIOD)) for t in timestamps]\n\n    # Plot the data\n    plt.plot(timestamps, y)\n    plt.xlabel('Time')\n    plt.ylabel('Amplitude')\n    plt.title('Cosine Wave')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, random, datetime, matplotlib\n```python\nimport pandas as pd\nfrom random import randint\nfrom datetime import datetime, timedelta\nimport matplotlib.pyplot as plt\n\n# Constants\nNUM_DAYS = 30\nNUM_SALES = 100\nPRODUCTS = ['Product A', 'Product B', 'Product C', 'Product D', 'Product E']\n\ndef generate_and_plot_sales_data():\n    \"\"\"\n    Generate NUM_SALES random sales data points for a list of PRODUCTS over the past NUM_DAYS,\n    and plot them as a time series.\n\n    Requirements:\n    - pandas\n    - random\n    - datetime\n    - matplotlib.pyplot\n\n    Example:\n    >>> generate_and_plot_sales_data()\n    \"\"\"\n    # Generate random dates\n    end_date = datetime.now()\n    start_date = end_date - timedelta(days=NUM_DAYS)\n    dates = [start_date + (end_date - start_date) * randint(0, NUM_SALES) / NUM_SALES for _ in range(NUM_SALES)]\n\n    # Generate random products\n    products = [PRODUCTS[randint(0, len(PRODUCTS)-1)] for _ in range(NUM_SALES)]\n\n    # Create DataFrame\n    df = pd.DataFrame({'Date': dates, 'Product': products})\n\n    # Count the sales for each product\n    df = df.groupby(['Date', 'Product']).size().unstack().fillna(0)\n\n    # Plot data\n    df.plot(kind='line', stacked=False)\n    plt.xlabel('Date')\n    plt.ylabel('Sales')\n    plt.title('Sales Data for the Past Month')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, matplotlib, random, math\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\nimport math\n\n# Constants\nNUM_POINTS = 1000\nX_RANGE = (-10, 10)\nY_RANGE = (-10, 10)\n\ndef plot_random_points_and_sin_wave():\n    \"\"\"\n    Plot NUM_POINTS random points within X_RANGE and Y_RANGE, and plot the sin wave.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - random\n    - math\n\n    Example:\n    >>> plot_random_points_and_sin_wave()\n    \"\"\"\n    # Generate and plot points\n    x_points = [X_RANGE[0] + (X_RANGE[1] - X_RANGE[0]) * np.random.rand() for _ in range(NUM_POINTS)]\n    y_points = [Y_RANGE[0] + (Y_RANGE[1] - Y_RANGE[0]) * np.random.rand() for _ in range(NUM_POINTS)]\n    plt.scatter(x_points, y_points, color='b')\n\n    # Generate and plot sin wave\n    x_wave = np.linspace(X_RANGE[0], X_RANGE[1], NUM_POINTS)\n    y_wave = np.sin(x_wave)\n    plt.plot(x_wave, y_wave, color='r')\n\n    plt.xlim(X_RANGE)\n    plt.ylim(Y_RANGE)\n    plt.show()\n```"}
{"seed": "{\"task_id\": 74955, \"prompt\": \"def f_74955(s):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"re.sub(r'[^\\\\w\\\\s]', '', s)\", \"test_start\": \"\\nimport re\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('ergf, earg. f?') == 'ergf earg f'\\n\"], \"entry_point\": \"f_74955\", \"intent\": \"\\u0423\\u0434\\u0430\\u043b\\u0438\\u0442\\u044c \\u0437\\u043d\\u0430\\u043a\\u0438 \\u043f\\u0440\\u0435\\u043f\\u0438\\u043d\\u0430\\u043d\\u0438\\u044f \\u0438\\u0437 \\u0441\\u0442\\u0440\\u043e\\u043a\\u0438 `s`\", \"library\": [\"re\"]}", "generation": "Scenario 1:\nre, nltk, string, matplotlib\n```python\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nimport matplotlib.pyplot as plt\n\n# Constants\nSTOPWORDS = set(stopwords.words('english'))\n\ndef plot_word_frequency(text):\n    \"\"\"\n    Remove punctuation from a string, then tokenize the string, \n    remove stop words, and plot a histogram of the word frequencies.\n\n    Parameters:\n    text (str): The input string.\n\n    Requirements:\n    - re\n    - nltk\n    - string\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_word_frequency(\"Hello, world! This is a test sentence. This sentence has some repeated words.\")\n    \"\"\"\n    text = re.sub(r'[^\\w\\s]', '', text)\n    tokens = nltk.word_tokenize(text)\n    filtered = [w for w in tokens if not w in STOPWORDS]\n    freq_dist = nltk.FreqDist(filtered)\n    freq_dist.plot(30,cumulative=False)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nre, os, glob\n```python\nimport re\nimport os\nimport glob\n\ndef rename_files(directory, pattern, replacement):\n    \"\"\"\n    Remove certain patterns from file names in a directory.\n\n    Parameters:\n    directory (str): The directory path.\n    pattern (str): The pattern to be removed.\n    replacement (str): The string to replace the pattern.\n\n    Requirements:\n    - re\n    - os\n    - glob\n\n    Example:\n    >>> rename_files('/path/to/directory', '\\d+', '')\n    \"\"\"\n    for filename in glob.glob(os.path.join(directory, '*')):\n        new_filename = re.sub(pattern, replacement, filename)\n        os.rename(filename, new_filename)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nre, collections, operator\n```python\nimport re\nimport collections\nimport operator\n\ndef most_common_word(text):\n    \"\"\"\n    Find the most common word in a text after removing punctuation and converting to lowercase.\n\n    Parameters:\n    text (str): The input text.\n\n    Returns:\n    str: The most common word.\n\n    Requirements:\n    - re\n    - collections\n    - operator\n\n    Example:\n    >>> most_common_word(\"Hello, world! This is a test sentence. This sentence has some repeated words.\")\n    \"\"\"\n    text = re.sub(r'[^\\w\\s]', '', text).lower()\n    words = re.findall(r'\\w+', text)\n    most_common_word = max(collections.Counter(words).items(), key=operator.itemgetter(1))[0]\n\n    return most_common_word\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nre, pandas, numpy\n```python\nimport re\nimport pandas as pd\nimport numpy as np\n\n# Sample data\n# data = pd.DataFrame({'Text': ['Hello, world!', 'This is a test.', 'Another sentence.']})\n\ndef count_words(data, column):\n    \"\"\"\n    Count the number of words in a pandas DataFrame column after removing punctuation.\n\n    Parameters:\n    data (DataFrame): The pandas DataFrame.\n    column (str): The column name.\n\n    Returns:\n    Series: A pandas Series with the word counts.\n\n    Requirements:\n    - re\n    - pandas\n    - numpy\n\n    Example:\n    >>> data = pd.DataFrame({'Text': ['Hello, world!', 'This is a test.', 'Another sentence.']})\n    >>> count_words(data, 'Text')\n    \"\"\"\n    data[column] = data[column].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n    word_counts = data[column].apply(lambda x: len(x.split()))\n\n    return word_counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nre, sklearn.feature_extraction.text, sklearn.naive_bayes, sklearn.model_selection\n```python\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import train_test_split\n\n# Sample data\n# X = ['Hello, world!', 'This is a test.', 'Another sentence.']\n# y = [0, 1, 0]\n\ndef classify_text(X, y):\n    \"\"\"\n    Remove punctuation from a list of text strings, \n    then vectorize the text and train a Naive Bayes classifier.\n\n    Parameters:\n    X (list): The list of text strings.\n    y (list): The list of labels.\n\n    Returns:\n    MultinomialNB: The trained classifier.\n\n    Requirements:\n    - re\n    - sklearn.feature_extraction.text.CountVectorizer\n    - sklearn.naive_bayes.MultinomialNB\n    - sklearn.model_selection.train_test_split\n\n    Example:\n    >>> X = ['Hello, world!', 'This is a test.', 'Another sentence.']\n    >>> y = [0, 1, 0]\n    >>> classifier = classify_text(X, y)\n    \"\"\"\n    X = [re.sub(r'[^\\w\\s]', '', s) for s in X]\n    vectorizer = CountVectorizer()\n    X = vectorizer.fit_transform(X)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n    classifier = MultinomialNB().fit(X_train, y_train)\n\n    return classifier\n```"}
{"seed": "{\"task_id\": 74955, \"prompt\": \"def f_74955(s):\\n\\t\", \"suffix\": \"\\n\\treturn s\", \"canonical_solution\": \"for p in string.punctuation:\\n\\n\\t\\tif p in s:\\n\\n\\t\\t\\ts = s.replace(p, '')\\n\", \"test_start\": \"\\nimport string\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(\\\"hello\\\") == \\\"hello\\\"\\n\", \"\\n    assert candidate(\\\"hello, world!\\\") == \\\"hello world\\\"\\n\", \"\\n    assert candidate(\\\"# % ^ . &\\\") == \\\"    \\\"\\n\"], \"entry_point\": \"f_74955\", \"intent\": \"\\u041e\\u0442\\u0431\\u0440\\u043e\\u0441\\u0438\\u0442\\u044c \\u043d\\u0435\\u0437\\u043d\\u0430\\u0447\\u0430\\u0449\\u0438\\u0435 \\u043d\\u0443\\u043b\\u0438 \\u0441\\u043f\\u0440\\u0430\\u0432\\u0430 `num`\", \"library\": [\"string\"]}", "generation": "Scenario 3: \nstring, re, collections, matplotlib\n```python\nimport string\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\n# Constants \nPUNCTUATION = string.punctuation\n\ndef analyze_text(text):\n    \"\"\"\n    Analyze a text by removing punctuation, counting word frequency and plot the 10 most common words.\n\n    Parameters:\n    text (str): The text to be analyzed.\n\n    Returns:\n    dict: A dictionary of word frequencies.\n\n    Requirements:\n    - string\n    - re\n    - collections.Counter\n    - matplotlib.pyplot\n\n    Example:\n    >>> analyze_text('Hello, world! This is a test. Hello, again.')\n    \"\"\"\n    # Remove punctuation\n    text_no_punct = re.sub('['+PUNCTUATION+']', ' ', text).lower()\n    word_list = text_no_punct.split()\n\n    # Count word frequency\n    word_counter = Counter(word_list)\n\n    # Plot the 10 most common words\n    most_common_words = word_counter.most_common(10)\n    words, counts = zip(*most_common_words)\n\n    plt.bar(words, counts)\n    plt.show()\n\n    return word_counter\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \ncsv, io, string, pandas\n```python\nimport csv\nimport io\nimport string\nimport pandas as pd\n\n# Constants\nPUNCTUATION = string.punctuation\n\ndef csv_text_processing(csv_content):\n    \"\"\"\n    Process a CSV content by removing punctuation from each cell, \n    and return a pandas DataFrame.\n\n    Parameters:\n    csv_content (str): The content of a CSV file.\n\n    Returns:\n    DataFrame: A pandas DataFrame.\n\n    Requirements:\n    - csv\n    - io\n    - string\n    - pandas\n\n    Example:\n    >>> csv_content = 'col1,col2\\n\"Hello, world!\",\"This is a test.\"\\n\"Hello, again.\",\"Another test.\"'\n    >>> df = csv_text_processing(csv_content)\n    >>> print(df)\n    \"\"\"\n    csv_reader = csv.reader(io.StringIO(csv_content))\n    processed_rows = []\n\n    for row in csv_reader:\n        processed_row = [re.sub('['+PUNCTUATION+']', '', cell) for cell in row]\n        processed_rows.append(processed_row)\n\n    df = pd.DataFrame(processed_rows[1:], columns=processed_rows[0])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nstring, hashlib, binascii\n```python\nimport string\nimport hashlib\nimport binascii\n\n# Constants\nPUNCTUATION = string.punctuation\n\ndef hash_string(s):\n    \"\"\"\n    Remove punctuation from a string, then hash the resulting string using SHA256.\n\n    Parameters:\n    s (str): The string to be hashed.\n\n    Returns:\n    str: The hex representation of the hashed string.\n\n    Requirements:\n    - string\n    - hashlib\n    - binascii\n\n    Example:\n    >>> hash_string('Hello, world!')\n    \"\"\"\n    s_no_punct = ''.join([c for c in s if c not in PUNCTUATION])\n\n    # Hash the string\n    hashed_s = hashlib.sha256(s_no_punct.encode('utf-8')).digest()\n\n    # Convert the hash to hex\n    hashed_s_hex = binascii.hexlify(hashed_s).decode()\n\n    return hashed_s_hex\n```\n`GPT_ODEX_BREAK`\n\nScenario 6: \nstring, nltk, sklearn\n```python\nimport string\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Constants\nPUNCTUATION = string.punctuation\nSTOPWORDS = set(stopwords.words('english'))\n\ndef text_vectorization(text):\n    \"\"\"\n    Remove punctuation and stop words from a text, then vectorize the resulting text.\n\n    Parameters:\n    text (str): The text to be vectorized.\n\n    Returns:\n    array: The vectorized text.\n\n    Requirements:\n    - string\n    - nltk.corpus\n    - sklearn.feature_extraction.text\n\n    Example:\n    >>> text_vectorization('This is a test. Just a test.')\n    \"\"\"\n    # Remove punctuation and convert to lower case\n    text_no_punct = ''.join([c for c in text if c not in PUNCTUATION]).lower()\n\n    # Remove stop words\n    text_no_stopwords = ' '.join([word for word in text_no_punct.split() if word not in STOPWORDS])\n\n    # Vectorize the text\n    vectorizer = CountVectorizer().fit_transform([text_no_stopwords])\n    vector = vectorizer.toarray()\n\n    return vector\n```\n`GPT_ODEX_BREAK`\n\nScenario 7: \nstring, nltk, gensim\n```python\nimport string\nfrom nltk.corpus import stopwords\nfrom gensim import corpora, models\n\n# Constants\nPUNCTUATION = string.punctuation\nSTOPWORDS = set(stopwords.words('english'))\n\ndef text_topic_modeling(texts):\n    \"\"\"\n    Remove punctuation and stop words from a list of texts, then perform topic modeling (LDA).\n\n    Parameters:\n    texts (list): The list of texts.\n\n    Returns:\n    LdaModel: The LDA model.\n\n    Requirements:\n    - string\n    - nltk.corpus\n    - gensim\n\n    Example:\n    >>> text_topic_modeling(['This is a test.', 'Just a test.'])\n    \"\"\"\n    # Remove punctuation and stop words\n    processed_texts = []\n    for text in texts:\n        text_no_punct = ''.join([c for c in text if c not in PUNCTUATION]).lower()\n        text_no_stopwords = [word for word in text_no_punct.split() if word not in STOPWORDS]\n        processed_texts.append(text_no_stopwords)\n\n    # Create the term dictionary\n    dictionary = corpora.Dictionary(processed_texts)\n\n    # Convert document into BOW\n    corpus = [dictionary.doc2bow(text) for text in processed_texts]\n\n    # Perform LDA\n    lda_model = models.LdaModel(corpus, num_topics=2, id2word=dictionary, passes=20)\n\n    return lda_model\n```"}
{"seed": "{\"task_id\": 300050, \"prompt\": \"def f_300050(my_list, sep):\\n\\t\", \"suffix\": \"\\n\\treturn s\", \"canonical_solution\": \"s = sep.join(my_list)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(['a','b','c','d','z'], '') == 'abcdz'\\n\", \"\\n    assert candidate(['a',' ','%','  ','!!'], '|') == 'a| |%|  |!!'\\n\"], \"entry_point\": \"f_300050\", \"intent\": \"\\u041f\\u0440\\u0435\\u043e\\u0431\\u0440\\u0430\\u0437\\u043e\\u0432\\u0430\\u0442\\u044c \\u0441\\u043f\\u0438\\u0441\\u043e\\u043a `my_list` \\u0432 \\u0441\\u0442\\u0440\\u043e\\u043a\\u0443 `s` c \\u0440\\u0430\\u0437\\u0434\\u0435\\u043b\\u0438\\u0442\\u0435\\u043b\\u0435\\u043c `sep` \", \"library\": []}", "generation": "Scenario 3:\nos, re, glob, pandas\n```python\nimport os\nimport re\nimport glob\nimport pandas as pd\n\n# Constants\nFILE_PATTERN = r'(.*)\\.csv'\nSEP = ','\n\ndef merge_csv_files(directory, filename):\n    \"\"\"\n    Merge all CSV files in a directory into one CSV file with a specified filename. \n    Only CSV files whose filenames match a specified pattern are considered.\n\n    Parameters:\n    directory (str): The directory where the CSV files are located.\n    filename (str): The filename for the merged CSV file.\n\n    Returns:\n    str: The path to the merged CSV file.\n\n    Requirements:\n    - os\n    - re\n    - glob\n    - pandas\n\n    Example:\n    >>> merge_csv_files('./data', 'merged.csv')\n    \"\"\"\n    csv_files = glob.glob(os.path.join(directory, '*.csv'))\n    \n    dfs = []\n    for file in csv_files:\n        if re.match(FILE_PATTERN, os.path.basename(file)):\n            df = pd.read_csv(file, sep=SEP)\n            dfs.append(df)\n    \n    merged_df = pd.concat(dfs, ignore_index=True)\n    merged_file = os.path.join(directory, filename)\n    merged_df.to_csv(merged_file, index=False, sep=SEP)\n\n    return merged_file\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, hashlib, binascii, base64\n```python\nimport os\nimport hashlib\nimport binascii\nimport base64\n\n# Constants\nSALT_SIZE = 16\nHASH_ITERATIONS = 100000\nHASH_SIZE = 64\n\ndef hash_password(password):\n    \"\"\"\n    Hash a password using PBKDF2 and return the salt and hash.\n\n    Parameters:\n    password (str): The password to be hashed.\n\n    Returns:\n    str: The salt and hash encoded in base64.\n\n    Requirements:\n    - os\n    - hashlib\n    - binascii\n    - base64\n\n    Example:\n    >>> hash_password('password123')\n    \"\"\"\n    salt = os.urandom(SALT_SIZE)\n    hash = hashlib.pbkdf2_hmac('sha256', password.encode(), salt, HASH_ITERATIONS, HASH_SIZE)\n    salt_hash = binascii.hexlify(salt + hash)\n    return base64.b64encode(salt_hash).decode()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nitertools, functools, operator\n```python\nimport itertools\nimport functools\nimport operator\n\n# Constants\nSEP = ','\n\ndef multiply_combinations(my_list):\n    \"\"\"\n    Generate all combinations of elements in a list, calculate the product of each combination, \n    and return the results as a string.\n\n    Parameters:\n    my_list (list): The list of elements. Each element must be an integer.\n\n    Returns:\n    str: The results as a string separated by a specified separator.\n\n    Requirements:\n    - itertools\n    - functools\n    - operator\n\n    Example:\n    >>> multiply_combinations([1, 2, 3])\n    \"\"\"\n    products = []\n    for r in range(1, len(my_list) + 1):\n        combinations = itertools.combinations(my_list, r)\n        for combination in combinations:\n            product = functools.reduce(operator.mul, combination)\n            products.append(product)\n    return SEP.join(map(str, products))\n```\n`GPT_ODEX_BREAK`\n\nScenario 6:\nnumpy, scipy.stats, matplotlib\n```python\nimport numpy as np\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\n\n# Constants\nBINS = 100\n\ndef plot_normal_distribution(mu, sigma):\n    \"\"\"\n    Generate and plot a normal distribution with a specified mean and standard deviation.\n\n    Parameters:\n    mu (float): The mean of the distribution.\n    sigma (float): The standard deviation of the distribution.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_normal_distribution(0, 1)\n    \"\"\"\n    x = np.linspace(mu - 3*sigma, mu + 3*sigma, BINS)\n    y = stats.norm.pdf(x, mu, sigma)\n    plt.plot(x, y)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 7:\npandas, random, matplotlib\n```python\nimport pandas as pd\nimport random\nimport matplotlib.pyplot as plt\n\n# Constants\nCATEGORIES = ['A', 'B', 'C', 'D', 'E']\nSEP = ','\n\ndef generate_and_plot_random_data(n):\n    \"\"\"\n    Generate a DataFrame of random data with a specified number of rows and plot the data.\n\n    Parameters:\n    n (int): The number of rows in the DataFrame.\n\n    Returns:\n    DataFrame: The generated DataFrame.\n\n    Requirements:\n    - pandas\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = generate_and_plot_random_data(1000)\n    >>> print(df)\n    \"\"\"\n    data = {'Category': [random.choice(CATEGORIES) for _ in range(n)],\n            'Value': [random.random() for _ in range(n)]}\n    df = pd.DataFrame(data)\n    df.groupby('Category').mean().plot(kind='bar')\n    plt.show()\n    return df\n```\n"}
{"seed": "{\"task_id\": 300050, \"prompt\": \"def f_300050(my_list, sep):\\n\\t\", \"suffix\": \"\\n\\treturn s\", \"canonical_solution\": \"s = sep.join(map(str, my_list))\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(['a','b','c','d','z'], '') == 'abcdz'\\n\", \"\\n    assert candidate(['a',' ','%','  ','!!'], '|') == 'a| |%|  |!!'\\n\"], \"entry_point\": \"f_300050\", \"intent\": \"\\u041f\\u0440\\u0435\\u043e\\u0431\\u0440\\u0430\\u0437\\u043e\\u0432\\u0430\\u0442\\u044c \\u0441\\u043f\\u0438\\u0441\\u043e\\u043a `my_list` \\u0432 \\u0441\\u0442\\u0440\\u043e\\u043a\\u0443 `s` c \\u0440\\u0430\\u0437\\u0434\\u0435\\u043b\\u0438\\u0442\\u0435\\u043b\\u0435\\u043c `sep` \", \"library\": []}", "generation": "Scenario 1:\npandas, itertools, random, numpy\n```python\nimport pandas as pd\nfrom itertools import permutations\nfrom random import random\nimport numpy as np\n\n# Constants\nNUMBERS = np.arange(10)\n\ndef generate_permutations_and_calculate_sum(df, numbers):\n    \"\"\"\n    Generate all permutations of a list of numbers and add a new column to a \n    given DataFrame with the sum of these numbers for each row. The DataFrame \n    should have the same number of rows as the length of the permutations.\n    \n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n    numbers (list): The list of numbers.\n    \n    Returns:\n    DataFrame: The modified DataFrame with the new column of sums.\n    \n    Requirements:\n    - pandas\n    - itertools\n    - random\n    - numpy\n    \n    Example:\n    >>> df = pd.DataFrame(np.random.rand(10, 5), columns=list('ABCDE'))\n    >>> numbers = np.arange(10)\n    >>> df = generate_permutations_and_calculate_sum(df, numbers)\n    >>> print(df)\n    \"\"\"\n    all_permutations = list(permutations(numbers))\n    df['Sum'] = [sum(p) for p in all_permutations]\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncollections, itertools, random\n```python\nfrom collections import Counter\nfrom itertools import chain\nfrom random import choice\n\n# Constants\nLETTERS = 'abcdefghijklmnopqrstuvwxyz'\n\ndef generate_string_and_count_letters(n):\n    \"\"\"\n    Generate a random string of n characters and count the number of occurrences \n    of each letter in the string.\n    \n    Parameters:\n    n (int): The number of characters in the string.\n    \n    Returns:\n    dict: A dictionary with the count of each letter.\n    \n    Requirements:\n    - collections\n    - itertools\n    - random\n    \n    Example:\n    >>> counts = generate_string_and_count_letters(1000)\n    >>> print(counts)\n    \"\"\"\n    string = ''.join(choice(LETTERS) for _ in range(n))\n    counts = dict(Counter(string))\n    return counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, matplotlib, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef create_dataframe_and_plot_corr_map(n):\n    \"\"\"\n    Create a DataFrame with n rows and 5 columns filled with random numbers \n    between -1 and 1. Then plot a correlation heatmap of the DataFrame.\n    \n    Parameters:\n    n (int): The number of rows in the DataFrame.\n    \n    Returns:\n    DataFrame: The created pandas DataFrame.\n    figure: The correlation heatmap.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib\n    - seaborn\n    \n    Example:\n    >>> df, fig = create_dataframe_and_plot_corr_map(100)\n    >>> print(df)\n    >>> plt.show()\n    \"\"\"\n    df = pd.DataFrame(np.random.uniform(-1, 1, size=(n, len(COLUMNS))), columns=COLUMNS)\n    fig, ax = plt.subplots(figsize=(10, 10))\n    sns.heatmap(df.corr(), annot=True, linewidths=.5, ax=ax)\n    return df, fig\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, scipy.stats\n```python\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import ttest_ind\n\n# Constants\nGROUPS = ['Group1', 'Group2']\n\ndef generate_data_and_perform_ttest(n):\n    \"\"\"\n    Generate two groups of n random numbers and perform a t-test to compare \n    the means of the two groups.\n    \n    Parameters:\n    n (int): The number of random numbers in each group.\n    \n    Returns:\n    float: The t statistic.\n    float: The p value.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - scipy.stats\n    \n    Example:\n    >>> t_stat, p_value = generate_data_and_perform_ttest(100)\n    >>> print(f'T-statistic: {t_stat}, P-value: {p_value}')\n    \"\"\"\n    group1 = np.random.rand(n)\n    group2 = np.random.rand(n)\n    t_stat, p_value = ttest_ind(group1, group2)\n    return t_stat, p_value\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, sklearn.decomposition\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.decomposition import PCA\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef create_dataframe_and_perform_pca(n):\n    \"\"\"\n    Create a DataFrame with n rows and 5 columns filled with random numbers \n    and perform Principal Component Analysis (PCA) on the DataFrame.\n    \n    Parameters:\n    n (int): The number of rows in the DataFrame.\n    \n    Returns:\n    DataFrame: The transformed DataFrame after PCA.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.decomposition\n    \n    Example:\n    >>> df = create_dataframe_and_perform_pca(100)\n    >>> print(df)\n    \"\"\"\n    df = pd.DataFrame(np.random.rand(n, len(COLUMNS)), columns=COLUMNS)\n    pca = PCA(n_components=2)\n    df_pca = pca.fit_transform(df)\n    return pd.DataFrame(df_pca)\n```\n"}
{"seed": "{\"task_id\": 52873, \"prompt\": \"def f_52873(s):\\n\\t\", \"suffix\": \"\\n\\treturn chars\", \"canonical_solution\": \"chars = list(s)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('*%^&gym3 34') == ['*','%','^','&','g','y','m','3',' ','3','4']\\n\"], \"entry_point\": \"f_52873\", \"intent\": \"\\u0420\\u0430\\u0437\\u0431\\u0438\\u0442\\u044c \\u0441\\u0442\\u0440\\u043e\\u043a\\u0443 `s` \\u043d\\u0430 \\u0441\\u043f\\u0438\\u0441\\u043e\\u043a \\u0441\\u0438\\u043c\\u0432\\u043e\\u043b\\u043e\\u0432 `chars`\", \"library\": []}", "generation": "Scenario 3:\npandas, numpy, itertools, random\n```python\nimport pandas as pd\nimport numpy as np\nimport itertools\nfrom random import choice\n\n# Constants\nLETTERS = list(\"abcdefghijklmnopqrstuvwxyz\")\nDIGITS = list(\"0123456789\")\nSPECIAL_CHARS = list(\"*%^&\")\nSPACE = [\" \"]\n\ndef generate_passwords(s):\n    \"\"\"\n    Generate a DataFrame of passwords with combinations of characters from the given string `s`.\n    Each password has a length of 8 characters.\n\n    Parameters:\n    s (str): The string from which to generate the passwords.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the generated passwords.\n\n    Requirements:\n    - pandas\n    - numpy\n    - itertools\n    - random\n\n    Example:\n    >>> passwords = generate_passwords('*%^&gym3 34')\n    >>> print(passwords)\n    \"\"\"\n    chars = list(s)\n    passwords = [''.join(p) for p in itertools.product(chars, repeat=8)]\n    passwords_df = pd.DataFrame(passwords, columns=['Password'])\n\n    return passwords_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, matplotlib, random\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import choice\n\n# Constants\nLETTERS = list(\"abcdefghijklmnopqrstuvwxyz\")\n\ndef plot_random_walk(s):\n    \"\"\"\n    Plot a 2D random walk based on the characters in the string `s`.\n    Each character corresponds to a direction (North, South, East, West).\n\n    Parameters:\n    s (str): The string from which to generate the random walk.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> plot_random_walk('gym34')\n    \"\"\"\n    directions = {\n        'g': np.array([0, 1]),  # North\n        'y': np.array([0, -1]),  # South\n        'm': np.array([1, 0]),  # East\n        '3': np.array([-1, 0]),  # West\n        '4': np.array([0, 0])  # Stay\n    }\n\n    position = np.array([0, 0])\n    positions = [position]\n\n    for char in s:\n        direction = directions.get(char, np.array([0, 0]))\n        position = position + direction\n        positions.append(position)\n\n    positions = np.array(positions)\n\n    plt.figure(figsize=(10, 10))\n    plt.plot(positions[:, 0], positions[:, 1], 'o-')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nrequests, json, pandas\n```python\nimport requests\nimport json\nimport pandas as pd\n\n# Constants\nURL_TEMPLATE = \"https://api.github.com/search/repositories?q=language:{}\"\n\ndef fetch_repositories(s):\n    \"\"\"\n    Fetch a list of the top 10 most starred repositories for a programming language `s` from GitHub.\n\n    Parameters:\n    s (str): The programming language.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the repository details.\n\n    Requirements:\n    - requests\n    - json\n    - pandas\n\n    Example:\n    >>> repos = fetch_repositories('python')\n    >>> print(repos)\n    \"\"\"\n    url = URL_TEMPLATE.format(s)\n    response = requests.get(url)\n    repos_json = json.loads(response.text)['items'][:10]\n\n    repos_df = pd.DataFrame(repos_json)\n\n    return repos_df[['name', 'html_url', 'stargazers_count']]\n```\n`GPT_ODEX_BREAK`\n\nScenario 6:\nnumpy, matplotlib, string\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport string\n\n# Constants\nLETTERS = list(string.ascii_lowercase)\n\ndef plot_character_frequency(s):\n    \"\"\"\n    Plot a histogram showing the frequency of each character in the string `s`.\n\n    Parameters:\n    s (str): The string to analyze.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - string\n\n    Example:\n    >>> plot_character_frequency('gym34')\n    \"\"\"\n    characters, counts = np.unique(list(s), return_counts=True)\n\n    plt.figure(figsize=(10, 5))\n    plt.bar(characters, counts)\n    plt.xlabel('Characters')\n    plt.ylabel('Frequency')\n    plt.title('Character Frequency')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 7:\nnumpy, random, string\n```python\nimport numpy as np\nfrom random import choice\nimport string\n\n# Constants\nLETTERS = list(string.ascii_lowercase)\n\ndef generate_random_string(s):\n    \"\"\"\n    Generate a random string of length `s` with characters from a-z.\n\n    Parameters:\n    s (str): The length of the string to generate.\n\n    Returns:\n    str: The generated string.\n\n    Requirements:\n    - numpy\n    - random\n    - string\n\n    Example:\n    >>> generate_random_string('10')\n    \"\"\"\n    length = int(s)\n    random_string = ''.join(choice(LETTERS) for _ in range(length))\n\n    return random_string\n```"}
{"seed": "{\"task_id\": 52873, \"prompt\": \"def f_52873(s):\\n\\t\", \"suffix\": \"\\n\\treturn chars\", \"canonical_solution\": \"chars = [c for c in s]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('*%^&gym3 34') == ['*','%','^','&','g','y','m','3',' ','3','4']\\n\"], \"entry_point\": \"f_52873\", \"intent\": \"\\u0420\\u0430\\u0437\\u0431\\u0438\\u0442\\u044c \\u0441\\u0442\\u0440\\u043e\\u043a\\u0443 `s` \\u043d\\u0430 \\u0441\\u043f\\u0438\\u0441\\u043e\\u043a \\u0441\\u0438\\u043c\\u0432\\u043e\\u043b\\u043e\\u0432 `chars`\", \"library\": []}", "generation": "Scenario 1: \npandas, matplotlib, seaborn, random\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom random import randint, choice\n\n# Constants\nLETTERS = 'abcdefghijklmnopqrstuvwxyz'\nNUMBERS = '0123456789'\nSPECIAL_CHARS = '*%&'\nCHARS = LETTERS + NUMBERS + SPECIAL_CHARS\n\ndef generate_and_plot_char_distribution(num_strings=100, string_length=10):\n    \"\"\"\n    Generate a specified number of random strings of a specified length, \n    count the occurrences of each character, and plot the distribution.\n\n    Parameters:\n    num_strings (int): The number of random strings to generate.\n    string_length (int): The length of each random string.\n\n    Returns:\n    Series: A pandas Series with the counts of each character.\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - seaborn\n    - random.choice\n    - random.randint\n\n    Example:\n    >>> char_distribution = generate_and_plot_char_distribution(1000, 20)\n    >>> print(char_distribution)\n    >>> sns.barplot(x=char_distribution.index, y=char_distribution.values)\n    \"\"\"\n    char_counts = pd.Series(0, index=list(CHARS))\n    \n    for _ in range(num_strings):\n        s = ''.join([choice(CHARS) for _ in range(string_length)])\n        for c in s:\n            char_counts[c] += 1\n    \n    return char_counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncollections, matplotlib, pandas, random\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom random import choice\n\n# Constants\nLETTERS = 'abcdefghijklmnopqrstuvwxyz'\nNUMBERS = '0123456789'\nSPECIAL_CHARS = '*%&'\nCHARS = LETTERS + NUMBERS + SPECIAL_CHARS\n\ndef generate_and_plot_char_frequencies(num_strings=100, string_length=10):\n    \"\"\"\n    Generate a specified number of random strings of a specified length, \n    calculate the frequencies of each character, and plot the frequencies.\n\n    Parameters:\n    num_strings (int): The number of random strings to generate.\n    string_length (int): The length of each random string.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the frequencies of each character.\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - collections.Counter\n    - random.choice\n\n    Example:\n    >>> char_frequencies = generate_and_plot_char_frequencies(1000, 20)\n    >>> print(char_frequencies)\n    >>> char_frequencies.plot(kind='bar')\n    \"\"\"\n    char_counter = Counter()\n\n    for _ in range(num_strings):\n        s = ''.join([choice(CHARS) for _ in range(string_length)])\n        char_counter.update(s)\n\n    total_chars = sum(char_counter.values())\n    char_frequencies = {char: count / total_chars for char, count in char_counter.items()}\n    \n    char_frequencies_df = pd.DataFrame(list(char_frequencies.items()), columns=['Character', 'Frequency'])\n\n    return char_frequencies_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, matplotlib, random\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import choice\n\n# Constants\nLETTERS = 'abcdefghijklmnopqrstuvwxyz'\nNUMBERS = '0123456789'\nSPECIAL_CHARS = '*%&'\nCHARS = LETTERS + NUMBERS + SPECIAL_CHARS\n\ndef generate_and_plot_char_histogram(num_strings=100, string_length=10):\n    \"\"\"\n    Generate a specified number of random strings of a specified length, \n    count the occurrences of each character, and plot a histogram of the counts.\n\n    Parameters:\n    num_strings (int): The number of random strings to generate.\n    string_length (int): The length of each random string.\n\n    Returns:\n    dict: A dictionary with the counts of each character.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - random.choice\n\n    Example:\n    >>> char_counts = generate_and_plot_char_histogram(1000, 20)\n    >>> print(char_counts)\n    >>> plt.hist(char_counts.values())\n    \"\"\"\n    char_counts = {char: 0 for char in CHARS}\n    \n    for _ in range(num_strings):\n        s = ''.join([choice(CHARS) for _ in range(string_length)])\n        for c in s:\n            char_counts[c] += 1\n    \n    return char_counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, pandas, seaborn, random\n```python\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom random import choice\n\n# Constants\nLETTERS = 'abcdefghijklmnopqrstuvwxyz'\nNUMBERS = '0123456789'\nSPECIAL_CHARS = '*%&'\nCHARS = LETTERS + NUMBERS + SPECIAL_CHARS\n\ndef generate_and_plot_char_boxplot(num_strings=100, string_length=10):\n    \"\"\"\n    Generate a specified number of random strings of a specified length, \n    count the occurrences of each character, and plot a boxplot of the counts.\n\n    Parameters:\n    num_strings (int): The number of random strings to generate.\n    string_length (int): The length of each random string.\n\n    Returns:\n    Series: A pandas Series with the counts of each character.\n\n    Requirements:\n    - numpy\n    - pandas\n    - seaborn\n    - random.choice\n\n    Example:\n    >>> char_counts = generate_and_plot_char_boxplot(1000, 20)\n    >>> print(char_counts)\n    >>> sns.boxplot(char_counts)\n    \"\"\"\n    char_counts = pd.Series(0, index=list(CHARS))\n    \n    for _ in range(num_strings):\n        s = ''.join([choice(CHARS) for _ in range(string_length)])\n        for c in s:\n            char_counts[c] += 1\n    \n    return char_counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, scipy.stats, random\n```python\nimport numpy as np\nfrom scipy import stats\nfrom random import choice\n\n# Constants\nLETTERS = 'abcdefghijklmnopqrstuvwxyz'\nNUMBERS = '0123456789'\nSPECIAL_CHARS = '*%&'\nCHARS = LETTERS + NUMBERS + SPECIAL_CHARS\n\ndef calculate_char_distribution_stats(num_strings=100, string_length=10):\n    \"\"\"\n    Generate a specified number of random strings of a specified length, \n    count the occurrences of each character, and calculate the mean, median, \n    mode, and standard deviation of the counts.\n\n    Parameters:\n    num_strings (int): The number of random strings to generate.\n    string_length (int): The length of each random string.\n\n    Returns:\n    dict: A dictionary with the mean, median, mode, and standard deviation of the counts.\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - random.choice\n\n    Example:\n    >>> char_distribution_stats = calculate_char_distribution_stats(1000, 20)\n    >>> print(char_distribution_stats)\n    \"\"\"\n    char_counts = {char: 0 for char in CHARS}\n    \n    for _ in range(num_strings):\n        s = ''.join([choice(CHARS) for _ in range(string_length)])\n        for c in s:\n            char_counts[c] += 1\n\n    counts = np.array(list(char_counts.values()))\n    \n    stats = {\n        'mean': np.mean(counts),\n        'median': np.median(counts),\n        'mode': stats.mode(counts)[0][0],\n        'std_dev': np.std(counts)\n    }\n    \n    return stats\n```\n"}
{"seed": "{\"task_id\": 648454, \"prompt\": \"def f_648454(num, n):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"round(num, n)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(100, 2) == 100\\n\", \"\\n    assert candidate(100.12345, 3) == 100.123\\n\"], \"entry_point\": \"f_648454\", \"intent\": \"\\u041e\\u043a\\u0440\\u0443\\u0433\\u043b\\u0438\\u0442\\u044c `num` \\u0434\\u043e `n` \\u0437\\u043d\\u0430\\u043a\\u043e\\u0432 \\u043f\\u043e\\u0441\\u043b\\u0435 \\u0437\\u0430\\u043f\\u044f\\u0442\\u043e\\u0439 \", \"library\": []}", "generation": "Scenario 1:\nnumpy, random, math, matplotlib\n```python\nimport numpy as np\nimport random\nimport math\nfrom matplotlib import pyplot as plt\n\n# Constants\nRANGE_START = 0\nRANGE_END = 100\nNUM_ELEMENTS = 1000\n\ndef generate_and_plot_normal_distribution(mu, sigma):\n    \"\"\"\n    Generate a list of numbers with a normal distribution and plot the histogram.\n\n    Parameters:\n    mu (float): The mean value.\n    sigma (float): The standard deviation.\n\n    Returns:\n    list: The generated list of numbers.\n\n    Requirements:\n    - numpy\n    - random\n    - math\n    - matplotlib.pyplot\n\n    Example:\n    >>> random_numbers = generate_and_plot_normal_distribution(50, 15)\n    >>> print(random_numbers)\n    >>> plt.hist(random_numbers, bins=50)\n    \"\"\"\n    normal_distribution = np.random.normal(mu, sigma, NUM_ELEMENTS)\n    rounded_distribution = [round(num, 2) for num in normal_distribution]\n\n    return rounded_distribution\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, pandas, sklearn.preprocessing\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Constants\nFEATURES = ['Feature1', 'Feature2', 'Feature3', 'Feature4', 'Feature5']\n\ndef scale_data(data):\n    \"\"\"\n    Scale the numeric values in a DataFrame to the range 0-1.\n\n    Parameters:\n    data (DataFrame): The input data.\n\n    Returns:\n    DataFrame: The scaled data.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.preprocessing\n\n    Example:\n    >>> df = pd.DataFrame(np.random.randint(0,100,size=(100, 5)), columns=FEATURES)\n    >>> scaled_df = scale_data(df)\n    >>> print(scaled_df)\n    \"\"\"\n    scaler = MinMaxScaler()\n    scaled_data = scaler.fit_transform(data)\n    scaled_df = pd.DataFrame(scaled_data, columns=FEATURES)\n\n    return scaled_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, random, numpy\n```python\nimport pandas as pd\nimport random\nimport numpy as np\n\n# Constants\nNAMES = ['John', 'Jane', 'Mary', 'James', 'Emma', 'Robert', 'Emily', 'Michael', 'Elizabeth', 'William']\nAGES = list(range(20, 70))\n\ndef generate_random_people_data(n):\n    \"\"\"\n    Generate a DataFrame with random people data (name and age).\n\n    Parameters:\n    n (int): The number of rows to generate.\n\n    Returns:\n    DataFrame: The DataFrame with the generated data.\n\n    Requirements:\n    - pandas\n    - random\n    - numpy\n\n    Example:\n    >>> df = generate_random_people_data(100)\n    >>> print(df)\n    \"\"\"\n    data = {\n        'Name': [random.choice(NAMES) for _ in range(n)],\n        'Age': [round(random.choice(AGES), 0) for _ in range(n)]\n    }\n\n    df = pd.DataFrame(data)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, matplotlib, random\n```python\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport random\n\n# Constants\nRANGE_START = 0\nRANGE_END = 100\n\ndef generate_and_plot_random_walk(n):\n    \"\"\"\n    Generate a random walk and plot it.\n\n    Parameters:\n    n (int): The number of steps in the walk.\n\n    Returns:\n    list: The generated random walk.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> walk = generate_and_plot_random_walk(1000)\n    >>> print(walk)\n    >>> plt.plot(walk)\n    \"\"\"\n    steps = np.random.choice([-1, 1], size=n)\n    walk = np.cumsum(steps)\n    walk = [round(num, 2) for num in walk]\n\n    return walk\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, pandas, seaborn\n```python\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n# Constants\nFEATURES = ['Feature1', 'Feature2', 'Feature3', 'Feature4', 'Feature5']\n\ndef generate_and_plot_correlation_matrix():\n    \"\"\"\n    Generate a DataFrame with random data and plot the correlation matrix.\n\n    Returns:\n    DataFrame: The DataFrame with the generated data.\n\n    Requirements:\n    - pandas\n    - numpy\n    - seaborn\n\n    Example:\n    >>> df = generate_and_plot_correlation_matrix()\n    >>> print(df)\n    >>> corr = df.corr()\n    >>> sns.heatmap(corr)\n    \"\"\"\n    data = np.random.random(size=(100, 5))\n    df = pd.DataFrame(data, columns=FEATURES)\n\n    return df\n```\n"}
{"seed": "{\"task_id\": 359110, \"prompt\": \"def f_359110(s):\\n\\t\", \"suffix\": \"\\n\\treturn s\", \"canonical_solution\": \"s = s.replace(' ', '')\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('     ') == ''\\n\", \"\\n    assert candidate('hello hi') == 'hellohi'\\n\", \"\\n    assert candidate('') == ''\\n\", \"\\n    assert candidate('same') == 'same'\\n\", \"\\n    assert candidate(' _ _ _') == '___'\\n\"], \"entry_point\": \"f_359110\", \"intent\": \"\\u0423\\u0434\\u0430\\u043b\\u0438\\u0442\\u044c \\u0432\\u0441\\u0435 \\u043f\\u0440\\u043e\\u0431\\u0435\\u043b\\u044b \\u0438\\u0437 \\u0441\\u0442\\u0440\\u043e\\u043a\\u0438 `s`\", \"library\": []}", "generation": "Scenario 1: \npandas, matplotlib, random, re\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import randint\nimport re\n\n# Constants\nSENTENCES = ['Hello world', 'Python is cool', 'I love programming', 'Data science is fun', 'Machine learning']\nWORDS = ['Python', 'programming', 'data', 'science', 'machine', 'learning']\n\ndef count_word_frequency(sentences, words):\n    \"\"\"\n    Count the frequency of specific words in a list of sentences and plot the result.\n    \n    Parameters:\n    sentences (list): The list of sentences.\n    words (list): The list of words to count.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with word frequencies.\n    \n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - random\n    - re\n    \n    Example:\n    >>> count_word_frequency(SENTENCES, WORDS)\n    \"\"\"\n    word_count = dict.fromkeys(words, 0)\n\n    for sentence in sentences:\n        for word in re.findall(r'\\b\\w+\\b', sentence):\n            if word in word_count:\n                word_count[word] += 1\n\n    df = pd.DataFrame.from_dict(word_count, orient='index', columns=['Frequency'])\n    df.sort_values('Frequency', ascending=False, inplace=True)\n    df.plot(kind='bar')\n    plt.show()\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \npandas, numpy, sklearn.preprocessing, re\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nimport re\n\n# Constants\nDATA = [\n    ['male', 'New York', 'high'],\n    ['female', 'Los Angeles', 'medium'],\n    ['male', 'San Francisco', 'low'],\n    ['female', 'Chicago', 'medium'],\n    ['male', 'New York', 'medium']\n]\nCOLUMNS = ['Gender', 'City', 'Income Level']\n\ndef preprocess_data(data, columns):\n    \"\"\"\n    Preprocess a list of data by removing spaces from string values, encoding categorical variables \n    and filling missing values.\n    \n    Parameters:\n    data (list): The list of data.\n    columns (list): The list of column names.\n    \n    Returns:\n    DataFrame: The preprocessed DataFrame.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.preprocessing\n    - re\n    \n    Example:\n    >>> preprocess_data(DATA, COLUMNS)\n    \"\"\"\n    df = pd.DataFrame(data, columns=columns)\n\n    # Remove spaces from string values\n    df = df.applymap(lambda s: re.sub(r'\\s+', '', str(s)) if isinstance(s, str) else s)\n\n    # Encode categorical variables\n    for col in df.columns[df.dtypes == 'object']:\n        le = LabelEncoder()\n        df[col] = le.fit_transform(df[col])\n\n    # Fill missing values\n    df.fillna(df.mean(), inplace=True)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \npandas, numpy, matplotlib, re\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re\n\n# Constants\nDATA = [\n    ['John Doe', 28, 'New York', 50000],\n    ['Jane Doe', 32, 'Los Angeles', 70000],\n    ['Mary Johnson', 35, 'San Francisco', 80000],\n    ['James Smith', 30, 'Chicago', 60000],\n    ['Michael Brown', 33, 'New York', 70000]\n]\nCOLUMNS = ['Name', 'Age', 'City', 'Salary']\n\ndef analyze_data(data, columns):\n    \"\"\"\n    Analyze a list of data by removing spaces from string values, calculating descriptive statistics, \n    and plotting histograms for numerical variables.\n    \n    Parameters:\n    data (list): The list of data.\n    columns (list): The list of column names.\n    \n    Returns:\n    DataFrame: The DataFrame with the descriptive statistics.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - re\n    \n    Example:\n    >>> analyze_data(DATA, COLUMNS)\n    \"\"\"\n    df = pd.DataFrame(data, columns=columns)\n\n    # Remove spaces from string values\n    df = df.applymap(lambda s: re.sub(r'\\s+', '', str(s)) if isinstance(s, str) else s)\n\n    # Calculate descriptive statistics\n    stats = df.describe()\n\n    # Plot histograms for numerical variables\n    df.hist()\n    plt.show()\n\n    return stats\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \npandas, numpy, sklearn.model_selection, re\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport re\n\n# Constants\nDATA = [\n    ['John Doe', 28, 'New York', 50000],\n    ['Jane Doe', 32, 'Los Angeles', 70000],\n    ['Mary Johnson', 35, 'San Francisco', 80000],\n    ['James Smith', 30, 'Chicago', 60000],\n    ['Michael Brown', 33, 'New York', 70000]\n]\nCOLUMNS = ['Name', 'Age', 'City', 'Salary']\n\ndef split_data(data, columns, test_size):\n    \"\"\"\n    Split a list of data into training and testing sets after removing spaces from string values.\n    \n    Parameters:\n    data (list): The list of data.\n    columns (list): The list of column names.\n    test_size (float): The size of the testing set.\n    \n    Returns:\n    DataFrame, DataFrame: The training and testing DataFrames.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.model_selection\n    - re\n    \n    Example:\n    >>> train, test = split_data(DATA, COLUMNS, 0.2)\n    \"\"\"\n    df = pd.DataFrame(data, columns=columns)\n\n    # Remove spaces from string values\n    df = df.applymap(lambda s: re.sub(r'\\s+', '', str(s)) if isinstance(s, str) else s)\n\n    # Split data into training and testing sets\n    train, test = train_test_split(df, test_size=test_size, random_state=42)\n\n    return train, test\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \npandas, numpy, seaborn, re\n```python\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport re\n\n# Constants\nDATA = [\n    ['John Doe', 28, 'New York', 50000],\n    ['Jane Doe', 32, 'Los Angeles', 70000],\n    ['Mary Johnson', 35, 'San Francisco', 80000],\n    ['James Smith', 30, 'Chicago', 60000],\n    ['Michael Brown', 33, 'New York', 70000]\n]\nCOLUMNS = ['Name', 'Age', 'City', 'Salary']\n\ndef visualize_data(data, columns):\n    \"\"\"\n    Visualize a list of data by removing spaces from string values and plotting a pairplot.\n    \n    Parameters:\n    data (list): The list of data.\n    columns (list): The list of column names.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - seaborn\n    - re\n    \n    Example:\n    >>> visualize_data(DATA, COLUMNS)\n    \"\"\"\n    df = pd.DataFrame(data, columns=columns)\n\n    # Remove spaces from string values\n    df = df.applymap(lambda s: re.sub(r'\\s+', '', str(s)) if isinstance(s, str) else s)\n\n    # Plot a pairplot\n    sns.pairplot(df)\n    plt.show()\n```"}
{"seed": "{\"task_id\": 359110, \"prompt\": \"def f_359110(s):\\n\\t\", \"suffix\": \"\\n\\treturn s\", \"canonical_solution\": \"s = ''.join(s.split())\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('     ') == ''\\n\", \"\\n    assert candidate('hello hi') == 'hellohi'\\n\", \"\\n    assert candidate('') == ''\\n\", \"\\n    assert candidate('same') == 'same'\\n\", \"\\n    assert candidate(' _ _ _') == '___'\\n\"], \"entry_point\": \"f_359110\", \"intent\": \"\\u0423\\u0434\\u0430\\u043b\\u0438\\u0442\\u044c \\u0432\\u0441\\u0435 \\u043f\\u0440\\u043e\\u0431\\u0435\\u043b\\u044b \\u0438\\u0437 \\u0441\\u0442\\u0440\\u043e\\u043a\\u0438 `s`\", \"library\": []}", "generation": "Scenario 1: \ncollections, re, string\n```python\nimport collections\nimport re\nimport string\n\n# Constants\nPUNCTUATION = string.punctuation\n\ndef count_words_without_spaces_punctuation(s):\n    \"\"\"\n    Count the occurrences of each word in a string after removing spaces and punctuation.\n\n    Parameters:\n    s (str): The input string.\n\n    Returns:\n    dict: A dictionary where the keys are words and the values are their counts.\n\n    Requirements:\n    - collections\n    - re\n    - string\n\n    Example:\n    >>> count_words_without_spaces_punctuation('Hello, World! Hello world.')\n    \"\"\"\n    s_without_spaces = ''.join(s.split())\n    s_without_spaces_punctuation = re.sub(f\"[{PUNCTUATION}]\", \"\", s_without_spaces)\n    word_counts = collections.Counter(s_without_spaces_punctuation.lower().split())\n\n    return dict(word_counts)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nitertools, string\n```python\nimport itertools\nimport string\n\n# Constants\nLETTERS = string.ascii_letters\n\ndef generate_combinations_without_spaces(s, n):\n    \"\"\"\n    Generate all possible n-length combinations of a string after removing spaces.\n\n    Parameters:\n    s (str): The input string.\n    n (int): The length of the combinations.\n\n    Returns:\n    list: A list of all possible n-length combinations.\n\n    Requirements:\n    - itertools\n    - string\n\n    Example:\n    >>> generate_combinations_without_spaces('abc def', 2)\n    \"\"\"\n    s_without_spaces = ''.join(s.split())\n    combinations = list(itertools.combinations(s_without_spaces, n))\n\n    return [\"\".join(combination) for combination in combinations]\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\njson, os\n```python\nimport json\nimport os\n\ndef write_string_without_spaces_to_file(s, filename):\n    \"\"\"\n    Write a string to a file after removing spaces.\n\n    Parameters:\n    s (str): The input string.\n    filename (str): The filename.\n\n    Returns:\n    str: The filepath.\n\n    Requirements:\n    - json\n    - os\n\n    Example:\n    >>> write_string_without_spaces_to_file('Hello, World!', 'file.txt')\n    \"\"\"\n    s_without_spaces = ''.join(s.split())\n    filepath = os.path.join(os.getcwd(), filename)\n    \n    with open(filepath, 'w') as f:\n        f.write(s_without_spaces)\n    \n    return filepath\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nbase64, hashlib\n```python\nimport base64\nimport hashlib\n\ndef hash_string_without_spaces(s):\n    \"\"\"\n    Compute the SHA-256 hash of a string after removing spaces.\n\n    Parameters:\n    s (str): The input string.\n\n    Returns:\n    str: The SHA-256 hash.\n\n    Requirements:\n    - base64\n    - hashlib\n\n    Example:\n    >>> hash_string_without_spaces('Hello, World!')\n    \"\"\"\n    s_without_spaces = ''.join(s.split())\n    hashed_s = hashlib.sha256(s_without_spaces.encode()).digest()\n    hashed_s_base64 = base64.b64encode(hashed_s).decode()\n\n    return hashed_s_base64\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nrandom, string\n```python\nimport random\nimport string\n\n# Constants\nLETTERS = string.ascii_letters\n\ndef generate_random_string_without_spaces(s, n):\n    \"\"\"\n    Generate a random n-length string without spaces.\n\n    Parameters:\n    n (int): The length of the string.\n\n    Returns:\n    str: The random string.\n\n    Requirements:\n    - random\n    - string\n\n    Example:\n    >>> generate_random_string_without_spaces(10)\n    \"\"\"\n    random_string = ''.join(random.choice(LETTERS) for _ in range(n))\n\n    return random_string\n```"}
{"seed": "{\"task_id\": 418982, \"prompt\": \"def f_418982(my_list):\\n\\t\", \"suffix\": \"\\n\\treturn list_value_counts\", \"canonical_solution\": \"list_value_counts = Counter(my_list)\", \"test_start\": \"\\nfrom collections import Counter\\n\\ndef check(candidate):\", \"test\": [\"\\n    c_1 = candidate([1,1,1,0])\\n    assert len(c_1) == 2 and c_1[1] == 3 and c_1[0] == 1\\n\", \"\\n    c_2 = candidate([])\\n    assert len(c_2) == 0 \\n\", \"\\n    c_3 = candidate(['h', 'h', 'a', -1])\\n    assert len(c_3) == 3 and c_3['h'] == 2 and c_3['a'] == 1 and c_3[-1] == 1\\n\", \"\\n    c_4 = candidate([5])\\n    assert len(c_4) == 1 and c_4[5] == 1\\n\", \"\\n    c_5 = candidate([-1,-1,-1])\\n    assert len(c_5) == 1 and c_5[-1] == 3 \\n\"], \"entry_point\": \"f_418982\", \"intent\": \"\\u041f\\u043e\\u0441\\u0447\\u0438\\u0442\\u0430\\u0442\\u044c \\u044d\\u043b\\u0435\\u043c\\u0435\\u043d\\u0442\\u044b \\u0432 \\u0441\\u043f\\u0438\\u0441\\u043a\\u0435 `my_list` \\u0432 `list_value_counts`\", \"library\": [\"collections\"]}", "generation": "Scenario 1: \ncollections, pandas, numpy, matplotlib\n```python\nfrom collections import Counter\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCATEGORIES = ['Fruits', 'Vegetables', 'Meat', 'Dairy', 'Grains']\n\ndef analyze_shopping_list(shopping_list):\n    \"\"\"\n    Analyze a shopping list, count the items in each category, and plot a bar graph \n    of the item counts.\n    \n    Parameters:\n    shopping_list (list): The shopping list with tuples in the form (category, item).\n    \n    Returns:\n    Counter: A Counter object with item counts for each category.\n    \n    Requirements:\n    - collections\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> shopping_list = [('Fruits', 'Apple'), ('Vegetables', 'Carrot'), \n                         ('Meat', 'Chicken'), ('Dairy', 'Milk'), ('Grains', 'Rice')]\n    >>> item_counts = analyze_shopping_list(shopping_list)\n    >>> print(item_counts)\n    >>> item_counts.plot(kind='bar')\n    \"\"\"\n    category_counts = Counter(category for category, _ in shopping_list)\n\n    df = pd.DataFrame.from_dict(category_counts, orient='index').reset_index()\n    df.columns = ['Category', 'Count']\n    df = df[df['Category'].isin(CATEGORIES)]\n    df.sort_values('Count', ascending=False, inplace=True)\n    \n    df.plot(x='Category', y='Count', kind='bar')\n    plt.show()\n\n    return category_counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \ncollections, datetime, random\n```python\nfrom collections import Counter\nfrom datetime import datetime, timedelta\nfrom random import randint\n\n# Constants\nUSERS = ['User1', 'User2', 'User3', 'User4', 'User5']\nACTIVITIES = ['Login', 'Logout', 'View', 'Edit', 'Delete']\n\ndef simulate_user_activity(start_time, end_time):\n    \"\"\"\n    Simulate user activity within a given time range, count the activities for each user, \n    and return the counts.\n    \n    Parameters:\n    start_time (datetime): The start time of the simulation.\n    end_time (datetime): The end time of the simulation.\n    \n    Returns:\n    Counter: A Counter object with activity counts for each user.\n    \n    Requirements:\n    - collections\n    - datetime\n    - random\n    \n    Example:\n    >>> start_time = datetime(2021, 1, 1, 8, 0, 0)\n    >>> end_time = datetime(2021, 1, 1, 17, 0, 0)\n    >>> activity_counts = simulate_user_activity(start_time, end_time)\n    >>> print(activity_counts)\n    \"\"\"\n    activity_log = []\n    current_time = start_time\n\n    while current_time <= end_time:\n        user = USERS[randint(0, len(USERS) - 1)]\n        activity = ACTIVITIES[randint(0, len(ACTIVITIES) - 1)]\n        activity_log.append((user, activity))\n        current_time += timedelta(minutes=randint(1, 60))\n\n    activity_counts = Counter(user for user, _ in activity_log)\n\n    return activity_counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \ncollections, pandas, matplotlib, seaborn\n```python\nfrom collections import Counter\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nSTUDENTS = ['John', 'Jane', 'Bob', 'Alice', 'Charlie', 'Debbie']\nCOURSES = ['Math', 'Science', 'English', 'History', 'Art', 'PE']\n\ndef analyze_student_courses(student_courses):\n    \"\"\"\n    Analyze a list of student course enrollments, count the enrollments for each course, \n    and create a heatmap of the enrollments.\n    \n    Parameters:\n    student_courses (list): The list of student course enrollments with tuples in the \n    form (student, course).\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the course enrollment counts.\n    \n    Requirements:\n    - collections\n    - pandas\n    - matplotlib.pyplot\n    - seaborn\n    \n    Example:\n    >>> student_courses = [('John', 'Math'), ('Jane', 'Science'), \n                           ('Bob', 'English'), ('Alice', 'History'), \n                           ('Charlie', 'Art'), ('Debbie', 'PE')]\n    >>> enrollment_counts = analyze_student_courses(student_courses)\n    >>> print(enrollment_counts)\n    >>> sns.heatmap(enrollment_counts, annot=True, cmap='Blues')\n    \"\"\"\n    course_counts = Counter(course for _, course in student_courses)\n\n    df = pd.DataFrame.from_dict(course_counts, orient='index').reset_index()\n    df.columns = ['Course', 'Count']\n    df = df[df['Course'].isin(COURSES)]\n    df.sort_values('Count', ascending=False, inplace=True)\n\n    pivot_df = df.pivot(index='Course', columns='Count', values='Count')\n    \n    return pivot_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \ncollections, re, nltk\n```python\nfrom collections import Counter\nimport re\nfrom nltk.corpus import stopwords\n\n# Constants\nSTOPWORDS = set(stopwords.words('english'))\n\ndef count_words(text):\n    \"\"\"\n    Count the non-stopword words in a text and return the counts.\n    \n    Parameters:\n    text (str): The text to analyze.\n    \n    Returns:\n    Counter: A Counter object with word counts.\n    \n    Requirements:\n    - collections\n    - re\n    - nltk.corpus.stopwords\n    \n    Example:\n    >>> text = \"This is a simple example text for counting words.\"\n    >>> word_counts = count_words(text)\n    >>> print(word_counts)\n    \"\"\"\n    words = re.findall(r'\\w+', text.lower())\n    words = [word for word in words if word not in STOPWORDS]\n    word_counts = Counter(words)\n\n    return word_counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \ncollections, numpy, pandas, sklearn.preprocessing\n```python\nfrom collections import Counter\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Constants\nFEATURES = ['Feature1', 'Feature2', 'Feature3', 'Feature4', 'Feature5']\n\ndef analyze_features(data):\n    \"\"\"\n    Analyze a list of feature vectors, count the occurrences of each feature, \n    scale the features to the range [0, 1], and return the counts and scaled features.\n    \n    Parameters:\n    data (list): The list of feature vectors.\n    \n    Returns:\n    tuple: A tuple with a Counter object with feature counts and a pandas DataFrame \n    with the scaled features.\n    \n    Requirements:\n    - collections\n    - numpy\n    - pandas\n    - sklearn.preprocessing.MinMaxScaler\n    \n    Example:\n    >>> data = [[1, 2, 3, 4, 5], [6, 7, 8, 9, 10], [11, 12, 13, 14, 15]]\n    >>> feature_counts, scaled_features = analyze_features(data)\n    >>> print(feature_counts)\n    >>> print(scaled_features)\n    \"\"\"\n    feature_counts = Counter(feature for vector in data for feature in vector)\n\n    scaler = MinMaxScaler()\n    scaled_features = scaler.fit_transform(data)\n    scaled_features = pd.DataFrame(scaled_features, columns=FEATURES)\n\n    return feature_counts, scaled_features\n```"}
{"seed": "{\"task_id\": 418982, \"prompt\": \"def f_418982(my_list):\\n\\t\", \"suffix\": \"\\n\\treturn list_value_counts\", \"canonical_solution\": \"list_value_counts = {i: my_list.count(i) for i in my_list}\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    c_1 = candidate([1,1,1,0])\\n    assert len(c_1) == 2 and c_1[1] == 3 and c_1[0] == 1\\n\", \"\\n    c_2 = candidate([])\\n    assert len(c_2) == 0 \\n\", \"\\n    c_3 = candidate(['h', 'h', 'a', -1])\\n    assert len(c_3) == 3 and c_3['h'] == 2 and c_3['a'] == 1 and c_3[-1] == 1\\n\", \"\\n    c_4 = candidate([5])\\n    assert len(c_4) == 1 and c_4[5] == 1\\n\", \"\\n    c_5 = candidate([-1,-1,-1])\\n    assert len(c_5) == 1 and c_5[-1] == 3 \\n\"], \"entry_point\": \"f_418982\", \"intent\": \"\\u041f\\u043e\\u0441\\u0447\\u0438\\u0442\\u0430\\u0442\\u044c \\u044d\\u043b\\u0435\\u043c\\u0435\\u043d\\u0442\\u044b \\u0432 \\u0441\\u043f\\u0438\\u0441\\u043a\\u0435 `my_list` \\u0432 `list_value_counts`\", \"library\": []}", "generation": "Scenario 1: \ncollections, pandas, matplotlib, random\n```python\nimport collections\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\n\n# Constants\nLETTERS = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']\n\ndef generate_random_letter_counts(n):\n    \"\"\"\n    Generate a random list of letters of length n, count the occurrences of each letter, \n    convert the counts to a pandas DataFrame, and display a bar chart of the counts.\n\n    Parameters:\n    n (int): The length of the random list of letters.\n\n    Returns:\n    DataFrame: A pandas DataFrame with counts of each letter.\n\n    Requirements:\n    - collections\n    - pandas\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> df = generate_random_letter_counts(1000)\n    >>> print(df)\n    >>> df.plot(kind='bar', x='Letter', y='Count')\n    \"\"\"\n    my_list = [random.choice(LETTERS) for _ in range(n)]\n    list_value_counts = collections.Counter(my_list)\n\n    df = pd.DataFrame(list(list_value_counts.items()), columns=['Letter', 'Count'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncollections, numpy, matplotlib, random\n```python\nimport collections\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\n\n# Constants\nNUMBERS = list(range(1, 11))\n\ndef generate_random_number_counts(n):\n    \"\"\"\n    Generate a random list of numbers of length n, count the occurrences of each number, \n    convert the counts to a numpy array, and display a histogram of the counts.\n\n    Parameters:\n    n (int): The length of the random list of numbers.\n\n    Returns:\n    np.array: A numpy array with counts of each number.\n\n    Requirements:\n    - collections\n    - numpy\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> arr = generate_random_number_counts(1000)\n    >>> print(arr)\n    >>> plt.hist(arr, bins=10)\n    \"\"\"\n    my_list = [random.choice(NUMBERS) for _ in range(n)]\n    list_value_counts = collections.Counter(my_list)\n\n    arr = np.array(list(list_value_counts.values()))\n\n    return arr\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ncollections, pandas, seaborn, random\n```python\nimport collections\nimport pandas as pd\nimport seaborn as sns\nimport random\n\n# Constants\nWORDS = ['apple', 'banana', 'cherry', 'date', 'elderberry']\n\ndef generate_random_word_counts(n):\n    \"\"\"\n    Generate a random list of words of length n, count the occurrences of each word, \n    convert the counts to a pandas DataFrame, and display a countplot of the counts.\n\n    Parameters:\n    n (int): The length of the random list of words.\n\n    Returns:\n    DataFrame: A pandas DataFrame with counts of each word.\n\n    Requirements:\n    - collections\n    - pandas\n    - seaborn\n    - random\n\n    Example:\n    >>> df = generate_random_word_counts(1000)\n    >>> print(df)\n    >>> sns.countplot(data=df, x='Word', y='Count')\n    \"\"\"\n    my_list = [random.choice(WORDS) for _ in range(n)]\n    list_value_counts = collections.Counter(my_list)\n\n    df = pd.DataFrame(list(list_value_counts.items()), columns=['Word', 'Count'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ncollections, numpy, pandas, random\n```python\nimport collections\nimport numpy as np\nimport pandas as pd\nimport random\n\n# Constants\nDIGITS = list(range(10))\n\ndef generate_random_digit_counts(n):\n    \"\"\"\n    Generate a random list of digits of length n, count the occurrences of each digit, \n    convert the counts to a pandas DataFrame with a column of numpy arrays.\n\n    Parameters:\n    n (int): The length of the random list of digits.\n\n    Returns:\n    DataFrame: A pandas DataFrame with counts of each digit as a numpy array.\n\n    Requirements:\n    - collections\n    - numpy\n    - pandas\n    - random\n\n    Example:\n    >>> df = generate_random_digit_counts(1000)\n    >>> print(df)\n    \"\"\"\n    my_list = [random.choice(DIGITS) for _ in range(n)]\n    list_value_counts = collections.Counter(my_list)\n\n    df = pd.DataFrame(list(list_value_counts.items()), columns=['Digit', 'Count'])\n    df['Count'] = df['Count'].apply(np.array)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\ncollections, matplotlib, numpy, random\n```python\nimport collections\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\n\n# Constants\nCOLORS = ['red', 'green', 'blue', 'yellow', 'purple']\n\ndef generate_random_color_counts(n):\n    \"\"\"\n    Generate a random list of colors of length n, count the occurrences of each color, \n    convert the counts to a numpy array, and plot a pie chart of the counts.\n\n    Parameters:\n    n (int): The length of the random list of colors.\n\n    Returns:\n    np.array: A numpy array with counts of each color.\n\n    Requirements:\n    - collections\n    - matplotlib.pyplot\n    - numpy\n    - random\n\n    Example:\n    >>> arr = generate_random_color_counts(1000)\n    >>> print(arr)\n    >>> plt.pie(arr, labels=COLORS)\n    \"\"\"\n    my_list = [random.choice(COLORS) for _ in range(n)]\n    list_value_counts = collections.Counter(my_list)\n\n    arr = np.array(list(list_value_counts.values()))\n\n    return arr\n```\n"}
{"seed": "{\"task_id\": 575604, \"prompt\": \"def f_575604(my_list):\\n\\t\", \"suffix\": \"\\n\\treturn my_list\", \"canonical_solution\": \"my_list = [el for el, _ in groupby(my_list)]\", \"test_start\": \"\\nfrom itertools import groupby\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert(candidate([1,1,2,2,3,3])) == [1,2,3]\\n\", \"\\n    assert(candidate([])) == []\\n\", \"\\n    assert(candidate([12,20,21,21,21,21,21])) == [12,20,21]\\n\", \"\\n    assert(candidate([0,0,0,0,0,0,0,0,0])) == [0]\\n\", \"\\n    assert(candidate([-100,-99,-98,-98,-91,-91,0])) == [-100,-99,-98,-91,0]\\n\"], \"entry_point\": \"f_575604\", \"intent\": \"\\u0423\\u0434\\u0430\\u043b\\u0438\\u0442\\u044c \\u043f\\u043e\\u0432\\u0442\\u043e\\u0440\\u044f\\u044e\\u0449\\u0438\\u0435\\u0441\\u044f \\u044d\\u043b\\u0435\\u043c\\u0435\\u043d\\u0442\\u044b \\u0432 \\u043e\\u0442\\u0441\\u043e\\u0440\\u0442\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u043d\\u044b\\u043c \\u0441\\u043f\\u0438\\u0441\\u043a\\u0435 `my_list`\", \"library\": [\"itertools\"]}", "generation": "Scenario 1: \npandas, itertools, numpy, matplotlib\n```python\nimport pandas as pd\nfrom itertools import groupby\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef analyze_data(df):\n    \"\"\"\n    This function takes a DataFrame, groups by 'Category' column, removes \n    consecutive duplicates in 'Value' column for each group, calculates the \n    mean of 'Value' column for each group and plots the result.\n    \n    Parameters:\n    df (DataFrame): The input DataFrame. It should have columns 'Category' and 'Value'.\n    \n    Returns:\n    DataFrame: The DataFrame with 'Category' and 'Mean Value' after removing duplicates.\n    \n    Requirements:\n    - pandas\n    - itertools.groupby\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = pd.DataFrame({\n    ...     'Category': ['A', 'A', 'A', 'B', 'B', 'B', 'C', 'C', 'C'],\n    ...     'Value': [1, 1, 2, 2, 2, 3, 3, 3, 4]\n    ... })\n    >>> print(analyze_data(df))\n    \"\"\"\n    data = []\n    for name, group in df.groupby('Category'):\n        group['Value'] = [k for k, _ in groupby(group['Value'])]\n        mean_value = np.mean(group['Value'])\n        data.append([name, mean_value])\n\n    result = pd.DataFrame(data, columns=['Category', 'Mean Value'])\n    result.plot(kind='bar', x='Category', y='Mean Value', legend=False)\n    plt.ylabel('Mean Value')\n    plt.title('Mean Value by Category')\n    plt.show()\n\n    return result\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \ncollections, datetime, itertools, random\n```python\nfrom collections import Counter\nfrom datetime import datetime\nfrom itertools import groupby\nimport random\n\n# List of possible events\nEVENTS = ['login', 'logout', 'purchase', 'search', 'click']\n\n# Constants\nSTART_DATE = datetime(2021, 1, 1)\nEND_DATE = datetime(2022, 1, 1)\nUSER_IDS = range(1, 101)  # 100 users\n\ndef simulate_user_activity():\n    \"\"\"\n    Simulate user activity by generating a list of events for a random user \n    in a random order, removing consecutive duplicates, and return the count \n    of each event type.\n    \n    Returns:\n    Counter: A counter object with event types and their counts.\n    \n    Requirements:\n    - collections.Counter\n    - datetime\n    - itertools.groupby\n    - random\n    \n    Example:\n    >>> print(simulate_user_activity())\n    \"\"\"\n    user_id = random.choice(USER_IDS)\n    date = START_DATE\n    events = []\n\n    while date < END_DATE:\n        event = random.choice(EVENTS)\n        events.append((date, user_id, event))\n        date += datetime.timedelta(minutes=random.randint(1, 60))  # next event in 1-60 minutes\n\n    # Sort events by date and remove consecutive duplicates\n    events.sort()\n    events = [(date, user_id, event) for (date, user_id, event), _ in groupby(events)]\n\n    # Count event types\n    event_counts = Counter(event for _, _, event in events)\n\n    return event_counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \npandas, itertools, numpy, matplotlib\n```python\nimport pandas as pd\nfrom itertools import groupby\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef analyze_sales_data(df):\n    \"\"\"\n    This function takes a DataFrame with sales data, groups by 'Product' column, \n    removes consecutive duplicates in 'Price' column for each group, calculates \n    the mean of 'Price' column for each group and plots the result.\n    \n    Parameters:\n    df (DataFrame): The input DataFrame. It should have columns 'Product' and 'Price'.\n    \n    Returns:\n    DataFrame: The DataFrame with 'Product' and 'Mean Price' after removing duplicates.\n    \n    Requirements:\n    - pandas\n    - itertools.groupby\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = pd.DataFrame({\n    ...     'Product': ['Apple', 'Apple', 'Banana', 'Banana', 'Cherry', 'Cherry'],\n    ...     'Price': [1, 1, 2, 2, 3, 3]\n    ... })\n    >>> print(analyze_sales_data(df))\n    \"\"\"\n    data = []\n    for name, group in df.groupby('Product'):\n        group['Price'] = [k for k, _ in groupby(group['Price'])]\n        mean_price = np.mean(group['Price'])\n        data.append([name, mean_price])\n\n    result = pd.DataFrame(data, columns=['Product', 'Mean Price'])\n    result.plot(kind='bar', x='Product', y='Mean Price', legend=False)\n    plt.ylabel('Mean Price')\n    plt.title('Mean Price by Product')\n    plt.show()\n\n    return result\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nitertools, random, matplotlib\n```python\nfrom itertools import groupby\nimport random\nimport matplotlib.pyplot as plt\n\n# Constants\nNUM_TRIALS = 1000\nVALUES = ['A', 'B', 'C', 'D', 'E']\n\ndef simulate_random_trials():\n    \"\"\"\n    Simulate a series of random trials, remove consecutive duplicates, and \n    plot the frequency of each value.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - itertools.groupby\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> simulate_random_trials()\n    \"\"\"\n    trials = [random.choice(VALUES) for _ in range(NUM_TRIALS)]\n    trials = [k for k, _ in groupby(trials)]\n    frequencies = {value: trials.count(value) for value in VALUES}\n\n    plt.bar(frequencies.keys(), frequencies.values())\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.title('Frequency of Values in Random Trials')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nitertools, os, time\n```python\nfrom itertools import groupby\nimport os\nimport time\n\ndef remove_duplicate_lines(file_name):\n    \"\"\"\n    Remove consecutive duplicate lines from a file.\n    \n    Parameters:\n    file_name (str): The name of the file.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - itertools.groupby\n    - os\n    - time\n    \n    Example:\n    >>> remove_duplicate_lines('my_file.txt')\n    \"\"\"\n    temp_file_name = f'{file_name}_{int(time.time())}.tmp'\n    with open(file_name, 'r') as file, open(temp_file_name, 'w') as temp_file:\n        lines = file.readlines()\n        lines = [k for k, _ in groupby(lines)]\n        temp_file.writelines(lines)\n\n    os.remove(file_name)\n    os.rename(temp_file_name, file_name)\n```\n"}
{"seed": "{\"task_id\": 575604, \"prompt\": \"def f_575604(my_list):\\n\\t\", \"suffix\": \"\\n\\treturn my_list\", \"canonical_solution\": \"my_list = list(set(my_list))\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([1,2,3,4,5]) == [1,2,3,4,5]\\n\", \"\\n    assert sorted(candidate([1,2,3,4,2,3,4])) == [1,2,3,4]\\n\", \"\\n    assert sorted(candidate(['a', 'b', 'bb'])) == ['a', 'b', 'bb']\\n\"], \"entry_point\": \"f_575604\", \"intent\": \"\\u0423\\u0434\\u0430\\u043b\\u0438\\u0442\\u044c \\u043f\\u043e\\u0432\\u0442\\u043e\\u0440\\u044f\\u044e\\u0449\\u0438\\u0435\\u0441\\u044f \\u044d\\u043b\\u0435\\u043c\\u0435\\u043d\\u0442\\u044b \\u0432 \\u043e\\u0442\\u0441\\u043e\\u0440\\u0442\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u043d\\u044b\\u043c \\u0441\\u043f\\u0438\\u0441\\u043a\\u0435 `my_list`\", \"library\": []}", "generation": "Scenario 1:\nnumpy, pandas, random\n```python\nimport numpy as np\nimport pandas as pd\nfrom random import randint\n\n# Constants\nWORDS = ['apple', 'banana', 'cherry', 'date', 'elderberry', 'fig', 'grape', 'honeydew']\n\ndef generate_word_frequency(my_list):\n    \"\"\"\n    Generate a word frequency table from the input list of words. \n\n    Parameters:\n    my_list (list): List of words.\n\n    Returns:\n    DataFrame: A pandas DataFrame with word frequencies.\n\n    Requirements:\n    - numpy\n    - pandas\n    - random\n\n    Example:\n    >>> my_list = [WORDS[randint(0, len(WORDS) - 1)] for _ in range(100)]\n    >>> table = generate_word_frequency(my_list)\n    >>> print(table)\n    \"\"\"\n    unique, counts = np.unique(my_list, return_counts=True)\n    word_freq = pd.DataFrame({'Word': unique, 'Count': counts})\n\n    return word_freq.sort_values('Count', ascending=False).reset_index(drop=True)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncollections, itertools, numpy\n```python\nimport collections\nimport itertools\nimport numpy as np\n\n# Constants\nLETTER_GROUPS = ['aeiou', 'bcdfghjklmnpqrstvwxyz']\n\ndef count_vowel_consonant_pairs(my_str):\n    \"\"\"\n    Count the number of vowel-consonant pairs in a given string.\n    \n    Parameters:\n    my_str (str): The input string.\n\n    Returns:\n    dict: A dictionary with vowel-consonant pairs as keys and their counts as values.\n\n    Requirements:\n    - collections\n    - itertools\n    - numpy\n\n    Example:\n    >>> count_vowel_consonant_pairs('hello world')\n    \"\"\"\n    pairs = list(itertools.product(*LETTER_GROUPS))\n    pair_counts = collections.defaultdict(int)\n\n    for i in range(len(my_str) - 1):\n        pair = tuple(sorted(my_str[i:i+2].lower()))\n        if pair in pairs:\n            pair_counts[pair] += 1\n\n    return pair_counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, pandas, matplotlib.pyplot\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nLABELS = ['A', 'B', 'C', 'D', 'E']\n\ndef visualize_data_distribution(my_list):\n    \"\"\"\n    Visualize the data distribution of a list of labels using a bar chart.\n\n    Parameters:\n    my_list (list): The list of labels.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> my_list = [LABELS[np.random.randint(0, len(LABELS))] for _ in range(100)]\n    >>> visualize_data_distribution(my_list)\n    \"\"\"\n    labels, counts = np.unique(my_list, return_counts=True)\n    df = pd.DataFrame({'Label': labels, 'Count': counts})\n\n    df.plot(kind='bar', x='Label', y='Count', legend=False)\n    plt.title('Data Distribution')\n    plt.xlabel('Label')\n    plt.ylabel('Count')\n    plt.show()\n\n    return None\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, matplotlib.pyplot\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCATEGORIES = ['Red', 'Green', 'Blue']\n\ndef plot_category_distribution(my_list):\n    \"\"\"\n    Plot the distribution of categories in a list using a pie chart.\n\n    Parameters:\n    my_list (list): The list of categories.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> my_list = [CATEGORIES[np.random.randint(0, len(CATEGORIES))] for _ in range(100)]\n    >>> plot_category_distribution(my_list)\n    \"\"\"\n    df = pd.Series(my_list).value_counts()\n    df.plot(kind='pie', autopct='%1.1f%%')\n    plt.title('Category Distribution')\n    plt.ylabel('')\n    plt.show()\n\n    return None\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, collections, matplotlib.pyplot\n```python\nimport numpy as np\nimport collections\nimport matplotlib.pyplot as plt\n\n# Constants\nNUMBERS = list(range(1, 11))\n\ndef plot_number_frequency(my_list):\n    \"\"\"\n    Plot the frequency of numbers in a list using a histogram.\n\n    Parameters:\n    my_list (list): The list of numbers.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - collections\n    - matplotlib.pyplot\n\n    Example:\n    >>> my_list = [NUMBERS[np.random.randint(0, len(NUMBERS))] for _ in range(100)]\n    >>> plot_number_frequency(my_list)\n    \"\"\"\n    counter = collections.Counter(my_list)\n    numbers, counts = zip(*counter.items())\n    plt.bar(numbers, counts)\n    plt.title('Number Frequency')\n    plt.xlabel('Number')\n    plt.ylabel('Frequency')\n    plt.show()\n\n    return None\n```"}
{"seed": "{\"task_id\": 397222, \"prompt\": \"def f_397222(num):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"math.sqrt(num)\", \"test_start\": \"\\nimport math \\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(1) == 1\\n\", \"\\n    assert candidate(144) == 12\\n\", \"\\n    assert candidate(400.0) == 20.0\\n\"], \"entry_point\": \"f_397222\", \"intent\": \"\\u041d\\u0430\\u0439\\u0442\\u0438 \\u043a\\u0432\\u0430\\u0440\\u0430\\u0442\\u043d\\u044b\\u0439 \\u043a\\u043e\\u0440\\u0435\\u043d\\u044c \\u0438\\u0437 `num`\", \"library\": [\"math\"]}", "generation": "Scenario 1: \npandas, math, matplotlib, random\n```python\nimport pandas as pd\nimport math\nimport matplotlib.pyplot as plt\nfrom random import randint, uniform\n\n# Constants\nDATA_SIZE = 100\n\ndef create_and_analyze_sqrt_data():\n    \"\"\"\n    Create a pandas DataFrame with 'num' and 'sqrt_num' columns, then analyze \n    the relationship between them by plotting a scatter plot.\n    \n    Requirements:\n    - pandas\n    - math\n    - matplotlib.pyplot\n    - random\n    \n    Example:\n    >>> create_and_analyze_sqrt_data()\n    \"\"\"\n    # Create a DataFrame\n    data = {'num': [uniform(0, 100) for _ in range(DATA_SIZE)]}\n    df = pd.DataFrame(data)\n    df['sqrt_num'] = df['num'].apply(math.sqrt)\n\n    # Plot the data\n    plt.scatter(df['num'], df['sqrt_num'])\n    plt.xlabel('num')\n    plt.ylabel('sqrt_num')\n    plt.title('Relationship between num and its square root')\n    plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nnumpy, math, matplotlib\n```python\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\n\n# Constants\nARRAY_SIZE = 1000\n\ndef array_sqrt_analysis():\n    \"\"\"\n    Create a numpy array with random values, calculate their square roots, and \n    analyze the relationship by plotting a histogram of the square roots.\n\n    Requirements:\n    - numpy\n    - math\n    - matplotlib.pyplot\n    \n    Example:\n    >>> array_sqrt_analysis()\n    \"\"\"\n    # Create a numpy array\n    arr = np.random.rand(ARRAY_SIZE)\n\n    # Calculate the square roots\n    sqrt_arr = np.sqrt(arr)\n\n    # Plot a histogram of the square roots\n    plt.hist(sqrt_arr, bins=30, edgecolor='black')\n    plt.xlabel('Square root')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of square roots')\n    plt.show()\n\n    return sqrt_arr\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \ncollections, math, statistics\n```python\nimport collections\nimport math\nimport statistics\n\n# Constants\nDATA_SIZE = 100\nMIN_VALUE = 1\nMAX_VALUE = 100\n\ndef analyze_sqrt_distribution():\n    \"\"\"\n    Generate a list of random integers, calculate their square roots, and analyze \n    the distribution by calculating the mean, median, and mode of the square roots.\n\n    Requirements:\n    - collections\n    - math\n    - statistics\n    \n    Example:\n    >>> analyze_sqrt_distribution()\n    \"\"\"\n    # Generate a list of random integers\n    data = [randint(MIN_VALUE, MAX_VALUE) for _ in range(DATA_SIZE)]\n\n    # Calculate the square roots\n    sqrt_data = [math.sqrt(num) for num in data]\n\n    # Calculate the mean, median, and mode\n    mean = statistics.mean(sqrt_data)\n    median = statistics.median(sqrt_data)\n    mode = statistics.mode(sqrt_data)\n\n    return mean, median, mode\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, math, matplotlib, scipy.stats\n```python\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\n\n# Constants\nARRAY_SIZE = 1000\n\ndef sqrt_and_normal_distribution():\n    \"\"\"\n    Generate a numpy array with random values, calculate their square roots, and \n    compare the distribution of the square roots with a normal distribution.\n\n    Requirements:\n    - numpy\n    - math\n    - matplotlib.pyplot\n    - scipy.stats\n    \n    Example:\n    >>> sqrt_and_normal_distribution()\n    \"\"\"\n    # Generate a numpy array\n    arr = np.random.rand(ARRAY_SIZE)\n\n    # Calculate the square roots\n    sqrt_arr = np.sqrt(arr)\n\n    # Plot the distribution of the square roots and a normal distribution for comparison\n    plt.hist(sqrt_arr, bins=30, density=True, alpha=0.5, color='g', edgecolor='black')\n    mu, sigma = stats.norm.fit(sqrt_arr)\n    best_fit_line = stats.norm.pdf(sqrt_arr, mu, sigma)\n    plt.plot(sqrt_arr, best_fit_line, color='r')\n    plt.xlabel('Square root')\n    plt.ylabel('Probability')\n    plt.title('Histogram of square roots vs Normal Distribution')\n    plt.show()\n\n    return sqrt_arr\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnumpy, math, matplotlib, sklearn.preprocessing\n```python\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Constants\nARRAY_SIZE = 1000\n\ndef normalize_and_analyze_sqrt():\n    \"\"\"\n    Generate a numpy array with random values, normalize the values, calculate \n    their square roots, and analyze the relationship by plotting a scatter plot.\n\n    Requirements:\n    - numpy\n    - math\n    - matplotlib.pyplot\n    - sklearn.preprocessing\n    \n    Example:\n    >>> normalize_and_analyze_sqrt()\n    \"\"\"\n    # Generate a numpy array\n    arr = np.random.rand(ARRAY_SIZE)\n\n    # Normalize the data\n    scaler = MinMaxScaler()\n    normalized_arr = scaler.fit_transform(arr.reshape(-1, 1))\n\n    # Calculate the square roots\n    sqrt_arr = np.sqrt(normalized_arr)\n\n    # Plot the data\n    plt.scatter(normalized_arr, sqrt_arr)\n    plt.xlabel('Normalized value')\n    plt.ylabel('Square root')\n    plt.title('Relationship between normalized value and its square root')\n    plt.show()\n\n    return sqrt_arr\n```\n"}
{"seed": "{\"task_id\": 459401, \"prompt\": \"def f_459401(my_number_strs):\\n\\t\", \"suffix\": \"\\n\\treturn my_numbers\", \"canonical_solution\": \"my_numbers = [int(item) for item in my_number_strs]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('123') == [1,2,3]\\n\", \"\\n    assert candidate(['1','2','3']) == [1,2,3]\\n\"], \"entry_point\": \"f_459401\", \"intent\": \"\\u041f\\u0440\\u0435\\u043e\\u0431\\u0440\\u0430\\u0437\\u043e\\u0432\\u0430\\u0442\\u044c \\u0441\\u043f\\u0438\\u0441\\u043e\\u043a \\u0441\\u0442\\u0440\\u043e\\u043a `my_number_strs` \\u0432 \\u0441\\u043f\\u0438\\u0441\\u043e\\u043a \\u0447\\u0438\\u0441\\u0435\\u043b `my_numbers`\", \"library\": []}", "generation": "Scenario 1:\nnumpy, matplotlib, itertools, random\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom itertools import cycle\nfrom random import randint\n\n# Constants\nCOLORS = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n\ndef multi_plot_random_walks(num_walks, steps):\n    \"\"\"\n    Generate multiple random walks and plot them with different colors.\n\n    Parameters:\n    num_walks (int): The number of random walks.\n    steps (int): The number of steps in each walk.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - itertools.cycle\n    - random.randint\n\n    Example:\n    >>> multi_plot_random_walks(5, 100)\n    \"\"\"\n    color_cycle = cycle(COLORS)\n    for _ in range(num_walks):\n        walk = np.cumsum([randint(-1, 1) for _ in range(steps)])\n        plt.plot(walk, next(color_cycle))\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, hashlib, binascii\n```python\nimport os\nimport hashlib\nfrom binascii import hexlify\n\ndef hash_password(password):\n    \"\"\"\n    Hash a password for storing. Saving the salt and hashed password as a string separated by a comma.\n\n    Parameters:\n    password (str): The password to be hashed.\n\n    Returns:\n    str: The salt and hashed password.\n\n    Requirements:\n    - os\n    - hashlib\n    - binascii.hexlify\n\n    Example:\n    >>> hashed_password = hash_password('my_password')\n    >>> print(hashed_password)\n    \"\"\"\n    salt = os.urandom(16)\n    hashed_password = hashlib.pbkdf2_hmac('sha256', password.encode(), salt, 100000)\n    return ','.join([hexlify(salt).decode(), hexlify(hashed_password).decode()])\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, scipy.stats, matplotlib\n```python\nimport numpy as np\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\n\ndef plot_normal_distribution(mu, sigma):\n    \"\"\"\n    Generate and plot a normal distribution with given mean and standard deviation.\n\n    Parameters:\n    mu (float): The mean of the distribution.\n    sigma (float): The standard deviation of the distribution.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_normal_distribution(0, 1)\n    \"\"\"\n    x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n    plt.plot(x, stats.norm.pdf(x, mu, sigma))\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef plot_dataframe_histogram(num_rows):\n    \"\"\"\n    Generate a DataFrame with random data and plot a histogram for each column.\n\n    Parameters:\n    num_rows (int): The number of rows in the DataFrame.\n\n    Returns:\n    DataFrame: A pandas DataFrame with random data.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = plot_dataframe_histogram(100)\n    >>> print(df)\n    \"\"\"\n    df = pd.DataFrame(np.random.rand(num_rows, len(COLUMNS)), columns=COLUMNS)\n    for col in df.columns:\n        df[col].plot.hist(alpha=0.5)\n    plt.show()\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, sklearn.datasets, sklearn.model_selection, sklearn.linear_model, matplotlib\n```python\nimport numpy as np\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\n\ndef perform_linear_regression(num_samples, num_features):\n    \"\"\"\n    Generate a regression problem, split the data, train a linear regression model, and plot the results.\n\n    Parameters:\n    num_samples (int): The number of samples.\n    num_features (int): The number of features.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - sklearn.datasets\n    - sklearn.model_selection\n    - sklearn.linear_model\n    - matplotlib.pyplot\n\n    Example:\n    >>> perform_linear_regression(100, 1)\n    \"\"\"\n    X, y = make_regression(n_samples=num_samples, n_features=num_features, noise=0.1)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    plt.scatter(X_test, y_test, color='blue')\n    plt.plot(X_test, y_pred, color='red')\n    plt.show()\n```"}
{"seed": "{\"task_id\": 235938, \"prompt\": \"def f_235938(s, my_subs):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"re.search(my_subs, s)\", \"test_start\": \"\\nimport re\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('this is a test string','test').group() == 'test'\\n\", \"\\n    assert candidate('this is another test string','thi').group() == 'thi'\\n\", \"\\n    assert candidate('','test') is None\\n\"], \"entry_point\": \"f_235938\", \"intent\": \"\\u041f\\u0440\\u043e\\u0432\\u0435\\u0440\\u0438\\u0442\\u044c \\u0435\\u0441\\u0442\\u044c \\u043b\\u0438 \\u043f\\u043e\\u0434\\u0441\\u0442\\u0440\\u043e\\u043a\\u0430 `my_subs` \\u0432 \\u0441\\u0442\\u0440\\u043e\\u043a\\u0435 `s`\", \"library\": [\"re\"]}", "generation": "Scenario 3: \nre, pandas, matplotlib, numpy\n```python\nimport re\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nKEYWORDS = ['python', 'data', 'machine learning', 'AI', 'deep learning']\n\ndef keyword_analysis(texts):\n    \"\"\"\n    Analyze a list of texts and calculate the frequency of certain keywords \n    using regular expressions. Return a pandas DataFrame with the results and \n    plot the frequency of the keywords.\n    \n    Parameters:\n    texts (list): A list of texts to analyze.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the frequency of the keywords.\n    \n    Requirements:\n    - re\n    - pandas\n    - matplotlib.pyplot\n    - numpy\n    \n    Example:\n    >>> texts = ['I love python and data science', 'Machine learning is awesome', \n    >>>          'AI and deep learning are the future of technology']\n    >>> df = keyword_analysis(texts)\n    >>> print(df)\n    >>> df.plot(kind='bar')\n    \"\"\"\n    keyword_counts = {keyword: 0 for keyword in KEYWORDS}\n\n    for text in texts:\n        for keyword in KEYWORDS:\n            if re.search(keyword, text, re.IGNORECASE):\n                keyword_counts[keyword] += 1\n\n    df = pd.DataFrame.from_dict(keyword_counts, orient='index', columns=['Frequency'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nre, os, datetime, hashlib\n```python\nimport re\nimport os\nfrom datetime import datetime\nimport hashlib\n\n# Constants\nFILENAME_PATTERN = r'\\.txt$'\n\ndef hash_txt_files_in_directory(directory):\n    \"\"\"\n    Traverse a given directory, find all .txt files using regular expressions, \n    calculate a hash (MD5) of each file's contents, and return a dictionary with \n    filenames and their corresponding hashes.\n\n    Parameters:\n    directory (str): The directory to traverse.\n\n    Returns:\n    dict: A dictionary with filenames and their corresponding hashes.\n\n    Requirements:\n    - re\n    - os\n    - datetime\n    - hashlib\n\n    Example:\n    >>> hash_dict = hash_txt_files_in_directory('/path/to/directory')\n    >>> print(hash_dict)\n    \"\"\"\n    hashed_files = {}\n\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if re.search(FILENAME_PATTERN, file):\n                with open(os.path.join(root, file), 'rb') as f:\n                    data = f.read()\n                    file_hash = hashlib.md5(data).hexdigest()\n                    hashed_files[file] = file_hash\n\n    return hashed_files\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nre, urllib.request, bs4, pandas\n```python\nimport re\nimport urllib.request\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\n# Constants\nURL_PATTERN = r'https?://[\\w/:%#\\$&\\?\\(\\)~\\.=\\+\\-]+'\n\ndef scrape_links_from_webpage(url):\n    \"\"\"\n    Scrape a webpage at a given URL, find all links using regular expressions, \n    and return a pandas DataFrame with the links and their text.\n\n    Parameters:\n    url (str): The URL of the webpage to scrape.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the links and their text.\n\n    Requirements:\n    - re\n    - urllib.request\n    - bs4.BeautifulSoup\n    - pandas\n\n    Example:\n    >>> df = scrape_links_from_webpage('https://www.example.com')\n    >>> print(df)\n    \"\"\"\n    with urllib.request.urlopen(url) as response:\n        html = response.read()\n        soup = BeautifulSoup(html, 'html.parser')\n        links = soup.find_all('a')\n\n    link_data = []\n\n    for link in links:\n        href = link.get('href')\n        if re.search(URL_PATTERN, href):\n            link_data.append([href, link.string])\n\n    df = pd.DataFrame(link_data, columns=['URL', 'Text'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 6: \nre, pandas, matplotlib, seaborn\n```python\nimport re\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nVALUE_PATTERN = r'\\b\\d+\\b'\n\ndef plot_value_distribution(dataframe, column):\n    \"\"\"\n    Analyze a pandas DataFrame, extract numeric values from a given column using \n    regular expressions, and plot the distribution of the values using seaborn.\n\n    Parameters:\n    dataframe (DataFrame): The pandas DataFrame to analyze.\n    column (str): The column in the DataFrame to extract values from.\n\n    Returns:\n    None\n\n    Requirements:\n    - re\n    - pandas\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> df = pd.DataFrame({'text': ['There are 10 apples', 'I have 20 dollars', 'He is 30 years old']})\n    >>> plot_value_distribution(df, 'text')\n    \"\"\"\n    values = []\n\n    for text in dataframe[column]:\n        matches = re.findall(VALUE_PATTERN, text)\n        values.extend([int(match) for match in matches])\n\n    plt.figure(figsize=(10, 6))\n    sns.histplot(values, kde=True)\n    plt.title('Value Distribution')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 7: \nre, requests, json, pandas\n```python\nimport re\nimport requests\nimport json\nimport pandas as pd\n\n# Constants\nAPI_ENDPOINT_PATTERN = r'https?://[\\w/:%#\\$&\\?\\(\\)~\\.=\\+\\-]+'\n\ndef call_apis_and_extract_data(api_endpoints, key):\n    \"\"\"\n    Call a list of APIs, extract a specific key from the JSON response using \n    regular expressions, and return a pandas DataFrame with the results.\n\n    Parameters:\n    api_endpoints (list): A list of API endpoints to call.\n    key (str): The key to extract from the JSON response.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the results.\n\n    Requirements:\n    - re\n    - requests\n    - json\n    - pandas\n\n    Example:\n    >>> apis = ['https://api.example.com/data1', 'https://api.example.com/data2']\n    >>> df = call_apis_and_extract_data(apis, 'value')\n    >>> print(df)\n    \"\"\"\n    data = []\n\n    for api in api_endpoints:\n        if re.search(API_ENDPOINT_PATTERN, api):\n            response = requests.get(api)\n            json_data = json.loads(response.text)\n            if key in json_data:\n                data.append(json_data[key])\n\n    df = pd.DataFrame(data, columns=[key])\n\n    return df\n```"}
{"seed": "{\"task_id\": 235938, \"prompt\": \"def f_235938(s, my_subs):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"my_subs in s\", \"test_start\": \"\\nimport re\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('this is a test string','test') == True\\n\", \"\\n    assert candidate('this is another test string','thi') == True\\n\", \"\\n    assert candidate('','test') == False\\n\", \"\\n    assert candidate('checking space', ' ') == True\\n\"], \"entry_point\": \"f_235938\", \"intent\": \"\\u041f\\u0440\\u043e\\u0432\\u0435\\u0440\\u0438\\u0442\\u044c \\u0435\\u0441\\u0442\\u044c \\u043b\\u0438 \\u043f\\u043e\\u0434\\u0441\\u0442\\u0440\\u043e\\u043a\\u0430 `my_subs` \\u0432 \\u0441\\u0442\\u0440\\u043e\\u043a\\u0435 `s`\", \"library\": [\"re\"]}", "generation": "Scenario 1: \npandas, re, numpy, random\n```python\nimport pandas as pd\nimport re\nimport numpy as np\nfrom random import choice\n\n# Constants\nSPECIAL_CHARACTERS = ['!', '@', '#', '$', '%', '^', '&', '*', '(', ')', '_', '+', '=', '-', '/']\n\n# Sample Data\ndata = {\n    'Name': ['John', 'Alice', 'Bob', 'Mike', 'Emma'],\n    'Email': ['john@gmail.com', 'alice@yahoo.com', 'bob@outlook.com', 'mike@live.com', 'emma@aol.com'],\n    'Password': ['password123', 'alice#123', 'bob*123', 'mike_123', 'emma+123']\n}\n\ndef password_strength_check(df):\n    \"\"\"\n    Given a DataFrame with users' names, emails and passwords, check if the \n    passwords meet the following conditions:\n    - Contains at least one special character\n    - Is not similar to the user's name or email\n    - Is not a common password (password123, qwerty, admin, etc.)\n\n    The function returns a new DataFrame with an additional column 'Strong Password' that\n    indicates whether each user's password is strong (True) or not (False).\n\n    Parameters:\n    df (DataFrame): The DataFrame with user data.\n\n    Returns:\n    DataFrame: The updated DataFrame with the 'Strong Password' column.\n\n    Requirements:\n    - pandas\n    - re\n    - numpy\n    - random\n\n    Example:\n    >>> df = pd.DataFrame(data)\n    >>> password_strength_check(df)\n    \"\"\"\n    common_passwords = ['password123', 'qwerty', 'admin', 'password', '123456']\n\n    def is_strong_password(row):\n        name = row['Name'].lower()\n        email = row['Email'].split('@')[0].lower()\n        password = row['Password']\n\n        if any(char in password for char in SPECIAL_CHARACTERS) and \\\n           name not in password.lower() and \\\n           email not in password.lower() and \\\n           password.lower() not in common_passwords:\n            return True\n        else:\n            return False\n\n    df['Strong Password'] = df.apply(is_strong_password, axis=1)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nre, string, pandas\n```python\nimport re\nimport string\nimport pandas as pd\n\n# Constants\nPUNCTUATIONS = string.punctuation\n\ndef count_punctuations(text_series):\n    \"\"\"\n    Given a Series of text data, count the number of punctuations in each text and \n    return a DataFrame with the original text and the count of punctuations.\n\n    Parameters:\n    text_series (Series): The pandas Series containing text data.\n\n    Returns:\n    DataFrame: A DataFrame with the original text and the count of punctuations.\n\n    Requirements:\n    - pandas\n    - re\n    - string\n\n    Example:\n    >>> text_series = pd.Series(['Hello, world!', 'Python is great.', 'How are you?'])\n    >>> count_punctuations(text_series)\n    \"\"\"\n    def count_punctuations_in_text(text):\n        return len(re.findall(f'[{PUNCTUATIONS}]', text))\n\n    df = pd.DataFrame({\n        'Text': text_series,\n        'Punctuation Count': text_series.apply(count_punctuations_in_text)\n    })\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nre, pandas, matplotlib.pyplot\n```python\nimport re\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef plot_word_frequency(text_series, word):\n    \"\"\"\n    Given a Series of text data, count the frequency of a specific word in the text and \n    plot the frequency count.\n\n    Parameters:\n    text_series (Series): The pandas Series containing text data.\n    word (str): The word to count the frequency of.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - re\n    - matplotlib.pyplot\n\n    Example:\n    >>> text_series = pd.Series(['Hello, world!', 'Python is great.', 'How are you?'])\n    >>> plot_word_frequency(text_series, 'is')\n    \"\"\"\n    def count_word_in_text(text):\n        return len(re.findall(rf'\\b{word}\\b', text, re.IGNORECASE))\n\n    df = pd.DataFrame({\n        'Text': text_series,\n        'Word Count': text_series.apply(count_word_in_text)\n    })\n\n    df['Word Count'].plot(kind='bar')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nos, re, pandas\n```python\nimport os\nimport re\nimport pandas as pd\n\ndef search_files(directory, regex):\n    \"\"\"\n    Given a directory, search all files in the directory and its subdirectories that match a regex pattern.\n\n    Parameters:\n    directory (str): The directory to search in.\n    regex (str): The regex pattern to match.\n\n    Returns:\n    DataFrame: A DataFrame with the file paths and file names that match the pattern.\n\n    Requirements:\n    - pandas\n    - re\n    - os\n\n    Example:\n    >>> search_files('./', r'^test_.*\\.py$')\n    \"\"\"\n    matching_files = []\n\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if re.match(regex, file):\n                matching_files.append([os.path.join(root, file), file])\n\n    df = pd.DataFrame(matching_files, columns=['File Path', 'File Name'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nre, pandas, matplotlib.pyplot\n```python\nimport re\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef plot_email_providers(email_series):\n    \"\"\"\n    Given a Series of email addresses, extract the email provider (the part after '@' and before the last '.') \n    and plot a bar chart of the count of each email provider.\n\n    Parameters:\n    email_series (Series): The pandas Series containing email addresses.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - re\n    - matplotlib.pyplot\n\n    Example:\n    >>> email_series = pd.Series(['john@gmail.com', 'alice@yahoo.com', 'bob@outlook.com'])\n    >>> plot_email_providers(email_series)\n    \"\"\"\n    def get_email_provider(email):\n        match = re.search(r'@(.+)\\.', email)\n        return match.group(1) if match else None\n\n    df = pd.DataFrame({\n        'Email': email_series,\n        'Provider': email_series.apply(get_email_provider)\n    })\n\n    df['Provider'].value_counts().plot(kind='bar')\n    plt.show()\n```"}
{"seed": "{\"task_id\": 235938, \"prompt\": \"def f_235938(s, my_subs):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"s.find(my_subs) != -1 \", \"test_start\": \"\\nimport re\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('this is a test string','test') == True\\n\", \"\\n    assert candidate('this is another test string','thi') == True\\n\", \"\\n    assert candidate('','test') == False\\n\", \"\\n    assert candidate('checking space', ' ') == True\\n\"], \"entry_point\": \"f_235938\", \"intent\": \"\\u041f\\u0440\\u043e\\u0432\\u0435\\u0440\\u0438\\u0442\\u044c \\u0435\\u0441\\u0442\\u044c \\u043b\\u0438 \\u043f\\u043e\\u0434\\u0441\\u0442\\u0440\\u043e\\u043a\\u0430 `my_subs` \\u0432 \\u0441\\u0442\\u0440\\u043e\\u043a\\u0435 `s`\", \"library\": [\"re\"]}", "generation": "Scenario 1:\nre, json, os, hashlib\n```python\nimport re\nimport json\nimport os\nfrom hashlib import blake2b\n\n# Constants\nHASH_SECRET_KEY = b'some-secret-key'\n\ndef search_and_hash_in_files(directory, pattern):\n    \"\"\"\n    Search for a pattern in all text files in a given directory and returns a \n    hash of the file content if the pattern is found.\n    \n    Parameters:\n    directory (str): The directory to search the files in.\n    pattern (str): The pattern to search in the files.\n    \n    Returns:\n    json: A json object which contains file names as keys and their respective \n          hashes as values if the pattern is found in the file.\n    \n    Requirements:\n    - re\n    - json\n    - os\n    - hashlib.blake2b\n    \n    Example:\n    >>> search_and_hash_in_files('/home/user/documents', 'python')\n    \"\"\"\n    hash_dict = {}\n\n    for filename in os.listdir(directory):\n        if filename.endswith('.txt'):\n            with open(os.path.join(directory, filename), 'r') as file:\n                content = file.read()\n                if re.search(pattern, content):\n                    hash_object = blake2b(key=HASH_SECRET_KEY)\n                    hash_object.update(content.encode())\n                    hash_dict[filename] = hash_object.hexdigest()\n\n    return json.dumps(hash_dict)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nre, os, shutil, zipfile\n```python\nimport re\nimport os\nimport shutil\nimport zipfile\n\ndef archive_files_with_pattern(directory, pattern, archive_name):\n    \"\"\"\n    Archive all the files in a directory that match a given pattern into a ZIP file.\n    \n    Parameters:\n    directory (str): The directory to search the files in.\n    pattern (str): The pattern to match the file names.\n    archive_name (str): The name of the output archive file.\n    \n    Returns:\n    str: The path of the archive file.\n    \n    Requirements:\n    - re\n    - os\n    - shutil\n    - zipfile\n    \n    Example:\n    >>> archive_files_with_pattern('/home/user/documents', 'report', 'reports.zip')\n    \"\"\"\n    with zipfile.ZipFile(archive_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        for root, dirs, files in os.walk(directory):\n            for file in files:\n                if re.search(pattern, file):\n                    zipf.write(os.path.join(root, file), arcname=os.path.basename(os.path.join(root, file)))\n\n    return os.path.abspath(archive_name)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nre, requests, bs4, csv\n```python\nimport re\nimport requests\nfrom bs4 import BeautifulSoup\nimport csv\n\ndef scrape_and_save_emails(url, output_file):\n    \"\"\"\n    Scrape a webpage for email addresses and save them to a CSV file.\n    \n    Parameters:\n    url (str): The URL of the webpage to scrape.\n    output_file (str): The name of the output CSV file.\n    \n    Returns:\n    int: The number of email addresses found.\n    \n    Requirements:\n    - re\n    - requests\n    - bs4\n    - csv\n    \n    Example:\n    >>> scrape_and_save_emails('http://example.com', 'emails.csv')\n    \"\"\"\n    response = requests.get(url)\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n    emails = re.findall(email_pattern, str(soup))\n\n    with open(output_file, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(['Email'])\n        for email in emails:\n            writer.writerow([email])\n\n    return len(emails)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nre, pandas, matplotlib\n```python\nimport re\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef analyze_and_plot_words(text, pattern, title):\n    \"\"\"\n    Analyze a text for words that match a given pattern and plot a bar chart with \n    the counts of the matching words.\n    \n    Parameters:\n    text (str): The text to analyze.\n    pattern (str): The pattern to match the words.\n    title (str): The title of the plot.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - re\n    - pandas\n    - matplotlib.pyplot\n    \n    Example:\n    >>> analyze_and_plot_words('Some text...', '\\\\b[a-zA-Z]{4,}\\\\b', 'Word Counts')\n    \"\"\"\n    words = re.findall(pattern, text)\n    word_counts = pd.Series(words).value_counts()\n    \n    plt.figure(figsize=(10, 6))\n    word_counts.plot(kind='bar', color='skyblue')\n    plt.title(title)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nre, nltk, wordcloud\n```python\nimport re\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\ndef generate_word_cloud(text, pattern):\n    \"\"\"\n    Generate a word cloud from a text based on words that match a given pattern.\n    \n    Parameters:\n    text (str): The text to generate the word cloud from.\n    pattern (str): The pattern to match the words.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - re\n    - nltk.corpus.stopwords\n    - wordcloud.WordCloud\n    - matplotlib.pyplot\n    \n    Example:\n    >>> generate_word_cloud('Some text...', '\\\\b[a-zA-Z]{4,}\\\\b')\n    \"\"\"\n    words = re.findall(pattern, text)\n    words = [word for word in words if word not in stopwords.words('english')]\n    \n    word_cloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(words))\n    \n    plt.figure(figsize=(10, 6))\n    plt.imshow(word_cloud, interpolation='bilinear')\n    plt.axis('off')\n    plt.show()\n```"}
{"seed": "{\"task_id\": 427942, \"prompt\": \"def f_427942(my_list1, my_list2):\\n\\t\", \"suffix\": \"\\n\\treturn common_items\", \"canonical_solution\": \"count_values_l1 = Counter(my_list1)\\n\\n\\tcount_values_l2 = Counter(my_list2)\\n\\n\\tkeys1 = count_values_l1.keys()\\n\\n\\tkeys2 = count_values_l2.keys()\\n\\n\\tcommon_items = []\\n\\n\\tfor key in keys1:\\n\\n\\t\\tif key in keys2:\\n\\n\\t\\t\\tcommon_items += [key] * min(count_values_l1[key], count_values_l2[key])\\n\\t\", \"test_start\": \"\\nfrom collections import Counter\\n\\ndef check(candidate):\", \"test\": [\"\\n    l1 = []\\n    l2 = []\\n    assert (candidate(l1, l2) == [])\\n\", \"\\n    l1 = [1, 2, 3]\\n    l2 = [4, 5, 6]\\n    assert (candidate(l1, l2)) == []\\n\", \"\\n    l1 = [1, 2, 3]\\n    l2 = [1, 2, 3]\\n    assert (candidate(l1, l2) == [1, 2, 3])\\n\", \"\\n    l1 = [1, 2, 3, 4]\\n    l2 = [3, 4, 5, 6]\\n    assert (candidate(l1, l2) == [3, 4])\\n\", \"\\n    l1 = [1, 2, 3, 4]\\n    l2 = [5, 6, 7, 8]\\n    assert (candidate(l1, l2) != [1])\\n\"], \"entry_point\": \"f_427942\", \"intent\": \"\\u041d\\u0430\\u0439\\u0442\\u0438 \\u043e\\u0431\\u0449\\u0438\\u0435 \\u044d\\u043b\\u0435\\u043c\\u0435\\u043d\\u0442\\u044b `common_items` \\u0441\\u043f\\u0438\\u0441\\u043a\\u0430 `my_list1` \\u0438 \\u0441\\u043f\\u0438\\u0441\\u043a\\u0430 `my_list2` \", \"library\": [\"collections\"]}", "generation": "Scenario 1: \ncollections, itertools, random, matplotlib\n```python\nfrom collections import Counter\nfrom itertools import chain\nimport random\nimport matplotlib.pyplot as plt\n\n# Constants\nFRUITS = ['apple', 'banana', 'cherry', 'date', 'elderberry']\n\ndef common_fruits_distribution(basket1, basket2):\n    \"\"\"\n    Given two baskets of fruits, find common fruits in both baskets and show the \n    distribution of these common fruits in a bar chart.\n\n    Parameters:\n    basket1, basket2 (list): The two baskets of fruits.\n\n    Returns:\n    list: The common fruits.\n\n    Requirements:\n    - collections\n    - itertools\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> basket1 = [random.choice(FRUITS) for _ in range(100)]\n    >>> basket2 = [random.choice(FRUITS) for _ in range(100)]\n    >>> common_fruits_distribution(basket1, basket2)\n    \"\"\"\n    counter1 = Counter(basket1)\n    counter2 = Counter(basket2)\n    \n    common_fruits = list((counter1 & counter2).elements())\n    \n    plt.hist(common_fruits, bins=len(FRUITS), edgecolor='black')\n    plt.title('Distribution of Common Fruits')\n    plt.xlabel('Fruit')\n    plt.ylabel('Count')\n    plt.show()\n    \n    return common_fruits\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncollections, numpy, matplotlib\n```python\nfrom collections import Counter\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef common_and_unique_elements(arr1, arr2):\n    \"\"\"\n    Given two numpy arrays, find the common and unique elements of both arrays, \n    and visualize the count of these elements in a bar chart.\n\n    Parameters:\n    arr1, arr2 (numpy.ndarray): The two numpy arrays.\n\n    Returns:\n    dict: The count of common and unique elements.\n\n    Requirements:\n    - collections\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> arr1 = np.random.randint(1, 10, 20)\n    >>> arr2 = np.random.randint(1, 10, 20)\n    >>> common_and_unique_elements(arr1, arr2)\n    \"\"\"\n    counter1 = Counter(arr1)\n    counter2 = Counter(arr2)\n    \n    common_elements = list((counter1 & counter2).elements())\n    unique_elements = list(((counter1 | counter2) - (counter1 & counter2)).elements())\n    \n    element_counts = {'common': len(common_elements), 'unique': len(unique_elements)}\n    \n    plt.bar(element_counts.keys(), element_counts.values(), color=['blue', 'orange'])\n    plt.title('Count of Common and Unique Elements')\n    plt.ylabel('Count')\n    plt.show()\n    \n    return element_counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ncollections, numpy, matplotlib\n```python\nfrom collections import Counter\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef common_elements_and_their_counts(arr1, arr2):\n    \"\"\"\n    Given two numpy arrays, find the common elements and their counts, \n    and visualize these counts in a bar chart.\n\n    Parameters:\n    arr1, arr2 (numpy.ndarray): The two numpy arrays.\n\n    Returns:\n    dict: The count of common elements.\n\n    Requirements:\n    - collections\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> arr1 = np.random.randint(1, 10, 20)\n    >>> arr2 = np.random.randint(1, 10, 20)\n    >>> common_elements_and_their_counts(arr1, arr2)\n    \"\"\"\n    counter1 = Counter(arr1)\n    counter2 = Counter(arr2)\n    \n    common_elements = counter1 & counter2\n    \n    plt.bar(common_elements.keys(), common_elements.values(), color='green')\n    plt.title('Count of Common Elements')\n    plt.xlabel('Element')\n    plt.ylabel('Count')\n    plt.show()\n    \n    return dict(common_elements)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ncollections, numpy, matplotlib\n```python\nfrom collections import Counter\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef common_elements_distribution(arr1, arr2):\n    \"\"\"\n    Given two numpy arrays, find the common elements and visualize \n    the distribution of these elements in a histogram.\n\n    Parameters:\n    arr1, arr2 (numpy.ndarray): The two numpy arrays.\n\n    Returns:\n    list: The common elements.\n\n    Requirements:\n    - collections\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> arr1 = np.random.randint(1, 10, 20)\n    >>> arr2 = np.random.randint(1, 10, 20)\n    >>> common_elements_distribution(arr1, arr2)\n    \"\"\"\n    counter1 = Counter(arr1)\n    counter2 = Counter(arr2)\n    \n    common_elements = list((counter1 & counter2).elements())\n    \n    plt.hist(common_elements, bins=np.max(common_elements), edgecolor='black')\n    plt.title('Distribution of Common Elements')\n    plt.xlabel('Element')\n    plt.ylabel('Count')\n    plt.show()\n    \n    return common_elements\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\ncollections, pandas, matplotlib\n```python\nfrom collections import Counter\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nANIMALS = ['dog', 'cat', 'rabbit', 'hamster', 'goldfish']\n\ndef common_pets_distribution(df1, df2):\n    \"\"\"\n    Given two pandas dataframes containing pet data, find the common pets in both dataframes \n    and show the distribution of these common pets in a bar chart.\n\n    Parameters:\n    df1, df2 (pandas.DataFrame): The two dataframes containing pet data.\n\n    Returns:\n    list: The common pets.\n\n    Requirements:\n    - collections\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> df1 = pd.DataFrame({'Pet': [random.choice(ANIMALS) for _ in range(100)]})\n    >>> df2 = pd.DataFrame({'Pet': [random.choice(ANIMALS) for _ in range(100)]})\n    >>> common_pets_distribution(df1, df2)\n    \"\"\"\n    counter1 = Counter(df1['Pet'])\n    counter2 = Counter(df2['Pet'])\n    \n    common_pets = list((counter1 & counter2).elements())\n    \n    plt.hist(common_pets, bins=len(ANIMALS), edgecolor='black')\n    plt.title('Distribution of Common Pets')\n    plt.xlabel('Pet')\n    plt.ylabel('Count')\n    plt.show()\n    \n    return common_pets\n```"}
{"seed": "{\"task_id\": 427942, \"prompt\": \"def f_427942(my_list1, my_list2):\\n\\t\", \"suffix\": \"\\n\\treturn common_items\", \"canonical_solution\": \"common_items = list(set(my_list1) & set(my_list2))\", \"test_start\": \"\\nfrom collections import Counter\\n\\ndef check(candidate):\", \"test\": [\"\\n    l1 = []\\n    l2 = []\\n    assert (candidate(l1, l2) == [])\\n\", \"\\n    l1 = [1, 2, 3]\\n    l2 = [4, 5, 6]\\n    assert (candidate(l1, l2)) == []\\n\", \"\\n    l1 = [1, 2, 3]\\n    l2 = [1, 2, 3]\\n    assert (candidate(l1, l2) == [1, 2, 3])\\n\", \"\\n    l1 = [1, 2, 3, 4]\\n    l2 = [3, 4, 5, 6]\\n    assert (candidate(l1, l2) == [3, 4])\\n\", \"\\n    l1 = [1, 2, 3, 4]\\n    l2 = [5, 6, 7, 8]\\n    assert (candidate(l1, l2) != [1])\\n\"], \"entry_point\": \"f_427942\", \"intent\": \"\\u041d\\u0430\\u0439\\u0442\\u0438 \\u043e\\u0431\\u0449\\u0438\\u0435 \\u044d\\u043b\\u0435\\u043c\\u0435\\u043d\\u0442\\u044b `common_items` \\u0441\\u043f\\u0438\\u0441\\u043a\\u0430 `my_list1` \\u0438 \\u0441\\u043f\\u0438\\u0441\\u043a\\u0430 `my_list2` \", \"library\": [\"collections\"]}", "generation": "Scenario 1:\nnumpy, pandas, matplotlib, collections\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n\n# Constants\nCATEGORIES = [\"Books\", \"Electronics\", \"Clothing\", \"Sports\", \"Groceries\"]\n\ndef common_purchases(customer1_purchases, customer2_purchases):\n    \"\"\"\n    Find common purchases between two customers and plot the frequency distribution \n    of these common purchases.\n\n    Parameters:\n    customer1_purchases (list): The list of purchases for customer 1.\n    customer2_purchases (list): The list of purchases for customer 2.\n\n    Returns:\n    list: The list of common purchases.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - collections.Counter\n\n    Example:\n    >>> customer1 = ['Books', 'Electronics', 'Clothing', 'Books']\n    >>> customer2 = ['Electronics', 'Sports', 'Books', 'Groceries', 'Books']\n    >>> common_purchases(customer1, customer2)\n    \"\"\"\n    common_items = list(set(customer1_purchases) & set(customer2_purchases))\n\n    common_items_count = Counter(common_items)\n\n    # Create DataFrame from the Counter\n    df = pd.DataFrame.from_dict(common_items_count, orient='index').reset_index()\n\n    # Rename the columns\n    df = df.rename(columns={'index':'item', 0:'count'})\n\n    # Plot the frequency distribution of the common purchases\n    df.plot.bar(x='item', y='count', rot=0)\n\n    return common_items\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, pandas, matplotlib, collections\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n\n# Constants\nSTATES = ['New York', 'California', 'Texas', 'Florida', 'Washington']\n\ndef common_visited_states(user1_states, user2_states):\n    \"\"\"\n    Find common states visited between two users and plot the frequency distribution \n    of these common visited states.\n\n    Parameters:\n    user1_states (list): The list of states visited by user 1.\n    user2_states (list): The list of states visited by user 2.\n\n    Returns:\n    list: The list of common visited states.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - collections.Counter\n\n    Example:\n    >>> user1 = ['New York', 'California', 'Texas', 'New York']\n    >>> user2 = ['California', 'Florida', 'New York', 'Washington', 'New York']\n    >>> common_visited_states(user1, user2)\n    \"\"\"\n    common_states = list(set(user1_states) & set(user2_states))\n\n    common_states_count = Counter(common_states)\n\n    # Create DataFrame from the Counter\n    df = pd.DataFrame.from_dict(common_states_count, orient='index').reset_index()\n\n    # Rename the columns\n    df = df.rename(columns={'index':'state', 0:'count'})\n\n    # Plot the frequency distribution of the common visited states\n    df.plot.bar(x='state', y='count', rot=0)\n\n    return common_states\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nrandom, collections, matplotlib, pandas\n```python\nimport random\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants\nFRUITS = ['Apple', 'Banana', 'Cherry', 'Date', 'Elderberry']\n\ndef common_fruits(basket1, basket2):\n    \"\"\"\n    Find common fruits in two baskets and plot the frequency distribution \n    of these common fruits.\n\n    Parameters:\n    basket1 (list): The list of fruits in basket 1.\n    basket2 (list): The list of fruits in basket 2.\n\n    Returns:\n    list: The list of common fruits.\n\n    Requirements:\n    - random\n    - collections.Counter\n    - matplotlib.pyplot\n    - pandas\n\n    Example:\n    >>> basket1 = ['Apple', 'Banana', 'Cherry', 'Apple']\n    >>> basket2 = ['Banana', 'Date', 'Apple', 'Elderberry', 'Apple']\n    >>> common_fruits(basket1, basket2)\n    \"\"\"\n    common_fruits = list(set(basket1) & set(basket2))\n\n    common_fruits_count = Counter(common_fruits)\n\n    # Create DataFrame from the Counter\n    df = pd.DataFrame.from_dict(common_fruits_count, orient='index').reset_index()\n\n    # Rename the columns\n    df = df.rename(columns={'index':'fruit', 0:'count'})\n\n    # Plot the frequency distribution of the common fruits\n    df.plot.bar(x='fruit', y='count', rot=0)\n\n    return common_fruits\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nrandom, collections, matplotlib, pandas\n```python\nimport random\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants\nCOUNTRIES = ['USA', 'UK', 'Germany', 'France', 'Canada']\n\ndef common_countries(traveler1, traveler2):\n    \"\"\"\n    Find common countries visited by two travelers and plot the frequency distribution \n    of these common countries.\n\n    Parameters:\n    traveler1 (list): The list of countries visited by traveler 1.\n    traveler2 (list): The list of countries visited by traveler 2.\n\n    Returns:\n    list: The list of common countries.\n\n    Requirements:\n    - random\n    - collections.Counter\n    - matplotlib.pyplot\n    - pandas\n\n    Example:\n    >>> traveler1 = ['USA', 'UK', 'Germany', 'USA']\n    >>> traveler2 = ['UK', 'France', 'USA', 'Canada', 'USA']\n    >>> common_countries(traveler1, traveler2)\n    \"\"\"\n    common_countries = list(set(traveler1) & set(traveler2))\n\n    common_countries_count = Counter(common_countries)\n\n    # Create DataFrame from the Counter\n    df = pd.DataFrame.from_dict(common_countries_count, orient='index').reset_index()\n\n    # Rename the columns\n    df = df.rename(columns={'index':'country', 0:'count'})\n\n    # Plot the frequency distribution of the common countries\n    df.plot.bar(x='country', y='count', rot=0)\n\n    return common_countries\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nrandom, collections, matplotlib, pandas\n```python\nimport random\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants\nCARS = ['Ford', 'Toyota', 'BMW', 'Mercedes', 'Audi']\n\ndef common_cars(driver1, driver2):\n    \"\"\"\n    Find common cars driven by two drivers and plot the frequency distribution \n    of these common cars.\n\n    Parameters:\n    driver1 (list): The list of cars driven by driver 1.\n    driver2 (list): The list of cars driven by driver 2.\n\n    Returns:\n    list: The list of common cars.\n\n    Requirements:\n    - random\n    - collections.Counter\n    - matplotlib.pyplot\n    - pandas\n\n    Example:\n    >>> driver1 = ['Ford', 'Toyota', 'BMW', 'Ford']\n    >>> driver2 = ['Toyota', 'Mercedes', 'Ford', 'Audi', 'Ford']\n    >>> common_cars(driver1, driver2)\n    \"\"\"\n    common_cars = list(set(driver1) & set(driver2))\n\n    common_cars_count = Counter(common_cars)\n\n    # Create DataFrame from the Counter\n    df = pd.DataFrame.from_dict(common_cars_count, orient='index').reset_index()\n\n    # Rename the columns\n    df = df.rename(columns={'index':'car', 0:'count'})\n\n    # Plot the frequency distribution of the common cars\n    df.plot.bar(x='car', y='count', rot=0)\n\n    return common_cars\n```\n"}
{"seed": "{\"task_id\": 414593, \"prompt\": \"def f_414593(file_path):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"os.path.exists(file_path)\", \"test_start\": \"\\nimport os\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(\\\"random_script.ipynb\\\") == False\\n\", \"\\n    with open('existing_script.sh', 'w') as fw: \\n        fw.write(\\\"echo 'hello!'\\\")\\n    assert candidate('existing_script.sh') == True\\n\"], \"entry_point\": \"f_414593\", \"intent\": \"\\u041f\\u0440\\u043e\\u0432\\u0435\\u0440\\u0438\\u0442\\u044c \\u0441\\u0443\\u0449\\u0435\\u0441\\u0442\\u0432\\u0443\\u0435\\u0442 \\u043b\\u0438 \\u0444\\u0430\\u0439\\u043b `file_path`\", \"library\": [\"os\"]}", "generation": "Scenario 1:\nos, pandas, matplotlib, re, datetime\n```python\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport re\nfrom datetime import datetime\n\n# Constants\nALLOWED_FILE_EXTENSIONS = ['.csv', '.xlsx']\n\ndef analyze_sales(file_path):\n    \"\"\"\n    Check if a sales data file exists and if it does, load the data, perform some basic analysis and plot a sales trend.\n\n    Parameters:\n    file_path (str): The path of the sales data file.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the sales data.\n    str: The result of the analysis.\n    Plot: The sales trend plot.\n\n    Requirements:\n    - os\n    - pandas\n    - matplotlib\n    - re\n    - datetime\n\n    Example:\n    >>> analyze_sales('sales_data.csv')\n    \"\"\"\n    if not os.path.exists(file_path):\n        return 'File does not exist.'\n\n    _, file_extension = os.path.splitext(file_path)\n    if file_extension not in ALLOWED_FILE_EXTENSIONS:\n        return 'Unsupported file extension.'\n\n    if file_extension == '.csv':\n        sales_data = pd.read_csv(file_path)\n    elif file_extension == '.xlsx':\n        sales_data = pd.read_excel(file_path)\n    \n    # Perform some basic analysis\n    total_sales = sales_data['Sales'].sum()\n    best_month = sales_data.loc[sales_data['Sales'].idxmax()]['Month']\n\n    # Plot sales trend\n    sales_data.plot(x='Month', y='Sales', kind='line')\n    plt.title('Sales Trend')\n    plt.show()\n\n    return sales_data, f'Total sales: {total_sales}. Best sales month: {best_month}.'\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, shutil, glob, datetime\n```python\nimport os\nimport shutil\nimport glob\nfrom datetime import datetime\n\n# Constants\nBACKUP_DIR = '/backup/'\n\ndef backup_files(directory):\n    \"\"\"\n    Check if a directory exists and if it does, backup all the files in the directory to a backup directory.\n\n    Parameters:\n    directory (str): The path of the directory to backup.\n\n    Returns:\n    str: The result of the backup operation.\n\n    Requirements:\n    - os\n    - shutil\n    - glob\n    - datetime\n\n    Example:\n    >>> backup_files('/data/')\n    \"\"\"\n    if not os.path.exists(directory):\n        return 'Directory does not exist.'\n\n    backup_subdir = os.path.join(BACKUP_DIR, datetime.now().strftime('%Y%m%d%H%M%S'))\n\n    # Create a new backup subdirectory\n    os.makedirs(backup_subdir, exist_ok=True)\n\n    # Copy all files in the directory to the backup directory\n    files = glob.glob(os.path.join(directory, '*'))\n    for file in files:\n        shutil.copy(file, backup_subdir)\n\n    return f'Backup completed. All files have been backed up to {backup_subdir}.'\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, csv, random, time\n```python\nimport os\nimport csv\nimport random\nimport time\n\n# Constants\nALLOWED_FILE_EXTENSIONS = ['.csv']\n\ndef random_row(file_path):\n    \"\"\"\n    Check if a csv file exists and if it does, select a random row from the file and return it.\n\n    Parameters:\n    file_path (str): The path of the csv file.\n\n    Returns:\n    list: A list containing the data in the selected row.\n\n    Requirements:\n    - os\n    - csv\n    - random\n    - time\n\n    Example:\n    >>> random_row('data.csv')\n    \"\"\"\n    if not os.path.exists(file_path):\n        return 'File does not exist.'\n\n    _, file_extension = os.path.splitext(file_path)\n    if file_extension not in ALLOWED_FILE_EXTENSIONS:\n        return 'Unsupported file extension.'\n\n    with open(file_path, 'r') as f:\n        reader = csv.reader(f)\n        data = list(reader)\n\n    random.seed(time.time())\n    random_row = random.choice(data)\n\n    return random_row\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, sqlite3, pandas, datetime\n```python\nimport os\nimport sqlite3\nimport pandas as pd\nfrom datetime import datetime\n\n# Constants\nDB_EXT = '.db'\n\ndef query_database(db_path, query):\n    \"\"\"\n    Check if a database file exists and if it does, execute a SQL query and return the result.\n\n    Parameters:\n    db_path (str): The path of the database file.\n    query (str): The SQL query to execute.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the result of the query.\n\n    Requirements:\n    - os\n    - sqlite3\n    - pandas\n    - datetime\n\n    Example:\n    >>> query_database('data.db', 'SELECT * FROM sales')\n    \"\"\"\n    if not os.path.exists(db_path):\n        return 'Database file does not exist.'\n\n    _, file_extension = os.path.splitext(db_path)\n    if file_extension != DB_EXT:\n        return 'Unsupported file extension.'\n\n    conn = sqlite3.connect(db_path)\n\n    df = pd.read_sql_query(query, conn)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, hashlib, binascii\n```python\nimport os\nimport hashlib\nimport binascii\n\n# Constants\nSALT_SIZE = 16\n\ndef hash_password(password):\n    \"\"\"\n    Hash a password with a random salt using PBKDF2.\n\n    Parameters:\n    password (str): The password to hash.\n\n    Returns:\n    str: The hashed password.\n\n    Requirements:\n    - os\n    - hashlib\n    - binascii\n\n    Example:\n    >>> hash_password('password123')\n    \"\"\"\n    salt = os.urandom(SALT_SIZE)\n    hashed_password = hashlib.pbkdf2_hmac('sha256', password.encode(), salt, 100000)\n    hashed_password = binascii.hexlify(hashed_password).decode()\n\n    return hashed_password\n```\n"}
{"seed": "{\"task_id\": 50064, \"prompt\": \"def f_50064(data, file_path):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"\\n\\twith open(file_path, \\\"w\\\") as file:\\n\\t\\tprint(data, file=file)\\n\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    names = ['Jessa', 'Eric', 'Bob']\\n    file_path = './test_1.txt'\\n    candidate(names, file_path)\\n\\n    with open(file_path, 'r') as fr:\\n        lines = fr.readlines()\\n    assert lines == [\\\"['Jessa', 'Eric', 'Bob']\\\\n\\\"]\\n\"], \"entry_point\": \"f_50064\", \"intent\": \"\\u0417\\u0430\\u043f\\u0438\\u0441\\u0430\\u0442\\u044c \\u0434\\u0430\\u043d\\u043d\\u044b\\u0435 `data` \\u0432 \\u0444\\u0430\\u0439\\u043b `file_path`\", \"library\": []}", "generation": "Scenario 1:\npandas, json, os, datetime\n```python\nimport pandas as pd\nimport json\nimport os\nfrom datetime import datetime\n\n# Constants\nDATE_FORMAT = \"%Y-%m-%d %H:%M:%S\"\n\ndef convert_json_to_csv(json_file_path, csv_file_path):\n    \"\"\"\n    Convert a JSON file to a CSV file. The original JSON file is deleted after successful conversion.\n\n    Parameters:\n    json_file_path (str): Path of the JSON file.\n    csv_file_path (str): Path where the CSV file will be saved.\n\n    Returns:\n    bool: Returns True if successful, False otherwise.\n\n    Requirements:\n    - pandas\n    - json\n    - os\n    - datetime\n\n    Example:\n    >>> convert_json_to_csv('data.json', 'data.csv')\n    \"\"\"\n    try:\n        with open(json_file_path, 'r') as file:\n            data = json.load(file)\n\n        df = pd.DataFrame(data)\n        df.to_csv(csv_file_path, index=False)\n\n        os.remove(json_file_path)\n\n        return True\n    except Exception as e:\n        print(f\"Error occurred at {datetime.now().strftime(DATE_FORMAT)}: {str(e)}\")\n        \n        return False\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, json, matplotlib, seaborn\n```python\nimport pandas as pd\nimport json\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef plot_json_data(json_file_path, x_field, y_field):\n    \"\"\"\n    Plot a scatter plot of two numeric fields in a JSON data file.\n\n    Parameters:\n    json_file_path (str): Path of the JSON file.\n    x_field (str): Field to be used for x-axis.\n    y_field (str): Field to be used for y-axis.\n\n    Returns:\n    None.\n\n    Requirements:\n    - pandas\n    - json\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> plot_json_data('data.json', 'age', 'income')\n    \"\"\"\n    with open(json_file_path, 'r') as file:\n        data = json.load(file)\n\n    df = pd.DataFrame(data)\n\n    plt.figure(figsize=(10, 6))\n    sns.scatterplot(x=df[x_field], y=df[y_field])\n    plt.title(f'Scatter plot of {x_field} vs {y_field}')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, json, sklearn, matplotlib\n```python\nimport pandas as pd\nimport json\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\n\ndef train_and_plot(json_file_path, x_field, y_field):\n    \"\"\"\n    Train a linear regression model with x_field as predictor and y_field as target,\n    and plot the regression line against the training data.\n\n    Parameters:\n    json_file_path (str): Path of the JSON file.\n    x_field (str): Field to be used as predictor.\n    y_field (str): Field to be used as target.\n\n    Returns:\n    None.\n\n    Requirements:\n    - pandas\n    - json\n    - sklearn.model_selection\n    - sklearn.linear_model\n    - matplotlib.pyplot\n\n    Example:\n    >>> train_and_plot('data.json', 'age', 'income')\n    \"\"\"\n    with open(json_file_path, 'r') as file:\n        data = json.load(file)\n\n    df = pd.DataFrame(data)\n\n    X = df[[x_field]]\n    y = df[y_field]\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    plt.scatter(X_train, y_train, color='blue')\n    plt.plot(X_train, model.predict(X_train), color='red')\n    plt.title(f'Regression line of {y_field} against {x_field}')\n    plt.xlabel(x_field)\n    plt.ylabel(y_field)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\njson, os, shutil, datetime\n```python\nimport json\nimport os\nimport shutil\nfrom datetime import datetime\n\n# Constants\nDATE_FORMAT = \"%Y-%m-%d %H:%M:%S\"\nARCHIVE_DIR = \"archive\"\n\ndef archive_and_create_new_file(file_path, data):\n    \"\"\"\n    Archive the existing file and create a new file with the given data.\n\n    Parameters:\n    file_path (str): Path of the file to be archived.\n    data (object): Data to be written to the new file.\n\n    Returns:\n    None.\n\n    Requirements:\n    - json\n    - os\n    - shutil\n    - datetime\n\n    Example:\n    >>> data = {\"name\": \"John Doe\", \"age\": 30}\n    >>> archive_and_create_new_file('data.json', data)\n    \"\"\"\n    if os.path.exists(file_path):\n        if not os.path.exists(ARCHIVE_DIR):\n            os.makedirs(ARCHIVE_DIR)\n\n        shutil.move(file_path, os.path.join(ARCHIVE_DIR, f\"{datetime.now().strftime(DATE_FORMAT)}_{os.path.basename(file_path)}\"))\n\n    with open(file_path, 'w') as file:\n        json.dump(data, file)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\njson, pandas, seaborn, matplotlib\n```python\nimport json\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef plot_histogram(json_file_path, field):\n    \"\"\"\n    Plot a histogram of a field in a JSON data file.\n\n    Parameters:\n    json_file_path (str): Path of the JSON file.\n    field (str): Field to be plotted.\n\n    Returns:\n    None.\n\n    Requirements:\n    - json\n    - pandas\n    - seaborn\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_histogram('data.json', 'age')\n    \"\"\"\n    with open(json_file_path, 'r') as file:\n        data = json.load(file)\n\n    df = pd.DataFrame(data)\n\n    plt.figure(figsize=(10, 6))\n    sns.histplot(df[field], kde=True)\n    plt.title(f'Histogram of {field}')\n    plt.show()\n```\n"}
{"seed": "{\"task_id\": 50064, \"prompt\": \"def f_50064(data, file_path):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"\\n\\twith open(file_path, mode='a', encoding='utf-8') as f:\\n\\t\\tprint(data, file=f)\\n\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    names = ['Jessa', 'Eric', 'Bob']\\n    file_path = './test_1.txt'\\n    candidate(names, file_path)\\n\\n    with open(file_path, 'r') as fr:\\n        lines = fr.readlines()\\n    assert lines == [\\\"['Jessa', 'Eric', 'Bob']\\\\n\\\"]\\n\"], \"entry_point\": \"f_50064\", \"intent\": \"\\u0417\\u0430\\u043f\\u0438\\u0441\\u0430\\u0442\\u044c \\u0434\\u0430\\u043d\\u043d\\u044b\\u0435 `data` \\u0432 \\u0444\\u0430\\u0439\\u043b `file_path`\", \"library\": []}", "generation": "Scenario 1:\npandas, json, os\n```python\nimport pandas as pd\nimport json\nimport os\n\ndef save_data_to_json(data, file_path):\n    \"\"\"\n    Save a pandas DataFrame to a JSON file.\n    \n    Parameters:\n    data (DataFrame): The pandas DataFrame.\n    file_path (str): The path to the file.\n\n    Requirements:\n    - pandas\n    - json\n    - os\n\n    Example:\n    >>> df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n    >>> save_data_to_json(df, './test.json')\n    \"\"\"\n    if not os.path.exists(os.path.dirname(file_path)):\n        os.makedirs(os.path.dirname(file_path))\n\n    data.to_json(file_path, orient=\"records\")\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, pandas, matplotlib, os\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\n\ndef plot_and_save_histogram(data, file_path):\n    \"\"\"\n    Plot a histogram of a numpy array and save it to a file.\n    \n    Parameters:\n    data (numpy.array): The numpy array.\n    file_path (str): The path to the file.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - os\n\n    Example:\n    >>> data = np.random.normal(size=1000)\n    >>> plot_and_save_histogram(data, './histogram.png')\n    \"\"\"\n    if not os.path.exists(os.path.dirname(file_path)):\n        os.makedirs(os.path.dirname(file_path))\n    \n    plt.hist(data, bins=30, alpha=0.5)\n    plt.savefig(file_path)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, os, csv\n```python\nimport pandas as pd\nimport os\nimport csv\n\ndef save_data_to_csv(data, file_path):\n    \"\"\"\n    Save a pandas DataFrame to a CSV file.\n    \n    Parameters:\n    data (DataFrame): The pandas DataFrame.\n    file_path (str): The path to the file.\n\n    Requirements:\n    - pandas\n    - os\n    - csv\n\n    Example:\n    >>> df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n    >>> save_data_to_csv(df, './test.csv')\n    \"\"\"\n    if not os.path.exists(os.path.dirname(file_path)):\n        os.makedirs(os.path.dirname(file_path))\n\n    data.to_csv(file_path, index=False)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, matplotlib, os\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\ndef plot_and_save_scatter(data, file_path):\n    \"\"\"\n    Plot a scatter plot of a numpy array and save it to a file.\n    \n    Parameters:\n    data (numpy.array): The numpy array.\n    file_path (str): The path to the file.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - os\n\n    Example:\n    >>> data = np.random.rand(100, 2)\n    >>> plot_and_save_scatter(data, './scatter.png')\n    \"\"\"\n    if not os.path.exists(os.path.dirname(file_path)):\n        os.makedirs(os.path.dirname(file_path))\n    \n    plt.scatter(data[:,0], data[:,1])\n    plt.savefig(file_path)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, json, csv, os\n```python\nimport pandas as pd\nimport json\nimport csv\nimport os\n\ndef convert_json_to_csv(json_file_path, csv_file_path):\n    \"\"\"\n    Convert a JSON file to a CSV file.\n    \n    Parameters:\n    json_file_path (str): The path to the JSON file.\n    csv_file_path (str): The path to the CSV file.\n\n    Requirements:\n    - pandas\n    - json\n    - csv\n    - os\n\n    Example:\n    >>> convert_json_to_csv('./test.json', './test.csv')\n    \"\"\"\n    if not os.path.exists(os.path.dirname(csv_file_path)):\n        os.makedirs(os.path.dirname(csv_file_path))\n\n    data = pd.read_json(json_file_path)\n    data.to_csv(csv_file_path, index=False)\n```"}
{"seed": "{\"task_id\": 507330, \"prompt\": \"def f_507330(my_dict, val):\\n\\t\", \"suffix\": \"\\n\\treturn my_key\", \"canonical_solution\": \"my_key = None\\n\\n\\tfor k, v in my_dict.items():\\n\\n\\t\\tif v == val:\\n\\n\\t\\t\\tmy_key = k\\n\\n\\t\\t\\tbreak\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate({1:\\\"abc\\\", 5:\\\"klm\\\", 2:\\\"pqr\\\"}, \\\"pqr\\\") == 2\\n\", \"\\n    assert candidate({1:\\\"abc\\\", 5:\\\"klm\\\", 2:\\\"pqr\\\"}, \\\"tuv\\\") == None\\n\", \"\\n    assert (candidate({4.221:\\\"uwv\\\", -1.009:\\\"pow\\\"}, \\\"uwv\\\") - 4.221 ) < 1e-6\\n\", \"\\n    assert candidate({\\\"as2q\\\":\\\"piqr\\\", \\\"#wwq\\\":\\\"say\\\", \\\"Rwc\\\":\\\"koala\\\", \\\"35\\\":\\\"kangaroo\\\"}, \\\"say\\\") == \\\"#wwq\\\"\\n\"], \"entry_point\": \"f_507330\", \"intent\": \"\\u041f\\u043e\\u043b\\u0443\\u0447\\u0438\\u0442\\u044c \\u043a\\u043b\\u044e\\u0447 `my_key` \\u0438\\u0437 \\u0441\\u043b\\u043e\\u0432\\u0430\\u0440\\u044f `my_dict` \\u043f\\u043e \\u0437\\u043d\\u0430\\u0447\\u0435\\u043d\\u0438\\u044e `val` \", \"library\": []}", "generation": "Scenario 1:\npandas, os, datetime, matplotlib\n```python\nimport pandas as pd\nimport os\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\n\n# Path to the directory where the CSV files are stored\nCSV_DIRECTORY = \"/path/to/csv/directory\"\n\ndef plot_sales_from_csv_files(product_id):\n    \"\"\"\n    Read sales data from all CSV files in a directory, find sales for a given product ID and plot \n    the sales over time.\n\n    Parameters:\n    product_id (str): The product ID to look for.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - os\n    - datetime\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_sales_from_csv_files('P123456')\n    \"\"\"\n    sales_data = pd.DataFrame()\n\n    for filename in os.listdir(CSV_DIRECTORY):\n        if filename.endswith(\".csv\"):\n            df = pd.read_csv(os.path.join(CSV_DIRECTORY, filename))\n            sales_data = sales_data.append(df[df['Product ID'] == product_id], ignore_index=True)\n\n    sales_data['Date'] = pd.to_datetime(sales_data['Date'])\n    sales_data.sort_values('Date', inplace=True)\n\n    plt.plot(sales_data['Date'], sales_data['Sales'])\n    plt.title(f'Sales of Product {product_id} Over Time')\n    plt.xlabel('Date')\n    plt.ylabel('Sales')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, pandas, sklearn.preprocessing, sklearn.decomposition\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\n# Constants\nFEATURES = ['Feature1', 'Feature2', 'Feature3', 'Feature4', 'Feature5']\n\ndef perform_pca(data, n_components):\n    \"\"\"\n    Perform Principal Component Analysis (PCA) on a dataset.\n\n    Parameters:\n    data (DataFrame): The data to perform PCA on.\n    n_components (int): The number of principal components to keep.\n\n    Returns:\n    DataFrame: The transformed data.\n\n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.preprocessing\n    - sklearn.decomposition\n\n    Example:\n    >>> data = pd.DataFrame(np.random.rand(100, 5), columns=FEATURES)\n    >>> transformed_data = perform_pca(data, 2)\n    >>> print(transformed_data)\n    \"\"\"\n    data = data[FEATURES]\n    scaler = StandardScaler()\n    scaled_data = scaler.fit_transform(data)\n\n    pca = PCA(n_components=n_components)\n    pca_data = pca.fit_transform(scaled_data)\n\n    pca_data_df = pd.DataFrame(pca_data, columns=[f'PC{i+1}' for i in range(n_components)])\n\n    return pca_data_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, shutil, glob, time\n```python\nimport os\nimport shutil\nimport glob\nimport time\n\n# Constants\nSOURCE_DIRECTORY = \"/path/to/source/directory\"\nDESTINATION_DIRECTORY = \"/path/to/destination/directory\"\nFILE_EXTENSION = \".txt\"\n\ndef move_files_older_than_days(days):\n    \"\"\"\n    Move all files with a certain extension from a source directory to a destination directory if \n    they are older than a certain number of days.\n\n    Parameters:\n    days (int): The number of days.\n\n    Returns:\n    None\n\n    Requirements:\n    - os\n    - shutil\n    - glob\n    - time\n\n    Example:\n    >>> move_files_older_than_days(30)\n    \"\"\"\n    for filename in glob.glob(os.path.join(SOURCE_DIRECTORY, f\"*{FILE_EXTENSION}\")):\n        if os.path.getmtime(filename) < time.time() - days * 86400:\n            shutil.move(filename, DESTINATION_DIRECTORY)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, pandas, seaborn, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nFEATURES = ['Feature1', 'Feature2', 'Feature3', 'Feature4', 'Feature5']\n\ndef plot_correlation_matrix(data):\n    \"\"\"\n    Plot a correlation matrix for a dataset.\n\n    Parameters:\n    data (DataFrame): The data to plot the correlation matrix for.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - pandas\n    - seaborn\n    - matplotlib.pyplot\n\n    Example:\n    >>> data = pd.DataFrame(np.random.rand(100, 5), columns=FEATURES)\n    >>> plot_correlation_matrix(data)\n    \"\"\"\n    correlation_matrix = data[FEATURES].corr()\n\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n    plt.title('Correlation Matrix')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, random, matplotlib, datetime\n```python\nimport pandas as pd\nimport random\nimport matplotlib.pyplot as plt\nfrom datetime import datetime, timedelta\n\n# Constants\nPRODUCTS = ['Product1', 'Product2', 'Product3', 'Product4', 'Product5']\n\ndef generate_and_plot_sales_data(days):\n    \"\"\"\n    Generate and plot random sales data for a list of products over a certain number of days.\n\n    Parameters:\n    days (int): The number of days.\n\n    Returns:\n    DataFrame: The generated sales data.\n\n    Requirements:\n    - pandas\n    - random\n    - matplotlib.pyplot\n    - datetime\n\n    Example:\n    >>> sales_data = generate_and_plot_sales_data(365)\n    >>> print(sales_data)\n    \"\"\"\n    sales_data = []\n\n    for i in range(days):\n        date = (datetime.today() - timedelta(days=i)).date()\n        for product in PRODUCTS:\n            sales = random.randint(0, 100)\n            sales_data.append([date, product, sales])\n\n    sales_data_df = pd.DataFrame(sales_data, columns=['Date', 'Product', 'Sales'])\n    sales_data_df.sort_values('Date', inplace=True)\n\n    for product in PRODUCTS:\n        plt.plot(sales_data_df[sales_data_df['Product'] == product]['Date'], \n                 sales_data_df[sales_data_df['Product'] == product]['Sales'],\n                 label=product)\n\n    plt.title('Sales Over Time')\n    plt.xlabel('Date')\n    plt.ylabel('Sales')\n    plt.legend()\n    plt.show()\n\n    return sales_data_df\n```"}
{"seed": "{\"task_id\": 507330, \"prompt\": \"def f_507330(my_dict, val):\\n\\t\", \"suffix\": \"\\n\\treturn my_key\", \"canonical_solution\": \"reversed_dict = dict(zip(my_dict.values(), my_dict.keys()))\\n\\n\\tmy_key = reversed_dict[val]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate({1:\\\"abc\\\", 5:\\\"klm\\\", 2:\\\"pqr\\\"}, \\\"pqr\\\") == 2\\n\", \"\\n    assert (candidate({4.221:\\\"uwv\\\", -1.009:\\\"pow\\\"}, \\\"uwv\\\") - 4.221 ) < 1e-6\\n\", \"\\n    assert candidate({\\\"as2q\\\":\\\"piqr\\\", \\\"#wwq\\\":\\\"say\\\", \\\"Rwc\\\":\\\"koala\\\", \\\"35\\\":\\\"kangaroo\\\"}, \\\"say\\\") == \\\"#wwq\\\"\\n\"], \"entry_point\": \"f_507330\", \"intent\": \"\\u041f\\u043e\\u043b\\u0443\\u0447\\u0438\\u0442\\u044c \\u043a\\u043b\\u044e\\u0447 `my_key` \\u0438\\u0437 \\u0441\\u043b\\u043e\\u0432\\u0430\\u0440\\u044f `my_dict` \\u043f\\u043e \\u0437\\u043d\\u0430\\u0447\\u0435\\u043d\\u0438\\u044e `val` \", \"library\": []}", "generation": "Scenario 1:\npandas, datetime, random, matplotlib\n```python\nimport pandas as pd\nfrom datetime import datetime\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nPRODUCTS = [\"T-shirt\", \"Shoes\", \"Jeans\", \"Jacket\", \"Hat\", \"Belt\", \"Gloves\"]\nCOLORS = [\"Black\", \"White\", \"Red\", \"Blue\", \"Green\", \"Yellow\", \"Purple\"]\n\ndef generate_inventory_report():\n    \"\"\"\n    Generate a report of product inventory for a list of products in different colors.\n\n    Returns:\n    DataFrame: A pandas DataFrame with inventory information for the products.\n    \n    Requirements:\n    - pandas\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> report = generate_inventory_report()\n    >>> print(report)\n    >>> report['Inventory'].hist()\n    \"\"\"\n    report_data = []\n\n    for product in PRODUCTS:\n        for color in COLORS:\n            inventory = randint(0, 100)\n            report_data.append([product, color, inventory])\n\n    report_df = pd.DataFrame(report_data, columns=['Product', 'Color', 'Inventory'])\n\n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncollections, random, datetime\n```python\nimport collections\nfrom datetime import datetime\nfrom random import choice, randint\n\n# Constants\nACTIVITIES = [\"Running\", \"Swimming\", \"Cycling\", \"Yoga\", \"Weightlifting\"]\nDAYS = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n\ndef generate_weekly_activity_plan():\n    \"\"\"\n    Generate a weekly activity plan with a selection of activities for each day.\n\n    Returns:\n    dict: A dictionary with days of the week as keys and activities as values.\n\n    Requirements:\n    - collections\n    - random\n    - datetime\n    \n    Example:\n    >>> plan = generate_weekly_activity_plan()\n    >>> print(plan)\n    \"\"\"\n    activity_plan = collections.OrderedDict()\n\n    for day in DAYS:\n        activities = [choice(ACTIVITIES) for _ in range(randint(1, 3))]\n        activity_plan[day] = activities\n\n    return activity_plan\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, random, matplotlib\n```python\nimport pandas as pd\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nSTUDENTS = [\"John\", \"Mary\", \"Bob\", \"Alice\", \"Tom\", \"Sara\", \"Jack\"]\nSUBJECTS = [\"Math\", \"English\", \"History\", \"Science\", \"Art\"]\n\ndef generate_student_grades():\n    \"\"\"\n    Generate a report of student grades for a list of subjects.\n\n    Returns:\n    DataFrame: A pandas DataFrame with grades for the students.\n    \n    Requirements:\n    - pandas\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> grades = generate_student_grades()\n    >>> print(grades)\n    >>> grades['Average'].plot(kind='bar')\n    \"\"\"\n    grade_data = []\n\n    for student in STUDENTS:\n        grades = [randint(60, 100) for _ in range(len(SUBJECTS))]\n        average = sum(grades) / len(grades)\n        grade_data.append([student] + grades + [average])\n\n    columns = ['Student'] + SUBJECTS + ['Average']\n    grades_df = pd.DataFrame(grade_data, columns=columns)\n\n    return grades_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ncollections, random, string\n```python\nimport collections\nfrom random import choices\nimport string\n\n# Constants\nLETTERS = string.ascii_lowercase\nWORD_LENGTH = 5\nWORD_COUNT = 10\n\ndef generate_random_words():\n    \"\"\"\n    Generate a list of random words with a specified length.\n\n    Returns:\n    list: A list of random words.\n\n    Requirements:\n    - collections\n    - random\n    - string\n    \n    Example:\n    >>> words = generate_random_words()\n    >>> print(words)\n    \"\"\"\n    words = []\n\n    for _ in range(WORD_COUNT):\n        word = ''.join(choices(LETTERS, k=WORD_LENGTH))\n        words.append(word)\n\n    return words\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, random, matplotlib\n```python\nimport pandas as pd\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nTEAMS = [\"Team A\", \"Team B\", \"Team C\", \"Team D\", \"Team E\"]\nGAMES = 10\n\ndef generate_team_scores():\n    \"\"\"\n    Generate a report of team scores for a number of games.\n\n    Returns:\n    DataFrame: A pandas DataFrame with scores for the teams.\n    \n    Requirements:\n    - pandas\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> scores = generate_team_scores()\n    >>> print(scores)\n    >>> scores.sum().plot(kind='bar')\n    \"\"\"\n    score_data = []\n\n    for _ in range(GAMES):\n        scores = [randint(0, 5) for _ in range(len(TEAMS))]\n        score_data.append(scores)\n\n    scores_df = pd.DataFrame(score_data, columns=TEAMS)\n\n    return scores_df\n```\nAbove is the illustration."}
{"seed": "{\"task_id\": 30653, \"prompt\": \"def f_30653(num):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"num % 2 == 0\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(0) == True\\n\", \"\\n    assert candidate(99) == False\\n\", \"\\n    assert candidate(3) == False\\n\", \"\\n    assert candidate(4) == True\\n\"], \"entry_point\": \"f_30653\", \"intent\": \"\\u041f\\u0440\\u043e\\u0432\\u0435\\u0440\\u0438\\u0442\\u044c \\u0447\\u0438\\u0441\\u043b\\u043e 'num' \\u043d\\u0430 \\u0447\\u0435\\u0442\\u043d\\u043e\\u0441\\u0442\\u044c\", \"library\": []}", "generation": "Scenario 1:\nnumpy, pandas, matplotlib.pyplot\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nNUMBERS = np.arange(1, 101)\n\ndef analyze_even_numbers():\n    \"\"\"\n    Analyze the even numbers in a given range (1-100) and plot their distribution.\n    \n    Returns:\n    int: The count of even numbers.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    \n    Example:\n    >>> even_count = analyze_even_numbers()\n    >>> print(even_count)\n    \"\"\"\n    even_numbers = NUMBERS[NUMBERS % 2 == 0]\n    even_series = pd.Series(even_numbers)\n    even_series.plot(kind='hist', bins=50)\n    plt.show()\n\n    return len(even_numbers)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, scipy.stats, matplotlib.pyplot\n```python\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\n# Constants\nNUMBERS = np.arange(1, 101)\n\ndef plot_normal_distribution():\n    \"\"\"\n    Fit a normal distribution to the even numbers in a given range (1-100) and plot the distribution.\n    \n    Returns:\n    tuple: The mean and standard deviation of the fitted distribution.\n    \n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n    \n    Example:\n    >>> mean, std_dev = plot_normal_distribution()\n    >>> print(mean, std_dev)\n    \"\"\"\n    even_numbers = NUMBERS[NUMBERS % 2 == 0]\n    mu, std = norm.fit(even_numbers)\n\n    plt.hist(even_numbers, bins=25, density=True, alpha=0.6, color='g')\n\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, mu, std)\n    plt.plot(x, p, 'k', linewidth=2)\n    title = \"Fit results: mu = %.2f,  std = %.2f\" % (mu, std)\n    plt.title(title)\n\n    plt.show()\n\n    return mu, std\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, itertools, collections\n```python\nimport numpy as np\nimport itertools\nfrom collections import Counter\n\n# Constants\nNUMBERS = np.arange(1, 11)\n\ndef analyze_even_combinations():\n    \"\"\"\n    Analyze the combinations of 5 numbers (from a range of 1-10), and count how many combinations have an even sum.\n    \n    Returns:\n    int: The count of combinations with even sum.\n    \n    Requirements:\n    - numpy\n    - itertools\n    - collections\n    \n    Example:\n    >>> even_combinations_count = analyze_even_combinations()\n    >>> print(even_combinations_count)\n    \"\"\"\n    combinations = list(itertools.combinations(NUMBERS, 5))\n    sums = [sum(combination) for combination in combinations]\n    counter = Counter([s % 2 == 0 for s in sums])\n    return counter[True]\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, scipy.optimize, matplotlib.pyplot\n```python\nimport numpy as np\nfrom scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\n\n# Constants\nNUMBERS = np.arange(1, 21)\n\ndef fit_curve():\n    \"\"\"\n    Fit a curve (polynomial of degree 2) to the even numbers in a given range (1-20) using the least squares method, \n    and plot the original data and the fitted curve.\n    \n    Returns:\n    ndarray: The optimal values for the parameters so that the sum of the squared residuals is minimized.\n    \n    Requirements:\n    - numpy\n    - scipy.optimize\n    - matplotlib.pyplot\n    \n    Example:\n    >>> optimal_parameters = fit_curve()\n    >>> print(optimal_parameters)\n    \"\"\"\n    even_numbers = NUMBERS[NUMBERS % 2 == 0]\n\n    def func(x, a, b, c):\n        return a * x**2 + b * x + c\n\n    popt, pcov = curve_fit(func, even_numbers, even_numbers**2)\n\n    plt.figure()\n    plt.plot(even_numbers, even_numbers**2, 'ko', label=\"Original Data\")\n    plt.plot(even_numbers, func(even_numbers, *popt), 'r-', label=\"Fitted Curve\")\n    plt.legend()\n    plt.show()\n\n    return popt\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, sympy\n```python\nimport numpy as np\nfrom sympy import *\n\n# Constants\nNUMBERS = np.arange(1, 101)\n\ndef count_prime_in_evens():\n    \"\"\"\n    Count the number of prime numbers in the even numbers in a given range (1-100).\n    \n    Returns:\n    int: The count of prime numbers.\n    \n    Requirements:\n    - numpy\n    - sympy\n    \n    Example:\n    >>> prime_count = count_prime_in_evens()\n    >>> print(prime_count)\n    \"\"\"\n    even_numbers = NUMBERS[NUMBERS % 2 == 0]\n    primes = [num for num in even_numbers if isprime(num)]\n    return len(primes)\n```\n"}
{"seed": "{\"task_id\": 299026, \"prompt\": \"def f_299026(my_list):\\n\\t\", \"suffix\": \"\\n\\treturn my_list\", \"canonical_solution\": \"my_list.reverse()\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([1, 2, 3]) == [3, 2, 1]\\n\", \"\\n    assert candidate([1, 2, 3, 4, 5]) == [5, 4, 3, 2, 1]\\n\", \"\\n    assert candidate([1, 1, 0, 1]) == [1, 0, 1, 1]\\n\"], \"entry_point\": \"f_299026\", \"intent\": \"\\u041f\\u0435\\u0440\\u0435\\u0432\\u0435\\u0440\\u043d\\u0443\\u0442\\u044c \\u0441\\u043f\\u0438\\u0441\\u043e\\u043a `my_list`\", \"library\": []}", "generation": "Scenario 1: \nnumpy, statistics, matplotlib, pandas\n```python\nimport numpy as np\nimport statistics\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants\nCOLUMN_NAMES = ['Value']\n\ndef analyze_and_plot(my_list):\n    \"\"\"\n    Analyze a list of numeric values, calculate mean, median and standard deviation, \n    then plot a histogram of the values and a box plot of the statistics.\n    \n    Parameters:\n    my_list (list): The list of numeric values to analyze.\n    \n    Returns:\n    dict: A dictionary with mean, median and standard deviation.\n    \n    Requirements:\n    - numpy\n    - statistics\n    - matplotlib.pyplot\n    - pandas\n    \n    Example:\n    >>> analyze_and_plot([1, 2, 3, 4, 5])\n    \"\"\"\n    # Calculate statistics\n    mean_val = np.mean(my_list)\n    median_val = np.median(my_list)\n    std_dev = np.std(my_list)\n\n    # Plot histogram\n    plt.hist(my_list, bins='auto')\n    plt.title('Histogram of Values')\n    plt.show()\n\n    # Plot box plot\n    df = pd.DataFrame(my_list, columns=COLUMN_NAMES)\n    df.boxplot(column=COLUMN_NAMES)\n    plt.title('Box Plot of Values')\n    plt.show()\n    \n    return {'mean': mean_val, 'median': median_val, 'std_dev': std_dev}\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncollections, operator, itertools\n```python\nfrom collections import Counter\nimport operator\nimport itertools\n\ndef most_common_elements(my_list, n):\n    \"\"\"\n    Return the n most common elements and their counts from the list.\n\n    Parameters:\n    my_list (list): The list to analyze.\n    n (int): The number of common elements to return.\n\n    Returns:\n    list: A list of tuples where the first element of the tuple is the element \n          and the second element is the count.\n\n    Requirements:\n    - collections\n    - operator\n    - itertools\n\n    Example:\n    >>> most_common_elements(['apple', 'banana', 'apple', 'orange', 'banana', 'banana'], 2)\n    \"\"\"\n    counter = Counter(my_list)\n    most_common = counter.most_common(n)\n    return most_common\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, math, matplotlib\n```python\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\n\n# Constants\nX_VALUES = np.linspace(-10, 10, 100)\n\ndef plot_functions(my_list):\n    \"\"\"\n    Given a list of functions, plot them on the same graph.\n\n    Parameters:\n    my_list (list): The list of functions to plot.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - math\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_functions([math.sin, math.cos, math.tan])\n    \"\"\"\n    for func in my_list:\n        y_values = [func(x) for x in X_VALUES]\n        plt.plot(X_VALUES, y_values)\n    \n    plt.legend([func.__name__ for func in my_list])\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nrequests, json, csv, os\n```python\nimport requests\nimport json\nimport csv\nimport os\n\n# Constants\nAPI_URL = 'http://api.openweathermap.org/data/2.5/weather'\nAPI_KEY = 'your_api_key'\nOUTPUT_FILE = 'weather_data.csv'\n\ndef fetch_and_write_weather_data(my_list):\n    \"\"\"\n    Given a list of city names, fetch weather data for each city using the OpenWeatherMap API,\n    and write the data to a CSV file.\n\n    Parameters:\n    my_list (list): The list of city names.\n\n    Returns:\n    str: The path to the output CSV file.\n\n    Requirements:\n    - requests\n    - json\n    - csv\n    - os\n\n    Example:\n    >>> fetch_and_write_weather_data(['London', 'Paris', 'New York'])\n    \"\"\"\n    with open(OUTPUT_FILE, 'w', newline='') as csvfile:\n        fieldnames = ['city', 'temperature', 'humidity']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        \n        writer.writeheader()\n\n        for city in my_list:\n            params = {'q': city, 'appid': API_KEY}\n            response = requests.get(API_URL, params=params)\n            data = response.json()\n            writer.writerow({'city': city, 'temperature': data['main']['temp'], 'humidity': data['main']['humidity']})\n    \n    return os.path.realpath(csvfile.name)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, pandas, sklearn.decomposition, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLORS = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n\ndef perform_pca_and_plot(my_list, labels):\n    \"\"\"\n    Perform Principal Component Analysis (PCA) on a list of high-dimensional vectors, \n    then plot the first two principal components.\n\n    Parameters:\n    my_list (list): The list of high-dimensional vectors.\n    labels (list): The list of labels for the vectors.\n\n    Returns:\n    PCA: The PCA object fitted to the data.\n\n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.decomposition\n    - matplotlib.pyplot\n\n    Example:\n    >>> perform_pca_and_plot([[2.5, 2.4], [0.5, 0.7], [2.2, 2.9], [1.9, 2.2], [3.1, 3.0]], ['A', 'B', 'A', 'A', 'B'])\n    \"\"\"\n    pca = PCA(n_components=2)\n    principal_components = pca.fit_transform(my_list)\n    \n    df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\n    df['Label'] = labels\n\n    fig = plt.figure(figsize=(8, 8))\n    ax = fig.add_subplot(1, 1, 1)\n    ax.set_xlabel('Principal Component 1', fontsize=15)\n    ax.set_ylabel('Principal Component 2', fontsize=15)\n    ax.set_title('2 component PCA', fontsize=20)\n\n    unique_labels = np.unique(labels)\n    for label, color in zip(unique_labels, COLORS):\n        indices = df['Label'] == label\n        ax.scatter(df.loc[indices, 'PC1'], df.loc[indices, 'PC2'], c=color, s=50)\n\n    ax.legend(unique_labels)\n    ax.grid()\n\n    return pca\n```"}
{"seed": "{\"task_id\": 299026, \"prompt\": \"def f_299026(my_list):\\n\\t\", \"suffix\": \"\\n\\treturn my_list\", \"canonical_solution\": \"my_list = my_list[::-1]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([1, 2, 3]) == [3, 2, 1]\\n\", \"\\n    assert candidate([1, 2, 3, 4, 5]) == [5, 4, 3, 2, 1]\\n\", \"\\n    assert candidate([1, 1, 0, 1]) == [1, 0, 1, 1]\\n\"], \"entry_point\": \"f_299026\", \"intent\": \"\\u041f\\u0435\\u0440\\u0435\\u0432\\u0435\\u0440\\u043d\\u0443\\u0442\\u044c \\u0441\\u043f\\u0438\\u0441\\u043e\\u043a `my_list`\", \"library\": []}", "generation": "Scenario 1:\npandas, numpy, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCATEGORIES = ['Electronics', 'Fashion', 'Home & Kitchen', 'Books', 'Sports & Outdoors']\n\ndef generate_sales_report(sales_data):\n    \"\"\"\n    Generate a report of sales distribution across different categories and plot a pie chart.\n    \n    Parameters:\n    sales_data (DataFrame): The sales data in DataFrame. The DataFrame should have 'Category' and 'Sales' columns.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with sales distribution for the categories.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> sales_data = pd.DataFrame({\n    ...     'Category': ['Electronics', 'Fashion', 'Home & Kitchen', 'Books', 'Sports & Outdoors'],\n    ...     'Sales': [20000, 15000, 30000, 10000, 25000]\n    ... })\n    >>> report = generate_sales_report(sales_data)\n    >>> print(report)\n    >>> report['Sales'].plot(kind='pie', labels=report['Category'], autopct='%1.1f%%')\n    \"\"\"\n    report_data = []\n\n    for category in CATEGORIES:\n        sales = sales_data[sales_data['Category'] == category]['Sales'].sum()\n        report_data.append([category, sales])\n\n    report_df = pd.DataFrame(report_data, columns=['Category', 'Sales'])\n\n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, pandas, datetime\n```python\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\n# Constants\nDAYS = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n\ndef find_busy_days(traffic_data):\n    \"\"\"\n    Find the busiest days of the week based on the traffic data.\n    \n    Parameters:\n    traffic_data (DataFrame): The traffic data in DataFrame. The DataFrame should have 'Date' and 'Visitors' columns.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the busiest days and the number of visitors.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - datetime\n    \n    Example:\n    >>> traffic_data = pd.DataFrame({\n    ...     'Date': pd.date_range(start='1/1/2022', periods=30),\n    ...     'Visitors': np.random.randint(500, 1500, size=30)\n    ... })\n    >>> busy_days = find_busy_days(traffic_data)\n    >>> print(busy_days)\n    \"\"\"\n    traffic_data['Day'] = traffic_data['Date'].dt.day_name()\n    traffic_data_grouped = traffic_data.groupby('Day')['Visitors'].sum().reset_index()\n    traffic_data_grouped['Day'] = pd.Categorical(traffic_data_grouped['Day'], categories=DAYS, ordered=True)\n    traffic_data_grouped = traffic_data_grouped.sort_values('Day')\n\n    return traffic_data_grouped\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nrandom, numpy, pandas\n```python\nimport random\nimport numpy as np\nimport pandas as pd\n\n# Constants\nNAMES = ['John', 'Mike', 'Sara', 'Emma', 'Nick', 'Anna', 'Tom', 'Lisa', 'Sam', 'Linda']\n\ndef generate_student_scores():\n    \"\"\"\n    Generate a DataFrame of student scores in different subjects.\n    \n    Parameters:\n    None\n    \n    Returns:\n    DataFrame: A pandas DataFrame with student names and their scores in different subjects.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - random\n    \n    Example:\n    >>> scores = generate_student_scores()\n    >>> print(scores)\n    \"\"\"\n    scores_data = {\n        'Name': NAMES,\n        'Math': np.random.randint(50, 100, size=len(NAMES)),\n        'Science': np.random.randint(50, 100, size=len(NAMES)),\n        'English': np.random.randint(50, 100, size=len(NAMES)),\n        'History': np.random.randint(50, 100, size=len(NAMES)),\n        'Art': np.random.randint(50, 100, size=len(NAMES))\n    }\n\n    scores_df = pd.DataFrame(scores_data)\n\n    return scores_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nmatplotlib, pandas, numpy\n```python\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\n# Constants\nMONTHS = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n\ndef plot_sales_trend(sales_data):\n    \"\"\"\n    Plot the monthly sales trend based on the sales data.\n    \n    Parameters:\n    sales_data (DataFrame): The sales data in DataFrame. The DataFrame should have 'Month' and 'Sales' columns.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> sales_data = pd.DataFrame({\n    ...     'Month': MONTHS,\n    ...     'Sales': np.random.randint(2000, 5000, size=len(MONTHS))\n    ... })\n    >>> plot_sales_trend(sales_data)\n    \"\"\"\n    plt.figure(figsize=(10, 6))\n    plt.plot(sales_data['Month'], sales_data['Sales'], marker='o')\n    plt.title('Monthly Sales Trend')\n    plt.xlabel('Month')\n    plt.ylabel('Sales')\n    plt.grid(True)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, datetime, random\n```python\nimport numpy as np\nfrom datetime import datetime, timedelta\nimport random\n\n# Constants\nNAMES = ['John', 'Mike', 'Sara', 'Emma', 'Nick', 'Anna', 'Tom', 'Lisa', 'Sam', 'Linda']\nSTART_DATE = datetime(2022, 1, 1)\nEND_DATE = datetime(2022, 12, 31)\nDATES = np.arange(START_DATE, END_DATE, timedelta(days=1)).astype(datetime)\n\ndef generate_logins():\n    \"\"\"\n    Generate a numpy array of random logins with user names and dates.\n    \n    Parameters:\n    None\n    \n    Returns:\n    numpy.array: A numpy array with user names and dates of their logins.\n    \n    Requirements:\n    - numpy\n    - datetime\n    - random\n    \n    Example:\n    >>> logins = generate_logins()\n    >>> print(logins)\n    \"\"\"\n    logins = np.array([(random.choice(NAMES), random.choice(DATES)) for _ in range(1000)], dtype=object)\n\n    return logins\n```\n"}
{"seed": "{\"task_id\": 299026, \"prompt\": \"def f_299026(my_list):\\n\\t\", \"suffix\": \"\\n\\treturn my_list\", \"canonical_solution\": \"my_list = list(reversed(my_list))\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([1, 2, 3]) == [3, 2, 1]\\n\", \"\\n    assert candidate([1, 2, 3, 4, 5]) == [5, 4, 3, 2, 1]\\n\", \"\\n    assert candidate([1, 1, 0, 1]) == [1, 0, 1, 1]\\n\"], \"entry_point\": \"f_299026\", \"intent\": \"\\u041f\\u0435\\u0440\\u0435\\u0432\\u0435\\u0440\\u043d\\u0443\\u0442\\u044c \\u0441\\u043f\\u0438\\u0441\\u043e\\u043a `my_list`\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, matplotlib, random\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint, shuffle\n\n# Constants\nDATAFRAME_SIZE = 100\nCATEGORIES = ['A', 'B', 'C', 'D', 'E']\n\ndef create_and_analyze_dataframe():\n    \"\"\"\n    Create a pandas DataFrame with 'CATEGORIES' as categories and \n    random integers as values. Reverse the DataFrame, analyze it, \n    and plot the category distribution.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with analyzed data.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> df = create_and_analyze_dataframe()\n    >>> print(df)\n    >>> df['Categories'].value_counts().plot(kind='bar')\n    \"\"\"\n    data = []\n    for _ in range(DATAFRAME_SIZE):\n        shuffle(CATEGORIES)\n        category = CATEGORIES[0]\n        value = randint(0, 100)\n        data.append([category, value])\n        \n    df = pd.DataFrame(data, columns=['Categories', 'Values'])\n    reversed_df = df.iloc[::-1]\n    \n    mean_values = reversed_df.groupby('Categories').mean()\n    mean_values.plot.bar()\n\n    return mean_values\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, datetime\n```python\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\n\n# Constants\nDATE_RANGE = pd.date_range(start='1/1/2020', end='1/10/2020')\nCATEGORIES = ['A', 'B', 'C', 'D', 'E']\n\ndef generate_data_and_find_previous_day():\n    \"\"\"\n    Generate a DataFrame with random values for given categories and dates.\n    For each row, find the value of the same category from the previous day.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the original and previous day values.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - datetime\n\n    Example:\n    >>> df = generate_data_and_find_previous_day()\n    >>> print(df)\n    \"\"\"\n    data = []\n    for date in DATE_RANGE:\n        for category in CATEGORIES:\n            value = np.random.randint(0,100)\n            data.append([date, category, value])\n            \n    df = pd.DataFrame(data, columns=['Date', 'Category', 'Value'])\n    df['Previous Day Value'] = df.groupby('Category')['Value'].shift()\n    \n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, matplotlib, random\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import shuffle\n\n# Constants\nVECTOR_SIZE = 10\n\ndef reverse_vector_and_plot():\n    \"\"\"\n    Generate a numpy array (vector) with random numbers, reverse it \n    and plot both the original and reversed vectors.\n    \n    Returns:\n    numpy.ndarray: The reversed numpy array.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> reversed_vector = reverse_vector_and_plot()\n    >>> print(reversed_vector)\n    \"\"\"\n    vector = np.random.randint(100, size=(VECTOR_SIZE))\n    reversed_vector = vector[::-1]\n    \n    plt.figure(figsize=(12, 6))\n    plt.plot(vector, 'b-', label='Original')\n    plt.plot(reversed_vector, 'r-', label='Reversed')\n    plt.legend()\n    plt.show()\n\n    return reversed_vector\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, random, datetime\n```python\nimport pandas as pd\nfrom random import randint\nfrom datetime import datetime, timedelta\n\n# Constants\nDAYS = 30\nCATEGORIES = ['A', 'B', 'C', 'D', 'E']\n\ndef generate_and_reverse_dataframe():\n    \"\"\"\n    Generate a pandas DataFrame with 'CATEGORIES' as categories, \n    random integers as values and dates for the past 'DAYS'. \n    Reverse the DataFrame and return it.\n    \n    Returns:\n    DataFrame: The reversed pandas DataFrame.\n\n    Requirements:\n    - pandas\n    - random\n    - datetime\n\n    Example:\n    >>> reversed_df = generate_and_reverse_dataframe()\n    >>> print(reversed_df)\n    \"\"\"\n    data = []\n    for _ in range(DAYS):\n        date = datetime.now() - timedelta(days=_)\n        category = CATEGORIES[randint(0, len(CATEGORIES)-1)]\n        value = randint(0, 100)\n        data.append([date, category, value])\n        \n    df = pd.DataFrame(data, columns=['Date', 'Category', 'Value'])\n    reversed_df = df.iloc[::-1]\n\n    return reversed_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, matplotlib, random\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nMATRIX_DIMENSION = 10\n\ndef create_and_plot_reversed_matrix():\n    \"\"\"\n    Create a 10x10 numpy matrix with random integers, reverse it and plot both \n    the original and reversed matrices using matplotlib's imshow.\n    \n    Returns:\n    numpy.ndarray: The reversed numpy matrix.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> reversed_matrix = create_and_plot_reversed_matrix()\n    >>> print(reversed_matrix)\n    \"\"\"\n    matrix = np.random.randint(100, size=(MATRIX_DIMENSION, MATRIX_DIMENSION))\n    reversed_matrix = np.flip(matrix, axis=0)\n    \n    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n    axs[0].imshow(matrix)\n    axs[0].set_title('Original Matrix')\n    axs[1].imshow(reversed_matrix)\n    axs[1].set_title('Reversed Matrix')\n    plt.show()\n\n    return reversed_matrix\n```"}
{"seed": "{\"task_id\": 81572, \"prompt\": \"def f_81572(my_list, item):\\n\\t\", \"suffix\": \"\\n\\treturn my_list\", \"canonical_solution\": \"my_list.remove(item)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    data = [1, 4, 5]\\n    candidate(data, 4)\\n    assert data == [1, 5]\\n\", \"\\n    data = [1, 5]\\n    try:\\n        candidate(data, 0)\\n    except:\\n        assert data == [1, 5]\\n\"], \"entry_point\": \"f_81572\", \"intent\": \"\\u0423\\u0434\\u0430\\u043b\\u0438\\u0442\\u044c \\u044d\\u043b\\u0435\\u043c\\u0435\\u043d\\u0442 `item` \\u0438\\u0437 \\u0441\\u043f\\u0438\\u0441\\u043a\\u0430 `my_list`\", \"library\": []}", "generation": "Scenario 1: \npandas, itertools, random\n```python\nimport pandas as pd\nimport itertools\nimport random\n\n# Constants\nITEMS = ['apple', 'banana', 'cherry', 'date', 'elderberry', 'fig', 'grape', 'honeydew']\n\ndef remove_random_items(dataframe, column_name, num_items):\n    \"\"\"\n    Remove a specific number of random items from a specified column in a pandas DataFrame.\n\n    Parameters:\n    dataframe (DataFrame): The pandas DataFrame.\n    column_name (str): The column from which to remove items.\n    num_items (int): The number of items to remove.\n\n    Returns:\n    DataFrame: The updated DataFrame after removing the items.\n\n    Requirements:\n    - pandas\n    - itertools\n    - random\n\n    Example:\n    >>> dataframe = pd.DataFrame({'fruits': ITEMS})\n    >>> remove_random_items(dataframe, 'fruits', 3)\n    \"\"\"\n    if column_name not in dataframe.columns:\n        raise ValueError(f\"'{column_name}' not found in DataFrame columns\")\n\n    if num_items > len(dataframe):\n        raise ValueError(\"num_items cannot be greater than the number of items in the DataFrame\")\n\n    indices_to_remove = random.sample(range(len(dataframe)), num_items)\n    dataframe.drop(indices_to_remove, inplace=True)\n    dataframe.reset_index(drop=True, inplace=True)\n\n    return dataframe\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nnumpy, matplotlib.pyplot, random\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\n\n# Constants\nCOLORS = ['red', 'blue', 'green', 'yellow', 'purple', 'orange']\n\ndef plot_random_walk(num_steps, item_removal_steps):\n    \"\"\"\n    Plot a random walk and remove a step at specific intervals.\n\n    Parameters:\n    num_steps (int): The total number of steps in the random walk.\n    item_removal_steps (int): The interval at which to remove a step.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> plot_random_walk(1000, 100)\n    \"\"\"\n    random_walk = np.random.randint(-1, 2, size=num_steps).cumsum()\n    colors = [random.choice(COLORS) for _ in range(num_steps)]\n    \n    for i in range(item_removal_steps, num_steps, item_removal_steps):\n        random_walk = np.delete(random_walk, i)\n        colors.pop(i)\n\n    plt.scatter(range(len(random_walk)), random_walk, c=colors)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \npandas, numpy, random\n```python\nimport pandas as pd\nimport numpy as np\nimport random\n\ndef remove_random_rows(dataframe, num_rows):\n    \"\"\"\n    Remove a specific number of random rows from a pandas DataFrame.\n\n    Parameters:\n    dataframe (DataFrame): The pandas DataFrame.\n    num_rows (int): The number of rows to remove.\n\n    Returns:\n    DataFrame: The updated DataFrame after removing the rows.\n\n    Requirements:\n    - pandas\n    - numpy\n    - random\n\n    Example:\n    >>> dataframe = pd.DataFrame(np.random.randint(0,100,size=(100, 4)), columns=list('ABCD'))\n    >>> remove_random_rows(dataframe, 10)\n    \"\"\"\n    if num_rows > len(dataframe):\n        raise ValueError(\"num_rows cannot be greater than the number of rows in the DataFrame\")\n\n    indices_to_remove = random.sample(range(len(dataframe)), num_rows)\n    dataframe.drop(indices_to_remove, inplace=True)\n    dataframe.reset_index(drop=True, inplace=True)\n\n    return dataframe\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, math, random\n```python\nimport numpy as np\nimport math\nimport random\n\ndef remove_elements_from_array(array, num_elements):\n    \"\"\"\n    Remove a specific number of elements from a numpy array.\n\n    Parameters:\n    array (ndarray): The numpy array.\n    num_elements (int): The number of elements to remove.\n\n    Returns:\n    ndarray: The updated numpy array after removing the elements.\n\n    Requirements:\n    - numpy\n    - math\n    - random\n\n    Example:\n    >>> array = np.array([1, 2, 3, 4, 5])\n    >>> remove_elements_from_array(array, 2)\n    \"\"\"\n    if num_elements > len(array):\n        raise ValueError(\"num_elements cannot be greater than the number of elements in the array\")\n\n    indices_to_remove = random.sample(range(len(array)), num_elements)\n    array = np.delete(array, indices_to_remove)\n\n    return array\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \npandas, sklearn.preprocessing, random, numpy\n```python\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport random\nimport numpy as np\n\ndef standardize_and_remove_random(dataframe, num_rows):\n    \"\"\"\n    Standardize a DataFrame using sklearn's StandardScaler and remove a specific number of random rows.\n\n    Parameters:\n    dataframe (DataFrame): The pandas DataFrame.\n    num_rows (int): The number of rows to remove.\n\n    Returns:\n    DataFrame: The updated DataFrame after standardizing and removing the rows.\n\n    Requirements:\n    - pandas\n    - sklearn.preprocessing\n    - random\n    - numpy\n\n    Example:\n    >>> dataframe = pd.DataFrame(np.random.randint(0,100,size=(100, 4)), columns=list('ABCD'))\n    >>> standardize_and_remove_random(dataframe, 10)\n    \"\"\"\n    if num_rows > len(dataframe):\n        raise ValueError(\"num_rows cannot be greater than the number of rows in the DataFrame\")\n\n    scaler = StandardScaler()\n    dataframe[dataframe.columns] = scaler.fit_transform(dataframe)\n\n    indices_to_remove = random.sample(range(len(dataframe)), num_rows)\n    dataframe.drop(indices_to_remove, inplace=True)\n    dataframe.reset_index(drop=True, inplace=True)\n\n    return dataframe\n```"}
{"seed": "{\"task_id\": 81572, \"prompt\": \"def f_81572(my_list, item):\\n\\t\", \"suffix\": \"\\n\\treturn my_list\", \"canonical_solution\": \"del my_list[my_list.index(item)]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    data = [1, 4, 5]\\n    candidate(data, 4)\\n    assert data == [1, 5]\\n\", \"\\n    data = [1, 5]\\n    try:\\n        candidate(data, 0)\\n    except:\\n        assert data == [1, 5]\\n\"], \"entry_point\": \"f_81572\", \"intent\": \"\\u0423\\u0434\\u0430\\u043b\\u0438\\u0442\\u044c \\u044d\\u043b\\u0435\\u043c\\u0435\\u043d\\u0442 `item` \\u0438\\u0437 \\u0441\\u043f\\u0438\\u0441\\u043a\\u0430 `my_list`\", \"library\": []}", "generation": "Scenario 1:\npandas, numpy, random\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import randint\n\n# Constants\nCATEGORIES = ['Electronics', 'Books', 'Clothing', 'Groceries', 'Toys']\nITEMS_PER_CATEGORY = {\n    'Electronics': ['Laptop', 'Smartphone', 'Headphones', 'Television', 'Camera'],\n    'Books': ['Novel', 'Textbook', 'Biography', 'Comic', 'Magazine'],\n    'Clothing': ['Shirt', 'Pants', 'Dress', 'Shoes', 'Hat'],\n    'Groceries': ['Fruits', 'Vegetables', 'Meat', 'Bread', 'Milk'],\n    'Toys': ['Action Figure', 'Board Game', 'Doll', 'Puzzle', 'Video Game']\n}\n\ndef remove_item_from_inventory(category, item):\n    \"\"\"\n    Remove an item from a category in a mock inventory, represented as a pandas DataFrame. \n    The DataFrame is initially populated with 5 items in each category, with random quantities between 1 and 10.\n    \n    Parameters:\n    category (str): Category from which the item should be removed.\n    item (str): The item to be removed.\n\n    Returns:\n    DataFrame: The updated inventory DataFrame.\n\n    Requirements:\n    - pandas\n    - numpy\n    - random\n\n    Example:\n    >>> remove_item_from_inventory('Electronics', 'Laptop')\n    \"\"\"\n    # Initialize inventory DataFrame\n    data = []\n    for category, items in ITEMS_PER_CATEGORY.items():\n        for item in items:\n            data.append([category, item, randint(1, 10)])\n    inventory = pd.DataFrame(data, columns=['Category', 'Item', 'Quantity'])\n\n    # Remove item from inventory\n    inventory = inventory[~((inventory['Category'] == category) & (inventory['Item'] == item))]\n\n    return inventory\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, matplotlib.pyplot, statistics\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom statistics import mean\n\n# Constants\nNUM_SAMPLES = 100\nMEAN = 0\nSTD_DEV = 1\n\ndef remove_outliers(data):\n    \"\"\"\n    Remove outliers from a numpy array of data, where outliers are defined as points that are \n    more than 2 standard deviations away from the mean. Plot the data before and after removing outliers.\n    \n    Parameters:\n    data (numpy array): The data from which to remove outliers.\n\n    Returns:\n    numpy array: The data after removing outliers.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - statistics\n\n    Example:\n    >>> data = np.random.normal(MEAN, STD_DEV, NUM_SAMPLES)\n    >>> remove_outliers(data)\n    \"\"\"\n    # Plot original data\n    plt.hist(data, bins='auto')\n    plt.title(\"Original Data\")\n    plt.show()\n\n    # Calculate mean and standard deviation\n    data_mean = mean(data)\n    data_std = np.std(data)\n\n    # Define outliers\n    outliers = (data < (data_mean - 2 * data_std)) | (data > (data_mean + 2 * data_std))\n\n    # Remove outliers\n    data = data[~outliers]\n\n    # Plot data after removing outliers\n    plt.hist(data, bins='auto')\n    plt.title(\"Data After Removing Outliers\")\n    plt.show()\n\n    return data\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, shutil, sys\n```python\nimport os\nimport shutil\nimport sys\n\ndef move_files(source_dir, dest_dir, file_ext):\n    \"\"\"\n    Move all files with a given extension from one directory to another.\n    \n    Parameters:\n    source_dir (str): The source directory.\n    dest_dir (str): The destination directory.\n    file_ext (str): The file extension.\n\n    Returns:\n    None\n\n    Requirements:\n    - os\n    - shutil\n    - sys\n\n    Example:\n    >>> move_files('/path/to/source', '/path/to/destination', '.txt')\n    \"\"\"\n    try:\n        # Ensure destination directory exists\n        if not os.path.exists(dest_dir):\n            os.makedirs(dest_dir)\n        \n        # Move files\n        for file_name in os.listdir(source_dir):\n            if file_name.endswith(file_ext):\n                shutil.move(os.path.join(source_dir, file_name), dest_dir)\n    except Exception as e:\n        print(f\"Error: {e}\", file=sys.stderr)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, matplotlib.pyplot\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nNUM_SAMPLES = 1000\n\ndef analyze_data(data):\n    \"\"\"\n    Analyze a pandas DataFrame of data. Remove any rows containing NaN values, \n    calculate basic statistics (mean, median, standard deviation), and plot a histogram of the data.\n    \n    Parameters:\n    data (DataFrame): The data to analyze.\n\n    Returns:\n    dict: A dictionary of statistics.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> data = pd.DataFrame(np.random.randn(NUM_SAMPLES), columns=['Values'])\n    >>> analyze_data(data)\n    \"\"\"\n    # Remove NaN values\n    data = data.dropna()\n\n    # Calculate statistics\n    mean = data.mean().item()\n    median = data.median().item()\n    std = data.std().item()\n\n    # Plot histogram\n    plt.hist(data['Values'], bins='auto')\n    plt.title(\"Histogram of Values\")\n    plt.show()\n\n    return {'mean': mean, 'median': median, 'std': std}\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nrandom, math, matplotlib.pyplot\n```python\nimport random\nimport math\nimport matplotlib.pyplot as plt\n\n# Constants\nNUM_SAMPLES = 1000\n\ndef generate_and_plot_random_numbers():\n    \"\"\"\n    Generate a list of random numbers, calculate their square roots, and plot both the original numbers \n    and their square roots in a scatter plot.\n    \n    Parameters:\n    None\n\n    Returns:\n    list: The list of square roots.\n\n    Requirements:\n    - random\n    - math\n    - matplotlib.pyplot\n\n    Example:\n    >>> generate_and_plot_random_numbers()\n    \"\"\"\n    # Generate random numbers\n    numbers = [random.random() for _ in range(NUM_SAMPLES)]\n\n    # Calculate square roots\n    square_roots = [math.sqrt(num) for num in numbers]\n\n    # Plot numbers and square roots\n    plt.scatter(numbers, square_roots)\n    plt.title(\"Numbers and Their Square Roots\")\n    plt.xlabel(\"Number\")\n    plt.ylabel(\"Square Root\")\n    plt.show()\n\n    return square_roots\n```"}
{"seed": "{\"task_id\": 592812, \"prompt\": \"def f_592812(s, sep):\\n\\t\", \"suffix\": \"\\n\\treturn s_list\", \"canonical_solution\": \"s_list = s.strip().split(sep)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    s = \\\"   Jam, sauce, ketchup, penguins  \\\"\\n    assert candidate(s, ',') == ['Jam', ' sauce', ' ketchup', ' penguins']\\n\"], \"entry_point\": \"f_592812\", \"intent\": \"\\u0420\\u0430\\u0437\\u0434\\u0435\\u043b\\u0438\\u0442\\u044c \\u0441\\u0442\\u0440\\u043e\\u043a\\u0443 `s` \\u043d\\u0430 \\u0441\\u043f\\u0438\\u0441\\u043e\\u043a \\u043f\\u043e\\u0434\\u0441\\u0442\\u0440\\u043e\\u043a `s_list` \\u043f\\u043e \\u0440\\u0430\\u0437\\u0434\\u0435\\u043b\\u0438\\u0442\\u0435\\u043b\\u044e `sep` \", \"library\": []}", "generation": "Scenario 1:\npandas, re, collections, matplotlib\n```python\nimport pandas as pd\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef count_word_frequency(text, n_most_common):\n    \"\"\"\n    Split the given text into words using regular expressions and count the \n    frequency of each word. Returns the n most common words and their counts.\n    Then, it plots the word frequency using matplotlib.\n    \n    Parameters:\n    text (str): The input text.\n    n_most_common (int): The number of most common words to return.\n\n    Returns:\n    list: A list of tuples, each containing a word and its count.\n\n    Requirements:\n    - pandas\n    - re\n    - collections\n    - matplotlib.pyplot\n    \n    Example:\n    >>> text = \"apple banana apple orange banana apple\"\n    >>> print(count_word_frequency(text, 2))\n    >>> [('apple', 3), ('banana', 2)]\n    \"\"\"\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    word_counts = Counter(words)\n    most_common_words = word_counts.most_common(n_most_common)\n\n    # plot the word frequency\n    df = pd.DataFrame(most_common_words, columns=['Word', 'Count'])\n    df.set_index('Word').plot(kind='bar')\n\n    return most_common_words\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, pandas, re, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nimport re\nimport matplotlib.pyplot as plt\n\n# Constants\nSEPARATOR = ','\n\ndef plot_correlation_matrix(csv_data):\n    \"\"\"\n    Takes a CSV string, parses it into a pandas DataFrame, computes the \n    correlation matrix, and plots it using matplotlib.\n\n    Parameters:\n    csv_data (str): The CSV data.\n\n    Returns:\n    DataFrame: The correlation matrix.\n\n    Requirements:\n    - numpy\n    - pandas\n    - re\n    - matplotlib.pyplot\n\n    Example:\n    >>> csv_data = \"A,B,C\\\\n1,2,3\\\\n4,5,6\\\\n7,8,9\"\n    >>> print(plot_correlation_matrix(csv_data))\n    \"\"\"\n    # parse CSV data into DataFrame\n    lines = csv_data.strip().split('\\n')\n    headers = lines[0].split(SEPARATOR)\n    data = [line.split(SEPARATOR) for line in lines[1:]]\n    df = pd.DataFrame(data, columns=headers)\n\n    # compute correlation matrix\n    correlation_matrix = df.corr()\n\n    # plot correlation matrix\n    plt.imshow(correlation_matrix, cmap='hot', interpolation='nearest')\n    plt.show()\n\n    return correlation_matrix\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ncollections, string, re, matplotlib\n```python\nimport collections\nimport string\nimport re\nimport matplotlib.pyplot as plt\n\n# Constants\nPUNCTUATION = string.punctuation\n\ndef count_letter_frequency(text):\n    \"\"\"\n    Count the frequency of each letter in the input text, ignoring case \n    and punctuation. Plot the frequency of each letter using matplotlib.\n\n    Parameters:\n    text (str): The input text.\n\n    Returns:\n    dict: A dictionary mapping each letter to its count.\n\n    Requirements:\n    - collections\n    - string\n    - re\n    - matplotlib.pyplot\n\n    Example:\n    >>> text = \"Hello, World!\"\n    >>> print(count_letter_frequency(text))\n    \"\"\"\n    # remove punctuation and convert to lowercase\n    text = re.sub(f'[{PUNCTUATION}]', '', text).lower()\n\n    # count letters\n    letter_counts = collections.Counter(text)\n\n    # plot letter frequencies\n    plt.bar(list(letter_counts.keys()), list(letter_counts.values()))\n    plt.show()\n\n    return dict(letter_counts)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, re, numpy, matplotlib\n```python\nimport pandas as pd\nimport re\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nSEPARATOR = ','\n\ndef analyze_data(csv_data):\n    \"\"\"\n    Takes a CSV string, parses it into a pandas DataFrame, computes the \n    mean, median, and mode of each column, and plots a histogram for each column.\n\n    Parameters:\n    csv_data (str): The CSV data.\n\n    Returns:\n    DataFrame: A DataFrame with the mean, median, and mode for each column.\n\n    Requirements:\n    - pandas\n    - re\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> csv_data = \"A,B,C\\\\n1,2,3\\\\n4,5,6\\\\n7,8,9\"\n    >>> print(analyze_data(csv_data))\n    \"\"\"\n    # parse CSV data into DataFrame\n    lines = csv_data.strip().split('\\n')\n    headers = lines[0].split(SEPARATOR)\n    data = [list(map(float, line.split(SEPARATOR))) for line in lines[1:]]\n    df = pd.DataFrame(data, columns=headers)\n\n    # compute statistics\n    means = df.mean()\n    medians = df.median()\n    modes = df.mode().iloc[0]\n\n    # plot histograms\n    df.hist()\n    plt.show()\n\n    # return statistics\n    return pd.DataFrame({\n        'Mean': means,\n        'Median': medians,\n        'Mode': modes\n    })\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nre, pandas, sklearn, matplotlib\n```python\nimport re\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\n\ndef plot_word_frequency(texts):\n    \"\"\"\n    Takes a list of texts, vectorizes the texts into a document-term matrix using \n    CountVectorizer, sums up the counts of each word across all texts, and plots \n    the word frequencies using matplotlib.\n\n    Parameters:\n    texts (list): The list of texts.\n\n    Returns:\n    DataFrame: The word frequencies.\n\n    Requirements:\n    - re\n    - pandas\n    - sklearn\n    - matplotlib.pyplot\n\n    Example:\n    >>> texts = [\"apple banana\", \"banana orange\", \"orange apple banana\"]\n    >>> print(plot_word_frequency(texts))\n    \"\"\"\n    vectorizer = CountVectorizer()\n    dtm = vectorizer.fit_transform(texts)\n    word_counts = dtm.sum(axis=0).A1\n    word_frequencies = pd.Series(word_counts, index=vectorizer.get_feature_names_out())\n\n    word_frequencies.plot(kind='bar')\n    plt.show()\n\n    return word_frequencies\n```\n"}
{"seed": "{\"task_id\": 506457, \"prompt\": \"def f_506457(num):\\n\\t\", \"suffix\": \"\\n\\treturn digits\", \"canonical_solution\": \"digits = [int(i) for i in str(num)]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    s = \\\"5400\\\"\\n    assert candidate(s) == [5, 4, 0, 0]\\n\", \"\\n    s1 = \\\"b33\\\"\\n    try:\\n        candidate(s1)\\n    except:\\n        assert True\\n\"], \"entry_point\": \"f_506457\", \"intent\": \"\\u0420\\u0430\\u0437\\u0431\\u0438\\u0442\\u044c \\u0447\\u0438\\u0441\\u043b\\u043e `num` \\u043d\\u0430 \\u0441\\u043f\\u0438\\u0441\\u043e\\u043a \\u0447\\u0438\\u0441\\u0435\\u043b `digits`\", \"library\": []}", "generation": "Scenario 1:\npandas, matplotlib, random, itertools\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import randint\nimport itertools\n\n# Constants\nNUMBERS = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nMAX_DIGITS = 4\n\ndef generate_number_frequency(num_samples):\n    \"\"\"\n    Generate a bar plot showing the frequency of each digit (0-9) appearing in \n    a set of random numbers with a variable number of digits.\n    \n    Parameters:\n    num_samples (int): The number of random numbers to generate.\n    \n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - random\n    - itertools\n    \n    Example:\n    >>> generate_number_frequency(1000)\n    \"\"\"\n    number_frequency = {num: 0 for num in NUMBERS}\n\n    for _ in range(num_samples):\n        num_digits = randint(1, MAX_DIGITS)\n        number = [str(NUMBERS[randint(0, len(NUMBERS)-1)]) for _ in range(num_digits)]\n        for digit in itertools.chain.from_iterable(number):\n            number_frequency[int(digit)] += 1\n\n    number_frequency_df = pd.DataFrame.from_dict(number_frequency, orient='index', columns=['Frequency'])\n    number_frequency_df.sort_index().plot(kind='bar')\n\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, random, math\n```python\nimport numpy as np\nfrom random import randint\nimport math\n\n# Constants\nMAX_DIGITS = 4\n\ndef compute_square_root(num_samples):\n    \"\"\"\n    Compute the square root of a set of random numbers with a variable number of digits.\n    The results are returned in a numpy array.\n    \n    Parameters:\n    num_samples (int): The number of random numbers to generate.\n    \n    Returns:\n    numpy.ndarray: An array of square roots.\n    \n    Requirements:\n    - numpy\n    - random\n    - math\n    \n    Example:\n    >>> compute_square_root(5)\n    \"\"\"\n    numbers = []\n    for _ in range(num_samples):\n        num_digits = randint(1, MAX_DIGITS)\n        number = int(''.join(str(randint(0, 9)) for _ in range(num_digits)))\n        numbers.append(math.sqrt(number))\n\n    return np.array(numbers)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, pandas, seaborn, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nMAX_DIGITS = 4\n\ndef plot_digit_histogram(num_samples):\n    \"\"\"\n    Generate a histogram showing the distribution of the number of digits in a set \n    of random numbers.\n    \n    Parameters:\n    num_samples (int): The number of random numbers to generate.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - seaborn\n    - matplotlib.pyplot\n    \n    Example:\n    >>> plot_digit_histogram(1000)\n    \"\"\"\n    data = []\n    for _ in range(num_samples):\n        num_digits = np.random.randint(1, MAX_DIGITS+1)\n        number = int(''.join(str(np.random.randint(0, 10)) for _ in range(num_digits)))\n        data.append(len(str(number)))\n\n    df = pd.DataFrame(data, columns=['Number of Digits'])\n    plt.figure(figsize=(8, 6))\n    sns.histplot(data=df, x='Number of Digits', bins=MAX_DIGITS, kde=True)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, pandas, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nMAX_DIGITS = 4\n\ndef plot_digit_frequency(num_samples):\n    \"\"\"\n    Generate a line plot showing the frequency of each digit (0-9) in a set of \n    random numbers with a variable number of digits.\n    \n    Parameters:\n    num_samples (int): The number of random numbers to generate.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    \n    Example:\n    >>> plot_digit_frequency(1000)\n    \"\"\"\n    digit_frequency = pd.Series(np.zeros(10, dtype=int), index=np.arange(10))\n\n    for _ in range(num_samples):\n        num_digits = np.random.randint(1, MAX_DIGITS+1)\n        number = int(''.join(str(np.random.randint(0, 10)) for _ in range(num_digits)))\n        digits, counts = np.unique(list(str(number)), return_counts=True)\n        digit_frequency.loc[digits] += counts\n\n    digit_frequency.plot(kind='line', marker='o')\n    plt.xlabel('Digit')\n    plt.ylabel('Frequency')\n    plt.title('Frequency of Digits in Random Numbers')\n    plt.grid(True)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, random, pandas, matplotlib\n```python\nimport numpy as np\nfrom random import randint\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nMAX_DIGITS = 4\n\ndef generate_and_plot_numbers(num_samples):\n    \"\"\"\n    Generate a set of random numbers with a variable number of digits and plot them\n    in a scatter plot.\n    \n    Parameters:\n    num_samples (int): The number of random numbers to generate.\n    \n    Requirements:\n    - numpy\n    - random\n    - pandas\n    - matplotlib.pyplot\n    \n    Example:\n    >>> generate_and_plot_numbers(1000)\n    \"\"\"\n    numbers = []\n    for _ in range(num_samples):\n        num_digits = randint(1, MAX_DIGITS)\n        number = int(''.join(str(randint(0, 9)) for _ in range(num_digits)))\n        numbers.append(number)\n\n    df = pd.DataFrame(numbers, columns=['Numbers'])\n    df.plot(kind='scatter', x=df.index, y='Numbers')\n\n    plt.show()\n```\n`GPT_ODEX_BREAK`"}
{"seed": "{\"task_id\": 597892, \"prompt\": \"def f_597892(file_path):\\n\\t\", \"suffix\": \"\\n\\treturn lines\", \"canonical_solution\": \"with open(file_path) as file:\\n\\n\\t\\tlines = [line.rstrip() for line in file.readlines()]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    res = ['12', '10', '9', '15', '10', '120']\\n    with open('./test.txt', 'w') as fw: \\n        for a in res: \\n            fw.write(f'{a}\\\\n')\\n    assert candidate('./test.txt') == res\\n\"], \"entry_point\": \"f_597892\", \"intent\": \"\\u041f\\u0440\\u043e\\u0447\\u0438\\u0442\\u0430\\u0442\\u044c \\u0441\\u0442\\u0440\\u043e\\u043a\\u0438 \\u0438\\u0437 \\u0444\\u0430\\u0439\\u043b\\u0430 `file_path` \\u0432 `lines` \\u0431\\u0435\\u0437 \\u043f\\u0435\\u0440\\u0435\\u043d\\u043e\\u0441\\u0430 \\u0441\\u0442\\u0440\\u043e\\u043a\\u0438\", \"library\": []}", "generation": "Scenario 1: \npandas, csv, collections, matplotlib\n```python\nimport pandas as pd\nimport csv\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef analyze_file_contents(file_path):\n    \"\"\"\n    Read a file with comma-separated values, count the frequency of each value, \n    and plot the top 10 most frequent values.\n    \n    Parameters:\n    file_path (str): The path to the file.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the frequency of each value.\n    \n    Requirements:\n    - pandas\n    - csv\n    - collections.Counter\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = analyze_file_contents('data.csv')\n    >>> print(df)\n    \"\"\"\n    with open(file_path, 'r') as file:\n        reader = csv.reader(file)\n        all_values = [item for sublist in reader for item in sublist]\n\n    counter = Counter(all_values)\n    df = pd.DataFrame.from_dict(counter, orient='index').reset_index()\n    df = df.rename(columns={'index':'value', 0:'count'})\n\n    df = df.sort_values(by='count', ascending=False)\n    df[:10].plot(x='value', y='count', kind='bar')\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, codecs, re, collections\n```python\nimport os\nimport codecs\nimport re\nfrom collections import Counter\n\ndef count_word_frequency_in_files(directory_path):\n    \"\"\"\n    Count the frequency of each word in all text files in a given directory.\n\n    Parameters:\n    directory_path (str): The path to the directory.\n\n    Returns:\n    dict: A dictionary with each word and its frequency.\n\n    Requirements:\n    - os\n    - codecs\n    - re\n    - collections.Counter\n\n    Example:\n    >>> word_freq = count_word_frequency_in_files('./documents')\n    >>> print(word_freq)\n    \"\"\"\n    word_counter = Counter()\n    for file in os.listdir(directory_path):\n        if file.endswith('.txt'):\n            with codecs.open(os.path.join(directory_path, file), 'r', encoding='utf8') as f:\n                contents = f.read()\n                words = re.findall(r'\\b\\w+\\b', contents)\n                word_counter.update(words)\n\n    return dict(word_counter)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \npandas, csv, numpy, matplotlib\n```python\nimport pandas as pd\nimport csv\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef analyze_temperature_data(file_path):\n    \"\"\"\n    Read a file with comma-separated temperature values, calculate the average temperature, \n    and plot the temperature distribution.\n    \n    Parameters:\n    file_path (str): The path to the file.\n    \n    Returns:\n    float: The average temperature.\n    \n    Requirements:\n    - pandas\n    - csv\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> avg_temp = analyze_temperature_data('temps.csv')\n    >>> print(avg_temp)\n    \"\"\"\n    with open(file_path, 'r') as file:\n        reader = csv.reader(file)\n        all_temps = [float(temp) for sublist in reader for temp in sublist]\n\n    average_temp = np.mean(all_temps)\n    plt.hist(all_temps, bins=20)\n\n    return average_temp\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, PIL, matplotlib\n```python\nimport os\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\ndef analyze_image_sizes(directory_path):\n    \"\"\"\n    Analyze the sizes of all images in a given directory and plot the distribution \n    of width and height.\n\n    Parameters:\n    directory_path (str): The path to the directory.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the width and height of each image.\n\n    Requirements:\n    - os\n    - PIL.Image\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = analyze_image_sizes('./images')\n    >>> print(df)\n    \"\"\"\n    sizes = []\n    for file in os.listdir(directory_path):\n        if file.endswith('.jpg') or file.endswith('.png'):\n            image = Image.open(os.path.join(directory_path, file))\n            sizes.append(image.size)\n\n    df = pd.DataFrame(sizes, columns=['Width', 'Height'])\n    df.plot(kind='scatter', x='Width', y='Height')\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, csv, numpy, matplotlib\n```python\nimport pandas as pd\nimport csv\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef analyze_salary_data(file_path):\n    \"\"\"\n    Read a file with comma-separated salary values, calculate the median salary, \n    and plot the salary distribution.\n    \n    Parameters:\n    file_path (str): The path to the file.\n    \n    Returns:\n    float: The median salary.\n    \n    Requirements:\n    - pandas\n    - csv\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> median_salary = analyze_salary_data('salaries.csv')\n    >>> print(median_salary)\n    \"\"\"\n    with open(file_path, 'r') as file:\n        reader = csv.reader(file)\n        all_salaries = [float(sal) for sublist in reader for sal in sublist]\n\n    median_salary = np.median(all_salaries)\n    plt.hist(all_salaries, bins=20)\n\n    return median_salary\n```"}
{"seed": "{\"task_id\": 251048, \"prompt\": \"def f_251048(my_list):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"for x in my_list:\\n \\n\\t\\tprint(x)\", \"test_start\": \"\\nimport io\\nimport sys\\n\\ndef check(candidate):\", \"test\": [\"\\n    capturedOutput = io.StringIO()  \\n    sys.stdout = capturedOutput                 \\n    candidate([1, 2, 3])                                   \\n    sys.stdout = sys.__stdout__                \\n    assert capturedOutput.getvalue() == '1\\\\n2\\\\n3\\\\n'\\n\"], \"entry_point\": \"f_251048\", \"intent\": \"\\u0412\\u044b\\u0432\\u0435\\u0441\\u0442\\u0438 \\u0441\\u043f\\u0438\\u0441\\u043e\\u043a `my_list` \\u043f\\u043e\\u0441\\u0442\\u0440\\u043e\\u0447\\u043d\\u043e\", \"library\": [\"io\", \"sys\"]}", "generation": "Scenario 1: \npandas, matplotlib, numpy, random\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom random import choice\n\n# Constants\nCATEGORIES = [\"Electronics\", \"Books\", \"Clothing\", \"Sports\", \"Home Decor\"]\n\ndef plot_random_product_data(num_entries):\n    \"\"\"\n    Generate a random product data for a given number of entries and plot the \n    distribution of product categories.\n    \n    Parameters:\n    num_entries (int): The number of entries to generate.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - numpy\n    - random\n    \n    Example:\n    >>> plot_random_product_data(100)\n    \"\"\"\n    product_data = {\n        \"product_id\": np.arange(1, num_entries+1),\n        \"category\": [choice(CATEGORIES) for _ in range(num_entries)],\n        \"price\": np.random.randint(1, 500, size=num_entries)\n    }\n\n    df = pd.DataFrame(product_data)\n    df[\"category\"].value_counts().plot(kind=\"bar\")\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncsv, pandas, os, glob\n```python\nimport csv\nimport pandas as pd\nimport os\nimport glob\n\ndef merge_csv_files(directory):\n    \"\"\"\n    Merge all csv files in a given directory into a single pandas DataFrame.\n    \n    Parameters:\n    directory (str): The directory containing the csv files.\n    \n    Returns:\n    DataFrame: A pandas DataFrame containing the merged data.\n    \n    Requirements:\n    - csv\n    - pandas\n    - os\n    - glob\n    \n    Example:\n    >>> df = merge_csv_files('/path/to/csv/files')\n    \"\"\"\n    all_files = glob.glob(os.path.join(directory, \"*.csv\"))\n    df_from_each_file = (pd.read_csv(f) for f in all_files)\n    merged_df   = pd.concat(df_from_each_file, ignore_index=True)\n    return merged_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, random, matplotlib\n```python\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\n\ndef plot_random_walk(steps):\n    \"\"\"\n    Simulate a random walk of given steps and plot the trajectory.\n    \n    Parameters:\n    steps (int): The number of steps in the random walk.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - numpy\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> plot_random_walk(1000)\n    \"\"\"\n    walk = [0]  # start at zero\n    for i in range(steps):\n        step = 1 if random.randint(0, 1) else -1\n        walk.append(walk[-1] + step)\n    plt.plot(walk)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, matplotlib, datetime, random\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom datetime import datetime, timedelta\nfrom random import randint\n\ndef plot_random_time_series(days):\n    \"\"\"\n    Generate a random time series data for a given number of days and plot it.\n    \n    Parameters:\n    days (int): The number of days for the time series.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - datetime\n    - random\n    \n    Example:\n    >>> plot_random_time_series(365)\n    \"\"\"\n    dates = pd.date_range(datetime.now(), periods=days).tolist()\n    values = [randint(1, 100) for _ in range(days)]\n    df = pd.DataFrame({'Date': dates, 'Value': values})\n    df.set_index('Date', inplace=True)\n    df.plot()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, random, matplotlib, scipy.stats\n```python\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\ndef plot_random_normal_distribution(n):\n    \"\"\"\n    Generate n random numbers from a normal distribution, plot the histogram,\n    and fit a normal distribution to the data.\n    \n    Parameters:\n    n (int): The number of random numbers to generate.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - numpy\n    - random\n    - matplotlib.pyplot\n    - scipy.stats\n    \n    Example:\n    >>> plot_random_normal_distribution(1000)\n    \"\"\"\n    data = np.random.normal(0, 1, n)\n    plt.hist(data, bins=30, density=True, alpha=0.6, color='g')\n\n    # Fit a normal distribution to the data\n    mu, std = stats.norm.fit(data)\n\n    # Plot the PDF\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, mu, std)\n    plt.plot(x, p, 'k', linewidth=2)\n    plt.title(f\"Fit results: mu = {mu:.2f},  std = {std:.2f}\")\n    plt.show()\n```"}
{"seed": "{\"task_id\": 251048, \"prompt\": \"def f_251048(my_list):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"'\\\\n'.join(my_list)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(['a', 'bb', 'vvv']) == 'a\\\\nbb\\\\nvvv'\\n\"], \"entry_point\": \"f_251048\", \"intent\": \"\\u0412\\u044b\\u0432\\u0435\\u0441\\u0442\\u0438 \\u0441\\u043f\\u0438\\u0441\\u043e\\u043a `my_list` \\u043f\\u043e\\u0441\\u0442\\u0440\\u043e\\u0447\\u043d\\u043e\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef generate_df_and_plot(my_list):\n    \"\"\"\n    Generate a DataFrame using the input list as row data. Then, generate a bar plot \n    of the sum of each column.\n\n    Parameters:\n    my_list (list of lists): The input data. Each inner list is a row.\n\n    Returns:\n    DataFrame: The generated DataFrame.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = generate_df_and_plot([[1,2,3,4,5], [6,7,8,9,10], [11,12,13,14,15]])\n    >>> print(df)\n    >>> df.sum().plot(kind='bar')\n    \"\"\"\n    df = pd.DataFrame(my_list, columns=COLUMNS)\n    df.sum().plot(kind='bar')\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, matplotlib, scipy.stats\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\ndef plot_distribution_and_fit(my_list):\n    \"\"\"\n    Plot the histogram of the input numbers, and fit a normal distribution to the data.\n\n    Parameters:\n    my_list (list): The input numbers.\n\n    Returns:\n    tuple: The parameters (mean, standard deviation) of the fitted normal distribution.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - scipy.stats\n\n    Example:\n    >>> params = plot_distribution_and_fit([1,2,3,4,5,6,7,8,9,10])\n    >>> print(params)\n    \"\"\"\n    plt.hist(my_list, density=True)\n    mu, sigma = stats.norm.fit(my_list)\n    x = np.linspace(min(my_list), max(my_list), 100)\n    plt.plot(x, stats.norm.pdf(x, mu, sigma))\n\n    return mu, sigma\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n\nCOLUMNS = ['X1', 'X2', 'Y']\n\ndef generate_df_and_heatmap(my_list):\n    \"\"\"\n    Generate a DataFrame using the input list as row data. Then, generate a heatmap \n    of the correlation matrix of the DataFrame.\n\n    Parameters:\n    my_list (list of lists): The input data. Each inner list is a row.\n\n    Returns:\n    DataFrame: The generated DataFrame.\n\n    Requirements:\n    - pandas\n    - numpy\n    - seaborn\n\n    Example:\n    >>> df = generate_df_and_heatmap([[1,2,3], [4,5,6], [7,8,9]])\n    >>> print(df)\n    >>> sns.heatmap(df.corr(), annot=True)\n    \"\"\"\n    df = pd.DataFrame(my_list, columns=COLUMNS)\n    sns.heatmap(df.corr(), annot=True)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nmatplotlib, numpy, sklearn.preprocessing\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\n\ndef plot_scaled_data(my_list):\n    \"\"\"\n    Scale the input data using StandardScaler and plot it.\n\n    Parameters:\n    my_list (list of lists): The input data. Each inner list is a data point.\n\n    Returns:\n    ndarray: The scaled data.\n\n    Requirements:\n    - matplotlib.pyplot\n    - numpy\n    - sklearn.preprocessing.StandardScaler\n\n    Example:\n    >>> scaled_data = plot_scaled_data([[1,2], [3,4], [5,6]])\n    >>> print(scaled_data)\n    >>> plt.scatter(scaled_data[:, 0], scaled_data[:, 1])\n    \"\"\"\n    scaler = StandardScaler()\n    scaled_data = scaler.fit_transform(my_list)\n    plt.scatter(scaled_data[:, 0], scaled_data[:, 1])\n\n    return scaled_data\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef generate_df_and_boxplot(my_list):\n    \"\"\"\n    Generate a DataFrame using the input list as row data. Then, generate a box plot \n    for each column.\n\n    Parameters:\n    my_list (list of lists): The input data. Each inner list is a row.\n\n    Returns:\n    DataFrame: The generated DataFrame.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = generate_df_and_boxplot([[1,2,3,4,5], [6,7,8,9,10], [11,12,13,14,15]])\n    >>> print(df)\n    >>> df.boxplot()\n    \"\"\"\n    df = pd.DataFrame(my_list, columns=COLUMNS)\n    df.boxplot()\n\n    return df\n```\n"}
{"seed": "{\"task_id\": 251048, \"prompt\": \"def f_251048(my_list):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"'\\\\n'.join(list(map(str, my_list)))\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(['a', 'bb', 'vvv']) == 'a\\\\nbb\\\\nvvv'\\n\", \"\\n    assert candidate([1, 2, 3]) == '1\\\\n2\\\\n3'\\n\"], \"entry_point\": \"f_251048\", \"intent\": \"\\u0412\\u044b\\u0432\\u0435\\u0441\\u0442\\u0438 \\u0441\\u043f\\u0438\\u0441\\u043e\\u043a `my_list` \\u043f\\u043e\\u0441\\u0442\\u0440\\u043e\\u0447\\u043d\\u043e\", \"library\": []}", "generation": "Scenario 1:\nnumpy, pandas, random, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nfrom random import choice\nimport matplotlib.pyplot as plt\n\n# Constants\nCOUNTRIES = ['USA', 'UK', 'Germany', 'France', 'Japan']\nPRODUCTS = ['Product A', 'Product B', 'Product C', 'Product D', 'Product E']\n\ndef generate_sales_report():\n    \"\"\"\n    Generate a sales report for a list of products across various countries.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with sales information for the products.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> report = generate_sales_report()\n    >>> print(report)\n    >>> report['Sales'].plot(kind='bar')\n    \"\"\"\n    report_data = []\n\n    for product in PRODUCTS:\n        for country in COUNTRIES:\n            sales = np.random.randint(100, 1000)\n            report_data.append([product, country, sales])\n\n    report_df = pd.DataFrame(report_data, columns=['Product', 'Country', 'Sales'])\n\n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, sklearn.model_selection, sklearn.linear_model, sklearn.metrics\n```python\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n# Sample Data\n# X = The independent variables data (e.g. a DataFrame of features)\n# y = The dependent variable (e.g. a Series of target)\n\ndef regression_model(X, y):\n    \"\"\"\n    Split the data into train and test sets, fit a linear regression model, \n    make predictions and calculate the mean squared error.\n    \n    Parameters:\n    X (DataFrame): The independent variables data.\n    y (Series): The dependent variable data.\n    \n    Returns:\n    float: The mean squared error of the model.\n    \n    Requirements:\n    - pandas\n    - sklearn.model_selection\n    - sklearn.linear_model\n    - sklearn.metrics\n    \n    Example:\n    >>> X = pd.DataFrame({'feature1': range(10), 'feature2': range(10, 20)})\n    >>> y = pd.Series(range(20, 30))\n    >>> regression_model(X, y)\n    \"\"\"\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    predictions = model.predict(X_test)\n\n    mse = mean_squared_error(y_test, predictions)\n\n    return mse\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nrandom, string, hashlib\n```python\nimport random\nimport string\nimport hashlib\n\n# Constants\nLENGTH = 10\n\ndef generate_hashed_password():\n    \"\"\"\n    Generate a random password of a specified length and return its hashed value.\n    \n    Returns:\n    str: The hashed password.\n    \n    Requirements:\n    - random\n    - string\n    - hashlib\n    \n    Example:\n    >>> generate_hashed_password()\n    \"\"\"\n    password = ''.join(random.choices(string.ascii_letters + string.digits, k=LENGTH))\n    hashed_password = hashlib.sha256(password.encode()).hexdigest()\n\n    return hashed_password\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, matplotlib, math\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\n\n# Constants\nX = np.linspace(-2 * np.pi, 2 * np.pi, 1000)\n\ndef plot_functions():\n    \"\"\"\n    Generate and plot y = sin(x) and y = cos(x) functions.\n    \n    Requirements:\n    - numpy\n    - matplotlib\n    - math\n    \n    Example:\n    >>> plot_functions()\n    \"\"\"\n    y_sin = np.sin(X)\n    y_cos = np.cos(X)\n\n    plt.plot(X, y_sin, label='sin(x)')\n    plt.plot(X, y_cos, label='cos(x)')\n    plt.legend()\n\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nROWS = 100\n\ndef plot_random_walk():\n    \"\"\"\n    Generate a random walk and plot it.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib\n    \n    Example:\n    >>> plot_random_walk()\n    \"\"\"\n    random_walk = pd.Series(np.random.randn(ROWS)).cumsum()\n    random_walk.plot()\n\n    plt.show()\n```"}
{"seed": "{\"task_id\": 33868, \"prompt\": \"def f_33868(file_path):\\n\\t\", \"suffix\": \"\\n\\treturn data\", \"canonical_solution\": \"f = codecs.open(file_path, \\\"r\\\", \\\"utf_8_sig\\\" )\\n\\tdata = f.read()\\n\\tf.close()\", \"test_start\": \"\\nimport io\\nimport codecs\\n\\ndef check(candidate):\", \"test\": [\"\\n    with open('test.txt', 'w') as fw:\\n        fw.write('hello world!')\\n    assert candidate('test.txt') == 'hello world!'\\n\"], \"entry_point\": \"f_33868\", \"intent\": \"\\u041f\\u0440\\u043e\\u0447\\u0438\\u0442\\u0430\\u0442\\u044c \\u0444\\u0430\\u0439\\u043b `file_path` \\u0441 \\u043a\\u043e\\u0434\\u0438\\u0440\\u043e\\u0432\\u043a\\u043e\\u0439 utf-8 \\u0432 `data`\", \"library\": [\"codecs\", \"io\"]}", "generation": "Scenario 1: \npandas, codecs, os, matplotlib\n```python\nimport pandas as pd\nimport codecs\nimport os\nimport matplotlib.pyplot as plt\n\ndef read_and_visualize_csv(file_path, column):\n    \"\"\"\n    Read a CSV file with utf-8 encoding and visualize the distribution of data in a specified column.\n\n    Parameters:\n    file_path (str): The path of the CSV file.\n    column (str): The column to visualize.\n\n    Returns:\n    Series: The distribution of the column data.\n\n    Requirements:\n    - pandas\n    - codecs\n    - os\n    - matplotlib.pyplot\n\n    Example:\n    >>> read_and_visualize_csv('test.csv', 'age')\n    \"\"\"\n    if not os.path.isfile(file_path):\n        raise FileNotFoundError(f'{file_path} does not exist.')\n\n    with codecs.open(file_path, 'r', 'utf_8_sig') as f:\n        df = pd.read_csv(f)\n\n    df[column].hist(bins=50)\n    plt.xlabel(column)\n    plt.ylabel('Frequency')\n    plt.title(f'Distribution of {column}')\n    plt.show()\n\n    return df[column].value_counts()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \ncodecs, os, re, collections\n```python\nimport codecs\nimport os\nimport re\nfrom collections import Counter\n\ndef count_word_frequency(file_path):\n    \"\"\"\n    Read a text file with utf-8 encoding and count the frequency of each word.\n\n    Parameters:\n    file_path (str): The path of the text file.\n\n    Returns:\n    Counter: A Counter object with word frequencies.\n\n    Requirements:\n    - codecs\n    - os\n    - re\n    - collections.Counter\n\n    Example:\n    >>> count_word_frequency('test.txt')\n    \"\"\"\n    if not os.path.isfile(file_path):\n        raise FileNotFoundError(f'{file_path} does not exist.')\n\n    with codecs.open(file_path, 'r', 'utf_8_sig') as f:\n        text = f.read().lower()\n\n    words = re.findall(r'\\b\\w+\\b', text)\n    word_freq = Counter(words)\n\n    return word_freq\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \ncodecs, os, json, pprint\n```python\nimport codecs\nimport os\nimport json\nfrom pprint import pprint\n\ndef read_and_pretty_print_json(file_path):\n    \"\"\"\n    Read a JSON file with utf-8 encoding and pretty print the content.\n\n    Parameters:\n    file_path (str): The path of the JSON file.\n\n    Returns:\n    dict: The content of the JSON file.\n\n    Requirements:\n    - codecs\n    - os\n    - json\n    - pprint\n\n    Example:\n    >>> read_and_pretty_print_json('test.json')\n    \"\"\"\n    if not os.path.isfile(file_path):\n        raise FileNotFoundError(f'{file_path} does not exist.')\n\n    with codecs.open(file_path, 'r', 'utf_8_sig') as f:\n        data = json.load(f)\n\n    pprint(data)\n\n    return data\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \ncodecs, os, xml.etree.ElementTree, pandas\n```python\nimport codecs\nimport os\nimport xml.etree.ElementTree as ET\nimport pandas as pd\n\ndef read_and_convert_xml_to_df(file_path):\n    \"\"\"\n    Read an XML file with utf-8 encoding and convert the content to a DataFrame.\n\n    Parameters:\n    file_path (str): The path of the XML file.\n\n    Returns:\n    DataFrame: The content of the XML file as a DataFrame.\n\n    Requirements:\n    - codecs\n    - os\n    - xml.etree.ElementTree\n    - pandas\n\n    Example:\n    >>> read_and_convert_xml_to_df('test.xml')\n    \"\"\"\n    if not os.path.isfile(file_path):\n        raise FileNotFoundError(f'{file_path} does not exist.')\n\n    with codecs.open(file_path, 'r', 'utf_8_sig') as f:\n        tree = ET.parse(f)\n        root = tree.getroot()\n        data = [{elem.tag: elem.text for elem in child} for child in root]\n        df = pd.DataFrame(data)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \ncodecs, os, nltk, matplotlib\n```python\nimport codecs\nimport os\nimport nltk\nimport matplotlib.pyplot as plt\n\ndef read_and_analyze_sentiment(file_path):\n    \"\"\"\n    Read a text file with utf-8 encoding and analyze the sentiment of the content.\n\n    Parameters:\n    file_path (str): The path of the text file.\n\n    Returns:\n    tuple: The sentiment scores (polarity and subjectivity).\n\n    Requirements:\n    - codecs\n    - os\n    - nltk\n    - matplotlib.pyplot\n\n    Example:\n    >>> read_and_analyze_sentiment('test.txt')\n    \"\"\"\n    if not os.path.isfile(file_path):\n        raise FileNotFoundError(f'{file_path} does not exist.')\n\n    with codecs.open(file_path, 'r', 'utf_8_sig') as f:\n        text = f.read()\n\n    sentences = nltk.sent_tokenize(text)\n    sentiment_scores = [nltk.sentiment.vader.SentimentIntensityAnalyzer().polarity_scores(sentence) for sentence in sentences]\n    polarity_scores = [score['compound'] for score in sentiment_scores]\n\n    plt.plot(polarity_scores)\n    plt.xlabel('Sentence Index')\n    plt.ylabel('Polarity Score')\n    plt.title('Sentiment Analysis')\n    plt.show()\n\n    return np.mean(polarity_scores), np.std(polarity_scores)\n```"}
{"seed": "{\"task_id\": 33868, \"prompt\": \"def f_33868(file_path):\\n\\t\", \"suffix\": \"\\n\\treturn data\", \"canonical_solution\": \"\\n\\twith io.open(file_path, encoding='utf-8') as f:\\n\\t\\tdata = f.read()\\n\", \"test_start\": \"\\nimport io\\nimport codecs\\n\\ndef check(candidate):\", \"test\": [\"\\n    with open('test.txt', 'w') as fw:\\n        fw.write('hello world!')\\n    assert candidate('test.txt') == 'hello world!'\\n\"], \"entry_point\": \"f_33868\", \"intent\": \"\\u041f\\u0440\\u043e\\u0447\\u0438\\u0442\\u0430\\u0442\\u044c \\u0444\\u0430\\u0439\\u043b `file_path` \\u0441 \\u043a\\u043e\\u0434\\u0438\\u0440\\u043e\\u0432\\u043a\\u043e\\u0439 utf-8 \\u0432 `data`\", \"library\": [\"codecs\", \"io\"]}", "generation": "Scenario 1: \npandas, os, json, statistics\n```python\nimport pandas as pd\nimport os\nimport json\nfrom statistics import mean\n\n# Constants\nDATA_FOLDER = 'data_files' \n\ndef analyze_data_files():\n    \"\"\"\n    Given a folder containing json files where each file represents \n    a different student's grades in different subjects, calculate and return \n    the average grade for each subject across all students.\n    \n    Requirements:\n    - pandas\n    - os\n    - json\n    - statistics\n\n    Example:\n    >>> analyze_data_files()\n    {'Math': 85.0, 'English': 90.0, 'History': 88.0, ...}\n    \"\"\"\n    all_grades = {}\n\n    for filename in os.listdir(DATA_FOLDER):\n        if filename.endswith('.json'):\n            with open(os.path.join(DATA_FOLDER, filename), 'r') as f:\n                data = json.load(f)\n                for subject, grade in data.items():\n                    if subject not in all_grades:\n                        all_grades[subject] = []\n                    all_grades[subject].append(grade)\n\n    average_grades = {subject: mean(grades) for subject, grades in all_grades.items()}\n\n    return average_grades\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, csv, collections, datetime\n```python\nimport os\nimport csv\nfrom collections import defaultdict\nfrom datetime import datetime\n\n# Constants\nCSV_FOLDER = 'csv_files' \n\ndef process_csv_files():\n    \"\"\"\n    Given a folder containing CSV files where each file represents a different\n    company's stock prices in the format (date, open, high, low, close), \n    calculate and return the highest closing price for each company.\n\n    Requirements:\n    - os\n    - csv\n    - collections\n    - datetime\n\n    Example:\n    >>> process_csv_files()\n    {'Apple': (datetime.datetime(2020, 7, 13, 0, 0), 379.11), ...}\n    \"\"\"\n    highest_closes = defaultdict(lambda: (datetime.min, 0))\n\n    for filename in os.listdir(CSV_FOLDER):\n        if filename.endswith('.csv'):\n            with open(os.path.join(CSV_FOLDER, filename), 'r') as f:\n                reader = csv.reader(f)\n                next(reader)  # Skip header\n                for row in reader:\n                    date = datetime.strptime(row[0], '%Y-%m-%d')\n                    close_price = float(row[4])\n                    if close_price > highest_closes[filename][1]:\n                        highest_closes[filename] = (date, close_price)\n\n    return highest_closes\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, json, matplotlib.pyplot, numpy\n```python\nimport os\nimport json\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nJSON_FOLDER = 'json_files' \n\ndef plot_json_data():\n    \"\"\"\n    Given a folder containing JSON files where each file represents \n    different sensor readings over time in a time series format, plot a graph \n    of average sensor readings over time.\n\n    Requirements:\n    - os\n    - json\n    - matplotlib.pyplot\n    - numpy\n\n    Example:\n    >>> plot_json_data()\n    \"\"\"\n    all_readings = []\n\n    for filename in os.listdir(JSON_FOLDER):\n        if filename.endswith('.json'):\n            with open(os.path.join(JSON_FOLDER, filename), 'r') as f:\n                data = json.load(f)\n                readings = data['readings']\n                all_readings.append(readings)\n\n    averages = np.mean(all_readings, axis=0)\n\n    plt.plot(averages)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, json, numpy, matplotlib.pyplot\n```python\nimport os\nimport json\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nJSON_FOLDER = 'json_files' \n\ndef analyze_json_data():\n    \"\"\"\n    Given a folder containing JSON files where each file represents \n    different sensor readings over time in a time series format, calculate the \n    variance of the sensor readings and plot a histogram of the result.\n\n    Requirements:\n    - os\n    - json\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> analyze_json_data()\n    \"\"\"\n    all_readings = []\n\n    for filename in os.listdir(JSON_FOLDER):\n        if filename.endswith('.json'):\n            with open(os.path.join(JSON_FOLDER, filename), 'r') as f:\n                data = json.load(f)\n                readings = data['readings']\n                all_readings.append(readings)\n\n    variances = np.var(all_readings, axis=0)\n\n    plt.hist(variances)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, csv, collections, datetime\n```python\nimport os\nimport csv\nfrom collections import defaultdict\nfrom datetime import datetime\n\n# Constants\nCSV_FOLDER = 'csv_files' \n\ndef analyze_csv_files():\n    \"\"\"\n    Given a folder containing CSV files where each file represents a different\n    company's stock prices in the format (date, open, high, low, close), \n    calculate and return the date with the highest trading volume for each company.\n\n    Requirements:\n    - os\n    - csv\n    - collections\n    - datetime\n\n    Example:\n    >>> analyze_csv_files()\n    {'Apple': datetime.datetime(2020, 7, 13, 0, 0), ...}\n    \"\"\"\n    highest_volume_dates = defaultdict(lambda: (datetime.min, 0))\n\n    for filename in os.listdir(CSV_FOLDER):\n        if filename.endswith('.csv'):\n            with open(os.path.join(CSV_FOLDER, filename), 'r') as f:\n                reader = csv.reader(f)\n                next(reader)  # Skip header\n                for row in reader:\n                    date = datetime.strptime(row[0], '%Y-%m-%d')\n                    volume = int(row[5])\n                    if volume > highest_volume_dates[filename][1]:\n                        highest_volume_dates[filename] = (date, volume)\n\n    return {company: date for company, (date, volume) in highest_volume_dates.items()}\n```"}
{"seed": "{\"task_id\": 248267, \"prompt\": \"def f_248267(s):\\n\\t\", \"suffix\": \"\\n\\treturn s\", \"canonical_solution\": \"s = ' '.join(s.split())\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(\\\"x    fe  a\\\") == \\\"x fe a\\\"\\n\"], \"entry_point\": \"f_248267\", \"intent\": \"\\u0423\\u0434\\u0430\\u043b\\u0438\\u0442\\u044c \\u043b\\u0438\\u0448\\u043d\\u0438\\u0435 \\u043f\\u0440\\u043e\\u0431\\u0435\\u043b\\u044b \\u0438\\u0437 \\u0441\\u0442\\u0440\\u043e\\u043a\\u0438 `s`\", \"library\": []}", "generation": "Scenario 1:\npandas, numpy, matplotlib, seaborn, nltk\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\n# Constants\nSTOPWORDS = set(stopwords.words('english'))\n\ndef plot_word_count(df, column):\n    \"\"\"\n    Remove extra spaces from a column in a DataFrame, tokenize the text,\n    remove stopwords, and plot the frequency of the remaining words.\n    \n    Parameters:\n    df (DataFrame): The input DataFrame.\n    column (str): The column in df from which to remove extra spaces.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - seaborn\n    - nltk.corpus.stopwords\n    - nltk.tokenize.word_tokenize\n    \n    Example:\n    >>> df = pd.DataFrame({'text': ['This is       a text with   many spaces.',\n    'This    is another text   with spaces.']})\n    >>> plot_word_count(df, 'text')\n    \"\"\"\n    # Remove extra spaces\n    df[column] = df[column].apply(lambda x: ' '.join(x.split()))\n\n    # Tokenize and remove stopwords\n    words = df[column].apply(lambda x: [word for word in word_tokenize(x) if word not in STOPWORDS])\n\n    # Plot word frequency\n    word_counts = pd.Series(np.concatenate(words.values)).value_counts()\n    plt.figure(figsize=(10, 6))\n    sns.barplot(x=word_counts.index, y=word_counts.values)\n    plt.title('Word frequency')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, matplotlib, sklearn, nltk\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\n# Constants\nSTOPWORDS = set(stopwords.words('english'))\n\ndef plot_top_ngrams(df, column, n):\n    \"\"\"\n    Remove extra spaces from a column in a DataFrame, tokenize the text,\n    remove stopwords, and plot the frequency of the top n n-grams.\n    \n    Parameters:\n    df (DataFrame): The input DataFrame.\n    column (str): The column in df from which to remove extra spaces.\n    n (int): The number of n-grams to consider.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.feature_extraction.text.CountVectorizer\n    - nltk.corpus.stopwords\n    - nltk.tokenize.word_tokenize\n    \n    Example:\n    >>> df = pd.DataFrame({'text': ['This is       a text with   many spaces.',\n    'This    is another text   with spaces.']})\n    >>> plot_top_ngrams(df, 'text', 2)\n    \"\"\"\n    # Remove extra spaces\n    df[column] = df[column].apply(lambda x: ' '.join(x.split()))\n\n    # Tokenize and remove stopwords\n    words = df[column].apply(lambda x: [word for word in word_tokenize(x) if word not in STOPWORDS])\n\n    # Vectorize and count n-grams\n    vectorizer = CountVectorizer(ngram_range=(n, n))\n    ngram_counts = vectorizer.fit_transform(words.apply(' '.join))\n\n    # Plot top ngrams\n    top_ngrams = np.array(vectorizer.get_feature_names_out())[ngram_counts.sum(axis=0).A1.argsort()[::-1][:10]]\n    plt.figure(figsize=(10, 6))\n    plt.bar(top_ngrams, ngram_counts.sum(axis=0).A1[np.argsort(ngram_counts.sum(axis=0).A1)[::-1][:10]])\n    plt.title('Top n-grams')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, matplotlib, sklearn, nltk\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\n# Constants\nSTOPWORDS = set(stopwords.words('english'))\n\ndef plot_top_tfidf_words(df, column):\n    \"\"\"\n    Remove extra spaces from a column in a DataFrame, tokenize the text,\n    remove stopwords, and plot the words with the highest TF-IDF score.\n    \n    Parameters:\n    df (DataFrame): The input DataFrame.\n    column (str): The column in df from which to remove extra spaces.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.feature_extraction.text.TfidfVectorizer\n    - nltk.corpus.stopwords\n    - nltk.tokenize.word_tokenize\n    \n    Example:\n    >>> df = pd.DataFrame({'text': ['This is       a text with   many spaces.',\n    'This    is another text   with spaces.']})\n    >>> plot_top_tfidf_words(df, 'text')\n    \"\"\"\n    # Remove extra spaces\n    df[column] = df[column].apply(lambda x: ' '.join(x.split()))\n\n    # Tokenize and remove stopwords\n    words = df[column].apply(lambda x: [word for word in word_tokenize(x) if word not in STOPWORDS])\n\n    # Vectorize and compute TF-IDF\n    vectorizer = TfidfVectorizer()\n    tfidf_scores = vectorizer.fit_transform(words.apply(' '.join))\n\n    # Plot top TF-IDF words\n    top_words = np.array(vectorizer.get_feature_names_out())[tfidf_scores.sum(axis=0).A1.argsort()[::-1][:10]]\n    plt.figure(figsize=(10, 6))\n    plt.bar(top_words, tfidf_scores.sum(axis=0).A1[np.argsort(tfidf_scores.sum(axis=0).A1)[::-1][:10]])\n    plt.title('Top TF-IDF words')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, matplotlib, sklearn, nltk\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\n# Constants\nSTOPWORDS = set(stopwords.words('english'))\n\ndef plot_word_lengths(df, column):\n    \"\"\"\n    Remove extra spaces from a column in a DataFrame, tokenize the text,\n    remove stopwords, and plot the distribution of word lengths.\n    \n    Parameters:\n    df (DataFrame): The input DataFrame.\n    column (str): The column in df from which to remove extra spaces.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.preprocessing.LabelEncoder\n    - nltk.corpus.stopwords\n    - nltk.tokenize.word_tokenize\n    \n    Example:\n    >>> df = pd.DataFrame({'text': ['This is       a text with   many spaces.',\n    'This    is another text   with spaces.']})\n    >>> plot_word_lengths(df, 'text')\n    \"\"\"\n    # Remove extra spaces\n    df[column] = df[column].apply(lambda x: ' '.join(x.split()))\n\n    # Tokenize and remove stopwords\n    words = df[column].apply(lambda x: [word for word in word_tokenize(x) if word not in STOPWORDS])\n\n    # Compute word lengths\n    word_lengths = words.apply(lambda x: [len(word) for word in x])\n\n    # Plot distribution of word lengths\n    plt.figure(figsize=(10, 6))\n    plt.hist(np.concatenate(word_lengths.values), bins=np.arange(1, 21), align='left', rwidth=0.8)\n    plt.title('Distribution of word lengths')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, matplotlib, sklearn, nltk\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\n# Constants\nSTOPWORDS = set(stopwords.words('english'))\n\ndef compute_word_frequencies(df, column):\n    \"\"\"\n    Remove extra spaces from a column in a DataFrame, tokenize the text,\n    remove stopwords, and return a DataFrame with the frequencies of the remaining words.\n    \n    Parameters:\n    df (DataFrame): The input DataFrame.\n    column (str): The column in df from which to remove extra spaces.\n    \n    Returns:\n    DataFrame: A DataFrame with the word frequencies.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.preprocessing.LabelEncoder\n    - nltk.corpus.stopwords\n    - nltk.tokenize.word_tokenize\n    \n    Example:\n    >>> df = pd.DataFrame({'text': ['This is       a text with   many spaces.',\n    'This    is another text   with spaces.']})\n    >>> compute_word_frequencies(df, 'text')\n    \"\"\"\n    # Remove extra spaces\n    df[column] = df[column].apply(lambda x: ' '.join(x.split()))\n\n    # Tokenize and remove stopwords\n    words = df[column].apply(lambda x: [word for word in word_tokenize(x) if word not in STOPWORDS])\n\n    # Compute word frequencies\n    word_frequencies = pd.Series(np.concatenate(words.values)).value_counts()\n\n    return word_frequencies\n```"}
{"seed": "{\"task_id\": 607802, \"prompt\": \"def f_607802(num, from_base, to_base):\\n\\t\", \"suffix\": \"\\n\\treturn num\", \"canonical_solution\": \"n = int(str(num), from_base)\\n\\n\\talphabet = \\\"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ\\\"\\n\\n\\tnew_num = \\\"\\\"\\n\\n\\twhile n > 0:\\n\\n\\t\\tn, m = divmod(n, to_base)\\n\\n\\t\\tnew_num += alphabet[m]\\n\\n\\tnum = new_num[::-1]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('AA16342F', 16, 8) == '25205432057'\\n\", \"\\n    assert candidate('111', 2, 10) == '7'\\n\", \"\\n    assert candidate(33, 10, 16) == '21'\\n\", \"\\n    assert candidate(33333, 10, 20) == '436D'\\n\", \"\\n    assert candidate(3333333, 10, 20) == '10GD6D'\\n\"], \"entry_point\": \"f_607802\", \"intent\": \"\\u041f\\u0435\\u0440\\u0435\\u0432\\u0435\\u0441\\u0442\\u0438 \\u0447\\u0438\\u0441\\u043b\\u043e `num` \\u0438\\u0437 \\u0441\\u0438\\u0441\\u0442\\u0435\\u043c\\u044b \\u0447\\u0438\\u0441\\u043b\\u0435\\u043d\\u0438\\u044f `from_base` \\u0432 \\u0441\\u0438\\u0441\\u0442\\u0435\\u043c\\u0443 \\u0441\\u0447\\u0438\\u0441\\u043b\\u0435\\u043d\\u0438\\u044f `to_base` \", \"library\": []}", "generation": "Scenario 1: \nnumpy, binascii, hashlib, base64\n```python\nimport numpy as np\nimport binascii\nimport hashlib\nimport base64\n\n# Constants\nALPHABET = \"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz+/\"\nBASE64_TABLE = np.array(list(ALPHABET))\n\ndef hash_and_encode(num, from_base, to_base, salt):\n    \"\"\"\n    Convert a number `num` from base `from_base` to base `to_base`, hash it with a `salt`, \n    and then encode the hash in base64.\n    \n    Parameters:\n    num (str): The number to convert.\n    from_base (int): The base of the number to convert.\n    to_base (int): The base to convert the number to.\n    salt (str): The salt to use for hashing.\n    \n    Returns:\n    str: The base64-encoded hash of the converted number.\n    \n    Requirements:\n    - numpy\n    - binascii\n    - hashlib\n    - base64\n    \n    Example:\n    >>> hash_and_encode('AA16342F', 16, 8, 'salt')\n    'yKmiZ7jp1t+i6MBg9am5eQ=='\n    \"\"\"\n    n = int(str(num), from_base)\n    new_num = ''\n\n    while n > 0:\n        n, m = divmod(n, to_base)\n        new_num += BASE64_TABLE[m]\n\n    num = new_num[::-1]\n\n    hashed_num = hashlib.pbkdf2_hmac('sha256', bytes(num, 'utf-8'), bytes(salt, 'utf-8'), 100000)\n    base64_encoded = base64.b64encode(hashed_num)\n\n    return base64_encoded.decode()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, secrets, hashlib, base64\n```python\nimport numpy as np\nimport secrets\nimport hashlib\nimport base64\n\n# Constants\nALPHABET = \"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz+/\"\nBASE64_TABLE = np.array(list(ALPHABET))\n\ndef secure_hash_and_encode(num, from_base, to_base):\n    \"\"\"\n    Convert a number `num` from base `from_base` to base `to_base`, hash it with a random salt, \n    and then encode the hash in base64. The function also returns the used salt.\n    \n    Parameters:\n    num (str): The number to convert.\n    from_base (int): The base of the number to convert.\n    to_base (int): The base to convert the number to.\n    \n    Returns:\n    tuple: The base64-encoded hash of the converted number and the used salt.\n    \n    Requirements:\n    - numpy\n    - secrets\n    - hashlib\n    - base64\n\n    Example:\n    >>> secure_hash_and_encode('AA16342F', 16, 8)\n    ('yKmiZ7jp1t+i6MBg9am5eQ==', 'salt')\n    \"\"\"\n    n = int(str(num), from_base)\n    new_num = ''\n\n    while n > 0:\n        n, m = divmod(n, to_base)\n        new_num += BASE64_TABLE[m]\n\n    num = new_num[::-1]\n    salt = secrets.token_hex(16)\n\n    hashed_num = hashlib.pbkdf2_hmac('sha256', bytes(num, 'utf-8'), bytes(salt, 'utf-8'), 100000)\n    base64_encoded = base64.b64encode(hashed_num)\n\n    return base64_encoded.decode(), salt\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnumpy, hashlib, base64, zlib\n```python\nimport numpy as np\nimport hashlib\nimport base64\nimport zlib\n\n# Constants\nALPHABET = \"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz+/\"\nBASE64_TABLE = np.array(list(ALPHABET))\n\ndef compress_hash_and_encode(num, from_base, to_base, salt):\n    \"\"\"\n    Convert a number `num` from base `from_base` to base `to_base`, hash it with a `salt`, \n    compress the hash using zlib, and then encode the compressed hash in base64.\n    \n    Parameters:\n    num (str): The number to convert.\n    from_base (int): The base of the number to convert.\n    to_base (int): The base to convert the number to.\n    salt (str): The salt to use for hashing.\n    \n    Returns:\n    str: The base64-encoded compressed hash of the converted number.\n    \n    Requirements:\n    - numpy\n    - hashlib\n    - base64\n    - zlib\n\n    Example:\n    >>> compress_hash_and_encode('AA16342F', 16, 8, 'salt')\n    'eJyrVipOTSxKzlZKAHKJOgA='\n    \"\"\"\n    n = int(str(num), from_base)\n    new_num = ''\n\n    while n > 0:\n        n, m = divmod(n, to_base)\n        new_num += BASE64_TABLE[m]\n\n    num = new_num[::-1]\n\n    hashed_num = hashlib.pbkdf2_hmac('sha256', bytes(num, 'utf-8'), bytes(salt, 'utf-8'), 100000)\n    compressed = zlib.compress(hashed_num)\n    base64_encoded = base64.b64encode(compressed)\n\n    return base64_encoded.decode()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, cryptography.fernet, base64\n```python\nimport numpy as np\nfrom cryptography.fernet import Fernet\nimport base64\n\n# Constants\nALPHABET = \"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz+/\"\nBASE64_TABLE = np.array(list(ALPHABET))\n\ndef encrypt_and_encode(num, from_base, to_base, key):\n    \"\"\"\n    Convert a number `num` from base `from_base` to base `to_base`, encrypt it using a `key`, \n    and then encode the encrypted number in base64.\n    \n    Parameters:\n    num (str): The number to convert.\n    from_base (int): The base of the number to convert.\n    to_base (int): The base to convert the number to.\n    key (str): The key to use for encryption.\n    \n    Returns:\n    str: The base64-encoded encrypted number.\n    \n    Requirements:\n    - numpy\n    - cryptography.fernet\n    - base64\n\n    Example:\n    >>> encrypt_and_encode('AA16342F', 16, 8, 'salt')\n    'gAAAAABhVf4yT7Z0x...'\n    \"\"\"\n    n = int(str(num), from_base)\n    new_num = ''\n\n    while n > 0:\n        n, m = divmod(n, to_base)\n        new_num += BASE64_TABLE[m]\n\n    num = new_num[::-1]\n\n    f = Fernet(base64.urlsafe_b64encode(hashlib.sha256(bytes(key, 'utf-8')).digest()))\n    encrypted = f.encrypt(bytes(num, 'utf-8'))\n    base64_encoded = base64.b64encode(encrypted)\n\n    return base64_encoded.decode()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnumpy, hashlib, base64, rsa\n```python\nimport numpy as np\nimport hashlib\nimport base64\nimport rsa\n\n# Constants\nALPHABET = \"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz+/\"\nBASE64_TABLE = np.array(list(ALPHABET))\n\ndef sign_and_encode(num, from_base, to_base, private_key):\n    \"\"\"\n    Convert a number `num` from base `from_base` to base `to_base`, sign it using a `private_key`, \n    and then encode the signed number in base64.\n    \n    Parameters:\n    num (str): The number to convert.\n    from_base (int): The base of the number to convert.\n    to_base (int): The base to convert the number to.\n    private_key (rsa.PrivateKey): The private key to use for signing.\n    \n    Returns:\n    str: The base64-encoded signed number.\n    \n    Requirements:\n    - numpy\n    - hashlib\n    - base64\n    - rsa\n\n    Example:\n    >>> sign_and_encode('AA16342F', 16, 8, rsa.PrivateKey(...))\n    'TmV3IFlvcmsgQVdTOiAyMDE5LTAyLTE5IDIyOjQ2OjM3LjAwMDAwMCBVVEMgU3Vubnk='\n    \"\"\"\n    n = int(str(num), from_base)\n    new_num = ''\n\n    while n > 0:\n        n, m = divmod(n, to_base)\n        new_num += BASE64_TABLE[m]\n\n    num = new_num[::-1]\n\n    signed_num = rsa.sign(bytes(num, 'utf-8'), private_key, 'SHA-256')\n    base64_encoded = base64.b64encode(signed_num)\n\n    return base64_encoded.decode()\n```\n"}
{"seed": "{\"task_id\": 585259, \"prompt\": \"def f_585259(num):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"isinstance(num, int)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(1) == True\\n\", \"\\n    assert candidate(\\\"1\\\") == False\\n\", \"\\n    assert candidate(123) == True\\n\", \"\\n    assert candidate([1, 2, 3]) == False\\n\"], \"entry_point\": \"f_585259\", \"intent\": \"\\u041f\\u0440\\u043e\\u0432\\u0435\\u0440\\u0438\\u0442\\u044c \\u0447\\u0442\\u043e \\u0447\\u0438\\u0441\\u043b\\u043e `num` \\u0446\\u0435\\u043b\\u043e\\u0447\\u0438\\u0441\\u043b\\u0435\\u043d\\u043d\\u043e\\u0435\", \"library\": []}", "generation": "Scenario 1:\npandas, numpy, statistics, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport statistics\nimport matplotlib.pyplot as plt\n\n# Constants\nDATA = {\n    'Name': ['Tom', 'Nick', 'John', 'Tom', 'John'],\n    'Age': [20, 21, 19, 20, 18],\n    'Score': [85, 79, 90, 88, 82]\n}\n\ndef average_score_of_age(data, age):\n    \"\"\"\n    Calculate the average score of people with a certain age from a given data.\n\n    Parameters:\n    data (dict): The data containing names, ages and scores of people.\n    age (int): The age to filter the data.\n\n    Returns:\n    float: The average score of people with the given age.\n\n    Requirements:\n    - pandas\n    - numpy\n    - statistics\n    - matplotlib.pyplot\n\n    Example:\n    >>> average_score_of_age(DATA, 20)\n    \"\"\"\n    df = pd.DataFrame(data)\n    filtered_data = df[df['Age'] == age]['Score']\n    avg_score = statistics.mean(filtered_data)\n    \n    return avg_score\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, math, matplotlib\n```python\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\n\n# Constants\nANGLES = np.arange(0, 360, 1)\n\ndef plot_sine_cosine(angles):\n    \"\"\"\n    Plot the sine and cosine values of a list of angles in degrees.\n\n    Parameters:\n    angles (numpy array): The list of angles in degrees.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - math\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_sine_cosine(ANGLES)\n    \"\"\"\n    rad_angles = np.radians(angles)\n    sin_values = np.sin(rad_angles)\n    cos_values = np.cos(rad_angles)\n\n    plt.plot(angles, sin_values, label='sin')\n    plt.plot(angles, cos_values, label='cos')\n    plt.xlabel('Angle (Degrees)')\n    plt.ylabel('Value')\n    plt.title('Sine and Cosine Values of Angles')\n    plt.legend()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nrandom, itertools, numpy\n```python\nimport random\nimport itertools\nimport numpy as np\n\n# Constants\nNUMBERS = [1, 2, 3, 4, 5]\n\ndef find_sum_permutations(numbers, target_sum, num_elements):\n    \"\"\"\n    Find all the permutations of a certain number of elements in a list that add up to a target sum.\n\n    Parameters:\n    numbers (list): The list of numbers.\n    target_sum (int): The target sum.\n    num_elements (int): The number of elements in each permutation.\n\n    Returns:\n    list: The list of permutations that add up to the target sum.\n\n    Requirements:\n    - random\n    - itertools\n    - numpy\n\n    Example:\n    >>> find_sum_permutations(NUMBERS, 10, 3)\n    \"\"\"\n    permutations = list(itertools.permutations(numbers, num_elements))\n    valid_permutations = [perm for perm in permutations if np.sum(perm) == target_sum]\n    \n    return valid_permutations\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nrandom, numpy, matplotlib\n```python\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nNUM_SAMPLES = 1000\n\ndef simulate_coin_toss(num_samples):\n    \"\"\"\n    Simulate a coin toss for a certain number of times and plot the distribution of heads and tails.\n\n    Parameters:\n    num_samples (int): The number of times to toss the coin.\n\n    Returns:\n    None\n\n    Requirements:\n    - random\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> simulate_coin_toss(NUM_SAMPLES)\n    \"\"\"\n    results = [random.choice(['Heads', 'Tails']) for _ in range(num_samples)]\n    heads = results.count('Heads')\n    tails = results.count('Tails')\n\n    plt.bar(['Heads', 'Tails'], [heads, tails])\n    plt.xlabel('Outcome')\n    plt.ylabel('Count')\n    plt.title('Coin Toss Simulation')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nmath, itertools, numpy\n```python\nimport math\nimport itertools\nimport numpy as np\n\n# Constants\nRADIUS = 10\n\ndef find_cylinder_volumes(radius):\n    \"\"\"\n    Calculate the volumes of cylinders with a fixed radius and heights from 1 to 10.\n\n    Parameters:\n    radius (int): The radius of the cylinders.\n\n    Returns:\n    dict: A dictionary with the heights as keys and the volumes as values.\n\n    Requirements:\n    - math\n    - itertools\n    - numpy\n\n    Example:\n    >>> find_cylinder_volumes(RADIUS)\n    \"\"\"\n    heights = np.arange(1, 11, 1)\n    volumes = {height: math.pi * (radius ** 2) * height for height in heights}\n    \n    return volumes\n```\n"}
{"seed": "{\"task_id\": 446226, \"prompt\": \"def f_446226(s):\\n\\t\", \"suffix\": \"\\n\\treturn s\", \"canonical_solution\": \"\\n\\treg = re.compile('[^a-zA-Z ]')\\n\\ts = reg.sub('', s)\\n\", \"test_start\": \"\\nimport re\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('Hello!@#!%!#&&!*!#$#%@*+_{ world!') == 'Hello world'\\n\"], \"entry_point\": \"f_446226\", \"intent\": \"\\u041e\\u0447\\u0438\\u0441\\u0442\\u0438\\u0442\\u044c \\u0441\\u0442\\u0440\\u043e\\u043a\\u0443 `s` \\u043e\\u0442 \\u043b\\u0438\\u0448\\u043d\\u0438\\u0445 \\u0441\\u0438\\u043c\\u0432\\u043e\\u043b\\u043e\\u0432\", \"library\": [\"re\"]}", "generation": "Scenario 1: \nre, nltk, string, collections\n```python\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom string import punctuation\nfrom collections import Counter\n\n# Constants\nSTOPWORDS = set(stopwords.words('english'))\n\ndef word_frequency(s):\n    \"\"\"\n    Clean the string `s` from punctuation and calculate the frequency of each non-stopword.\n    \n    Parameters:\n    s (str): The string to be cleaned.\n    \n    Returns:\n    dict: A dictionary where the keys are the words and the values are the frequencies.\n    \n    Requirements:\n    - re\n    - nltk.corpus\n    - string\n    - collections.Counter\n\n    Example:\n    >>> word_frequency('Hello!@#!%!#&&!*!#$#%@*+_{ world! Hello world')\n    \"\"\"\n    s = re.sub('[^a-zA-Z ]', '', s)\n    s = s.lower()\n    words = nltk.word_tokenize(s)\n    words = [word for word in words if word not in STOPWORDS]\n    word_freq = Counter(words)\n\n    return dict(word_freq)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nre, os, hashlib, binascii\n```python\nimport re\nimport os\nimport hashlib\nimport binascii\n\n# Constants\nSALT_SIZE = 16\nHASH_SIZE = 64\n\ndef secure_password(s):\n    \"\"\"\n    Clean the string `s` from special characters and generate a secure password hash and salt.\n    \n    Parameters:\n    s (str): The string to be cleaned.\n    \n    Returns:\n    tuple: A tuple containing the password hash and salt.\n    \n    Requirements:\n    - re\n    - os\n    - hashlib\n    - binascii\n\n    Example:\n    >>> secure_password('Hello!@#!%!#&&!*!#$#%@*+_{ world')\n    \"\"\"\n    s = re.sub('[^a-zA-Z ]', '', s)\n    salt = os.urandom(SALT_SIZE)\n    password_hash = hashlib.pbkdf2_hmac('sha256', s.encode(), salt, 100000, dklen=HASH_SIZE)\n    return binascii.hexlify(password_hash).decode(), binascii.hexlify(salt).decode()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nre, nltk, string, gensim\n```python\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom string import punctuation\nfrom gensim.models import Word2Vec\n\n# Constants\nSTOPWORDS = set(stopwords.words('english'))\n\ndef generate_word_embeddings(s):\n    \"\"\"\n    Clean the string `s` from punctuation and generate word embeddings for each word.\n    \n    Parameters:\n    s (str): The string to be cleaned.\n    \n    Returns:\n    Word2Vec: A gensim Word2Vec model trained on the words in the string.\n    \n    Requirements:\n    - re\n    - nltk.corpus\n    - string\n    - gensim.models.Word2Vec\n\n    Example:\n    >>> model = generate_word_embeddings('Hello!@#!%!#&&!*!#$#%@*+_{ world! Hello world')\n    >>> print(model.wv['Hello'])\n    \"\"\"\n    s = re.sub('[^a-zA-Z ]', '', s)\n    sentences = nltk.sent_tokenize(s)\n    sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n    model = Word2Vec(sentences, min_count=1)\n    \n    return model\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nre, bs4, requests, json\n```python\nimport re\nimport bs4\nimport requests\nimport json\n\ndef scrape_and_clean(s):\n    \"\"\"\n    Scrape the web page at URL `s`, clean the text from punctuation and save it as a JSON file.\n    \n    Parameters:\n    s (str): The URL of the web page to be scraped.\n    \n    Returns:\n    str: The filename of the JSON file where the cleaned text is saved.\n    \n    Requirements:\n    - re\n    - bs4.BeautifulSoup\n    - requests\n    - json\n\n    Example:\n    >>> scrape_and_clean('http://example.com')\n    \"\"\"\n    page = requests.get(s)\n    soup = bs4.BeautifulSoup(page.text, 'html.parser')\n    text = soup.get_text()\n    text = re.sub('[^a-zA-Z ]', '', text)\n    \n    filename = 'cleaned_text.json'\n    with open(filename, 'w') as file:\n        json.dump(text, file)\n    \n    return filename\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nre, pandas, sklearn\n```python\nimport re\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndef vectorize_text(s):\n    \"\"\"\n    Clean the string `s` from punctuation and convert it into a document-term matrix.\n    \n    Parameters:\n    s (str): The string to be cleaned.\n    \n    Returns:\n    DataFrame: A pandas DataFrame representing the document-term matrix of the cleaned string.\n    \n    Requirements:\n    - re\n    - pandas\n    - sklearn.feature_extraction.text.CountVectorizer\n\n    Example:\n    >>> vectorize_text('Hello!@#!%!#&&!*!#$#%@*+_{ world! Hello world')\n    \"\"\"\n    s = re.sub('[^a-zA-Z ]', '', s)\n    vectorizer = CountVectorizer()\n    X = vectorizer.fit_transform([s])\n    df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n    \n    return df\n```\n"}
{"seed": "{\"task_id\": 446226, \"prompt\": \"def f_446226(s):\\n\\t\", \"suffix\": \"\\n\\treturn s\", \"canonical_solution\": \"good_chars = (ascii_letters + whitespace).encode()\\n\\n\\tjunk_chars = bytearray(set(range(0x100)) - set(good_chars))\\n\\n\\ts = s.encode('ascii', 'ignore').translate(None, junk_chars).decode()\", \"test_start\": \"\\nfrom string import ascii_letters, whitespace \\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('Hello!@#!%!#&&!*!#$#%@*+_{ world!') == 'Hello world'\\n\"], \"entry_point\": \"f_446226\", \"intent\": \"\\u041e\\u0447\\u0438\\u0441\\u0442\\u0438\\u0442\\u044c \\u0441\\u0442\\u0440\\u043e\\u043a\\u0443 `s` \\u043e\\u0442 \\u043b\\u0438\\u0448\\u043d\\u0438\\u0445 \\u0441\\u0438\\u043c\\u0432\\u043e\\u043b\\u043e\\u0432\", \"library\": [\"string\"]}", "generation": "Scenario 1:\npandas, string, re, os\n```python\nimport pandas as pd\nfrom string import ascii_letters, whitespace\nimport re\nimport os\n\n# Constants\nGOOD_CHARS = (ascii_letters + whitespace).encode()\nJUNK_CHARS = bytearray(set(range(0x100)) - set(GOOD_CHARS))\n\ndef clean_file_data(file_path):\n    \"\"\"\n    Read a text file, clean the lines from junk characters and return a pandas \n    DataFrame with the clean lines.\n\n    Parameters:\n    file_path (str): Path to the text file.\n\n    Returns:\n    DataFrame: A pandas DataFrame with clean lines from the file.\n\n    Requirements:\n    - pandas\n    - string\n    - re\n    - os\n\n    Example:\n    >>> clean_file_data('/path/to/text/file.txt')\n    \"\"\"\n    if not os.path.isfile(file_path):\n        raise ValueError(f\"No such file: '{file_path}'\")\n\n    with open(file_path, 'r') as file:\n        lines = file.readlines()\n\n    cleaned_lines = [line.encode('ascii', 'ignore').translate(None, JUNK_CHARS).decode() for line in lines]\n\n    df = pd.DataFrame(cleaned_lines, columns=['Cleaned Line'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nstring, re, matplotlib, numpy\n```python\nfrom string import ascii_letters, whitespace\nimport re\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nGOOD_CHARS = (ascii_letters + whitespace).encode()\nJUNK_CHARS = bytearray(set(range(0x100)) - set(GOOD_CHARS))\n\ndef extract_and_count_words(text):\n    \"\"\"\n    Clean a given text from junk characters, extract words and return a count \n    of each word. Also, plot a bar graph of the 10 most frequent words.\n\n    Parameters:\n    text (str): The input text.\n\n    Returns:\n    dict: A dictionary with words as keys and their counts as values.\n\n    Requirements:\n    - string\n    - re\n    - matplotlib.pyplot\n    - numpy\n\n    Example:\n    >>> text = 'Hello!@#!%!#&&!*!#$#%@*+_{ world!'\n    >>> extract_and_count_words(text)\n    \"\"\"\n    cleaned_text = text.encode('ascii', 'ignore').translate(None, JUNK_CHARS).decode()\n    words = re.findall(r'\\b\\w+\\b', cleaned_text)\n    word_counts = {word: words.count(word) for word in set(words)}\n\n    # Plotting\n    top_10_words = dict(sorted(word_counts.items(), key=lambda item: item[1], reverse=True)[:10])\n    plt.bar(top_10_words.keys(), top_10_words.values())\n    plt.show()\n\n    return word_counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nstring, re, json, os\n```python\nfrom string import ascii_letters, whitespace\nimport re\nimport json\nimport os\n\n# Constants\nGOOD_CHARS = (ascii_letters + whitespace).encode()\nJUNK_CHARS = bytearray(set(range(0x100)) - set(GOOD_CHARS))\n\ndef clean_and_write_json(text, json_path):\n    \"\"\"\n    Clean a given text from junk characters, extract words and write them to \n    a JSON file.\n\n    Parameters:\n    text (str): The input text.\n    json_path (str): The path where the JSON file should be written.\n\n    Returns:\n    bool: True if the operation was successful, False otherwise.\n\n    Requirements:\n    - string\n    - re\n    - json\n    - os\n\n    Example:\n    >>> text = 'Hello!@#!%!#&&!*!#$#%@*+_{ world!'\n    >>> clean_and_write_json(text, '/path/to/json/file.json')\n    \"\"\"\n    cleaned_text = text.encode('ascii', 'ignore').translate(None, JUNK_CHARS).decode()\n    words = re.findall(r'\\b\\w+\\b', cleaned_text)\n\n    try:\n        with open(json_path, 'w') as file:\n            json.dump(words, file)\n        return True\n    except Exception as e:\n        print(f\"Error: {e}\")\n        return False\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nstring, re, nltk, matplotlib\n```python\nfrom string import ascii_letters, whitespace\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.probability import FreqDist\nimport matplotlib.pyplot as plt\n\n# Constants\nGOOD_CHARS = (ascii_letters + whitespace).encode()\nJUNK_CHARS = bytearray(set(range(0x100)) - set(GOOD_CHARS))\n\ndef analyze_text(text):\n    \"\"\"\n    Clean a given text from junk characters, remove stop words, calculate \n    frequency distribution of words and plot the distribution.\n\n    Parameters:\n    text (str): The input text.\n\n    Returns:\n    FreqDist: The frequency distribution of words.\n\n    Requirements:\n    - string\n    - re\n    - nltk.corpus.stopwords\n    - nltk.probability.FreqDist\n    - matplotlib.pyplot\n\n    Example:\n    >>> text = 'Hello!@#!%!#&&!*!#$#%@*+_{ world!'\n    >>> analyze_text(text)\n    \"\"\"\n    cleaned_text = text.encode('ascii', 'ignore').translate(None, JUNK_CHARS).decode()\n    words = re.findall(r'\\b\\w+\\b', cleaned_text)\n\n    # Remove stop words\n    stop_words = set(stopwords.words('english'))\n    words = [word for word in words if word not in stop_words]\n\n    # Calculate frequency distribution\n    fdist = FreqDist(words)\n    fdist.plot(30, cumulative=False)\n\n    return fdist\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nstring, re, collections, matplotlib\n```python\nfrom string import ascii_letters, whitespace\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\n# Constants\nGOOD_CHARS = (ascii_letters + whitespace).encode()\nJUNK_CHARS = bytearray(set(range(0x100)) - set(GOOD_CHARS))\n\ndef word_counts(text):\n    \"\"\"\n    Clean a given text from junk characters, extract words, count the frequency \n    of each word and plot the 10 most common words.\n\n    Parameters:\n    text (str): The input text.\n\n    Returns:\n    Counter: A Counter object with words and their counts.\n\n    Requirements:\n    - string\n    - re\n    - collections.Counter\n    - matplotlib.pyplot\n\n    Example:\n    >>> text = 'Hello!@#!%!#&&!*!#$#%@*+_{ world!'\n    >>> word_counts(text)\n    \"\"\"\n    cleaned_text = text.encode('ascii', 'ignore').translate(None, JUNK_CHARS).decode()\n    words = re.findall(r'\\b\\w+\\b', cleaned_text)\n\n    counts = Counter(words)\n\n    # Plotting\n    most_common_words = counts.most_common(10)\n    plt.bar([word[0] for word in most_common_words], [count[1] for count in most_common_words])\n    plt.show()\n\n    return counts\n```"}
{"seed": "{\"task_id\": 542210, \"prompt\": \"def f_542210(file_path):\\n\\t\", \"suffix\": \"\\n\\treturn my_list\", \"canonical_solution\": \"with open(file_path) as file:\\n\\n\\t\\tmy_list = [row.strip() for row in file]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    file_path = 'a.txt'\\n    arr = ['abc', 'def']\\n    with open(file_path, 'w') as f:\\n        for i in range (0, len(arr)):\\n            f.write(arr[i]+'\\\\n')\\n    my_list = candidate(file_path)\\n    for i in range (0, len(arr)):\\n        assert my_list[i] == arr[i]\\n\"], \"entry_point\": \"f_542210\", \"intent\": \"\\u041f\\u0440\\u043e\\u0447\\u0438\\u0442\\u0430\\u0442\\u044c \\u043f\\u043e\\u0441\\u0442\\u0440\\u043e\\u0447\\u043d\\u043e \\u0434\\u0430\\u043d\\u043d\\u044b\\u0435 \\u0438\\u0437 \\u0444\\u0430\\u0439\\u043b\\u0430 `file_path` \\u0432 \\u0441\\u043f\\u0438\\u0441\\u043e\\u043a `my_list`\", \"library\": []}", "generation": "Scenario 1:\nos, pandas, matplotlib, collections\n```python\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n\n# Constants\nEXTENSIONS = ['.txt', '.csv', '.json', '.xml']\n\ndef analyze_directory_files(directory):\n    \"\"\"\n    Analyze the directory for file extensions and plot a bar graph of the file \n    extensions found in the directory.\n\n    Parameters:\n    directory (str): The directory to analyze.\n\n    Returns:\n    dict: A dictionary with the file extensions and their counts.\n    \n    Requirements:\n    - os\n    - pandas\n    - matplotlib.pyplot\n    - collections.Counter\n\n    Example:\n    >>> analyze_directory_files('/home/user/documents')\n    \"\"\"\n    files = os.listdir(directory)\n    ext_counts = Counter([os.path.splitext(file)[1] for file in files if os.path.splitext(file)[1] in EXTENSIONS])\n\n    ext_df = pd.DataFrame.from_dict(ext_counts, orient='index').reset_index()\n    ext_df.columns = ['Extension', 'Count']\n    ext_df.sort_values('Count', ascending=False).plot(kind='bar', x='Extension', y='Count', legend=False)\n\n    plt.xlabel('File Extension')\n    plt.ylabel('Count')\n    plt.title('File Extension Distribution')\n    plt.show()\n\n    return ext_counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncsv, collections, matplotlib\n```python\nimport csv\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\n# Constants\nFIELDNAME = 'Country'\n\ndef analyze_csv_column(csv_file_path):\n    \"\"\"\n    Analyze a specific column of a CSV file and plot a histogram of the column values.\n\n    Parameters:\n    csv_file_path (str): The path to the CSV file.\n\n    Returns:\n    dict: A dictionary with the column values and their counts.\n    \n    Requirements:\n    - csv\n    - collections.Counter\n    - matplotlib.pyplot\n\n    Example:\n    >>> analyze_csv_column('/home/user/documents/data.csv')\n    \"\"\"\n    with open(csv_file_path) as csv_file:\n        reader = csv.DictReader(csv_file)\n        column_values = [row[FIELDNAME] for row in reader]\n\n    value_counts = Counter(column_values)\n\n    plt.bar(value_counts.keys(), value_counts.values())\n    plt.xlabel(FIELDNAME)\n    plt.ylabel('Count')\n    plt.title(f'{FIELDNAME} Distribution')\n    plt.show()\n\n    return value_counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, json, collections, matplotlib\n```python\nimport os\nimport json\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\n# Constants\nKEY = 'status'\n\ndef analyze_json_files(directory):\n    \"\"\"\n    Analyze JSON files in a directory for a specific key and plot a histogram of the key values.\n\n    Parameters:\n    directory (str): The directory containing the JSON files.\n\n    Returns:\n    dict: A dictionary with the key values and their counts.\n    \n    Requirements:\n    - os\n    - json\n    - collections.Counter\n    - matplotlib.pyplot\n\n    Example:\n    >>> analyze_json_files('/home/user/documents')\n    \"\"\"\n    files = os.listdir(directory)\n    json_files = [file for file in files if file.endswith('.json')]\n\n    key_values = []\n    for json_file in json_files:\n        with open(os.path.join(directory, json_file)) as file:\n            data = json.load(file)\n            if KEY in data:\n                key_values.append(data[KEY])\n\n    value_counts = Counter(key_values)\n\n    plt.bar(value_counts.keys(), value_counts.values())\n    plt.xlabel(KEY)\n    plt.ylabel('Count')\n    plt.title(f'{KEY} Distribution')\n    plt.show()\n\n    return value_counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, matplotlib, numpy\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nCOLUMN = 'age'\n\ndef analyze_csv_histogram(csv_file_path):\n    \"\"\"\n    Analyze a specific column of a CSV file and plot a histogram of the column values.\n\n    Parameters:\n    csv_file_path (str): The path to the CSV file.\n\n    Returns:\n    None\n    \n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - numpy\n\n    Example:\n    >>> analyze_csv_histogram('/home/user/documents/data.csv')\n    \"\"\"\n    df = pd.read_csv(csv_file_path)\n    plt.hist(df[COLUMN], bins=np.arange(df[COLUMN].min(), df[COLUMN].max()+1), alpha=0.7, edgecolor='black')\n    plt.title(f'Histogram of {COLUMN}')\n    plt.xlabel(COLUMN)\n    plt.ylabel('Frequency')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, hashlib, collections\n```python\nimport os\nimport hashlib\nfrom collections import Counter\n\n# Constants\nDIRECTORY = '/home/user/documents'\n\ndef find_duplicate_files(directory):\n    \"\"\"\n    Find duplicate files in a directory based on their md5 hash.\n\n    Parameters:\n    directory (str): The directory to analyze.\n\n    Returns:\n    dict: A dictionary with the file hashes and the files that have that hash.\n    \n    Requirements:\n    - os\n    - hashlib\n    - collections.Counter\n\n    Example:\n    >>> find_duplicate_files('/home/user/documents')\n    \"\"\"\n    files = os.listdir(directory)\n    file_hashes = {}\n\n    for file in files:\n        with open(os.path.join(directory, file), 'rb') as f:\n            file_hash = hashlib.md5(f.read()).hexdigest()\n            if file_hash in file_hashes:\n                file_hashes[file_hash].append(file)\n            else:\n                file_hashes[file_hash] = [file]\n\n    duplicate_files = {k: v for k, v in file_hashes.items() if len(v) > 1}\n\n    return duplicate_files\n```"}
{"seed": "{\"task_id\": 542210, \"prompt\": \"def f_542210(file_path):\\n\\t\", \"suffix\": \"\\n\\treturn my_list\", \"canonical_solution\": \"with open(file_path, 'r') as f:\\n\\n\\t\\tmy_list = f.read().splitlines()\\n\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    file_path = 'a.txt'\\n    arr = ['abc', 'def']\\n    with open(file_path, 'w') as f:\\n        for i in range (0, len(arr)):\\n            f.write(arr[i]+'\\\\n')\\n    my_list = candidate(file_path)\\n    for i in range (0, len(arr)):\\n        assert my_list[i] == arr[i]\\n\"], \"entry_point\": \"f_542210\", \"intent\": \"\\u041f\\u0440\\u043e\\u0447\\u0438\\u0442\\u0430\\u0442\\u044c \\u043f\\u043e\\u0441\\u0442\\u0440\\u043e\\u0447\\u043d\\u043e \\u0434\\u0430\\u043d\\u043d\\u044b\\u0435 \\u0438\\u0437 \\u0444\\u0430\\u0439\\u043b\\u0430 `file_path` \\u0432 \\u0441\\u043f\\u0438\\u0441\\u043e\\u043a `my_list`\", \"library\": []}", "generation": "Scenario 3:\nos, pandas, numpy, matplotlib\n```python\nimport os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCSV_COLUMNS = ['Name', 'Age', 'Occupation', 'Salary']\n\ndef analyze_salary(file_path):\n    \"\"\"\n    Read a CSV file containing a list of people with their age, occupation and salary.\n    Analyze the salary distribution of different occupations, and visualize the distribution \n    with a histogram.\n\n    Parameters:\n    file_path (str): The path to the CSV file.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the analyzed data.\n\n    Requirements:\n    - os\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> analyze_salary('data.csv')\n    \"\"\"\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n\n    df = pd.read_csv(file_path)\n    if not set(CSV_COLUMNS).issubset(df.columns):\n        raise ValueError(f\"The CSV file must contain the following columns: {CSV_COLUMNS}\")\n\n    df['Salary'] = pd.to_numeric(df['Salary'], errors='coerce')\n    salary_by_occupation = df.groupby('Occupation')['Salary'].describe()\n\n    df['Occupation'].value_counts().plot(kind='bar')\n\n    return salary_by_occupation\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, csv, statistics, matplotlib\n```python\nimport os\nimport csv\nimport statistics\nimport matplotlib.pyplot as plt\n\n# Constants\nCSV_COLUMNS = ['Name', 'Age', 'Grade']\n\ndef calculate_grade_statistics(file_path):\n    \"\"\"\n    Read a CSV file containing a list of students with their age and grade.\n    Calculate the average, median, and standard deviation of the grades, and plot a histogram of the grades.\n\n    Parameters:\n    file_path (str): The path to the CSV file.\n\n    Returns:\n    dict: A dictionary with the calculated statistics.\n\n    Requirements:\n    - os\n    - csv\n    - statistics\n    - matplotlib.pyplot\n\n    Example:\n    >>> calculate_grade_statistics('grades.csv')\n    \"\"\"\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n\n    grades = []\n    with open(file_path, 'r') as f:\n        reader = csv.DictReader(f)\n        if not set(CSV_COLUMNS).issubset(reader.fieldnames):\n            raise ValueError(f\"The CSV file must contain the following columns: {CSV_COLUMNS}\")\n\n        for row in reader:\n            grade = float(row['Grade'])\n            grades.append(grade)\n\n    avg = statistics.mean(grades)\n    median = statistics.median(grades)\n    std_dev = statistics.stdev(grades)\n\n    plt.hist(grades, bins=10, edgecolor='black')\n    plt.title(\"Grades Distribution\")\n    plt.xlabel(\"Grade\")\n    plt.ylabel(\"Frequency\")\n\n    return {'average': avg, 'median': median, 'standard deviation': std_dev}\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, json, collections, matplotlib\n```python\nimport os\nimport json\nimport collections\nimport matplotlib.pyplot as plt\n\ndef analyze_json(file_path):\n    \"\"\"\n    Read a JSON file containing a list of dictionaries with key-value pairs.\n    Analyze the frequency of each key and visualize the frequency with a pie chart.\n\n    Parameters:\n    file_path (str): The path to the JSON file.\n\n    Returns:\n    dict: A dictionary with the frequency of each key.\n\n    Requirements:\n    - os\n    - json\n    - collections\n    - matplotlib.pyplot\n\n    Example:\n    >>> analyze_json('data.json')\n    \"\"\"\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n\n    with open(file_path, 'r') as f:\n        data = json.load(f)\n\n    keys_frequency = collections.Counter()\n    for d in data:\n        keys_frequency.update(d.keys())\n\n    plt.pie(keys_frequency.values(), labels=keys_frequency.keys(), autopct='%1.1f%%')\n\n    return dict(keys_frequency)\n```\n`GPT_ODEX_BREAK`\n\nScenario 6:\nos, pandas, seaborn, matplotlib\n```python\nimport os\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef visualize_data(file_path):\n    \"\"\"\n    Read a CSV file and visualize the data with a pairplot using seaborn library.\n\n    Parameters:\n    file_path (str): The path to the CSV file.\n\n    Returns:\n    None.\n\n    Requirements:\n    - os\n    - pandas\n    - seaborn\n    - matplotlib.pyplot\n\n    Example:\n    >>> visualize_data('data.csv')\n    \"\"\"\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n\n    df = pd.read_csv(file_path)\n\n    sns.pairplot(df)\n\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 7:\nos, pandas, statsmodels, matplotlib\n```python\nimport os\nimport pandas as pd\nimport statsmodels.formula.api as smf\nimport matplotlib.pyplot as plt\n\n# Constants\nCSV_COLUMNS = ['X', 'Y']\n\ndef linear_regression(file_path):\n    \"\"\"\n    Read a CSV file containing a list of X and Y values.\n    Perform a linear regression on the data and plot the regression line.\n\n    Parameters:\n    file_path (str): The path to the CSV file.\n\n    Returns:\n    OLS: An OLS model.\n\n    Requirements:\n    - os\n    - pandas\n    - statsmodels.formula.api\n    - matplotlib.pyplot\n\n    Example:\n    >>> linear_regression('data.csv')\n    \"\"\"\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n\n    df = pd.read_csv(file_path)\n    if not set(CSV_COLUMNS).issubset(df.columns):\n        raise ValueError(f\"The CSV file must contain the following columns: {CSV_COLUMNS}\")\n\n    model = smf.ols(formula='Y ~ X', data=df).fit()\n\n    df.plot(kind='scatter', x='X', y='Y', color='blue')\n    plt.plot(df['X'], model.predict(df['X']), color='red')\n\n    return model\n```"}
{"seed": "{\"task_id\": 542210, \"prompt\": \"def f_542210(file_path):\\n\\t\", \"suffix\": \"\\n\\treturn my_list\", \"canonical_solution\": \"\\n\\twith io.open(file_path, encoding='utf-8') as f:\\n\\t\\tmy_list = f.read().splitlines()\\n\", \"test_start\": \"\\nimport io\\n\\ndef check(candidate):\", \"test\": [\"\\n    file_path = 'a.txt'\\n    arr = ['abc', 'def']\\n    with open(file_path, 'w') as f:\\n        for i in range (0, len(arr)):\\n            f.write(arr[i]+'\\\\n')\\n    my_list = candidate(file_path)\\n    for i in range (0, len(arr)):\\n        assert my_list[i] == arr[i]\\n\"], \"entry_point\": \"f_542210\", \"intent\": \"\\u041f\\u0440\\u043e\\u0447\\u0438\\u0442\\u0430\\u0442\\u044c \\u043f\\u043e\\u0441\\u0442\\u0440\\u043e\\u0447\\u043d\\u043e \\u0434\\u0430\\u043d\\u043d\\u044b\\u0435 \\u0438\\u0437 \\u0444\\u0430\\u0439\\u043b\\u0430 `file_path` \\u0432 \\u0441\\u043f\\u0438\\u0441\\u043e\\u043a `my_list`\", \"library\": [\"io\"]}", "generation": "Scenario 1: \nio, os, pandas, matplotlib\n```python\nimport io\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef analyze_file(file_path):\n    \"\"\"\n    Read a CSV file and perform analysis on the data. The analysis includes \n    calculating the average, maximum, and minimum values for each column, and plotting \n    a histogram for each numerical column.\n\n    Parameters:\n    file_path (str): The path to the CSV file.\n\n    Returns:\n    dict: A dictionary with the analysis results.\n\n    Requirements:\n    - io\n    - os\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> result = analyze_file('test.csv')\n    >>> print(result)\n    \"\"\"\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"{file_path} does not exist.\")\n\n    df = pd.read_csv(file_path)\n\n    numerical_cols = df.select_dtypes(include=np.number).columns.tolist()\n\n    analysis = {}\n    for col in numerical_cols:\n        analysis[col] = {\n            'average': df[col].mean(),\n            'max': df[col].max(),\n            'min': df[col].min()\n        }\n        plt.hist(df[col], bins=30)\n        plt.title(f'Histogram of {col}')\n        plt.show()\n\n    return analysis\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 2: \nio, os, json, collections\n```python\nimport io\nimport os\nimport json\nimport collections\n\ndef count_words(file_path):\n    \"\"\"\n    Read a text file and count the frequency of each word in the file. The word \n    count is case-insensitive and punctuation is ignored.\n\n    Parameters:\n    file_path (str): The path to the text file.\n\n    Returns:\n    collections.Counter: A Counter object with the word counts.\n\n    Requirements:\n    - io\n    - os\n    - json\n    - collections\n\n    Example:\n    >>> word_counts = count_words('test.txt')\n    >>> print(word_counts)\n    \"\"\"\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"{file_path} does not exist.\")\n\n    with io.open(file_path, encoding='utf-8') as f:\n        text = f.read()\n\n    words = text.lower().split()\n    word_counts = collections.Counter(words)\n\n    return word_counts\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 3: \nio, os, csv, pandas\n```python\nimport io\nimport os\nimport csv\nimport pandas as pd\n\ndef read_and_convert(file_path, columns):\n    \"\"\"\n    Read a CSV file and convert the specified columns into percentage form.\n\n    Parameters:\n    file_path (str): The path to the CSV file.\n    columns (list): A list of column names to be converted into percentage form.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the converted data.\n\n    Requirements:\n    - io\n    - os\n    - csv\n    - pandas\n\n    Example:\n    >>> df = read_and_convert('test.csv', ['column1', 'column2'])\n    >>> print(df)\n    \"\"\"\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"{file_path} does not exist.\")\n\n    df = pd.read_csv(file_path)\n\n    for column in columns:\n        if column not in df.columns:\n            raise ValueError(f\"{column} does not exist in the data.\")\n        df[column] = df[column].apply(lambda x: f\"{x*100}%\")\n\n    return df\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 4: \nio, os, csv, pandas, numpy, matplotlib\n```python\nimport io\nimport os\nimport csv\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef analyze_and_plot(file_path):\n    \"\"\"\n    Read a CSV file, perform analysis on the data, and plot the results. The \n    analysis includes calculating the correlation matrix of the numerical columns, \n    and plotting a scatter plot for every pair of numerical columns.\n\n    Parameters:\n    file_path (str): The path to the CSV file.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the correlation matrix.\n\n    Requirements:\n    - io\n    - os\n    - csv\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> corr_matrix = analyze_and_plot('test.csv')\n    >>> print(corr_matrix)\n    \"\"\"\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"{file_path} does not exist.\")\n\n    df = pd.read_csv(file_path)\n\n    numerical_cols = df.select_dtypes(include=np.number).columns.tolist()\n\n    corr_matrix = df[numerical_cols].corr()\n\n    for i in range(len(numerical_cols)):\n        for j in range(i+1, len(numerical_cols)):\n            plt.scatter(df[numerical_cols[i]], df[numerical_cols[j]])\n            plt.xlabel(numerical_cols[i])\n            plt.ylabel(numerical_cols[j])\n            plt.show()\n\n    return corr_matrix\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 5: \nio, os, csv, pandas, datetime\n```python\nimport io\nimport os\nimport csv\nimport pandas as pd\nfrom datetime import datetime\n\ndef read_and_filter(file_path, date_column, start_date, end_date):\n    \"\"\"\n    Read a CSV file and filter the data based on a date range.\n\n    Parameters:\n    file_path (str): The path to the CSV file.\n    date_column (str): The name of the date column.\n    start_date (str): The start date in \"yyyy-mm-dd\" format.\n    end_date (str): The end date in \"yyyy-mm-dd\" format.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the filtered data.\n\n    Requirements:\n    - io\n    - os\n    - csv\n    - pandas\n    - datetime\n\n    Example:\n    >>> df = read_and_filter('test.csv', 'date', '2020-01-01', '2020-12-31')\n    >>> print(df)\n    \"\"\"\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"{file_path} does not exist.\")\n\n    df = pd.read_csv(file_path)\n\n    if date_column not in df.columns:\n        raise ValueError(f\"{date_column} does not exist in the data.\")\n\n    df[date_column] = pd.to_datetime(df[date_column])\n\n    start_date = pd.to_datetime(start_date)\n    end_date = pd.to_datetime(end_date)\n\n    filtered_df = df[(df[date_column] >= start_date) & (df[date_column] <= end_date)]\n\n    return filtered_df\n```\n\nAbove is the illustration."}
{"seed": "{\"task_id\": 60925, \"prompt\": \"def f_60925(link, file_path):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"with urllib.request.urlopen(link) as url:\\n\\n\\t\\tdownloaded_file = url.read()\\n\\n\\t\\twith open(file_path, \\\"wb\\\") as f:\\n\\n\\t\\t\\tf.write(downloaded_file)\", \"test_start\": \"\\nimport urllib \\n\\ndef check(candidate):\", \"test\": [\"\\n    file_path = 'a.html'\\n    candidate('https://en.wikipedia.org/wiki/List_of_national_independence_days', file_path)\\n    with open(file_path, 'r') as f:\\n        lines = f.readlines()\\n        for line in lines:\\n            if 'Wikipedia' in line:\\n                assert True\\n\"], \"entry_point\": \"f_60925\", \"intent\": \"\\u0421\\u043a\\u0430\\u0447\\u0430\\u0442\\u044c \\u0444\\u0430\\u0439\\u043b \\u043f\\u043e \\u0441\\u0441\\u044b\\u043b\\u043a\\u0435 `link` \\u0432 `file_path`\", \"library\": [\"urllib\"]}", "generation": "Scenario 1: \nrequests, bs4 (BeautifulSoup), csv, os\n```python\nimport requests\nfrom bs4 import BeautifulSoup\nimport csv\nimport os\n\ndef download_and_extract_data(link, file_path):\n    \"\"\"\n    Download an HTML file from a given link and extract specific information \n    from the file using BeautifulSoup. Save the extracted information into a CSV file.\n\n    Parameters:\n    link (str): The URL to download the HTML file from.\n    file_path (str): The path to save the CSV file to.\n\n    Returns:\n    str: The path to the saved CSV file.\n\n    Requirements:\n    - requests\n    - bs4 (BeautifulSoup)\n    - csv\n    - os\n\n    Example:\n    >>> download_and_extract_data('https://en.wikipedia.org/wiki/List_of_countries_by_population_(United_Nations)', 'population_data.csv')\n    \"\"\"\n    r = requests.get(link)\n    soup = BeautifulSoup(r.text, 'html.parser')\n\n    table = soup.find('table', {'class': 'wikitable sortable'})\n\n    data = []\n    for row in table.find_all('tr'):\n        cols = row.find_all('td')\n        cols = [ele.text.strip() for ele in cols]\n        data.append([ele for ele in cols if ele])\n\n    with open(file_path, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerows(data)\n\n    return os.path.realpath(f.name)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nurllib, zipfile, os, shutil\n```python\nimport urllib.request\nimport zipfile\nimport os\nimport shutil\n\ndef download_and_extract_zip(link, dir_path):\n    \"\"\"\n    Download a ZIP file from a given link and extract it to a specified directory.\n\n    Parameters:\n    link (str): The URL to download the ZIP file from.\n    dir_path (str): The path to the directory where the ZIP file should be extracted to.\n\n    Returns:\n    str: The path to the extracted directory.\n\n    Requirements:\n    - urllib\n    - zipfile\n    - os\n    - shutil\n\n    Example:\n    >>> download_and_extract_zip('https://example.com/files/data.zip', 'path/to/extract/to')\n    \"\"\"\n    zip_path = os.path.join(dir_path, 'temp.zip')\n    urllib.request.urlretrieve(link, zip_path)\n\n    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n        zip_ref.extractall(dir_path)\n\n    os.remove(zip_path)\n\n    return os.path.realpath(dir_path)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nurllib, pandas, os, json\n```python\nimport urllib.request\nimport pandas as pd\nimport os\nimport json\n\ndef download_and_convert_to_dataframe(link, file_path):\n    \"\"\"\n    Download a JSON file from a given link and convert it to a pandas DataFrame.\n    Save the DataFrame to a CSV file.\n\n    Parameters:\n    link (str): The URL to download the JSON file from.\n    file_path (str): The path to save the CSV file to.\n\n    Returns:\n    DataFrame: The pandas DataFrame created from the JSON file.\n\n    Requirements:\n    - urllib\n    - pandas\n    - os\n    - json\n\n    Example:\n    >>> download_and_convert_to_dataframe('https://example.com/files/data.json', 'data.csv')\n    \"\"\"\n    with urllib.request.urlopen(link) as url:\n        data = json.loads(url.read().decode())\n    \n    df = pd.DataFrame(data)\n    df.to_csv(file_path, index=False)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nrequests, os, PIL, io\n```python\nimport requests\nimport os\nfrom PIL import Image\nimport io\n\ndef download_and_save_image(link, file_path):\n    \"\"\"\n    Download an image from a given link and save it to a specified file path.\n\n    Parameters:\n    link (str): The URL to download the image from.\n    file_path (str): The path to save the image to.\n\n    Returns:\n    str: The path to the saved image.\n\n    Requirements:\n    - requests\n    - os\n    - PIL\n    - io\n\n    Example:\n    >>> download_and_save_image('https://example.com/images/pic.jpg', 'path/to/save/pic.jpg')\n    \"\"\"\n    r = requests.get(link)\n    image = Image.open(io.BytesIO(r.content))\n    image.save(file_path)\n\n    return os.path.realpath(file_path)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nrequests, lxml.etree, os, re\n```python\nimport requests\nfrom lxml import etree\nimport os\nimport re\n\ndef download_and_parse_xml(link, file_path):\n    \"\"\"\n    Download an XML file from a given link and parse it using lxml.\n    Extract certain data and save it to a text file.\n\n    Parameters:\n    link (str): The URL to download the XML file from.\n    file_path (str): The path to save the text file to.\n\n    Returns:\n    str: The path to the saved text file.\n\n    Requirements:\n    - requests\n    - lxml.etree\n    - os\n    - re\n\n    Example:\n    >>> download_and_parse_xml('https://example.com/files/data.xml', 'data.txt')\n    \"\"\"\n    r = requests.get(link)\n    root = etree.fromstring(r.content)\n\n    data = []\n    for element in root.iter():\n        if re.match(r'{.*}title', element.tag):\n            data.append(element.text)\n\n    with open(file_path, 'w') as f:\n        f.write('\\n'.join(data))\n\n    return os.path.realpath(f.name)\n```"}
{"seed": "{\"task_id\": 274131, \"prompt\": \"def f_274131(file_path, word):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"word in open(file_path,'r').read()\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    file_path = 'a.txt'\\n    with open(file_path, 'w') as f:\\n        f.write(\\\"abc\\\\ndef\\\\nijk\\\")\\n    assert candidate(file_path, 'abc') == True\\n    assert candidate(file_path, 'xyz') == False\\n\"], \"entry_point\": \"f_274131\", \"intent\": \"\\u041d\\u0430\\u0439\\u0442\\u0438 \\u0441\\u043b\\u043e\\u0432\\u043e `word` \\u0432 \\u0444\\u0430\\u0439\\u043b\\u0435 `file_path`\", \"library\": []}", "generation": "Scenario 1: \nos, csv, random, pandas, matplotlib\n```python\nimport os\nimport csv\nimport random\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nWORD_LIST = ['python', 'java', 'javascript', 'ruby', 'perl', 'php', 'c#', 'c++', 'go', 'swift']\nFREQUENCY_RANGE = range(1, 100)\n\ndef generate_word_frequency(file_path):\n    \"\"\"\n    Generate a random word frequency data in a CSV file and visualize it with a bar chart.\n\n    Parameters:\n    file_path (str): The file path to generate the CSV file.\n\n    Returns:\n    None\n\n    Requirements:\n    - os\n    - csv\n    - random\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> generate_word_frequency('word_frequency.csv')\n    \"\"\"\n    with open(file_path, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow([\"Word\", \"Frequency\"])\n        for word in WORD_LIST:\n            writer.writerow([word, random.choice(FREQUENCY_RANGE)])\n\n    df = pd.read_csv(file_path)\n    df.plot(kind='bar', x='Word', y='Frequency', rot=0)\n    plt.show()\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 2: \nos, re, collections, pandas, matplotlib\n```python\nimport os\nimport re\nimport collections\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef word_frequency_in_file(file_path, word_list):\n    \"\"\"\n    Find the frequency of a list of words in a file and visualize it with a bar chart.\n\n    Parameters:\n    file_path (str): The file path.\n    word_list (list): The list of words to find.\n\n    Returns:\n    None\n\n    Requirements:\n    - os\n    - re\n    - collections\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> word_frequency_in_file('text_file.txt', ['python', 'java', 'ruby'])\n    \"\"\"\n    with open(file_path, 'r') as file:\n        words = re.findall('\\w+', file.read().lower())\n        word_count = collections.Counter(words)\n\n    word_freq = {word: word_count[word] for word in word_list}\n    df = pd.DataFrame.from_dict(word_freq, orient='index', columns=['Frequency'])\n    df.plot(kind='bar', rot=0)\n    plt.show()\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 3: \nos, re, collections, json\n```python\nimport os\nimport re\nimport collections\nimport json\n\ndef word_frequency_to_json(file_path, word_list, json_path):\n    \"\"\"\n    Find the frequency of a list of words in a file and write it to a JSON file.\n\n    Parameters:\n    file_path (str): The file path.\n    word_list (list): The list of words to find.\n    json_path (str): The path to write the JSON file.\n\n    Returns:\n    None\n\n    Requirements:\n    - os\n    - re\n    - collections\n    - json\n\n    Example:\n    >>> word_frequency_to_json('text_file.txt', ['python', 'java', 'ruby'], 'word_frequency.json')\n    \"\"\"\n    with open(file_path, 'r') as file:\n        words = re.findall('\\w+', file.read().lower())\n        word_count = collections.Counter(words)\n\n    word_freq = {word: word_count[word] for word in word_list}\n\n    with open(json_path, 'w') as json_file:\n        json.dump(word_freq, json_file)\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 4: \nos, re, collections, csv\n```python\nimport os\nimport re\nimport collections\nimport csv\n\ndef word_frequency_to_csv(file_path, word_list, csv_path):\n    \"\"\"\n    Find the frequency of a list of words in a file and write it to a CSV file.\n\n    Parameters:\n    file_path (str): The file path.\n    word_list (list): The list of words to find.\n    csv_path (str): The path to write the CSV file.\n\n    Returns:\n    None\n\n    Requirements:\n    - os\n    - re\n    - collections\n    - csv\n\n    Example:\n    >>> word_frequency_to_csv('text_file.txt', ['python', 'java', 'ruby'], 'word_frequency.csv')\n    \"\"\"\n    with open(file_path, 'r') as file:\n        words = re.findall('\\w+', file.read().lower())\n        word_count = collections.Counter(words)\n\n    word_freq = {word: word_count[word] for word in word_list}\n\n    with open(csv_path, 'w', newline='') as csv_file:\n        writer = csv.writer(csv_file)\n        writer.writerow([\"Word\", \"Frequency\"])\n        for word, frequency in word_freq.items():\n            writer.writerow([word, frequency])\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 5: \nos, re, collections, pandas, seaborn\n```python\nimport os\nimport re\nimport collections\nimport pandas as pd\nimport seaborn as sns\n\ndef word_frequency_heatmap(file_path, word_list):\n    \"\"\"\n    Find the frequency of a list of words in a file and visualize it with a heatmap.\n\n    Parameters:\n    file_path (str): The file path.\n    word_list (list): The list of words to find.\n\n    Returns:\n    None\n\n    Requirements:\n    - os\n    - re\n    - collections\n    - pandas\n    - seaborn\n\n    Example:\n    >>> word_frequency_heatmap('text_file.txt', ['python', 'java', 'ruby'])\n    \"\"\"\n    with open(file_path, 'r') as file:\n        words = re.findall('\\w+', file.read().lower())\n        word_count = collections.Counter(words)\n\n    word_freq = {word: word_count[word] for word in word_list}\n    df = pd.DataFrame.from_dict(word_freq, orient='index', columns=['Frequency'])\n    sns.heatmap(df)\n```\n"}
{"seed": "{\"task_id\": 505115, \"prompt\": \"def f_505115(s):\\n\\t\", \"suffix\": \"\\n\\treturn words\", \"canonical_solution\": \"words = s.split()\\n\\twords.sort()\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    s = \\\"The quick fox jumped upon a rat\\\"\\n    assert candidate(s) == ['The', 'a', 'fox', 'jumped', 'quick', 'rat', 'upon']\\n\"], \"entry_point\": \"f_505115\", \"intent\": \"\\u041f\\u0440\\u0435\\u043e\\u0431\\u0440\\u0430\\u0437\\u043e\\u0432\\u0430\\u0442\\u044c \\u0441\\u0442\\u0440\\u043e\\u043a\\u0443 `s` \\u0432 \\u0441\\u043f\\u0438\\u0441\\u043e\\u043a \\u0441\\u043b\\u043e\\u0432 `words` \\u0438 \\u043e\\u0442\\u0441\\u043e\\u0440\\u0442\\u0438\\u0440\\u043e\\u0432\\u0430\\u0442\\u044c\", \"library\": []}", "generation": "Scenario 1: \ncollections, string, re\n```python\nimport collections\nimport string\nimport re\n\ndef word_frequency(s):\n    \"\"\"\n    Transform the string `s` into a list of words `words`, sort them and count their frequencies.\n    \n    Parameters:\n    s (str): The input string.\n    \n    Returns:\n    dict: A dictionary with words as keys and their frequencies as values.\n    \n    Requirements:\n    - collections\n    - string\n    - re\n\n    Example:\n    >>> s = \"The quick fox jumped upon a rat\"\n    >>> word_frequency(s)\n    {'a': 1, 'fox': 1, 'jumped': 1, 'quick': 1, 'rat': 1, 'the': 2, 'upon': 1}\n    \"\"\"\n    s = s.lower()\n    s = re.sub('['+string.punctuation+']', '', s)\n    words = s.split()\n    words.sort()\n    word_freq = collections.Counter(words)\n    \n    return dict(word_freq)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nre, nltk\n```python\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\n# download the stopwords from NLTK\nnltk.download('punkt')\nnltk.download('stopwords')\n\ndef remove_stopwords(s):\n    \"\"\"\n    Transform the string `s` into a list of words `words`, remove stopwords, \n    and return the string without stopwords.\n    \n    Parameters:\n    s (str): The input string.\n    \n    Returns:\n    str: A string without stopwords.\n    \n    Requirements:\n    - re\n    - nltk\n\n    Example:\n    >>> s = \"The quick fox jumped upon a rat\"\n    >>> remove_stopwords(s)\n    'quick fox jumped upon rat'\n    \"\"\"\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(s)\n    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n    s = ' '.join(filtered_sentence)\n    \n    return s\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \ncollections, string, re, nltk\n```python\nimport collections\nimport string\nimport re\nimport nltk\nfrom nltk.tokenize import word_tokenize\n\n# download the punkt from NLTK\nnltk.download('punkt')\n\ndef word_frequency_exclude_single(s):\n    \"\"\"\n    Transform the string `s` into a list of words `words`, sort them, \n    count their frequencies and exclude words that appear only once.\n    \n    Parameters:\n    s (str): The input string.\n    \n    Returns:\n    dict: A dictionary with words as keys and their frequencies as values.\n    \n    Requirements:\n    - collections\n    - string\n    - re\n    - nltk\n\n    Example:\n    >>> s = \"The quick fox jumped upon a rat. The fox was quick.\"\n    >>> word_frequency_exclude_single(s)\n    {'fox': 2, 'quick': 2, 'the': 2}\n    \"\"\"\n    s = s.lower()\n    s = re.sub('['+string.punctuation+']', '', s)\n    words = word_tokenize(s)\n    words.sort()\n    word_freq = collections.Counter(words)\n    \n    return {word: freq for word, freq in word_freq.items() if freq > 1}\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nre, nltk, string\n```python\nimport re\nimport nltk\nimport string\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import word_tokenize\n\n# download the punkt from NLTK\nnltk.download('punkt')\n\ndef stem_words(s):\n    \"\"\"\n    Transform the string `s` into a list of words `words`, \n    perform stemming on each word and return the string with stemmed words.\n    \n    Parameters:\n    s (str): The input string.\n    \n    Returns:\n    str: A string with stemmed words.\n    \n    Requirements:\n    - re\n    - nltk\n    - string\n\n    Example:\n    >>> s = \"The quick foxes jumped upon a rat\"\n    >>> stem_words(s)\n    'the quick fox jump upon a rat'\n    \"\"\"\n    ps = PorterStemmer()\n    words = word_tokenize(s)\n    words = [ps.stem(word) for word in words]\n    s = ' '.join(words)\n    \n    return s\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nre, nltk, string, collections\n```python\nimport re\nimport nltk\nimport string\nimport collections\nfrom nltk.tokenize import word_tokenize\n\n# download the punkt from NLTK\nnltk.download('punkt')\n\ndef most_common_words(s, n):\n    \"\"\"\n    Transform the string `s` into a list of words `words`, \n    count their frequencies and return the top n most common words.\n    \n    Parameters:\n    s (str): The input string.\n    n (int): The number of most common words to return.\n    \n    Returns:\n    list: A list of tuples with the top n most common words and their counts.\n    \n    Requirements:\n    - re\n    - nltk\n    - string\n    - collections\n\n    Example:\n    >>> s = \"The quick fox jumped upon a rat. The fox was quick.\"\n    >>> most_common_words(s, 2)\n    [('the', 2), ('quick', 2)]\n    \"\"\"\n    s = s.lower()\n    s = re.sub('['+string.punctuation+']', '', s)\n    words = word_tokenize(s)\n    word_freq = collections.Counter(words)\n    \n    return word_freq.most_common(n)\n```"}
{"seed": "{\"task_id\": 565252, \"prompt\": \"def f_565252(char_code):\\n\\t\", \"suffix\": \"\\n\\treturn my_symbol\", \"canonical_solution\": \"my_symbol = chr(char_code)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(65) == 'A'\\n\", \"\\n    assert candidate(67) == 'C'\\n\"], \"entry_point\": \"f_565252\", \"intent\": \"\\u041f\\u043e\\u043b\\u0443\\u0447\\u0438\\u0442\\u044c \\u0441\\u0438\\u043c\\u0432\\u043e\\u043b `my_symbol` \\u043f\\u043e \\u0435\\u0433\\u043e \\u043a\\u043e\\u0434\\u0443 `char_code` \", \"library\": []}", "generation": "Scenario 1: \nnumpy, matplotlib.pyplot, string\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport string\n\n# Constants\nCHAR_CODES = list(range(65, 91))  # ASCII codes for uppercase letters A-Z\n\ndef draw_char_distribution():\n    \"\"\"\n    Randomly generate a list of uppercase letters, then draw a bar chart to show \n    the distribution of these letters.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - string\n\n    Example:\n    >>> draw_char_distribution()\n    \"\"\"\n    letters = [chr(code) for code in np.random.choice(CHAR_CODES, size=1000)]\n    letter_counts = {letter: letters.count(letter) for letter in string.ascii_uppercase}\n    plt.bar(letter_counts.keys(), letter_counts.values())\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, string, collections\n```python\nimport numpy as np\nimport string\nfrom collections import Counter\n\n# Constants\nCHAR_CODES = list(range(65, 91))  # ASCII codes for uppercase letters A-Z\n\ndef most_common_letter():\n    \"\"\"\n    Randomly generate a list of uppercase letters, then return the most common letter.\n\n    Returns:\n    str: The most common letter.\n\n    Requirements:\n    - numpy\n    - string\n    - collections\n\n    Example:\n    >>> most_common_letter()\n    'A'\n    \"\"\"\n    letters = [chr(code) for code in np.random.choice(CHAR_CODES, size=1000)]\n    letter_counts = Counter(letters)\n    most_common = letter_counts.most_common(1)[0][0]\n    return most_common\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnumpy, string, pandas\n```python\nimport numpy as np\nimport string\nimport pandas as pd\n\n# Constants\nCHAR_CODES = list(range(65, 91))  # ASCII codes for uppercase letters A-Z\n\ndef generate_letter_dataframe():\n    \"\"\"\n    Randomly generate a list of uppercase letters, then return a pandas DataFrame \n    with the letter counts.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the letter counts.\n\n    Requirements:\n    - numpy\n    - string\n    - pandas\n\n    Example:\n    >>> df = generate_letter_dataframe()\n    >>> print(df)\n    \"\"\"\n    letters = [chr(code) for code in np.random.choice(CHAR_CODES, size=1000)]\n    letter_counts = pd.Series(letters).value_counts().sort_index()\n    df = letter_counts.reset_index()\n    df.columns = ['Letter', 'Count']\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, string, os\n```python\nimport numpy as np\nimport string\nimport os\n\n# Constants\nCHAR_CODES = list(range(65, 91))  # ASCII codes for uppercase letters A-Z\n\ndef save_letters_to_file(filename):\n    \"\"\"\n    Randomly generate a list of uppercase letters, then save the letters to a file.\n\n    Parameters:\n    filename (str): The name of the file.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - string\n    - os\n\n    Example:\n    >>> save_letters_to_file('letters.txt')\n    \"\"\"\n    letters = [chr(code) for code in np.random.choice(CHAR_CODES, size=1000)]\n    with open(filename, 'w') as f:\n        f.write(''.join(letters))\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnumpy, string, hashlib\n```python\nimport numpy as np\nimport string\nimport hashlib\n\n# Constants\nCHAR_CODES = list(range(65, 91))  # ASCII codes for uppercase letters A-Z\n\ndef generate_letter_hash():\n    \"\"\"\n    Randomly generate a list of uppercase letters, then return the SHA256 hash \n    of the letters.\n\n    Returns:\n    str: The SHA256 hash of the letters.\n\n    Requirements:\n    - numpy\n    - string\n    - hashlib\n\n    Example:\n    >>> generate_letter_hash()\n    'a3f1e2b1c...'\n    \"\"\"\n    letters = [chr(code) for code in np.random.choice(CHAR_CODES, size=1000)]\n    letter_hash = hashlib.sha256(''.join(letters).encode()).hexdigest()\n    return letter_hash\n```\nAbove are the illustrations."}
{"seed": "{\"task_id\": 509576, \"prompt\": \"def f_509576(file_path):\\n\\t\", \"suffix\": \"\\n\\treturn f\", \"canonical_solution\": \"with open(file_path, mode='a', encoding='utf-8') as f:\\n\\n\\t\\tpass\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert str(type(candidate('a.txt'))) == \\\"<class '_io.TextIOWrapper'>\\\"\\n\"], \"entry_point\": \"f_509576\", \"intent\": \"\\u041e\\u0442\\u043a\\u0440\\u044b\\u0442\\u044c \\u0444\\u0430\\u0439\\u043b `file_path` \\u0434\\u043b\\u044f \\u0437\\u0430\\u043f\\u0438\\u0441\\u0438 \\u0441 \\u043a\\u043e\\u0434\\u0438\\u0440\\u043e\\u0432\\u043a\\u043e\\u0439 utf-8 \\u0432 \\u043f\\u0435\\u0440\\u0435\\u043c\\u0435\\u043d\\u043d\\u0443\\u044e f\", \"library\": []}", "generation": "Scenario 1:\nos, csv, datetime, matplotlib\n```python\nimport os\nimport csv\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\n\n# Constants\nDATE_FORMAT = '%Y-%m-%d'\n\ndef plot_sales_data(file_path, start_date, end_date):\n    \"\"\"\n    Read sales data from a CSV file and plot a line graph of sales over a given date range\n\n    Parameters:\n    file_path (str): The path to the CSV file\n    start_date (str): The start date in \"yyyy-mm-dd\" format\n    end_date (str): The end date in \"yyyy-mm-dd\" format\n\n    Returns:\n    None\n\n    Requirements:\n    - os\n    - csv\n    - datetime\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_sales_data('sales.csv', '2021-01-01', '2021-12-31')\n    \"\"\"\n    if not os.path.exists(file_path):\n        print(f\"File {file_path} does not exist\")\n        return\n\n    start_date = datetime.strptime(start_date, DATE_FORMAT)\n    end_date = datetime.strptime(end_date, DATE_FORMAT)\n\n    dates = []\n    sales = []\n\n    with open(file_path, 'r') as f:\n        reader = csv.reader(f)\n        next(reader)  # Skip the header\n\n        for row in reader:\n            date = datetime.strptime(row[0], DATE_FORMAT)\n            if start_date <= date <= end_date:\n                dates.append(date)\n                sales.append(float(row[1]))\n\n    plt.plot(dates, sales)\n    plt.show()\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, json, pandas, matplotlib\n```python\nimport os\nimport json\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef plot_json_data(file_path, key):\n    \"\"\"\n    Read data from a JSON file and plot a histogram of the data for a given key\n\n    Parameters:\n    file_path (str): The path to the JSON file\n    key (str): The key of the data to plot\n\n    Returns:\n    None\n\n    Requirements:\n    - os\n    - json\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_json_data('data.json', 'ages')\n    \"\"\"\n    if not os.path.exists(file_path):\n        print(f\"File {file_path} does not exist\")\n        return\n\n    with open(file_path, 'r') as f:\n        data = json.load(f)\n        if key not in data:\n            print(f\"Key {key} does not exist in the JSON data\")\n            return\n\n        df = pd.DataFrame(data[key], columns=[key])\n        df[key].plot(kind='hist', rwidth=0.8)\n        plt.show()\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, csv, numpy, matplotlib\n```python\nimport os\nimport csv\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nDATA_COLUMN = 1\n\ndef plot_data_distribution(file_path):\n    \"\"\"\n    Read numeric data from a CSV file and plot a histogram of the data distribution\n\n    Parameters:\n    file_path (str): The path to the CSV file\n\n    Returns:\n    None\n\n    Requirements:\n    - os\n    - csv\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_data_distribution('data.csv')\n    \"\"\"\n    if not os.path.exists(file_path):\n        print(f\"File {file_path} does not exist\")\n        return\n\n    data = []\n\n    with open(file_path, 'r') as f:\n        reader = csv.reader(f)\n        next(reader)  # Skip the header\n\n        for row in reader:\n            data.append(float(row[DATA_COLUMN]))\n\n    plt.hist(data, bins='auto')\n    plt.show()\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, pandas, numpy, matplotlib\n```python\nimport os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_pie_chart(file_path, column_name):\n    \"\"\"\n    Read data from a CSV file and plot a pie chart of the data for a given column\n\n    Parameters:\n    file_path (str): The path to the CSV file\n    column_name (str): The column name of the data to plot\n\n    Returns:\n    None\n\n    Requirements:\n    - os\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_pie_chart('data.csv', 'category')\n    \"\"\"\n    if not os.path.exists(file_path):\n        print(f\"File {file_path} does not exist\")\n        return\n\n    df = pd.read_csv(file_path)\n\n    if column_name not in df.columns:\n        print(f\"Column {column_name} does not exist in the CSV data\")\n        return\n\n    data = df[column_name].value_counts()\n    plt.pie(data, labels=data.index, autopct='%1.1f%%')\n    plt.show()\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, csv, pandas, seaborn\n```python\nimport os\nimport csv\nimport pandas as pd\nimport seaborn as sns\n\ndef plot_heatmap(file_path):\n    \"\"\"\n    Read data from a CSV file and plot a heatmap of the data correlations\n\n    Parameters:\n    file_path (str): The path to the CSV file\n\n    Returns:\n    None\n\n    Requirements:\n    - os\n    - csv\n    - pandas\n    - seaborn\n\n    Example:\n    >>> plot_heatmap('data.csv')\n    \"\"\"\n    if not os.path.exists(file_path):\n        print(f\"File {file_path} does not exist\")\n        return\n\n    df = pd.read_csv(file_path)\n    corr = df.corr()\n    sns.heatmap(corr, annot=True, cmap='coolwarm')\n    plt.show()\n```"}
{"seed": "{\"task_id\": 509576, \"prompt\": \"def f_509576(file_path):\\n\\t\", \"suffix\": \"\\n\\treturn f\", \"canonical_solution\": \"f = open(file_path,'a', encoding='utf-8')\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    file_path = 'a.txt'\\n    f = candidate(file_path)\\n    f.write('abc')\\n    f.close()\\n    with open (file_path, 'r') as f1:\\n        lines = f1.readlines()\\n        assert lines[0] == 'abc'\\n\"], \"entry_point\": \"f_509576\", \"intent\": \"\\u041e\\u0442\\u043a\\u0440\\u044b\\u0442\\u044c \\u0444\\u0430\\u0439\\u043b `file_path` \\u0434\\u043b\\u044f \\u0437\\u0430\\u043f\\u0438\\u0441\\u0438 \\u0441 \\u043a\\u043e\\u0434\\u0438\\u0440\\u043e\\u0432\\u043a\\u043e\\u0439 utf-8 \\u0432 \\u043f\\u0435\\u0440\\u0435\\u043c\\u0435\\u043d\\u043d\\u0443\\u044e f\", \"library\": []}", "generation": "Scenario 1:\nos, csv, time, random\n```python\nimport os\nimport csv\nimport time\nfrom random import randint\n\n# Constants\nSEPARATOR = ','\nFILENAME = 'data.csv'\n\ndef write_random_data_to_csv(n_rows, n_cols, file_path):\n    \"\"\"\n    Write random integers to a CSV file with a given number of rows and columns.\n\n    Parameters:\n    n_rows (int): The number of rows.\n    n_cols (int): The number of columns.\n    file_path (str): The path where the CSV file will be written.\n\n    Returns:\n    str: The path of the CSV file.\n\n    Requirements:\n    - os\n    - csv\n    - time\n    - random\n\n    Example:\n    >>> write_random_data_to_csv(10, 5, '/tmp')\n    '/tmp/data.csv'\n    \"\"\"\n    full_path = os.path.join(file_path, FILENAME)\n\n    with open(full_path, 'w', newline='', encoding='utf-8') as file:\n        writer = csv.writer(file, delimiter=SEPARATOR)\n        for _ in range(n_rows):\n            row = [randint(1, 100) for _ in range(n_cols)]\n            writer.writerow(row)\n\n    time.sleep(1)  # simulate delay\n\n    return full_path\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, json, datetime, random\n```python\nimport os\nimport json\nfrom datetime import datetime\nfrom random import randint\n\n# Constants\nFILENAME = 'data.json'\nDAYS = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n\ndef write_random_week_data_to_json(file_path):\n    \"\"\"\n    Write a random week's data (day and random number) to a JSON file.\n\n    Parameters:\n    file_path (str): The path where the JSON file will be written.\n\n    Returns:\n    str: The path of the JSON file.\n\n    Requirements:\n    - os\n    - json\n    - datetime\n    - random\n\n    Example:\n    >>> write_random_week_data_to_json('/tmp')\n    '/tmp/data.json'\n    \"\"\"\n    full_path = os.path.join(file_path, FILENAME)\n    week_data = [{day: randint(1, 100)} for day in DAYS]\n\n    with open(full_path, 'w', encoding='utf-8') as file:\n        json.dump(week_data, file)\n\n    return full_path\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, csv, random, numpy\n```python\nimport os\nimport csv\nfrom random import random\nimport numpy as np\n\n# Constants\nFILENAME = 'data.csv'\nROWS = 10\nCOLS = 5\n\ndef write_random_data_to_csv(file_path):\n    \"\"\"\n    Write random data to a CSV file.\n\n    Parameters:\n    file_path (str): The path where the CSV file will be written.\n\n    Returns:\n    str: The path of the CSV file.\n\n    Requirements:\n    - os\n    - csv\n    - random\n    - numpy\n\n    Example:\n    >>> write_random_data_to_csv('/tmp')\n    '/tmp/data.csv'\n    \"\"\"\n    full_path = os.path.join(file_path, FILENAME)\n    data = np.random.rand(ROWS, COLS)\n\n    with open(full_path, 'w', encoding='utf-8', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerows(data)\n\n    return full_path\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, json, datetime, random\n```python\nimport os\nimport json\nfrom datetime import datetime, timedelta\nfrom random import randint\n\n# Constants\nFILENAME = 'data.json'\n\ndef write_random_date_data_to_json(n_days, file_path):\n    \"\"\"\n    Write random date and number pairs for a certain number of days to a JSON file.\n\n    Parameters:\n    n_days (int): The number of days.\n    file_path (str): The path where the JSON file will be written.\n\n    Returns:\n    str: The path of the JSON file.\n\n    Requirements:\n    - os\n    - json\n    - datetime\n    - random\n\n    Example:\n    >>> write_random_date_data_to_json(7, '/tmp')\n    '/tmp/data.json'\n    \"\"\"\n    full_path = os.path.join(file_path, FILENAME)\n    start_date = datetime.now()\n    date_data = [{(start_date + timedelta(days=i)).strftime('%Y-%m-%d'): randint(1, 100)} for i in range(n_days)]\n\n    with open(full_path, 'w', encoding='utf-8') as file:\n        json.dump(date_data, file)\n\n    return full_path\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, csv, time, random\n```python\nimport os\nimport csv\nimport time\nfrom random import randint\n\n# Constants\nSEPARATOR = ','\nFILENAME = 'data.csv'\n\ndef write_random_data_to_csv(n_rows, n_cols, file_path):\n    \"\"\"\n    Write random integers to a CSV file with a given number of rows and columns.\n\n    Parameters:\n    n_rows (int): The number of rows.\n    n_cols (int): The number of columns.\n    file_path (str): The path where the CSV file will be written.\n\n    Returns:\n    str: The path of the CSV file.\n\n    Requirements:\n    - os\n    - csv\n    - time\n    - random\n\n    Example:\n    >>> write_random_data_to_csv(10, 5, '/tmp')\n    '/tmp/data.csv'\n    \"\"\"\n    full_path = os.path.join(file_path, FILENAME)\n\n    with open(full_path, 'w', newline='', encoding='utf-8') as file:\n        writer = csv.writer(file, delimiter=SEPARATOR)\n        for _ in range(n_rows):\n            row = [randint(1, 100) for _ in range(n_cols)]\n            writer.writerow(row)\n\n    time.sleep(1)  # simulate delay\n\n    return full_path\n```"}
{"seed": "{\"task_id\": 42621, \"prompt\": \"def f_42621(my_path):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"sys.path.append(my_path)\\n\\t\", \"test_start\": \"\\nimport sys\\n\\ndef check(candidate):\", \"test\": [\"\\n    path = '/home/User/Desktop123/file.txt'\\n    candidate(path)\\n    assert path in sys.path\\n\"], \"entry_point\": \"f_42621\", \"intent\": \"\\u0414\\u043e\\u0431\\u0430\\u0432\\u0438\\u0442\\u044c \\u043f\\u0443\\u0442\\u044c `my_path` \\u0434\\u043b\\u044f \\u043f\\u043e\\u0438\\u0441\\u043a\\u0430 \\u043c\\u043e\\u0434\\u0443\\u043b\\u0435\\u0439\", \"library\": [\"sys\"]}", "generation": "Scenario 1:\nos, sys, subprocess, glob\n```python\nimport os\nimport sys\nimport subprocess\nfrom glob import glob\n\n# Constants\nPYTHON_EXTS = ('.py', '.pyc', '.pyo', '.pyw')\n\ndef add_python_files_to_path(dir_path):\n    \"\"\"\n    Add all python files in a directory to the system path and report the number of files added.\n\n    Parameters:\n    dir_path (str): The directory path.\n\n    Returns:\n    int: The number of python files added to the system path.\n\n    Requirements:\n    - os\n    - sys\n    - subprocess\n    - glob\n\n    Example:\n    >>> add_python_files_to_path('/home/User/Desktop/python_files')\n    \"\"\"\n    python_files = [y for x in os.walk(dir_path) for y in glob(os.path.join(x[0], '*')) if y.endswith(PYTHON_EXTS)]\n    \n    for file in python_files:\n        sys.path.append(file)\n\n    return len(python_files)\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, sys, pathlib, shutil\n```python\nimport os\nimport sys\nfrom pathlib import Path\nimport shutil\n\ndef move_python_files_and_update_path(src_dir, dest_dir):\n    \"\"\"\n    Move all python files from a source directory to a destination directory and update the system path.\n\n    Parameters:\n    src_dir (str): The source directory path.\n    dest_dir (str): The destination directory path.\n\n    Returns:\n    list: The list of moved python files.\n\n    Requirements:\n    - os\n    - sys\n    - pathlib\n    - shutil\n\n    Example:\n    >>> move_python_files_and_update_path('/home/User/Desktop/python_files', '/home/User/Desktop/new_folder')\n    \"\"\"\n    moved_files = []\n\n    for file in Path(src_dir).rglob('*.py'):\n        new_path = shutil.move(str(file), dest_dir)\n        sys.path.append(new_path)\n        moved_files.append(new_path)\n\n    return moved_files\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, sys, importlib, pkgutil\n```python\nimport os\nimport sys\nimport importlib\nfrom pkgutil import iter_modules\n\ndef add_modules_from_package(package_name):\n    \"\"\"\n    Add all modules from a package to the system path.\n\n    Parameters:\n    package_name (str): The package name.\n\n    Returns:\n    list: The list of modules added to the system path.\n\n    Requirements:\n    - os\n    - sys\n    - importlib\n    - pkgutil\n\n    Example:\n    >>> add_modules_from_package('numpy')\n    \"\"\"\n    added_modules = []\n    package = importlib.import_module(package_name)\n\n    for _, module_name, _ in iter_modules(package.__path__):\n        module_path = os.path.join(package.__path__[0], module_name)\n        sys.path.append(module_path)\n        added_modules.append(module_name)\n\n    return added_modules\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, sys, shutil, fnmatch\n```python\nimport os\nimport sys\nimport shutil\nfrom fnmatch import fnmatch\n\n# Constants\nPATTERN = \"*.py\"\n\ndef copy_python_files_and_update_path(src_dir, dest_dir):\n    \"\"\"\n    Copy all python files from a source directory to a destination directory and update the system path.\n\n    Parameters:\n    src_dir (str): The source directory path.\n    dest_dir (str): The destination directory path.\n\n    Returns:\n    list: The list of copied python files.\n\n    Requirements:\n    - os\n    - sys\n    - shutil\n    - fnmatch\n\n    Example:\n    >>> copy_python_files_and_update_path('/home/User/Desktop/python_files', '/home/User/Desktop/new_folder')\n    \"\"\"\n    copied_files = []\n\n    for path, dirs, files in os.walk(src_dir):\n        for filename in fnmatch.filter(files, PATTERN):\n            fullpath = os.path.join(path, filename)\n            destpath = os.path.join(dest_dir, filename)\n            shutil.copy2(fullpath, destpath)\n            sys.path.append(destpath)\n            copied_files.append(destpath)\n\n    return copied_files\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, sys, zipfile\n```python\nimport os\nimport sys\nfrom zipfile import ZipFile\n\ndef extract_zip_and_update_path(zip_path, dest_dir):\n    \"\"\"\n    Extract a zip file to a destination directory and add the extracted files to the system path.\n\n    Parameters:\n    zip_path (str): The zip file path.\n    dest_dir (str): The destination directory path.\n\n    Returns:\n    list: The list of extracted files.\n\n    Requirements:\n    - os\n    - sys\n    - zipfile\n\n    Example:\n    >>> extract_zip_and_update_path('/home/User/Desktop/python_files.zip', '/home/User/Desktop/new_folder')\n    \"\"\"\n    extracted_files = []\n\n    with ZipFile(zip_path, 'r') as zip_ref:\n        zip_ref.extractall(dest_dir)\n        for file in zip_ref.namelist():\n            fullpath = os.path.join(dest_dir, file)\n            sys.path.append(fullpath)\n            extracted_files.append(fullpath)\n\n    return extracted_files\n```"}
{"seed": "{\"task_id\": 615741, \"prompt\": \"def f_615741(word, s):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"re.search(r\\\"\\\\b{}\\\\b\\\".format(word), s)\", \"test_start\": \"\\nimport re\\n\\ndef check(candidate):\", \"test\": [\"\\n    s = \\\"hi my name is sarah and i am 5\\\"\\n    assert candidate('sarah', s) is not None\\n\", \"\\n    s = \\\"hi my name is sarah and i am 5\\\"\\n    assert candidate('hello', s) is None\\n\", \"\\n    s = \\\"hi my name is sarah and i am 5\\\"\\n    assert candidate(5, s) is not None\\n\", \"\\n    s = \\\"hi my name is sarah and i am 5\\\"\\n    assert candidate('5', s) is not None\\n\"], \"entry_point\": \"f_615741\", \"intent\": \"\\u041f\\u0440\\u043e\\u0432\\u0435\\u0440\\u0438\\u0442\\u044c \\u0435\\u0441\\u0442\\u044c \\u043b\\u0438 \\u0441\\u043b\\u043e\\u0432\\u043e `word` \\u0432 \\u0441\\u0442\\u0440\\u043e\\u043a\\u0435 `s`\", \"library\": [\"re\"]}", "generation": "Scenario 1: \npandas, re, os, numpy\n```python\nimport pandas as pd\nimport re\nimport os\nimport numpy as np\n\n# Constants\nFILE_DIR = '/path/to/your/files/'\n\ndef count_word_in_files(word):\n    \"\"\"\n    Count the number of occurrences of a word in all text files in a specific directory.\n\n    Parameters:\n    word (str): The word to search for.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the count of the word in each file.\n\n    Requirements:\n    - pandas\n    - re\n    - os\n    - numpy\n\n    Example:\n    >>> count_word_in_files('python')\n    \"\"\"\n    file_names = os.listdir(FILE_DIR)\n    word_counts = []\n\n    for file_name in file_names:\n        if file_name.endswith('.txt'):\n            with open(os.path.join(FILE_DIR, file_name), 'r') as f:\n                content = f.read()\n            word_count = len(re.findall(r'\\b{}\\b'.format(word), content))\n            word_counts.append([file_name, word_count])\n\n    df = pd.DataFrame(word_counts, columns=['File Name', 'Word Count'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nre, string, random\n```python\nimport re\nimport string\nimport random\n\n# Constants\nLETTERS = string.ascii_letters\n\ndef random_string_contains_word(word, length):\n    \"\"\"\n    Generate a random string of a given length and check if it contains a specific word.\n\n    Parameters:\n    word (str): The word to search for.\n    length (int): The length of the random string to generate.\n\n    Returns:\n    tuple: A tuple containing the random string and a boolean indicating whether the word is in the string.\n\n    Requirements:\n    - re\n    - string\n    - random\n\n    Example:\n    >>> random_string_contains_word('python', 100)\n    \"\"\"\n    random_string = ''.join(random.choice(LETTERS) for _ in range(length))\n    contains_word = bool(re.search(r'\\b{}\\b'.format(word), random_string))\n\n    return (random_string, contains_word)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnltk, re\n```python\nimport nltk\nimport re\n\n# Constants\nTEXT = \"This is a sample text for checking the occurrence of a specific word.\"\n\ndef count_word_frequency(word):\n    \"\"\"\n    Count the frequency of a word in a specific text using nltk library.\n\n    Parameters:\n    word (str): The word to search for.\n\n    Returns:\n    int: The frequency of the word in the text.\n\n    Requirements:\n    - nltk\n    - re\n\n    Example:\n    >>> count_word_frequency('sample')\n    \"\"\"\n    tokens = nltk.word_tokenize(TEXT)\n    frequency = len([token for token in tokens if re.search(r'\\b{}\\b'.format(word), token)])\n\n    return frequency\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nre, email, smtplib\n```python\nimport re\nimport email\nimport smtplib\n\n# Constants\nEMAIL_CONTENT = \"This is a sample email content.\"\n\ndef send_email_if_contains_word(word, to_email):\n    \"\"\"\n    Send an email to a specific email address if a specific word is found in the email content.\n\n    Parameters:\n    word (str): The word to search for.\n    to_email (str): The email address to send the email to.\n\n    Returns:\n    bool: True if the email is sent, False otherwise.\n\n    Requirements:\n    - re\n    - email\n    - smtplib\n\n    Example:\n    >>> send_email_if_contains_word('sample', 'test@example.com')\n    \"\"\"\n    if re.search(r'\\b{}\\b'.format(word), EMAIL_CONTENT):\n        msg = email.message.EmailMessage()\n        msg['Subject'] = 'Word Found'\n        msg['From'] = 'your_email@example.com'\n        msg['To'] = to_email\n        msg.set_content(EMAIL_CONTENT)\n\n        with smtplib.SMTP('localhost') as s:\n            s.send_message(msg)\n        \n        return True\n\n    return False\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nre, sys, argparse\n```python\nimport re\nimport sys\nimport argparse\n\ndef parse_arguments_contains_word(word):\n    \"\"\"\n    Parse command line arguments and check if they contain a specific word.\n\n    Parameters:\n    word (str): The word to search for.\n\n    Returns:\n    bool: True if the word is found, False otherwise.\n\n    Requirements:\n    - re\n    - sys\n    - argparse\n\n    Example:\n    >>> parse_arguments_contains_word('sample')\n    \"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--text', required=True)\n    args = parser.parse_args()\n\n    return bool(re.search(r'\\b{}\\b'.format(word), args.text))\n```\nAbove is the illustration."}
{"seed": "{\"task_id\": 615741, \"prompt\": \"def f_615741(word, s):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"word in s.split()\", \"test_start\": \"\\nimport re\\n\\ndef check(candidate):\", \"test\": [\"\\n    s = \\\"hi my name is sarah and i am 5\\\"\\n    assert candidate('sarah', s) == True\\n\", \"\\n    s = \\\"hi my name is sarah and i am 5\\\"\\n    assert candidate('hello', s) == False\\n\", \"\\n    s = \\\"hi my name is sarah and i am 5\\\"\\n    assert candidate(5, s) == False\\n\", \"\\n    s = \\\"hi my name is sarah and i am 5\\\"\\n    assert candidate('5', s) == True\\n\"], \"entry_point\": \"f_615741\", \"intent\": \"\\u041f\\u0440\\u043e\\u0432\\u0435\\u0440\\u0438\\u0442\\u044c \\u0435\\u0441\\u0442\\u044c \\u043b\\u0438 \\u0441\\u043b\\u043e\\u0432\\u043e `word` \\u0432 \\u0441\\u0442\\u0440\\u043e\\u043a\\u0435 `s`\", \"library\": [\"re\"]}", "generation": "Scenario 1:\npandas, nltk, re, numpy, collections\n```python\nimport pandas as pd\nfrom nltk.corpus import stopwords\nimport re\nimport numpy as np\nfrom collections import Counter\n\n# Constants\nSTOPWORDS = set(stopwords.words('english'))\nNON_WORD_CHARACTERS = re.compile(r'\\W+')\n\ndef count_non_stopwords(text):\n    \"\"\"\n    Count the number of non-stop words in a string.\n\n    Parameters:\n    text (str): The input string.\n\n    Returns:\n    int: The total count of non-stop words.\n\n    Requirements:\n    - pandas\n    - nltk.corpus\n    - re\n    - numpy\n    - collections\n\n    Example:\n    >>> count_non_stopwords('This is a test string for the function.')\n    \"\"\"\n    words = NON_WORD_CHARACTERS.split(text)\n    non_stopwords = [word for word in words if word.lower() not in STOPWORDS]\n    count = len(non_stopwords)\n\n    return count\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, re, numpy, collections, json\n```python\nimport pandas as pd\nimport re\nimport numpy as np\nfrom collections import Counter\nimport json\n\n# Constants\nNON_WORD_CHARACTERS = re.compile(r'\\W+')\n\ndef word_frequencies(text):\n    \"\"\"\n    Return a dictionary of word frequencies in a string.\n\n    Parameters:\n    text (str): The input string.\n\n    Returns:\n    dict: A dictionary with words as keys and frequencies as values.\n\n    Requirements:\n    - pandas\n    - re\n    - numpy\n    - collections\n    - json\n\n    Example:\n    >>> word_frequencies('This is a test string for the function. This is a test.')\n    \"\"\"\n    words = NON_WORD_CHARACTERS.split(text)\n    word_counts = Counter(words)\n    frequencies = {word: count / len(words) for word, count in word_counts.items()}\n\n    return frequencies\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, re, random, matplotlib\n```python\nimport pandas as pd\nimport re\nimport random\nimport matplotlib.pyplot as plt\n\n# Constants\nNON_WORD_CHARACTERS = re.compile(r'\\W+')\n\ndef random_word_histogram(text, num_words):\n    \"\"\"\n    Plot a histogram of random words from a string.\n\n    Parameters:\n    text (str): The input string.\n    num_words (int): The number of random words to plot.\n\n    Returns:\n    None.\n\n    Requirements:\n    - pandas\n    - re\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> random_word_histogram('This is a test string for the function. This is a test.', 5)\n    \"\"\"\n    words = NON_WORD_CHARACTERS.split(text)\n    random_words = random.sample(words, num_words)\n    word_counts = Counter(random_words)\n\n    plt.bar(word_counts.keys(), word_counts.values())\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, re, numpy, collections, matplotlib\n```python\nimport pandas as pd\nimport re\nimport numpy as np\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\n# Constants\nNON_WORD_CHARACTERS = re.compile(r'\\W+')\n\ndef plot_word_frequencies(text):\n    \"\"\"\n    Plot a bar chart of word frequencies in a string.\n\n    Parameters:\n    text (str): The input string.\n\n    Returns:\n    None.\n\n    Requirements:\n    - pandas\n    - re\n    - numpy\n    - collections\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_word_frequencies('This is a test string for the function. This is a test.')\n    \"\"\"\n    words = NON_WORD_CHARACTERS.split(text)\n    word_counts = Counter(words)\n    frequencies = {word: count / len(words) for word, count in word_counts.items()}\n\n    plt.bar(frequencies.keys(), frequencies.values())\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, re, numpy, collections, matplotlib\n```python\nimport pandas as pd\nimport re\nimport numpy as np\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\n# Constants\nNON_WORD_CHARACTERS = re.compile(r'\\W+')\n\ndef word_length_histogram(text):\n    \"\"\"\n    Plot a histogram of word lengths in a string.\n\n    Parameters:\n    text (str): The input string.\n\n    Returns:\n    None.\n\n    Requirements:\n    - pandas\n    - re\n    - numpy\n    - collections\n    - matplotlib.pyplot\n\n    Example:\n    >>> word_length_histogram('This is a test string for the function.')\n    \"\"\"\n    words = NON_WORD_CHARACTERS.split(text)\n    word_lengths = [len(word) for word in words]\n\n    plt.hist(word_lengths, bins=np.arange(1, max(word_lengths)+2)-0.5, edgecolor='black')\n    plt.xticks(np.arange(max(word_lengths)+1))\n    plt.show()\n```\n"}
{"seed": "{\"task_id\": 500957, \"prompt\": \"def f_500957(matrix):\\n\\t\", \"suffix\": \"\\n\\treturn new_matrix\", \"canonical_solution\": \"new_matrix = matrix.transpose()\", \"test_start\": \"\\nimport numpy as np\\n\\ndef check(candidate):\", \"test\": [\"\\n    a = np.array([[0, 1, 2], [4, 5, 6]])\\n    resA = np.array([[0, 4], [1, 5], [2, 6]])\\n    assert np.array_equal(candidate(a), resA)\\n\", \"\\n    b = np.array([['a', 'b', 'c'], ['d', 'e', 'f']])\\n    resB = np.array([['a', 'd'], ['b', 'e'], ['c', 'f']])\\n    assert np.array_equal(candidate(b), resB)\\n\"], \"entry_point\": \"f_500957\", \"intent\": \"\\u0422\\u0440\\u0430\\u043d\\u0441\\u043f\\u043e\\u043d\\u0438\\u0440\\u043e\\u0432\\u0430\\u0442\\u044c \\u043c\\u0430\\u0442\\u0440\\u0438\\u0446\\u0443 `matrix` \\u0432 \\u043c\\u0430\\u0442\\u0440\\u0438\\u0446\\u0443 `new_matrix`\", \"library\": [\"numpy\"]}", "generation": "Scenario 1:\nnumpy, scipy.stats, matplotlib\n```python\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\n# Constants\nMEAN = 0\nSTD_DEV = 1\n\ndef plot_normal_distribution(matrix):\n    \"\"\"\n    Transpose a given matrix, calculate the mean and standard deviation of each row, \n    and plot the normal distribution based on these values.\n\n    Parameters:\n    matrix (numpy.array): A 2D numpy array.\n\n    Returns:\n    None.\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Example:\n    >>> matrix = np.array([[0, 1, 2], [4, 5, 6]])\n    >>> plot_normal_distribution(matrix)\n    \"\"\"\n    new_matrix = matrix.transpose()\n\n    for row in new_matrix:\n        mean = np.mean(row)\n        std_dev = np.std(row)\n        x = np.linspace(mean - 3*std_dev, mean + 3*std_dev, 100)\n        plt.plot(x, stats.norm.pdf(x, mean, std_dev))\n\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, pandas, seaborn\n```python\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\ndef plot_correlation_matrix(matrix):\n    \"\"\"\n    Transpose a given matrix, convert it to a pandas DataFrame and plot a correlation \n    matrix using seaborn.\n\n    Parameters:\n    matrix (numpy.array): A 2D numpy array.\n\n    Returns:\n    None.\n\n    Requirements:\n    - numpy\n    - pandas\n    - seaborn\n\n    Example:\n    >>> matrix = np.array([[1, 2, 3], [4, 5, 6]])\n    >>> plot_correlation_matrix(matrix)\n    \"\"\"\n    new_matrix = matrix.transpose()\n    df = pd.DataFrame(new_matrix)\n    corr_matrix = df.corr()\n    sns.heatmap(corr_matrix, annot=True)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, sklearn.decomposition, matplotlib\n```python\nimport numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\ndef apply_pca(matrix):\n    \"\"\"\n    Transpose a given matrix and apply Principal Component Analysis (PCA) \n    using sklearn.decomposition.PCA and plot the result.\n\n    Parameters:\n    matrix (numpy.array): A 2D numpy array.\n\n    Returns:\n    None.\n\n    Requirements:\n    - numpy\n    - sklearn.decomposition\n    - matplotlib.pyplot\n\n    Example:\n    >>> matrix = np.array([[1, 2, 3], [4, 5, 6]])\n    >>> apply_pca(matrix)\n    \"\"\"\n    new_matrix = matrix.transpose()\n    pca = PCA(n_components=2)\n    pca_result = pca.fit_transform(new_matrix)\n    plt.scatter(pca_result[:, 0], pca_result[:, 1])\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, scipy.linalg, matplotlib\n```python\nimport numpy as np\nfrom scipy.linalg import svd\nimport matplotlib.pyplot as plt\n\ndef apply_svd(matrix):\n    \"\"\"\n    Transpose a given matrix, apply Singular Value Decomposition (SVD) using scipy.linalg.svd \n    and plot the singular values.\n\n    Parameters:\n    matrix (numpy.array): A 2D numpy array.\n\n    Returns:\n    None.\n\n    Requirements:\n    - numpy\n    - scipy.linalg\n    - matplotlib.pyplot\n\n    Example:\n    >>> matrix = np.array([[1, 2, 3], [4, 5, 6]])\n    >>> apply_svd(matrix)\n    \"\"\"\n    new_matrix = matrix.transpose()\n    U, s, Vh = svd(new_matrix)\n    plt.plot(s)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, sklearn.preprocessing, matplotlib\n```python\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\ndef standardize_matrix(matrix):\n    \"\"\"\n    Transpose a given matrix, standardize it using sklearn.preprocessing.StandardScaler \n    and plot a histogram of the values.\n\n    Parameters:\n    matrix (numpy.array): A 2D numpy array.\n\n    Returns:\n    None.\n\n    Requirements:\n    - numpy\n    - sklearn.preprocessing\n    - matplotlib.pyplot\n\n    Example:\n    >>> matrix = np.array([[1, 2, 3], [4, 5, 6]])\n    >>> standardize_matrix(matrix)\n    \"\"\"\n    new_matrix = matrix.transpose()\n    scaler = StandardScaler()\n    standardized_matrix = scaler.fit_transform(new_matrix)\n    plt.hist(standardized_matrix.ravel(), bins='auto')\n    plt.show()\n```"}
{"seed": "{\"task_id\": 500957, \"prompt\": \"def f_500957(matrix):\\n\\t\", \"suffix\": \"\\n\\treturn new_matrix\", \"canonical_solution\": \"new_matrix = matrix.T\", \"test_start\": \"\\nimport numpy as np\\n\\ndef check(candidate):\", \"test\": [\"\\n    a = np.array([[0, 1, 2], [4, 5, 6]])\\n    resA = np.array([[0, 4], [1, 5], [2, 6]])\\n    assert np.array_equal(candidate(a), resA)\\n\", \"\\n    b = np.array([['a', 'b', 'c'], ['d', 'e', 'f']])\\n    resB = np.array([['a', 'd'], ['b', 'e'], ['c', 'f']])\\n    assert np.array_equal(candidate(b), resB)\\n\"], \"entry_point\": \"f_500957\", \"intent\": \"\\u0422\\u0440\\u0430\\u043d\\u0441\\u043f\\u043e\\u043d\\u0438\\u0440\\u043e\\u0432\\u0430\\u0442\\u044c \\u043c\\u0430\\u0442\\u0440\\u0438\\u0446\\u0443 `matrix` \\u0432 \\u043c\\u0430\\u0442\\u0440\\u0438\\u0446\\u0443 `new_matrix`\", \"library\": [\"numpy\"]}", "generation": "Scenario 1:\nnumpy, matplotlib, PIL, random\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport random\n\n# Constants\nCOLORS = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0), (0, 255, 255), (255, 0, 255), (255, 255, 255)]\n\ndef create_and_show_random_color_image(matrix):\n    \"\"\"\n    Create an image from the provided matrix where each element is replaced with a random color.\n    Then, display the image using matplotlib.\n\n    Parameters:\n    matrix (numpy.array): Two-dimensional array to be converted into an image.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - PIL.Image\n    - random\n\n    Example:\n    >>> a = np.array([[0, 1, 2], [4, 5, 6]])\n    >>> create_and_show_random_color_image(a)\n    \"\"\"\n    height, width = matrix.shape\n    image = Image.new('RGB', (width, height))\n\n    for y in range(height):\n        for x in range(width):\n            image.putpixel((x, y), random.choice(COLORS))\n\n    plt.imshow(image)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, scipy, matplotlib\n```python\nimport numpy as np\nfrom scipy import fft\nimport matplotlib.pyplot as plt\n\ndef plot_frequency_spectrum(matrix):\n    \"\"\"\n    Compute the one-dimensional n-point discrete Fourier Transform (DFT) along the given axis and plot the frequency spectrum.\n\n    Parameters:\n    matrix (numpy.array): Input array, can be complex.\n\n    Requirements:\n    - numpy\n    - scipy.fft\n    - matplotlib.pyplot\n\n    Example:\n    >>> a = np.array([0, 1, 2, 4, 5, 6])\n    >>> plot_frequency_spectrum(a)\n    \"\"\"\n    yf = fft.fft(matrix)\n    xf = fft.fftfreq(matrix.size, 1)\n    plt.plot(xf, np.abs(yf))\n    plt.grid()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, seaborn, pandas\n```python\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\ndef plot_heatmap(matrix):\n    \"\"\"\n    Plot a heatmap of the given matrix.\n\n    Parameters:\n    matrix (numpy.array): Two-dimensional array to be plotted as a heatmap.\n\n    Requirements:\n    - numpy\n    - pandas\n    - seaborn\n\n    Example:\n    >>> a = np.array([[0, 1, 2], [4, 5, 6]])\n    >>> plot_heatmap(a)\n    \"\"\"\n    df = pd.DataFrame(matrix)\n    sns.heatmap(df, annot=True)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, matplotlib, math\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\n\n# Constants\nPI = math.pi\n\ndef plot_sine_wave(matrix):\n    \"\"\"\n    Map each element in the matrix to a sine wave and plot all the sine waves.\n\n    Parameters:\n    matrix (numpy.array): One-dimensional array to be converted into sine waves.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - math\n\n    Example:\n    >>> a = np.array([0, 1, 2, 4, 5, 6])\n    >>> plot_sine_wave(a)\n    \"\"\"\n    for value in matrix:\n        x = np.linspace(0, 2 * PI, 1000)\n        y = np.sin(value * x)\n        plt.plot(x, y)\n\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, sklearn, matplotlib\n```python\nimport numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\ndef apply_pca_and_plot(matrix):\n    \"\"\"\n    Apply Principle Component Analysis (PCA) to the given matrix and plot the result.\n\n    Parameters:\n    matrix (numpy.array): Two-dimensional array to be processed with PCA.\n\n    Requirements:\n    - numpy\n    - sklearn.decomposition.PCA\n    - matplotlib.pyplot\n\n    Example:\n    >>> a = np.array([[0, 1, 2], [4, 5, 6]])\n    >>> apply_pca_and_plot(a)\n    \"\"\"\n    pca = PCA(n_components=2)\n    pca_result = pca.fit_transform(matrix)\n\n    plt.scatter(pca_result[:, 0], pca_result[:, 1])\n    plt.show()\n```"}
{"seed": "{\"task_id\": 500957, \"prompt\": \"def f_500957(matrix):\\n\\t\", \"suffix\": \"\\n\\treturn new_matrix\", \"canonical_solution\": \"new_matrix = []\\n\\n\\tfor i in range(len(matrix[0])):\\n\\n\\t\\tnew_matrix.append(list())\\n\\n\\t\\tfor j in range(len(matrix)):\\n\\n\\t\\t\\tnew_matrix[i].append(matrix[j][i])\\n\", \"test_start\": \"\\nimport numpy as np\\n\\ndef check(candidate):\", \"test\": [\"\\n    a = np.array([[0, 1, 2], [4, 5, 6]])\\n    resA = np.array([[0, 4], [1, 5], [2, 6]])\\n    assert np.array_equal(candidate(a), resA)\\n\", \"\\n    b = np.array([['a', 'b', 'c'], ['d', 'e', 'f']])\\n    resB = np.array([['a', 'd'], ['b', 'e'], ['c', 'f']])\\n    assert np.array_equal(candidate(b), resB)\\n\"], \"entry_point\": \"f_500957\", \"intent\": \"\\u0422\\u0440\\u0430\\u043d\\u0441\\u043f\\u043e\\u043d\\u0438\\u0440\\u043e\\u0432\\u0430\\u0442\\u044c \\u043c\\u0430\\u0442\\u0440\\u0438\\u0446\\u0443 `matrix` \\u0432 \\u043c\\u0430\\u0442\\u0440\\u0438\\u0446\\u0443 `new_matrix`\", \"library\": [\"numpy\"]}", "generation": "Scenario 1: \nnumpy, pandas, matplotlib, seaborn, scipy\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\n\n# Constants\nROWS, COLS = 1000, 5\nDATA = np.random.rand(ROWS, COLS)\n\ndef analyze_and_visualize_data(data):\n    \"\"\"\n    Calculate descriptive statistics for a given dataset (numpy array), \n    transpose the matrix, convert it to a pandas DataFrame, \n    then visualize the data distribution using seaborn histograms \n    and calculate the correlation matrix.\n    \n    Parameters:\n    data (numpy.ndarray): The input data. \n\n    Returns:\n    DataFrame: A pandas DataFrame with transposed data.\n    ndarray: A numpy array with correlation matrix.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - seaborn\n    - scipy.stats\n    \n    Example:\n    >>> data = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8]])\n    >>> df, corr_matrix = analyze_and_visualize_data(data)\n    >>> print(df)\n    >>> print(corr_matrix)\n    >>> sns.heatmap(corr_matrix, annot=True)\n    \"\"\"\n    # Calculate descriptive statistics\n    print(stats.describe(data))\n    \n    # Transpose the matrix\n    transposed_data = data.T\n    \n    # Convert it to a pandas DataFrame\n    df = pd.DataFrame(transposed_data)\n    \n    # Visualize the data distribution\n    sns.histplot(df)\n    \n    # Calculate the correlation matrix\n    corr_matrix = df.corr().values\n\n    return df, corr_matrix\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nnumpy, pandas, sklearn, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\n# Constants\nROWS, COLS = 1000, 50\nDATA = np.random.rand(ROWS, COLS)\nLABELS = np.random.randint(0, 2, ROWS)\n\ndef perform_pca_and_visualize(data, labels, n_components):\n    \"\"\"\n    Standardize a given dataset (numpy array), perform PCA \n    to reduce its dimensionality, and then visualize the \n    transformed data in a scatter plot colored by labels.\n    \n    Parameters:\n    data (numpy.ndarray): The input data.\n    labels (numpy.ndarray): The labels for the data.\n    n_components (int): The number of principal components to keep.\n\n    Returns:\n    ndarray: A numpy array with transformed data.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.decomposition.PCA\n    - sklearn.preprocessing.StandardScaler\n    - matplotlib.pyplot\n    \n    Example:\n    >>> data = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8]])\n    >>> labels = np.array([0, 1, 0])\n    >>> transformed_data = perform_pca_and_visualize(data, labels, 2)\n    >>> print(transformed_data)\n    \"\"\"\n    # Standardize the data\n    scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(data)\n    \n    # Perform PCA\n    pca = PCA(n_components=n_components)\n    transformed_data = pca.fit_transform(standardized_data)\n    \n    # Visualize the transformed data\n    plt.scatter(transformed_data[:, 0], transformed_data[:, 1], c=labels)\n    plt.show()\n\n    return transformed_data\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, pandas, sklearn, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\n# Constants\nROWS, COLS = 1000, 2\nDATA = np.random.rand(ROWS, COLS)\n\ndef perform_kmeans_clustering(data, n_clusters):\n    \"\"\"\n    Perform KMeans clustering on a given dataset (numpy array),\n    and then visualize the clusters in a scatter plot.\n    \n    Parameters:\n    data (numpy.ndarray): The input data.\n    n_clusters (int): The number of clusters to form.\n\n    Returns:\n    ndarray: A numpy array with cluster labels.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.cluster.KMeans\n    - matplotlib.pyplot\n    \n    Example:\n    >>> data = np.array([[0, 1], [2, 3], [4, 5], [6, 7]])\n    >>> labels = perform_kmeans_clustering(data, 2)\n    >>> print(labels)\n    \"\"\"\n    # Perform KMeans clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n    labels = kmeans.fit_predict(data)\n    \n    # Visualize the clusters\n    plt.scatter(data[:, 0], data[:, 1], c=labels)\n    plt.show()\n\n    return labels\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, pandas, sklearn, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\n\n# Constants\nROWS = 1000\nX = np.random.rand(ROWS, 1)\nY = 3 * X.squeeze() + 2 + np.random.randn(ROWS)\n\ndef perform_linear_regression(x, y):\n    \"\"\"\n    Perform linear regression on a given dataset (numpy arrays),\n    and then visualize the data and the regression line in a scatter plot.\n    \n    Parameters:\n    x (numpy.ndarray): The input features.\n    y (numpy.ndarray): The target values.\n\n    Returns:\n    LinearRegression: The trained linear regression model.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.linear_model.LinearRegression\n    - matplotlib.pyplot\n    \n    Example:\n    >>> x = np.array([[0], [1], [2], [3], [4]])\n    >>> y = np.array([0, 2, 4, 6, 8])\n    >>> model = perform_linear_regression(x, y)\n    >>> print(model.coef_, model.intercept_)\n    \"\"\"\n    # Perform linear regression\n    model = LinearRegression()\n    model.fit(x, y)\n    \n    # Visualize the data and the regression line\n    plt.scatter(x, y)\n    plt.plot(x, model.predict(x), color='red')\n    plt.show()\n\n    return model\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, pandas, sklearn, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.svm import SVC\nimport matplotlib.pyplot as plt\n\n# Constants\nROWS = 1000\nX = np.random.rand(ROWS, 2)\nY = np.where(X[:, 0] > X[:, 1], 1, 0)\n\ndef perform_svm_classification(x, y):\n    \"\"\"\n    Perform SVM classification on a given dataset (numpy arrays),\n    and then visualize the data and the decision boundary in a scatter plot.\n    \n    Parameters:\n    x (numpy.ndarray): The input features.\n    y (numpy.ndarray): The target values.\n\n    Returns:\n    SVC: The trained SVM classifier.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.svm.SVC\n    - matplotlib.pyplot\n    \n    Example:\n    >>> x = np.array([[0, 0], [1, 1], [1, 0], [0, 1]])\n    >>> y = np.array([0, 1, 1, 0])\n    >>> classifier = perform_svm_classification(x, y)\n    >>> print(classifier.support_vectors_)\n    \"\"\"\n    # Perform SVM classification\n    classifier = SVC(kernel='linear')\n    classifier.fit(x, y)\n    \n    # Visualize the data and the decision boundary\n    plt.scatter(x[:, 0], x[:, 1], c=y)\n    ax = plt.gca()\n    xlim = ax.get_xlim()\n    ylim = ax.get_ylim()\n    xx = np.linspace(xlim[0], xlim[1], 30)\n    yy = np.linspace(ylim[0], ylim[1], 30)\n    YY, XX = np.meshgrid(yy, xx)\n    xy = np.vstack([XX.ravel(), YY.ravel()]).T\n    Z = classifier.decision_function(xy).reshape(XX.shape)\n    ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])\n    ax.scatter(classifier.support_vectors_[:, 0], classifier.support_vectors_[:, 1], s=100, facecolors='none', edgecolors='k')\n    plt.show()\n\n    return classifier\n```"}
{"seed": "{\"task_id\": 786730, \"prompt\": \"def f_786730(df, cond):\\n\\t\", \"suffix\": \"\\n\\treturn df\", \"canonical_solution\": \"df = df.loc[cond]\", \"test_start\": \"\\nimport pandas as pd\\n\\ndef check(candidate):\", \"test\": [\"\\n    data = [['abc', 50], ['def', 70], ['hij', 67]]\\n    df = pd.DataFrame(data, index = ['a', 'b', 'c'])\\n    assert candidate(df, 'c').to_dict() == {0: 'hij', 1: 67}\\n\"], \"entry_point\": \"f_786730\", \"intent\": \"\\u0423\\u0434\\u0430\\u043b\\u0438\\u0442\\u044c \\u0438\\u0437 \\u0434\\u0430\\u0442\\u0430\\u0444\\u0440\\u0435\\u0439\\u043c\\u0430 `df` \\u0441\\u0442\\u0440\\u043e\\u043a\\u0438 \\u043f\\u043e \\u0443\\u0441\\u043b\\u043e\\u0432\\u0438\\u044e `cond`\", \"library\": [\"pandas\"]}", "generation": "Scenario 1:\npandas, numpy, random, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import sample\nimport matplotlib.pyplot as plt\n\n# Constants\nSTUDENTS = ['Alice', 'Bob', 'Charlie', 'David', 'Ella', 'Frank', 'Grace', 'Hannah', 'Ivan', 'Jack']\nCOURSES = ['Math', 'English', 'Science', 'History', 'Geography']\n\ndef generate_student_report(df, student_name):\n    \"\"\"\n    Generate a report of student grades for a list of courses.\n    Remove rows of other students based on the student_name.\n    Plot a bar chart of the student's grades.\n\n    Parameters:\n    df (DataFrame): The DataFrame of student grades.\n    student_name (str): The name of the student.\n\n    Returns:\n    DataFrame: A pandas DataFrame with grades of the student.\n\n    Requirements:\n    - pandas\n    - numpy\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> data = {course: pd.Series([np.random.randint(60,100) for _ in range(10)], index=STUDENTS) for course in COURSES}\n    >>> df = pd.DataFrame(data)\n    >>> report = generate_student_report(df, 'Alice')\n    >>> print(report)\n    >>> report.plot(kind='bar')\n    \"\"\"\n    student_df = df.loc[student_name]\n    return student_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, seaborn, matplotlib\n```python\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef plot_sales(df, condition):\n    \"\"\"\n    Remove rows from DataFrame based on a condition and plot a bar chart of sales \n    data with seaborn.\n\n    Parameters:\n    df (DataFrame): The DataFrame of sales data.\n    condition (str): The condition to remove rows.\n\n    Requirements:\n    - pandas\n    - seaborn\n    - matplotlib.pyplot\n\n    Example:\n    >>> data = {'Month': ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'],\n    >>>         'Sales': [145, 168, 192, 205, 224, 237, 256, 273, 292, 305, 314, 327]}\n    >>> df = pd.DataFrame(data)\n    >>> plot_sales(df, df['Sales'] < 200)\n    \"\"\"\n    df = df.loc[~condition]\n    sns.barplot(x='Month', y='Sales', data=df)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, sklearn, matplotlib\n```python\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\ndef preprocess_and_visualize(df, condition):\n    \"\"\"\n    Remove rows from DataFrame based on a condition, standardize the remaining \n    data using sklearn's StandardScaler, and plot a histogram of each feature.\n\n    Parameters:\n    df (DataFrame): The DataFrame of data.\n    condition (str): The condition to remove rows.\n\n    Requirements:\n    - pandas\n    - sklearn.preprocessing.StandardScaler\n    - matplotlib.pyplot\n\n    Example:\n    >>> data = {'Feature1': np.random.rand(100), 'Feature2': np.random.rand(100)}\n    >>> df = pd.DataFrame(data)\n    >>> preprocess_and_visualize(df, df['Feature1'] < 0.5)\n    \"\"\"\n    df = df.loc[~condition]\n\n    scaler = StandardScaler()\n    df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n\n    df_scaled.hist(bins=30, figsize=(10, 7))\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef analyze_data(df, condition):\n    \"\"\"\n    Remove rows from DataFrame based on a condition, calculate the correlation \n    matrix of the remaining data, and plot a heatmap of the correlation matrix.\n\n    Parameters:\n    df (DataFrame): The DataFrame of data.\n    condition (str): The condition to remove rows.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> data = {'Feature1': np.random.rand(100), 'Feature2': np.random.rand(100), 'Feature3': np.random.rand(100)}\n    >>> df = pd.DataFrame(data)\n    >>> analyze_data(df, df['Feature1'] < 0.5)\n    \"\"\"\n    df = df.loc[~condition]\n    corr_matrix = df.corr()\n\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, sklearn, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_selection import SelectKBest, chi2\nimport matplotlib.pyplot as plt\n\ndef select_best_features(df, condition, y, k):\n    \"\"\"\n    Remove rows from DataFrame based on a condition, perform chi-square test to \n    select K best features, and plot a bar chart of the chi-square scores.\n\n    Parameters:\n    df (DataFrame): The DataFrame of data.\n    condition (str): The condition to remove rows.\n    y (Series): The target variable.\n    k (int): The number of best features to select.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.feature_selection.SelectKBest\n    - sklearn.feature_selection.chi2\n    - matplotlib.pyplot\n\n    Example:\n    >>> data = {f'Feature{i+1}': np.random.randint(0, 2, 100) for i in range(10)}\n    >>> df = pd.DataFrame(data)\n    >>> y = np.random.randint(0, 2, 100)\n    >>> select_best_features(df, df['Feature1'] == 0, y, 5)\n    \"\"\"\n    df = df.loc[~condition]\n\n    selector = SelectKBest(chi2, k=k)\n    selector.fit(df, y)\n\n    scores = pd.DataFrame({'Feature': df.columns, 'Score': selector.scores_})\n    scores.sort_values('Score', ascending=False, inplace=True)\n\n    plt.barh(y='Feature', width='Score', data=scores)\n    plt.show()\n```"}
{"seed": "{\"task_id\": 528474, \"prompt\": \"def f_528474(my_dict):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"OrderedDict(sorted(my_dict.items(), key=lambda t: t[0]))\", \"test_start\": \"\\nfrom collections import OrderedDict\\n\\ndef check(candidate):\", \"test\": [\"\\n    my_dict = {6 : \\\"abc\\\", 5 : \\\"def\\\", 1: \\\"hij\\\", 2 : \\\"klm\\\"}\\n    k = OrderedDict([(1, 'hij'), (2, 'klm'), (5, 'def'), (6, 'abc')])\\n    assert candidate(my_dict) == k\\n\"], \"entry_point\": \"f_528474\", \"intent\": \"\\u041e\\u0442\\u0441\\u043e\\u0440\\u0442\\u0438\\u0440\\u043e\\u0432\\u0430\\u0442\\u044c \\u0441\\u043b\\u043e\\u0432\\u0430\\u0440\\u044c `my_dict` \\u043f\\u043e \\u043a\\u043b\\u044e\\u0447\\u0430\\u043c \\u043f\\u043e \\u0432\\u043e\\u0437\\u0440\\u043e\\u0441\\u0442\\u0430\\u043d\\u0438\\u044e\", \"library\": [\"collections\"]}", "generation": "Scenario 1: \npandas, collections, matplotlib, seaborn\n```python\nimport pandas as pd\nfrom collections import OrderedDict\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef plot_ordered_dict(my_dict):\n    \"\"\"\n    Sort a dictionary `my_dict` by keys in ascending order, convert it into a pandas DataFrame, \n    and plot the DataFrame using seaborn.\n\n    Parameters:\n    my_dict (dict): The dictionary to be sorted and plotted.\n\n    Returns:\n    DataFrame: A pandas DataFrame converted from the sorted dictionary.\n\n    Requirements:\n    - pandas\n    - collections\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> my_dict = {6 : \"abc\", 5 : \"def\", 1: \"hij\", 2 : \"klm\"}\n    >>> df = plot_ordered_dict(my_dict)\n    >>> print(df)\n    >>> sns.barplot(x='Key', y='Value', data=df)\n    \"\"\"\n    ordered_dict = OrderedDict(sorted(my_dict.items(), key=lambda t: t[0]))\n    df = pd.DataFrame(list(ordered_dict.items()), columns=['Key', 'Value'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncollections, json, os, time\n```python\nfrom collections import OrderedDict\nimport json\nimport os\nimport time\n\ndef save_ordered_dict(my_dict, path):\n    \"\"\"\n    Sort a dictionary `my_dict` by keys in ascending order, save it as a JSON file at a given path, \n    and return the datetime of the last modification of this file.\n\n    Parameters:\n    my_dict (dict): The dictionary to be sorted and saved.\n    path (str): The path to save the JSON file.\n\n    Returns:\n    str: The datetime of the last modification of the file.\n\n    Requirements:\n    - collections\n    - json\n    - os\n    - time\n\n    Example:\n    >>> my_dict = {6 : \"abc\", 5 : \"def\", 1: \"hij\", 2 : \"klm\"}\n    >>> print(save_ordered_dict(my_dict, './my_dict.json'))\n    \"\"\"\n    ordered_dict = OrderedDict(sorted(my_dict.items(), key=lambda t: t[0]))\n\n    with open(path, 'w') as f:\n        json.dump(ordered_dict, f)\n\n    modification_time = time.ctime(os.path.getmtime(path))\n\n    return modification_time\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ncollections, prettytable, itertools\n```python\nfrom collections import OrderedDict\nfrom prettytable import PrettyTable\nfrom itertools import chain\n\ndef display_ordered_dict(my_dict):\n    \"\"\"\n    Sort a dictionary `my_dict` by keys in ascending order and display it in a pretty table.\n\n    Parameters:\n    my_dict (dict): The dictionary to be sorted and displayed.\n\n    Returns:\n    None\n\n    Requirements:\n    - collections\n    - prettytable\n    - itertools\n\n    Example:\n    >>> my_dict = {6 : \"abc\", 5 : \"def\", 1: \"hij\", 2 : \"klm\"}\n    >>> display_ordered_dict(my_dict)\n    \"\"\"\n    ordered_dict = OrderedDict(sorted(my_dict.items(), key=lambda t: t[0]))\n    table = PrettyTable(['Key', 'Value'])\n    \n    for key, value in ordered_dict.items():\n        table.add_row([key, value])\n\n    print(table)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ncollections, numpy, pandas\n```python\nfrom collections import OrderedDict\nimport numpy as np\nimport pandas as pd\n\ndef convert_dict_to_df(my_dict):\n    \"\"\"\n    Sort a dictionary `my_dict` by keys in ascending order, convert it into a pandas DataFrame, \n    and add a new column 'Square Value' where each value is the square of the corresponding key.\n\n    Parameters:\n    my_dict (dict): The dictionary to be sorted and converted.\n\n    Returns:\n    DataFrame: A pandas DataFrame converted from the sorted dictionary.\n\n    Requirements:\n    - collections\n    - numpy\n    - pandas\n\n    Example:\n    >>> my_dict = {6 : \"abc\", 5 : \"def\", 1: \"hij\", 2 : \"klm\"}\n    >>> df = convert_dict_to_df(my_dict)\n    >>> print(df)\n    \"\"\"\n    ordered_dict = OrderedDict(sorted(my_dict.items(), key=lambda t: t[0]))\n    df = pd.DataFrame(list(ordered_dict.items()), columns=['Key', 'Value'])\n    df['Square Value'] = np.square(df['Key'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\ncollections, matplotlib, operator\n```python\nfrom collections import OrderedDict\nimport matplotlib.pyplot as plt\nfrom operator import itemgetter\n\ndef plot_dict_by_value(my_dict):\n    \"\"\"\n    Sort a dictionary `my_dict` by values in ascending order and plot the keys and values in a bar chart.\n\n    Parameters:\n    my_dict (dict): The dictionary to be sorted and plotted.\n\n    Returns:\n    None\n\n    Requirements:\n    - collections\n    - matplotlib.pyplot\n    - operator\n\n    Example:\n    >>> my_dict = {6 : \"abc\", 5 : \"def\", 1: \"hij\", 2 : \"klm\"}\n    >>> plot_dict_by_value(my_dict)\n    \"\"\"\n    ordered_dict = OrderedDict(sorted(my_dict.items(), key=itemgetter(1)))\n    \n    plt.bar(ordered_dict.keys(), ordered_dict.values())\n    plt.show()\n```"}
{"seed": "{\"task_id\": 229015, \"prompt\": \"def f_229015():\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"## -*- coding: utf-8 -*-\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    candidate()\\n    s = \\\"\\u041f\\u0440\\u0438\\u0432\\u0435\\u0442\\\"\\n    assert s == \\\"\\u041f\\u0440\\u0438\\u0432\\u0435\\u0442\\\"\\n\"], \"entry_point\": \"f_229015\", \"intent\": \"\\u0423\\u0441\\u0442\\u0430\\u043d\\u043e\\u0432\\u0438\\u0442\\u044c \\u043a\\u043e\\u0434\\u0438\\u0440\\u043e\\u0432\\u043a\\u0443 utf-8 \\u0434\\u043b\\u044f \\u0444\\u0430\\u0439\\u043b\\u0430\", \"library\": []}", "generation": "Scenario 1: \npandas, os, codecs, matplotlib\n```python\nimport pandas as pd\nimport os\nimport codecs\nimport matplotlib.pyplot as plt\n\n# Constants\nENCODING = 'utf-8'\nFILE_PATH = '/path/to/file.txt'\n\ndef read_and_plot(file_path=FILE_PATH, encoding=ENCODING):\n    \"\"\"\n    Read a CSV file with the specified encoding and plot a histogram of a specific column.\n\n    Parameters:\n    file_path (str): The path to the CSV file.\n    encoding (str): The encoding of the CSV file.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - os\n    - codecs\n    - matplotlib.pyplot\n\n    Example:\n    >>> read_and_plot()\n    \"\"\"\n    assert os.path.exists(file_path), f\"File does not exist at {file_path}\"\n\n    with codecs.open(file_path, 'r', encoding=encoding) as f:\n        df = pd.read_csv(f)\n    \n    df['column_name'].plot(kind='hist')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nos, codecs, numpy, matplotlib\n```python\nimport os\nimport codecs\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nENCODING = 'utf-8'\nFILE_PATH = '/path/to/file.txt'\n\ndef calculate_and_plot(file_path=FILE_PATH, encoding=ENCODING):\n    \"\"\"\n    Read a text file with the specified encoding and plot a pie chart of the word frequency.\n\n    Parameters:\n    file_path (str): The path to the text file.\n    encoding (str): The encoding of the text file.\n\n    Returns:\n    None\n\n    Requirements:\n    - os\n    - codecs\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> calculate_and_plot()\n    \"\"\"\n    assert os.path.exists(file_path), f\"File does not exist at {file_path}\"\n\n    with codecs.open(file_path, 'r', encoding=encoding) as f:\n        words = f.read().split()\n\n    unique, counts = np.unique(words, return_counts=True)\n    plt.pie(counts, labels=unique, autopct='%1.1f%%')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nos, codecs, nltk, collections\n```python\nimport os\nimport codecs\nfrom nltk.corpus import stopwords\nfrom collections import Counter\n\n# Constants\nENCODING = 'utf-8'\nFILE_PATH = '/path/to/file.txt'\n\ndef count_words(file_path=FILE_PATH, encoding=ENCODING):\n    \"\"\"\n    Read a text file with the specified encoding and return the frequency of non-stopwords.\n\n    Parameters:\n    file_path (str): The path to the text file.\n    encoding (str): The encoding of the text file.\n\n    Returns:\n    collections.Counter: A Counter object of word frequencies.\n\n    Requirements:\n    - os\n    - codecs\n    - nltk.corpus\n    - collections\n\n    Example:\n    >>> count_words()\n    \"\"\"\n    assert os.path.exists(file_path), f\"File does not exist at {file_path}\"\n\n    with codecs.open(file_path, 'r', encoding=encoding) as f:\n        words = f.read().lower().split()\n    \n    words = [word for word in words if word not in stopwords.words('english')]\n    word_counts = Counter(words)\n\n    return word_counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nos, codecs, spacy\n```python\nimport os\nimport codecs\nimport spacy\n\n# Constants\nENCODING = 'utf-8'\nFILE_PATH = '/path/to/file.txt'\n\ndef extract_entities(file_path=FILE_PATH, encoding=ENCODING):\n    \"\"\"\n    Read a text file with the specified encoding and extract named entities.\n\n    Parameters:\n    file_path (str): The path to the text file.\n    encoding (str): The encoding of the text file.\n\n    Returns:\n    list: A list of named entities.\n\n    Requirements:\n    - os\n    - codecs\n    - spacy\n\n    Example:\n    >>> extract_entities()\n    \"\"\"\n    assert os.path.exists(file_path), f\"File does not exist at {file_path}\"\n\n    nlp = spacy.load('en_core_web_sm')\n\n    with codecs.open(file_path, 'r', encoding=encoding) as f:\n        doc = nlp(f.read())\n    \n    entities = [(ent.text, ent.label_) for ent in doc.ents]\n\n    return entities\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nos, codecs, sklearn, numpy\n```python\nimport os\nimport codecs\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\n\n# Constants\nENCODING = 'utf-8'\nFILE_PATH = '/path/to/file.txt'\n\ndef vectorize_text(file_path=FILE_PATH, encoding=ENCODING):\n    \"\"\"\n    Read a text file with the specified encoding and return a document-term matrix.\n\n    Parameters:\n    file_path (str): The path to the text file.\n    encoding (str): The encoding of the text file.\n\n    Returns:\n    numpy.ndarray: A document-term matrix.\n\n    Requirements:\n    - os\n    - codecs\n    - sklearn.feature_extraction.text\n    - numpy\n\n    Example:\n    >>> vectorize_text()\n    \"\"\"\n    assert os.path.exists(file_path), f\"File does not exist at {file_path}\"\n\n    with codecs.open(file_path, 'r', encoding=encoding) as f:\n        text = [f.read()]\n\n    vectorizer = CountVectorizer()\n    X = vectorizer.fit_transform(text)\n\n    return X.toarray()\n```"}
{"seed": "{\"task_id\": 517085, \"prompt\": \"def f_517085(file_path):\\n\\t\", \"suffix\": \"\\n\\treturn f\", \"canonical_solution\": \"with open(file_path, mode='a', encoding='utf-8') as f:\\n\\n\\t\\tpass\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    file_path = 'opera.txt'\\n    f = candidate(file_path)\\n    assert f.name == file_path\\n    assert f.encoding == 'utf-8'\\n\"], \"entry_point\": \"f_517085\", \"intent\": \"\\u041e\\u0442\\u043a\\u0440\\u044b\\u0442\\u044c \\u0444\\u0430\\u0439\\u043b `file_path` \\u0434\\u043b\\u044f \\u0437\\u0430\\u043f\\u0438\\u0441\\u0438 \\u0441 \\u043a\\u043e\\u0434\\u0438\\u0440\\u043e\\u0432\\u043a\\u043e\\u0439 utf-8 \\u0432 \\u043f\\u0435\\u0440\\u0435\\u043c\\u0435\\u043d\\u043d\\u0443\\u044e f\", \"library\": []}", "generation": "Scenario 3:\nos, pandas, csv, re\n```python\nimport os\nimport pandas as pd\nimport csv\nimport re\n\n# Constants\nFILE_FORMAT = '.txt'\nCSV_FORMAT = '.csv'\n\ndef convert_txt_to_csv(directory):\n    \"\"\"\n    Convert all text files in a directory into CSV format by extracting words and \n    creating a single column CSV file for each text file.\n\n    Parameters:\n    directory (str): The directory containing the text files.\n\n    Returns:\n    list: The list of created CSV files.\n\n    Requirements:\n    - os\n    - pandas\n    - csv\n    - re\n\n    Example:\n    >>> convert_txt_to_csv('./txt_files/')\n    \"\"\"\n    csv_files = []\n\n    for filename in os.listdir(directory):\n        if filename.endswith(FILE_FORMAT):\n            txt_file = os.path.join(directory, filename)\n            csv_file = os.path.splitext(txt_file)[0] + CSV_FORMAT\n\n            with open(txt_file, 'r', encoding='utf-8') as file:\n                words = re.findall(r'\\b\\w+\\b', file.read())\n                df = pd.DataFrame(words, columns=[\"Words\"])\n                df.to_csv(csv_file, index=False, quoting=csv.QUOTE_NONNUMERIC)\n                csv_files.append(csv_file)\n\n    return csv_files\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, pathlib, shutil\n```python\nimport os\nfrom pathlib import Path\nimport shutil\n\n# Constants\nFILE_FORMAT = '.txt'\n\ndef move_files_with_extension(source_dir, target_dir, file_extension=FILE_FORMAT):\n    \"\"\"\n    Move all files with a certain extension from one directory to another.\n\n    Parameters:\n    source_dir (str): The source directory.\n    target_dir (str): The target directory.\n    file_extension (str): The file extension.\n\n    Returns:\n    list: The list of moved files.\n\n    Requirements:\n    - os\n    - pathlib\n    - shutil\n\n    Example:\n    >>> move_files_with_extension('./source_dir/', './target_dir/', '.txt')\n    \"\"\"\n    moved_files = []\n\n    for filename in os.listdir(source_dir):\n        if filename.endswith(file_extension):\n            source_file = os.path.join(source_dir, filename)\n            target_file = os.path.join(target_dir, filename)\n            shutil.move(source_file, target_file)\n            moved_files.append(target_file)\n\n    return moved_files\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, hashlib, binascii\n```python\nimport os\nimport hashlib\nimport binascii\n\n# Constants\nSALT_SIZE = 16\n\ndef hash_password(password):\n    \"\"\"\n    Hash a password for storing in the database.\n\n    Parameters:\n    password (str): The password to be hashed.\n\n    Returns:\n    str: The hashed password.\n\n    Requirements:\n    - os\n    - hashlib\n    - binascii\n\n    Example:\n    >>> hash_password('my_password')\n    \"\"\"\n    salt = hashlib.sha256(os.urandom(SALT_SIZE)).hexdigest().encode('ascii')\n    pwdhash = hashlib.pbkdf2_hmac('sha512', password.encode('utf-8'), salt, 100000)\n    pwdhash = binascii.hexlify(pwdhash)\n    return (salt + pwdhash).decode('ascii')\n```\n`GPT_ODEX_BREAK`\n\nScenario 6:\nos, csv, json\n```python\nimport os\nimport csv\nimport json\n\n# Constants\nFILE_FORMAT = '.csv'\nJSON_FORMAT = '.json'\n\ndef convert_csv_to_json(directory):\n    \"\"\"\n    Convert all CSV files in a directory into JSON format.\n\n    Parameters:\n    directory (str): The directory containing the CSV files.\n\n    Returns:\n    list: The list of created JSON files.\n\n    Requirements:\n    - os\n    - csv\n    - json\n\n    Example:\n    >>> convert_csv_to_json('./csv_files/')\n    \"\"\"\n    json_files = []\n\n    for filename in os.listdir(directory):\n        if filename.endswith(FILE_FORMAT):\n            csv_file = os.path.join(directory, filename)\n            json_file = os.path.splitext(csv_file)[0] + JSON_FORMAT\n\n            with open(csv_file, 'r') as file:\n                reader = csv.DictReader(file)\n                rows = list(reader)\n\n            with open(json_file, 'w') as file:\n                json.dump(rows, file, indent=4)\n\n            json_files.append(json_file)\n\n    return json_files\n```\n`GPT_ODEX_BREAK`\n\nScenario 7:\nos, pandas, matplotlib\n```python\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nFILE_FORMAT = '.csv'\n\ndef generate_histograms(directory):\n    \"\"\"\n    Generate a histogram for each CSV file in a directory.\n\n    Parameters:\n    directory (str): The directory containing the CSV files.\n\n    Requirements:\n    - os\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> generate_histograms('./csv_files/')\n    \"\"\"\n    for filename in os.listdir(directory):\n        if filename.endswith(FILE_FORMAT):\n            csv_file = os.path.join(directory, filename)\n\n            df = pd.read_csv(csv_file)\n            df.hist()\n            plt.show()\n```\nAbove is the illustration."}
{"seed": "{\"task_id\": 517085, \"prompt\": \"def f_517085(file_path):\\n\\t\", \"suffix\": \"\\n\\treturn f\", \"canonical_solution\": \"f = open(file_path,'a', encoding='utf-8')\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    file_path = 'opera.txt'\\n    f = candidate(file_path)\\n    assert f.name == file_path\\n    assert f.encoding == 'utf-8'\\n\"], \"entry_point\": \"f_517085\", \"intent\": \"\\u041e\\u0442\\u043a\\u0440\\u044b\\u0442\\u044c \\u0444\\u0430\\u0439\\u043b `file_path` \\u0434\\u043b\\u044f \\u0437\\u0430\\u043f\\u0438\\u0441\\u0438 \\u0441 \\u043a\\u043e\\u0434\\u0438\\u0440\\u043e\\u0432\\u043a\\u043e\\u0439 utf-8 \\u0432 \\u043f\\u0435\\u0440\\u0435\\u043c\\u0435\\u043d\\u043d\\u0443\\u044e f\", \"library\": []}", "generation": "Scenario 1:\n```python\nimport os\nimport csv\nfrom collections import Counter\n\ndef count_word_frequency(file_path):\n    \"\"\"\n    Open a text file and count the frequency of each word in the file.\n\n    Parameters:\n    file_path (str): The path to the text file.\n\n    Returns:\n    dict: A dictionary with words as keys and their frequencies as values.\n\n    Requirements:\n    - os\n    - csv\n    - collections.Counter\n\n    Example:\n    >>> count_word_frequency('sample_text.txt')\n    \"\"\"\n    if not os.path.isfile(file_path):\n        raise ValueError(\"Invalid file path!\")\n    \n    with open(file_path, 'r', encoding='utf-8') as file:\n        text = file.read().lower().split()\n        word_frequency = dict(Counter(text))\n        \n    return word_frequency\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\n```python\nimport os\nimport json\nfrom operator import itemgetter\n\ndef sort_json_file(file_path):\n    \"\"\"\n    Open a JSON file, sort the data by keys and write the sorted data back to the JSON file.\n\n    Parameters:\n    file_path (str): The path to the JSON file.\n\n    Requirements:\n    - os\n    - json\n    - operator.itemgetter\n\n    Example:\n    >>> sort_json_file('sample.json')\n    \"\"\"\n    if not os.path.isfile(file_path):\n        raise ValueError(\"Invalid file path!\")\n    \n    with open(file_path, 'r', encoding='utf-8') as file:\n        data = json.load(file)\n        sorted_data = dict(sorted(data.items(), key=itemgetter(0)))\n    \n    with open(file_path, 'w', encoding='utf-8') as file:\n        json.dump(sorted_data, file)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\n```python\nimport os\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n\ndef plot_csv_data(file_path):\n    \"\"\"\n    Open a CSV file using pandas and plot the data using matplotlib.\n\n    Parameters:\n    file_path (str): The path to the CSV file.\n\n    Requirements:\n    - os\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_csv_data('sample_data.csv')\n    \"\"\"\n    if not os.path.isfile(file_path):\n        raise ValueError(\"Invalid file path!\")\n    \n    df = pd.read_csv(file_path)\n    df.plot(kind='bar')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\n```python\nimport os\nimport csv\n\ndef calculate_average(file_path, column_name):\n    \"\"\"\n    Open a CSV file, calculate and return the average of a specific column.\n\n    Parameters:\n    file_path (str): The path to the CSV file.\n    column_name (str): The name of the column.\n\n    Returns:\n    float: The average of the specific column.\n\n    Requirements:\n    - os\n    - csv\n\n    Example:\n    >>> calculate_average('sample_data.csv', 'age')\n    \"\"\"\n    if not os.path.isfile(file_path):\n        raise ValueError(\"Invalid file path!\")\n    \n    with open(file_path, 'r', encoding='utf-8') as file:\n        reader = csv.DictReader(file)\n        column_values = [float(row[column_name]) for row in reader]\n        average = sum(column_values) / len(column_values)\n    \n    return average\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\n```python\nimport os\nimport yaml\n\ndef get_yaml_data(file_path, key):\n    \"\"\"\n    Open a YAML file and return the data of a specific key.\n\n    Parameters:\n    file_path (str): The path to the YAML file.\n    key (str): The key.\n\n    Returns:\n    Any: The data of the specific key in the YAML file.\n\n    Requirements:\n    - os\n    - yaml\n\n    Example:\n    >>> get_yaml_data('sample.yaml', 'version')\n    \"\"\"\n    if not os.path.isfile(file_path):\n        raise ValueError(\"Invalid file path!\")\n    \n    with open(file_path, 'r', encoding='utf-8') as file:\n        data = yaml.safe_load(file)\n    \n    return data.get(key)\n```"}
{"seed": "{\"task_id\": 690089, \"prompt\": \"def f_690089(url, data):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"requests.post(url, json=data)\", \"test_start\": \"\\nimport requests\\n\\ndef check(candidate):\", \"test\": [\"\\n    data_dict = {'name': 'dev', 'role': 'leader'}\\n    res = candidate('https://reqres.in/api/users', data_dict)\\n    assert res.url == 'https://reqres.in/api/users'\\n    for k, v in data_dict.items():\\n        assert res.json()[k] == v\\n\"], \"entry_point\": \"f_690089\", \"intent\": \"\\u041e\\u0442\\u043f\\u0440\\u0430\\u0432\\u0438\\u0442\\u044c POST-\\u0437\\u0430\\u043f\\u0440\\u043e\\u0441 c JSON \\u0434\\u0430\\u043d\\u043d\\u044b\\u043c\\u0438 `data` \\u043d\\u0430 `url`\", \"library\": [\"requests\"]}", "generation": "Scenario 1:\nrequests, json, time, random, hashlib\n```python\nimport requests\nimport json\nimport time\nfrom random import randint\nimport hashlib\n\n# Constants\nUSER_AGENTS = [\n    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537', \n    'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:54.0) Gecko/20100101 Firefox/54.0',\n    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36'\n]\n\ndef data_post_request(url, data_dict):\n    \"\"\"\n    Send a POST request with JSON data `data_dict` to `url` with random user-agent and \n    calculate the SHA-256 hash of the response content.\n    \n    Parameters:\n    url (str): The URL to which the request is sent.\n    data_dict (dict): The JSON data to be sent.\n\n    Returns:\n    str: The SHA-256 hash of the response content.\n\n    Requirements:\n    - requests\n    - json\n    - time\n    - random\n    - hashlib\n\n    Example:\n    >>> data_dict = {'name': 'dev', 'role': 'leader'}\n    >>> print(data_post_request('https://reqres.in/api/users', data_dict))\n    \"\"\"\n    headers = {\n        'User-Agent': USER_AGENTS[randint(0, len(USER_AGENTS)-1)],\n        'Content-Type': 'application/json'\n    }\n    time.sleep(randint(1, 3))  # Simulate human behavior by random sleep\n    response = requests.post(url, data=json.dumps(data_dict), headers=headers)\n    response_content_hash = hashlib.sha256(response.content).hexdigest()\n\n    return response_content_hash\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nrequests, bs4, re, pandas\n```python\nimport requests\nfrom bs4 import BeautifulSoup\nimport re\nimport pandas as pd\n\n# Constants\nURL_PATTERN = r\"https?://[A-Za-z0-9./]+\"\n\ndef get_all_links(url):\n    \"\"\"\n    Send a GET request to a `url` and extract all links from the HTML content of the page.\n\n    Parameters:\n    url (str): The URL to which the request is sent.\n\n    Returns:\n    DataFrame: A pandas DataFrame with all extracted links.\n\n    Requirements:\n    - requests\n    - bs4 (BeautifulSoup)\n    - re\n    - pandas\n\n    Example:\n    >>> print(get_all_links('https://www.python.org/'))\n    \"\"\"\n    response = requests.get(url)\n    soup = BeautifulSoup(response.content, 'html.parser')\n\n    all_links = []\n    for link in soup.find_all('a'):\n        href = link.get('href')\n        if href and re.match(URL_PATTERN, href):\n            all_links.append(href)\n\n    df = pd.DataFrame(all_links, columns=['URL'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nrequests, json, time, random\n```python\nimport requests\nimport json\nimport time\nfrom random import randint\n\n# Constants\nURLS = ['https://reqres.in/api/users?page=1', 'https://reqres.in/api/users?page=2', 'https://reqres.in/api/users?page=3']\n\ndef get_and_combine_data():\n    \"\"\"\n    Send GET requests to multiple URLs and combine the JSON data from the responses.\n\n    Returns:\n    dict: A dictionary with combined data.\n\n    Requirements:\n    - requests\n    - json\n    - time\n    - random\n\n    Example:\n    >>> print(get_and_combine_data())\n    \"\"\"\n    combined_data = {}\n    for url in URLS:\n        response = requests.get(url)\n        data = response.json()\n        combined_data.update(data)\n        time.sleep(randint(1, 3))  # Simulate human behavior by random sleep\n\n    return combined_data\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nrequests, json, pandas, matplotlib.pyplot\n```python\nimport requests\nimport json\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef get_and_visualize_data(url):\n    \"\"\"\n    Send a GET request to a `url`, convert the JSON data from the response to a pandas DataFrame and \n    visualize the data using a bar chart.\n\n    Parameters:\n    url (str): The URL to which the request is sent.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the data.\n\n    Requirements:\n    - requests\n    - json\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> print(get_and_visualize_data('https://reqres.in/api/users'))\n    \"\"\"\n    response = requests.get(url)\n    data = response.json()\n    df = pd.DataFrame(data['data'])\n\n    df['first_name'].value_counts().plot(kind='bar')\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nrequests, json, pandas, numpy\n```python\nimport requests\nimport json\nimport pandas as pd\nimport numpy as np\n\ndef get_and_analyze_data(url):\n    \"\"\"\n    Send a GET request to a `url`, convert the JSON data from the response to a pandas DataFrame and \n    perform some analysis on the data.\n\n    Parameters:\n    url (str): The URL to which the request is sent.\n\n    Returns:\n    dict: A dictionary with the analysis results.\n\n    Requirements:\n    - requests\n    - json\n    - pandas\n    - numpy\n\n    Example:\n    >>> print(get_and_analyze_data('https://reqres.in/api/users'))\n    \"\"\"\n    response = requests.get(url)\n    data = response.json()\n\n    df = pd.DataFrame(data['data'])\n    analysis_results = {\n        'count': len(df),\n        'average_id': np.mean(df['id']),\n        'max_id': np.max(df['id']),\n        'min_id': np.min(df['id'])\n    }\n\n    return analysis_results\n```\n"}
{"seed": "{\"task_id\": 550742, \"prompt\": \"def f_550742(file_path):\\n\\t\", \"suffix\": \"\\n\\treturn count_lines\", \"canonical_solution\": \"count_lines = sum(1 for line in open(file_path, 'r'))\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    file_path = 'a.txt'\\n    with open(file_path, mode = 'a') as f:\\n        f.write('3\\\\n4\\\\n5\\\\n')\\n    assert candidate(file_path) == 3\\n\"], \"entry_point\": \"f_550742\", \"intent\": \"\\u0412\\u044b\\u0432\\u0435\\u0441\\u0442\\u0438 \\u043a\\u043e\\u043b\\u0438\\u0447\\u0435\\u0441\\u0442\\u0432\\u043e \\u0441\\u0442\\u0440\\u043e\\u043a `count_lines` \\u0432 \\u0444\\u0430\\u0439\\u043b\\u0435 `file_path`\", \"library\": []}", "generation": "Scenario 1:\nos, csv, pandas, numpy\n```python\nimport os\nimport csv\nimport pandas as pd\nimport numpy as np\n\ndef get_file_stats(file_path):\n    \"\"\"\n    Count the number of lines, words, and characters in a CSV file and return \n    a DataFrame with the statistics for each column.\n\n    Parameters:\n    file_path (str): The path to the CSV file.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the statistics for each column.\n\n    Requirements:\n    - os\n    - csv\n    - pandas\n    - numpy\n\n    Example:\n    >>> file_path = 'sample.csv'\n    >>> stats_df = get_file_stats(file_path)\n    >>> print(stats_df)\n    \"\"\"\n    if not os.path.isfile(file_path):\n        raise ValueError(f\"{file_path} does not exist.\")\n\n    with open(file_path, 'r') as f:\n        reader = csv.reader(f)\n        headers = next(reader)\n        data = list(reader)\n        rows = len(data)\n        words = sum(len(row) for row in data)\n        characters = sum(sum(len(word) for word in row) for row in data)\n\n    stats_df = pd.DataFrame({\n        'Lines': [rows],\n        'Words': [words],\n        'Characters': [characters]\n    }, index=[os.path.basename(file_path)])\n\n    return stats_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, re, collections, pandas\n```python\nimport os\nimport re\nimport collections\nimport pandas as pd\n\ndef count_word_frequency(file_path):\n    \"\"\"\n    Count the frequency of each word in a text file and return a DataFrame \n    with the words and their counts sorted in descending order.\n\n    Parameters:\n    file_path (str): The path to the text file.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the words and their counts.\n\n    Requirements:\n    - os\n    - re\n    - collections\n    - pandas\n\n    Example:\n    >>> file_path = 'sample.txt'\n    >>> freq_df = count_word_frequency(file_path)\n    >>> print(freq_df)\n    \"\"\"\n    if not os.path.isfile(file_path):\n        raise ValueError(f\"{file_path} does not exist.\")\n\n    with open(file_path, 'r') as f:\n        text = f.read().lower()\n        words = re.findall(r'\\b\\w+\\b', text)\n        word_counts = collections.Counter(words)\n\n    freq_df = pd.DataFrame(word_counts.items(), columns=['Word', 'Count']).sort_values(by='Count', ascending=False)\n\n    return freq_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, csv, pandas, matplotlib\n```python\nimport os\nimport csv\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef plot_csv_data(file_path, x_column, y_column):\n    \"\"\"\n    Plot the data from a CSV file using matplotlib.\n\n    Parameters:\n    file_path (str): The path to the CSV file.\n    x_column (str): The name of the column to be used as the x-axis.\n    y_column (str): The name of the column to be used as the y-axis.\n\n    Returns:\n    None\n\n    Requirements:\n    - os\n    - csv\n    - pandas\n    - matplotlib\n\n    Example:\n    >>> file_path = 'sample.csv'\n    >>> plot_csv_data(file_path, 'Date', 'Sales')\n    \"\"\"\n    if not os.path.isfile(file_path):\n        raise ValueError(f\"{file_path} does not exist.\")\n\n    df = pd.read_csv(file_path)\n    df.plot(x=x_column, y=y_column, kind='line')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, csv, pandas, numpy\n```python\nimport os\nimport csv\nimport pandas as pd\nimport numpy as np\n\ndef calculate_csv_stats(file_path):\n    \"\"\"\n    Calculate the mean, median, mode, and standard deviation of each numeric \n    column in a CSV file and return a DataFrame with the statistics.\n\n    Parameters:\n    file_path (str): The path to the CSV file.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the statistics for each numeric column.\n\n    Requirements:\n    - os\n    - csv\n    - pandas\n    - numpy\n\n    Example:\n    >>> file_path = 'sample.csv'\n    >>> stats_df = calculate_csv_stats(file_path)\n    >>> print(stats_df)\n    \"\"\"\n    if not os.path.isfile(file_path):\n        raise ValueError(f\"{file_path} does not exist.\")\n\n    df = pd.read_csv(file_path)\n    numeric_df = df.select_dtypes(include=[np.number])\n\n    stats_df = pd.DataFrame({\n        'Mean': numeric_df.mean(),\n        'Median': numeric_df.median(),\n        'Mode': numeric_df.mode().iloc[0],\n        'Standard Deviation': numeric_df.std()\n    })\n\n    return stats_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, csv, pandas, matplotlib\n```python\nimport os\nimport csv\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef plot_histogram(file_path, column):\n    \"\"\"\n    Plot a histogram of a numeric column from a CSV file using matplotlib.\n\n    Parameters:\n    file_path (str): The path to the CSV file.\n    column (str): The name of the numeric column to be plotted.\n\n    Returns:\n    None\n\n    Requirements:\n    - os\n    - csv\n    - pandas\n    - matplotlib\n\n    Example:\n    >>> file_path = 'sample.csv'\n    >>> plot_histogram(file_path, 'Age')\n    \"\"\"\n    if not os.path.isfile(file_path):\n        raise ValueError(f\"{file_path} does not exist.\")\n\n    df = pd.read_csv(file_path)\n    df[column].hist(bins=10)\n    plt.show()\n```"}
{"seed": "{\"task_id\": 928295, \"prompt\": \"def f_928295(file_path):\\n\\t\", \"suffix\": \"\\n\\treturn my_list\", \"canonical_solution\": \"with open(file_path) as file:\\n\\n\\t\\tmy_list = [row.strip() for row in file]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    file_path = 'a.txt'\\n    with open(file_path, mode = 'a') as f:\\n        f.write('3\\\\n4\\\\n5\\\\n')\\n    assert candidate(file_path) == ['3', '4', '5']\\n\"], \"entry_point\": \"f_928295\", \"intent\": \"\\u041f\\u0440\\u043e\\u0447\\u0438\\u0442\\u0430\\u0442\\u044c \\u043f\\u043e\\u0441\\u0442\\u0440\\u043e\\u0447\\u043d\\u043e \\u0434\\u0430\\u043d\\u043d\\u044b\\u0435 \\u0438\\u0437 \\u0444\\u0430\\u0439\\u043b\\u0430 `file_path` \\u0432 \\u0441\\u043f\\u0438\\u0441\\u043e\\u043a `my_list`\", \"library\": []}", "generation": "Scenario 1:\npandas, json, os, csv\n```python\nimport pandas as pd\nimport json\nimport os\nimport csv\n\n# Constants\nCSV_FILE = 'file_path.csv'\nJSON_FILE = 'file_path.json'\n\ndef convert_csv_to_json(csv_file=CSV_FILE, json_file=JSON_FILE):\n    \"\"\"\n    This function reads a CSV file and converts it into a JSON file.\n\n    Parameters:\n    csv_file (str): Path to the CSV file.\n    json_file (str): Path to the JSON file where to save the data.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - json\n    - os\n    - csv\n\n    Example:\n    >>> convert_csv_to_json('file_path.csv', 'file_path.json')\n    \"\"\"\n    if not os.path.isfile(csv_file):\n        raise FileNotFoundError(f\"{csv_file} does not exist.\")\n\n    # Read CSV file\n    df = pd.read_csv(csv_file)\n    \n    # Convert to JSON\n    json_data = df.to_json(orient='records')\n\n    # Write to JSON file\n    with open(json_file, 'w') as f:\n        f.write(json_data)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, matplotlib, numpy, os\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# Constants\nCSV_FILE = 'file_path.csv'\n\ndef plot_data_from_csv(csv_file=CSV_FILE):\n    \"\"\"\n    This function reads a CSV file and plots the data.\n\n    Parameters:\n    csv_file (str): Path to the CSV file.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - numpy\n    - os\n\n    Example:\n    >>> plot_data_from_csv('file_path.csv')\n    \"\"\"\n    if not os.path.isfile(csv_file):\n        raise FileNotFoundError(f\"{csv_file} does not exist.\")\n\n    # Read CSV file\n    df = pd.read_csv(csv_file)\n\n    # Plot data\n    df.plot(kind='line')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, sklearn.model_selection, sklearn.linear_model, os\n```python\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport os\n\n# Constants\nCSV_FILE = 'file_path.csv'\nTARGET = 'target_column'\n\ndef train_model_from_csv(csv_file=CSV_FILE, target=TARGET):\n    \"\"\"\n    This function reads a CSV file, splits the data into training and testing sets, and trains a linear regression model.\n\n    Parameters:\n    csv_file (str): Path to the CSV file.\n    target (str): Target column for the model.\n\n    Returns:\n    LinearRegression: Trained linear regression model.\n\n    Requirements:\n    - pandas\n    - sklearn.model_selection.train_test_split\n    - sklearn.linear_model.LinearRegression\n    - os\n\n    Example:\n    >>> model = train_model_from_csv('file_path.csv', 'target_column')\n    \"\"\"\n    if not os.path.isfile(csv_file):\n        raise FileNotFoundError(f\"{csv_file} does not exist.\")\n\n    # Read CSV file\n    df = pd.read_csv(csv_file)\n\n    # Split into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(df.drop(target, axis=1), df[target], test_size=0.2, random_state=42)\n\n    # Train model\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    \n    return model\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, sklearn.ensemble, sklearn.metrics, os\n```python\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nimport os\n\n# Constants\nCSV_FILE = 'file_path.csv'\nTARGET = 'target_column'\n\ndef evaluate_model_from_csv(csv_file=CSV_FILE, target=TARGET):\n    \"\"\"\n    This function reads a CSV file, splits the data into training and testing sets, trains a random forest classifier, and evaluates its accuracy.\n\n    Parameters:\n    csv_file (str): Path to the CSV file.\n    target (str): Target column for the model.\n\n    Returns:\n    float: Accuracy of the model.\n\n    Requirements:\n    - pandas\n    - sklearn.ensemble.RandomForestClassifier\n    - sklearn.metrics.accuracy_score\n    - os\n\n    Example:\n    >>> accuracy = evaluate_model_from_csv('file_path.csv', 'target_column')\n    \"\"\"\n    if not os.path.isfile(csv_file):\n        raise FileNotFoundError(f\"{csv_file} does not exist.\")\n\n    # Read CSV file\n    df = pd.read_csv(csv_file)\n\n    # Split into training and testing sets\n    X = df.drop(target, axis=1)\n    y = df[target]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    # Train model\n    model = RandomForestClassifier(n_estimators=100)\n    model.fit(X_train, y_train)\n\n    # Evaluate model\n    y_pred = model.predict(X_test)\n\n    return accuracy_score(y_test, y_pred)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, os, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nimport os\nimport seaborn as sns\n\n# Constants\nCSV_FILE = 'file_path.csv'\n\ndef plot_corr_from_csv(csv_file=CSV_FILE):\n    \"\"\"\n    This function reads a CSV file and plots the correlation matrix of its columns.\n\n    Parameters:\n    csv_file (str): Path to the CSV file.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - numpy\n    - os\n    - seaborn\n\n    Example:\n    >>> plot_corr_from_csv('file_path.csv')\n    \"\"\"\n    if not os.path.isfile(csv_file):\n        raise FileNotFoundError(f\"{csv_file} does not exist.\")\n\n    # Read CSV file\n    df = pd.read_csv(csv_file)\n\n    # Calculate correlation matrix\n    corr = df.corr()\n\n    # Plot correlation matrix\n    sns.heatmap(corr, annot=True, cmap='coolwarm')\n    plt.show()\n```\n"}
{"seed": "{\"task_id\": 928295, \"prompt\": \"def f_928295(file_path):\\n\\t\", \"suffix\": \"\\n\\treturn my_list\", \"canonical_solution\": \"with open(file_path, 'r') as f:\\n\\n\\t\\tmy_list = f.read().splitlines()\\n\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    file_path = 'a.txt'\\n    with open(file_path, mode = 'a') as f:\\n        f.write('3\\\\n4\\\\n5\\\\n')\\n    assert candidate(file_path) == ['3', '4', '5']\\n\"], \"entry_point\": \"f_928295\", \"intent\": \"\\u041f\\u0440\\u043e\\u0447\\u0438\\u0442\\u0430\\u0442\\u044c \\u043f\\u043e\\u0441\\u0442\\u0440\\u043e\\u0447\\u043d\\u043e \\u0434\\u0430\\u043d\\u043d\\u044b\\u0435 \\u0438\\u0437 \\u0444\\u0430\\u0439\\u043b\\u0430 `file_path` \\u0432 \\u0441\\u043f\\u0438\\u0441\\u043e\\u043a `my_list`\", \"library\": []}", "generation": "Scenario 1:\nos, pandas, csv, random\n```python\nimport os\nimport pandas as pd\nimport csv\nfrom random import randint\n\n# Constants\nFILE_PATH = 'sample_data.csv'\n\ndef generate_random_dataset(file_path):\n    \"\"\"\n    Generate a random dataset with three columns: 'Name', 'Age', 'Occupation'.\n    The data should be written to a CSV file. After writing the data, read the \n    file and return a pandas DataFrame.\n\n    Parameters:\n    file_path (str): The path to the file.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the data.\n\n    Requirements:\n    - os\n    - pandas\n    - csv\n    - random\n\n    Example:\n    >>> df = generate_random_dataset(FILE_PATH)\n    >>> print(df.head())\n    \"\"\"\n    names = ['John', 'Anna', 'Peter', 'Linda', 'Tom']\n    occupations = ['Engineer', 'Doctor', 'Teacher', 'Nurse', 'Driver']\n\n    with open(file_path, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Name', 'Age', 'Occupation'])\n        for _ in range(100):\n            writer.writerow([names[randint(0, 4)], randint(18, 65), occupations[randint(0, 4)]])\n\n    return pd.read_csv(file_path)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, matplotlib, os, random\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nfrom random import randint\n\n# Constants\nFILE_PATH = 'random_numbers.npy'\n\ndef generate_and_plot_random_numbers(file_path):\n    \"\"\"\n    Generate an array of random numbers, save it to a file, \n    then load the array from the file and plot a histogram.\n\n    Parameters:\n    file_path (str): The path to the file.\n\n    Returns:\n    array: A numpy array of random numbers.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - os\n    - random\n\n    Example:\n    >>> arr = generate_and_plot_random_numbers(FILE_PATH)\n    >>> print(arr)\n    \"\"\"\n    arr = np.random.randint(low=0, high=100, size=1000)\n    np.save(file_path, arr)\n\n    if os.path.exists(file_path):\n        arr = np.load(file_path)\n        plt.hist(arr, bins=10, alpha=0.5)\n        plt.show()\n    \n    return arr\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\njson, os, pandas, matplotlib\n```python\nimport json\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nFILE_PATH = 'data.json'\n\ndef process_json_data(file_path):\n    \"\"\"\n    Read a JSON file, convert it to a pandas DataFrame, and plot \n    the distribution of values for a specific column.\n\n    Parameters:\n    file_path (str): The path to the file.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the data.\n\n    Requirements:\n    - json\n    - os\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = process_json_data(FILE_PATH)\n    >>> print(df.head())\n    \"\"\"\n    with open(file_path, 'r') as f:\n        data = json.load(f)\n\n    df = pd.DataFrame(data)\n    df['column_name'].value_counts().plot(kind='bar')\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nast, os, csv, pandas\n```python\nimport ast\nimport os\nimport csv\nimport pandas as pd\n\n# Constants\nFILE_PATH = 'data.txt'\n\ndef process_text_data(file_path):\n    \"\"\"\n    Read a text file where each line is a list of values. Convert each line to a list using ast.literal_eval, \n    write the data to a CSV file, then read the CSV file into a pandas DataFrame.\n\n    Parameters:\n    file_path (str): The path to the file.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the data.\n\n    Requirements:\n    - ast\n    - os\n    - csv\n    - pandas\n\n    Example:\n    >>> df = process_text_data(FILE_PATH)\n    >>> print(df.head())\n    \"\"\"\n    data = []\n    with open(file_path, 'r') as f:\n        for line in f:\n            data.append(ast.literal_eval(line.strip()))\n\n    with open('data.csv', 'w', newline='') as f:\n        writer = csv.writer(f)\n        for row in data:\n            writer.writerow(row)\n\n    df = pd.read_csv('data.csv')\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nre, os, nltk, pandas\n```python\nimport re\nimport os\nimport nltk\nfrom nltk.corpus import stopwords\nimport pandas as pd\n\n# Constants\nFILE_PATH = 'text_data.txt'\n\ndef process_text_and_count_words(file_path):\n    \"\"\"\n    Read a text file, tokenize the text, remove stopwords, \n    and count the frequency of the remaining words. Return a pandas DataFrame \n    with the words and their frequencies.\n\n    Parameters:\n    file_path (str): The path to the file.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the words and their frequencies.\n\n    Requirements:\n    - re\n    - os\n    - nltk\n    - pandas\n\n    Example:\n    >>> df = process_text_and_count_words(FILE_PATH)\n    >>> print(df.head())\n    \"\"\"\n    with open(file_path, 'r') as f:\n        text = f.read().lower()\n\n    words = re.findall(r'\\b\\w+\\b', text)\n    words = [word for word in words if word not in stopwords.words('english')]\n\n    word_counts = nltk.FreqDist(words)\n    df = pd.DataFrame(list(word_counts.items()), columns=['Word', 'Frequency'])\n\n    return df\n```"}
{"seed": "{\"task_id\": 928295, \"prompt\": \"def f_928295(file_path):\\n\\t\", \"suffix\": \"\\n\\treturn my_list\", \"canonical_solution\": \"\\n\\twith io.open(file_path, encoding='utf-8') as f:\\n\\t\\tmy_list = f.read().splitlines()\\n\", \"test_start\": \"\\nimport io\\n\\ndef check(candidate):\", \"test\": [\"\\n    file_path = 'a.txt'\\n    with open(file_path, mode = 'a') as f:\\n        f.write('3\\\\n4\\\\n5\\\\n')\\n    assert candidate(file_path) == ['3', '4', '5']\\n\"], \"entry_point\": \"f_928295\", \"intent\": \"\\u041f\\u0440\\u043e\\u0447\\u0438\\u0442\\u0430\\u0442\\u044c \\u043f\\u043e\\u0441\\u0442\\u0440\\u043e\\u0447\\u043d\\u043e \\u0434\\u0430\\u043d\\u043d\\u044b\\u0435 \\u0438\\u0437 \\u0444\\u0430\\u0439\\u043b\\u0430 `file_path` \\u0432 \\u0441\\u043f\\u0438\\u0441\\u043e\\u043a `my_list`\", \"library\": [\"io\"]}", "generation": "Scenario 1: \ncsv, io, pandas, matplotlib\n```python\nimport csv\nimport io\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef analyze_csv_file(file_path):\n    \"\"\"\n    Read the CSV file from the given path, generate a pandas DataFrame, perform \n    analysis, and plot the data.\n    \n    The CSV file should contain two columns: 'Date' and 'Value'.\n    \n    Parameters:\n    file_path (str): The path to the CSV file.\n    \n    Returns:\n    DataFrame: The generated pandas DataFrame.\n    \n    Requirements:\n    - csv\n    - io\n    - pandas\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = analyze_csv_file('data.csv')\n    >>> print(df.describe())\n    >>> df.plot(x='Date', y='Value')\n    \"\"\"\n    with io.open(file_path, mode='r', encoding='utf-8') as f:\n        reader = csv.DictReader(f)\n        data = list(reader)\n\n    df = pd.DataFrame(data)\n\n    df['Date'] = pd.to_datetime(df['Date'])\n    df['Value'] = pd.to_numeric(df['Value'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nio, os, zipfile, glob\n```python\nimport io\nimport os\nimport zipfile\nimport glob\n\ndef extract_zip_files(directory):\n    \"\"\"\n    Extract all zip files in the given directory.\n\n    Parameters:\n    directory (str): The directory containing the zip files.\n\n    Requirements:\n    - io\n    - os\n    - zipfile\n    - glob\n\n    Example:\n    >>> extract_zip_files('/path/to/directory')\n    \"\"\"\n    zip_files = glob.glob(os.path.join(directory, '*.zip'))\n\n    for zip_file in zip_files:\n        with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n            zip_ref.extractall(directory)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nio, json, requests, base64\n```python\nimport io\nimport json\nimport requests\nimport base64\n\ndef download_image_from_url(url, output_file):\n    \"\"\"\n    Download an image file from a URL and save it to the output file path in base64 format.\n\n    Parameters:\n    url (str): The URL of the image to download.\n    output_file (str): The path to the output file.\n\n    Requirements:\n    - io\n    - json\n    - requests\n    - base64\n\n    Example:\n    >>> download_image_from_url('https://example.com/image.jpg', '/path/to/output.txt')\n    \"\"\"\n    response = requests.get(url)\n    image_data = base64.b64encode(response.content).decode('utf-8')\n\n    with io.open(output_file, 'w', encoding='utf-8') as f:\n        json.dump({'image': image_data}, f)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nio, os, shutil, fnmatch\n```python\nimport io\nimport os\nimport shutil\nimport fnmatch\n\ndef move_files(source_dir, target_dir, file_pattern):\n    \"\"\"\n    Move all files matching a pattern from a source directory to a target directory.\n\n    Parameters:\n    source_dir (str): The source directory.\n    target_dir (str): The target directory.\n    file_pattern (str): The file pattern to match.\n\n    Requirements:\n    - io\n    - os\n    - shutil\n    - fnmatch\n\n    Example:\n    >>> move_files('/path/to/source', '/path/to/target', '*.txt')\n    \"\"\"\n    for root, dirnames, filenames in os.walk(source_dir):\n        for filename in fnmatch.filter(filenames, file_pattern):\n            shutil.move(os.path.join(root, filename), target_dir)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nio, hashlib, os, binascii\n```python\nimport io\nimport hashlib\nimport os\nimport binascii\n\ndef compute_file_checksum(file_path):\n    \"\"\"\n    Compute the MD5 checksum of a file.\n\n    Parameters:\n    file_path (str): The path to the file.\n\n    Returns:\n    str: The MD5 checksum of the file.\n\n    Requirements:\n    - io\n    - hashlib\n    - os\n    - binascii\n\n    Example:\n    >>> checksum = compute_file_checksum('/path/to/file.txt')\n    >>> print(checksum)\n    \"\"\"\n    hasher = hashlib.md5()\n\n    with io.open(file_path, 'rb') as f:\n        while True:\n            data = f.read(8192)\n            if not data:\n                break\n            hasher.update(data)\n\n    return binascii.hexlify(hasher.digest()).decode()\n```"}
{"seed": "{\"task_id\": 255207, \"prompt\": \"def f_255207():\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"## -*- coding: utf-8 -*-\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    candidate()\\n    s = \\\"\\u041f\\u0440\\u0438\\u0432\\u0435\\u0442\\\"\\n    assert s == \\\"\\u041f\\u0440\\u0438\\u0432\\u0435\\u0442\\\"\\n\"], \"entry_point\": \"f_255207\", \"intent\": \"\\u0423\\u0441\\u0442\\u0430\\u043d\\u043e\\u0432\\u0438\\u0442\\u044c \\u043a\\u043e\\u0434\\u0438\\u0440\\u043e\\u0432\\u043a\\u0443 utf-8 \\u0434\\u043b\\u044f \\u0444\\u0430\\u0439\\u043b\\u0430\", \"library\": []}", "generation": "Scenario 1:\npandas, re, matplotlib, random\n```python\nimport pandas as pd\nimport re\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nWORD_PATTERN = r\"\\b[a-zA-Z]+\\b\"\n\ndef analyze_text_file(file_path):\n    \"\"\"\n    Read the text from a file, count the frequency of every word, and \n    plot a bar chart for the top 10 frequency words.\n\n    Parameters:\n    file_path (str): The path to the text file.\n\n    Returns:\n    DataFrame: A pandas DataFrame with words and their frequencies.\n\n    Requirements:\n    - pandas\n    - re\n    - matplotlib.pyplot\n\n    Example:\n    >>> analyze_text_file('my_file.txt')\n    \"\"\"\n    with open(file_path, 'r', encoding='utf-8') as file:\n        text = file.read().lower()\n\n    words = re.findall(WORD_PATTERN, text)\n    word_counts = pd.Series(words).value_counts().sort_values(ascending=False)\n\n    top_10_words = word_counts[:10]\n    top_10_words.plot(kind='bar')\n\n    return word_counts.to_frame('Frequency')\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, scipy.stats, matplotlib\n```python\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\n# Constants\nMU = 0\nSIGMA = 1\nNUM_SAMPLES = 1000\n\ndef generate_normal_distribution():\n    \"\"\"\n    Generate a normal distribution with mean 0 and standard deviation 1, \n    and plot the histogram along with the probability density function.\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Example:\n    >>> generate_normal_distribution()\n    \"\"\"\n    samples = np.random.normal(MU, SIGMA, NUM_SAMPLES)\n\n    plt.hist(samples, bins=30, density=True, alpha=0.6, color='g')\n\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, MU, SIGMA)\n    plt.plot(x, p, 'k', linewidth=2)\n\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nDATA = {\n    'Product': ['Product A', 'Product B', 'Product C', 'Product D', 'Product E'],\n    'Sales': [1500, 1200, 1000, 700, 600]\n}\n\ndef plot_sales_data():\n    \"\"\"\n    Given sales data for different products, plot a pie chart to show \n    the proportion of sales for each product.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_sales_data()\n    \"\"\"\n    df = pd.DataFrame(DATA)\n    df['Sales'] = df['Sales'] / df['Sales'].sum()\n\n    plt.pie(df['Sales'], labels=df['Product'], autopct='%1.1f%%')\n    plt.title('Sales by Product')\n\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, matplotlib, random\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nNUM_POINTS = 100\n\ndef plot_random_walk():\n    \"\"\"\n    Generate a random walk and plot it.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> plot_random_walk()\n    \"\"\"\n    walk = [0]\n    for i in range(NUM_POINTS - 1):\n        walk.append(walk[-1] + randint(-1, 1))\n\n    plt.plot(walk)\n    plt.title('Random Walk')\n\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, pandas, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nNUM_SAMPLES = 1000\n\ndef plot_normal_dist_histogram():\n    \"\"\"\n    Generate samples from a standard normal distribution and plot a histogram.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_normal_dist_histogram()\n    \"\"\"\n    data = np.random.randn(NUM_SAMPLES)\n    df = pd.DataFrame(data, columns=['Value'])\n\n    df['Value'].plot(kind='hist', bins=30)\n    plt.title('Histogram of Normal Distribution')\n\n    plt.show()\n```\n"}
{"seed": "{\"task_id\": 777723, \"prompt\": \"def f_777723(s, sub_s):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"s.replace(sub_s, \\\"\\\")\\n\\t\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(\\\"1;2;3;4;5\\\", \\\";\\\") == \\\"12345\\\"\\n\", \"\\n    assert candidate(\\\"1 2 3 4 5\\\", \\\" \\\") == \\\"12345\\\"\\n\"], \"entry_point\": \"f_777723\", \"intent\": \"\\u0423\\u0434\\u0430\\u043b\\u0438\\u0442\\u044c \\u043f\\u043e\\u0434\\u0441\\u0442\\u0440\\u043e\\u043a\\u0443 `sub_s`  \\u0438\\u0437 \\u0441\\u0442\\u0440\\u043e\\u043a\\u0438 `s`\", \"library\": []}", "generation": "Scenario 1:\npandas, seaborn, matplotlib, nltk\n```python\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\nSTOPWORDS = set(stopwords.words('english'))\n\ndef count_word_frequency(text, exclude_stopwords):\n    \"\"\"\n    Count the frequency of words in a text. Option to exclude common English stopwords.\n\n    Parameters:\n    text (str): The input text.\n    exclude_stopwords (bool): Whether to exclude common English stopwords.\n\n    Returns:\n    DataFrame: A pandas DataFrame with word frequencies.\n\n    Requirements:\n    - pandas\n    - seaborn\n    - matplotlib.pyplot\n    - nltk.corpus.stopwords\n    - nltk.tokenize.word_tokenize\n\n    Example:\n    >>> text = \"This is a sample text. This text is for testing the word frequency function.\"\n    >>> df = count_word_frequency(text, True)\n    >>> print(df)\n    >>> sns.barplot(data=df, x='Word', y='Frequency')\n    \"\"\"\n    words = word_tokenize(text)\n    if exclude_stopwords:\n        words = [word for word in words if word not in STOPWORDS]\n\n    freq_df = pd.DataFrame(words, columns=['Word'])\n    freq_df['Frequency'] = freq_df.groupby('Word')['Word'].transform('count')\n    freq_df = freq_df.drop_duplicates()\n\n    return freq_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, pandas, matplotlib, scipy.stats\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\ndef generate_normal_distribution(mu, sigma, n):\n    \"\"\"\n    Generate a normal distribution with specified mean and standard deviation, and plot \n    the histogram and the probability density function.\n\n    Parameters:\n    mu (float): The mean of the normal distribution.\n    sigma (float): The standard deviation of the normal distribution.\n    n (int): The number of random numbers to generate.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - scipy.stats.norm\n\n    Example:\n    >>> generate_normal_distribution(0, 1, 1000)\n    \"\"\"\n    data = np.random.normal(mu, sigma, n)\n    df = pd.DataFrame(data, columns=['Value'])\n\n    plt.figure(figsize=(10, 5))\n\n    plt.subplot(1, 2, 1)\n    df['Value'].hist(bins=30, density=True)\n    plt.title('Histogram')\n\n    plt.subplot(1, 2, 2)\n    x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n    plt.plot(x, norm.pdf(x, mu, sigma))\n    plt.title('Probability Density Function')\n\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, matplotlib, sklearn.preprocessing, re\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nimport re\n\ndef normalize_and_plot(data):\n    \"\"\"\n    Normalize a list of numerical data using MinMaxScaler and plot the original and \n    normalized data.\n\n    Parameters:\n    data (list): The list of numerical data.\n\n    Returns:\n    list: The normalized data.\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - sklearn.preprocessing.MinMaxScaler\n    - re\n\n    Example:\n    >>> data = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n    >>> normalized_data = normalize_and_plot(data)\n    >>> print(normalized_data)\n    \"\"\"\n    data = [[x] for x in data]\n    scaler = MinMaxScaler()\n    normalized_data = scaler.fit_transform(data)\n\n    plt.figure(figsize=(10, 5))\n\n    plt.subplot(1, 2, 1)\n    plt.plot(data)\n    plt.title('Original Data')\n\n    plt.subplot(1, 2, 2)\n    plt.plot(normalized_data)\n    plt.title('Normalized Data')\n\n    plt.show()\n\n    return normalized_data.flatten().tolist()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, datetime, matplotlib, sklearn.linear_model\n```python\nimport pandas as pd\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\ndef linear_regression(x, y):\n    \"\"\"\n    Perform linear regression on two lists of numbers and plot the data points and \n    the regression line.\n\n    Parameters:\n    x (list): The list of x-values.\n    y (list): The list of y-values.\n\n    Returns:\n    LinearRegression: The fitted LinearRegression model.\n\n    Requirements:\n    - pandas\n    - datetime\n    - matplotlib.pyplot\n    - sklearn.linear_model.LinearRegression\n\n    Example:\n    >>> x = [1, 2, 3, 4, 5]\n    >>> y = [2, 4, 5, 4, 5]\n    >>> model = linear_regression(x, y)\n    >>> print(model.coef_, model.intercept_)\n    \"\"\"\n    x = np.array(x).reshape(-1, 1)\n    y = np.array(y)\n\n    model = LinearRegression()\n    model.fit(x, y)\n\n    plt.scatter(x, y, color='blue')\n    plt.plot(x, model.predict(x), color='red')\n    plt.show()\n\n    return model\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, matplotlib, sklearn.cluster, sklearn.datasets\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\n\ndef kmeans_clustering(n_samples, n_clusters):\n    \"\"\"\n    Generate a 2D dataset with specified number of samples, perform K-means clustering with \n    specified number of clusters, and plot the data points and cluster centers.\n\n    Parameters:\n    n_samples (int): The number of samples.\n    n_clusters (int): The number of clusters.\n\n    Returns:\n    KMeans: The fitted KMeans model.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.cluster.KMeans\n    - sklearn.datasets.make_blobs\n\n    Example:\n    >>> model = kmeans_clustering(100, 3)\n    >>> print(model.cluster_centers_)\n    \"\"\"\n    X, _ = make_blobs(n_samples=n_samples, centers=n_clusters, random_state=0)\n\n    model = KMeans(n_clusters=n_clusters)\n    model.fit(X)\n\n    plt.scatter(X[:, 0], X[:, 1], c=model.labels_)\n    plt.scatter(model.cluster_centers_[:, 0], model.cluster_centers_[:, 1], c='red')\n    plt.show()\n\n    return model\n```"}
{"seed": "{\"task_id\": 422461, \"prompt\": \"def f_422461(my_list):\\n\\t\", \"suffix\": \"\\n\\treturn my_list\", \"canonical_solution\": \"my_list = [value for value in my_list if value]\\n\\t\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([1, 2, 0, 3, 4, 5]) == [1, 2, 3, 4, 5]\\n\", \"\\n    assert candidate(['1', '2', '', '3', '4', '5']) == ['1', '2', '3', '4', '5']\\n\"], \"entry_point\": \"f_422461\", \"intent\": \"\\u0423\\u0434\\u0430\\u043b\\u0438\\u0442\\u044c \\u043f\\u0443\\u0441\\u0442\\u044b\\u0435 \\u044d\\u043b\\u0435\\u043c\\u0435\\u043d\\u0442\\u044b \\u0438\\u0437 \\u0441\\u043f\\u0438\\u0441\\u043a\\u0430 `my_list`\", \"library\": []}", "generation": "Scenario 1:\npandas, numpy, matplotlib, random\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import choice\n\n# Constants\nLIST_LENGTH = 1000\nELEMENTS = list(range(LIST_LENGTH))\nCOLORS = ['red', 'blue', 'green', 'yellow', 'purple']\n\ndef generate_and_analyze_list():\n    \"\"\"\n    Generate a list of random numbers from a defined set, remove zero elements, \n    count occurrences of each number, and plot the distribution.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - random.choice\n    \n    Example:\n    >>> generate_and_analyze_list()\n    \"\"\"\n    my_list = [choice(ELEMENTS) for _ in range(LIST_LENGTH)]\n    my_list = [value for value in my_list if value]  # Removing zeros\n\n    counts = pd.Series(my_list).value_counts().sort_index()\n    plt.bar(counts.index, counts, color=choice(COLORS))\n    plt.show()\n\n    return counts\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, matplotlib, random\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import choice\n\n# Constants\nELEMENTS = ['apple', 'banana', 'cherry', 'date', 'elderberry', '', 'grape']\n\ndef generate_and_analyze_fruit_list():\n    \"\"\"\n    Generate a list of random fruits from a defined set, remove empty strings, \n    count occurrences of each fruit, and plot the distribution.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - random.choice\n    \n    Example:\n    >>> generate_and_analyze_fruit_list()\n    \"\"\"\n    my_list = [choice(ELEMENTS) for _ in range(LIST_LENGTH)]\n    my_list = [value for value in my_list if value]  # Removing empty strings\n\n    counts = pd.Series(my_list).value_counts().sort_index()\n    plt.barh(counts.index, counts)\n    plt.show()\n\n    return counts\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, matplotlib, random\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import choice\n\n# Constants\nELEMENTS = np.arange(-10, 11)\n\ndef generate_and_analyze_number_list():\n    \"\"\"\n    Generate a list of random numbers from a defined set, remove zero elements,\n    plot histogram of the list and calculate mean, median and standard deviation.\n    \n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - random.choice\n    \n    Example:\n    >>> generate_and_analyze_number_list()\n    \"\"\"\n    my_list = np.array([choice(ELEMENTS) for _ in range(LIST_LENGTH)])\n    my_list = my_list[my_list != 0]  # Removing zeros\n\n    plt.hist(my_list, bins=len(ELEMENTS)-1)\n    plt.show()\n\n    mean = my_list.mean()\n    median = np.median(my_list)\n    std_dev = my_list.std()\n\n    return mean, median, std_dev\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, matplotlib, random\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import choice\n\n# Constants\nELEMENTS = np.array([1, 2, 3, 4, 5, 0, 6, 7, 8, 9, 10])\n\ndef generate_and_plot_cumulative_sum():\n    \"\"\"\n    Generate a list of random numbers from a defined set, remove zero elements,\n    and plot the cumulative sum of the list.\n    \n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - random.choice\n    \n    Example:\n    >>> generate_and_plot_cumulative_sum()\n    \"\"\"\n    my_list = np.array([choice(ELEMENTS) for _ in range(LIST_LENGTH)])\n    my_list = my_list[my_list != 0]  # Removing zeros\n\n    plt.plot(my_list.cumsum())\n    plt.show()\n\n    return my_list.cumsum()\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, matplotlib, random\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import choice\n\n# Constants\nELEMENTS = np.linspace(-1, 1, 100)\n\ndef generate_and_plot_scatter():\n    \"\"\"\n    Generate two lists of random numbers from a defined set, remove zero elements,\n    and plot the scatter plot of the two lists.\n    \n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - random.choice\n    \n    Example:\n    >>> generate_and_plot_scatter()\n    \"\"\"\n    my_list1 = np.array([choice(ELEMENTS) for _ in range(LIST_LENGTH)])\n    my_list2 = np.array([choice(ELEMENTS) for _ in range(LIST_LENGTH)])\n\n    my_list1 = my_list1[my_list1 != 0]  # Removing zeros\n    my_list2 = my_list2[my_list2 != 0]  # Removing zeros\n\n    plt.scatter(my_list1, my_list2)\n    plt.show()\n\n    return my_list1, my_list2\n```"}
{"seed": "{\"task_id\": 422461, \"prompt\": \"def f_422461(my_list):\\n\\t\", \"suffix\": \"\\n\\treturn my_list\", \"canonical_solution\": \"my_list = [value for value in my_list if value != [None]]\\n\\t\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([1, 2, [None], 3, 4, 5]) == [1, 2, 3, 4, 5]\\n\"], \"entry_point\": \"f_422461\", \"intent\": \"\\u0423\\u0434\\u0430\\u043b\\u0438\\u0442\\u044c \\u043f\\u0443\\u0441\\u0442\\u044b\\u0435 \\u044d\\u043b\\u0435\\u043c\\u0435\\u043d\\u0442\\u044b \\u0438\\u0437 \\u0441\\u043f\\u0438\\u0441\\u043a\\u0430 `my_list`\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, random, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nCATEGORIES = ['Electronics', 'Fashion', 'Groceries', 'Books', 'Automotive']\nSALES = [randint(1000, 5000) for _ in range(5)]\nPRODUCTS = ['Product ' + str(i) for i in range(1, 21)]\n\ndef generate_sales_report():\n    \"\"\"\n    Generate a report of sales for a list of products across various categories.\n\n    Returns:\n    DataFrame: A pandas DataFrame with sales for the products.\n\n    Requirements:\n    - pandas\n    - numpy\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> report = generate_sales_report()\n    >>> print(report)\n    >>> report['Sales'].groupby(report['Category']).sum().plot(kind='bar')\n    \"\"\"\n    report_data = []\n\n    for product in PRODUCTS:\n        category = CATEGORIES[randint(0, len(CATEGORIES)-1)]\n        sales = SALES[randint(0, len(SALES)-1)]\n        report_data.append([product, category, sales])\n\n    report_df = pd.DataFrame(report_data, columns=['Product', 'Category', 'Sales'])\n\n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nnumpy, matplotlib, os, random\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nfrom random import randint\n\n# Constants\nFILES = ['file' + str(i) for i in range(1, 11)]\nSIZES = [randint(1000, 5000) for _ in range(10)]\n\ndef plot_file_sizes():\n    \"\"\"\n    Generate a bar plot of file sizes for a list of files.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - os\n    - random\n\n    Example:\n    >>> plot_file_sizes()\n    \"\"\"\n    sizes = []\n\n    for file in FILES:\n        size = os.path.getsize(file)\n        sizes.append(size)\n\n    y_pos = np.arange(len(FILES))\n    plt.bar(y_pos, sizes, align='center', alpha=0.5)\n    plt.xticks(y_pos, FILES)\n    plt.ylabel('Size')\n    plt.title('File sizes')\n\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \npandas, numpy, random, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nSTUDENTS = ['Student ' + str(i) for i in range(1, 21)]\nGRADES = [randint(0, 100) for _ in range(20)]\n\ndef generate_grade_report():\n    \"\"\"\n    Generate a report of grades for a list of students.\n\n    Returns:\n    DataFrame: A pandas DataFrame with grades for the students.\n\n    Requirements:\n    - pandas\n    - numpy\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> report = generate_grade_report()\n    >>> print(report)\n    >>> report['Grade'].hist(bins=10)\n    \"\"\"\n    report_data = []\n\n    for student in STUDENTS:\n        grade = GRADES[randint(0, len(GRADES)-1)]\n        report_data.append([student, grade])\n\n    report_df = pd.DataFrame(report_data, columns=['Student', 'Grade'])\n\n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, os, matplotlib.pyplot, numpy\n```python\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nFILES = ['file' + str(i) for i in range(1, 11)]\nSIZES = [np.random.randint(100, 10000) for _ in range(10)]\nFILES_SIZES = pd.DataFrame({'File': FILES, 'Size': SIZES})\n\ndef plot_files_sizes(df):\n    \"\"\"\n    Plot a bar chart of files sizes.\n\n    Parameters:\n    df (DataFrame): A pandas DataFrame with columns 'File' and 'Size'.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - os\n    - matplotlib.pyplot\n    - numpy\n\n    Example:\n    >>> plot_files_sizes(FILES_SIZES)\n    \"\"\"\n    df = df.sort_values('Size')\n    plt.bar(df['File'], df['Size'])\n    plt.xlabel('Files')\n    plt.ylabel('Size')\n    plt.title('Files sizes')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, random, matplotlib.pyplot\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nPRODUCTS = ['Product ' + str(i) for i in range(1, 21)]\nPRICES = [randint(1, 100) for _ in range(20)]\nQUANTITIES = [randint(1, 10) for _ in range(20)]\n\ndef generate_sales_report():\n    \"\"\"\n    Generate a report of sales for a list of products.\n\n    Returns:\n    DataFrame: A pandas DataFrame with sales for the products.\n\n    Requirements:\n    - pandas\n    - numpy\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> report = generate_sales_report()\n    >>> print(report)\n    >>> report['Revenue'].groupby(report['Product']).sum().plot(kind='bar')\n    \"\"\"\n    report_data = []\n\n    for product in PRODUCTS:\n        price = PRICES[randint(0, len(PRICES)-1)]\n        quantity = QUANTITIES[randint(0, len(QUANTITIES)-1)]\n        revenue = price * quantity\n        report_data.append([product, price, quantity, revenue])\n\n    report_df = pd.DataFrame(report_data, columns=['Product', 'Price', 'Quantity', 'Revenue'])\n\n    return report_df\n```"}
{"seed": "{\"task_id\": 533108, \"prompt\": \"def f_533108(my_list):\\n\\t\", \"suffix\": \"\\n\\treturn list_value_counts\", \"canonical_solution\": \"list_value_counts = Counter(my_list)\", \"test_start\": \"\\nfrom collections import Counter\\n\\ndef check(candidate):\", \"test\": [\"\\n    my_list = [10, 10, 23, 10, 123, 66, 78, 123] \\n    list_value_counts = {10: 3, 23: 1, 123: 2, 66: 1, 78: 1}\\n\\n    assert candidate(my_list) == list_value_counts\\n\"], \"entry_point\": \"f_533108\", \"intent\": \"\\u041f\\u043e\\u0441\\u0447\\u0438\\u0442\\u0430\\u0442\\u044c \\u044d\\u043b\\u0435\\u043c\\u0435\\u043d\\u0442\\u044b \\u0432 \\u0441\\u043f\\u0438\\u0441\\u043a\\u0435 `my_list` \\u0432 `list_value_counts`\", \"library\": [\"collections\"]}", "generation": "Scenario 1:\ncollections, numpy, matplotlib\n```python\nfrom collections import Counter\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nVALUES = [10, 10, 23, 10, 123, 66, 78, 123]\n\ndef plot_histogram(values):\n    \"\"\"\n    Plot a histogram of value counts in the given list using matplotlib.\n\n    Parameters:\n    values (list): The list of values.\n\n    Returns:\n    None.\n\n    Requirements:\n    - collections\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_histogram(VALUES)\n    \"\"\"\n    value_counts = Counter(values)\n    labels, counts = zip(*value_counts.items())\n    indexes = np.arange(len(labels))\n    plt.bar(indexes, counts, 1)\n    plt.xticks(indexes, labels)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncollections, matplotlib, pandas\n```python\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants\nVALUES = [10, 10, 23, 10, 123, 66, 78, 123]\n\ndef plot_pie_chart(values):\n    \"\"\"\n    Plot a pie chart of value counts in the given list using matplotlib.\n\n    Parameters:\n    values (list): The list of values.\n\n    Returns:\n    None.\n\n    Requirements:\n    - collections\n    - matplotlib.pyplot\n    - pandas\n\n    Example:\n    >>> plot_pie_chart(VALUES)\n    \"\"\"\n    value_counts = Counter(values)\n    df = pd.DataFrame.from_dict(value_counts, orient='index')\n    df.plot(kind='pie', y=0, legend=False)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ncollections, numpy, scipy\n```python\nfrom collections import Counter\nimport numpy as np\nfrom scipy import stats\n\n# Constants\nVALUES = [10, 10, 23, 10, 123, 66, 78, 123]\n\ndef find_mode(values):\n    \"\"\"\n    Find the mode of values in the given list using scipy.\n\n    Parameters:\n    values (list): The list of values.\n\n    Returns:\n    int: The mode of the values.\n\n    Requirements:\n    - collections\n    - numpy\n    - scipy\n\n    Example:\n    >>> find_mode(VALUES)\n    \"\"\"\n    value_counts = Counter(values)\n    mode = stats.mode(np.array(list(value_counts.values())))[0][0]\n    return mode\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ncollections, pandas, json, csv\n```python\nfrom collections import Counter\nimport pandas as pd\nimport json\nimport csv\n\n# Constants\nVALUES = [10, 10, 23, 10, 123, 66, 78, 123]\n\ndef save_value_counts(values, file_format):\n    \"\"\"\n    Save the value counts of the given list in the specified file format (csv or json).\n\n    Parameters:\n    values (list): The list of values.\n    file_format (str): The file format to save the value counts. It can be 'csv' or 'json'.\n\n    Returns:\n    None.\n\n    Requirements:\n    - collections\n    - pandas\n    - json\n    - csv\n\n    Example:\n    >>> save_value_counts(VALUES, 'json')\n    >>> save_value_counts(VALUES, 'csv')\n    \"\"\"\n    value_counts = Counter(values)\n    df = pd.DataFrame.from_dict(value_counts, orient='index', columns=['Count'])\n    if file_format == 'json':\n        df.to_json('value_counts.json')\n    elif file_format == 'csv':\n        df.to_csv('value_counts.csv')\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\ncollections, numpy, matplotlib, seaborn\n```python\nfrom collections import Counter\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nVALUES = [10, 10, 23, 10, 123, 66, 78, 123]\n\ndef plot_heatmap(values):\n    \"\"\"\n    Plot a heatmap of value counts in the given list using seaborn.\n\n    Parameters:\n    values (list): The list of values.\n\n    Returns:\n    None.\n\n    Requirements:\n    - collections\n    - numpy\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> plot_heatmap(VALUES)\n    \"\"\"\n    value_counts = Counter(values)\n    labels, counts = zip(*value_counts.items())\n    matrix = np.array(counts).reshape(int(np.sqrt(len(counts))), -1)\n    sns.heatmap(matrix, annot=True, fmt=\"d\")\n    plt.show()\n```"}
{"seed": "{\"task_id\": 533108, \"prompt\": \"def f_533108(my_list):\\n\\t\", \"suffix\": \"\\n\\treturn list_value_counts\", \"canonical_solution\": \"list_value_counts = {i: my_list.count(i) for i in my_list}\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    my_list = [10, 10, 23, 10, 123, 66, 78, 123] \\n    list_value_counts = {10: 3, 23: 1, 123: 2, 66: 1, 78: 1}\\n\\n    assert candidate(my_list) == list_value_counts\\n\"], \"entry_point\": \"f_533108\", \"intent\": \"\\u041f\\u043e\\u0441\\u0447\\u0438\\u0442\\u0430\\u0442\\u044c \\u044d\\u043b\\u0435\\u043c\\u0435\\u043d\\u0442\\u044b \\u0432 \\u0441\\u043f\\u0438\\u0441\\u043a\\u0435 `my_list` \\u0432 `list_value_counts`\", \"library\": []}", "generation": "Scenario 1:\npandas, random, collections\n```python\nimport pandas as pd\nfrom random import randint\nfrom collections import Counter\n\n# Constants\nANIMALS = ['Cat', 'Dog', 'Bird', 'Fish', 'Rabbit']\n\ndef generate_animal_population(n):\n    \"\"\"\n    Generate a random animal population and count the number of each animal.\n    \n    Parameters:\n    n (int): The size of the population.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the count of each animal.\n    \n    Requirements:\n    - pandas\n    - random\n    - collections.Counter\n    \n    Example:\n    >>> population = generate_animal_population(100)\n    >>> print(population)\n    \"\"\"\n    population = [ANIMALS[randint(0, len(ANIMALS)-1)] for _ in range(n)]\n    population_counts = dict(Counter(population))\n    population_df = pd.DataFrame(list(population_counts.items()), columns=['Animal', 'Count'])\n\n    return population_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, matplotlib, collections\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n\ndef plot_histogram(my_list):\n    \"\"\"\n    Plot a histogram of the elements in my_list.\n    \n    Parameters:\n    my_list (list): The list of elements.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - collections.Counter\n    \n    Example:\n    >>> my_list = [10, 10, 23, 10, 123, 66, 78, 123]\n    >>> plot_histogram(my_list)\n    \"\"\"\n    element_counts = dict(Counter(my_list))\n    plt.bar(element_counts.keys(), element_counts.values())\n    plt.show()\n\n    return None\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, scipy.stats, matplotlib\n```python\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef plot_normal_distribution(mu, sigma):\n    \"\"\"\n    Plot a normal distribution with mean mu and standard deviation sigma.\n    \n    Parameters:\n    mu (float): The mean of the distribution.\n    sigma (float): The standard deviation of the distribution.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n    \n    Example:\n    >>> plot_normal_distribution(0, 1)\n    \"\"\"\n    x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n    plt.plot(x, stats.norm.pdf(x, mu, sigma))\n    plt.show()\n\n    return None\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef generate_and_plot_dataframe(n):\n    \"\"\"\n    Generate a DataFrame with n rows and 5 columns, and plot the data.\n    \n    Parameters:\n    n (int): The number of rows in the DataFrame.\n    \n    Returns:\n    DataFrame: The generated pandas DataFrame.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = generate_and_plot_dataframe(100)\n    >>> print(df)\n    \"\"\"\n    df = pd.DataFrame(np.random.rand(n, 5), columns=COLUMNS)\n    df.plot()\n    plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\ncollections, matplotlib, random\n```python\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nfrom random import choices\n\n# Constants\nFRUITS = ['Apple', 'Banana', 'Cherry', 'Date', 'Elderberry']\n\ndef fruit_basket(n):\n    \"\"\"\n    Randomly select n fruits, count the number of each fruit, and plot the counts.\n    \n    Parameters:\n    n (int): The number of fruits to select.\n    \n    Returns:\n    Counter: The counts of each fruit.\n    \n    Requirements:\n    - collections.Counter\n    - matplotlib.pyplot\n    - random.choices\n    \n    Example:\n    >>> basket = fruit_basket(100)\n    >>> print(basket)\n    \"\"\"\n    basket = choices(FRUITS, k=n)\n    fruit_counts = Counter(basket)\n    plt.bar(fruit_counts.keys(), fruit_counts.values())\n    plt.show()\n\n    return fruit_counts\n```"}
{"seed": "{\"task_id\": 594690, \"prompt\": \"def f_594690(s, shift):\\n\\t\", \"suffix\": \"\\n\\treturn new_s\", \"canonical_solution\": \"alpha = ' abcdefghijklmnopqrstuvwxyz'\\n\\n\\tnew_s = ''\\n\\n\\tfor c in s:\\n\\n\\t\\tnew_s += alpha[(alpha.index(c) + shift) % len(alpha)]\\n\\t\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('hello', 3) == 'khoor'\\n\", \"\\n    assert candidate('world', 3) == 'zruog'\\n\"], \"entry_point\": \"f_594690\", \"intent\": \"\\u0417\\u0430\\u0448\\u0438\\u0444\\u0440\\u043e\\u0432\\u0430\\u0442\\u044c \\u0441\\u0442\\u0440\\u043e\\u043a\\u0443 `s` \\u0432 \\u0441\\u0442\\u0440\\u043e\\u043a\\u0443 `new_s` \\u0448\\u0438\\u0444\\u0440\\u043e\\u043c \\u0426\\u0435\\u0437\\u0430\\u0440\\u044f \\u0441\\u043e \\u0441\\u0434\\u0432\\u0438\\u0433\\u043e\\u043c `shift`\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, random, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import randint, choice\nimport matplotlib.pyplot as plt\n\n# Constants\nALPHABET = ' abcdefghijklmnopqrstuvwxyz'\nNAMES = ['John', 'Mary', 'Bob', 'Alice', 'Tom']\n\ndef generate_encrypted_messages(shift):\n    \"\"\"\n    Generate encrypted messages using the Caesar cipher with a given shift for a list of names.\n    \n    Parameters:\n    shift (int): The Caesar shift.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the original messages and encrypted messages.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> report = generate_encrypted_messages(3)\n    >>> print(report)\n    >>> report['Message Length'].hist(bins=30)\n    \"\"\"\n    report_data = []\n\n    for name in NAMES:\n        message = ''.join([choice(ALPHABET) for _ in range(randint(10, 100))])\n        encrypted_message = ''.join([ALPHABET[(ALPHABET.index(c) + shift) % len(ALPHABET)] for c in message])\n        report_data.append([name, message, encrypted_message, len(message)])\n\n    report_df = pd.DataFrame(report_data, columns=['Name', 'Original Message', 'Encrypted Message', 'Message Length'])\n\n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, random, matplotlib\n```python\nimport numpy as np\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nALPHABET = ' abcdefghijklmnopqrstuvwxyz'\nMESSAGES = ['hello', 'world', 'python', 'programming', 'is', 'fun']\n\ndef plot_caesar_shifts():\n    \"\"\"\n    Plot the distribution of Caesar shifts that map the original messages to the encrypted ones.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - numpy\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> plot_caesar_shifts()\n    \"\"\"\n    shifts = []\n\n    for message in MESSAGES:\n        shift = randint(1, len(ALPHABET) - 1)\n        encrypted_message = ''.join([ALPHABET[(ALPHABET.index(c) + shift) % len(ALPHABET)] for c in message])\n        shifts.append(shift)\n\n    plt.hist(shifts, bins=np.arange(1, len(ALPHABET)), alpha=0.7, color='g')\n    plt.title('Distribution of Caesar Shifts')\n    plt.xlabel('Shift')\n    plt.ylabel('Frequency')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \npandas, numpy, random, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nALPHABET = ' abcdefghijklmnopqrstuvwxyz'\nWORDS = ['hello', 'world', 'python', 'programming', 'is', 'fun']\n\ndef generate_word_statistics(shift):\n    \"\"\"\n    Generate statistics (mean, median, mode) of the lengths of the original words and \n    the encrypted words using the Caesar cipher with a given shift.\n    \n    Parameters:\n    shift (int): The Caesar shift.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the statistics.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> stats = generate_word_statistics(3)\n    >>> print(stats)\n    >>> stats.plot(kind='bar')\n    \"\"\"\n    stats_data = []\n\n    for word in WORDS:\n        original_length = len(word)\n        encrypted_word = ''.join([ALPHABET[(ALPHABET.index(c) + shift) % len(ALPHABET)] for c in word])\n        encrypted_length = len(encrypted_word)\n        stats_data.append([original_length, encrypted_length])\n\n    stats_df = pd.DataFrame(stats_data, columns=['Original Length', 'Encrypted Length'])\n\n    return stats_df.describe().loc[['mean', '50%', 'mode']]\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \ncollections, random, matplotlib\n```python\nimport collections\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nALPHABET = ' abcdefghijklmnopqrstuvwxyz'\nMESSAGES = ['hello', 'world', 'python', 'programming', 'is', 'fun']\n\ndef plot_letter_frequency(shift):\n    \"\"\"\n    Plot the letter frequency in the original messages and the encrypted messages \n    using the Caesar cipher with a given shift.\n    \n    Parameters:\n    shift (int): The Caesar shift.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - collections\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> plot_letter_frequency(3)\n    \"\"\"\n    original_counter = collections.Counter()\n    encrypted_counter = collections.Counter()\n\n    for message in MESSAGES:\n        original_counter.update(message)\n        encrypted_message = ''.join([ALPHABET[(ALPHABET.index(c) + shift) % len(ALPHABET)] for c in message])\n        encrypted_counter.update(encrypted_message)\n\n    plt.figure(figsize=(12, 6))\n\n    plt.subplot(1, 2, 1)\n    plt.bar(original_counter.keys(), original_counter.values(), color='b')\n    plt.title('Original Messages')\n    plt.xlabel('Letter')\n    plt.ylabel('Frequency')\n\n    plt.subplot(1, 2, 2)\n    plt.bar(encrypted_counter.keys(), encrypted_counter.values(), color='r')\n    plt.title('Encrypted Messages')\n    plt.xlabel('Letter')\n    plt.ylabel('Frequency')\n\n    plt.tight_layout()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, random, matplotlib\n```python\nimport numpy as np\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nALPHABET = ' abcdefghijklmnopqrstuvwxyz'\nMESSAGES = ['hello', 'world', 'python', 'programming', 'is', 'fun']\n\ndef calculate_shift_correlation():\n    \"\"\"\n    Calculate the correlation between the lengths of the original messages and \n    the Caesar shifts that map the original messages to the encrypted ones.\n    \n    Returns:\n    float: The correlation coefficient.\n    \n    Requirements:\n    - numpy\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> correlation = calculate_shift_correlation()\n    >>> print(correlation)\n    \"\"\"\n    lengths = []\n    shifts = []\n\n    for message in MESSAGES:\n        lengths.append(len(message))\n        shift = randint(1, len(ALPHABET) - 1)\n        shifts.append(shift)\n\n    correlation = np.corrcoef(lengths, shifts)[0, 1]\n\n    plt.scatter(lengths, shifts, alpha=0.7, color='g')\n    plt.title('Correlation between Message Length and Caesar Shift')\n    plt.xlabel('Message Length')\n    plt.ylabel('Caesar Shift')\n    plt.show()\n\n    return correlation\n```"}
{"seed": "{\"task_id\": 424441, \"prompt\": \"def f_424441(s):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"s == s[::-1]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('racecar') == True\\n\", \"\\n    assert candidate('world') == False\\n\"], \"entry_point\": \"f_424441\", \"intent\": \"\\u041f\\u0440\\u043e\\u0432\\u0435\\u0440\\u0438\\u0442\\u044c \\u044f\\u0432\\u043b\\u044f\\u0435\\u0442\\u0441\\u044f \\u043b\\u0438 \\u0441\\u0442\\u0440\\u043e\\u043a\\u0430 `s` \\u043f\\u0430\\u043b\\u0438\\u043d\\u0434\\u0440\\u043e\\u043c\\u043e\\u043c\", \"library\": []}", "generation": "Scenario 1:\nnumpy, random, re, difflib\n```python\nimport numpy as np\nimport random\nimport re\nfrom difflib import SequenceMatcher\n\n# Constants\nMIN_WORDS = 3\nMAX_WORDS = 10\n\n# Sample data\nWORDS_POOL = ['apple', 'banana', 'racecar', 'world', 'level', 'madam', 'radar', 'rotor']\n\ndef generate_palindrome_sentence():\n    \"\"\"\n    Generate a sentence that is a palindrome by randomly choosing the words from a pool.\n    The sentence should have a random length between a minimum and maximum limit. \n    Ignore spacing, case, and punctuation while checking for palindrome.\n\n    Returns:\n    str: The generated palindrome sentence.\n\n    Requirements:\n    - numpy\n    - random\n    - re\n    - difflib\n\n    Example:\n    >>> sentence = generate_palindrome_sentence()\n    >>> print(sentence)\n    \"\"\"\n    sentence_length = np.random.randint(MIN_WORDS, MAX_WORDS+1)\n    sentence = []\n    for _ in range(sentence_length):\n        word = random.choice(WORDS_POOL)\n        sentence.append(word)\n\n    # Ensure the sentence is a palindrome\n    if sentence != sentence[::-1]:\n        # Make the sentence a palindrome by mirroring the first half\n        sentence = sentence[:len(sentence)//2] + sentence[len(sentence)//2:][::-1]\n\n    sentence_str = ' '.join(sentence)\n\n    return sentence_str\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnltk, re, collections, matplotlib\n```python\nimport nltk\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\n# Constants\nSTOPWORDS = nltk.corpus.stopwords.words('english')\n\ndef analyze_text(text):\n    \"\"\"\n    Analyze a given text and perform the following operations:\n    - Tokenize the text.\n    - Remove stopwords.\n    - Count the frequency of the words.\n    - Plot the top 10 most frequent words.\n\n    Parameters:\n    text (str): The text to analyze.\n\n    Returns:\n    Counter: The Counter object with word frequencies.\n\n    Requirements:\n    - nltk\n    - re\n    - collections\n    - matplotlib.pyplot\n\n    Example:\n    >>> text = \"Apple and banana are fruits. An apple a day keeps the doctor away.\"\n    >>> word_freq = analyze_text(text)\n    >>> print(word_freq.most_common(10))\n    >>> plt.bar(*zip(*word_freq.most_common(10)))\n    \"\"\"\n    # Tokenize\n    tokens = nltk.word_tokenize(text)\n\n    # Remove stopwords\n    words = [word for word in tokens if word not in STOPWORDS]\n\n    # Count word frequencies\n    word_freq = Counter(words)\n\n    # Plot\n    plt.bar(*zip(*word_freq.most_common(10)))\n    plt.show()\n\n    return word_freq\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, random, difflib\n```python\nimport numpy as np\nimport random\nfrom difflib import SequenceMatcher\n\n# Constants\nMIN_LENGTH = 5\nMAX_LENGTH = 10\n\n# Sample data\nLETTERS = 'abcdefghijklmnopqrstuvwxyz'\n\ndef generate_similar_string(s):\n    \"\"\"\n    Generate a string that is similar to a given string `s` with a random length between a minimum and maximum limit.\n    The similarity is calculated by SequenceMatcher from difflib.\n\n    Parameters:\n    s (str): The string to which the generated string should be similar.\n\n    Returns:\n    str: The generated similar string.\n\n    Requirements:\n    - numpy\n    - random\n    - difflib\n\n    Example:\n    >>> s = 'apple'\n    >>> similar_s = generate_similar_string(s)\n    >>> print(similar_s)\n    \"\"\"\n    string_length = np.random.randint(MIN_LENGTH, MAX_LENGTH+1)\n    similar_s = ''.join(random.choice(LETTERS) for _ in range(string_length))\n\n    # Ensure the string is similar to the given string\n    similarity = SequenceMatcher(None, s, similar_s).ratio()\n    if similarity < 0.5:\n        # Make the string more similar by replacing some characters\n        for i in range(len(similar_s)):\n            if similar_s[i] not in s:\n                similar_s = similar_s[:i] + s[i%len(s)] + similar_s[i+1:]\n\n    return similar_s\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, random, difflib, matplotlib\n```python\nimport numpy as np\nimport random\nfrom difflib import SequenceMatcher\nimport matplotlib.pyplot as plt\n\n# Constants\nMIN_LENGTH = 5\nMAX_LENGTH = 10\nSIMILARITY_THRESHOLD = 0.5\n\n# Sample data\nLETTERS = 'abcdefghijklmnopqrstuvwxyz'\n\ndef analyze_string_similarity(s_list):\n    \"\"\"\n    Analyze the similarity of a list of strings `s_list` by calculating the average similarity score of \n    each string to all other strings in the list. The similarity is calculated by SequenceMatcher from difflib.\n    Plot the similarity scores.\n\n    Parameters:\n    s_list (list): The list of strings to analyze.\n\n    Returns:\n    list: The list of average similarity scores.\n\n    Requirements:\n    - numpy\n    - random\n    - difflib\n    - matplotlib.pyplot\n\n    Example:\n    >>> s_list = ['apple', 'apples', 'ape', 'app', 'april']\n    >>> avg_scores = analyze_string_similarity(s_list)\n    >>> print(avg_scores)\n    >>> plt.bar(s_list, avg_scores)\n    \"\"\"\n    avg_scores = []\n    for s in s_list:\n        scores = []\n        for other_s in s_list:\n            if s != other_s:\n                similarity = SequenceMatcher(None, s, other_s).ratio()\n                scores.append(similarity)\n        avg_score = np.mean(scores)\n        avg_scores.append(avg_score)\n\n    # Plot\n    plt.bar(s_list, avg_scores)\n    plt.show()\n\n    return avg_scores\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, random, re, difflib\n```python\nimport numpy as np\nimport random\nimport re\nfrom difflib import SequenceMatcher\n\n# Constants\nMIN_LENGTH = 5\nMAX_LENGTH = 10\n\n# Sample data\nLETTERS = 'abcdefghijklmnopqrstuvwxyz'\n\ndef generate_string_and_check_similarity(s):\n    \"\"\"\n    Generate a string with a random length between a minimum and maximum limit. \n    Check if the generated string is similar to a given string `s`.\n    The similarity is calculated by SequenceMatcher from difflib.\n\n    Parameters:\n    s (str): The string to which the generated string should be similar.\n\n    Returns:\n    tuple: The generated string and a boolean indicating if it's similar to `s`.\n\n    Requirements:\n    - numpy\n    - random\n    - re\n    - difflib\n\n    Example:\n    >>> s = 'apple'\n    >>> generated_s, is_similar = generate_string_and_check_similarity(s)\n    >>> print(generated_s, is_similar)\n    \"\"\"\n    string_length = np.random.randint(MIN_LENGTH, MAX_LENGTH+1)\n    generated_s = ''.join(random.choice(LETTERS) for _ in range(string_length))\n\n    # Check similarity\n    similarity = SequenceMatcher(None, s, generated_s).ratio()\n    is_similar = similarity >= 0.5\n\n    return generated_s, is_similar\n```\n"}
{"seed": "{\"task_id\": 424441, \"prompt\": \"def f_424441(s):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"s ==''.join([c for c in reversed(s)])\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('racecar') == True\\n\", \"\\n    assert candidate('world') == False\\n\"], \"entry_point\": \"f_424441\", \"intent\": \"\\u041f\\u0440\\u043e\\u0432\\u0435\\u0440\\u0438\\u0442\\u044c \\u044f\\u0432\\u043b\\u044f\\u0435\\u0442\\u0441\\u044f \\u043b\\u0438 \\u0441\\u0442\\u0440\\u043e\\u043a\\u0430 `s` \\u043f\\u0430\\u043b\\u0438\\u043d\\u0434\\u0440\\u043e\\u043c\\u043e\\u043c\", \"library\": []}", "generation": "Scenario 1: \nnumpy, pandas, matplotlib.pyplot, random\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import choice\n\n# Constants\nLETTERS = list('abcdefghijklmnopqrstuvwxyz')\n\ndef generate_palin_freq_distribution(n_words=5000, word_length=5):\n    \"\"\"\n    Generate a frequency distribution of the number of palindromes in a list of random words of a given length.\n\n    Parameters:\n    n_words (int): The number of random words to generate.\n    word_length (int): The length of each word.\n\n    Returns:\n    Series: A pandas Series representing the frequency distribution.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - random.choice\n\n    Example:\n    >>> dist = generate_palin_freq_distribution(10000, 7)\n    >>> print(dist)\n    >>> dist.plot(kind='bar')\n    \"\"\"\n    words = [''.join([choice(LETTERS) for _ in range(word_length)]) for _ in range(n_words)]\n    palins = [word for word in words if word == ''.join([c for c in reversed(word)])]\n    freq_dist = pd.Series(palins).value_counts()\n    \n    return freq_dist\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, pandas, random, string\n```python\nimport numpy as np\nimport pandas as pd\nfrom random import choice, randint\nimport string\n\n# Constants\nLETTERS = list(string.ascii_lowercase)\n\ndef generate_dataframe_with_palindromes(n_rows=5000, n_cols=5, word_length=5):\n    \"\"\"\n    Generate a DataFrame with a given number of rows and columns, \n    filled with random words of a given length, and replace a random subset of these words with palindromes.\n\n    Parameters:\n    n_rows (int): The number of rows in the DataFrame.\n    n_cols (int): The number of columns in the DataFrame.\n    word_length (int): The length of each word.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the generated data.\n\n    Requirements:\n    - numpy\n    - pandas\n    - random.choice\n    - random.randint\n    - string.ascii_lowercase\n\n    Example:\n    >>> df = generate_dataframe_with_palindromes(10000, 10, 7)\n    >>> print(df)\n    \"\"\"\n    df = pd.DataFrame(np.random.choice(LETTERS, size=(n_rows, n_cols), replace=True).tolist())\n    df = df.applymap(lambda x: ''.join(x))\n\n    for i in range(n_rows):\n        for j in range(n_cols):\n            if randint(0, 1):\n                df.iloc[i, j] = df.iloc[i, j][::-1]\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, pandas, matplotlib.pyplot, random\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import randint, choice\n\n# Constants\nLETTERS = list('abcdefghijklmnopqrstuvwxyz')\n\ndef generate_and_plot_palin_freq_distributions(n_samples=5000, word_length=5, bins=10):\n    \"\"\"\n    Generate a number of samples of random words of a given length, calculate the frequency of palindromes in each sample,\n    and plot the resulting frequency distributions.\n\n    Parameters:\n    n_samples (int): The number of samples to generate.\n    word_length (int): The length of each word.\n    bins (int): The number of bins for the histogram.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - random.randint\n    - random.choice\n\n    Example:\n    >>> generate_and_plot_palin_freq_distributions(10000, 7, 20)\n    \"\"\"\n    palin_counts = []\n\n    for _ in range(n_samples):\n        words = [''.join([choice(LETTERS) for _ in range(word_length)]) for _ in range(randint(0, n_samples))]\n        palins = [word for word in words if word == ''.join([c for c in reversed(word)])]\n        palin_counts.append(len(palins))\n\n    plt.hist(palin_counts, bins=bins)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, pandas, random, string\n```python\nimport numpy as np\nimport pandas as pd\nfrom random import choice, randint\nimport string\n\n# Constants\nLETTERS = list(string.ascii_lowercase)\n\ndef generate_dataframe_with_random_palindromes(n_rows=5000, n_cols=5, word_length=5):\n    \"\"\"\n    Generate a DataFrame with a given number of rows and columns, \n    filled with random words of a given length, and replace a random number of these words with palindromes.\n\n    Parameters:\n    n_rows (int): The number of rows in the DataFrame.\n    n_cols (int): The number of columns in the DataFrame.\n    word_length (int): The length of each word.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the generated data.\n\n    Requirements:\n    - numpy\n    - pandas\n    - random.choice\n    - random.randint\n    - string.ascii_lowercase\n\n    Example:\n    >>> df = generate_dataframe_with_random_palindromes(10000, 10, 7)\n    >>> print(df)\n    \"\"\"\n    df = pd.DataFrame(np.random.choice(LETTERS, size=(n_rows, n_cols), replace=True).tolist())\n    df = df.applymap(lambda x: ''.join(x))\n\n    for _ in range(randint(0, n_rows*n_cols)):\n        i = randint(0, n_rows-1)\n        j = randint(0, n_cols-1)\n        df.iloc[i, j] = df.iloc[i, j][::-1]\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, pandas, matplotlib.pyplot, random\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import choice\n\n# Constants\nLETTERS = list('abcdefghijklmnopqrstuvwxyz')\n\ndef generate_and_plot_palin_freq_distribution(n_words=5000, word_length=5, bins=10):\n    \"\"\"\n    Generate a number of random words of a given length, calculate the frequency of palindromes among these words,\n    and plot the resulting frequency distribution.\n\n    Parameters:\n    n_words (int): The number of words to generate.\n    word_length (int): The length of each word.\n    bins (int): The number of bins for the histogram.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - random.choice\n\n    Example:\n    >>> generate_and_plot_palin_freq_distribution(10000, 7, 20)\n    \"\"\"\n    words = [''.join([choice(LETTERS) for _ in range(word_length)]) for _ in range(n_words)]\n    palins = [word for word in words if word == ''.join([c for c in reversed(word)])]\n    freq_dist = pd.Series(palins).value_counts()\n\n    plt.hist(freq_dist.values, bins=bins)\n    plt.show()\n```\n"}
{"seed": "{\"task_id\": 630116, \"prompt\": \"def f_630116(my_f, x):\\n\\t\", \"suffix\": \"\\n\\treturn d\", \"canonical_solution\": \"d = derivative(my_f, x)\", \"test_start\": \"\\nfrom scipy.misc import derivative\\n\\ndef check(candidate):\", \"test\": [\"\\n    def f(x):\\n        return x**3 + x**2\\n        \\n    assert candidate(f, 1.0) == 6.0\\n\"], \"entry_point\": \"f_630116\", \"intent\": \"\\u041d\\u0430\\u0439\\u0442\\u0438 \\u043f\\u0440\\u043e\\u0438\\u0437\\u0432\\u043e\\u0434\\u043d\\u0443\\u044e `d` \\u0444\\u0443\\u043d\\u043a\\u0446\\u0438\\u0438 `my_f`\\u0432 \\u0442\\u043e\\u0447\\u043a\\u0435 `x`\", \"library\": [\"scipy\"]}", "generation": "Scenario 1:\nnumpy, sympy, matplotlib, scipy\n```python\nimport numpy as np\nfrom sympy import symbols, diff\nimport matplotlib.pyplot as plt\nfrom scipy.misc import derivative\n\n# Constants\nX_RANGE = np.linspace(-10, 10, 400)\n\nx = symbols('x')\n\ndef plot_derivative(my_f, x_val):\n    \"\"\"\n    Calculate derivative of the given function at a point and plot the function \n    and its derivative over a range.\n\n    Parameters:\n    my_f (function): The function to differentiate.\n    x_val (float): The point at which to evaluate the derivative.\n\n    Returns:\n    float: The derivative at the given point.\n\n    Requirements:\n    - numpy\n    - sympy\n    - matplotlib.pyplot\n    - scipy.misc\n\n    Example:\n    >>> def f(x): return x**3 + x**2\n    >>> plot_derivative(f, 1.0)\n    \"\"\"\n    d = derivative(my_f, x_val)\n\n    y = my_f(X_RANGE)\n    dy = derivative(my_f, X_RANGE)\n\n    plt.figure(figsize=(12, 6))\n\n    plt.subplot(1, 2, 1)\n    plt.plot(X_RANGE, y, label=f'f(x) = {my_f(x)}')\n    plt.title('Function')\n\n    plt.subplot(1, 2, 2)\n    plt.plot(X_RANGE, dy, label=f\"f'(x) = {diff(my_f(x))}\")\n    plt.title('Derivative')\n\n    plt.legend()\n    plt.show()\n\n    return d\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nscipy, matplotlib, numpy, math\n```python\nfrom scipy.misc import derivative\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport math\n\n# Constants\nX_RANGE = np.linspace(-2 * np.pi, 2 * np.pi, 1000)\n\ndef plot_sin_cos_derivative(x_val):\n    \"\"\"\n    Calculate the derivative of sin(x) and cos(x) functions at a point and \n    plot these functions and their derivatives over a range.\n\n    Parameters:\n    x_val (float): The point at which to evaluate the derivative.\n\n    Returns:\n    tuple: The derivative of sin(x) and cos(x) at the given point.\n\n    Requirements:\n    - scipy.misc\n    - matplotlib.pyplot\n    - numpy\n    - math\n\n    Example:\n    >>> plot_sin_cos_derivative(1.0)\n    \"\"\"\n    sin_derivative = derivative(math.sin, x_val)\n    cos_derivative = derivative(math.cos, x_val)\n\n    plt.figure(figsize=(12, 6))\n\n    plt.subplot(2, 2, 1)\n    plt.plot(X_RANGE, np.sin(X_RANGE))\n    plt.title('sin(x)')\n\n    plt.subplot(2, 2, 2)\n    plt.plot(X_RANGE, np.cos(X_RANGE))\n    plt.title('cos(x)')\n\n    plt.subplot(2, 2, 3)\n    plt.plot(X_RANGE, derivative(math.sin, X_RANGE))\n    plt.title(\"sin'(x)\")\n\n    plt.subplot(2, 2, 4)\n    plt.plot(X_RANGE, derivative(math.cos, X_RANGE))\n    plt.title(\"cos'(x)\")\n\n    plt.tight_layout()\n    plt.show()\n\n    return sin_derivative, cos_derivative\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nscipy, numpy, matplotlib\n```python\nfrom scipy.misc import derivative\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nX_RANGE = np.linspace(-10, 10, 100)\n\ndef plot_func_derivative(f):\n    \"\"\"\n    Calculate the derivative of a function and plot the function and its derivative.\n\n    Parameters:\n    f (function): The function to be differentiated.\n\n    Returns:\n    None\n\n    Requirements:\n    - scipy.misc\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> def my_func(x): return x**2 + 2*x + 1\n    >>> plot_func_derivative(my_func)\n    \"\"\"\n    y = f(X_RANGE)\n    dy = derivative(f, X_RANGE)\n\n    plt.figure(figsize=(10, 5))\n\n    plt.subplot(1, 2, 1)\n    plt.plot(X_RANGE, y, label='f(x)')\n    plt.title('Function')\n\n    plt.subplot(1, 2, 2)\n    plt.plot(X_RANGE, dy, label=\"f'(x)\")\n    plt.title('Derivative')\n\n    plt.legend()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nscipy, numpy, matplotlib\n```python\nfrom scipy.misc import derivative\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nX_RANGE = np.linspace(-10, 10, 100)\n\ndef plot_func_and_first_second_derivatives(f):\n    \"\"\"\n    Calculate the first and second derivatives of a function and plot the function, \n    first derivative and second derivative.\n\n    Parameters:\n    f (function): The function to be differentiated.\n\n    Returns:\n    None\n\n    Requirements:\n    - scipy.misc\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> def my_func(x): return x**3 + x**2 + x + 1\n    >>> plot_func_and_first_second_derivatives(my_func)\n    \"\"\"\n    y = f(X_RANGE)\n    dy = derivative(f, X_RANGE)\n    d2y = derivative(f, X_RANGE, n=2)\n\n    plt.figure(figsize=(15, 5))\n\n    plt.subplot(1, 3, 1)\n    plt.plot(X_RANGE, y, label='f(x)')\n    plt.title('Function')\n\n    plt.subplot(1, 3, 2)\n    plt.plot(X_RANGE, dy, label=\"f'(x)\")\n    plt.title('First derivative')\n\n    plt.subplot(1, 3, 3)\n    plt.plot(X_RANGE, d2y, label=\"f''(x)\")\n    plt.title('Second derivative')\n\n    plt.legend()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nscipy, sympy, numpy, matplotlib\n```python\nfrom scipy.misc import derivative\nfrom sympy import symbols, diff\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nX_RANGE = np.linspace(-10, 10, 100)\n\nx = symbols('x')\n\ndef plot_func_derivative_and_tangent_line(my_f, x_val):\n    \"\"\"\n    Calculate the derivative of a function at a point, plot the function, its \n    derivative, and the tangent line at the given point.\n\n    Parameters:\n    my_f (function): The function to differentiate.\n    x_val (float): The point at which to evaluate the derivative.\n\n    Returns:\n    float: The derivative at the given point.\n\n    Requirements:\n    - scipy.misc\n    - sympy\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> def f(x): return x**3 + x**2\n    >>> plot_func_derivative_and_tangent_line(f, 1.0)\n    \"\"\"\n    d = derivative(my_f, x_val)\n\n    y = my_f(X_RANGE)\n    dy = derivative(my_f, X_RANGE)\n    tangent_line = d * (X_RANGE - x_val) + my_f(x_val)\n\n    plt.figure(figsize=(15, 5))\n\n    plt.subplot(1, 3, 1)\n    plt.plot(X_RANGE, y, label=f'f(x) = {my_f(x)}')\n    plt.title('Function')\n\n    plt.subplot(1, 3, 2)\n    plt.plot(X_RANGE, dy, label=f\"f'(x) = {diff(my_f(x))}\")\n    plt.title('Derivative')\n\n    plt.subplot(1, 3, 3)\n    plt.plot(X_RANGE, y, label='f(x)')\n    plt.plot(X_RANGE, tangent_line, label='Tangent line')\n    plt.legend()\n    plt.title('Function and Tangent Line')\n\n    plt.show()\n\n    return d\n```"}
{"seed": "{\"task_id\": 577060, \"prompt\": \"def f_577060(s, n, new_char):\\n\\t\", \"suffix\": \"\\n\\treturn s\", \"canonical_solution\": \"s = s[:n] + new_char + s[n+1:]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(\\\"Hello, world!\\\", 0, \\\"y\\\") == \\\"yello, world!\\\"\\n\"], \"entry_point\": \"f_577060\", \"intent\": \"\\u0417\\u0430\\u043c\\u0435\\u043d\\u0438\\u0442\\u044c \\u0441\\u0438\\u043c\\u0432\\u043e\\u043b \\u0441 \\u0438\\u043d\\u0434\\u0435\\u043a\\u0441\\u043e\\u043c `n` \\u0432 \\u0441\\u0442\\u0440\\u043e\\u043a\\u0435 `s` \\u043d\\u0430 `new_char`\", \"library\": []}", "generation": "Scenario 1: \npandas, nltk, sklearn, re\n```python\nimport pandas as pd\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport re\n\n# Constants\nSTOPWORDS = set(nltk.corpus.stopwords.words('english'))\nLEMMATIZER = WordNetLemmatizer()\nVECTORIZER = CountVectorizer(stop_words=STOPWORDS)\n\ndef replace_and_analyze_text(data, column, index, word, new_word):\n    \"\"\"\n    Replace a word at a specific index in a column of a DataFrame with a new word \n    and perform a simple text analysis of the column after the replacement.\n    \n    Parameters:\n    data (DataFrame): The pandas DataFrame.\n    column (str): The column name.\n    index (int): The row index.\n    word (str): The word to be replaced.\n    new_word (str): The new word.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with word counts.\n    \n    Requirements:\n    - pandas\n    - nltk.corpus\n    - nltk.stem.WordNetLemmatizer\n    - sklearn.feature_extraction.text.CountVectorizer\n    - re\n    \n    Example:\n    >>> data = pd.DataFrame({'text': ['Hello world!', 'Goodbye world!']})\n    >>> replace_and_analyze_text(data, 'text', 0, 'Hello', 'Hi')\n    \"\"\"\n    # Replace the word\n    data.loc[index, column] = re.sub(r'\\b'+word+r'\\b', new_word, data.loc[index, column])\n    \n    # Perform text analysis\n    text_data = data[column].apply(lambda x: ' '.join([LEMMATIZER.lemmatize(word) for word in x.split() if word not in STOPWORDS]))\n    word_counts = pd.DataFrame(VECTORIZER.fit_transform(text_data).toarray(),\n                               columns=VECTORIZER.get_feature_names_out())\n    \n    return word_counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\njson, os, hashlib, re\n```python\nimport json\nimport os\nimport hashlib\nimport re\n\n# Constants\nJSON_FILE = 'sample.json'\nOUTPUT_DIR = 'output'\nBUFFER_SIZE = 65536\n\ndef replace_and_hash_json(json_key, index, new_value):\n    \"\"\"\n    Replace a value at a specific index in a JSON file with a new value and \n    calculate the MD5 hash of the modified JSON file.\n    \n    Parameters:\n    json_key (str): The key in the JSON file.\n    index (int): The index of the value.\n    new_value (str): The new value.\n    \n    Returns:\n    str: The MD5 hash of the modified JSON file.\n    \n    Requirements:\n    - json\n    - os\n    - hashlib\n    - re\n\n    Example:\n    >>> replace_and_hash_json('name', 0, 'New name')\n    \"\"\"\n    with open(JSON_FILE, 'r+') as file:\n        data = json.load(file)\n        data[json_key][index] = new_value\n        file.seek(0)\n        json.dump(data, file)\n        file.truncate()\n\n    # Calculate the MD5 hash\n    md5 = hashlib.md5()\n    with open(JSON_FILE, 'rb') as file:\n        while True:\n            data = file.read(BUFFER_SIZE)\n            if not data:\n                break\n            md5.update(data)\n    \n    return md5.hexdigest()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnumpy, pandas, matplotlib, seaborn\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nCOLUMN_NAMES = ['A', 'B', 'C', 'D', 'E']\n\ndef replace_and_visualize_data(data, column, index, new_value):\n    \"\"\"\n    Replace a value at a specific index in a DataFrame with a new value and \n    visualize the distribution of the data in each column.\n    \n    Parameters:\n    data (DataFrame): The pandas DataFrame.\n    column (str): The column name.\n    index (int): The row index.\n    new_value (float): The new value.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - seaborn\n    \n    Example:\n    >>> data = pd.DataFrame(np.random.rand(10, 5), columns=COLUMN_NAMES)\n    >>> replace_and_visualize_data(data, 'A', 0, 0.5)\n    \"\"\"\n    # Replace the value\n    data.loc[index, column] = new_value\n    \n    # Visualize the data\n    plt.figure(figsize=(10, 8))\n    for i, column in enumerate(data.columns):\n        plt.subplot(len(data.columns), 1, i+1)\n        sns.histplot(data[column], bins=20, kde=True)\n    plt.tight_layout()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \ndatetime, re, pandas, matplotlib\n```python\nimport datetime\nimport re\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nDATE_FORMAT = \"%Y-%m-%d %H:%M:%S\"\n\ndef replace_and_plot_dates(data, column, index, new_date):\n    \"\"\"\n    Replace a date at a specific index in a DataFrame with a new date and \n    plot a histogram of the dates.\n    \n    Parameters:\n    data (DataFrame): The pandas DataFrame.\n    column (str): The column name.\n    index (int): The row index.\n    new_date (str): The new date in the format \"yyyy-mm-dd hh:mm:ss\".\n    \n    Returns:\n    None\n    \n    Requirements:\n    - datetime\n    - re\n    - pandas\n    - matplotlib.pyplot\n    \n    Example:\n    >>> data = pd.DataFrame({'date': pd.date_range(start='1/1/2020', periods=100)})\n    >>> replace_and_plot_dates(data, 'date', 0, '2021-01-01 00:00:00')\n    \"\"\"\n    # Replace the date\n    data.loc[index, column] = datetime.datetime.strptime(new_date, DATE_FORMAT)\n    \n    # Plot the dates\n    data[column].hist()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \ncsv, pandas, sklearn, re\n```python\nimport csv\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport re\n\n# Constants\nCSV_FILE = 'data.csv'\n\ndef replace_and_scale_data(column, index, new_value):\n    \"\"\"\n    Replace a value at a specific index in a CSV file with a new value and \n    standardize the values in the column.\n    \n    Parameters:\n    column (str): The column name.\n    index (int): The row index.\n    new_value (str): The new value.\n    \n    Returns:\n    DataFrame: A DataFrame with the standardized values.\n    \n    Requirements:\n    - csv\n    - pandas\n    - sklearn.preprocessing.StandardScaler\n    - re\n\n    Example:\n    >>> replace_and_scale_data('A', 0, '100')\n    \"\"\"\n    # Load the CSV file\n    data = pd.read_csv(CSV_FILE)\n    \n    # Replace the value\n    data.loc[index, column] = new_value\n    \n    # Standardize the values\n    scaler = StandardScaler()\n    data[column] = scaler.fit_transform(data[[column]])\n    \n    return data\n```\n"}
{"seed": "{\"task_id\": 458373, \"prompt\": \"def f_458373(s):\\n\\t\", \"suffix\": \"\\n\\treturn chars\", \"canonical_solution\": \"chars = list(s)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(\\\"Hello!\\\") == [\\\"H\\\", \\\"e\\\", \\\"l\\\", \\\"l\\\", \\\"o\\\", \\\"!\\\"] \\n\"], \"entry_point\": \"f_458373\", \"intent\": \"\\u0420\\u0430\\u0437\\u0431\\u0438\\u0442\\u044c \\u0441\\u0442\\u0440\\u043e\\u043a\\u0443 `s` \\u043d\\u0430 \\u0441\\u043f\\u0438\\u0441\\u043e\\u043a \\u0441\\u0438\\u043c\\u0432\\u043e\\u043b\\u043e\\u0432 `chars`\", \"library\": []}", "generation": "Scenario 1:\npandas, matplotlib, random, string\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import choice\nimport string\n\n# Constants\nLETTERS = string.ascii_letters\n\ndef generate_random_strings(n, length):\n    \"\"\"\n    Generate a pandas DataFrame with n random strings of a specified length. \n    Then plot a histogram of letter frequencies in those strings.\n    \n    Parameters:\n    n (int): The number of strings to generate.\n    length (int): The length of each string.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the generated strings.\n    \n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - random\n    - string\n    \n    Example:\n    >>> df = generate_random_strings(10, 5)\n    >>> print(df)\n    >>> df['String'].str.cat(sep='').value_counts().plot(kind='bar')\n    \"\"\"\n    strings = [''.join(choice(LETTERS) for _ in range(length)) for _ in range(n)]\n    df = pd.DataFrame(strings, columns=['String'])\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, matplotlib, random, string\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import choice\nimport string\n\n# Constants\nLETTERS = string.ascii_letters\n\ndef random_string_histogram(n, length):\n    \"\"\"\n    Generate n random strings of a specified length and plot a histogram of letter frequencies.\n    \n    Parameters:\n    n (int): The number of strings to generate.\n    length (int): The length of each string.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - random\n    - string\n    \n    Example:\n    >>> random_string_histogram(1000, 5)\n    \"\"\"\n    strings = [''.join(choice(LETTERS) for _ in range(length)) for _ in range(n)]\n    letters, frequencies = np.unique(list(''.join(strings)), return_counts=True)\n    plt.bar(letters, frequencies)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ncollections, re, nltk\n```python\nfrom collections import Counter\nimport re\nimport nltk\n\n# Constants\nSTOPWORDS = set(nltk.corpus.stopwords.words('english'))\n\ndef count_non_stopwords(text):\n    \"\"\"\n    Count the non-stopword words in a text string.\n    \n    Parameters:\n    text (str): The text string to analyze.\n    \n    Returns:\n    Counter: A Counter object with the frequencies of non-stopword words.\n    \n    Requirements:\n    - collections\n    - re\n    - nltk\n    \n    Example:\n    >>> count_non_stopwords('This is a test. This is only a test.')\n    Counter({'test.': 2})\n    \"\"\"\n    words = re.findall(r'\\b\\w+\\b', text)\n    non_stopwords = [word for word in words if word not in STOPWORDS]\n    return Counter(non_stopwords)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, seaborn, matplotlib.pyplot\n```python\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['Name', 'Age', 'Height', 'Weight']\n\ndef create_and_plot_dataframe(data):\n    \"\"\"\n    Create a pandas DataFrame from a list of lists and plot a pairplot of the numerical columns.\n    \n    Parameters:\n    data (list): The data to create the DataFrame from.\n    \n    Returns:\n    DataFrame: The created pandas DataFrame.\n    \n    Requirements:\n    - pandas\n    - seaborn\n    - matplotlib.pyplot\n    \n    Example:\n    >>> data = [['Alice', 25, 170, 60], ['Bob', 30, 180, 80], ['Charlie', 35, 175, 75]]\n    >>> df = create_and_plot_dataframe(data)\n    >>> print(df)\n    >>> sns.pairplot(df[['Age', 'Height', 'Weight']])\n    >>> plt.show()\n    \"\"\"\n    df = pd.DataFrame(data, columns=COLUMNS)\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, matplotlib, random\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import random\n\n# Constants\nTHRESHOLD = 0.5\n\ndef random_walk(steps):\n    \"\"\"\n    Perform a random walk of a specified number of steps and plot the path.\n    \n    Parameters:\n    steps (int): The number of steps in the random walk.\n    \n    Returns:\n    ndarray: A numpy array with the positions at each step.\n    \n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - random\n    \n    Example:\n    >>> path = random_walk(100)\n    >>> plt.plot(path)\n    >>> plt.show()\n    \"\"\"\n    walk = np.zeros(steps)\n    for i in range(1, steps):\n        walk[i] = walk[i-1] + (1 if random() > THRESHOLD else -1)\n    return walk\n```"}
{"seed": "{\"task_id\": 458373, \"prompt\": \"def f_458373(s):\\n\\t\", \"suffix\": \"\\n\\treturn chars\", \"canonical_solution\": \"chars = [c for c in s]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(\\\"Hello!\\\") == [\\\"H\\\", \\\"e\\\", \\\"l\\\", \\\"l\\\", \\\"o\\\", \\\"!\\\"] \\n\"], \"entry_point\": \"f_458373\", \"intent\": \"\\u0420\\u0430\\u0437\\u0431\\u0438\\u0442\\u044c \\u0441\\u0442\\u0440\\u043e\\u043a\\u0443 `s` \\u043d\\u0430 \\u0441\\u043f\\u0438\\u0441\\u043e\\u043a \\u0441\\u0438\\u043c\\u0432\\u043e\\u043b\\u043e\\u0432 `chars`\", \"library\": []}", "generation": "Scenario 1: \npandas, nltk, collections, matplotlib\n```python\nimport pandas as pd\nfrom nltk.tokenize import word_tokenize\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\n# Constants\nCOMMON_WORDS = ['the', 'be', 'to', 'of', 'and', 'a', 'in', 'that', 'have', 'I']\n\ndef word_frequency_analysis(text):\n    \"\"\"\n    Tokenize a text into words, count the frequency of each word, and \n    visualize the top 10 most frequent words excluding common words.\n\n    Parameters:\n    text (str): The input text.\n\n    Returns:\n    DataFrame: A pandas DataFrame with word frequency.\n\n    Requirements:\n    - pandas\n    - nltk.tokenize\n    - collections\n    - matplotlib.pyplot\n\n    Example:\n    >>> text = \"This is a simple text for testing the word frequency analysis function.\"\n    >>> word_frequency = word_frequency_analysis(text)\n    >>> print(word_frequency)\n    >>> word_frequency.plot(kind='bar', x='Word', y='Frequency')\n    \"\"\"\n    words = word_tokenize(text)\n    words = [word.lower() for word in words if word.isalpha()]\n\n    word_counter = Counter(words)\n    \n    for word in COMMON_WORDS:\n        if word in word_counter:\n            del word_counter[word]\n\n    word_frequency = pd.DataFrame(word_counter.most_common(10), columns=['Word', 'Frequency'])\n\n    return word_frequency\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nre, string, collections\n```python\nimport re\nimport string\nfrom collections import Counter\n\n# Constants\nPUNCTUATION = string.punctuation\n\ndef count_alphanumeric_chars(text):\n    \"\"\"\n    Split a string into alphanumeric characters and count the frequency of each character.\n\n    Parameters:\n    text (str): The input string.\n\n    Returns:\n    dict: A dictionary with the frequency of each alphanumeric character.\n\n    Requirements:\n    - re\n    - string\n    - collections\n\n    Example:\n    >>> text = \"Hello, World!\"\n    >>> char_counts = count_alphanumeric_chars(text)\n    >>> print(char_counts)\n    \"\"\"\n    text = re.sub('['+PUNCTUATION+']', '', text)\n    char_counts = Counter(text)\n\n    return char_counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnltk, string\n```python\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\n\n# Constants\nSTOPWORDS = set(stopwords.words('english'))\n\ndef remove_stopwords(text):\n    \"\"\"\n    Split a string into words and remove the stopwords.\n\n    Parameters:\n    text (str): The input string.\n\n    Returns:\n    str: The string without stopwords.\n\n    Requirements:\n    - nltk\n    - nltk.corpus\n    - string\n\n    Example:\n    >>> text = \"This is a simple text.\"\n    >>> text_without_stopwords = remove_stopwords(text)\n    >>> print(text_without_stopwords)\n    \"\"\"\n    words = nltk.word_tokenize(text)\n    words_without_stopwords = [word for word in words if word not in STOPWORDS and word not in string.punctuation]\n\n    return ' '.join(words_without_stopwords)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nstring, collections, matplotlib\n```python\nimport string\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef count_punctuation(text):\n    \"\"\"\n    Count the frequency of each punctuation in a text and visualize it.\n\n    Parameters:\n    text (str): The input text.\n\n    Returns:\n    Counter: A Counter object with the frequency of each punctuation.\n\n    Requirements:\n    - string\n    - collections\n    - matplotlib.pyplot\n\n    Example:\n    >>> text = \"Hello, World! This is a test.\"\n    >>> punctuation_counts = count_punctuation(text)\n    >>> print(punctuation_counts)\n    >>> plt.bar(punctuation_counts.keys(), punctuation_counts.values())\n    \"\"\"\n    punctuation_counts = Counter(char for char in text if char in string.punctuation)\n\n    return punctuation_counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nstring, collections, matplotlib\n```python\nimport string\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\n# Constants\nLETTERS = string.ascii_lowercase\n\ndef count_letters(text):\n    \"\"\"\n    Count the frequency of each letter in a text and visualize it.\n\n    Parameters:\n    text (str): The input text.\n\n    Returns:\n    Counter: A Counter object with the frequency of each letter.\n\n    Requirements:\n    - string\n    - collections\n    - matplotlib.pyplot\n\n    Example:\n    >>> text = \"Hello, World! This is a test.\"\n    >>> letter_counts = count_letters(text)\n    >>> print(letter_counts)\n    >>> plt.bar(letter_counts.keys(), letter_counts.values())\n    \"\"\"\n    text = text.lower()\n    letter_counts = Counter(char for char in text if char in LETTERS)\n\n    return letter_counts\n```"}
{"seed": "{\"task_id\": 879670, \"prompt\": \"def f_879670(s, i):\\n\\t\", \"suffix\": \"\\n\\treturn s\", \"canonical_solution\": \"s = s[:i] + s[i+1:]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(\\\"Hello!\\\", 0) == \\\"ello!\\\"\\n\", \"\\n    assert candidate(\\\"Hello!\\\", 2) == \\\"Helo!\\\"\\n\"], \"entry_point\": \"f_879670\", \"intent\": \"\\u0423\\u0434\\u0430\\u043b\\u0438\\u0442\\u044c \\u0441\\u0438\\u043c\\u0432\\u043e\\u043b \\u0441 \\u0438\\u043d\\u0434\\u0435\\u043a\\u0441\\u043e\\u043c `i` \\u0438\\u0437 \\u0441\\u0442\\u0440\\u043e\\u043a\\u0438 `s`\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, random, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import choice\nimport matplotlib.pyplot as plt\n\n# Constants\nLETTERS = list('abcdefghijklmnopqrstuvwxyz')\nWORDS = ['apple', 'banana', 'cherry', 'date', 'elderberry', 'fig', 'grape', 'honeydew']\n\ndef generate_random_text_data(length):\n    \"\"\"\n    Generate a pandas DataFrame of random text data of a given length, \n    where each entry is a randomly chosen word from a list, \n    and a column is added that counts the number of occurrences of a certain letter in that word.\n    \n    Parameters:\n    length (int): The length of the DataFrame.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with random text data.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = generate_random_text_data(10)\n    >>> print(df)\n    >>> df['Letter Count'].plot(kind='bar')\n    \"\"\"\n    data = []\n    for _ in range(length):\n        word = choice(WORDS)\n        letter = choice(LETTERS)\n        letter_count = word.count(letter)\n        data.append([word, letter, letter_count])\n\n    df = pd.DataFrame(data, columns=['Word', 'Letter', 'Letter Count'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, random, matplotlib\n```python\nimport numpy as np\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nMIN_VAL = 0\nMAX_VAL = 100\n\ndef plot_random_numbers_histogram(length, bins):\n    \"\"\"\n    Generate an array of random integers of a given length and plot a histogram of their distribution.\n    \n    Parameters:\n    length (int): The length of the array.\n    bins (int): The number of bins for the histogram.\n    \n    Returns:\n    None, but a histogram is displayed.\n    \n    Requirements:\n    - numpy\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> plot_random_numbers_histogram(1000, 10)\n    \"\"\"\n    arr = np.array([randint(MIN_VAL, MAX_VAL) for _ in range(length)])\n    \n    plt.hist(arr, bins=bins)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnumpy, matplotlib\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nMIN_VAL = -10\nMAX_VAL = 10\n\ndef plot_sin_and_cos(x):\n    \"\"\"\n    Plot the sin and cos of a given array on the same graph.\n    \n    Parameters:\n    x (numpy array): The array of x values.\n    \n    Returns:\n    None, but a graph is displayed.\n    \n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> x = np.linspace(MIN_VAL, MAX_VAL, 100)\n    >>> plot_sin_and_cos(x)\n    \"\"\"\n    y_sin = np.sin(x)\n    y_cos = np.cos(x)\n    \n    plt.plot(x, y_sin, label='sin(x)')\n    plt.plot(x, y_cos, label='cos(x)')\n    plt.legend()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \npandas, numpy, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nLENGTH = 100\n\ndef plot_dataframe(df):\n    \"\"\"\n    Plot all columns of a given pandas DataFrame on the same graph.\n    \n    Parameters:\n    df (pandas DataFrame): The DataFrame to plot.\n    \n    Returns:\n    None, but a graph is displayed.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = pd.DataFrame({\n    ...     'A': np.random.rand(LENGTH),\n    ...     'B': np.random.rand(LENGTH),\n    ...     'C': np.random.rand(LENGTH)\n    ... })\n    >>> plot_dataframe(df)\n    \"\"\"\n    for col in df.columns:\n        plt.plot(df.index, df[col], label=col)\n    \n    plt.legend()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnumpy, matplotlib, scipy\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import curve_fit\n\n# Constants\nSTART = 0\nEND = 10\nPOINTS = 100\n\ndef fit_and_plot_sin(x, y):\n    \"\"\"\n    Fit a sine wave to a given set of data points and plot the original data and the fitted curve.\n    \n    Parameters:\n    x (numpy array): The x values of the data points.\n    y (numpy array): The y values of the data points.\n    \n    Returns:\n    None, but a graph is displayed.\n    \n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - scipy.optimize.curve_fit\n    \n    Example:\n    >>> x = np.linspace(START, END, POINTS)\n    >>> y = np.sin(x) + np.random.normal(scale=0.1, size=POINTS)\n    >>> fit_and_plot_sin(x, y)\n    \"\"\"\n    def sin_func(x, a, b, c, d):\n        return a * np.sin(b * x + c) + d\n\n    popt, _ = curve_fit(sin_func, x, y, p0=[1, 1, 0, 0])\n\n    y_fit = sin_func(x, *popt)\n\n    plt.plot(x, y, 'b-', label='data')\n    plt.plot(x, y_fit, 'r-', label='fit: a=%5.3f, b=%5.3f, c=%5.3f, d=%5.3f' % tuple(popt))\n    plt.legend()\n    plt.show()\n```\n"}
{"seed": "{\"task_id\": 119418, \"prompt\": \"def f_119418(dicts):\\n\\t\", \"suffix\": \"\\n\\treturn common_dict\", \"canonical_solution\": \"\\n\\tcommon_dict = {}\\n\\tfor cur_dict in dicts:\\n\\t\\tfor key in cur_dict:\\n\\t\\t\\tif key not in common_dict:\\n\\t\\t\\t\\tcommon_dict[key] = [cur_dict[key]]\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\tcommon_dict[key] += [cur_dict[key]]\\n\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([{\\\"key1\\\": \\\"value1\\\"}, {\\\"key2\\\":\\\"value2\\\"}, {\\\"key1\\\":\\\"value3\\\", \\\"key3\\\": \\\"value4\\\"}]) ==         {'key1': ['value1', 'value3'], 'key2': ['value2'], 'key3': ['value4']}\\n\"], \"entry_point\": \"f_119418\", \"intent\": \"\\u041e\\u0431\\u044a\\u0435\\u0434\\u0438\\u043d\\u0438\\u0442\\u044c \\u0441\\u043f\\u0438\\u0441\\u043e\\u043a \\u0441\\u043b\\u043e\\u0432\\u0430\\u0440\\u0435\\u0439 `dicts` \\u0441 \\u043e\\u0434\\u0438\\u043d\\u0430\\u043a\\u043e\\u0432\\u044b\\u043c\\u0438 \\u043a\\u043b\\u044e\\u0447\\u0430\\u043c\\u0438 \\u0432 \\u043e\\u0431\\u0449\\u0438\\u0439 \\u0441\\u043b\\u043e\\u0432\\u0430\\u0440\\u044c `common_dict`\", \"library\": []}", "generation": "Scenario 1:\nnumpy, pandas, matplotlib.pyplot, os\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\n\n# Constants\nFILE_PATH = \"data_files/\"\nEXTENSION = \".csv\"\n\ndef plot_distribution_of_common_keys(dicts):\n    \"\"\"\n    Generate a bar plot showing the distribution of common keys across a list of dictionaries.\n    \n    Parameters:\n    dicts (list): A list of dictionaries.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - os\n    \n    Example:\n    >>> dicts = [{\"key1\": \"value1\"}, {\"key2\":\"value2\"}, {\"key1\":\"value3\", \"key3\": \"value4\"}]\n    >>> plot_distribution_of_common_keys(dicts)\n    \"\"\"\n    common_dict = {}\n    for cur_dict in dicts:\n        for key in cur_dict:\n            if key not in common_dict:\n                common_dict[key] = [cur_dict[key]]\n                continue\n            common_dict[key] += [cur_dict[key]]\n\n    # Save dataframe to a CSV file\n    df = pd.DataFrame(common_dict)\n    df.to_csv(os.path.join(FILE_PATH, 'common_dict' + EXTENSION), index=False)\n\n    # Plot the distribution of common keys\n    df.count().plot(kind='bar')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\njson, collections, matplotlib.pyplot, os\n```python\nimport json\nimport collections\nimport matplotlib.pyplot as plt\nimport os\n\n# Constants\nFILE_PATH = \"data_files/\"\nEXTENSION = \".json\"\n\ndef plot_common_keys_distribution_from_json_files(json_files):\n    \"\"\"\n    Combine multiple JSON files into a single dictionary and plot the distribution of common keys.\n    \n    Parameters:\n    json_files (list): A list of JSON file names.\n\n    Returns:\n    None\n\n    Requirements:\n    - json\n    - collections\n    - matplotlib.pyplot\n    - os\n    \n    Example:\n    >>> json_files = [\"file1.json\", \"file2.json\", \"file3.json\"]\n    >>> plot_common_keys_distribution_from_json_files(json_files)\n    \"\"\"\n    common_dict = collections.defaultdict(list)\n\n    for json_file in json_files:\n        with open(os.path.join(FILE_PATH, json_file), 'r') as file:\n            cur_dict = json.load(file)\n            for key, value in cur_dict.items():\n                common_dict[key].append(value)\n\n    # Plot the distribution of common keys\n    plt.bar(common_dict.keys(), [len(v) for v in common_dict.values()])\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, pandas, seaborn, os\n```python\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport os\n\n# Constants\nFILE_PATH = \"data_files/\"\n\ndef analyze_common_dict_distribution(dicts):\n    \"\"\"\n    Analyze the distribution of common keys across a list of dictionaries using a box plot.\n\n    Parameters:\n    dicts (list): A list of dictionaries.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - pandas\n    - seaborn\n    - os\n    \n    Example:\n    >>> dicts = [{\"key1\": \"value1\"}, {\"key2\":\"value2\"}, {\"key1\":\"value3\", \"key3\": \"value4\"}]\n    >>> analyze_common_dict_distribution(dicts)\n    \"\"\"\n    common_dict = {}\n    for cur_dict in dicts:\n        for key in cur_dict:\n            if key not in common_dict:\n                common_dict[key] = [cur_dict[key]]\n                continue\n            common_dict[key] += [cur_dict[key]]\n\n    # Convert the dictionary to a DataFrame\n    df = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in common_dict.items() ]))\n\n    # Save dataframe to a CSV file\n    df.to_csv(os.path.join(FILE_PATH, 'common_dict.csv'), index=False)\n\n    # Plot the distribution of common keys\n    sns.boxplot(data=df)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, pandas, matplotlib.pyplot, os, json\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport json\n\n# Constants\nFILE_PATH = \"data_files/\"\nEXTENSION = \".json\"\n\ndef analyze_json_files(json_files):\n    \"\"\"\n    Combine multiple JSON files into a single dictionary and analyze the distribution of common keys.\n\n    Parameters:\n    json_files (list): A list of JSON file names.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - os\n    - json\n    \n    Example:\n    >>> json_files = [\"file1.json\", \"file2.json\", \"file3.json\"]\n    >>> analyze_json_files(json_files)\n    \"\"\"\n    common_dict = {}\n\n    for json_file in json_files:\n        with open(os.path.join(FILE_PATH, json_file), 'r') as file:\n            cur_dict = json.load(file)\n            for key, value in cur_dict.items():\n                if key not in common_dict:\n                    common_dict[key] = [value]\n                    continue\n                common_dict[key] += [value]\n\n    # Convert the dictionary to a DataFrame\n    df = pd.DataFrame(common_dict)\n\n    # Save dataframe to a JSON file\n    df.to_json(os.path.join(FILE_PATH, 'common_dict' + EXTENSION), orient='split')\n\n    # Plot the distribution of common keys\n    df.count().plot(kind='bar')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, pandas, seaborn, os, json\n```python\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport os\nimport json\n\n# Constants\nFILE_PATH = \"data_files/\"\nEXTENSION = \".json\"\n\ndef analyze_json_files_with_boxplot(json_files):\n    \"\"\"\n    Combine multiple JSON files into a single dictionary and analyze the distribution of common keys using a box plot.\n\n    Parameters:\n    json_files (list): A list of JSON file names.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - pandas\n    - seaborn\n    - os\n    - json\n    \n    Example:\n    >>> json_files = [\"file1.json\", \"file2.json\", \"file3.json\"]\n    >>> analyze_json_files_with_boxplot(json_files)\n    \"\"\"\n    common_dict = {}\n\n    for json_file in json_files:\n        with open(os.path.join(FILE_PATH, json_file), 'r') as file:\n            cur_dict = json.load(file)\n            for key, value in cur_dict.items():\n                if key not in common_dict:\n                    common_dict[key] = [value]\n                    continue\n                common_dict[key] += [value]\n\n    # Convert the dictionary to a DataFrame\n    df = pd.DataFrame(common_dict)\n\n    # Save dataframe to a JSON file\n    df.to_json(os.path.join(FILE_PATH, 'common_dict' + EXTENSION), orient='split')\n\n    # Plot the distribution of common keys\n    sns.boxplot(data=df)\n    plt.show()\n```"}
{"seed": "{\"task_id\": 60790, \"prompt\": \"def f_60790(filepath):\\n\\t\", \"suffix\": \"\\n\\treturn lib\", \"canonical_solution\": \"lib = CDLL(filepath)\", \"test_start\": \"\\nfrom ctypes import CDLL\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(\\\"libc.so.6\\\")._name == \\\"libc.so.6\\\"\\n\"], \"entry_point\": \"f_60790\", \"intent\": \"\\u041f\\u043e\\u0434\\u043a\\u043b\\u044e\\u0447\\u0438\\u0442\\u044c dll \\u0444\\u0430\\u0439\\u043b `filepath` \\u0432 \\u043f\\u0435\\u0440\\u0435\\u043c\\u0435\\u043d\\u043d\\u0443\\u044e `lib`\", \"library\": [\"ctypes\"]}", "generation": "Scenario 1: \nctypes, os, json, csv\n```python\nfrom ctypes import CDLL\nimport os\nimport json\nimport csv\n\ndef json_to_csv_dll(filepath, json_file, csv_file):\n    \"\"\"\n    Load a DLL file, read data from a JSON file, convert it to CSV, and save it to a CSV file using the DLL file.\n\n    Parameters:\n    filepath (str): The path of the DLL file.\n    json_file (str): The path of the JSON file.\n    csv_file (str): The path of the CSV file.\n\n    Returns:\n    str: The name of the DLL file.\n\n    Requirements:\n    - ctypes\n    - os\n    - json\n    - csv\n\n    Example:\n    >>> json_to_csv_dll('libc.so.6', 'data.json', 'data.csv')\n    \"\"\"\n    lib = CDLL(filepath)\n\n    with open(json_file, 'r') as jf:\n        data = json.load(jf)\n\n    with open(csv_file, 'w', newline='') as cf:\n        writer = csv.writer(cf)\n        writer.writerow(data.keys())\n        writer.writerows(zip(*data.values()))\n\n    return lib._name\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nctypes, os, sys, subprocess\n```python\nfrom ctypes import CDLL\nimport os\nimport sys\nimport subprocess\n\ndef system_info_dll(filepath):\n    \"\"\"\n    Load a DLL file, get system information, and print it using the DLL file.\n\n    Parameters:\n    filepath (str): The path of the DLL file.\n\n    Returns:\n    str: The name of the DLL file.\n\n    Requirements:\n    - ctypes\n    - os\n    - sys\n    - subprocess\n\n    Example:\n    >>> system_info_dll('libc.so.6')\n    \"\"\"\n    lib = CDLL(filepath)\n\n    uname = os.uname()\n    print(f'System: {uname.sysname}')\n    print(f'Node Name: {uname.nodename}')\n    print(f'Release: {uname.release}')\n    print(f'Version: {uname.version}')\n    print(f'Machine: {uname.machine}')\n\n    python_version = sys.version\n    print(f'Python Version: {python_version}')\n\n    pip_version = subprocess.check_output(['pip', '--version'])\n    print(f'PIP Version: {pip_version.decode(\"utf-8\")}')\n\n    return lib._name\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nctypes, os, shutil, glob\n```python\nfrom ctypes import CDLL\nimport os\nimport shutil\nimport glob\n\ndef move_dll_files(filepath, destination_dir):\n    \"\"\"\n    Load a DLL file and move all DLL files in the same directory to another directory.\n\n    Parameters:\n    filepath (str): The path of the DLL file.\n    destination_dir (str): The path of the destination directory.\n\n    Returns:\n    str: The name of the DLL file.\n\n    Requirements:\n    - ctypes\n    - os\n    - shutil\n    - glob\n\n    Example:\n    >>> move_dll_files('libc.so.6', 'destination_dir')\n    \"\"\"\n    lib = CDLL(filepath)\n\n    dll_dir = os.path.dirname(filepath)\n    dll_files = glob.glob(os.path.join(dll_dir, '*.dll'))\n\n    for dll_file in dll_files:\n        shutil.move(dll_file, destination_dir)\n\n    return lib._name\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nctypes, os, datetime, pytz\n```python\nfrom ctypes import CDLL\nimport os\nfrom datetime import datetime\nimport pytz\n\ndef dll_file_info(filepath):\n    \"\"\"\n    Load a DLL file, get its information such as creation time, modification time, and size, \n    and print it.\n\n    Parameters:\n    filepath (str): The path of the DLL file.\n\n    Returns:\n    str: The name of the DLL file.\n\n    Requirements:\n    - ctypes\n    - os\n    - datetime\n    - pytz\n\n    Example:\n    >>> dll_file_info('libc.so.6')\n    \"\"\"\n    lib = CDLL(filepath)\n\n    file_stat = os.stat(filepath)\n\n    creation_time = datetime.fromtimestamp(file_stat.st_ctime, pytz.UTC)\n    print(f'Creation Time: {creation_time}')\n\n    modification_time = datetime.fromtimestamp(file_stat.st_mtime, pytz.UTC)\n    print(f'Modification Time: {modification_time}')\n\n    file_size = file_stat.st_size\n    print(f'Size: {file_size} bytes')\n\n    return lib._name\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nctypes, os, hashlib, binascii\n```python\nfrom ctypes import CDLL\nimport os\nimport hashlib\nimport binascii\n\ndef dll_file_hash(filepath):\n    \"\"\"\n    Load a DLL file, calculate its MD5 and SHA256 hashes, and print them.\n\n    Parameters:\n    filepath (str): The path of the DLL file.\n\n    Returns:\n    str: The name of the DLL file.\n\n    Requirements:\n    - ctypes\n    - os\n    - hashlib\n    - binascii\n\n    Example:\n    >>> dll_file_hash('libc.so.6')\n    \"\"\"\n    lib = CDLL(filepath)\n\n    with open(filepath, 'rb') as f:\n        data = f.read()\n\n    md5_hash = hashlib.md5(data).digest()\n    print(f'MD5 Hash: {binascii.hexlify(md5_hash).decode()}')\n\n    sha256_hash = hashlib.sha256(data).digest()\n    print(f'SHA256 Hash: {binascii.hexlify(sha256_hash).decode()}')\n\n    return lib._name\n```"}
{"seed": "{\"task_id\": 299217, \"prompt\": \"def f_299217(url):\\n\\t\", \"suffix\": \"\\n\\treturn cookies\", \"canonical_solution\": \"\\n\\tsession = requests.Session()\\n\\tresponse = session.get(url)\\n\\tcookies = session.cookies.get_dict()\\n\", \"test_start\": \"\\nimport requests\\nfrom datetime import date\\n\\ndef check(candidate):\", \"test\": [\"\\n    today = date.today().strftime(\\\"%Y-%m-%d\\\")\\n    assert candidate(\\\"http://google.com\\\")['1P_JAR'].startswith(today)\\n\"], \"entry_point\": \"f_299217\", \"intent\": \"\\u041f\\u043e\\u043b\\u0443\\u0447\\u0438\\u0442\\u044c cookie `cookies` \\u0441 \\u0441\\u0430\\u0439\\u0442\\u0430 `url`\", \"library\": [\"datetime\", \"requests\"]}", "generation": "Scenario 1: \nrequests, bs4, pandas, csv, re\n```python\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport csv\nimport re\n\n# Constants\nHEADERS = {'User-Agent': 'Mozilla/5.0'}\n\ndef fetch_and_store_product_data(url, filename):\n    \"\"\"\n    Fetch product data from a webpage and store it in a CSV file. Extract the product name, price,\n    and product URL from the page. The product data is stored in a pandas DataFrame and then written to a CSV file.\n\n    Parameters:\n    url (str): The URL of the webpage to fetch.\n    filename (str): The name of the CSV file to store the product data.\n\n    Returns:\n    str: The name of the CSV file.\n\n    Requirements:\n    - requests\n    - bs4.BeautifulSoup\n    - pandas\n    - csv\n    - re\n\n    Example:\n    >>> fetch_and_store_product_data('http://example.com/products', 'products.csv')\n    \"\"\"\n    response = requests.get(url, headers=HEADERS)\n    soup = BeautifulSoup(response.content, \"html.parser\")\n\n    products = soup.find_all('div', class_='product')\n\n    product_data = []\n\n    for product in products:\n        name = product.find('h1', class_='product-name').text\n        price = re.findall(r'\\d+.\\d+', product.find('span', class_='product-price').text)[0]\n        product_url = product.find('a', class_='product-link')['href']\n\n        product_data.append([name, price, product_url])\n\n    product_df = pd.DataFrame(product_data, columns=['Name', 'Price', 'URL'])\n    product_df.to_csv(filename, index=False, quoting=csv.QUOTE_MINIMAL)\n\n    return filename\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nrequests, json, urllib, os, time\n```python\nimport requests\nimport json\nfrom urllib.parse import urljoin\nimport os\nimport time\n\n# Constants\nHEADERS = {'User-Agent': 'Mozilla/5.0'}\nDOWNLOAD_FOLDER = 'downloads/'\n\ndef download_images_from_json(url, json_key):\n    \"\"\"\n    Download all images from a JSON response of a webpage. The images URLs are present in a \n    list inside the JSON response. After downloading the images, it returns the list of \n    downloaded images' file names.\n\n    Parameters:\n    url (str): The URL of the webpage to fetch the JSON.\n    json_key (str): The key in the JSON response containing the list of image URLs.\n\n    Returns:\n    list: List of downloaded images' file names.\n\n    Requirements:\n    - requests\n    - json\n    - urllib.parse\n    - os\n    - time\n\n    Example:\n    >>> download_images_from_json('http://example.com/api/images', 'images')\n    \"\"\"\n    response = requests.get(url, headers=HEADERS)\n    data = json.loads(response.text)\n\n    image_urls = data[json_key]\n    downloaded_images = []\n\n    if not os.path.exists(DOWNLOAD_FOLDER):\n        os.makedirs(DOWNLOAD_FOLDER)\n\n    for image_url in image_urls:\n        image_filename = os.path.join(DOWNLOAD_FOLDER, image_url.split('/')[-1])\n        with open(image_filename, 'wb') as f:\n            f.write(requests.get(urljoin(url, image_url)).content)\n        downloaded_images.append(image_filename)\n\n        time.sleep(1)  # To prevent overwhelming the server\n\n    return downloaded_images\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nrequests, http.cookies, datetime, pytz\n```python\nimport requests\nfrom http.cookies import SimpleCookie\nfrom datetime import datetime, timedelta\nimport pytz\n\n# Constants\nHEADERS = {'User-Agent': 'Mozilla/5.0'}\n\ndef get_cookie_expiry_date(url, cookie_name):\n    \"\"\"\n    Get the expiry date of a specific cookie from a website.\n\n    Parameters:\n    url (str): The URL of the webpage to fetch the cookies.\n    cookie_name (str): The name of the cookie.\n\n    Returns:\n    datetime: The expiry date of the cookie.\n\n    Requirements:\n    - requests\n    - http.cookies.SimpleCookie\n    - datetime\n    - pytz\n\n    Example:\n    >>> get_cookie_expiry_date('http://example.com', 'session')\n    \"\"\"\n    response = requests.get(url, headers=HEADERS)\n    cookies = SimpleCookie()\n    cookies.load(response.headers['Set-Cookie'])\n\n    cookie = cookies[cookie_name]\n    expiry_date = datetime.fromtimestamp(int(cookie['expires']), pytz.UTC)\n\n    return expiry_date\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nrequests, lxml.html, pandas, matplotlib.pyplot\n```python\nimport requests\nimport lxml.html\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nHEADERS = {'User-Agent': 'Mozilla/5.0'}\n\ndef fetch_and_plot_data(url, xpath):\n    \"\"\"\n    Fetch data from a webpage using XPath and plot a histogram of the data.\n\n    Parameters:\n    url (str): The URL of the webpage to fetch the data.\n    xpath (str): The XPath to extract the data.\n\n    Returns:\n    Figure: The matplotlib figure.\n\n    Requirements:\n    - requests\n    - lxml.html\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> fetch_and_plot_data('http://example.com/data', '//table[@id=\"data\"]//td[2]/text()')\n    \"\"\"\n    response = requests.get(url, headers=HEADERS)\n    tree = lxml.html.fromstring(response.content)\n\n    data = tree.xpath(xpath)\n    data = pd.to_numeric(data, errors='coerce').dropna()\n\n    fig = plt.figure(figsize=(10, 6))\n    plt.hist(data, bins=50, color='c', edgecolor='black')\n    plt.title('Data Histogram')\n    plt.xlabel('Values')\n    plt.ylabel('Frequency')\n\n    return fig\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nrequests, beautifulsoup4, csv, os\n```python\nimport requests\nfrom bs4 import BeautifulSoup\nimport csv\nimport os\n\n# Constants\nHEADERS = {'User-Agent': 'Mozilla/5.0'}\nCSV_FOLDER = 'csv/'\n\ndef fetch_and_write_table_data(url, table_id, filename):\n    \"\"\"\n    Fetch a HTML table from a webpage and write the table data to a CSV file.\n\n    Parameters:\n    url (str): The URL of the webpage to fetch the table.\n    table_id (str): The ID of the table in the webpage.\n    filename (str): The name of the CSV file to store the table data.\n\n    Returns:\n    str: The name of the CSV file.\n\n    Requirements:\n    - requests\n    - bs4.BeautifulSoup\n    - csv\n    - os\n\n    Example:\n    >>> fetch_and_write_table_data('http://example.com/data', 'data-table', 'data.csv')\n    \"\"\"\n    response = requests.get(url, headers=HEADERS)\n    soup = BeautifulSoup(response.content, \"html.parser\")\n\n    table = soup.find('table', {'id': table_id})\n\n    if not os.path.exists(CSV_FOLDER):\n        os.makedirs(CSV_FOLDER)\n\n    with open(os.path.join(CSV_FOLDER, filename), 'w', newline='') as f:\n        writer = csv.writer(f)\n        for row in table.find_all('tr'):\n            writer.writerow([cell.text for cell in row.find_all('td')])\n\n    return filename\n```"}
{"seed": "{\"task_id\": 65166, \"prompt\": \"def f_65166(f):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \" inspect.getargspec(f)\", \"test_start\": \"\\nimport inspect\\n\\ndef check(candidate):\", \"test\": [\"\\n    def f(x): return 2*x\\n    assert candidate(f) == inspect.ArgSpec(args=['x'], varargs=None, keywords=None, defaults=None)\\n\", \"\\n    def g(x, y=2): return x*y\\n    assert candidate(g) == inspect.ArgSpec(args=['x', 'y'], varargs=None, keywords=None, defaults=(2,))\\n\"], \"entry_point\": \"f_65166\", \"intent\": \"\\u041f\\u043e\\u043b\\u0443\\u0447\\u0438\\u0442\\u044c \\u0441\\u043f\\u0435\\u0446\\u0438\\u0444\\u0438\\u043a\\u0430\\u0446\\u0438\\u044e \\u0430\\u0440\\u0433\\u0443\\u043c\\u0435\\u043d\\u0442\\u043e\\u0432 \\u0444\\u0443\\u043d\\u043a\\u0446\\u0438\\u0438 `f`\", \"library\": [\"inspect\"]}", "generation": "Scenario 1:\ninspect, types, pandas, numpy\n```python\nimport inspect\nimport types\nimport pandas as pd\nimport numpy as np\n\ndef get_function_info(f):\n    \"\"\"\n    Get the specifications of a function `f` with additional details such as\n    function name and the number of arguments. Also, check if the function is\n    a lambda function or not. \n\n    Parameters:\n    f (function): The function to inspect.\n\n    Returns:\n    dict: A dictionary with function specifications.\n\n    Requirements:\n    - inspect\n    - types\n    - pandas\n    - numpy\n\n    Example:\n    >>> def f(x): return 2*x\n    >>> get_function_info(f)\n    \"\"\"\n    spec = inspect.getfullargspec(f)\n\n    return {\n        'function_name': f.__name__,\n        'is_lambda': isinstance(f, types.LambdaType),\n        'args': spec.args,\n        'defaults': spec.defaults,\n        'annotations': spec.annotations\n    }\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ninspect, types, matplotlib, numpy, pandas\n```python\nimport inspect\nimport types\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\ndef plot_function_info(f_list):\n    \"\"\"\n    Plot a bar graph to display the number of arguments of functions in `f_list`.\n\n    Parameters:\n    f_list (list): List of functions to inspect.\n\n    Returns:\n    None\n\n    Requirements:\n    - inspect\n    - types\n    - matplotlib\n    - numpy\n    - pandas\n\n    Example:\n    >>> def f(x): return 2*x\n    >>> def g(x, y=2): return x*y\n    >>> plot_function_info([f, g])\n    \"\"\"\n    func_info = []\n    for f in f_list:\n        spec = inspect.getfullargspec(f)\n        func_info.append([f.__name__, len(spec.args)])\n\n    df = pd.DataFrame(func_info, columns=['Function Name', 'Number of Arguments'])\n    df.set_index('Function Name', inplace=True)\n    df.plot(kind='bar')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ninspect, types, pandas, numpy, os\n```python\nimport inspect\nimport types\nimport pandas as pd\nimport numpy as np\nimport os\n\ndef export_function_info(f_list, file_path):\n    \"\"\"\n    Export the specifications of functions in `f_list` to a CSV file.\n\n    Parameters:\n    f_list (list): List of functions to inspect.\n    file_path (str): Path to the CSV file to export the data.\n\n    Returns:\n    None\n\n    Requirements:\n    - inspect\n    - types\n    - pandas\n    - numpy\n    - os\n\n    Example:\n    >>> def f(x): return 2*x\n    >>> def g(x, y=2): return x*y\n    >>> export_function_info([f, g], './function_info.csv')\n    \"\"\"\n    func_info = []\n    for f in f_list:\n        spec = inspect.getfullargspec(f)\n        func_info.append([f.__name__, len(spec.args), spec.defaults, spec.annotations, isinstance(f, types.LambdaType)])\n\n    df = pd.DataFrame(func_info, columns=['Function Name', 'Number of Arguments', 'Defaults', 'Annotations', 'Is Lambda'])\n    df.to_csv(file_path, index=False)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ninspect, types, pandas, numpy, math\n```python\nimport inspect\nimport types\nimport pandas as pd\nimport numpy as np\nimport math\n\ndef analyze_function_info(f):\n    \"\"\"\n    Analyze a function `f` and return a dictionary with its specifications,\n    square root of the number of arguments, and the number of lambda functions\n    in defaults if any.\n\n    Parameters:\n    f (function): The function to inspect.\n\n    Returns:\n    dict: A dictionary with function specifications.\n\n    Requirements:\n    - inspect\n    - types\n    - pandas\n    - numpy\n    - math\n\n    Example:\n    >>> def f(x): return 2*x\n    >>> analyze_function_info(f)\n    \"\"\"\n    spec = inspect.getfullargspec(f)\n\n    info = {\n        'function_name': f.__name__,\n        'sqrt_args': math.sqrt(len(spec.args)),\n    }\n\n    if spec.defaults:\n        info['lambda_in_defaults'] = sum(1 for d in spec.defaults if isinstance(d, types.LambdaType))\n\n    return info\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\ninspect, types, pandas, numpy, json\n```python\nimport inspect\nimport types\nimport pandas as pd\nimport numpy as np\nimport json\n\ndef get_function_info_json(f):\n    \"\"\"\n    Get the specifications of a function `f` as a JSON string.\n\n    Parameters:\n    f (function): The function to inspect.\n\n    Returns:\n    str: A JSON string with function specifications.\n\n    Requirements:\n    - inspect\n    - types\n    - pandas\n    - numpy\n    - json\n\n    Example:\n    >>> def f(x): return 2*x\n    >>> get_function_info_json(f)\n    \"\"\"\n    spec = inspect.getfullargspec(f)\n\n    info = {\n        'function_name': f.__name__,\n        'args': spec.args,\n        'defaults': spec.defaults,\n        'annotations': spec.annotations,\n        'is_lambda': isinstance(f, types.LambdaType)\n    }\n\n    return json.dumps(info)\n```\n"}
{"seed": "{\"task_id\": 206151, \"prompt\": \"def f_206151(cmd):\\n\\t\", \"suffix\": \"\\n\\treturn result\", \"canonical_solution\": \"result = os.popen(cmd, 'r', 1)\", \"test_start\": \"\\nimport os\\nfrom unittest.mock import Mock\\n\\ndef check(candidate):\", \"test\": [\"\\n    os.popen = Mock(return_value = True)\\n    assert candidate('python3') == True\\n\"], \"entry_point\": \"f_206151\", \"intent\": \"\\u041f\\u043e\\u043b\\u0443\\u0447\\u0438\\u0442\\u044c \\u043f\\u043e\\u0442\\u043e\\u043a \\u0432\\u044b\\u0432\\u043e\\u0434\\u0430 \\u0434\\u0430\\u043d\\u043d\\u044b\\u0445 \\u043a\\u043e\\u043d\\u0441\\u043e\\u043b\\u044c\\u043d\\u043e\\u0439 \\u043a\\u043e\\u043c\\u0430\\u043d\\u0434\\u044b `cmd` \\u0432 \\u0444\\u0430\\u0439\\u043b\\u043e\\u0432\\u044b\\u0439 \\u043e\\u0431\\u044a\\u0435\\u043a\\u0442 `result`\", \"library\": [\"os\"]}", "generation": "Scenario 1: \nos, subprocess, time, datetime\n```python\nimport os\nimport subprocess\nfrom time import sleep\nfrom datetime import datetime\n\n# Constants\nLOG_FILE = 'log.txt'\n\ndef monitor_command_execution(cmd):\n    \"\"\"\n    Execute a command using subprocess, monitor its execution every second, and write the status \n    (whether it's still running or finished) along with the current timestamp to a log file.\n\n    Parameters:\n    cmd (str): The command to be executed.\n\n    Returns:\n    str: The final status of the command execution.\n\n    Requirements:\n    - os\n    - subprocess\n    - time\n    - datetime\n\n    Example:\n    >>> monitor_command_execution('ping -c 5 www.google.com')\n    \"\"\"\n    proc = subprocess.Popen(cmd, shell=True)\n    with open(LOG_FILE, 'w') as f:\n        while proc.poll() is None:\n            f.write(f'{datetime.now()}: Command \"{cmd}\" is still running\\n')\n            sleep(1)\n        f.write(f'{datetime.now()}: Command \"{cmd}\" has finished with exit code {proc.returncode}\\n')\n\n    return 'Command has finished execution'\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nshutil, os, tarfile, time\n```python\nimport shutil\nimport os\nimport tarfile\nfrom time import time\n\n# Constants\nBACKUP_DIR = './backup/'\n\ndef backup_files(file_paths):\n    \"\"\"\n    Backup a list of files into a tar.gz file. Each backup file should be named with the current timestamp.\n\n    Parameters:\n    file_paths (list): A list of file paths to be backed up.\n\n    Returns:\n    str: The path to the backup file.\n\n    Requirements:\n    - shutil\n    - os\n    - tarfile\n    - time\n\n    Example:\n    >>> backup_files(['/path/to/file1.txt', '/path/to/file2.txt'])\n    \"\"\"\n    if not os.path.exists(BACKUP_DIR):\n        os.makedirs(BACKUP_DIR)\n\n    backup_file = f'{BACKUP_DIR}backup_{int(time())}.tar.gz'\n\n    with tarfile.open(backup_file, 'w:gz') as tar:\n        for file in file_paths:\n            tar.add(file)\n\n    return backup_file\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, csv, pathlib, datetime\n```python\nimport os\nimport csv\nfrom pathlib import Path\nfrom datetime import datetime\n\n# Constants\nCSV_DIR = './csv_files/'\n\ndef summarize_csv_files():\n    \"\"\"\n    Summarize all the csv files in a directory, including the file name, file size, and last modified time.\n\n    Returns:\n    list: A list of tuples, each tuple contains the file name, file size, and last modified time.\n\n    Requirements:\n    - os\n    - csv\n    - pathlib\n    - datetime\n\n    Example:\n    >>> summarize_csv_files()\n    \"\"\"\n    csv_files = Path(CSV_DIR).glob('*.csv')\n    summary = []\n    \n    for csv_file in csv_files:\n        size = os.path.getsize(csv_file)\n        modified_time = datetime.fromtimestamp(os.path.getmtime(csv_file))\n        summary.append((csv_file.name, size, modified_time))\n\n    return summary\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, subprocess, shlex, time\n```python\nimport os\nimport subprocess\nimport shlex\nfrom time import sleep\n\n# Constants\nSLEEP_TIME = 1\n\ndef run_and_monitor_command(cmd):\n    \"\"\"\n    Run a command and monitor its output in real-time. If the command runs for more than 5 seconds, terminate it.\n\n    Parameters:\n    cmd (str): The command to be executed.\n\n    Returns:\n    str: The output of the command or the termination message.\n\n    Requirements:\n    - os\n    - subprocess\n    - shlex\n    - time\n\n    Example:\n    >>> run_and_monitor_command('ls -l')\n    \"\"\"\n    process = subprocess.Popen(shlex.split(cmd), stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n    start_time = time()\n    \n    while True:\n        output = process.stdout.readline().decode()\n        if output == '' and process.poll() is not None:\n            break\n        if time() - start_time > 5:\n            process.terminate()\n            return 'Command execution time exceeded 5 seconds. The command has been terminated.'\n        if output:\n            print(output.strip())\n\n    return_code = process.poll()\n    return f\"Command exited with return code: {return_code}\"\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, subprocess, time, datetime\n```python\nimport os\nimport subprocess\nfrom time import sleep\nfrom datetime import datetime\n\n# Constants\nLOG_FILE = 'process_log.txt'\n\ndef log_process_output(cmd):\n    \"\"\"\n    Execute a command, log its output to a file along with the current timestamp.\n\n    Parameters:\n    cmd (str): The command to be executed.\n\n    Returns:\n    str: The final status of the command execution.\n\n    Requirements:\n    - os\n    - subprocess\n    - time\n    - datetime\n\n    Example:\n    >>> log_process_output('ls -l')\n    \"\"\"\n    with open(LOG_FILE, 'w') as f:\n        process = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n        while True:\n            output = process.stdout.readline().decode()\n            if output == '' and process.poll() is not None:\n                break\n            if output:\n                f.write(f'{datetime.now()}: {output}')\n        return_code = process.poll()\n        f.write(f'Command exited with return code: {return_code}\\n')\n\n    return 'Command execution and logging completed'\n```"}
{"seed": "{\"task_id\": 118927, \"prompt\": \"def f_118927(my_tuple):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"list(my_tuple)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate((1, 2, 3)) == [1, 2, 3]\\n\", \"\\n    assert candidate((1, \\\"222\\\", 3)) == [1, \\\"222\\\", 3]\\n\"], \"entry_point\": \"f_118927\", \"intent\": \"\\u041f\\u0440\\u0435\\u043e\\u0431\\u0440\\u0430\\u0437\\u043e\\u0432\\u0430\\u0442\\u044c \\u043a\\u043e\\u0440\\u0442\\u0435\\u0436 `my_tuple` \\u0432 \\u0441\\u043f\\u0438\\u0441\\u043e\\u043a\", \"library\": []}", "generation": "Scenario 1: \npandas, datetime, random, matplotlib.pyplot\n```python\nimport pandas as pd\nfrom datetime import datetime\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nSTOCKS = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA']\nDAYS = 30\n\ndef generate_stock_prices(start_date, days=DAYS):\n    \"\"\"\n    Generate a report of stock prices for a list of companies over a specified number of days \n    starting from a given date.\n    \n    Parameters:\n    start_date (str): The start date in 'yyyy-mm-dd' format.\n    days (int): The number of days for which to generate data.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with stock prices for the companies.\n    \n    Requirements:\n    - pandas\n    - datetime\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> report = generate_stock_prices('2022-01-01', 7)\n    >>> print(report)\n    >>> report.plot(x='Date', y=STOCKS)\n    \"\"\"\n    date = datetime.strptime(start_date, '%Y-%m-%d')\n    \n    data = {'Date': [date + timedelta(days=i) for i in range(days)]}\n    \n    for stock in STOCKS:\n        data[stock] = [randint(80, 200) for _ in range(days)]\n\n    df = pd.DataFrame(data)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nrequests, json, pandas\n```python\nimport requests\nimport json\nimport pandas as pd\n\n# Constants\nURL = 'https://api.github.com/repos/{owner}/{repo}/commits'\n\ndef fetch_github_commits(owner, repo, since_date):\n    \"\"\"\n    Fetch the list of commits from a GitHub repository since a specified date.\n    \n    Parameters:\n    owner (str): The owner of the repository.\n    repo (str): The name of the repository.\n    since_date (str): The date (in 'yyyy-mm-dd' format) since which to fetch commits.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with commits data.\n    \n    Requirements:\n    - requests\n    - json\n    - pandas\n    \n    Example:\n    >>> commits = fetch_github_commits('pandas-dev', 'pandas', '2022-01-01')\n    >>> print(commits)\n    \"\"\"\n    response = requests.get(URL.format(owner=owner, repo=repo), params={'since': since_date})\n    \n    data = json.loads(response.text)\n    \n    df = pd.json_normalize(data)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, scipy.stats, matplotlib.pyplot\n```python\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\n# Constants\nSAMPLE_SIZE = 1000\n\ndef plot_normal_distribution(mu, sigma, sample_size=SAMPLE_SIZE):\n    \"\"\"\n    Generate a sample from a normal distribution with specified mean and standard deviation \n    and plot its histogram and probability density function.\n    \n    Parameters:\n    mu (float): The mean of the normal distribution.\n    sigma (float): The standard deviation of the normal distribution.\n    sample_size (int): The size of the sample to generate.\n    \n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n    \n    Example:\n    >>> plot_normal_distribution(0, 1)\n    \"\"\"\n    sample = np.random.normal(mu, sigma, sample_size)\n    \n    plt.hist(sample, bins=30, density=True)\n\n    x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n    plt.plot(x, stats.norm.pdf(x, mu, sigma))\n\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, matplotlib.pyplot\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nSAMPLE_SIZE = 1000\n\ndef plot_random_walk(sample_size=SAMPLE_SIZE):\n    \"\"\"\n    Generate a random walk and plot it.\n    \n    Parameters:\n    sample_size (int): The size of the sample to generate.\n    \n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> plot_random_walk(500)\n    \"\"\"\n    steps = np.random.choice([-1, 1], size=sample_size)\n    walk = np.cumsum(steps)\n    \n    plt.plot(walk)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, sklearn.model_selection, sklearn.linear_model, sklearn.metrics\n```python\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n# Sample data\ndf = pd.DataFrame(data={'X': np.random.rand(100), 'Y': np.random.rand(100)})\n\ndef perform_linear_regression(df, test_size=0.2):\n    \"\"\"\n    Perform linear regression on a given DataFrame and return the mean squared error of the test set.\n    \n    Parameters:\n    df (DataFrame): The DataFrame containing the data.\n    test_size (float): The proportion of the DataFrame to include in the test split.\n    \n    Returns:\n    float: The mean squared error of the test set.\n    \n    Requirements:\n    - pandas\n    - sklearn.model_selection\n    - sklearn.linear_model\n    - sklearn.metrics\n    \n    Example:\n    >>> mse = perform_linear_regression(df)\n    >>> print(mse)\n    \"\"\"\n    X_train, X_test, Y_train, Y_test = train_test_split(df[['X']], df['Y'], test_size=test_size)\n    \n    model = LinearRegression()\n    model.fit(X_train, Y_train)\n    \n    Y_pred = model.predict(X_test)\n    \n    mse = mean_squared_error(Y_test, Y_pred)\n    \n    return mse\n```"}
{"seed": "{\"task_id\": 175425, \"prompt\": \"def f_175425(username, my_encode):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"os.environ(username).decode(my_encode)\", \"test_start\": \"\\nimport os\\nfrom unittest.mock import Mock\\n\\ndef check(candidate):\", \"test\": [\"\\n    os.environ = Mock(return_value = b'abc')\\n    assert candidate('username', 'utf-8') == 'abc'\\n\"], \"entry_point\": \"f_175425\", \"intent\": \"\\u0423\\u0441\\u0442\\u0430\\u043d\\u043e\\u0432\\u0438\\u0442\\u044c \\u043a\\u043e\\u0434\\u0438\\u0440\\u043e\\u0432\\u043a\\u0443 `my_encode` \\u0434\\u043b\\u044f \\u043f\\u0435\\u0440\\u0435\\u043c\\u0435\\u043d\\u043d\\u044b\\u0445 \\u043e\\u043a\\u0440\\u0443\\u0436\\u0435\\u043d\\u0438\\u044f \\u043f\\u043e\\u043b\\u044c\\u0437\\u043e\\u0432\\u0430\\u0442\\u0435\\u043b\\u044f `username`\", \"library\": [\"os\"]}", "generation": "Scenario 1:\nos, cryptography.fernet, base64, getpass\n```python\nimport os\nfrom cryptography.fernet import Fernet\nimport base64\nfrom getpass import getpass\n\nKEY = base64.urlsafe_b64encode(getpass(\"Enter a secret key: \").encode()).ljust(32, b'0')\n\ndef encode_decode_environment_variable(var_name, operation):\n    \"\"\"\n    Encode or decode an environment variable using Fernet symmetric encryption.\n    \n    Parameters:\n    var_name (str): The name of the environment variable.\n    operation (str): The operation to perform. Can be 'encode' or 'decode'.\n    \n    Returns:\n    str: The encoded or decoded value of the environment variable.\n\n    Requirements:\n    - os\n    - cryptography.fernet\n    - base64\n    - getpass\n\n    Example:\n    >>> os.environ['TEST'] = 'Hello, World!'\n    >>> encode_decode_environment_variable('TEST', 'encode')\n    'gAAAAABh...'\n    >>> encode_decode_environment_variable('TEST', 'decode')\n    'Hello, World!'\n    \"\"\"\n    cipher_suite = Fernet(KEY)\n    \n    if operation == 'encode':\n        encoded_text = cipher_suite.encrypt(os.environ[var_name].encode())\n    elif operation == 'decode':\n        encoded_text = cipher_suite.decrypt(os.environ[var_name].encode())\n    else:\n        raise ValueError(\"Operation must be 'encode' or 'decode'\")\n    \n    return encoded_text.decode()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, json, hashlib\n```python\nimport os\nimport json\nimport hashlib\n\ndef hash_environment_variables(var_names):\n    \"\"\"\n    Create a hash of the specified environment variables.\n\n    Parameters:\n    var_names (list): A list of environment variable names.\n\n    Returns:\n    str: A SHA256 hash of the environment variables.\n\n    Requirements:\n    - os\n    - json\n    - hashlib\n\n    Example:\n    >>> os.environ['VAR1'] = 'value1'\n    >>> os.environ['VAR2'] = 'value2'\n    >>> hash_environment_variables(['VAR1', 'VAR2'])\n    '6dcd4ce23d88e2ee95838f7b014b6284f0b9c9cdc76e1585b6c5b6e8e6f4b6b2'\n    \"\"\"\n    vars_dict = {var: os.environ[var] for var in var_names}\n    vars_json = json.dumps(vars_dict, sort_keys=True)\n\n    vars_hash = hashlib.sha256()\n    vars_hash.update(vars_json.encode())\n\n    return vars_hash.hexdigest()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, argparse, sys\n```python\nimport os\nimport argparse\nimport sys\n\ndef parse_arguments_and_set_environment():\n    \"\"\"\n    Parse command line arguments and set them as environment variables.\n\n    Requirements:\n    - os\n    - argparse\n    - sys\n\n    Example:\n    >>> sys.argv = ['script.py', '--var1', 'value1', '--var2', 'value2']\n    >>> parse_arguments_and_set_environment()\n    >>> os.environ['var1']\n    'value1'\n    >>> os.environ['var2']\n    'value2'\n    \"\"\"\n    parser = argparse.ArgumentParser()\n\n    for arg in sys.argv[1:]:\n        if arg.startswith('--'):\n            parser.add_argument(arg)\n\n    args = parser.parse_args()\n\n    for arg in vars(args):\n        os.environ[arg] = getattr(args, arg)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, pathlib, json\n```python\nimport os\nfrom pathlib import Path\nimport json\n\nCONFIG_FILE = Path(os.getenv(\"CONFIG_FILE\", \"/etc/myapp/config.json\"))\n\ndef load_config_to_environment():\n    \"\"\"\n    Load a configuration file and set its contents as environment variables.\n\n    Requirements:\n    - os\n    - pathlib.Path\n    - json\n\n    Example:\n    >>> os.environ['CONFIG_FILE'] = '/path/to/config.json'\n    >>> load_config_to_environment()\n    >>> os.environ['var1']\n    'value1'\n    >>> os.environ['var2']\n    'value2'\n    \"\"\"\n    with CONFIG_FILE.open() as f:\n        config = json.load(f)\n\n    for key, value in config.items():\n        os.environ[key] = str(value)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, psutil\n```python\nimport os\nimport psutil\n\ndef get_environment_variable_usage(pid):\n    \"\"\"\n    Get the memory usage of the process with the given PID.\n\n    Parameters:\n    pid (int): The PID of the process.\n\n    Returns:\n    str: A string representing the memory usage of the process.\n\n    Requirements:\n    - os\n    - psutil\n\n    Example:\n    >>> get_environment_variable_usage(os.getpid())\n    'Memory usage: 28.6 MB'\n    \"\"\"\n    process = psutil.Process(pid)\n    mem_info = process.memory_info()\n\n    return f\"Memory usage: {mem_info.rss / 1024 / 1024:.1f} MB\"\n```"}
{"seed": "{\"task_id\": 141157, \"prompt\": \"def f_141157(my_func):\\n\\t\", \"suffix\": \"\\n\\treturn my_var\", \"canonical_solution\": \"my_var = my_func()\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    def my_func(): return -1\\n    assert candidate(my_func) == -1\\n\"], \"entry_point\": \"f_141157\", \"intent\": \"\\u041f\\u0440\\u0438\\u0441\\u0432\\u043e\\u0438\\u0442\\u044c \\u043f\\u0435\\u0440\\u0435\\u043c\\u0435\\u043d\\u043d\\u043e\\u0439 `my_var` \\u0440\\u0435\\u0437\\u0443\\u043b\\u044c\\u0442\\u0430\\u0442 \\u0440\\u0430\\u0431\\u043e\\u0442\\u044b \\u0444\\u0443\\u043d\\u043a\\u0446\\u0438\\u0438 `my_func`\", \"library\": []}", "generation": "Scenario 1:\nnumpy, pandas, random, statistics\n```python\nimport numpy as np\nimport pandas as pd\nfrom random import randint\nfrom statistics import mean\n\ndef generate_random_data_and_compute_mean(my_func):\n    \"\"\"\n    Generate a pandas DataFrame with random numeric data using a given function, \n    and compute the mean of the data.\n\n    Parameters:\n    my_func (function): A function that generates a random number.\n\n    Returns:\n    float: The mean of the generated data.\n\n    Requirements:\n    - pandas\n    - numpy\n    - random\n    - statistics\n\n    Example:\n    >>> def my_func(): return randint(0, 100)\n    >>> mean_value = generate_random_data_and_compute_mean(my_func)\n    >>> print(mean_value)\n    \"\"\"\n    data = {'Values': [my_func() for _ in range(100)]}\n    df = pd.DataFrame(data)\n    mean_value = mean(df['Values'])\n\n    return mean_value\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, datetime, time, pytz\n```python\nimport os\nfrom datetime import datetime\nimport time\nimport pytz\n\ndef get_file_creation_time_in_utc(my_func):\n    \"\"\"\n    Create a file using a given function and get the file creation time in UTC.\n\n    Parameters:\n    my_func (function): A function that creates a file.\n\n    Returns:\n    datetime: The file creation time in UTC.\n\n    Requirements:\n    - os\n    - datetime\n    - time\n    - pytz\n\n    Example:\n    >>> def my_func(): open('test_file.txt', 'w').close()\n    >>> creation_time = get_file_creation_time_in_utc(my_func)\n    >>> print(creation_time)\n    \"\"\"\n    my_func()\n    creation_time = os.path.getctime('test_file.txt')\n    creation_time = datetime.fromtimestamp(creation_time, tz=pytz.UTC)\n\n    return creation_time\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, scipy.optimize, matplotlib.pyplot\n```python\nimport numpy as np\nfrom scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\n\ndef fit_curve_to_data(my_func):\n    \"\"\"\n    Generate data using a given function and fit a curve to the data.\n\n    Parameters:\n    my_func (function): A function that generates y values for a given x.\n\n    Returns:\n    function: The optimized function.\n\n    Requirements:\n    - numpy\n    - scipy.optimize\n    - matplotlib.pyplot\n\n    Example:\n    >>> def my_func(x): return x**2\n    >>> optimized_func = fit_curve_to_data(my_func)\n    >>> x = np.linspace(-10, 10, 100)\n    >>> plt.plot(x, optimized_func(x))\n    \"\"\"\n    x = np.linspace(-10, 10, 100)\n    y = my_func(x)\n    popt, _ = curve_fit(my_func, x, y)\n\n    def optimized_func(x): return my_func(x, *popt)\n\n    return optimized_func\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nrequests, bs4, re\n```python\nimport requests\nfrom bs4 import BeautifulSoup\nimport re\n\ndef extract_links_from_webpage(my_func):\n    \"\"\"\n    Extract all links from a webpage using a given function that returns the webpage URL.\n\n    Parameters:\n    my_func (function): A function that returns a webpage URL.\n\n    Returns:\n    list: A list of links extracted from the webpage.\n\n    Requirements:\n    - requests\n    - bs4\n    - re\n\n    Example:\n    >>> def my_func(): return 'https://www.example.com'\n    >>> links = extract_links_from_webpage(my_func)\n    >>> print(links)\n    \"\"\"\n    url = my_func()\n    response = requests.get(url)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    links = [a['href'] for a in soup.find_all('a', href=True) if re.match('http', a['href'])]\n\n    return links\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, sklearn.linear_model\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\ndef create_linear_model(my_func):\n    \"\"\"\n    Create a linear regression model using data generated by a given function.\n\n    Parameters:\n    my_func (function): A function that generates a pandas DataFrame.\n\n    Returns:\n    LinearRegression: The trained linear regression model.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.linear_model\n\n    Example:\n    >>> def my_func(): return pd.DataFrame({'x': np.random.rand(100), 'y': np.random.rand(100)})\n    >>> model = create_linear_model(my_func)\n    \"\"\"\n    df = my_func()\n    X = df['x'].values.reshape(-1, 1)\n    y = df['y'].values\n    model = LinearRegression().fit(X, y)\n\n    return model\n```"}
{"seed": "{\"task_id\": 209059, \"prompt\": \"def f_209059(my_symbol, my_string):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"re.sub(' +', my_symbol, my_string)\", \"test_start\": \"\\nimport re \\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('?','abc def  jkl') == 'abc?def?jkl'\\n\"], \"entry_point\": \"f_209059\", \"intent\": \"\\u0417\\u0430\\u043c\\u0435\\u043d\\u0438\\u0442\\u044c \\u043f\\u0440\\u043e\\u0431\\u0435\\u043b\\u044b \\u0432 \\u0441\\u0442\\u0440\\u043e\\u043a\\u0435 `my_string` \\u043d\\u0430 \\u0441\\u0438\\u043c\\u0432\\u043e\\u043b `my_symbol`\", \"library\": [\"re\"]}", "generation": "Scenario 1: \npandas, re, matplotlib, random\n```python\nimport pandas as pd\nimport re\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nSYMBOLS = ['*', '#', '+', '-', '@', '&', '%', '$', '!', '~', '^']\n\ndef generate_and_plot_symbol_replaced_string(my_string):\n    \"\"\"\n    Replace spaces in a string with a random symbol from a predefined list, \n    generate a frequency plot of the symbols in the replaced string.\n\n    Parameters:\n    my_string (str): The input string.\n    \n    Returns:\n    str: The string with spaces replaced by a random symbol.\n    \n    Requirements:\n    - pandas\n    - re\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> replaced_string = generate_and_plot_symbol_replaced_string('Hello World! How are you?')\n    >>> print(replaced_string)\n    \"\"\"\n    my_symbol = SYMBOLS[randint(0, len(SYMBOLS) - 1)]\n    replaced_string = re.sub(' +', my_symbol, my_string)\n\n    pd.Series(list(replaced_string)).value_counts().plot(kind='bar')\n\n    return replaced_string\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nnumpy, re, string, random\n```python\nimport numpy as np\nimport re\nfrom string import punctuation\nfrom random import choice\n\ndef replace_and_count_punctuation(my_string):\n    \"\"\"\n    Replace all spaces in a string with a random punctuation mark \n    and count the occurrence of each punctuation mark in the replaced string.\n\n    Parameters:\n    my_string (str): The input string.\n\n    Returns:\n    dict: A dictionary with the count of each punctuation mark.\n    \n    Requirements:\n    - numpy\n    - re\n    - string\n    - random\n\n    Example:\n    >>> counts = replace_and_count_punctuation('Hello World! How are you?')\n    >>> print(counts)\n    \"\"\"\n    my_symbol = choice(list(punctuation))\n    replaced_string = re.sub(' +', my_symbol, my_string)\n\n    counts = {punc: replaced_string.count(punc) for punc in punctuation}\n\n    return counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nre, Math, random, collections\n```python\nimport re\nimport math\nfrom random import randint\nfrom collections import Counter\n\n# Constants\nSYMBOLS = ['*', '#', '+', '-', '@', '&', '%', '$', '!', '~', '^']\n\ndef count_and_calc_log(my_string):\n    \"\"\"\n    Replace all spaces in a string with a random symbol from a predefined list, \n    count the occurrence of each symbol and calculate the logarithm of each count.\n\n    Parameters:\n    my_string (str): The input string.\n\n    Returns:\n    dict: A dictionary with the logarithm of the count of each symbol.\n    \n    Requirements:\n    - re\n    - math\n    - random\n    - collections\n\n    Example:\n    >>> log_counts = count_and_calc_log('Hello World! How are you?')\n    >>> print(log_counts)\n    \"\"\"\n    my_symbol = SYMBOLS[randint(0, len(SYMBOLS) - 1)]\n    replaced_string = re.sub(' +', my_symbol, my_string)\n\n    counts = Counter(replaced_string)\n    log_counts = {symbol: math.log(count) for symbol, count in counts.items()}\n\n    return log_counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nre, numpy, matplotlib, random\n```python\nimport re\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import choice\n\n# Constants\nSYMBOLS = ['*', '#', '+', '-', '@', '&', '%', '$', '!', '~', '^']\n\ndef replace_and_plot_histogram(my_string):\n    \"\"\"\n    Replace all spaces in a string with a random symbol from a predefined list, \n    and plot a histogram of the ASCII values of the characters in the replaced string.\n\n    Parameters:\n    my_string (str): The input string.\n\n    Returns:\n    str: The string with spaces replaced by a random symbol.\n    \n    Requirements:\n    - re\n    - numpy\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> replaced_string = replace_and_plot_histogram('Hello World! How are you?')\n    >>> print(replaced_string)\n    \"\"\"\n    my_symbol = SYMBOLS[randint(0, len(SYMBOLS) - 1)]\n    replaced_string = re.sub(' +', my_symbol, my_string)\n\n    ascii_values = [ord(char) for char in replaced_string]\n    plt.hist(ascii_values, bins=np.arange(32, 127))\n\n    return replaced_string\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nre, json, os, random\n```python\nimport re\nimport json\nimport os\nfrom random import choice\n\n# Constants\nSYMBOLS = ['*', '#', '+', '-', '@', '&', '%', '$', '!', '~', '^']\n\ndef replace_write_and_read_back(my_string, filename):\n    \"\"\"\n    Replace all spaces in a string with a random symbol from a predefined list, \n    write the replaced string to a JSON file, then read back the string from the file.\n\n    Parameters:\n    my_string (str): The input string.\n    filename (str): The name of the JSON file.\n\n    Returns:\n    str: The string read back from the file.\n    \n    Requirements:\n    - re\n    - json\n    - os\n    - random\n\n    Example:\n    >>> read_string = replace_write_and_read_back('Hello World! How are you?', 'test.json')\n    >>> print(read_string)\n    \"\"\"\n    my_symbol = SYMBOLS[randint(0, len(SYMBOLS) - 1)]\n    replaced_string = re.sub(' +', my_symbol, my_string)\n\n    with open(filename, 'w') as f:\n        json.dump(replaced_string, f)\n\n    with open(filename, 'r') as f:\n        read_string = json.load(f)\n\n    os.remove(filename)\n\n    return read_string\n```"}
{"seed": "{\"task_id\": 256777, \"prompt\": \"def f_256777(old_words, new_words, s):\\n\\t\", \"suffix\": \"\\n\\treturn s\", \"canonical_solution\": \"\\n\\twords = dict(zip(old_words, new_words))\\n\\tfor word in words:\\n\\t\\ts = re.sub(r\\\"{}\\\".format(word), words[word], s)\\n\", \"test_start\": \"\\nimport re \\n\\ndef check(candidate):\", \"test\": [\"\\n    s = 'I do not love you anymore. I hate you and shall never pardon for this crime.'\\n    o = ['love', 'hate', 'pardon']\\n    n = ['like', 'detest', 'forgive']\\n    assert candidate(o, n, s) == 'I do not like you anymore. I detest you and shall never forgive for this crime.'\\n\"], \"entry_point\": \"f_256777\", \"intent\": \"\\u0417\\u0430\\u043c\\u0435\\u043d\\u0438\\u0442\\u044c \\u0441\\u043f\\u0438\\u0441\\u043e\\u043a \\u0441\\u043b\\u043e\\u0432 `old_words` \\u043d\\u0430 \\u0441\\u043f\\u0438\\u0441\\u043e\\u043a \\u0441\\u043b\\u043e\\u0432 `new_words` \\u0432 \\u0441\\u0442\\u0440\\u043e\\u043a\\u0435 `s`\", \"library\": [\"re\"]}", "generation": "Scenario 1:\npandas, numpy, nltk, re\n```python\nimport pandas as pd\nimport numpy as np\nimport nltk\nfrom nltk.corpus import stopwords\nimport re\n\n# Constants\nSTOPWORDS = set(stopwords.words('english'))\n\ndef replace_words_in_dataframe(df, old_words, new_words):\n    \"\"\"\n    Replace a list of words `old_words` with a list of words `new_words` in a pandas DataFrame `df` \n    and count the frequency of each word in 'new_words'.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame in which to replace the words.\n    old_words (list): The list of words to replace.\n    new_words (list): The list of words to replace with.\n\n    Returns:\n    DataFrame: A pandas DataFrame with word and its frequency.\n\n    Requirements:\n    - pandas\n    - numpy\n    - nltk.corpus.stopwords\n    - re\n\n    Example:\n    >>> old_words = ['love', 'hate', 'pardon']\n    >>> new_words = ['like', 'detest', 'forgive']\n    >>> df = pd.DataFrame({'text': ['I do not love you anymore. I hate you and shall never pardon for this crime.']})\n    >>> replace_words_in_dataframe(df, old_words, new_words)\n    \"\"\"\n    word_map = dict(zip(old_words, new_words))\n    for word in word_map:\n        df['text'] = df['text'].replace(r'\\b{}\\b'.format(word), word_map[word], regex=True)\n\n    word_counts = pd.Series(np.concatenate([x.split() for x in df['text'].values])).value_counts()\n    word_counts = word_counts[~word_counts.index.isin(STOPWORDS)]\n\n    return word_counts.to_frame().reset_index().rename(columns={'index': 'Word', 0: 'Frequency'})\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\njson, re, os, shutil\n```python\nimport json\nimport re\nimport os\nimport shutil\n\n# Constants\nJSON_FILE_PATH = './json_files/'\n\ndef replace_words_in_files(old_words, new_words):\n    \"\"\"\n    Replace a list of words `old_words` with a list of words `new_words` in all JSON files in a directory \n    and save the updated files in a new directory.\n\n    Parameters:\n    old_words (list): The list of words to replace.\n    new_words (list): The list of words to replace with.\n\n    Returns:\n    str: The path of new directory.\n\n    Requirements:\n    - json\n    - re\n    - os\n    - shutil\n\n    Example:\n    >>> old_words = ['love', 'hate', 'pardon']\n    >>> new_words = ['like', 'detest', 'forgive']\n    >>> replace_words_in_files(old_words, new_words)\n    \"\"\"\n    word_map = dict(zip(old_words, new_words))\n    new_dir = JSON_FILE_PATH + 'new_files/'\n    os.makedirs(new_dir, exist_ok=True)\n\n    for file_name in os.listdir(JSON_FILE_PATH):\n        if file_name.endswith('.json'):\n            with open(JSON_FILE_PATH + file_name, 'r') as file:\n                data = json.load(file)\n                for key, value in data.items():\n                    if isinstance(value, str):\n                        for word in word_map:\n                            value = re.sub(r'\\b{}\\b'.format(word), word_map[word], value)\n                        data[key] = value\n\n            with open(new_dir + file_name, 'w') as file:\n                json.dump(data, file)\n\n    return new_dir\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnltk, re, matplotlib, collections\n```python\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport re\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n\n# Constants\nSTOPWORDS = set(stopwords.words('english'))\n\ndef text_analysis(old_words, new_words, text):\n    \"\"\"\n    Replace a list of words `old_words` with a list of words `new_words` in a text, \n    then perform text analysis to find the frequency of each word and plot a bar graph.\n\n    Parameters:\n    old_words (list): The list of words to replace.\n    new_words (list): The list of words to replace with.\n    text (str): The text in which to replace the words.\n\n    Returns:\n    None\n\n    Requirements:\n    - nltk.corpus.stopwords\n    - nltk.tokenize.word_tokenize\n    - re\n    - matplotlib.pyplot\n    - collections.Counter\n\n    Example:\n    >>> old_words = ['love', 'hate', 'pardon']\n    >>> new_words = ['like', 'detest', 'forgive']\n    >>> text = 'I do not love you anymore. I hate you and shall never pardon for this crime.'\n    >>> text_analysis(old_words, new_words, text)\n    \"\"\"\n    word_map = dict(zip(old_words, new_words))\n    for word in word_map:\n        text = re.sub(r'\\b{}\\b'.format(word), word_map[word], text)\n\n    words = word_tokenize(text)\n    words = [word for word in words if word.isalnum()]\n    word_counts = Counter(words)\n\n    plt.bar(word_counts.keys(), word_counts.values())\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, re, sklearn\n```python\nimport pandas as pd\nimport numpy as np\nimport re\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Constants\nMAX_FEATURES = 5000\n\ndef replace_words_get_tfidf(old_words, new_words, texts):\n    \"\"\"\n    Replace a list of words `old_words` with a list of words `new_words` in a list of texts,\n    then calculate the TF-IDF scores of the words in the texts.\n\n    Parameters:\n    old_words (list): The list of words to replace.\n    new_words (list): The list of words to replace with.\n    texts (list): The list of texts in which to replace the words.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the TF-IDF scores of the words.\n\n    Requirements:\n    - pandas\n    - numpy\n    - re\n    - sklearn.feature_extraction.text.TfidfVectorizer\n\n    Example:\n    >>> old_words = ['love', 'hate', 'pardon']\n    >>> new_words = ['like', 'detest', 'forgive']\n    >>> texts = ['I do not love you anymore.', 'I hate you and shall never pardon for this crime.']\n    >>> replace_words_get_tfidf(old_words, new_words, texts)\n    \"\"\"\n    word_map = dict(zip(old_words, new_words))\n    for i in range(len(texts)):\n        for word in word_map:\n            texts[i] = re.sub(r'\\b{}\\b'.format(word), word_map[word], texts[i])\n\n    vectorizer = TfidfVectorizer(max_features=MAX_FEATURES)\n    tfidf_matrix = vectorizer.fit_transform(texts)\n    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n\n    return tfidf_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnltk, re, wordcloud, matplotlib\n```python\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport re\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\n# Constants\nSTOPWORDS = set(stopwords.words('english'))\n\ndef replace_words_generate_wordcloud(old_words, new_words, text):\n    \"\"\"\n    Replace a list of words `old_words` with a list of words `new_words` in a text, \n    then generate a word cloud of the words in the text.\n\n    Parameters:\n    old_words (list): The list of words to replace.\n    new_words (list): The list of words to replace with.\n    text (str): The text in which to replace the words.\n\n    Returns:\n    None\n\n    Requirements:\n    - nltk.corpus.stopwords\n    - nltk.tokenize.word_tokenize\n    - re\n    - wordcloud.WordCloud\n    - matplotlib.pyplot\n\n    Example:\n    >>> old_words = ['love', 'hate', 'pardon']\n    >>> new_words = ['like', 'detest', 'forgive']\n    >>> text = 'I do not love you anymore. I hate you and shall never pardon for this crime.'\n    >>> replace_words_generate_wordcloud(old_words, new_words, text)\n    \"\"\"\n    word_map = dict(zip(old_words, new_words))\n    for word in word_map:\n        text = re.sub(r'\\b{}\\b'.format(word), word_map[word], text)\n\n    words = word_tokenize(text)\n    words = ' '.join([word for word in words if word.isalnum() and word not in STOPWORDS])\n\n    wordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                stopwords = STOPWORDS, \n                min_font_size = 10).generate(words)\n\n    plt.figure(figsize = (8, 8), facecolor = None) \n    plt.imshow(wordcloud) \n    plt.axis(\"off\") \n    plt.tight_layout(pad = 0) \n    plt.show()\n```\n"}
{"seed": "{\"task_id\": 112581, \"prompt\": \"def f_112581(tree, val):\\n\\t\", \"suffix\": \"\\n\\treturn node\", \"canonical_solution\": \"\\n\\tdef search(root, key):\\n\\t\\tif root is None or root.val == key:\\n\\t\\t\\treturn root\\n\\t\\tif root.val < key:\\n\\t\\t\\treturn search(root.right,key)\\n\\t\\treturn search(root.left,key)\\n\\tnode = search(tree, val)\\n\", \"test_start\": \"\\nclass Node:\\n    def __init__(self, val):\\n        self.val = val\\n        self.left = None\\n        self.right = None\\n\\ndef check(candidate):\", \"test\": [\"\\n    root7 = Node(7)\\n    root3 = Node(3)\\n    root11 = Node(11)\\n    root4 = Node(4)\\n    root7.left = root3\\n    root7.right = root11\\n    root3.right = root4\\n    assert candidate(root7, 4) == root4\\n\"], \"entry_point\": \"f_112581\", \"intent\": \"\\u041d\\u0430\\u0439\\u0442\\u0438 \\u0443\\u0437\\u0435\\u043b `node` \\u0440\\u0430\\u0432\\u043d\\u044b\\u0439 \\u0437\\u043d\\u0430\\u0447\\u0435\\u043d\\u0438\\u044e `val` \\u0432 \\u0431\\u0438\\u043d\\u0430\\u0440\\u043d\\u043e\\u043c \\u0434\\u0435\\u0440\\u0435\\u0432\\u0435 `tree`\", \"library\": []}", "generation": "Scenario 1: \ncollections, random, networkx, matplotlib\n```python\nfrom collections import defaultdict\nimport random\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# Constants\nNODE_VALUES = list(range(1, 101))  # Node values for the binary tree\nEDGE_COLORS = ['r', 'g', 'b', 'c', 'm', 'y', 'k']  # Colors for the edges\n\n# Binary tree node\nclass Node:\n    def __init__(self, val):\n        self.val = val\n        self.left = None\n        self.right = None\n\ndef f_112581_v1(tree, val):\n    \"\"\"\n    Find a node equal to 'val' in binary tree 'tree'. If found, \n    build a graph using networkx and plot the graph with matplotlib.\n\n    Parameters:\n    tree (Node): The root node of the binary tree.\n    val (int): The value to be searched in the tree.\n\n    Returns:\n    Node: The node equal to 'val'. If not found, return None.\n    \n    Requirements:\n    - collections\n    - random\n    - networkx\n    - matplotlib.pyplot\n\n    Example:\n    >>> root = Node(50)\n    >>> # Build the binary tree...\n    >>> found_node = f_112581_v1(root, 25)\n    \"\"\"\n    def search(root, key):\n        if root is None or root.val == key:\n            return root\n        if root.val < key:\n            return search(root.right, key)\n        return search(root.left, key)\n\n    node = search(tree, val)\n\n    if node is not None:\n        edge_list = defaultdict(list)\n\n        def traverse(root, parent=None):\n            if root is None:\n                return\n            if parent is not None:\n                edge_list[parent].append(root.val)\n            traverse(root.left, root.val)\n            traverse(root.right, root.val)\n\n        traverse(tree)\n\n        G = nx.Graph(edge_list)\n        edge_colors = [random.choice(EDGE_COLORS) for _ in range(G.number_of_edges())]\n\n        nx.draw(G, with_labels=True, edge_color=edge_colors)\n        plt.show()\n\n    return node\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nnumpy, scipy.stats, pandas, matplotlib\n```python\nimport numpy as np\nimport scipy.stats as stats\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nNODE_VALUES = list(range(1, 101))  # Node values for the binary tree\n\n# Binary tree node\nclass Node:\n    def __init__(self, val):\n        self.val = val\n        self.left = None\n        self.right = None\n\ndef f_112581_v2(tree, val):\n    \"\"\"\n    Find a node equal to 'val' in binary tree 'tree'. If found, \n    calculate the depth of each node in the tree, build a histogram \n    showing the distribution of depths, and plot the histogram with matplotlib.\n\n    Parameters:\n    tree (Node): The root node of the binary tree.\n    val (int): The value to be searched in the tree.\n\n    Returns:\n    Node: The node equal to 'val'. If not found, return None.\n    \n    Requirements:\n    - numpy\n    - scipy.stats\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> root = Node(50)\n    >>> # Build the binary tree...\n    >>> found_node = f_112581_v2(root, 25)\n    \"\"\"\n    def search(root, key):\n        if root is None or root.val == key:\n            return root\n        if root.val < key:\n            return search(root.right, key)\n        return search(root.left, key)\n\n    node = search(tree, val)\n\n    if node is not None:\n        depths = []\n\n        def traverse(root, depth=0):\n            if root is None:\n                return\n            depths.append(depth)\n            traverse(root.left, depth+1)\n            traverse(root.right, depth+1)\n\n        traverse(tree)\n\n        depths_array = np.array(depths)\n        depths_df = pd.DataFrame(depths_array, columns=['Depth'])\n\n        plt.hist(depths_df['Depth'], bins='auto', density=True, alpha=0.7, rwidth=0.85)\n        plt.xlabel('Depth')\n        plt.ylabel('Density')\n        plt.title('Histogram of Node Depths')\n\n        plt.show()\n\n    return node\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nrandom, queue, matplotlib\n```python\nimport random\nimport queue\nimport matplotlib.pyplot as plt\n\n# Constants\nNODE_VALUES = list(range(1, 101))  # Node values for the binary tree\n\n# Binary tree node\nclass Node:\n    def __init__(self, val):\n        self.val = val\n        self.left = None\n        self.right = None\n\ndef f_112581_v3(tree, val):\n    \"\"\"\n    Find a node equal to 'val' in binary tree 'tree'. If found, \n    perform a breadth-first traversal of the tree and plot the \n    number of nodes at each level using matplotlib.\n\n    Parameters:\n    tree (Node): The root node of the binary tree.\n    val (int): The value to be searched in the tree.\n\n    Returns:\n    Node: The node equal to 'val'. If not found, return None.\n    \n    Requirements:\n    - random\n    - queue\n    - matplotlib.pyplot\n\n    Example:\n    >>> root = Node(50)\n    >>> # Build the binary tree...\n    >>> found_node = f_112581_v3(root, 25)\n    \"\"\"\n    def search(root, key):\n        if root is None or root.val == key:\n            return root\n        if root.val < key:\n            return search(root.right, key)\n        return search(root.left, key)\n\n    node = search(tree, val)\n\n    if node is not None:\n        q = queue.Queue()\n        q.put((tree, 0))\n        levels = []\n\n        while not q.empty():\n            node, level = q.get()\n            if node is not None:\n                levels.append(level)\n                q.put((node.left, level+1))\n                q.put((node.right, level+1))\n\n        plt.hist(levels, bins=max(levels)+1, alpha=0.7, rwidth=0.85)\n        plt.xlabel('Level')\n        plt.ylabel('Number of Nodes')\n        plt.title('Number of Nodes at Each Level')\n\n        plt.show()\n\n    return node\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, pandas, seaborn\n```python\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n# Constants\nNODE_VALUES = list(range(1, 101))  # Node values for the binary tree\n\n# Binary tree node\nclass Node:\n    def __init__(self, val):\n        self.val = val\n        self.left = None\n        self.right = None\n\ndef f_112581_v4(tree, val):\n    \"\"\"\n    Find a node equal to 'val' in binary tree 'tree'. If found, \n    calculate the balance factor (difference between left and right \n    subtree heights) for each node in the tree, build a DataFrame with \n    the balance factors, and plot a box plot of the balance factors using seaborn.\n\n    Parameters:\n    tree (Node): The root node of the binary tree.\n    val (int): The value to be searched in the tree.\n\n    Returns:\n    Node: The node equal to 'val'. If not found, return None.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - seaborn\n\n    Example:\n    >>> root = Node(50)\n    >>> # Build the binary tree...\n    >>> found_node = f_112581_v4(root, 25)\n    \"\"\"\n    def search(root, key):\n        if root is None or root.val == key:\n            return root\n        if root.val < key:\n            return search(root.right, key)\n        return search(root.left, key)\n\n    node = search(tree, val)\n\n    if node is not None:\n        balance_factors = []\n\n        def height(root):\n            if root is None:\n                return 0\n            return max(height(root.left), height(root.right)) + 1\n\n        def traverse(root):\n            if root is None:\n                return\n            left_height = height(root.left)\n            right_height = height(root.right)\n            balance_factors.append(left_height - right_height)\n            traverse(root.left)\n            traverse(root.right)\n\n        traverse(tree)\n\n        balance_factors_df = pd.DataFrame(balance_factors, columns=['Balance Factor'])\n\n        sns.boxplot(data=balance_factors_df, x='Balance Factor')\n\n    return node\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nheapq, matplotlib\n```python\nimport heapq\nimport matplotlib.pyplot as plt\n\n# Constants\nNODE_VALUES = list(range(1, 101))  # Node values for the binary tree\n\n# Binary tree node\nclass Node:\n    def __init__(self, val):\n        self.val = val\n        self.left = None\n        self.right = None\n\ndef f_112581_v5(tree, val):\n    \"\"\"\n    Find a node equal to 'val' in binary tree 'tree'. If found, \n    perform an in-order traversal of the tree, build a min heap \n    with the node values, and plot the heap using matplotlib.\n\n    Parameters:\n    tree (Node): The root node of the binary tree.\n    val (int): The value to be searched in the tree.\n\n    Returns:\n    Node: The node equal to 'val'. If not found, return None.\n    \n    Requirements:\n    - heapq\n    - matplotlib.pyplot\n\n    Example:\n    >>> root = Node(50)\n    >>> # Build the binary tree...\n    >>> found_node = f_112581_v5(root, 25)\n    \"\"\"\n    def search(root, key):\n        if root is None or root.val == key:\n            return root\n        if root.val < key:\n            return search(root.right, key)\n        return search(root.left, key)\n\n    node = search(tree, val)\n\n    if node is not None:\n        heap = []\n\n        def traverse(root):\n            if root is None:\n                return\n            traverse(root.left)\n            heapq.heappush(heap, root.val)\n            traverse(root.right)\n\n        traverse(tree)\n\n        plt.plot(heap)\n        plt.xlabel('Index')\n        plt.ylabel('Value')\n        plt.title('Min Heap')\n\n        plt.show()\n\n    return node\n```\n"}
{"seed": "{\"task_id\": 186169, \"prompt\": \"def f_186169(a):\\n\\t\", \"suffix\": \"\\n\\treturn a\", \"canonical_solution\": \"l = len(a)\\n\\n\\tfor k in range(l-1): \\n\\n\\t\\tfor i in range(l-1):\\n\\n\\t\\t\\tif a[i][i] < a[i+1][i+1]:\\n\\n\\t\\t\\t\\ta[i][i],a[i+1][i+1] = a[i+1][i+1],a[i][i]\\n\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    a = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\\n    assert(candidate(a) == [[9, 2, 3], [4, 5, 6], [7, 8, 1]])\\n\", \"\\n    b = [[1, 2, 3], [4, 5, 6]]\\n    assert(candidate(b) == [[5, 2, 3], [4, 1, 6]])\\n\"], \"entry_point\": \"f_186169\", \"intent\": \"\\u041e\\u0442\\u0441\\u043e\\u0440\\u0442\\u0438\\u0440\\u043e\\u0432\\u0430\\u0442\\u044c \\u044d\\u043b\\u0435\\u043c\\u0435\\u043d\\u0442\\u044b \\u043d\\u0430 \\u0433\\u043b\\u0430\\u0432\\u043d\\u043e\\u0439 \\u0434\\u0438\\u0430\\u0433\\u043e\\u043d\\u0430\\u043b\\u0438 \\u043a\\u0432\\u0430\\u0434\\u0440\\u0430\\u0442\\u043d\\u043e\\u0439 \\u043c\\u0430\\u0442\\u0440\\u0438\\u0446\\u044b `a` \\u043f\\u043e \\u0443\\u0431\\u044b\\u0432\\u0430\\u043d\\u0438\\u044e\", \"library\": []}", "generation": "Scenario 1: \nnumpy, pandas, itertools\n```python\nimport numpy as np\nimport pandas as pd\nfrom itertools import combinations\n\n# Constants\nDATA = [[1, 5, 7, 2], [4, 8, 9, 5], [7, 8, 9, 10], [11, 12, 13, 14]]\n\ndef find_combinations_with_highest_sum(data):\n    \"\"\" \n    Find all combinations of diagonal elements in a square matrix and return the\n    combination with the highest sum.\n    \n    Parameters:\n    data (list): The square matrix.\n    \n    Returns:\n    tuple: The combination with the highest sum.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - itertools\n    \n    Example:\n    >>> find_combinations_with_highest_sum(DATA)\n    \"\"\"\n    # Convert the list to a numpy array\n    data_array = np.array(data)\n    num_rows, num_cols = data_array.shape\n\n    # Ensure the array is a square matrix\n    if num_rows != num_cols:\n        raise ValueError(\"The input data is not a square matrix\")\n\n    # Extract the diagonal elements\n    diagonal_elements = np.diag(data_array)\n\n    # Generate all combinations of the diagonal elements\n    combinations_list = list(combinations(diagonal_elements, 2))\n\n    # Calculate the sum of each combination\n    combination_sums = [sum(comb) for comb in combinations_list]\n\n    # Find the combination with the highest sum\n    highest_sum_combination = combinations_list[np.argmax(combination_sums)]\n\n    return highest_sum_combination\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, matplotlib\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nDATA = [[1, 5, 7, 2], [4, 8, 9, 5], [7, 8, 9, 10], [11, 12, 13, 14]]\n\ndef plot_diagonal_elements(data):\n    \"\"\"\n    Plot the diagonal elements of a square matrix.\n    \n    Parameters:\n    data (list): The square matrix.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - numpy\n    - matplotlib\n    \n    Example:\n    >>> plot_diagonal_elements(DATA)\n    \"\"\"\n    # Convert the list to a numpy array\n    data_array = np.array(data)\n    num_rows, num_cols = data_array.shape\n\n    # Ensure the array is a square matrix\n    if num_rows != num_cols:\n        raise ValueError(\"The input data is not a square matrix\")\n\n    # Extract the diagonal elements\n    diagonal_elements = np.diag(data_array)\n\n    # Plot the diagonal elements\n    plt.plot(diagonal_elements)\n    plt.xlabel(\"Index\")\n    plt.ylabel(\"Value\")\n    plt.title(\"Diagonal Elements\")\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, pandas\n```python\nimport numpy as np\nimport pandas as pd\n\n# Constants\nDATA = [[1, 5, 7, 2], [4, 8, 9, 5], [7, 8, 9, 10], [11, 12, 13, 14]]\n\ndef create_dataframe_from_diagonal_elements(data):\n    \"\"\"\n    Create a pandas DataFrame from the diagonal elements of a square matrix.\n    \n    Parameters:\n    data (list): The square matrix.\n    \n    Returns:\n    DataFrame: The DataFrame with the diagonal elements.\n    \n    Requirements:\n    - numpy\n    - pandas\n    \n    Example:\n    >>> create_dataframe_from_diagonal_elements(DATA)\n    \"\"\"\n    # Convert the list to a numpy array\n    data_array = np.array(data)\n    num_rows, num_cols = data_array.shape\n\n    # Ensure the array is a square matrix\n    if num_rows != num_cols:\n        raise ValueError(\"The input data is not a square matrix\")\n\n    # Extract the diagonal elements\n    diagonal_elements = np.diag(data_array)\n\n    # Create a DataFrame from the diagonal elements\n    df = pd.DataFrame({\"Diagonal Elements\": diagonal_elements})\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, scipy.stats\n```python\nimport numpy as np\nfrom scipy import stats\n\n# Constants\nDATA = [[1, 5, 7, 2], [4, 8, 9, 5], [7, 8, 9, 10], [11, 12, 13, 14]]\n\ndef calculate_statistics_of_diagonal_elements(data):\n    \"\"\"\n    Calculate the mean, median, and mode of the diagonal elements of a square matrix.\n    \n    Parameters:\n    data (list): The square matrix.\n    \n    Returns:\n    tuple: The mean, median, and mode of the diagonal elements.\n    \n    Requirements:\n    - numpy\n    - scipy.stats\n    \n    Example:\n    >>> calculate_statistics_of_diagonal_elements(DATA)\n    \"\"\"\n    # Convert the list to a numpy array\n    data_array = np.array(data)\n    num_rows, num_cols = data_array.shape\n\n    # Ensure the array is a square matrix\n    if num_rows != num_cols:\n        raise ValueError(\"The input data is not a square matrix\")\n\n    # Extract the diagonal elements\n    diagonal_elements = np.diag(data_array)\n\n    # Calculate the mean, median, and mode\n    mean = np.mean(diagonal_elements)\n    median = np.median(diagonal_elements)\n    mode = stats.mode(diagonal_elements)\n\n    return mean, median, mode\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, pandas, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nDATA = [[1, 5, 7, 2], [4, 8, 9, 5], [7, 8, 9, 10], [11, 12, 13, 14]]\n\ndef plot_histogram_of_diagonal_elements(data):\n    \"\"\"\n    Plot a histogram of the diagonal elements of a square matrix.\n    \n    Parameters:\n    data (list): The square matrix.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib\n    \n    Example:\n    >>> plot_histogram_of_diagonal_elements(DATA)\n    \"\"\"\n    # Convert the list to a numpy array\n    data_array = np.array(data)\n    num_rows, num_cols = data_array.shape\n\n    # Ensure the array is a square matrix\n    if num_rows != num_cols:\n        raise ValueError(\"The input data is not a square matrix\")\n\n    # Extract the diagonal elements\n    diagonal_elements = np.diag(data_array)\n\n    # Plot a histogram of the diagonal elements\n    plt.hist(diagonal_elements, bins='auto')\n    plt.xlabel(\"Value\")\n    plt.ylabel(\"Frequency\")\n    plt.title(\"Histogram of Diagonal Elements\")\n    plt.show()\n```"}
{"seed": "{\"task_id\": 186169, \"prompt\": \"def f_186169(a):\\n\\t\", \"suffix\": \"\\n\\treturn a\", \"canonical_solution\": \"l = len(a)\\n\\n\\titems = [a[i][i] for i in range(l)]\\n\\n\\titems.sort(reverse=True)\\n\\n\\tfor i in range(l):\\n\\n\\t\\ta[i][i] = items[i]\\n\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    a = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\\n    assert(candidate(a) == [[9, 2, 3], [4, 5, 6], [7, 8, 1]])\\n\", \"\\n    b = [[1, 2, 3], [4, 5, 6]]\\n    assert(candidate(b) == [[5, 2, 3], [4, 1, 6]])\\n\"], \"entry_point\": \"f_186169\", \"intent\": \"\\u041e\\u0442\\u0441\\u043e\\u0440\\u0442\\u0438\\u0440\\u043e\\u0432\\u0430\\u0442\\u044c \\u044d\\u043b\\u0435\\u043c\\u0435\\u043d\\u0442\\u044b \\u043d\\u0430 \\u0433\\u043b\\u0430\\u0432\\u043d\\u043e\\u0439 \\u0434\\u0438\\u0430\\u0433\\u043e\\u043d\\u0430\\u043b\\u0438 \\u043a\\u0432\\u0430\\u0434\\u0440\\u0430\\u0442\\u043d\\u043e\\u0439 \\u043c\\u0430\\u0442\\u0440\\u0438\\u0446\\u044b `a` \\u043f\\u043e \\u0443\\u0431\\u044b\\u0432\\u0430\\u043d\\u0438\\u044e\", \"library\": []}", "generation": "Scenario 1: \nnumpy, pandas, matplotlib, random\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import choice\n\n# Constants\nCOLORS = ['Red', 'Blue', 'Green', 'Yellow', 'Purple']\n\ndef color_matrix(matrix):\n    \"\"\"\n    Assign a color to each element in the matrix. Then plot a bar chart showing the \n    frequency of each color in the matrix.\n\n    Parameters:\n    matrix (numpy.ndarray): The matrix.\n\n    Returns:\n    pandas.DataFrame: A DataFrame where each element of the matrix is assigned a color.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> a = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    >>> color_df = color_matrix(a)\n    >>> print(color_df)\n    >>> color_df.values.ravel().value_counts().plot(kind='bar')\n    \"\"\"\n    color_df = pd.DataFrame(matrix)\n\n    for i in range(color_df.shape[0]):\n        for j in range(color_df.shape[1]):\n            color_df.at[i, j] = choice(COLORS)\n\n    return color_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nnumpy, itertools, pandas, random\n```python\nimport numpy as np\nimport itertools\nimport pandas as pd\nfrom random import shuffle\n\n# Constants\nLETTERS = list('ABCDEFGHIJKLMNOPQRSTUVWXYZ')\n\ndef letter_matrix(matrix):\n    \"\"\"\n    Replace each element in the matrix with a letter. Then return a DataFrame where \n    each unique number in the original matrix is mapped to a unique letter.\n\n    Parameters:\n    matrix (numpy.ndarray): The matrix.\n\n    Returns:\n    pandas.DataFrame: A DataFrame where each unique number in the matrix is mapped to a letter.\n\n    Requirements:\n    - numpy\n    - itertools\n    - pandas\n    - random\n\n    Example:\n    >>> a = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    >>> letter_df = letter_matrix(a)\n    >>> print(letter_df)\n    \"\"\"\n    unique_numbers = np.unique(matrix)\n    shuffle(LETTERS)\n    mapping = dict(itertools.zip_longest(unique_numbers, LETTERS))\n\n    letter_df = pd.DataFrame(matrix)\n\n    for i in range(letter_df.shape[0]):\n        for j in range(letter_df.shape[1]):\n            letter_df.at[i, j] = mapping[letter_df.at[i, j]]\n\n    return letter_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnumpy, collections, pandas, random\n```python\nimport numpy as np\nimport collections\nimport pandas as pd\nfrom random import randrange\n\n# Constants\nMAX_RANDOM_VALUE = 100\n\ndef random_matrix(matrix):\n    \"\"\"\n    Replace each element in the matrix with a random integer. Then return a DataFrame \n    where each original element in the matrix is mapped to a random integer.\n\n    Parameters:\n    matrix (numpy.ndarray): The matrix.\n\n    Returns:\n    pandas.DataFrame: A DataFrame where each original element in the matrix is mapped to a random integer.\n\n    Requirements:\n    - numpy\n    - collections\n    - pandas\n    - random\n\n    Example:\n    >>> a = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    >>> random_df = random_matrix(a)\n    >>> print(random_df)\n    \"\"\"\n    unique_numbers = np.unique(matrix)\n    random_values = [randrange(MAX_RANDOM_VALUE) for _ in unique_numbers]\n    mapping = dict(zip(unique_numbers, random_values))\n\n    random_df = pd.DataFrame(matrix)\n\n    for i in range(random_df.shape[0]):\n        for j in range(random_df.shape[1]):\n            random_df.at[i, j] = mapping[random_df.at[i, j]]\n\n    return random_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, itertools, pandas, random\n```python\nimport numpy as np\nimport itertools\nimport pandas as pd\nfrom random import choice\n\n# Constants\nSHAPES = ['Circle', 'Square', 'Triangle', 'Star', 'Hexagon']\n\ndef shape_matrix(matrix):\n    \"\"\"\n    Replace each element in the matrix with a shape. Then return a DataFrame where \n    each unique number in the original matrix is mapped to a unique shape.\n\n    Parameters:\n    matrix (numpy.ndarray): The matrix.\n\n    Returns:\n    pandas.DataFrame: A DataFrame where each unique number in the matrix is mapped to a shape.\n\n    Requirements:\n    - numpy\n    - itertools\n    - pandas\n    - random\n\n    Example:\n    >>> a = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    >>> shape_df = shape_matrix(a)\n    >>> print(shape_df)\n    \"\"\"\n    unique_numbers = np.unique(matrix)\n    shuffle(SHAPES)\n    mapping = dict(itertools.zip_longest(unique_numbers, SHAPES))\n\n    shape_df = pd.DataFrame(matrix)\n\n    for i in range(shape_df.shape[0]):\n        for j in range(shape_df.shape[1]):\n            shape_df.at[i, j] = mapping[shape_df.at[i, j]]\n\n    return shape_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnumpy, collections, pandas, random\n```python\nimport numpy as np\nimport collections\nimport pandas as pd\nfrom random import randint\n\n# Constants\nMIN_RANDOM_VALUE = 1\nMAX_RANDOM_VALUE = 100\n\ndef weight_matrix(matrix):\n    \"\"\"\n    Multiply each element in the matrix with a random integer. Then return a DataFrame \n    where each original element in the matrix is multiplied with a random integer.\n\n    Parameters:\n    matrix (numpy.ndarray): The matrix.\n\n    Returns:\n    pandas.DataFrame: A DataFrame where each original element in the matrix is multiplied with a random integer.\n\n    Requirements:\n    - numpy\n    - collections\n    - pandas\n    - random\n\n    Example:\n    >>> a = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    >>> weight_df = weight_matrix(a)\n    >>> print(weight_df)\n    \"\"\"\n    weight_df = pd.DataFrame(matrix)\n\n    for i in range(weight_df.shape[0]):\n        for j in range(weight_df.shape[1]):\n            weight_df.at[i, j] *= randint(MIN_RANDOM_VALUE, MAX_RANDOM_VALUE)\n\n    return weight_df\n```\n"}
{"seed": "{\"task_id\": 298270, \"prompt\": \"def f_298270(s):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"s.encode('unicode_escape')\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(\\\"hello\\\") == b\\\"hello\\\"\\n\"], \"entry_point\": \"f_298270\", \"intent\": \"\\u042d\\u043a\\u0440\\u0430\\u043d\\u0438\\u0440\\u043e\\u0432\\u0430\\u0442\\u044c \\u0441\\u043f\\u0435\\u0446\\u0441\\u0438\\u043c\\u0432\\u043e\\u043b\\u044b \\u0432 \\u0441\\u0442\\u0440\\u043e\\u043a\\u0435 `s`\", \"library\": []}", "generation": "Scenario 1:\nunicodedata, re, requests, pandas\n```python\nimport unicodedata\nimport re\nimport requests\nimport pandas as pd\n\n# Constants\nURL = 'https://api.github.com/users/'\n\ndef get_user_info(usernames):\n    \"\"\"\n    Normalize the usernames to ASCII, fetch profile information from GitHub API, \n    and return a DataFrame with username and the number of public repos.\n\n    Parameters:\n    usernames (list of str): The list of usernames.\n\n    Returns:\n    DataFrame: A pandas DataFrame with username and the number of public repos.\n\n    Requirements:\n    - unicodedata\n    - re\n    - requests\n    - pandas\n\n    Example:\n    >>> get_user_info(['torvalds', 'guido'])\n    \"\"\"\n    normalized_usernames = [unicodedata.normalize('NFKD', username).encode('ascii', 'ignore').decode() for username in usernames]\n\n    data = []\n    for username in normalized_usernames:\n        response = requests.get(URL + username)\n        response_json = response.json()\n        data.append([username, response_json['public_repos']])\n\n    df = pd.DataFrame(data, columns=['Username', 'Public Repos'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, pathlib, hashlib, unicodedata\n```python\nimport os\nimport pathlib\nfrom hashlib import md5\nimport unicodedata\n\n# Constants\nDIRECTORY = '/path/to/directory'\n\ndef get_file_info(directory):\n    \"\"\"\n    Get information about the files in a directory. Normalize the file names to ASCII, \n    calculate their MD5 hash, and return a dictionary with file name, size, and MD5 hash.\n\n    Parameters:\n    directory (str): The directory path.\n\n    Returns:\n    dict: A dictionary with file name, size, and MD5 hash.\n\n    Requirements:\n    - os\n    - pathlib\n    - hashlib\n    - unicodedata\n\n    Example:\n    >>> get_file_info('/path/to/directory')\n    \"\"\"\n    files_info = {}\n\n    for file_path in pathlib.Path(directory).iterdir():\n        if file_path.is_file():\n            normalized_file_name = unicodedata.normalize('NFKD', file_path.name).encode('ascii', 'ignore').decode()\n\n            with open(file_path, 'rb') as file:\n                file_content = file.read()\n                file_hash = md5(file_content).hexdigest()\n\n            files_info[normalized_file_name] = {'Size': os.path.getsize(file_path), 'MD5 Hash': file_hash}\n\n    return files_info\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nunicodedata, csv, collections, matplotlib\n```python\nimport unicodedata\nimport csv\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\n# Constants\nCSV_FILE = '/path/to/file.csv'\n\ndef count_words_from_csv(csv_file):\n    \"\"\"\n    Normalize the text in a CSV file to ASCII, count the frequency of words, \n    and plot the 10 most common words.\n\n    Parameters:\n    csv_file (str): The CSV file path.\n\n    Returns:\n    None.\n\n    Requirements:\n    - unicodedata\n    - csv\n    - collections\n    - matplotlib.pyplot\n\n    Example:\n    >>> count_words_from_csv('/path/to/file.csv')\n    \"\"\"\n    words = []\n    with open(csv_file, 'r') as file:\n        reader = csv.reader(file)\n        for row in reader:\n            for word in row:\n                normalized_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode()\n                words.append(normalized_word)\n\n    word_counter = Counter(words)\n    most_common_words = word_counter.most_common(10)\n\n    labels, values = zip(*most_common_words)\n    plt.bar(labels, values)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nunicodedata, random, string, csv\n```python\nimport unicodedata\nimport random\nimport string\nimport csv\n\n# Constants\nCSV_FILE = '/path/to/file.csv'\nALPHABET = string.ascii_letters + string.digits\n\ndef generate_and_save_passwords(csv_file, usernames, password_length=12):\n    \"\"\"\n    Generate random passwords for the given usernames, normalize the usernames to ASCII, \n    and save them into a CSV file.\n\n    Parameters:\n    csv_file (str): The CSV file path.\n    usernames (list of str): The list of usernames.\n    password_length (int): The length of the passwords (default is 12).\n\n    Returns:\n    None.\n\n    Requirements:\n    - unicodedata\n    - random\n    - string\n    - csv\n\n    Example:\n    >>> generate_and_save_passwords('/path/to/file.csv', ['user1', 'user2'])\n    \"\"\"\n    with open(csv_file, 'w') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Username', 'Password'])\n        for username in usernames:\n            normalized_username = unicodedata.normalize('NFKD', username).encode('ascii', 'ignore').decode()\n            password = ''.join(random.choice(ALPHABET) for _ in range(password_length))\n            writer.writerow([normalized_username, password])\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nunicodedata, re, json, requests\n```python\nimport unicodedata\nimport re\nimport json\nimport requests\n\n# Constants\nURL = 'https://api.github.com/users/'\n\ndef fetch_and_normalize_github_info(username):\n    \"\"\"\n    Fetch user information from GitHub API, normalize the user data to ASCII, \n    and return a dictionary with the normalized data.\n\n    Parameters:\n    username (str): The username.\n\n    Returns:\n    dict: A dictionary with the normalized user data.\n\n    Requirements:\n    - unicodedata\n    - re\n    - json\n    - requests\n\n    Example:\n    >>> fetch_and_normalize_github_info('torvalds')\n    \"\"\"\n    response = requests.get(URL + username)\n    user_data = response.json()\n\n    normalized_user_data = {}\n    for key, value in user_data.items():\n        if isinstance(value, str):\n            normalized_value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode()\n            normalized_user_data[key] = normalized_value\n        else:\n            normalized_user_data[key] = value\n\n    return normalized_user_data\n```"}
{"seed": "{\"task_id\": 72284, \"prompt\": \"def f_72284(my_list, f):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"list(filter(f, my_list))\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    def f(x): return x > 1\\n    assert candidate([1, 2, 3], f) == [2, 3]\\n\", \"\\n    def g(x): return x%5\\n    assert candidate([1, 2, 3, 5], g) == [1, 2, 3]\\n\"], \"entry_point\": \"f_72284\", \"intent\": \"\\u041f\\u043e\\u043b\\u0443\\u0447\\u0438\\u0442\\u044c \\u044d\\u043b\\u0435\\u043c\\u0435\\u043d\\u0442\\u044b \\u0441\\u043f\\u0438\\u0441\\u043a\\u0430 `my_list` \\u0443\\u0434\\u043e\\u0432\\u043b\\u0435\\u0442\\u0432\\u043e\\u0440\\u044f\\u044e\\u0449\\u0438\\u0435 \\u0444\\u0443\\u043d\\u043a\\u0446\\u0438\\u0438 `f`\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, matplotlib, functools\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom functools import reduce\n\n# Sample data\ndata = {\n    'Name': ['Tom', 'Nick', 'John', 'Tom', 'John'],\n    'Age': [20, 21, 19, 20, 19],\n    'Score': [90, 85, 88, 92, 89]\n}\n\ndf = pd.DataFrame(data)\n\ndef filter_and_plot(df, column, func, threshold):\n    \"\"\"\n    Filters rows in a DataFrame based on a function and a threshold value, then\n    plots the filtered data.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame to filter.\n    column (str): The column to apply the function to.\n    func (function): The function to apply to the column values.\n    threshold (int or float): The threshold value to filter the DataFrame.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - functools.reduce\n\n    Example:\n    >>> filter_and_plot(df, 'Score', np.mean, 90)\n    \"\"\"\n    df[column] = df[column].apply(func)\n    filtered_df = df[df[column] > threshold]\n    filtered_df.plot.bar(x='Name', y=column, rot=0)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nnumpy, matplotlib, itertools\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom itertools import filterfalse\n\n# Constants\nN = 1000\nnp.random.seed(42)\nx = np.random.normal(size=N)\ny = np.random.normal(size=N)\n\ndef plot_filtered_data(x, y, func):\n    \"\"\"\n    Filters elements in two numpy arrays based on a function, then plots the \n    filtered data.\n\n    Parameters:\n    x, y (numpy.ndarray): The numpy arrays to filter.\n    func (function): The function to apply to the array elements.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - itertools.filterfalse\n\n    Example:\n    >>> plot_filtered_data(x, y, lambda x: x > 0)\n    \"\"\"\n    filter_func = np.vectorize(func)\n    mask = filter_func(x) & filter_func(y)\n    x_filtered, y_filtered = x[mask], y[mask]\n    plt.scatter(x_filtered, y_filtered)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \npandas, seaborn, sklearn.preprocessing\n```python\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\n\n# Sample data\ndata = {\n    'Name': ['Tom', 'Nick', 'John', 'Tom', 'John'],\n    'Age': [20, 21, 19, 20, 19],\n    'Score': [90, 85, 88, 92, 89]\n}\n\ndf = pd.DataFrame(data)\n\ndef standardize_and_plot(df, column, func):\n    \"\"\"\n    Standardizes a column in a DataFrame, then plots a histogram of the values\n    that pass a function test.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame to standardize and filter.\n    column (str): The column to standardize and filter.\n    func (function): The function to apply to the standardized column values.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - seaborn\n    - sklearn.preprocessing.StandardScaler\n\n    Example:\n    >>> standardize_and_plot(df, 'Score', lambda x: x > 0)\n    \"\"\"\n    scaler = StandardScaler()\n    df[column] = scaler.fit_transform(df[[column]]).flatten()\n    filtered_df = df[df[column].apply(func)]\n    sns.histplot(filtered_df[column])\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, matplotlib, scipy.stats\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n# Constants\nN = 1000\nnp.random.seed(42)\nx = np.random.normal(size=N)\n\ndef plot_transformed_data(x, func):\n    \"\"\"\n    Transforms elements in a numpy array based on a function, then plots the \n    transformed data.\n\n    Parameters:\n    x (numpy.ndarray): The numpy array to transform.\n    func (function): The function to apply to the array elements.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - scipy.stats.norm\n\n    Example:\n    >>> plot_transformed_data(x, lambda x: x > 0)\n    \"\"\"\n    transformed_x = np.array(list(filter(func, x)))\n    plt.hist(transformed_x, bins=30, density=True, alpha=0.6, color='g')\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \npandas, numpy, seaborn, functools\n```python\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom functools import reduce\n\n# Sample data\ndata = {\n    'Name': ['Tom', 'Nick', 'John', 'Tom', 'John'],\n    'Age': [20, 21, 19, 20, 19],\n    'Score': [90, 85, 88, 92, 89]\n}\n\ndf = pd.DataFrame(data)\n\ndef filter_and_plot(df, column, func):\n    \"\"\"\n    Filters rows in a DataFrame based on a function, then\n    plots the filtered data.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame to filter.\n    column (str): The column to apply the function to.\n    func (function): The function to apply to the column values.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - numpy\n    - seaborn\n    - functools.reduce\n\n    Example:\n    >>> filter_and_plot(df, 'Score', np.mean)\n    \"\"\"\n    df[column] = df[column].apply(func)\n    filtered_df = df[df[column] > df[column].mean()]\n    sns.boxplot(x='Name', y=column, data=filtered_df)\n```"}
{"seed": "{\"task_id\": 83341, \"prompt\": \"def f_83341(dicts, key):\\n\\t\", \"suffix\": \"\\n\\treturn dicts\", \"canonical_solution\": \"dicts.sort(key=lambda x: x[key])\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    dicts = [\\n        {'k1': 234, 'f4': 57e6, 'gp': 0}, \\n        {'k1': 436, 'f4': 436, 'gp': 45}, \\n        {'k1': -33234, 'f4': -2.5, 'gp': 0.0}\\n    ]\\n    assert candidate(dicts, 'k1') == [\\n        {'k1': -33234, 'f4': -2.5, 'gp': 0.0}, \\n        {'k1': 234, 'f4': 57e6, 'gp': 0}, \\n        {'k1': 436, 'f4': 436, 'gp': 45}, \\n    ]\\n\"], \"entry_point\": \"f_83341\", \"intent\": \"\\u041e\\u0442\\u0441\\u043e\\u0440\\u0442\\u0438\\u0440\\u043e\\u0432\\u0430\\u0442\\u044c \\u0441\\u043f\\u0438\\u0441\\u043e\\u043a \\u0441\\u043b\\u043e\\u0432\\u0430\\u0440\\u0435\\u0439 `dicts` \\u043f\\u043e \\u043a\\u043b\\u044e\\u0447\\u0443 `key`\", \"library\": []}", "generation": "Scenario 1:\npandas, datetime, random, matplotlib\n```python\nimport pandas as pd\nfrom datetime import datetime\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nFRUITS = ['Apple', 'Banana', 'Cherry', 'Date', 'Elderberry']\nSTORES = ['Store A', 'Store B', 'Store C', 'Store D', 'Store E']\n\ndef generate_fruit_sales_report(month, year):\n    \"\"\"\n    Generate a report of fruit sales for a list of stores for a specific month and year.\n    \n    Parameters:\n    month (int): The month for the report.\n    year (int): The year for the report.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with fruit sales for the stores.\n    \n    Requirements:\n    - pandas\n    - datetime\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> report = generate_fruit_sales_report(1, 2023)\n    >>> print(report)\n    >>> report['Sales'].plot(kind='bar')\n    \"\"\"\n    report_data = []\n\n    for store in STORES:\n        for fruit in FRUITS:\n            sales = randint(0, 100)\n            report_data.append([store, fruit, month, year, sales])\n\n    report_df = pd.DataFrame(report_data, columns=['Store', 'Fruit', 'Month', 'Year', 'Sales'])\n\n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, json, pandas, matplotlib\n```python\nimport os\nimport json\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nDIR_PATH = './data'\n\ndef generate_report_from_json_files():\n    \"\"\"\n    Generate a report from JSON files in a directory and plot the data.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the data from the JSON files.\n    \n    Requirements:\n    - os\n    - json\n    - pandas\n    - matplotlib.pyplot\n    \n    Example:\n    >>> report = generate_report_from_json_files()\n    >>> print(report)\n    >>> report.plot(kind='bar')\n    \"\"\"\n    report_data = []\n\n    for file_name in os.listdir(DIR_PATH):\n        if file_name.endswith('.json'):\n            with open(os.path.join(DIR_PATH, file_name)) as file:\n                data = json.load(file)\n                report_data.append(data)\n\n    report_df = pd.DataFrame(report_data)\n\n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, pandas, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef generate_random_dataframe_and_plot(n):\n    \"\"\"\n    Generate a DataFrame with random data and plot the data.\n    \n    Parameters:\n    n (int): The number of rows in the DataFrame.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with random data.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = generate_random_dataframe_and_plot(10)\n    >>> print(df)\n    >>> df.plot(kind='bar')\n    \"\"\"\n    data = np.random.rand(n, len(COLUMNS))\n    df = pd.DataFrame(data, columns=COLUMNS)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nscipy, numpy, matplotlib\n```python\nfrom scipy import stats\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_normal_distribution(mu, sigma, num_samples):\n    \"\"\"\n    Generate and plot a normal distribution\n\n    Parameters:\n    mu (float): The mean of the distribution.\n    sigma (float): The standard deviation of the distribution.\n    num_samples (int): The number of samples to generate.\n\n    Returns:\n    list: A list of samples from the normal distribution.\n\n    Requirements:\n    - scipy\n    - numpy\n    - matplotlib\n\n    Example:\n    >>> samples = plot_normal_distribution(0, 1, 1000)\n    >>> plt.hist(samples, bins=30, density=True)\n    \"\"\"\n    samples = np.random.normal(mu, sigma, num_samples)\n    plt.hist(samples, bins=30, density=True)\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, mu, sigma)\n    plt.plot(x, p, 'k', linewidth=2)\n    plt.show()\n    return samples.tolist()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, pandas, seaborn, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef generate_and_visualize_correlation_matrix(n):\n    \"\"\"\n    Generate a DataFrame with random data, compute correlation matrix and visualize it using a heatmap.\n    \n    Parameters:\n    n (int): The number of rows in the DataFrame.\n    \n    Returns:\n    DataFrame: A correlation matrix as a pandas DataFrame.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - seaborn\n    - matplotlib.pyplot\n    \n    Example:\n    >>> corr_matrix = generate_and_visualize_correlation_matrix(10)\n    >>> print(corr_matrix)\n    \"\"\"\n    data = np.random.rand(n, len(COLUMNS))\n    df = pd.DataFrame(data, columns=COLUMNS)\n    corr_matrix = df.corr()\n\n    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n    plt.show()\n\n    return corr_matrix\n```"}
{"seed": "{\"task_id\": 30683, \"prompt\": \"def f_30683(filepath):\\n\\t\", \"suffix\": \"\\n\\treturn lines\", \"canonical_solution\": \"with open(filepath) as f:\\n\\n\\t\\tlines = f.readlines()[1::2]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    filepath = \\\"odd_lines.txt\\\"\\n    with open(filepath, 'w') as fw: \\n        for idx in range(10): fw.write(f\\\"{idx}\\\\n\\\")\\n    assert candidate(filepath) == [f\\\"{idx}\\\\n\\\" for idx in range(1,10,2)]\\n\", \"\\n    filepath = \\\"odd_lines.txt\\\"\\n    with open(filepath, 'w') as fw: \\n        for idx in range(20): \\n            if (idx % 2) == 0: fw.write(f\\\"{idx}\\\\n\\\")\\n            else: fw.write(\\\"\\\\n\\\")\\n    assert candidate(filepath) == [\\\"\\\\n\\\" for _ in range(10)]\\n\"], \"entry_point\": \"f_30683\", \"intent\": \"\\u041f\\u043e\\u043b\\u0443\\u0447\\u0438\\u0442\\u044c \\u0441\\u043f\\u0438\\u0441\\u043e\\u043a \\u043d\\u0435\\u0447\\u0435\\u0442\\u043d\\u044b\\u0445 \\u0441\\u0442\\u0440\\u043e\\u043a `lines` \\u0444\\u0430\\u0439\\u043b\\u0430 `filepath` \", \"library\": []}", "generation": "Scenario 1:\npandas, csv, os, re\n```python\nimport pandas as pd\nimport csv\nimport os\nimport re\n\n# Constants\nCSV_FILE_PATH = \"file.csv\"\n\ndef extract_odd_lines_with_value_greater_than_avg(filepath):\n    \"\"\"\n    Extract odd lines of a CSV file where the numeric value in the first column is \n    greater than the average of all values in the first column.\n\n    Parameters:\n    filepath (str): The path of the CSV file.\n\n    Returns:\n    DataFrame: A pandas DataFrame with odd lines of CSV file where the numeric \n    value in the first column is greater than the average of all values in the \n    first column.\n\n    Requirements:\n    - pandas\n    - csv\n    - os\n    - re\n\n    Example:\n    >>> extract_odd_lines_with_value_greater_than_avg('file.csv')\n    \"\"\"\n    if not os.path.exists(filepath):\n        return pd.DataFrame()\n\n    data = pd.read_csv(filepath, header=None)\n    numeric_data = pd.to_numeric(data[0], errors='coerce')\n    avg = numeric_data.mean()\n\n    odd_lines = data.iloc[1::2]\n    odd_lines = odd_lines[pd.to_numeric(odd_lines[0], errors='coerce') > avg]\n\n    return odd_lines\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, pathlib, csv, pandas\n```python\nimport os\nfrom pathlib import Path\nimport csv\nimport pandas as pd\n\n# Constants\nCSV_FILE_PATH = \"file.csv\"\n\ndef extract_odd_lines_with_word_count_greater_than_avg(filepath):\n    \"\"\"\n    Extract odd lines of a CSV file where the word count in the first column is \n    greater than the average word count of all values in the first column.\n\n    Parameters:\n    filepath (str): The path of the CSV file.\n\n    Returns:\n    DataFrame: A pandas DataFrame with odd lines of CSV file where the word count \n    in the first column is greater than the average word count of all values in \n    the first column.\n\n    Requirements:\n    - os\n    - pathlib\n    - csv\n    - pandas\n\n    Example:\n    >>> extract_odd_lines_with_word_count_greater_than_avg('file.csv')\n    \"\"\"\n    if not Path(filepath).is_file():\n        return pd.DataFrame()\n\n    data = pd.read_csv(filepath, header=None)\n    word_counts = data[0].str.split().str.len()\n    avg = word_counts.mean()\n\n    odd_lines = data.iloc[1::2]\n    odd_lines = odd_lines[odd_lines[0].str.split().str.len() > avg]\n\n    return odd_lines\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ncsv, pandas, random, os, shutil\n```python\nimport csv\nimport pandas as pd\nimport random\nimport os\nimport shutil\n\n# Constants\nCSV_FILE_PATH = \"file.csv\"\n\ndef shuffle_odd_lines_and_write_to_new_file(filepath):\n    \"\"\"\n    Shuffle the odd lines of a CSV file and write them to a new file.\n\n    Parameters:\n    filepath (str): The path of the CSV file.\n\n    Returns:\n    str: The path of the new CSV file.\n\n    Requirements:\n    - csv\n    - pandas\n    - random\n    - os\n    - shutil\n\n    Example:\n    >>> shuffle_odd_lines_and_write_to_new_file('file.csv')\n    \"\"\"\n    if not os.path.exists(filepath):\n        return ''\n\n    data = pd.read_csv(filepath, header=None)\n\n    odd_lines = data.iloc[1::2]\n    odd_lines = odd_lines.sample(frac=1).reset_index(drop=True)\n\n    new_filepath = filepath[:-4] + '_shuffled.csv'\n    odd_lines.to_csv(new_filepath, index=False, header=False)\n\n    return new_filepath\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, os, csv, statistics\n```python\nimport pandas as pd\nimport os\nimport csv\nimport statistics\n\n# Constants\nCSV_FILE_PATH = \"file.csv\"\n\ndef calculate_median_of_odd_lines(filepath):\n    \"\"\"\n    Calculate the median of numeric values in the first column of odd lines of a CSV file.\n\n    Parameters:\n    filepath (str): The path of the CSV file.\n\n    Returns:\n    float: The median of numeric values in the first column of odd lines.\n\n    Requirements:\n    - pandas\n    - os\n    - csv\n    - statistics\n\n    Example:\n    >>> calculate_median_of_odd_lines('file.csv')\n    \"\"\"\n    if not os.path.exists(filepath):\n        return None\n\n    data = pd.read_csv(filepath, header=None)\n\n    odd_lines = data.iloc[1::2]\n    numeric_values = pd.to_numeric(odd_lines[0], errors='coerce')\n    numeric_values = numeric_values.dropna()\n\n    median = statistics.median(numeric_values) if not numeric_values.empty else None\n\n    return median\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\ncsv, pandas, os, re, itertools\n```python\nimport csv\nimport pandas as pd\nimport os\nimport re\nfrom itertools import compress\n\n# Constants\nCSV_FILE_PATH = \"file.csv\"\n\ndef extract_odd_lines_with_more_vowels_than_consonants(filepath):\n    \"\"\"\n    Extract odd lines of a CSV file where the number of vowels in the first column \n    is greater than the number of consonants.\n\n    Parameters:\n    filepath (str): The path of the CSV file.\n\n    Returns:\n    DataFrame: A pandas DataFrame with odd lines of CSV file where the number \n    of vowels in the first column is greater than the number of consonants.\n\n    Requirements:\n    - csv\n    - pandas\n    - os\n    - re\n    - itertools\n\n    Example:\n    >>> extract_odd_lines_with_more_vowels_than_consonants('file.csv')\n    \"\"\"\n    if not os.path.exists(filepath):\n        return pd.DataFrame()\n\n    data = pd.read_csv(filepath, header=None)\n    \n    is_vowel = lambda x: len(re.findall(r'[aeiou]', x, re.I)) > len(re.findall(r'[bcdfghjklmnpqrstvwxyz]', x, re.I))\n    mask = data[0].apply(is_vowel)\n\n    odd_lines = data.iloc[1::2]\n    odd_lines = odd_lines[mask[1::2]]\n\n    return odd_lines\n```"}
{"seed": "{\"task_id\": 30683, \"prompt\": \"def f_30683(filepath):\\n\\t\", \"suffix\": \"\\n\\treturn lines\", \"canonical_solution\": \"with open(filepath) as file:\\n\\n\\t\\tlines = [i.rstrip() for i in itertools.islice(file, 0, None, 2)]\", \"test_start\": \"\\nimport itertools \\n\\ndef check(candidate):\", \"test\": [\"\\n    filepath = \\\"odd_lines.txt\\\"\\n    with open(filepath, 'w') as fw: \\n        for idx in range(10): fw.write(f\\\"{idx}\\\\n\\\")\\n    assert candidate(filepath) == [f\\\"{idx}\\\" for idx in range(0,10,2)]\\n\", \"\\n    filepath = \\\"odd_lines.txt\\\"\\n    with open(filepath, 'w') as fw: \\n        for idx in range(20): \\n            if idx % 2: fw.write(f\\\"{idx}\\\\n\\\")\\n            else: fw.write(\\\"\\\\n\\\")\\n    assert candidate(filepath) == [\\\"\\\" for _ in range(10)]\\n\"], \"entry_point\": \"f_30683\", \"intent\": \"\\u041f\\u043e\\u043b\\u0443\\u0447\\u0438\\u0442\\u044c \\u0441\\u043f\\u0438\\u0441\\u043e\\u043a \\u043d\\u0435\\u0447\\u0435\\u0442\\u043d\\u044b\\u0445 \\u0441\\u0442\\u0440\\u043e\\u043a `lines` \\u0444\\u0430\\u0439\\u043b\\u0430 `filepath` \", \"library\": [\"itertools\"]}", "generation": "Scenario 1: \nos, hashlib, itertools\n```python\nimport os\nimport hashlib\nimport itertools\n\ndef hash_file_lines(filepath, lines_to_hash=10, hash_func='md5'):\n    \"\"\"\n    Hash the first 'lines_to_hash' lines of a file using a specified hash function.\n\n    Parameters:\n    filepath (str): Path to the file.\n    lines_to_hash (int): Number of lines to hash. Default is 10.\n    hash_func (str): Name of the hash function to use. Default is 'md5'. \n\n    Returns:\n    str: The resulting hash.\n\n    Requirements:\n    - os\n    - hashlib\n    - itertools\n\n    Example:\n    >>> hash_file_lines('sample_file.txt', lines_to_hash=5, hash_func='sha256')\n    \"\"\"\n    hash_func = getattr(hashlib, hash_func)()\n\n    with open(filepath, 'rb') as f:\n        for line in itertools.islice(f, lines_to_hash):\n            hash_func.update(line)\n\n    return hash_func.hexdigest()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nitertools, collections, re\n```python\nimport itertools\nimport collections\nimport re\n\ndef count_word_frequency(filepath, top_n=10):\n    \"\"\"\n    Count the frequency of words in a file and return the top 'n' most common words.\n\n    Parameters:\n    filepath (str): Path to the file.\n    top_n (int): Number of most common words to return. Default is 10.\n\n    Returns:\n    list: A list of tuples with the top 'n' most common words and their frequencies.\n\n    Requirements:\n    - itertools\n    - collections\n    - re\n\n    Example:\n    >>> count_word_frequency('sample_file.txt', top_n=5)\n    \"\"\"\n    with open(filepath, 'r') as f:\n        words = re.findall('\\w+', f.read().lower())\n        word_counts = collections.Counter(words)\n\n    return word_counts.most_common(top_n)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nitertools, json, os\n```python\nimport itertools\nimport json\nimport os\n\ndef extract_json_objects(filepath, max_objects=5):\n    \"\"\"\n    Extract the first 'max_objects' JSON objects from a file.\n\n    Parameters:\n    filepath (str): Path to the file.\n    max_objects (int): Maximum number of JSON objects to extract. Default is 5.\n\n    Returns:\n    list: A list of JSON objects.\n\n    Requirements:\n    - itertools\n    - json\n    - os\n\n    Example:\n    >>> extract_json_objects('sample_file.json', max_objects=3)\n    \"\"\"\n    json_objects = []\n\n    with open(filepath, 'r') as f:\n        for line in itertools.islice(f, max_objects):\n            json_objects.append(json.loads(line))\n\n    return json_objects\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nitertools, csv, collections\n```python\nimport itertools\nimport csv\nimport collections\n\ndef count_field_values(filepath, field_name, top_n=5):\n    \"\"\"\n    Count the frequency of values in a specific field in a CSV file, and return the top 'n' most common.\n\n    Parameters:\n    filepath (str): Path to the CSV file.\n    field_name (str): Name of the field.\n    top_n (int): Number of most common values to return. Default is 5.\n\n    Returns:\n    list: A list of tuples with the top 'n' most common values and their frequencies.\n\n    Requirements:\n    - itertools\n    - csv\n    - collections\n\n    Example:\n    >>> count_field_values('sample_file.csv', field_name='Country', top_n=3)\n    \"\"\"\n    with open(filepath, 'r') as f:\n        reader = csv.DictReader(f)\n        values = [row[field_name] for row in reader]\n        value_counts = collections.Counter(values)\n\n    return value_counts.most_common(top_n)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nitertools, pandas, numpy\n```python\nimport itertools\nimport pandas as pd\nimport numpy as np\n\ndef calculate_statistics(filepath, column_name):\n    \"\"\"\n    Calculate the mean, median, and standard deviation of a specific column in a CSV file.\n\n    Parameters:\n    filepath (str): Path to the CSV file.\n    column_name (str): Name of the column.\n\n    Returns:\n    dict: A dictionary with the mean, median, and standard deviation.\n\n    Requirements:\n    - itertools\n    - pandas\n    - numpy\n\n    Example:\n    >>> calculate_statistics('sample_file.csv', column_name='Age')\n    \"\"\"\n    df = pd.read_csv(filepath)\n    column = df[column_name]\n\n    return {\n        'mean': np.mean(column),\n        'median': np.median(column),\n        'std_dev': np.std(column)\n    }\n```"}
{"seed": "{\"task_id\": 241337, \"prompt\": \"def f_241337(s):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"urllib.parse.quote(s)\", \"test_start\": \"\\nimport urllib \\n\\ndef check(candidate):\", \"test\": [\"\\n    s = \\\"abc def/foo?bar=baz\\\"\\n    assert candidate(s) == \\\"abc%20def/foo%3Fbar%3Dbaz\\\"\\n\"], \"entry_point\": \"f_241337\", \"intent\": \"\\u041f\\u0440\\u0435\\u043e\\u0431\\u0440\\u0430\\u0437\\u043e\\u0432\\u0430\\u0442\\u044c \\u043a\\u0438\\u0440\\u0438\\u043b\\u043b\\u0438\\u0447\\u0435\\u0441\\u043a\\u0443\\u044e \\u0441\\u0442\\u0440\\u043e\\u043a\\u0443 `s` \\u0432 \\u0441\\u0442\\u0440\\u043e\\u043a\\u0443 \\u0441\\u043f\\u0435\\u0446\\u0441\\u0438\\u043c\\u0432\\u043e\\u043b\\u043e\\u0432 \\u0434\\u043b\\u044f http \\u0437\\u0430\\u043f\\u0440\\u043e\\u0441\\u0430 \", \"library\": [\"urllib\"]}", "generation": "Scenario 1: \nurllib, hashlib, base64\n```python\nimport urllib.parse\nimport hashlib\nimport base64\n\n# Constants\nSALT = b'some_random_salt'\n\ndef create_secure_link(s):\n    \"\"\"\n    Convert a provided string 's' to an URL-safe string and then hash it using SHA256. \n    The hash is then encoded in base64.\n    \n    Parameters:\n    s (str): The string to be converted and hashed.\n\n    Returns:\n    str: The base64 encoded SHA256 hash of the URL-safe string.\n    \n    Requirements:\n    - urllib.parse\n    - hashlib\n    - base64\n\n    Example:\n    >>> create_secure_link(\"abc def/foo?bar=baz\")\n    \"\"\"\n    url_safe_string = urllib.parse.quote(s)\n    hashed_string = hashlib.pbkdf2_hmac('sha256', url_safe_string.encode(), SALT, 100000)\n    base64_hash = base64.b64encode(hashed_string).decode()\n    return base64_hash\n ```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nurllib, bs4, requests\n```python\nimport urllib.parse\nfrom bs4 import BeautifulSoup\nimport requests\n\ndef find_links_in_webpage(url):\n    \"\"\"\n    Parse a webpage specified by the URL and find all links in the webpage. \n\n    Parameters:\n    url (str): The URL of the webpage.\n\n    Returns:\n    list: List of links found in the webpage.\n\n    Requirements:\n    - urllib.parse\n    - bs4\n    - requests\n\n    Example:\n    >>> find_links_in_webpage(\"https://example.com\")\n    \"\"\"\n    response = requests.get(url)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    links = [urllib.parse.urljoin(url, link.get('href')) for link in soup.find_all('a')]\n    return links\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nurllib, json, os\n```python\nimport urllib.parse\nimport json\nimport os\n\n# Constants\nDATA_DIR = '/path/to/data'\n\ndef save_json_from_url(url, filename):\n    \"\"\"\n    Download a json file from the provided URL and save it to a local file.\n\n    Parameters:\n    url (str): The URL of the json file.\n    filename (str): The name of the local file.\n\n    Returns:\n    str: The path of the saved file.\n\n    Requirements:\n    - urllib.parse\n    - json\n    - os\n\n    Example:\n    >>> save_json_from_url(\"https://example.com/data.json\", \"data.json\")\n    \"\"\"\n    response = urllib.request.urlopen(url)\n    data = json.loads(response.read())\n\n    file_path = os.path.join(DATA_DIR, filename)\n    with open(file_path, 'w') as f:\n        json.dump(data, f)\n\n    return file_path\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nurllib, csv, pandas\n```python\nimport urllib.parse\nimport csv\nimport pandas as pd\n\ndef load_csv_data_from_url(url):\n    \"\"\"\n    Download a CSV file from the provided URL and load it into a pandas DataFrame.\n\n    Parameters:\n    url (str): The URL of the CSV file.\n\n    Returns:\n    DataFrame: The loaded data.\n\n    Requirements:\n    - urllib.parse\n    - csv\n    - pandas\n\n    Example:\n    >>> load_csv_data_from_url(\"https://example.com/data.csv\")\n    \"\"\"\n    response = urllib.request.urlopen(url)\n    csv_file = csv.reader(response.read().decode('utf-8').splitlines())\n    df = pd.DataFrame(csv_file)\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nurllib, base64, PIL\n```python\nimport urllib.parse\nimport base64\nfrom PIL import Image\nfrom io import BytesIO\n\ndef load_image_from_base64url(url):\n    \"\"\"\n    Load an image from a provided base64 encoded URL.\n\n    Parameters:\n    url (str): The base64 encoded URL of the image.\n\n    Returns:\n    Image: The loaded image.\n\n    Requirements:\n    - urllib.parse\n    - base64\n    - PIL\n\n    Example:\n    >>> load_image_from_base64url(\"data:image/png;base64,iVBORw0KG...\")\n    \"\"\"\n    base64_data = urllib.parse.unquote(url.split(\",\")[1])\n    image_data = base64.b64decode(base64_data)\n    image = Image.open(BytesIO(image_data))\n    return image\n```"}
{"seed": "{\"task_id\": 241337, \"prompt\": \"def f_241337(s):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"urllib.parse.quote_plus(s)\", \"test_start\": \"\\nimport urllib \\n\\ndef check(candidate):\", \"test\": [\"\\n    s = \\\"abc def/foo?bar=baz\\\"\\n    assert candidate(s) == \\\"abc+def%2Ffoo%3Fbar%3Dbaz\\\"\\n\"], \"entry_point\": \"f_241337\", \"intent\": \"\\u041f\\u0440\\u0435\\u043e\\u0431\\u0440\\u0430\\u0437\\u043e\\u0432\\u0430\\u0442\\u044c \\u043a\\u0438\\u0440\\u0438\\u043b\\u043b\\u0438\\u0447\\u0435\\u0441\\u043a\\u0443\\u044e \\u0441\\u0442\\u0440\\u043e\\u043a\\u0443 `s` \\u0432 \\u0441\\u0442\\u0440\\u043e\\u043a\\u0443 \\u0441\\u043f\\u0435\\u0446\\u0441\\u0438\\u043c\\u0432\\u043e\\u043b\\u043e\\u0432 \\u0434\\u043b\\u044f http \\u0437\\u0430\\u043f\\u0440\\u043e\\u0441\\u0430 \", \"library\": [\"urllib\"]}", "generation": "Scenario 3: \nurllib, hashlib, base64, datetime\n```python\nimport urllib.parse\nimport hashlib\nimport base64\nfrom datetime import datetime\n\n# Constants\nSECRET_KEY = 'MySecretKey'\n\ndef generate_signed_url(query_params):\n    \"\"\"\n    Generate a signed URL with the given query parameters. The URL is signed using HMAC and a secret key.\n    \n    Parameters:\n    query_params (dict): A dictionary of query parameters.\n    \n    Returns:\n    str: The signed URL.\n    \n    Requirements:\n    - urllib.parse\n    - hashlib\n    - base64\n    - datetime\n    \n    Example:\n    >>> query_params = {'param1': 'value1', 'param2': 'value2'}\n    >>> signed_url = generate_signed_url(query_params)\n    >>> print(signed_url)\n    \"\"\"\n    # Sort the parameters by key\n    sorted_params = sorted(query_params.items())\n    \n    # Generate the query string\n    query_string = urllib.parse.urlencode(sorted_params)\n    \n    # Concatenate the current timestamp to the query string\n    timestamp = str(datetime.utcnow().timestamp())\n    string_to_sign = query_string + timestamp\n    \n    # Generate the HMAC signature\n    signature = base64.b64encode(hashlib.hmac.new(SECRET_KEY.encode(), string_to_sign.encode(), hashlib.sha256).digest()).decode()\n    \n    # Add the signature and timestamp to the query parameters\n    query_params['signature'] = signature\n    query_params['timestamp'] = timestamp\n    \n    # Generate the final signed URL\n    signed_url = 'https://example.com?' + urllib.parse.urlencode(query_params)\n    \n    return signed_url\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nurllib, os, zipfile, io\n```python\nimport urllib.request\nimport os\nimport zipfile\nimport io\n\n# Constants\nDOWNLOAD_PATH = '/tmp'\n\ndef download_and_extract_zip(url):\n    \"\"\"\n    Download a ZIP file from the given URL and extract its contents to a local directory.\n    \n    Parameters:\n    url (str): The URL of the ZIP file.\n    \n    Returns:\n    str: The path to the directory where the ZIP file was extracted.\n    \n    Requirements:\n    - urllib.request\n    - os\n    - zipfile\n    - io\n    \n    Example:\n    >>> url = 'https://example.com/myfile.zip'\n    >>> extracted_dir = download_and_extract_zip(url)\n    >>> print(extracted_dir)\n    \"\"\"\n    response = urllib.request.urlopen(url)\n    zip_file = zipfile.ZipFile(io.BytesIO(response.read()))\n    \n    extracted_dir = os.path.join(DOWNLOAD_PATH, os.path.basename(url))\n    os.makedirs(extracted_dir, exist_ok=True)\n    \n    zip_file.extractall(extracted_dir)\n    \n    return extracted_dir\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nurllib, bs4, re, collections\n```python\nimport urllib.request\nfrom bs4 import BeautifulSoup\nimport re\nfrom collections import Counter\n\ndef count_words_in_webpage(url):\n    \"\"\"\n    Count the frequency of different words in a webpage.\n    \n    Parameters:\n    url (str): The URL of the webpage.\n    \n    Returns:\n    dict: A dictionary where the keys are words and the values are the frequencies.\n    \n    Requirements:\n    - urllib.request\n    - bs4\n    - re\n    - collections\n    \n    Example:\n    >>> url = 'https://example.com'\n    >>> word_counts = count_words_in_webpage(url)\n    >>> print(word_counts)\n    \"\"\"\n    response = urllib.request.urlopen(url)\n    soup = BeautifulSoup(response, 'html.parser')\n    \n    text = soup.get_text()\n    words = re.findall(r'\\w+', text.lower())\n    \n    word_counts = Counter(words)\n    \n    return dict(word_counts)\n```\n`GPT_ODEX_BREAK`\n\nScenario 6: \nurllib, json, csv\n```python\nimport urllib.request\nimport json\nimport csv\n\n# Constants\nOUTPUT_FILE = 'output.csv'\n\ndef convert_json_to_csv(url):\n    \"\"\"\n    Download JSON data from a URL and convert it to a CSV file.\n    \n    Parameters:\n    url (str): The URL of the JSON data.\n    \n    Returns:\n    str: The path to the CSV file.\n    \n    Requirements:\n    - urllib.request\n    - json\n    - csv\n    \n    Example:\n    >>> url = 'https://api.example.com/data.json'\n    >>> csv_file = convert_json_to_csv(url)\n    >>> print(csv_file)\n    \"\"\"\n    response = urllib.request.urlopen(url)\n    data = json.loads(response.read())\n    \n    with open(OUTPUT_FILE, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(data[0].keys())\n        for row in data:\n            writer.writerow(row.values())\n    \n    return OUTPUT_FILE\n```\n`GPT_ODEX_BREAK`\n\nScenario 7: \nurllib, PIL, io, os\n```python\nimport urllib.request\nfrom PIL import Image\nimport io\nimport os\n\n# Constants\nDOWNLOAD_PATH = '/tmp'\n\ndef download_and_convert_image(url, format):\n    \"\"\"\n    Download an image from a URL, convert it to a different format, and save it to a local directory.\n    \n    Parameters:\n    url (str): The URL of the image.\n    format (str): The format to convert the image to.\n    \n    Returns:\n    str: The path to the converted image.\n    \n    Requirements:\n    - urllib.request\n    - PIL\n    - io\n    - os\n    \n    Example:\n    >>> url = 'https://example.com/myimage.jpg'\n    >>> converted_image = download_and_convert_image(url, 'PNG')\n    >>> print(converted_image)\n    \"\"\"\n    response = urllib.request.urlopen(url)\n    image = Image.open(io.BytesIO(response.read()))\n    \n    output_file = os.path.join(DOWNLOAD_PATH, os.path.basename(url).split('.')[0] + '.' + format.lower())\n    image.save(output_file, format)\n    \n    return output_file\n```\n"}
{"seed": "{\"task_id\": 64496, \"prompt\": \"def f_64496():\\n\\t\", \"suffix\": \"\\n\\treturn pub, priv\", \"canonical_solution\": \"(pub, priv) = rsa.newkeys(512)\", \"test_start\": \"\\nimport rsa \\n\\ndef check(candidate):\", \"test\": [\"\\n    pub, priv = candidate()\\n    assert pub.__class__ == rsa.key.PublicKey\\n    assert priv.__class__ == rsa.key.PrivateKey\\n\"], \"entry_point\": \"f_64496\", \"intent\": \"\\u0421\\u0433\\u0435\\u043d\\u0435\\u0440\\u0438\\u0440\\u043e\\u0432\\u0430\\u0442\\u044c \\u043e\\u0442\\u043a\\u0440\\u044b\\u0442\\u044b\\u0439 \\u043a\\u043b\\u044e\\u0447 `pub` \\u0438 \\u0437\\u0430\\u043a\\u0440\\u044b\\u0442\\u044b\\u0439 \\u043a\\u043b\\u044e\\u0447 `priv` \\u0434\\u043b\\u044f RSA \\u0448\\u0438\\u0444\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u044f\", \"library\": [\"rsa\"]}", "generation": "Scenario 1:\nrsa, os, hashlib, base64\n```python\nimport rsa\nimport os\nimport hashlib\nfrom base64 import b64encode, b64decode\n\ndef generate_and_store_keys():\n    \"\"\"\n    Generate a public and private key pair for RSA encryption, then store the private \n    key in a file after encrypting it with a password.\n\n    Returns:\n    PublicKey: The RSA public key.\n    str: The filename where the encrypted private key is stored.\n\n    Requirements:\n    - rsa\n    - os\n    - hashlib\n    - base64\n\n    Example:\n    >>> pub_key, filename = generate_and_store_keys()\n    >>> print(pub_key)\n    >>> print(filename)\n    \"\"\"\n    (pub_key, priv_key) = rsa.newkeys(512)\n    password = os.urandom(16)\n    priv_key_encrypted = rsa.encrypt(password, priv_key)\n    priv_key_encrypted = b64encode(priv_key_encrypted).decode('utf-8')\n\n    filename = 'private_key.txt'\n    with open(filename, 'w') as f:\n        f.write(priv_key_encrypted)\n\n    return pub_key, filename\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nrsa, json, urllib, hashlib\n```python\nimport rsa\nimport json\nimport urllib.request\nfrom hashlib import sha256\n\ndef generate_keys_and_verify_url(url):\n    \"\"\"\n    Generate public and private keys for RSA encryption, then fetch the content of a \n    URL, compute its SHA256 hash, and sign it with the private key.\n\n    Parameters:\n    url (str): The URL to fetch and sign.\n\n    Returns:\n    PublicKey: The RSA public key.\n    str: The signed SHA256 hash of the URL content.\n\n    Requirements:\n    - rsa\n    - json\n    - urllib.request\n    - hashlib\n\n    Example:\n    >>> pub_key, signed_hash = generate_keys_and_verify_url('https://www.example.com')\n    >>> print(pub_key)\n    >>> print(signed_hash)\n    \"\"\"\n    (pub_key, priv_key) = rsa.newkeys(512)\n\n    response = urllib.request.urlopen(url)\n    content = response.read()\n    hash_value = sha256(content).digest()\n\n    signed_hash = rsa.sign(hash_value, priv_key, 'SHA-256').hex()\n\n    return pub_key, signed_hash\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nrsa, os, zipfile, base64\n```python\nimport rsa\nimport os\nfrom zipfile import ZipFile\nfrom base64 import b64encode, b64decode\n\ndef encrypt_files_in_directory(directory):\n    \"\"\"\n    Generate a public and private key pair for RSA encryption, then encrypt all files \n    in a directory, zip them and store in a new file.\n\n    Parameters:\n    directory (str): The directory containing the files to be encrypted.\n\n    Returns:\n    PublicKey: The RSA public key.\n    str: The filename of the zip file containing the encrypted files.\n\n    Requirements:\n    - rsa\n    - os\n    - zipfile\n    - base64\n\n    Example:\n    >>> pub_key, zipfile_name = encrypt_files_in_directory('./my_directory')\n    >>> print(pub_key)\n    >>> print(zipfile_name)\n    \"\"\"\n    (pub_key, priv_key) = rsa.newkeys(512)\n    zipfile_name = 'encrypted_files.zip'\n\n    with ZipFile(zipfile_name, 'w') as zipf:\n        for filename in os.listdir(directory):\n            with open(os.path.join(directory, filename), 'rb') as f:\n                data = f.read()\n                encrypted_data = rsa.encrypt(data, pub_key)\n                zipf.writestr(filename, b64encode(encrypted_data).decode('utf-8'))\n\n    return pub_key, zipfile_name\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nrsa, os, cryptography.fernet, base64\n```python\nimport rsa\nimport os\nfrom cryptography.fernet import Fernet\nfrom base64 import b64encode, b64decode\n\ndef encrypt_decrypt_file(file_path):\n    \"\"\"\n    Generate a public and private key pair for RSA encryption, then encrypt a file \n    using Fernet symmetric encryption. The Fernet key is then encrypted with the \n    RSA public key, and both the encrypted file and the encrypted Fernet key are \n    stored in new files.\n\n    Parameters:\n    file_path (str): The path to the file to be encrypted.\n\n    Returns:\n    PublicKey: The RSA public key.\n    str: The filename of the encrypted file.\n    str: The filename of the file containing the encrypted Fernet key.\n\n    Requirements:\n    - rsa\n    - os\n    - cryptography.fernet\n    - base64\n\n    Example:\n    >>> pub_key, encrypted_file, encrypted_key_file = encrypt_decrypt_file('my_file.txt')\n    >>> print(pub_key)\n    >>> print(encrypted_file)\n    >>> print(encrypted_key_file)\n    \"\"\"\n    (pub_key, priv_key) = rsa.newkeys(512)\n    fernet_key = Fernet.generate_key()\n    fernet = Fernet(fernet_key)\n\n    with open(file_path, 'rb') as f:\n        data = f.read()\n        encrypted_data = fernet.encrypt(data)\n\n    encrypted_file = file_path + '.encrypted'\n    with open(encrypted_file, 'wb') as f:\n        f.write(encrypted_data)\n\n    encrypted_fernet_key = rsa.encrypt(fernet_key, pub_key)\n    encrypted_key_file = 'fernet_key.encrypted'\n    with open(encrypted_key_file, 'wb') as f:\n        f.write(b64encode(encrypted_fernet_key))\n\n    return pub_key, encrypted_file, encrypted_key_file\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nrsa, os, cryptography.hazmat.primitives.ciphers, base64\n```python\nimport rsa\nimport os\nfrom cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\nfrom cryptography.hazmat.primitives import padding\nfrom base64 import b64encode, b64decode\n\ndef encrypt_decrypt_aes(file_path):\n    \"\"\"\n    Generate a public and private key pair for RSA encryption, then encrypt a file \n    using AES symmetric encryption. The AES key is then encrypted with the RSA \n    public key, and both the encrypted file and the encrypted AES key are stored in \n    new files.\n\n    Parameters:\n    file_path (str): The path to the file to be encrypted.\n\n    Returns:\n    PublicKey: The RSA public key.\n    str: The filename of the encrypted file.\n    str: The filename of the file containing the encrypted AES key.\n\n    Requirements:\n    - rsa\n    - os\n    - cryptography.hazmat.primitives.ciphers\n    - cryptography.hazmat.primitives\n    - base64\n\n    Example:\n    >>> pub_key, encrypted_file, encrypted_key_file = encrypt_decrypt_aes('my_file.txt')\n    >>> print(pub_key)\n    >>> print(encrypted_file)\n    >>> print(encrypted_key_file)\n    \"\"\"\n    (pub_key, priv_key) = rsa.newkeys(512)\n    aes_key = os.urandom(32)\n    iv = os.urandom(16)\n    cipher = Cipher(algorithms.AES(aes_key), modes.CBC(iv))\n\n    with open(file_path, 'rb') as f:\n        data = f.read()\n        padder = padding.PKCS7(128).padder()\n        data = padder.update(data) + padder.finalize()\n        encryptor = cipher.encryptor()\n        encrypted_data = encryptor.update(data) + encryptor.finalize()\n\n    encrypted_file = file_path + '.encrypted'\n    with open(encrypted_file, 'wb') as f:\n        f.write(encrypted_data)\n\n    encrypted_aes_key = rsa.encrypt(aes_key, pub_key)\n    encrypted_key_file = 'aes_key.encrypted'\n    with open(encrypted_key_file, 'wb') as f:\n        f.write(b64encode(encrypted_aes_key))\n\n    return pub_key, encrypted_file, encrypted_key_file\n```"}
{"seed": "{\"task_id\": 121512, \"prompt\": \"def f_121512(my_dict, key, my_var):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"my_dict[key]  ==  my_var\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    dict1 = {1 : 'abc', 2 : 'def'}\\n    assert candidate(dict1, 1, 'abc')\\n\", \"\\n    dict1 = {1 : 'abc', 2 : 'def'}\\n    assert candidate(dict1, 2, 'abc') == False\\n\"], \"entry_point\": \"f_121512\", \"intent\": \"\\u041f\\u0440\\u043e\\u0432\\u0435\\u0440\\u0438\\u0442\\u044c \\u0440\\u0430\\u0432\\u043d\\u043e \\u043b\\u0438 \\u0437\\u043d\\u0430\\u0447\\u0435\\u043d\\u0438\\u0435 \\u044d\\u043b\\u0435\\u043c\\u0435\\u043d\\u0442\\u0430 \\u0441\\u043b\\u043e\\u0432\\u0430\\u0440\\u044f `my_dict` \\u0441 \\u043a\\u043b\\u044e\\u0447\\u043e\\u043c `key` \\u0437\\u043d\\u0430\\u0447\\u0435\\u043d\\u0438\\u044e \\u0432 \\u043f\\u0435\\u0440\\u0435\\u043c\\u0435\\u043d\\u043d\\u043e\\u0439 `my_var`\", \"library\": []}", "generation": "Scenario 1: \ncollections, re, itertools\n```python\nimport collections\nimport re\nimport itertools\n\n# Constants\nLETTERS = 'abcdefghijklmnopqrstuvwxyz'\n\ndef count_and_group_words(text, word_length, top_n):\n    \"\"\"\n    Count the frequency of words of a specific length in the given text and \n    return the top_n most frequent words grouped by their first letter.\n\n    Parameters:\n    text (str): The text to analyze.\n    word_length (int): The length of the words to consider.\n    top_n (int): The number of top words to consider.\n\n    Returns:\n    dict: A dictionary with the most frequent words grouped by their first letter.\n\n    Requirements:\n    - collections\n    - re\n    - itertools\n    \n    Example:\n    >>> text = \"Hello world! This is a test. We are testing the test function.\"\n    >>> count_and_group_words(text, 4, 2)\n    \"\"\"\n    words = re.findall(r'\\b\\w{%d}\\b' % word_length, text.lower())\n    counter = collections.Counter(words)\n    top_words = counter.most_common(top_n)\n\n    grouped_words = {}\n    for key, group in itertools.groupby(top_words, lambda x: x[0][0]):\n        grouped_words[key] = list(group)\n\n    return grouped_words\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nitertools, math, numpy\n```python\nimport itertools\nimport math\nimport numpy as np\n\n# Constants\nPRIME_NUMBERS = np.array([2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47])\n\ndef generate_prime_partitions(num):\n    \"\"\"\n    Generate all the possible partitions of a given number using only prime numbers.\n\n    Parameters:\n    num (int): The number to partition.\n\n    Returns:\n    list: A list of tuples representing the prime partitions.\n\n    Requirements:\n    - itertools\n    - math\n    - numpy\n    \n    Example:\n    >>> generate_prime_partitions(10)\n    \"\"\"\n    prime_partitions = []\n\n    for r in range(1, num+1):\n        for combination in itertools.combinations_with_replacement(PRIME_NUMBERS, r):\n            if sum(combination) == num:\n                prime_partitions.append(combination)\n\n    return prime_partitions\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, sklearn.preprocessing\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\n\n# Constants\nFEATURES = ['feature1', 'feature2', 'feature3', 'feature4', 'feature5']\n\ndef standardize_features(df):\n    \"\"\"\n    Standardize the features in a given pandas DataFrame.\n\n    Parameters:\n    df (DataFrame): The DataFrame containing the features to standardize.\n\n    Returns:\n    DataFrame: The DataFrame with the standardized features.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.preprocessing.StandardScaler\n    \n    Example:\n    >>> df = pd.DataFrame(np.random.rand(10, 5), columns=FEATURES)\n    >>> standardize_features(df)\n    \"\"\"\n    scaler = StandardScaler()\n    df[FEATURES] = scaler.fit_transform(df[FEATURES])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, scipy.spatial.distance, sklearn.cluster\n```python\nimport numpy as np\nfrom scipy.spatial.distance import cdist\nfrom sklearn.cluster import KMeans\n\n# Constants\nINITIAL_CLUSTERS = 2\n\ndef optimal_kmeans_clusters(data, max_clusters):\n    \"\"\"\n    Determine the optimal number of clusters for KMeans clustering using the Elbow method.\n\n    Parameters:\n    data (ndarray): The data to cluster.\n    max_clusters (int): The maximum number of clusters to consider.\n\n    Returns:\n    int: The optimal number of clusters.\n\n    Requirements:\n    - numpy\n    - scipy.spatial.distance.cdist\n    - sklearn.cluster.KMeans\n    \n    Example:\n    >>> data = np.random.rand(100, 2)\n    >>> optimal_kmeans_clusters(data, 10)\n    \"\"\"\n    distortions = []\n    for k in range(1, max_clusters+1):\n        kmeans = KMeans(n_clusters=k)\n        kmeans.fit(data)\n        distortions.append(sum(np.min(cdist(data, kmeans.cluster_centers_, 'euclidean'), axis=1)) / data.shape[0])\n\n    elbow_point = np.diff(distortions, 2).argmax() + 1 + INITIAL_CLUSTERS\n\n    return elbow_point\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, scipy.stats, matplotlib\n```python\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\n# Constants\nBINS = 50\n\ndef plot_normal_distribution(data):\n    \"\"\"\n    Plot the normal distribution of a given dataset.\n\n    Parameters:\n    data (ndarray): The data to plot.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n    \n    Example:\n    >>> data = np.random.normal(loc=0.0, scale=1.0, size=1000)\n    >>> plot_normal_distribution(data)\n    \"\"\"\n    plt.hist(data, bins=BINS, density=True, alpha=0.6, color='g')\n\n    mu, std = stats.norm.fit(data)\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, mu, std)\n    plt.plot(x, p, 'k', linewidth=2)\n\n    title = \"Fit results: mu = %.2f,  std = %.2f\" % (mu, std)\n    plt.title(title)\n\n    plt.show()\n```\n"}
{"seed": "{\"task_id\": 316065, \"prompt\": \"def f_316065(n):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"(n & (n-1) == 0) and n != 0\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n   assert candidate(2) == True\\n\", \"\\n   assert candidate(16) == True\\n\", \"\\n   assert candidate(40) == False\\n\"], \"entry_point\": \"f_316065\", \"intent\": \"\\u041f\\u0440\\u043e\\u0432\\u0435\\u0440\\u0438\\u0442\\u044c \\u044f\\u0432\\u043b\\u044f\\u0435\\u0442\\u0441\\u044f \\u043b\\u0438 \\u0447\\u0438\\u0441\\u043b\\u043e `n` \\u0441\\u0442\\u0435\\u043f\\u0435\\u043d\\u044c\\u044e 2\", \"library\": []}", "generation": "Scenario 3:\nnumpy, pandas, random, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nNUMBERS = list(range(1, 101))\n\ndef check_power_of_two_and_generate_report():\n    \"\"\"\n    Check if numbers from 1 to 100 are power of 2 and generate a report. \n    Also, plot a bar graph of the counts of 'Yes' and 'No'.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the results.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> report = check_power_of_two_and_generate_report()\n    >>> print(report)\n    >>> report['Is Power of 2?'].value_counts().plot(kind='bar')\n    \"\"\"\n    report_data = []\n\n    for num in NUMBERS:\n        is_power_of_2 = 'Yes' if (num & (num-1) == 0) and num != 0 else 'No'\n        report_data.append([num, is_power_of_2])\n\n    report_df = pd.DataFrame(report_data, columns=['Number', 'Is Power of 2?'])\n\n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, pandas, random, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nNUMBERS = np.random.randint(1, 1000, 100)\n\ndef check_power_of_two_and_visualize():\n    \"\"\"\n    Check if 100 randomly generated numbers are power of 2 and visualize the \n    distribution of the numbers that are power of 2.\n    \n    Returns:\n    List[bool]: A list of boolean values indicating whether the numbers are power of 2.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> results = check_power_of_two_and_visualize()\n    >>> plt.hist([num for num, is_power_of_2 in zip(NUMBERS, results) if is_power_of_2], bins=20)\n    \"\"\"\n    results = [(num & (num-1) == 0) and num != 0 for num in NUMBERS]\n\n    return results\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnumpy, itertools, collections\n```python\nimport numpy as np\nfrom itertools import product\nfrom collections import Counter\n\n# Constants\nNUMBERS = np.random.randint(1, 100, 20)\n\ndef find_power_of_two_pairs():\n    \"\"\"\n    Find all pairs of numbers in a list where the bitwise AND of the numbers is a power of 2.\n    \n    Returns:\n    List[tuple]: A list of tuples where each tuple is a pair of numbers.\n    \n    Requirements:\n    - numpy\n    - itertools\n    - collections\n    \n    Example:\n    >>> pairs = find_power_of_two_pairs()\n    >>> print(pairs)\n    \"\"\"\n    pairs = []\n\n    for pair in product(NUMBERS, repeat=2):\n        if (pair[0] & pair[1] & (pair[0] & pair[1] - 1) == 0) and pair[0] & pair[1] != 0:\n            pairs.append(pair)\n\n    return pairs\n```\n`GPT_ODEX_BREAK`\n\nScenario 6:\nnumpy, scipy.stats\n```python\nimport numpy as np\nfrom scipy import stats\n\n# Constants\nNUMBERS = np.random.randint(1, 100, 1000)\n\ndef calculate_power_of_two_statistics():\n    \"\"\"\n    Calculate the mean, median, and mode of the numbers that are power of 2 in a list of numbers.\n    \n    Returns:\n    Dict[str, float]: A dictionary with the mean, median, and mode.\n    \n    Requirements:\n    - numpy\n    - scipy.stats\n    \n    Example:\n    >>> stats = calculate_power_of_two_statistics()\n    >>> print(stats)\n    \"\"\"\n    power_of_2_numbers = [num for num in NUMBERS if (num & (num-1) == 0) and num != 0]\n\n    mean = np.mean(power_of_2_numbers)\n    median = np.median(power_of_2_numbers)\n    mode = stats.mode(power_of_2_numbers)[0][0]\n\n    return {'mean': mean, 'median': median, 'mode': mode}\n```\n`GPT_ODEX_BREAK`\n\nScenario 7:\nnumpy, matplotlib\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nNUMBERS = np.random.randint(1, 100, 1000)\n\ndef plot_power_of_two_distribution():\n    \"\"\"\n    Plot the distribution of the numbers that are power of 2 in a list of numbers.\n    \n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> plot_power_of_two_distribution()\n    \"\"\"\n    power_of_2_numbers = [num for num in NUMBERS if (num & (num-1) == 0) and num != 0]\n\n    plt.hist(power_of_2_numbers, bins=20)\n    plt.show()\n```\n"}
{"seed": "{\"task_id\": 205664, \"prompt\": \"def f_205664(url):\\n\\t\", \"suffix\": \"\\n\\treturn code\", \"canonical_solution\": \"with urllib.request.urlopen(url) as res:\\n\\t    code = res.read()\", \"test_start\": \"\\nimport urllib \\n\\ndef check(candidate):\", \"test\": [\"\\n    assert 'Wikipedia' in str(candidate('https://www.wikipedia.org'))\\n\", \"\\n    assert 'Google' in str(candidate('https://www.google.com'))\\n\"], \"entry_point\": \"f_205664\", \"intent\": \"\\u0421\\u043f\\u0430\\u0440\\u0441\\u0438\\u0442\\u044c HTML-\\u043a\\u043e\\u0434 \\u0441\\u0442\\u0440\\u0430\\u043d\\u0438\\u0446\\u044b `code` \\u043f\\u043e \\u0441\\u0441\\u044b\\u043b\\u043a\\u0435 `url`\", \"library\": [\"urllib\"]}", "generation": "Scenario 1:\nurllib, BeautifulSoup, datetime, pandas\n```python\nimport urllib.request\nfrom bs4 import BeautifulSoup\nfrom datetime import datetime\nimport pandas as pd\n\ndef fetch_and_parse(url):\n    \"\"\"\n    Fetch HTML content from a given URL, parse it using BeautifulSoup, \n    extract all paragraph tags, and return a pandas DataFrame with the \n    content of each paragraph and the time it was fetched.\n\n    Parameters:\n    url (str): The URL to fetch the HTML content from.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the content of each paragraph and the time it was fetched.\n\n    Requirements:\n    - urllib.request\n    - BeautifulSoup\n    - datetime\n    - pandas\n\n    Example:\n    >>> df = fetch_and_parse('https://www.wikipedia.org')\n    >>> print(df.head())\n    \"\"\"\n    with urllib.request.urlopen(url) as res:\n        html = res.read()\n    soup = BeautifulSoup(html, 'html.parser')\n    p_tags = soup.find_all('p')\n    p_content = [p.get_text() for p in p_tags]\n    fetch_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    df = pd.DataFrame({'content': p_content, 'fetch_time': fetch_time})\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nurllib, json, datetime, hashlib\n```python\nimport urllib.request\nimport json\nfrom datetime import datetime\nimport hashlib\n\ndef fetch_and_hash(url):\n    \"\"\"\n    Fetch JSON content from a given URL, parse it, and return a dictionary \n    with the original content and a SHA256 hash of the content.\n\n    Parameters:\n    url (str): The URL to fetch the JSON content from.\n\n    Returns:\n    dict: A dictionary with the original content and a SHA256 hash of the content.\n\n    Requirements:\n    - urllib.request\n    - json\n    - datetime\n    - hashlib\n\n    Example:\n    >>> data = fetch_and_hash('https://api.github.com/users/octocat')\n    >>> print(data)\n    \"\"\"\n    with urllib.request.urlopen(url) as res:\n        data = json.loads(res.read().decode())\n    json_str = json.dumps(data, sort_keys=True)\n    hash_object = hashlib.sha256(json_str.encode())\n    return {'content': data, 'hash': hash_object.hexdigest()}\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nurllib, lxml, pandas, datetime\n```python\nimport urllib.request\nfrom lxml import etree\nimport pandas as pd\nfrom datetime import datetime\n\ndef fetch_and_extract(url):\n    \"\"\"\n    Fetch XML content from a given URL, parse it using lxml, \n    extract data from certain tags, and return a pandas DataFrame \n    with the extracted data and the time it was fetched.\n\n    Parameters:\n    url (str): The URL to fetch the XML content from.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the extracted data and the time it was fetched.\n\n    Requirements:\n    - urllib.request\n    - lxml\n    - pandas\n    - datetime\n\n    Example:\n    >>> df = fetch_and_extract('http://www.w3schools.com/xml/note.xml')\n    >>> print(df.head())\n    \"\"\"\n    with urllib.request.urlopen(url) as res:\n        xml = res.read()\n    root = etree.fromstring(xml)\n    data = [child.text for child in root.iter()]\n    fetch_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    df = pd.DataFrame({'data': data, 'fetch_time': fetch_time})\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nurllib, re, datetime, pandas\n```python\nimport urllib.request\nimport re\nfrom datetime import datetime\nimport pandas as pd\n\ndef fetch_and_search(url, pattern):\n    \"\"\"\n    Fetch HTML content from a given URL, search for all occurrences of a \n    regular expression pattern, and return a pandas DataFrame with the \n    matches and the time they were found.\n\n    Parameters:\n    url (str): The URL to fetch the HTML content from.\n    pattern (str): The regular expression pattern to search for.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the matches and the time they were found.\n\n    Requirements:\n    - urllib.request\n    - re\n    - datetime\n    - pandas\n\n    Example:\n    >>> df = fetch_and_search('https://www.wikipedia.org', 'Wikipedia')\n    >>> print(df.head())\n    \"\"\"\n    with urllib.request.urlopen(url) as res:\n        html = res.read().decode()\n    matches = re.findall(pattern, html)\n    fetch_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    df = pd.DataFrame({'match': matches, 'fetch_time': fetch_time})\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nurllib, pyquery, datetime, pandas\n```python\nimport urllib.request\nfrom pyquery import PyQuery as pq\nfrom datetime import datetime\nimport pandas as pd\n\ndef fetch_and_parse_with_pyquery(url):\n    \"\"\"\n    Fetch HTML content from a given URL, parse it using PyQuery, \n    extract all anchor tags, and return a pandas DataFrame with the \n    content of each anchor and the time it was fetched.\n\n    Parameters:\n    url (str): The URL to fetch the HTML content from.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the content of each anchor and the time it was fetched.\n\n    Requirements:\n    - urllib.request\n    - pyquery\n    - datetime\n    - pandas\n\n    Example:\n    >>> df = fetch_and_parse_with_pyquery('https://www.wikipedia.org')\n    >>> print(df.head())\n    \"\"\"\n    with urllib.request.urlopen(url) as res:\n        html = res.read().decode()\n    d = pq(html)\n    anchors = [a.text for a in d('a')]\n    fetch_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    df = pd.DataFrame({'anchor': anchors, 'fetch_time': fetch_time})\n    return df\n```\n"}
{"seed": "{\"task_id\": 300767, \"prompt\": \"def f_300767(strings, sep):\\n\\t\", \"suffix\": \"\\n\\treturn strings_dict\", \"canonical_solution\": \"strings_dict = dict([x[0].strip().split(sep) for x in strings])\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    strings = [[\\\"Hello=world!\\\"], [\\\"Hey=World!\\\"], [\\\"hi=world\\\"]]\\n    sep = \\\"=\\\"\\n    assert list(candidate(strings, sep).items()) == [(\\\"Hello\\\",\\\"world!\\\"), (\\\"Hey\\\",\\\"World!\\\"), (\\\"hi\\\",\\\"world\\\")]\\n\", \"\\n    strings = [[\\\"Hello:1!\\\"], [\\\"Hey:2!\\\"], [\\\"hi:3\\\"]]\\n    sep = \\\":\\\"\\n    assert list(candidate(strings, sep).items()) == [(\\\"Hello\\\",\\\"1!\\\"), (\\\"Hey\\\",\\\"2!\\\"), (\\\"hi\\\",\\\"3\\\")]\\n\"], \"entry_point\": \"f_300767\", \"intent\": \"\\u041f\\u0440\\u0435\\u043e\\u0431\\u0440\\u0430\\u0437\\u043e\\u0432\\u0430\\u0442\\u044c \\u043c\\u0430\\u0441\\u0441\\u0438\\u0432 \\u0441\\u0442\\u0440\\u043e\\u043a `strings` \\u0432 \\u0441\\u043b\\u043e\\u0432\\u0430\\u0440\\u044c `strings_dict` \\u043f\\u043e \\u0440\\u0430\\u0437\\u0434\\u0435\\u043b\\u0438\\u0442\\u0435\\u043b\\u044e `sep`\", \"library\": []}", "generation": "Scenario 1: \npandas, os, csv, itertools\n```python\nimport pandas as pd\nimport os\nimport csv\nfrom itertools import islice\n\n# Constants\nFILENAME = 'sample.csv'\nSEP = '='\n\ndef create_df_from_files(strings, sep):\n    \"\"\"\n    Create a pandas DataFrame by reading and converting a list of csv files into\n    dictionaries where each key-value pair is separated by a provided separator.\n    \n    Parameters:\n    strings (list): The list of csv file names.\n    sep (str): The separator used in the csv files to distinguish key-value pairs.\n    \n    Returns:\n    DataFrame: A pandas DataFrame constructed from the dictionaries.\n\n    Requirements:\n    - pandas\n    - os\n    - csv\n    - itertools\n\n    Example:\n    >>> strings = ['file1.csv', 'file2.csv', 'file3.csv']\n    >>> sep = '='\n    >>> df = create_df_from_files(strings, sep)\n    >>> print(df)\n    \"\"\"\n    data = []\n\n    # Read and convert each file into a dictionary\n    for string in strings:\n        with open(string, 'r') as file:\n            reader = csv.reader(file)\n            for row in islice(reader, 1, None):  # Exclude header\n                data.append(dict([x.strip().split(sep) for x in row]))\n    \n    # Create dataframe from the data\n    df = pd.DataFrame(data)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \ncollections, string, re, nltk\n```python\nimport collections\nimport string\nimport re\nfrom nltk.corpus import stopwords\n\n# nltk.download('stopwords')  # Run this line if nltk stopwords are not downloaded\nSTOPWORDS = set(stopwords.words('english'))\n\ndef count_word_occurrences(strings, sep):\n    \"\"\"\n    Convert a list of strings into a dictionary where each key-value pair represents \n    a word and its number of occurrences in the strings. The strings are preprocessed \n    to remove punctuation and stopwords.\n    \n    Parameters:\n    strings (list): The list of strings.\n    sep (str): The separator used in the strings to distinguish words.\n    \n    Returns:\n    dict: A dictionary with each word and its number of occurrences.\n\n    Requirements:\n    - collections\n    - string\n    - re\n    - nltk.corpus.stopwords\n\n    Example:\n    >>> strings = ['Hello world!', 'Hey world!', 'Hi world!']\n    >>> sep = ' '\n    >>> word_occurrences = count_word_occurrences(strings, sep)\n    >>> print(word_occurrences)\n    \"\"\"\n    word_counts = collections.defaultdict(int)\n\n    for string in strings:\n        # Remove punctuation and convert to lowercase\n        string = re.sub('['+string.punctuation+']', '', string).lower()\n        words = string.split(sep)\n        # Count occurrences of each word\n        for word in words:\n            if word not in STOPWORDS:\n                word_counts[word] += 1\n\n    return dict(word_counts)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnumpy, random, matplotlib.pyplot\n```python\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\n\n# Constants\nSEPERATOR = ','\nSTRINGS = ['1,2,3', '4,5,6', '7,8,9']\n\ndef plot_histogram(strings, sep):\n    \"\"\"\n    Convert a list of strings into a histogram where each string represents \n    a list of numbers separated by a provided separator.\n    \n    Parameters:\n    strings (list): The list of strings.\n    sep (str): The separator used in the strings to distinguish numbers.\n    \n    Requirements:\n    - numpy\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> strings = ['1,2,3', '4,5,6', '7,8,9']\n    >>> sep = ','\n    >>> plot_histogram(strings, sep)\n    \"\"\"\n    numbers = []\n\n    for string in strings:\n        numbers.extend([int(num) for num in string.split(sep)])\n\n    plt.hist(numbers, bins=np.arange(min(numbers), max(numbers) + 1.5) - 0.5, edgecolor='black')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nhashlib, binascii, base64\n```python\nimport hashlib\nimport binascii\nimport base64\n\n# Constants\nSEPERATOR = ':'\nSTRINGS = ['Hello', 'World', 'Python']\n\ndef create_hash_dict(strings, sep):\n    \"\"\"\n    Convert a list of strings into a dictionary where each key-value pair represents \n    a string and its corresponding hash value.\n    \n    Parameters:\n    strings (list): The list of strings.\n    \n    Returns:\n    dict: A dictionary with each string and its hash value.\n\n    Requirements:\n    - hashlib\n    - binascii\n    - base64\n\n    Example:\n    >>> strings = ['Hello', 'World', 'Python']\n    >>> hash_dict = create_hash_dict(strings)\n    >>> print(hash_dict)\n    \"\"\"\n    hash_dict = {}\n\n    for string in strings:\n        # Create a salted hash of the string\n        salt = hashlib.sha256()\n        salt.update(binascii.unhexlify('2a'))\n        salted_hash = hashlib.pbkdf2_hmac('sha256', string.encode(), salt.digest(), 100000)\n        hash_dict[string] = base64.b64encode(salted_hash).decode()\n\n    return hash_dict\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\njson, requests, time\n```python\nimport json\nimport requests\nimport time\n\n# Constants\nSEPERATOR = ':'\nSTRINGS = ['https://jsonplaceholder.typicode.com/posts/1', \n           'https://jsonplaceholder.typicode.com/posts/2', \n           'https://jsonplaceholder.typicode.com/posts/3']\n\ndef get_json_data(strings):\n    \"\"\"\n    Fetch JSON data from a list of URLs and store the data in a list of dictionaries.\n    \n    Parameters:\n    strings (list): The list of URLs.\n    \n    Returns:\n    list: A list of dictionaries with JSON data.\n\n    Requirements:\n    - json\n    - requests\n    - time\n\n    Example:\n    >>> strings = ['https://jsonplaceholder.typicode.com/posts/1', \n                   'https://jsonplaceholder.typicode.com/posts/2', \n                   'https://jsonplaceholder.typicode.com/posts/3']\n    >>> json_data = get_json_data(strings)\n    >>> print(json_data)\n    \"\"\"\n    json_data = []\n\n    for string in strings:\n        response = requests.get(string)\n        if response.status_code == 200:\n            json_data.append(response.json())\n        else:\n            print(f\"Failed to fetch data from {string}\")\n        time.sleep(1)  # To prevent overloading the server with requests\n\n    return json_data\n```"}
{"seed": "{\"task_id\": 127208, \"prompt\": \"def f_127208(filepath):\\n\\t\", \"suffix\": \"\\n\\treturn res\", \"canonical_solution\": \"with open(filepath, 'r') as f:\\n\\n\\t\\tlines = [i for i in f.readlines() if not i.startswith('[')]\\n\\n\\t\\tp = re.compile('(?P<key>\\\\S*(?==))\\\\S*(?P<znah>(?<==)\\\\S*)')\\n\\n\\t\\tres = {}\\n\\n\\t\\tfor line in lines:\\n\\n\\t\\t\\tparsed = re.findall(p, line)[0]\\n\\n\\t\\t\\tres[parsed[0]] = parsed[1]\\n\", \"test_start\": \"\\nimport re \\n\\ndef check(candidate):\", \"test\": [\"\\n    filepath = \\\"sample.ini\\\"\\n    lines = [\\n        \\\"[[config]]\\\", \\\"controlPort=8900\\\", \\\"idleTimeout=20\\\", \\n        \\\"[[admin]]\\\", \\\"login=petrovich\\\", \\\"email=petrovich@zavod.ru\\\", \\\"password=secret\\\"\\n    ]\\n    with open(filepath, 'w') as fw:\\n        for line in lines: fw.write(line + '\\\\n')\\n    assert candidate(filepath) == {'controlPort': '8900', \\n                                   'idleTimeout': '20',\\n                                   'login': 'petrovich',\\n                                   'email': 'petrovich@zavod.ru',\\n                                   'password': 'secret'}\\n\", \"\\n    filepath = \\\"test.ini\\\"\\n    lines = [\\n        \\\"[[config]]\\\", \\\"controlPort=8080\\\", \\\"idleTimeout=100\\\", \\n        \\\"[[admin]]\\\", \\\"login=admin\\\", \\\"email=admin@email.ru\\\", \\\"password=secret\\\"\\n    ]\\n    with open(filepath, 'w') as fw:\\n        for line in lines: fw.write(line + '\\\\n')\\n    assert candidate(filepath) == {'controlPort': '8080', \\n                                   'idleTimeout': '100',\\n                                   'login': 'admin',\\n                                   'email': 'admin@email.ru',\\n                                   'password': 'secret'}\\n\"], \"entry_point\": \"f_127208\", \"intent\": \"\\u0420\\u0430\\u0441\\u043f\\u0430\\u0440\\u0441\\u0438\\u0442\\u044c ini-\\u0444\\u0430\\u0439\\u043b `filepath` \\u0432 \\u0441\\u043b\\u043e\\u0432\\u0430\\u0440\\u044c `res`\", \"library\": [\"re\"]}", "generation": "Scenario 1: \nre, os, json, collections\n```python\nimport re\nimport os\nimport json\nfrom collections import defaultdict\n\ndef parse_files_in_directory_to_json(directory):\n    \"\"\"\n    Parse all .ini files in a given directory to a json file where filename is the key and parsed contents of the file is the value.\n    Parsed contents should not include lines that start with '['.\n    \n    Parameters:\n    directory (str): The directory that contains .ini files.\n    \n    Returns:\n    str: The filepath of the generated json file.\n    \n    Requirements:\n    - re\n    - os\n    - json\n    - collections\n    \n    Example:\n    >>> parse_files_in_directory_to_json('path/to/directory')\n    \"\"\"\n    file_dict = defaultdict(dict)\n    for filename in os.listdir(directory):\n        if filename.endswith('.ini'):\n            with open(os.path.join(directory, filename), 'r') as f:\n                lines = [i for i in f.readlines() if not i.startswith('[')]\n                p = re.compile('(?P<key>\\S*(?==))\\S*(?P<znah>(?<==)\\S*)')\n                for line in lines:\n                    parsed = re.findall(p, line)[0]\n                    file_dict[filename][parsed[0]] = parsed[1]\n    json_path = os.path.join(directory, 'parsed_files.json')\n    with open(json_path, 'w') as json_file:\n        json.dump(file_dict, json_file)\n    return json_path\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nre, os, json, collections, csv\n```python\nimport re\nimport os\nimport json\nimport csv\nfrom collections import defaultdict\n\ndef parse_files_in_directory_to_csv(directory):\n    \"\"\"\n    Parse all .ini files in a given directory to a csv file where each row represents a file and columns represent parsed keys and values from the file.\n    Parsed contents should not include lines that start with '['.\n    \n    Parameters:\n    directory (str): The directory that contains .ini files.\n    \n    Returns:\n    str: The filepath of the generated csv file.\n    \n    Requirements:\n    - re\n    - os\n    - json\n    - collections\n    - csv\n    \n    Example:\n    >>> parse_files_in_directory_to_csv('path/to/directory')\n    \"\"\"\n    file_dict = defaultdict(dict)\n    for filename in os.listdir(directory):\n        if filename.endswith('.ini'):\n            with open(os.path.join(directory, filename), 'r') as f:\n                lines = [i for i in f.readlines() if not i.startswith('[')]\n                p = re.compile('(?P<key>\\S*(?==))\\S*(?P<znah>(?<==)\\S*)')\n                for line in lines:\n                    parsed = re.findall(p, line)[0]\n                    file_dict[filename][parsed[0]] = parsed[1]\n    csv_path = os.path.join(directory, 'parsed_files.csv')\n    with open(csv_path, 'w', newline='') as csv_file:\n        writer = csv.writer(csv_file)\n        for key, value in file_dict.items():\n            writer.writerow([key] + list(value.items()))\n    return csv_path\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nre, os, json, collections, pandas\n```python\nimport re\nimport os\nimport json\nimport pandas as pd\nfrom collections import defaultdict\n\ndef parse_files_in_directory_to_dataframe(directory):\n    \"\"\"\n    Parse all .ini files in a given directory to a pandas DataFrame where each row represents a file and columns represent parsed keys and values from the file.\n    Parsed contents should not include lines that start with '['.\n    \n    Parameters:\n    directory (str): The directory that contains .ini files.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with parsed contents from the files.\n    \n    Requirements:\n    - re\n    - os\n    - json\n    - collections\n    - pandas\n    \n    Example:\n    >>> parse_files_in_directory_to_dataframe('path/to/directory')\n    \"\"\"\n    file_dict = defaultdict(dict)\n    for filename in os.listdir(directory):\n        if filename.endswith('.ini'):\n            with open(os.path.join(directory, filename), 'r') as f:\n                lines = [i for i in f.readlines() if not i.startswith('[')]\n                p = re.compile('(?P<key>\\S*(?==))\\S*(?P<znah>(?<==)\\S*)')\n                for line in lines:\n                    parsed = re.findall(p, line)[0]\n                    file_dict[filename][parsed[0]] = parsed[1]\n    df = pd.DataFrame.from_dict(file_dict, orient='index')\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nre, os, json, collections, sqlite3\n```python\nimport re\nimport os\nimport json\nimport sqlite3\nfrom collections import defaultdict\n\ndef parse_files_in_directory_to_database(directory):\n    \"\"\"\n    Parse all .ini files in a given directory to a SQLite database where each row represents a file and columns represent parsed keys and values from the file.\n    Parsed contents should not include lines that start with '['.\n    \n    Parameters:\n    directory (str): The directory that contains .ini files.\n    \n    Returns:\n    str: The filepath of the generated SQLite database.\n    \n    Requirements:\n    - re\n    - os\n    - json\n    - collections\n    - sqlite3\n    \n    Example:\n    >>> parse_files_in_directory_to_database('path/to/directory')\n    \"\"\"\n    file_dict = defaultdict(dict)\n    for filename in os.listdir(directory):\n        if filename.endswith('.ini'):\n            with open(os.path.join(directory, filename), 'r') as f:\n                lines = [i for i in f.readlines() if not i.startswith('[')]\n                p = re.compile('(?P<key>\\S*(?==))\\S*(?P<znah>(?<==)\\S*)')\n                for line in lines:\n                    parsed = re.findall(p, line)[0]\n                    file_dict[filename][parsed[0]] = parsed[1]\n    db_path = os.path.join(directory, 'parsed_files.db')\n    conn = sqlite3.connect(db_path)\n    cur = conn.cursor()\n    cur.execute(\"CREATE TABLE ParsedFiles (FileName text, ParsedContent text)\")\n    for key, value in file_dict.items():\n        cur.execute(\"INSERT INTO ParsedFiles VALUES (?, ?)\", (key, json.dumps(value)))\n    conn.commit()\n    conn.close()\n    return db_path\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nre, os, json, collections, ftplib\n```python\nimport re\nimport os\nimport json\nfrom collections import defaultdict\nfrom ftplib import FTP\n\nFTP_SERVER = 'ftp.dlptest.com'\nFTP_USER = 'dlpuser'\nFTP_PASS = 'rNrKYTX9g7z3RgJRmxWuGHbeu'\n\ndef parse_files_in_ftp_directory_to_json(directory):\n    \"\"\"\n    Parse all .ini files in a given directory of an FTP server to a json file where filename is the key and parsed contents of the file is the value.\n    Parsed contents should not include lines that start with '['.\n    \n    Parameters:\n    directory (str): The directory that contains .ini files on the FTP server.\n    \n    Returns:\n    str: The filepath of the generated json file.\n    \n    Requirements:\n    - re\n    - os\n    - json\n    - collections\n    - ftplib\n    \n    Example:\n    >>> parse_files_in_ftp_directory_to_json('path/to/directory')\n    \"\"\"\n    ftp = FTP(FTP_SERVER)\n    ftp.login(FTP_USER, FTP_PASS)\n    file_dict = defaultdict(dict)\n    for filename in ftp.nlst(directory):\n        if filename.endswith('.ini'):\n            with open(filename, 'wb') as f:\n                ftp.retrbinary('RETR ' + filename, f.write)\n            with open(filename, 'r') as f:\n                lines = [i for i in f.readlines() if not i.startswith('[')]\n                p = re.compile('(?P<key>\\S*(?==))\\S*(?P<znah>(?<==)\\S*)')\n                for line in lines:\n                    parsed = re.findall(p, line)[0]\n                    file_dict[filename][parsed[0]] = parsed[1]\n            os.remove(filename)\n    ftp.quit()\n    json_path = os.path.join(directory, 'parsed_files.json')\n    with open(json_path, 'w') as json_file:\n        json.dump(file_dict, json_file)\n    return json_path\n```\n"}
{"seed": "{\"task_id\": 316488, \"prompt\": \"def f_316488(filepath):\\n\\t\", \"suffix\": \"\\n\\treturn nums\", \"canonical_solution\": \"with open(filepath) as f:\\n\\n\\t\\tnums = list(map(int, f.read().split()))\\n\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    with open('test.txt', 'w') as fw:\\n        for s in ['1', '2', '3', '4']:\\n            fw.write(s + '\\\\n')\\n\\n    lines = candidate('test.txt')\\n    assert isinstance(lines, list)\\n\", \"\\n    with open('test.txt', 'w') as fw:\\n        for s in ['1', '2', '3', '4']:\\n            fw.write(s + '\\\\n')\\n\\n    lines = candidate('test.txt')\\n    assert lines[0] == 1\\n\", \"\\n    with open('test.txt', 'w') as fw:\\n        for s in ['1', '2', '3', '4']:\\n            fw.write(s + '\\\\n')\\n\\n    lines = candidate('test.txt')\\n    assert len(lines) == 4\\n\"], \"entry_point\": \"f_316488\", \"intent\": \"\\u041f\\u0440\\u043e\\u0447\\u0438\\u0442\\u0430\\u0442\\u044c \\u0441\\u043f\\u0438\\u0441\\u043e\\u043a \\u0447\\u0438\\u0441\\u0435\\u043b `nums` \\u0438\\u0437 \\u0444\\u0430\\u0439\\u043b\\u0430 `filepath`\", \"library\": []}", "generation": "Scenario 1:\nos, pandas, matplotlib, numpy\n```python\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_histogram_from_file(filepath):\n    \"\"\"\n    Read a list of numbers from a file and plot a histogram of the numbers.\n    \n    Parameters:\n    filepath (str): The file path.\n    \n    Returns:\n    None: A histogram plot is shown.\n    \n    Requirements:\n    - os\n    - pandas\n    - matplotlib.pyplot\n    - numpy\n    \n    Example:\n    >>> plot_histogram_from_file('numbers.txt')\n    \"\"\"\n    if not os.path.exists(filepath):\n        raise FileNotFoundError(f'{filepath} does not exist')\n\n    # Read the numbers into a pandas DataFrame\n    df = pd.read_csv(filepath, header=None, names=['numbers'])\n    \n    # Plot the histogram\n    plt.hist(df['numbers'], bins=np.sqrt(len(df)).astype(int))\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\njson, pprint, os, math\n```python\nimport json\nimport pprint\nimport os\nimport math\n\ndef process_json_file(filepath):\n    \"\"\"\n    Read a JSON file, calculate the square root of each number in the list, \n    and print the processed list in a pretty format.\n    \n    Parameters:\n    filepath (str): The file path.\n    \n    Returns:\n    List: A list of square roots of numbers.\n    \n    Requirements:\n    - json\n    - pprint\n    - os\n    - math\n    \n    Example:\n    >>> process_json_file('numbers.json')\n    \"\"\"\n    if not os.path.exists(filepath):\n        raise FileNotFoundError(f'{filepath} does not exist')\n        \n    with open(filepath, 'r') as f:\n        numbers = json.load(f)\n\n    # Calculate the square root of each number\n    sqrt_numbers = [math.sqrt(num) for num in numbers]\n    \n    # Print the list in a pretty format\n    pprint.pprint(sqrt_numbers)\n    \n    return sqrt_numbers\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, csv, statistics, matplotlib\n```python\nimport os\nimport csv\nimport statistics\nimport matplotlib.pyplot as plt\n\ndef analyze_csv_file(filepath):\n    \"\"\"\n    Read a CSV file, calculate the mean and standard deviation of the numbers \n    in the first column, and plot the numbers with the mean and standard deviation.\n    \n    Parameters:\n    filepath (str): The file path.\n    \n    Returns:\n    Tuple: A tuple of mean and standard deviation.\n    \n    Requirements:\n    - os\n    - csv\n    - statistics\n    - matplotlib.pyplot\n    \n    Example:\n    >>> analyze_csv_file('data.csv')\n    \"\"\"\n    if not os.path.exists(filepath):\n        raise FileNotFoundError(f'{filepath} does not exist')\n        \n    with open(filepath, 'r') as f:\n        reader = csv.reader(f)\n        numbers = [int(row[0]) for row in reader]\n        \n    # Calculate the mean and standard deviation\n    mean = statistics.mean(numbers)\n    stdev = statistics.stdev(numbers)\n    \n    # Plot the numbers with the mean and standard deviation\n    plt.hist(numbers, bins=20)\n    plt.axvline(mean, color='r', linestyle='dashed', linewidth=2)\n    plt.axvline(mean + stdev, color='g', linestyle='dotted', linewidth=2)\n    plt.axvline(mean - stdev, color='g', linestyle='dotted', linewidth=2)\n    plt.show()\n    \n    return mean, stdev\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, pandas, seaborn, scipy\n```python\nimport os\nimport pandas as pd\nimport seaborn as sns\nfrom scipy.stats import norm\n\ndef plot_distribution_from_file(filepath):\n    \"\"\"\n    Read a list of numbers from a file and plot a distribution of the numbers.\n    \n    Parameters:\n    filepath (str): The file path.\n    \n    Returns:\n    None: A distribution plot is shown.\n    \n    Requirements:\n    - os\n    - pandas\n    - seaborn\n    - scipy.stats.norm\n    \n    Example:\n    >>> plot_distribution_from_file('numbers.txt')\n    \"\"\"\n    if not os.path.exists(filepath):\n        raise FileNotFoundError(f'{filepath} does not exist')\n\n    # Read the numbers into a pandas DataFrame\n    df = pd.read_csv(filepath, header=None, names=['numbers'])\n    \n    # Plot the distribution\n    sns.distplot(df['numbers'], fit=norm, kde=False)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, json, collections, pprint\n```python\nimport os\nimport json\nimport collections\nimport pprint\n\ndef count_frequency_from_json_file(filepath):\n    \"\"\"\n    Read a list of words from a JSON file and count the frequency of each word.\n    \n    Parameters:\n    filepath (str): The file path.\n    \n    Returns:\n    dict: A dictionary of word frequencies.\n    \n    Requirements:\n    - os\n    - json\n    - collections\n    - pprint\n    \n    Example:\n    >>> count_frequency_from_json_file('words.json')\n    \"\"\"\n    if not os.path.exists(filepath):\n        raise FileNotFoundError(f'{filepath} does not exist')\n        \n    with open(filepath, 'r') as f:\n        words = json.load(f)\n\n    # Count the frequency of each word\n    counter = collections.Counter(words)\n    \n    # Print the counter in a pretty format\n    pprint.pprint(counter)\n    \n    return counter\n```"}
{"seed": "{\"task_id\": 77855, \"prompt\": \"def f_77855(url, data):\\n\\t\", \"suffix\": \"\\n\\treturn res\", \"canonical_solution\": \"res = requests.post(url, json=data)\", \"test_start\": \"\\nimport requests\\n\\ndef check(candidate):\", \"test\": [\"\\n    data = {'key': 'value'}\\n    url = \\\"http://ptsv2.com/t/6ju5j-1661859736/post\\\"\\n    res = candidate(url, data)\\n    assert res.url == url\\n    assert res.text == \\\"Thank you for this dump. I hope you have a lovely day!\\\"\\n    assert res.request.body == b'{\\\"key\\\": \\\"value\\\"}'\\n\"], \"entry_point\": \"f_77855\", \"intent\": \"\\u041e\\u0442\\u043f\\u0440\\u0430\\u0432\\u0438\\u0442\\u044c POST-\\u0437\\u0430\\u043f\\u0440\\u043e\\u0441 `res` c JSON \\u0434\\u0430\\u043d\\u043d\\u044b\\u043c\\u0438 `data` \\u043d\\u0430 `url`\", \"library\": [\"requests\"]}", "generation": "Scenario 1:\nrequests, json, os, time\n```python\nimport requests\nimport json\nimport os\nimport time\n\n# Constants\nHEADERS = {\n    'Content-Type': 'application/json',\n    'User-Agent': 'Python Requests'\n}\nOUTPUT_DIR = 'output'\n\ndef download_and_save_json(url, output_file):\n    \"\"\"\n    Send a GET request to a `url` and save the JSON response to a file named `output_file` in \n    an output directory. If the output directory does not exist, create it. If the GET request \n    fails, wait for 5 seconds and try again.\n\n    Parameters:\n    url (str): The URL to send the GET request to.\n    output_file (str): The name of the file to save the JSON response to.\n\n    Returns:\n    str: The path to the saved file.\n\n    Requirements:\n    - requests\n    - json\n    - os\n    - time\n\n    Example:\n    >>> download_and_save_json('https://jsonplaceholder.typicode.com/posts/1', 'post.json')\n    \"\"\"\n    if not os.path.exists(OUTPUT_DIR):\n        os.mkdir(OUTPUT_DIR)\n        \n    output_path = os.path.join(OUTPUT_DIR, output_file)\n    \n    while True:\n        try:\n            response = requests.get(url, headers=HEADERS)\n            response.raise_for_status()\n            break\n        except requests.exceptions.HTTPError:\n            time.sleep(5)\n\n    with open(output_path, 'w') as f:\n        json.dump(response.json(), f)\n\n    return output_path\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nrequests, bs4, pandas, os\n```python\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport os\n\n# Constants\nHEADERS = {\n    'User-Agent': 'Python Requests'\n}\nBASE_URL = 'https://www.imdb.com'\n\ndef scrape_top_movies(output_file):\n    \"\"\"\n    Scrape the top 250 movies from IMDB and save it to a CSV file named `output_file` in \n    the current directory. Each movie should have the title, year, and rating.\n\n    Parameters:\n    output_file (str): The name of the file to save the scraped data to.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the scraped data.\n\n    Requirements:\n    - requests\n    - bs4.BeautifulSoup\n    - pandas\n    - os\n\n    Example:\n    >>> scrape_top_movies('top_movies.csv')\n    \"\"\"\n    response = requests.get(f'{BASE_URL}/chart/top', headers=HEADERS)\n    soup = BeautifulSoup(response.content, 'html.parser')\n\n    movies = []\n    for movie in soup.find_all('td', class_='titleColumn'):\n        title = movie.a.text\n        year = movie.span.text.strip('()')\n        rating = movie.nextSibling.nextSibling.text\n        movies.append([title, year, rating])\n\n    df = pd.DataFrame(movies, columns=['Title', 'Year', 'Rating'])\n    df.to_csv(output_file, index=False)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nrequests, json, csv, os\n```python\nimport requests\nimport json\nimport csv\nimport os\n\n# Constants\nHEADERS = {\n    'Content-Type': 'application/json',\n    'User-Agent': 'Python Requests'\n}\nAPI_URL = 'https://api.github.com/repos'\n\ndef get_repo_issues(owner, repo, output_file):\n    \"\"\"\n    Fetch issues of a specific GitHub repository and save them to a CSV file named `output_file` in \n    the current directory.\n\n    Parameters:\n    owner (str): The owner of the repository.\n    repo (str): The name of the repository.\n    output_file (str): The name of the file to save the issues to.\n\n    Returns:\n    str: The path to the saved file.\n\n    Requirements:\n    - requests\n    - json\n    - csv\n    - os\n\n    Example:\n    >>> get_repo_issues('python', 'cpython', 'issues.csv')\n    \"\"\"\n    response = requests.get(f'{API_URL}/{owner}/{repo}/issues', headers=HEADERS)\n    issues = response.json()\n\n    with open(output_file, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(['Number', 'Title', 'URL'])\n        for issue in issues:\n            writer.writerow([issue['number'], issue['title'], issue['html_url']])\n\n    return output_file\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nrequests, time, random, json\n```python\nimport requests\nimport time\nimport random\nimport json\n\n# Constants\nAPI_KEY = 'your_api_key'\nBASE_URL = 'https://api.openweathermap.org/data/2.5/weather'\nCITIES = ['London', 'Paris', 'Berlin', 'Rome', 'Madrid']\n\ndef get_weather_data():\n    \"\"\"\n    Fetch weather data for a list of cities from the OpenWeatherMap API. If a fetch fails, \n    wait for a random number of seconds between 1 and 5 before trying again.\n\n    Returns:\n    dict: A dictionary with the cities as keys and the weather data as values.\n\n    Requirements:\n    - requests\n    - time\n    - random\n    - json\n\n    Example:\n    >>> get_weather_data()\n    \"\"\"\n    weather_data = {}\n\n    for city in CITIES:\n        while True:\n            try:\n                response = requests.get(BASE_URL, params={'q': city, 'appid': API_KEY})\n                response.raise_for_status()\n                weather_data[city] = response.json()\n                break\n            except requests.exceptions.HTTPError:\n                time.sleep(random.randint(1, 5))\n\n    return weather_data\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nrequests, csv, json, os\n```python\nimport requests\nimport csv\nimport json\nimport os\n\n# Constants\nAPI_URL = 'https://api.github.com/users'\nHEADERS = {\n    'Accept': 'application/vnd.github.v3+json'\n}\n\ndef fetch_user_repos(username, output_file):\n    \"\"\"\n    Fetch the public repositories of a specific GitHub user and save the repository names and URLs \n    to a CSV file named `output_file` in the current directory.\n\n    Parameters:\n    username (str): The username of the GitHub user.\n    output_file (str): The name of the file to save the repository names and URLs to.\n\n    Returns:\n    str: The path to the saved file.\n\n    Requirements:\n    - requests\n    - csv\n    - json\n    - os\n\n    Example:\n    >>> fetch_user_repos('torvalds', 'repos.csv')\n    \"\"\"\n    response = requests.get(f'{API_URL}/{username}/repos', headers=HEADERS)\n    repos = response.json()\n\n    with open(output_file, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(['Name', 'URL'])\n        for repo in repos:\n            writer.writerow([repo['name'], repo['html_url']])\n\n    return output_file\n```\n"}
{"seed": "{\"task_id\": 25811, \"prompt\": \"def f_25811(arrs):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"for a in arrs:\\n\\n\\t\\tprint(a)\\n\", \"test_start\": \"\\nimport sys\\nfrom io import StringIO\\n\\ndef check(candidate):\", \"test\": [\"\\n    stdout = sys.stdout\\n    s = StringIO()\\n    sys.stdout = s\\n    arrs = [[1,2,3], [4,5,6]]\\n    candidate(arrs)\\n    sys.stdout = stdout    \\n    s.seek(0)\\n    assert len(s.read()) == 20\\n\", \"\\n    stdout = sys.stdout\\n    s = StringIO()\\n    sys.stdout = s\\n    arrs = [[\\\"H\\\", \\\"W\\\"], [\\\"A\\\", \\\"B\\\", \\\"C\\\"]]\\n    candidate(arrs)\\n    sys.stdout = stdout    \\n    s.seek(0)\\n    assert len(s.read()) == 27\\n\"], \"entry_point\": \"f_25811\", \"intent\": \"\\u0412\\u044b\\u0432\\u0435\\u0441\\u0442\\u0438 \\u0434\\u0432\\u0443\\u043c\\u0435\\u0440\\u043d\\u044b\\u0439 \\u043c\\u0430\\u0441\\u0441\\u0438\\u0432 `arrs` \\u043d\\u0430 \\u044d\\u043a\\u0440\\u0430\\u043d\", \"library\": [\"io\", \"sys\"]}", "generation": "Scenario 1:\nnumpy, scipy.stats, matplotlib, itertools\n```python\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nfrom itertools import combinations\n\n# Constants\nCOLORS = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n\ndef calculate_and_plot_correlations(arrs):\n    \"\"\"\n    Calculate the Pearson correlation coefficients between the arrays in 'arrs' \n    and plot the correlation matrix as a heatmap.\n\n    Parameters:\n    arrs (list of arrays): A list of 1D arrays.\n\n    Returns:\n    dict: A dictionary with pairs of arrays as keys and their correlation \n          coefficients as values.\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n    - itertools.combinations\n\n    Example:\n    >>> arrs = [np.random.rand(10) for _ in range(3)]\n    >>> correlations = calculate_and_plot_correlations(arrs)\n    >>> print(correlations)\n    \"\"\"\n    correlations = {}\n\n    for i, (arr1, arr2) in enumerate(combinations(arrs, 2)):\n        corr, _ = stats.pearsonr(arr1, arr2)\n        correlations[(i, i+1)] = corr\n    \n    corr_matrix = np.zeros((len(arrs), len(arrs)))\n\n    for (i, j), corr in correlations.items():\n        corr_matrix[i, j] = corr\n        corr_matrix[j, i] = corr\n\n    plt.matshow(corr_matrix, cmap='coolwarm')\n    plt.colorbar()\n    plt.show()\n\n    return correlations\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, matplotlib, sklearn.decomposition, sklearn.preprocessing\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\ndef apply_pca_and_plot(arrs):\n    \"\"\"\n    Apply Principal Component Analysis (PCA) to the arrays in 'arrs' and plot \n    the first two principal components.\n\n    Parameters:\n    arrs (list of arrays): A list of 1D arrays.\n\n    Returns:\n    ndarray: The transformed data array.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.decomposition.PCA\n    - sklearn.preprocessing.StandardScaler\n\n    Example:\n    >>> arrs = [np.random.rand(100) for _ in range(10)]\n    >>> transformed_data = apply_pca_and_plot(arrs)\n    >>> print(transformed_data)\n    \"\"\"\n    data = np.vstack(arrs).T\n\n    scaler = StandardScaler()\n    scaled_data = scaler.fit_transform(data)\n\n    pca = PCA(n_components=2)\n    transformed_data = pca.fit_transform(scaled_data)\n\n    plt.scatter(transformed_data[:, 0], transformed_data[:, 1])\n    plt.xlabel('Principal Component 1')\n    plt.ylabel('Principal Component 2')\n    plt.show()\n\n    return transformed_data\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, matplotlib, scipy.signal, sklearn.preprocessing\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.signal import spectrogram\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef calculate_spectrogram_and_plot(arrs):\n    \"\"\"\n    Calculate the spectrogram for each array in 'arrs' and plot them.\n\n    Parameters:\n    arrs (list of arrays): A list of 1D arrays representing signals.\n\n    Returns:\n    list of tuples: A list of tuples where each tuple contains the frequencies, \n                    time points and spectrogram of a signal.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - scipy.signal.spectrogram\n    - sklearn.preprocessing.MinMaxScaler\n\n    Example:\n    >>> arrs = [np.random.rand(1000) for _ in range(3)]\n    >>> spectrograms = calculate_spectrogram_and_plot(arrs)\n    >>> print(spectrograms)\n    \"\"\"\n    spectrograms = []\n\n    for i, arr in enumerate(arrs):\n        freqs, times, Sxx = spectrogram(arr)\n        Sxx = MinMaxScaler().fit_transform(Sxx)\n\n        plt.pcolormesh(times, freqs, Sxx, shading='gouraud')\n        plt.ylabel('Frequency [Hz]')\n        plt.xlabel('Time [sec]')\n        plt.title(f'Spectrogram of Signal {i+1}')\n        plt.show()\n\n        spectrograms.append((freqs, times, Sxx))\n\n    return spectrograms\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, scipy.optimize, matplotlib, itertools\n```python\nimport numpy as np\nfrom scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\nfrom itertools import cycle\n\n# Constants\nFIT_FUNCTIONS = [np.poly1d([1, 2]), np.exp, np.log]\nCOLORS = cycle(['b', 'g', 'r', 'c', 'm', 'y', 'k'])\n\ndef fit_functions_and_plot(arrs):\n    \"\"\"\n    Fit different functions to the data in 'arrs' and plot the data and the \n    fitted functions.\n\n    Parameters:\n    arrs (list of arrays): A list of 1D arrays representing y-values of data. \n                           The x-values are assumed to be range(len(arr)).\n\n    Returns:\n    list of tuples: A list of tuples where each tuple contains the function, \n                    the fitted parameters and the function's color.\n\n    Requirements:\n    - numpy\n    - scipy.optimize.curve_fit\n    - matplotlib.pyplot\n    - itertools.cycle\n\n    Example:\n    >>> arrs = [np.random.rand(10) for _ in range(3)]\n    >>> fits = fit_functions_and_plot(arrs)\n    >>> print(fits)\n    \"\"\"\n    fits = []\n\n    for arr, func in zip(arrs, FIT_FUNCTIONS):\n        x = np.arange(len(arr))\n        params, _ = curve_fit(func, x, arr)\n        color = next(COLORS)\n\n        plt.plot(x, arr, 'o', color=color)\n        plt.plot(x, func(x, *params), '-', color=color)\n        fits.append((func, params, color))\n\n    plt.show()\n\n    return fits\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, matplotlib, sklearn.cluster, sklearn.preprocessing\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef apply_kmeans_and_plot(arrs):\n    \"\"\"\n    Apply KMeans clustering to the arrays in 'arrs' and plot the clusters.\n\n    Parameters:\n    arrs (list of arrays): A list of 1D arrays.\n\n    Returns:\n    ndarray: The cluster labels for each data point.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.cluster.KMeans\n    - sklearn.preprocessing.MinMaxScaler\n\n    Example:\n    >>> arrs = [np.random.rand(100) for _ in range(2)]\n    >>> labels = apply_kmeans_and_plot(arrs)\n    >>> print(labels)\n    \"\"\"\n    data = np.vstack(arrs).T\n\n    scaler = MinMaxScaler()\n    scaled_data = scaler.fit_transform(data)\n\n    kmeans = KMeans(n_clusters=3)\n    labels = kmeans.fit_predict(scaled_data)\n\n    plt.scatter(scaled_data[:, 0], scaled_data[:, 1], c=labels)\n    plt.show()\n\n    return labels\n```"}
{"seed": "{\"task_id\": 25811, \"prompt\": \"def f_25811(arrs):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"\\n\\tfor a in arrs:\\n\\t\\tfor i in a:\\n\\t\\t\\tprint(i, end=\\\" \\\")\\n\\tprint()\\n\", \"test_start\": \"\\nimport sys\\nfrom io import StringIO\\n\\ndef check(candidate):\", \"test\": [\"\\n    stdout = sys.stdout\\n    s = StringIO()\\n    sys.stdout = s\\n    arrs = [[1,2,3], [4,5,6]]\\n    candidate(arrs)\\n    sys.stdout = stdout    \\n    s.seek(0)\\n    assert len(s.read()) == 13\\n\", \"\\n    stdout = sys.stdout\\n    s = StringIO()\\n    sys.stdout = s\\n    arrs = [[\\\"H\\\", \\\"W\\\"], [\\\"A\\\", \\\"B\\\", \\\"C\\\"]]\\n    candidate(arrs)\\n    sys.stdout = stdout    \\n    s.seek(0)\\n    assert len(s.read()) == 11\\n\"], \"entry_point\": \"f_25811\", \"intent\": \"\\u0412\\u044b\\u0432\\u0435\\u0441\\u0442\\u0438 \\u0434\\u0432\\u0443\\u043c\\u0435\\u0440\\u043d\\u044b\\u0439 \\u043c\\u0430\\u0441\\u0441\\u0438\\u0432 `arrs` \\u043d\\u0430 \\u044d\\u043a\\u0440\\u0430\\u043d\", \"library\": [\"io\", \"sys\"]}", "generation": "Scenario 1: \nnumpy, pandas, matplotlib, itertools\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom itertools import chain\n\ndef plot_histogram_for_array(arrs):\n    \"\"\"\n    Flatten a 2D array and plot a histogram of the values.\n\n    Parameters:\n    arrs (list): A 2D list of numbers.\n\n    Returns:\n    None.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - itertools.chain\n\n    Example:\n    >>> arrs = [[1, 2, 3], [4, 5, 6]]\n    >>> plot_histogram_for_array(arrs)\n    \"\"\"\n    flattened_arr = list(chain.from_iterable(arrs))\n\n    plt.hist(flattened_arr, bins = len(set(flattened_arr)))\n    plt.show()\n\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nnumpy, pandas, matplotlib, itertools\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom itertools import chain\n\ndef get_array_statistics(arrs):\n    \"\"\"\n    Flatten a 2D array and calculate statistical summary (mean, median, mode, standard deviation, variance). \n\n    Parameters:\n    arrs (list): A 2D list of numbers.\n\n    Returns:\n    dict: A dictionary of statistical summary\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - itertools.chain\n\n    Example:\n    >>> arrs = [[1, 2, 3], [4, 5, 6]]\n    >>> get_array_statistics(arrs)\n    {'Mean': 3.5, 'Median': 3.5, 'Mode': [1, 2, 3, 4, 5, 6], 'Standard Deviation': 1.87, 'Variance': 3.5}\n    \"\"\"\n    flattened_arr = list(chain.from_iterable(arrs))\n    stats = {\n        'Mean': np.mean(flattened_arr),\n        'Median': np.median(flattened_arr),\n        'Mode': list(pd.Series(flattened_arr).mode()),\n        'Standard Deviation': np.std(flattened_arr),\n        'Variance': np.var(flattened_arr)\n    }\n    return stats\n\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnumpy, pandas, seaborn, itertools\n```python\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom itertools import chain\n\ndef plot_density_for_array(arrs):\n    \"\"\"\n    Flatten a 2D array and plot a density plot of the values.\n\n    Parameters:\n    arrs (list): A 2D list of numbers.\n\n    Returns:\n    None.\n\n    Requirements:\n    - numpy\n    - pandas\n    - seaborn\n    - itertools.chain\n\n    Example:\n    >>> arrs = [[1, 2, 3], [4, 5, 6]]\n    >>> plot_density_for_array(arrs)\n    \"\"\"\n    flattened_arr = list(chain.from_iterable(arrs))\n\n    sns.kdeplot(flattened_arr)\n    plt.show()\n\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, pandas, sklearn.preprocessing, itertools\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nfrom itertools import chain\n\ndef normalize_2d_array(arrs):\n    \"\"\"\n    Flatten a 2D array and normalize the values between 0 and 1.\n\n    Parameters:\n    arrs (list): A 2D list of numbers.\n\n    Returns:\n    list: A list of normalized numbers.\n\n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.preprocessing\n    - itertools.chain\n\n    Example:\n    >>> arrs = [[1, 2, 3], [4, 5, 6]]\n    >>> normalize_2d_array(arrs)\n    [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]\n    \"\"\"\n    flattened_arr = np.array(list(chain.from_iterable(arrs))).reshape(-1, 1)\n    scaler = MinMaxScaler()\n    normalized_arr = scaler.fit_transform(flattened_arr)\n    return normalized_arr.flatten().tolist()\n\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnumpy, pandas, sklearn.decomposition, itertools\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.decomposition import PCA\nfrom itertools import chain\n\ndef apply_pca_to_2d_array(arrs):\n    \"\"\"\n    Apply Principal Component Analysis (PCA) to a 2D array and reduce it to 1D.\n\n    Parameters:\n    arrs (list): A 2D list of numbers.\n\n    Returns:\n    list: A list of numbers after PCA.\n\n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.decomposition\n    - itertools.chain\n\n    Example:\n    >>> arrs = [[1, 2, 3], [4, 5, 6]]\n    >>> apply_pca_to_2d_array(arrs)\n    [-1.5, -0.5, 0.5, 1.5]\n    \"\"\"\n    pca = PCA(n_components=1)\n    transformed_arr = pca.fit_transform(arrs)\n    return transformed_arr.flatten().tolist()\n\n```"}
{"seed": "{\"task_id\": 54032, \"prompt\": \"def f_54032(num_start, num_end):\\n\\t\", \"suffix\": \"\\n\\treturn num\", \"canonical_solution\": \"num = random.randint(num_start, num_end)\", \"test_start\": \"\\nimport random\\n\\ndef check(candidate):\", \"test\": [\"\\n    random_res = candidate(10, 20)\\n    assert random_res >= 10 and random_res <= 20\\n\", \"\\n    random_res = candidate(15, 30)\\n    assert random_res >= 15 and random_res <= 30\\n\"], \"entry_point\": \"f_54032\", \"intent\": \"\\u041f\\u043e\\u043b\\u0443\\u0447\\u0438\\u0442\\u044c \\u0441\\u043b\\u0443\\u0447\\u0430\\u0439\\u043d\\u043e\\u0435 \\u0447\\u0438\\u0441\\u043b\\u0430 `num` \\u0432 \\u0438\\u043d\\u0442\\u0435\\u0440\\u0432\\u0430\\u043b\\u0435 \\u043e\\u0442 \\u0447\\u0438\\u0441\\u043b\\u0430 `num_start` \\u0434\\u043e \\u0447\\u0438\\u0441\\u043b\\u0430 `num_end`\", \"library\": [\"random\"]}", "generation": "Scenario 1: \npandas, random, matplotlib.pyplot, numpy\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nCATEGORIES = ['Electronics', 'Clothing', 'Home and Kitchen', 'Health and Beauty', 'Sports and Outdoors']\n\ndef generate_sales_report(num_start, num_end):\n    \"\"\"\n    Generate a sales report for different categories of products with random sales values \n    between num_start and num_end.\n\n    Parameters:\n    num_start (int): The minimum sales value.\n    num_end (int): The maximum sales value.\n\n    Returns:\n    DataFrame: A pandas DataFrame with sales data for the categories.\n\n    Requirements:\n    - pandas\n    - random\n    - matplotlib.pyplot\n    - numpy\n\n    Example:\n    >>> report = generate_sales_report(1000, 5000)\n    >>> print(report)\n    >>> report['Sales'].plot(kind='bar')\n    \"\"\"\n    sales_data = []\n\n    for category in CATEGORIES:\n        sales = randint(num_start, num_end)\n        sales_data.append([category, sales])\n\n    sales_df = pd.DataFrame(sales_data, columns=['Category', 'Sales'])\n\n    return sales_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nrandom, numpy, matplotlib.pyplot\n```python\nimport numpy as np\nfrom random import randint\nimport matplotlib.pyplot as plt\n\ndef histogram_of_random_numbers(num_start, num_end):\n    \"\"\"\n    Generate a histogram of 1000 random numbers in the range from num_start to num_end.\n\n    Parameters:\n    num_start (int): The minimum number value.\n    num_end (int): The maximum number value.\n\n    Returns:\n    None\n\n    Requirements:\n    - random\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> histogram_of_random_numbers(1, 100)\n    \"\"\"\n    numbers = np.random.randint(num_start, num_end, 1000)\n    plt.hist(numbers, bins=20, alpha=0.5)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nrandom, numpy, pandas\n```python\nimport numpy as np\nfrom random import randint\nimport pandas as pd\n\n# Constants\nSTUDENTS = ['John', 'Jessica', 'Steve', 'Emily', 'Michael', 'Sarah', 'Jake', 'Emma']\n\ndef generate_student_scores(num_start, num_end):\n    \"\"\"\n    Generate a pandas DataFrame of random scores for a list of students in the range from num_start to num_end.\n\n    Parameters:\n    num_start (int): The minimum score value.\n    num_end (int): The maximum score value.\n\n    Returns:\n    DataFrame: A pandas DataFrame with student scores.\n\n    Requirements:\n    - random\n    - numpy\n    - pandas\n\n    Example:\n    >>> scores = generate_student_scores(60, 100)\n    >>> print(scores)\n    \"\"\"\n    scores_data = []\n\n    for student in STUDENTS:\n        score = randint(num_start, num_end)\n        scores_data.append([student, score])\n\n    scores_df = pd.DataFrame(scores_data, columns=['Student', 'Score'])\n\n    return scores_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nrandom, numpy, pandas, matplotlib.pyplot\n```python\nimport numpy as np\nfrom random import randint\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E', 'Team F', 'Team G', 'Team H']\n\ndef generate_team_scores(num_start, num_end):\n    \"\"\"\n    Generate a pandas DataFrame of random scores for a list of teams in the range from num_start to num_end \n    and plot a bar graph of the scores.\n\n    Parameters:\n    num_start (int): The minimum score value.\n    num_end (int): The maximum score value.\n\n    Returns:\n    DataFrame: A pandas DataFrame with team scores.\n\n    Requirements:\n    - random\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> scores = generate_team_scores(60, 100)\n    >>> print(scores)\n    >>> scores.plot(x='Team', y='Score', kind='bar')\n    \"\"\"\n    scores_data = []\n\n    for team in TEAMS:\n        score = randint(num_start, num_end)\n        scores_data.append([team, score])\n\n    scores_df = pd.DataFrame(scores_data, columns=['Team', 'Score'])\n\n    return scores_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nrandom, numpy, statistics\n```python\nimport numpy as np\nfrom random import randint\nimport statistics\n\ndef calculate_statistics(num_start, num_end):\n    \"\"\"\n    Generate a list of 1000 random numbers in the range from num_start to num_end \n    and calculate their mean, median, mode, and standard deviation.\n\n    Parameters:\n    num_start (int): The minimum number value.\n    num_end (int): The maximum number value.\n\n    Returns:\n    dict: A dictionary with the calculated statistics.\n\n    Requirements:\n    - random\n    - numpy\n    - statistics\n\n    Example:\n    >>> stats = calculate_statistics(1, 100)\n    >>> print(stats)\n    \"\"\"\n    numbers = np.random.randint(num_start, num_end, 1000)\n\n    stats = {\n        \"mean\": np.mean(numbers),\n        \"median\": np.median(numbers),\n        \"mode\": statistics.mode(numbers),\n        \"std_dev\": np.std(numbers)\n    }\n\n    return stats\n```"}
{"seed": "{\"task_id\": 71878, \"prompt\": \"def f_71878(l):\\n\\t\", \"suffix\": \"\\n\\treturn indices, l\", \"canonical_solution\": \"x = list(enumerate(l))\\n\\n\\trandom.shuffle(x)\\n\\n\\tindices, l = zip(*x)\\n\", \"test_start\": \"\\nimport random\\n\\ndef check(candidate):\", \"test\": [\"\\n    l = [1,2,3,4,5]\\n    indices, res = candidate(l)\\n    assert set(res) == set(l)\\n    assert set(indices) == set([0,1,2,3,4])\\n\", \"\\n    l = [\\\"a\\\",\\\"b\\\",\\\"c\\\",\\\"d\\\",\\\"e\\\"]\\n    indices, res = candidate(l)\\n    assert set(res) == set(l)\\n    assert set(indices) == set([0,1,2,3,4])\\n\"], \"entry_point\": \"f_71878\", \"intent\": \"\\u041f\\u0435\\u0440\\u0435\\u043c\\u0435\\u0448\\u0430\\u0442\\u044c \\u0441\\u043f\\u0438\\u0441\\u043e\\u043a `l` \\u0441\\u043e\\u0445\\u0440\\u0430\\u043d\\u0438\\u0432 \\u0438\\u0441\\u0445\\u043e\\u0434\\u043d\\u044b\\u0435 \\u0438\\u043d\\u0434\\u0435\\u043a\\u0441\\u044b `indices`\", \"library\": [\"random\"]}", "generation": "Scenario 1: \npandas, random, matplotlib, string\n```python\nimport pandas as pd\nimport random\nimport matplotlib.pyplot as plt\nimport string\n\n# Constants\nLETTERS = list(string.ascii_uppercase)\n\ndef random_letter_frequency(n):\n    \"\"\"\n    Generate a pandas DataFrame with the frequency of randomly chosen \n    uppercase English letters.\n\n    Parameters:\n    n (int): The number of times a letter is chosen.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the frequency of each letter.\n    \n    Requirements:\n    - pandas\n    - random\n    - matplotlib.pyplot\n    - string\n\n    Example:\n    >>> df = random_letter_frequency(1000)\n    >>> print(df)\n    >>> df.plot(kind='bar', legend=False)\n    \"\"\"\n    letters = [random.choice(LETTERS) for _ in range(n)]\n    series = pd.Series(letters)\n    frequency = series.value_counts().sort_index()\n\n    return frequency.to_frame('Frequency')\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncollections, string, random\n```python\nimport collections\nimport string\nimport random\n\n# Constants\nLETTERS = list(string.ascii_uppercase)\n\ndef modify_and_sort_dict(n):\n    \"\"\"\n    Generate a dictionary with the frequency of randomly chosen \n    uppercase English letters, then modify the dictionary by multiplying\n    each value by 2 and sort the dictionary by key.\n\n    Parameters:\n    n (int): The number of times a letter is chosen.\n\n    Returns:\n    dict: A dictionary with the frequency of each letter after modification and sorting.\n    \n    Requirements:\n    - collections\n    - string\n    - random\n\n    Example:\n    >>> d = modify_and_sort_dict(1000)\n    >>> print(d)\n    \"\"\"\n    letters = [random.choice(LETTERS) for _ in range(n)]\n    counter = collections.Counter(letters)\n    modified_counter = {k: v*2 for k, v in counter.items()}\n    sorted_counter = dict(sorted(modified_counter.items()))\n\n    return sorted_counter\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, matplotlib, random\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\n\n# Constants\nNUMBERS = list(range(1, 101))\n\ndef plot_random_numbers(n):\n    \"\"\"\n    Generate a list of n random numbers from 1 to 100, then plot \n    a histogram of the numbers.\n\n    Parameters:\n    n (int): The number of numbers to generate.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> plot_random_numbers(1000)\n    \"\"\"\n    numbers = [random.choice(NUMBERS) for _ in range(n)]\n    plt.hist(numbers, bins=20)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, random, string\n```python\nimport os\nimport random\nimport string\n\n# Constants\nLETTERS = list(string.ascii_lowercase)\n\ndef create_random_files(n):\n    \"\"\"\n    Create n random files with random names in the current directory.\n\n    Parameters:\n    n (int): The number of files to create.\n\n    Requirements:\n    - os\n    - random\n    - string\n\n    Example:\n    >>> create_random_files(10)\n    \"\"\"\n    for _ in range(n):\n        file_name = ''.join(random.choices(LETTERS, k=5)) + '.txt'\n        with open(file_name, 'w') as f:\n            f.write('This is a randomly generated file.')\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, random, matplotlib\n```python\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\n\n# Constants\nNUMBERS = list(range(1, 101))\n\ndef generate_and_plot_random_numbers(n):\n    \"\"\"\n    Generate n random numbers from 1 to 100, calculate their cumulative \n    sum and plot it.\n\n    Parameters:\n    n (int): The number of numbers to generate.\n\n    Returns:\n    np.array: The cumulative sum of the random numbers.\n\n    Requirements:\n    - numpy\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> cumsum = generate_and_plot_random_numbers(1000)\n    >>> print(cumsum)\n    \"\"\"\n    numbers = np.array([random.choice(NUMBERS) for _ in range(n)])\n    cumsum = np.cumsum(numbers)\n    plt.plot(cumsum)\n    plt.show()\n\n    return cumsum\n```"}
{"seed": "{\"task_id\": 197093, \"prompt\": \"def f_197093(n, total):\\n\\t\", \"suffix\": \"\\n\\treturn nums\", \"canonical_solution\": \"my_total = -1\\n\\n\\twhile my_total != total:\\n\\n\\t\\tnums = [random.randint(0,total) for _ in range(n)]\\n\\n\\t\\tmy_total = sum(nums)\\n\", \"test_start\": \"\\nimport random \\n\\ndef check(candidate):\", \"test\": [\"\\n    nums = candidate(2, 10) \\n    assert len(nums) == 2 and sum(nums) == 10\\n\", \"\\n    nums = candidate(3, 10) \\n    assert len(nums) == 3 and sum(nums) == 10\\n\"], \"entry_point\": \"f_197093\", \"intent\": \"\\u0421\\u0433\\u0435\\u043d\\u0435\\u043d\\u0438\\u0440\\u043e\\u0432\\u0430\\u0442\\u044c `n` \\u0441\\u043b\\u0443\\u0447\\u0430\\u0439\\u043d\\u044b\\u0445 \\u0447\\u0438\\u0441\\u0435\\u043b `nums`, \\u0441\\u0443\\u043c\\u043c\\u0430 \\u043a\\u043e\\u0442\\u043e\\u0440\\u044b\\u0445 \\u0440\\u0430\\u0432\\u043d\\u0430 `total`\", \"library\": [\"random\"]}", "generation": "Scenario 1: \nrandom, collections, operator, functools\n```python\nimport random\nfrom collections import Counter\nimport operator\nfrom functools import reduce\n\n# Constants\nN = 10\nTOTAL = 100\n\ndef generate_numbers_with_sum(n=N, total=TOTAL):\n    \"\"\"\n    Generate 'n' random numbers whose sum equals 'total' and return the frequency \n    of each number in descending order.\n    \n    Parameters:\n    n (int): The number of random numbers to generate.\n    total (int): The total sum of the generated numbers.\n\n    Returns:\n    list: A list of tuples with the number and its frequency in descending order.\n\n    Requirements:\n    - random\n    - collections.Counter\n    - operator\n    - functools.reduce\n\n    Example:\n    >>> generate_numbers_with_sum(10, 100)\n    \"\"\"\n    nums = []\n    while sum(nums) != total:\n        nums = [random.randint(0, total) for _ in range(n)]\n    \n    # Calculate frequency of each number\n    freq = Counter(nums)\n    \n    # Sort by frequency in descending order\n    freq = sorted(freq.items(), key=operator.itemgetter(1), reverse=True)\n    \n    return freq\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nrandom, math, statistics\n```python\nimport random\nimport math\nimport statistics\n\n# Constants\nN = 10\nTOTAL = 100\n\ndef generate_numbers_with_stats(n=N, total=TOTAL):\n    \"\"\"\n    Generate 'n' random numbers whose sum equals 'total' and calculate their mean, \n    median, mode, and standard deviation.\n    \n    Parameters:\n    n (int): The number of random numbers to generate.\n    total (int): The total sum of the generated numbers.\n\n    Returns:\n    dict: A dictionary with the mean, median, mode, and standard deviation.\n\n    Requirements:\n    - random\n    - math\n    - statistics\n\n    Example:\n    >>> generate_numbers_with_stats(10, 100)\n    \"\"\"\n    nums = []\n    while sum(nums) != total:\n        nums = [random.randint(0, total) for _ in range(n)]\n    \n    stats = {\n        'mean': statistics.mean(nums),\n        'median': statistics.median(nums),\n        'mode': statistics.mode(nums) if len(set(nums)) != len(nums) else 'No mode',\n        'std_dev': math.sqrt(statistics.variance(nums))\n    }\n    \n    return stats\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nrandom, itertools, collections\n```python\nimport random\nimport itertools\nfrom collections import defaultdict\n\n# Constants\nN = 10\nTOTAL = 100\n\ndef generate_combinations_with_sum(n=N, total=TOTAL):\n    \"\"\"\n    Generate 'n' random numbers and find all combinations that sum up to 'total'.\n    \n    Parameters:\n    n (int): The number of random numbers to generate.\n    total (int): The total sum for the combinations.\n\n    Returns:\n    dict: A dictionary with the key as the length of the combination and \n          the value as a list of combinations.\n\n    Requirements:\n    - random\n    - itertools\n    - collections.defaultdict\n\n    Example:\n    >>> generate_combinations_with_sum(10, 100)\n    \"\"\"\n    nums = [random.randint(0, total) for _ in range(n)]\n    \n    combinations = defaultdict(list)\n    for r in range(1, len(nums) + 1):\n        for combination in itertools.combinations(nums, r):\n            if sum(combination) == total:\n                combinations[r].append(combination)\n    \n    return combinations\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nrandom, numpy, pandas\n```python\nimport random\nimport numpy as np\nimport pandas as pd\n\n# Constants\nN = 10\nTOTAL = 100\n\ndef generate_numbers_with_analysis(n=N, total=TOTAL):\n    \"\"\"\n    Generate 'n' random numbers whose sum equals 'total' and perform analysis \n    on the dataset using pandas and numpy.\n    \n    Parameters:\n    n (int): The number of random numbers to generate.\n    total (int): The total sum of the generated numbers.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the analysis.\n\n    Requirements:\n    - random\n    - numpy\n    - pandas\n\n    Example:\n    >>> generate_numbers_with_analysis(10, 100)\n    \"\"\"\n    nums = []\n    while sum(nums) != total:\n        nums = [random.randint(0, total) for _ in range(n)]\n    \n    # Convert the list into a pandas Series\n    series = pd.Series(nums)\n    \n    # Perform analysis\n    analysis = {\n        'mean': series.mean(),\n        'median': series.median(),\n        'mode': series.mode().values,\n        'std_dev': np.std(series, ddof=1)\n    }\n    \n    # Convert the analysis into a DataFrame\n    analysis_df = pd.DataFrame.from_dict(analysis, orient='index', columns=['Value'])\n    \n    return analysis_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nrandom, bisect, array\n```python\nimport random\nimport bisect\nfrom array import array\n\n# Constants\nN = 10\nTOTAL = 100\n\ndef generate_sorted_numbers_with_sum(n=N, total=TOTAL):\n    \"\"\"\n    Generate 'n' random numbers whose sum equals 'total', sort them and \n    find the position to insert a new number to maintain the sorted order.\n    \n    Parameters:\n    n (int): The number of random numbers to generate.\n    total (int): The total sum of the generated numbers.\n\n    Returns:\n    tuple: A tuple with the sorted numbers as an array and the insertion position.\n\n    Requirements:\n    - random\n    - bisect\n    - array\n\n    Example:\n    >>> generate_sorted_numbers_with_sum(10, 100)\n    \"\"\"\n    nums = []\n    while sum(nums) != total:\n        nums = [random.randint(0, total) for _ in range(n)]\n    \n    # Sort the numbers\n    nums.sort()\n    \n    # Convert the list into an array\n    nums = array('i', nums)\n    \n    # Find the insertion position for a new number\n    new_num = random.randint(0, total)\n    pos = bisect.bisect(nums, new_num)\n    \n    return (nums, pos)\n```\n"}
{"seed": "{\"task_id\": 295537, \"prompt\": \"def f_295537(class_name):\\n\\t\", \"suffix\": \"\\n\\treturn my_obj\", \"canonical_solution\": \"my_obj = globals()[class_name]\", \"test_start\": \"\\nimport random \\n\\ndef check(candidate):\", \"test\": [\"\\n    class_name = random.sample(globals().keys(), 1)[0]\\n    assert candidate(class_name).__class__ == globals()[class_name].__class__\\n\"], \"entry_point\": \"f_295537\", \"intent\": \"\\u0421\\u043e\\u0437\\u0434\\u0430\\u0442\\u044c \\u043e\\u0431\\u044a\\u0435\\u043a\\u0442 \\u043a\\u043b\\u0430\\u0441\\u0441\\u0430 `my_obj` \\u043f\\u043e \\u043d\\u0430\\u0437\\u0432\\u0430\\u043d\\u0438\\u044e \\u0432 \\u0441\\u0442\\u0440\\u043e\\u043a\\u0435 `class_name`\", \"library\": [\"random\"]}", "generation": "Scenario 1:\nrandom, matplotlib, pandas, numpy\n```python\nimport random\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\n# Constants\nCLASSES = ['A', 'B', 'C', 'D', 'E']\n\ndef generate_random_data(class_name):\n    \"\"\"\n    Generate a pandas DataFrame of random data for a class specified by `class_name`. \n    Plot a histogram of the data.\n\n    Parameters:\n    class_name (str): The name of the class.\n\n    Returns:\n    DataFrame: A pandas DataFrame with random data for the class.\n\n    Requirements:\n    - random\n    - matplotlib.pyplot\n    - pandas\n    - numpy\n\n    Example:\n    >>> df = generate_random_data('A')\n    >>> print(df)\n    >>> df.hist(bins=20)\n    \"\"\"\n    if class_name not in CLASSES:\n        raise ValueError(f\"Invalid class name. Expected one of {CLASSES}\")\n\n    data = np.random.randn(100)\n    df = pd.DataFrame(data, columns=[class_name])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nrandom, numpy, pandas, sklearn.preprocessing\n```python\nimport random\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\n# Constants\nCLASSES = ['A', 'B', 'C', 'D', 'E']\n\ndef standardize_class_data(class_name):\n    \"\"\"\n    Generate a pandas DataFrame of random data for a class specified by `class_name` and standardize it.\n\n    Parameters:\n    class_name (str): The name of the class.\n\n    Returns:\n    DataFrame: A pandas DataFrame with standardized data for the class.\n\n    Requirements:\n    - random\n    - numpy\n    - pandas\n    - sklearn.preprocessing\n\n    Example:\n    >>> df = standardize_class_data('A')\n    >>> print(df)\n    \"\"\"\n    if class_name not in CLASSES:\n        raise ValueError(f\"Invalid class name. Expected one of {CLASSES}\")\n\n    data = np.random.randn(100)\n    df = pd.DataFrame(data, columns=[class_name])\n\n    scaler = StandardScaler()\n    df[class_name] = scaler.fit_transform(df[[class_name]])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nrandom, pandas, seaborn, matplotlib\n```python\nimport random\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nCLASSES = ['A', 'B', 'C', 'D', 'E']\n\ndef generate_and_plot_data(class_name):\n    \"\"\"\n    Generate a pandas DataFrame of random data for a class specified by `class_name` and plot a density plot.\n\n    Parameters:\n    class_name (str): The name of the class.\n\n    Returns:\n    DataFrame: A pandas DataFrame with random data for the class.\n\n    Requirements:\n    - random\n    - pandas\n    - seaborn\n    - matplotlib\n\n    Example:\n    >>> df = generate_and_plot_data('A')\n    >>> print(df)\n    >>> sns.kdeplot(df[class_name])\n    \"\"\"\n    if class_name not in CLASSES:\n        raise ValueError(f\"Invalid class name. Expected one of {CLASSES}\")\n\n    data = pd.Series(random.sample(range(1, 100), 25), name=class_name)\n    df = pd.DataFrame(data)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nrandom, pandas, numpy, sklearn.cluster, matplotlib\n```python\nimport random\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\n# Constants\nCLASSES = ['A', 'B', 'C', 'D', 'E']\n\ndef cluster_class_data(class_name):\n    \"\"\"\n    Generate a pandas DataFrame of random data for a class specified by `class_name` and perform K-means clustering.\n\n    Parameters:\n    class_name (str): The name of the class.\n\n    Returns:\n    DataFrame: A pandas DataFrame with clustered data for the class.\n\n    Requirements:\n    - random\n    - pandas\n    - numpy\n    - sklearn.cluster\n    - matplotlib\n\n    Example:\n    >>> df = cluster_class_data('A')\n    >>> print(df)\n    \"\"\"\n    if class_name not in CLASSES:\n        raise ValueError(f\"Invalid class name. Expected one of {CLASSES}\")\n\n    data = np.random.rand(100, 2)\n    df = pd.DataFrame(data, columns=[class_name+'1', class_name+'2'])\n\n    kmeans = KMeans(n_clusters=3)\n    df['Cluster'] = kmeans.fit_predict(df[[class_name+'1', class_name+'2']])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nrandom, pandas, numpy, sklearn.linear_model\n```python\nimport random\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\n# Constants\nCLASSES = ['A', 'B', 'C', 'D', 'E']\n\ndef fit_linear_model(class_name):\n    \"\"\"\n    Generate a pandas DataFrame of random data for a class specified by `class_name` and fit a linear model.\n\n    Parameters:\n    class_name (str): The name of the class.\n\n    Returns:\n    LinearRegression: A fitted Linear Regression model.\n\n    Requirements:\n    - random\n    - pandas\n    - numpy\n    - sklearn.linear_model\n\n    Example:\n    >>> model = fit_linear_model('A')\n    >>> print(model.coef_)\n    \"\"\"\n    if class_name not in CLASSES:\n        raise ValueError(f\"Invalid class name. Expected one of {CLASSES}\")\n\n    data = np.random.rand(100, 2)\n    df = pd.DataFrame(data, columns=[class_name+'1', class_name+'2'])\n\n    model = LinearRegression()\n    model.fit(df[[class_name+'1']], df[class_name+'2'])\n\n    return model\n```"}
{"seed": "{\"task_id\": 240287, \"prompt\": \"def f_240287(tupels):\\n\\t\", \"suffix\": \"\\n\\treturn my_dict\", \"canonical_solution\": \"my_dict = dict(tupels)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    tupels = ((0,0),(1,1),(2,2),(100,99))\\n    assert candidate(tupels) == {0:0, 1:1, 2:2, 100:99}\\n\", \"\\n    tupels = ((0,0),(1,1),(2,2),(1,99))\\n    assert candidate(tupels) == {0:0, 1:99, 2:2}\\n\", \"\\n    tupels = ((0,0),(1,1),(2,2),(80,None))\\n    assert candidate(tupels) == {0:0, 1:1, 2:2, 80: None}\\n\"], \"entry_point\": \"f_240287\", \"intent\": \"\\u041f\\u0440\\u0435\\u043e\\u0431\\u0440\\u0430\\u0437\\u043e\\u0432\\u0430\\u0442\\u044c \\u043a\\u043e\\u0440\\u0442\\u0435\\u0436 \\u043a\\u043e\\u0440\\u0442\\u0435\\u0436\\u0435\\u0439 \\u043f\\u0430\\u0440 \\u043a\\u043b\\u044e\\u0447-\\u0437\\u043d\\u0430\\u0447\\u0435\\u043d\\u0438\\u0435 `tupels` \\u0432 \\u0441\\u043b\\u043e\\u0432\\u0430\\u0440\\u044c `my_dict`\", \"library\": []}", "generation": "Scenario 1:\ncollections, itertools, numpy, matplotlib\n```python\nimport collections\nimport itertools\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nLABELS = ['Label 1', 'Label 2', 'Label 3', 'Label 4', 'Label 5']\n\ndef plot_distribution_of_labels(tuples):\n    \"\"\"\n    Convert a tuple of tuples into a dictionary and plot the distribution of labels\n    \n    Parameters:\n    tuples (tuple of tuples): The input tuple of tuples.\n\n    Returns:\n    dict: A dictionary with labels as keys and their counts as values.\n\n    Requirements:\n    - collections\n    - itertools\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> tuples = ((0, 'Label 1'), (1, 'Label 2'), (2, 'Label 3'), (1, 'Label 4'), (2, 'Label 5'))\n    >>> plot_distribution_of_labels(tuples)\n    \"\"\"\n    my_dict = dict(tuples)\n    count_dict = collections.Counter(my_dict.values())\n    \n    # Plot the distribution\n    plt.bar(count_dict.keys(), count_dict.values())\n    plt.xlabel('Labels')\n    plt.ylabel('Counts')\n    plt.title('Distribution of Labels')\n    plt.show()\n\n    return count_dict\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\njson, urllib, collections\n```python\nimport json\nimport urllib.request\nimport collections\n\ndef get_top_ten_most_common_words_in_url(url):\n    \"\"\"\n    Get the top ten most common words in the text content from a given URL.\n    \n    Parameters:\n    url (str): The URL from which the text content is to be retrieved.\n\n    Returns:\n    list: A list of tuples where each tuple is a word and its count.\n\n    Requirements:\n    - json\n    - urllib\n    - collections\n\n    Example:\n    >>> url = 'https://jsonplaceholder.typicode.com/posts/1'\n    >>> get_top_ten_most_common_words_in_url(url)\n    \"\"\"\n    with urllib.request.urlopen(url) as u:\n        data = json.loads(u.read().decode())\n        content = data.get('body', '')\n\n    words = content.split()\n    word_counts = collections.Counter(words)\n\n    return word_counts.most_common(10)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, pandas, seaborn\n```python\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n# Constants\nLABELS = ['Label 1', 'Label 2', 'Label 3', 'Label 4', 'Label 5']\n\ndef create_dataframe_and_visualize_correlation(tuples):\n    \"\"\"\n    Convert a tuple of tuples into a pandas DataFrame and visualize the correlation using seaborn.\n    \n    Parameters:\n    tuples (tuple of tuples): The input tuple of tuples.\n\n    Returns:\n    DataFrame: A pandas DataFrame created from the tuples.\n\n    Requirements:\n    - numpy\n    - pandas\n    - seaborn\n\n    Example:\n    >>> tuples = ((0, 1), (1, 2), (2, 3), (1, 4), (2, 5))\n    >>> create_dataframe_and_visualize_correlation(tuples)\n    \"\"\"\n    df = pd.DataFrame(list(tuples), columns=['Label', 'Value'])\n    \n    # Use seaborn to visualize the correlation\n    sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n    \n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, scipy.stats, matplotlib\n```python\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\n# Constants\nLABELS = ['Label 1', 'Label 2', 'Label 3', 'Label 4', 'Label 5']\n\ndef generate_normal_distribution_and_plot(tuples):\n    \"\"\"\n    Generate a normal distribution from a tuple of tuples and plot the distribution.\n    \n    Parameters:\n    tuples (tuple of tuples): The input tuple of tuples.\n\n    Returns:\n    list: A list of generated normal distribution.\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Example:\n    >>> tuples = ((0, 1), (1, 2), (2, 3), (1, 4), (2, 5))\n    >>> generate_normal_distribution_and_plot(tuples)\n    \"\"\"\n    dict_values = dict(tuples).values()\n    mu, sigma = np.mean(list(dict_values)), np.std(list(dict_values))\n    \n    # Generate normal distribution\n    s = np.random.normal(mu, sigma, 1000)\n    \n    # Plot the distribution\n    count, bins, ignored = plt.hist(s, 30, density=True)\n    plt.plot(bins, stats.norm.pdf(bins, mu, sigma), linewidth=2, color='r')\n    plt.show()\n    \n    return s.tolist()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, pandas, sklearn\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\n# Constants\nLABELS = ['Label 1', 'Label 2', 'Label 3', 'Label 4', 'Label 5']\n\ndef tuples_to_dataframe_and_label_encoding(tuples):\n    \"\"\"\n    Convert a tuple of tuples into a pandas DataFrame and perform label encoding.\n    \n    Parameters:\n    tuples (tuple of tuples): The input tuple of tuples.\n\n    Returns:\n    DataFrame: A pandas DataFrame created from the tuples.\n\n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.preprocessing.LabelEncoder\n\n    Example:\n    >>> tuples = ((0, 'Label 1'), (1, 'Label 2'), (2, 'Label 3'), (1, 'Label 4'), (2, 'Label 5'))\n    >>> tuples_to_dataframe_and_label_encoding(tuples)\n    \"\"\"\n    df = pd.DataFrame(list(tuples), columns=['Value', 'Label'])\n    \n    le = LabelEncoder()\n    df['Label'] = le.fit_transform(df['Label'])\n    \n    return df\n```"}
{"seed": "{\"task_id\": 26594, \"prompt\": \"def f_26594(s):\\n\\t\", \"suffix\": \"\\n\\treturn url\", \"canonical_solution\": \"url = re.findall(r\\\"\\\\[url=(.*)\\\\]\\\", s)[0]\", \"test_start\": \"\\nimport re \\n\\ndef check(candidate):\", \"test\": [\"\\n    s = \\\"[url=http://forum.sibnet.ru/index.php?showuser=xxxxx]\\\"\\n    assert candidate(s) == \\\"http://forum.sibnet.ru/index.php?showuser=xxxxx\\\"\\n\"], \"entry_point\": \"f_26594\", \"intent\": \"\\u041f\\u043e\\u043b\\u0443\\u0447\\u0438\\u0442\\u044c \\u0441\\u0441\\u044b\\u043b\\u043a\\u0443 `url` \\u0438\\u0437 BBCode \\u0441\\u0442\\u0440\\u043e\\u043a\\u0438 `s`\", \"library\": [\"re\"]}", "generation": "Scenario 1: \npandas, re, numpy, matplotlib\n```python\nimport pandas as pd\nimport re\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nURL_PATTERN = r'\\[url=(.*)\\]'\n\ndef extract_and_analyze_urls(text_data):\n    \"\"\"\n    Extract URLs from a list of BBCode strings and analyze the frequency of domain names.\n    \n    Parameters:\n    text_data (list): A list of BBCode strings.\n\n    Returns:\n    DataFrame: A pandas DataFrame with domain names and their frequency.\n\n    Requirements:\n    - pandas\n    - re\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> text_data = [\"[url=http://forum.sibnet.ru/index.php?showuser=xxxxx]\", \"[url=https://google.com]\", \"[url=http://forum.sibnet.ru/index.php?showuser=yyyyy]\"]\n    >>> df = extract_and_analyze_urls(text_data)\n    >>> print(df)\n    >>> df['Frequency'].plot(kind='bar')\n    \"\"\"\n    urls = [re.findall(URL_PATTERN, s)[0] for s in text_data]\n    domains = [re.search(r'(?:http.*://)?(?P<host>[^:/ ]+).*', url).group('host') for url in urls]\n\n    df = pd.DataFrame(domains, columns=['Domain'])\n    df['Frequency'] = df.groupby('Domain')['Domain'].transform('count')\n\n    return df.drop_duplicates()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nre, json, os, shutil\n```python\nimport re\nimport json\nimport os\nimport shutil\n\n# Constants\nURL_PATTERN = r'\\[url=(.*)\\]'\nFILE_PATH = 'data.json'\n\ndef extract_urls_and_save(text_data):\n    \"\"\"\n    Extract URLs from a list of BBCode strings and save them into a json file.\n    If the file already exists, make a backup before overwriting.\n\n    Parameters:\n    text_data (list): A list of BBCode strings.\n\n    Returns:\n    str: The path to the json file.\n\n    Requirements:\n    - re\n    - json\n    - os\n    - shutil\n\n    Example:\n    >>> text_data = [\"[url=http://forum.sibnet.ru/index.php?showuser=xxxxx]\", \"[url=https://google.com]\", \"[url=http://forum.sibnet.ru/index.php?showuser=yyyyy]\"]\n    >>> path = extract_urls_and_save(text_data)\n    >>> print(path)\n    \"\"\"\n    urls = [re.findall(URL_PATTERN, s)[0] for s in text_data]\n\n    if os.path.exists(FILE_PATH):\n        shutil.copy(FILE_PATH, FILE_PATH + '.bak')\n\n    with open(FILE_PATH, 'w') as f:\n        json.dump(urls, f)\n\n    return FILE_PATH\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nre, urllib.parse, collections\n```python\nimport re\nfrom urllib.parse import urlparse\nfrom collections import Counter\n\n# Constants\nURL_PATTERN = r'\\[url=(.*)\\]'\n\ndef extract_and_analyze_query_params(text_data):\n    \"\"\"\n    Extract URLs from a list of BBCode strings and analyze the frequency of query parameters.\n\n    Parameters:\n    text_data (list): A list of BBCode strings.\n\n    Returns:\n    dict: A dictionary with query parameters and their frequency.\n\n    Requirements:\n    - re\n    - urllib.parse\n    - collections\n\n    Example:\n    >>> text_data = [\"[url=http://forum.sibnet.ru/index.php?showuser=xxxxx&topic=zzzzz]\", \"[url=https://google.com?search=python]\", \"[url=http://forum.sibnet.ru/index.php?showuser=yyyyy&topic=aaaaa]\"]\n    >>> counter = extract_and_analyze_query_params(text_data)\n    >>> print(counter)\n    \"\"\"\n    urls = [re.findall(URL_PATTERN, s)[0] for s in text_data]\n    params = [urlparse(url).query for url in urls]\n\n    counter = Counter(params)\n\n    return dict(counter)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nre, requests, bs4\n```python\nimport re\nimport requests\nfrom bs4 import BeautifulSoup\n\n# Constants\nURL_PATTERN = r'\\[url=(.*)\\]'\n\ndef extract_urls_and_get_titles(text_data):\n    \"\"\"\n    Extract URLs from a list of BBCode strings and get the titles of the web pages.\n\n    Parameters:\n    text_data (list): A list of BBCode strings.\n\n    Returns:\n    dict: A dictionary with URLs and their titles.\n\n    Requirements:\n    - re\n    - requests\n    - bs4\n\n    Example:\n    >>> text_data = [\"[url=http://forum.sibnet.ru/index.php?showuser=xxxxx]\", \"[url=https://google.com]\", \"[url=http://forum.sibnet.ru/index.php?showuser=yyyyy]\"]\n    >>> titles = extract_urls_and_get_titles(text_data)\n    >>> print(titles)\n    \"\"\"\n    urls = [re.findall(URL_PATTERN, s)[0] for s in text_data]\n    titles = {}\n\n    for url in urls:\n        r = requests.get(url)\n        soup = BeautifulSoup(r.text, 'html.parser')\n        titles[url] = soup.title.string\n\n    return titles\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nre, csv, os\n```python\nimport re\nimport csv\nimport os\n\n# Constants\nURL_PATTERN = r'\\[url=(.*)\\]'\nFILE_PATH = 'urls.csv'\n\ndef extract_urls_and_save_to_csv(text_data):\n    \"\"\"\n    Extract URLs from a list of BBCode strings and save them into a csv file.\n    If the file already exists, append the URLs to it.\n\n    Parameters:\n    text_data (list): A list of BBCode strings.\n\n    Returns:\n    str: The path to the csv file.\n\n    Requirements:\n    - re\n    - csv\n    - os\n\n    Example:\n    >>> text_data = [\"[url=http://forum.sibnet.ru/index.php?showuser=xxxxx]\", \"[url=https://google.com]\", \"[url=http://forum.sibnet.ru/index.php?showuser=yyyyy]\"]\n    >>> path = extract_urls_and_save_to_csv(text_data)\n    >>> print(path)\n    \"\"\"\n    urls = [re.findall(URL_PATTERN, s)[0] for s in text_data]\n\n    mode = 'a' if os.path.exists(FILE_PATH) else 'w'\n\n    with open(FILE_PATH, mode, newline='') as f:\n        writer = csv.writer(f)\n        for url in urls:\n            writer.writerow([url])\n\n    return FILE_PATH\n```"}
{"seed": "{\"task_id\": 262974, \"prompt\": \"def f_262974(pattern, s, text):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"re.sub(pattern, r'{}'.format(s), text) \", \"test_start\": \"\\nimport re\\n\\ndef check(candidate):\", \"test\": [\"\\n    text= '\\u0417\\u0430\\u0433\\u043e\\u043b\\u043e\\u0432\\u043e\\u043a \\u0437\\u0430\\u0433\\u043e\\u043b\\u043e\\u0432\\u043e\\u043a \\u0437\\u0430\\u0433\\u043e\\u043b\\u043e\\u0432\\u043e\\u043a \\\\n \\u0421\\u043b\\u043e\\u0432\\u043e \\u0441\\u043b\\u043e\\u0432\\u043e \\u0441\\u043b\\u043e\\u0432\\u043e \\u0441\\u043b\\u043e\\u0432\\u043e \\u0441\\u043b\\u043e\\u0432\\u043e \\u0441\\u043b\\u043e\\u0432\\u043e \\u0441\\u043b\\u043e\\u0432\\u043e.'\\n    res = '\\\\x01\\\\n \\u0421\\u043b\\u043e\\u0432\\u043e \\u0441\\u043b\\u043e\\u0432\\u043e \\u0441\\u043b\\u043e\\u0432\\u043e \\u0441\\u043b\\u043e\\u0432\\u043e \\u0441\\u043b\\u043e\\u0432\\u043e \\u0441\\u043b\\u043e\\u0432\\u043e \\u0441\\u043b\\u043e\\u0432\\u043e.'\\n    pattern = \\\"^([\\u0410-\\u042f]{1}.{,30}\\\\n)\\\"\\n    assert candidate(pattern, '\\\\1\\\\n', text) == res\\n\"], \"entry_point\": \"f_262974\", \"intent\": \"\\u0412\\u0441\\u0442\\u0430\\u0432\\u0438\\u0442\\u044c \\u0441\\u0442\\u0440\\u043e\\u043a\\u0443 `s` \\u043f\\u043e\\u0441\\u043b\\u0435 \\u043f\\u043e\\u0434\\u0441\\u0442\\u0440\\u043e\\u043a\\u0438 \\u0432\\u044b\\u0434\\u0435\\u043b\\u0435\\u043d\\u043d\\u043e\\u0439 \\u0440\\u0435\\u0433\\u0443\\u043b\\u0440\\u043d\\u044b\\u043c \\u0432\\u044b\\u0440\\u0430\\u0436\\u0435\\u043d\\u0438\\u0435\\u043c `pattern` \\u0432 \\u0441\\u0442\\u0440\\u043e\\u043a\\u0435 `text`\", \"library\": [\"re\"]}", "generation": "Scenario 1: \npandas, re, random, math\n```python\nimport pandas as pd\nimport re\nfrom random import randint\nimport math\n\n# Constants\nPRODUCTS = ['Apple', 'Banana', 'Cherry', 'Date', 'Elderberry']\nPRICE_PATTERN = r\"\\d+\\.\\d+\"\n\ndef generate_product_list(text):\n    \"\"\"\n    Generate a product list with prices extracted from a text and random quantities.\n    A simple calculation is done to find the total cost for each product.\n    \n    Parameters:\n    text (str): The string from which the prices are extracted.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the products, their prices, quantities and total costs.\n    \n    Requirements:\n    - pandas\n    - re\n    - random\n    - math\n\n    Example:\n    >>> text = \"Apple costs 1.20, Banana costs 0.50, Cherry costs 2.00, Date costs 3.00, Elderberry costs 5.00\"\n    >>> product_list = generate_product_list(text)\n    >>> print(product_list)\n    \"\"\"\n    prices = re.findall(PRICE_PATTERN, text)\n    product_data = []\n\n    for i, product in enumerate(PRODUCTS):\n        price = float(prices[i])\n        quantity = randint(1, 10)\n        total_cost = round(price * quantity, 2)\n        product_data.append([product, price, quantity, total_cost])\n\n    product_df = pd.DataFrame(product_data, columns=['Product', 'Price', 'Quantity', 'Total Cost'])\n\n    return product_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nre, nltk, collections, string\n```python\nimport re\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nimport string\n\n# Constants\nSTOPWORDS = set(stopwords.words('english'))\n\ndef get_most_common_words(text):\n    \"\"\"\n    Find the five most common words in a text, excluding English stopwords.\n    \n    Parameters:\n    text (str): The text to analyze.\n    \n    Returns:\n    list: A list of tuples, where each tuple contains a word and its count.\n    \n    Requirements:\n    - re\n    - nltk.corpus.stopwords\n    - collections\n    - string\n\n    Example:\n    >>> text = \"This is a sample text. This text is meant to serve as a sample for this task.\"\n    >>> most_common_words = get_most_common_words(text)\n    >>> print(most_common_words)\n    \"\"\"\n    # Remove punctuation\n    text = re.sub('['+string.punctuation+']', '', text)\n    words = text.lower().split()\n\n    # Remove stopwords\n    words = [word for word in words if word not in STOPWORDS]\n\n    # Find most common words\n    most_common_words = Counter(words).most_common(5)\n\n    return most_common_words\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nre, os, shutil\n```python\nimport re\nimport os\nimport shutil\n\n# Constants\nFILE_PATTERN = r\"\\.txt$\"\n\ndef move_matching_files(source_directory, destination_directory):\n    \"\"\"\n    Move all .txt files from a source directory to a destination directory.\n    \n    Parameters:\n    source_directory (str): The path to the source directory.\n    destination_directory (str): The path to the destination directory.\n    \n    Returns:\n    int: The number of files moved.\n    \n    Requirements:\n    - re\n    - os\n    - shutil\n\n    Example:\n    >>> source_directory = \"/path/to/source/directory\"\n    >>> destination_directory = \"/path/to/destination/directory\"\n    >>> files_moved = move_matching_files(source_directory, destination_directory)\n    >>> print(files_moved)\n    \"\"\"\n    files_moved = 0\n\n    for filename in os.listdir(source_directory):\n        if re.search(FILE_PATTERN, filename):\n            shutil.move(os.path.join(source_directory, filename), destination_directory)\n            files_moved += 1\n\n    return files_moved\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nre, operator\n```python\nimport re\nimport operator\n\n# Constants\nOPERATORS = {\n    '+': operator.add,\n    '-': operator.sub,\n    '*': operator.mul,\n    '/': operator.truediv\n}\n\ndef calculate_expression(expression):\n    \"\"\"\n    Evaluate a simple arithmetic expression.\n    \n    Parameters:\n    expression (str): The arithmetic expression to evaluate.\n    \n    Returns:\n    float: The result of the calculation.\n    \n    Requirements:\n    - re\n    - operator\n\n    Example:\n    >>> expression = \"3 + 2 * 5\"\n    >>> result = calculate_expression(expression)\n    >>> print(result)\n    \"\"\"\n    elements = re.findall(r\"\\d+|\\+|\\-|\\*|\\/\", expression)\n\n    # Initialize result with the first number\n    result = float(elements.pop(0))\n\n    while elements:\n        # Pop operator and operand\n        operator_symbol = elements.pop(0)\n        operand = float(elements.pop(0))\n\n        # Perform operation and update result\n        result = OPERATORS[operator_symbol](result, operand)\n\n    return result\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nre, numpy, matplotlib.pyplot\n```python\nimport re\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nNUMBER_PATTERN = r\"\\d+\"\n\ndef plot_numbers(text):\n    \"\"\"\n    Extract all numbers from a text and plot a histogram of their distribution.\n    \n    Parameters:\n    text (str): The text from which the numbers are extracted.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - re\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> text = \"There are 3 apples, 5 bananas, 2 cherries, 7 dates and 1 elderberry.\"\n    >>> plot_numbers(text)\n    \"\"\"\n    numbers = re.findall(NUMBER_PATTERN, text)\n\n    # Convert to integers\n    numbers = list(map(int, numbers))\n\n    # Plot histogram\n    plt.hist(numbers, bins=np.arange(min(numbers), max(numbers) + 2) - 0.5, edgecolor='black')\n    plt.xticks(range(min(numbers), max(numbers) + 1))\n    plt.xlabel('Number')\n    plt.ylabel('Count')\n    plt.title('Distribution of Numbers')\n    plt.show()\n```\n`GPT_ODEX_BREAK`"}
{"seed": "{\"task_id\": 178993, \"prompt\": \"def f_178993(url):\\n\\t\", \"suffix\": \"\\n\\treturn code\", \"canonical_solution\": \"with urllib.request.urlopen(url) as res:\\n\\t    code = res.read()\", \"test_start\": \"\\nimport urllib.request\\n\\ndef check(candidate):\", \"test\": [\"\\n    f = urllib.request.urlopen('http://www.python.org/')\\n    res = f.read()\\n    assert candidate('http://www.python.org/') == res\\n\"], \"entry_point\": \"f_178993\", \"intent\": \"\\u0421\\u043f\\u0430\\u0440\\u0441\\u0438\\u0442\\u044c HTML-\\u043a\\u043e\\u0434 \\u0441\\u0442\\u0440\\u0430\\u043d\\u0438\\u0446\\u044b `code` \\u043f\\u043e \\u0441\\u0441\\u044b\\u043b\\u043a\\u0435 `url`\", \"library\": [\"urllib\"]}", "generation": "Scenario 1: \nurllib.request, bs4, re, pandas\n```python\nimport urllib.request\nfrom bs4 import BeautifulSoup\nimport re\nimport pandas as pd\n\n# Constants\nHEADERS = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n\ndef extract_player_stats(url):\n    \"\"\"\n    Extract player stats from NBA.com for a specific player.\n\n    Parameters:\n    url (str): The url of the player profile.\n\n    Returns:\n    DataFrame: A pandas DataFrame with player stats.\n\n    Requirements:\n    - urllib.request\n    - bs4\n    - re\n    - pandas\n\n    Example:\n    >>> url = 'https://www.nba.com/player/201/lebron-james/stats'\n    >>> stats = extract_player_stats(url)\n    >>> print(stats)\n    \"\"\"\n    request = urllib.request.Request(url, headers=HEADERS)\n    html = urllib.request.urlopen(request).read()\n\n    soup = BeautifulSoup(html, 'html.parser')\n    table = soup.find('table')\n    headers = [th.text for th in table.find_all('th')]\n    rows = table.find_all('tr')[1:]\n\n    data = []\n    for row in rows:\n        cols = row.find_all('td')\n        cols = [re.sub('\\n','',ele.text.strip()) for ele in cols]\n        data.append([ele for ele in cols if ele])\n\n    df = pd.DataFrame(data, columns=headers)\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nurllib.request, json, datetime, pytz\n```python\nimport urllib.request\nimport json\nfrom datetime import datetime\nimport pytz\n\n# Constants\nAPI_URL = 'https://api.sunrise-sunset.org/json?'\n\ndef sunrise_sunset_times(latitude, longitude):\n    \"\"\"\n    Get sunrise and sunset times for a given location.\n\n    Parameters:\n    latitude (float): The latitude of the location.\n    longitude (float): The longitude of the location.\n\n    Returns:\n    dict: A dictionary with sunrise and sunset times.\n\n    Requirements:\n    - urllib.request\n    - json\n    - datetime\n    - pytz\n\n    Example:\n    >>> sunrise_sunset_times(51.5074, -0.1278)\n    \"\"\"\n    url = f'{API_URL}lat={latitude}&lng={longitude}&formatted=0'\n    with urllib.request.urlopen(url) as res:\n        data = json.loads(res.read().decode())\n\n    sunrise_utc = datetime.fromisoformat(data['results']['sunrise'].replace('Z', '+00:00'))\n    sunset_utc = datetime.fromisoformat(data['results']['sunset'].replace('Z', '+00:00'))\n\n    timezone = pytz.timezone('Europe/London')\n    sunrise_local = sunrise_utc.astimezone(timezone)\n    sunset_local = sunset_utc.astimezone(timezone)\n\n    return {'sunrise': sunrise_local, 'sunset': sunset_local}\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nurllib.request, bs4, pandas, matplotlib.pyplot\n```python\nimport urllib.request\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nURL = 'https://www.worldometers.info/coronavirus/'\n\ndef plot_covid_cases():\n    \"\"\"\n    Plot the total number of COVID-19 cases by country.\n\n    Requirements:\n    - urllib.request\n    - bs4\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_covid_cases()\n    \"\"\"\n    with urllib.request.urlopen(URL) as res:\n        soup = BeautifulSoup(res, 'html.parser')\n\n    table = soup.find('table')\n    headers = [th.text for th in table.find_all('th')]\n    rows = table.find_all('tr')[1:]\n\n    data = []\n    for row in rows:\n        cols = row.find_all('td')\n        cols = [ele.text.strip() for ele in cols]\n        data.append([ele for ele in cols if ele])\n\n    df = pd.DataFrame(data, columns=headers)\n    df = df[['Country,Other', 'TotalCases']].dropna()\n    df['TotalCases'] = df['TotalCases'].str.replace(',', '').astype(int)\n    df = df.sort_values('TotalCases', ascending=False)\n\n    df.plot(x='Country,Other', y='TotalCases', kind='bar')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nurllib.request, lxml.html, pandas\n```python\nimport urllib.request\nimport lxml.html\nimport pandas as pd\n\n# Constants\nURL = 'https://www.bbc.co.uk/news/business-51706225'\n\ndef extract_article_text(url):\n    \"\"\"\n    Extract the text of an article from BBC News.\n\n    Parameters:\n    url (str): The url of the article.\n\n    Returns:\n    str: The article text.\n\n    Requirements:\n    - urllib.request\n    - lxml.html\n    - pandas\n\n    Example:\n    >>> url = 'https://www.bbc.co.uk/news/business-51706225'\n    >>> text = extract_article_text(url)\n    >>> print(text)\n    \"\"\"\n    with urllib.request.urlopen(url) as res:\n        html = res.read()\n\n    document = lxml.html.fromstring(html)\n    article = document.xpath('//div[@class=\"ssrcss-uf6wea-RichTextComponentWrapper e1xue1i84\"]')[0]\n    paragraphs = article.xpath('.//p')\n    text = ' '.join([p.text_content() for p in paragraphs])\n\n    return text\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nurllib.request, json, pandas\n```python\nimport urllib.request\nimport json\nimport pandas as pd\n\n# Constants\nAPI_URL = 'https://api.github.com/users/'\n\ndef get_github_repos(username):\n    \"\"\"\n    Get a list of public repositories for a given GitHub username.\n\n    Parameters:\n    username (str): The GitHub username.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the repository names and descriptions.\n\n    Requirements:\n    - urllib.request\n    - json\n    - pandas\n\n    Example:\n    >>> username = 'torvalds'\n    >>> repos = get_github_repos(username)\n    >>> print(repos)\n    \"\"\"\n    url = f'{API_URL}{username}/repos'\n    with urllib.request.urlopen(url) as res:\n        data = json.loads(res.read().decode())\n\n    repos = [{'name': repo['name'], 'description': repo['description']} for repo in data]\n    df = pd.DataFrame(repos)\n\n    return df\n```"}
{"seed": "{\"task_id\": 50259, \"prompt\": \"def f_50259(num, f):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"\\\"{:.{}f}\\\".format(num, f)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(500, 2) == '500.00'\\n\\n\"], \"entry_point\": \"f_50259\", \"intent\": \"\\u0412\\u044b\\u0432\\u0435\\u0441\\u0442\\u0438 \\u0447\\u0438\\u0441\\u043b\\u043e `num` \\u0441 \\u0442\\u043e\\u0447\\u043d\\u043e\\u0441\\u0442\\u044e \\u0434\\u043e `f` \\u0437\\u043d\\u0430\\u043a\\u043e\\u0432\", \"library\": []}", "generation": "Scenario 1:\npandas, random, math, matplotlib\n```python\nimport pandas as pd\nfrom random import uniform\nimport math\nimport matplotlib.pyplot as plt\n\n# Constants\nRANGE_START = 0\nRANGE_END = 1000\nPRECISION = 2\n\ndef generate_random_data(n):\n    \"\"\"\n    Generate a pandas DataFrame with n rows of random floating point numbers \n    within a range, each number formatted with a specific precision.\n    \n    Parameters:\n    n (int): The number of rows of data to generate.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the generated data.\n\n    Requirements:\n    - pandas\n    - random\n    - math\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = generate_random_data(10)\n    >>> print(df)\n    >>> df.plot(kind='bar')\n    \"\"\"\n    data = []\n    for _ in range(n):\n        num = uniform(RANGE_START, RANGE_END)\n        formatted_num = \"{:.{}f}\".format(num, PRECISION)\n        data.append([num, formatted_num])\n    df = pd.DataFrame(data, columns=['Number', 'Formatted Number'])\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, math, matplotlib, scipy.stats\n```python\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n# Constants\nSIZE = 1000\nPRECISION = 2\n\ndef plot_normal_distribution():\n    \"\"\"\n    Generate a random normal distribution of a specified size, format each number \n    with a specific precision, and plot the distribution.\n    \n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - math\n    - matplotlib.pyplot\n    - scipy.stats\n\n    Example:\n    >>> plot_normal_distribution()\n    \"\"\"\n    data = np.random.normal(size=SIZE)\n    formatted_data = [\"{:.{}f}\".format(num, PRECISION) for num in data]\n    plt.hist(formatted_data, bins='auto', density=True)\n    mu, std = stats.norm.fit(data)\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, mu, std)\n    plt.plot(x, p, 'k', linewidth=2)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, math, matplotlib, seaborn\n```python\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nSIZE = 500\nPRECISION = 2\n\ndef generate_and_visualize_data():\n    \"\"\"\n    Generate a numpy array of random floating point numbers of a specified size, \n    format each number with a specific precision, and visualize the array using a seaborn heatmap.\n    \n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - math\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> generate_and_visualize_data()\n    \"\"\"\n    data = np.random.rand(SIZE)\n    formatted_data = np.array([\"{:.{}f}\".format(num, PRECISION) for num in data])\n    formatted_data = formatted_data.reshape(int(math.sqrt(SIZE)), int(math.sqrt(SIZE)))\n    sns.heatmap(formatted_data, annot=True)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, math, matplotlib, sklearn.preprocessing\n```python\nimport numpy as np\nimport math\nfrom matplotlib import pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Constants\nSIZE = 1000\nPRECISION = 2\n\ndef normalize_and_plot():\n    \"\"\"\n    Generate a numpy array of random floating point numbers of a specified size, normalize the data \n    using min-max scaling, format each number with a specific precision, and plot the data.\n    \n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - math\n    - matplotlib.pyplot\n    - sklearn.preprocessing\n\n    Example:\n    >>> normalize_and_plot()\n    \"\"\"\n    data = np.random.rand(SIZE)\n    scaler = MinMaxScaler()\n    scaled_data = scaler.fit_transform(data.reshape(-1, 1))\n    formatted_data = [\"{:.{}f}\".format(num[0], PRECISION) for num in scaled_data]\n    plt.plot(formatted_data)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, math, pandas, seaborn\n```python\nimport numpy as np\nimport math\nimport pandas as pd\nimport seaborn as sns\n\n# Constants\nSIZE = 500\nPRECISION = 2\n\ndef generate_dataframe_and_plot():\n    \"\"\"\n    Generate a pandas DataFrame of random floating point numbers of a specified size, \n    format each number with a specific precision, and visualize the DataFrame using a seaborn pairplot.\n    \n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - math\n    - pandas\n    - seaborn\n\n    Example:\n    >>> generate_dataframe_and_plot()\n    \"\"\"\n    data = np.random.rand(SIZE)\n    formatted_data = pd.DataFrame({\"{:.{}f}\".format(num, PRECISION): num for num in data}, index=[0])\n    sns.pairplot(formatted_data)\n    plt.show()\n```\n"}
{"seed": "{\"task_id\": 27963, \"prompt\": \"def f_27963(a1, a2):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"for a in zip(a1, a2):\\n\\n\\t\\tprint(*a)\\n\", \"test_start\": \"\\nimport sys\\nimport io\\n\\ndef check(candidate):\", \"test\": [\"\\n    a1 = [1, 2, 3]\\n    a2 = [4, 5, 6]\\n    capturedOutput = io.StringIO() \\n    sys.stdout = capturedOutput \\n    candidate(a1, a2)\\n    sys.stdout = sys.__stdout__\\n    assert capturedOutput.getvalue() == '1 4\\\\n2 5\\\\n3 6\\\\n'\\n\"], \"entry_point\": \"f_27963\", \"intent\": \"\\u0412\\u044b\\u0432\\u0435\\u0441\\u0442\\u0438 \\u044d\\u043b\\u0435\\u043c\\u0435\\u043d\\u0442\\u044b \\u0441\\u043f\\u0438\\u0441\\u043a\\u043e\\u0432 `a1`\\u0438 `a2` \\u043f\\u043e\\u043f\\u0430\\u0440\\u043d\\u043e\", \"library\": [\"io\", \"sys\"]}", "generation": "Scenario 1:\npandas, itertools, numpy, statistics, matplotlib\n```python\nimport pandas as pd\nimport itertools\nimport numpy as np\nimport statistics as stats\nimport matplotlib.pyplot as plt\n\ndef analyze_data_pairs(list1, list2):\n    \"\"\"\n    Analyze pairs of data from two lists. For each pair, compute the mean, the median, \n    and the standard deviation. Return a DataFrame with the results and a histogram.\n\n    Parameters:\n    list1 (list): The first list of data.\n    list2 (list): The second list of data.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the statistical results for each pair.\n\n    Requirements:\n    - pandas\n    - itertools\n    - numpy\n    - statistics\n    - matplotlib.pyplot\n\n    Example:\n    >>> list1 = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n    >>> list2 = [[10, 11, 12], [13, 14, 15], [16, 17, 18]]\n    >>> df = analyze_data_pairs(list1, list2)\n    >>> print(df)\n    >>> df['Mean'].hist()\n    \"\"\"\n    data = []\n    for l1, l2 in zip(list1, list2):\n        pair_data = list(itertools.chain(l1, l2))\n        mean = np.mean(pair_data)\n        median = stats.median(pair_data)\n        std_dev = np.std(pair_data)\n        data.append([mean, median, std_dev])\n\n    df = pd.DataFrame(data, columns=['Mean', 'Median', 'Standard Deviation'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, itertools, matplotlib, random, statistics\n```python\nimport numpy as np\nimport itertools\nimport matplotlib.pyplot as plt\nimport random\nimport statistics as stats\n\n# Constants\nRANGE_START = 1\nRANGE_END = 100\nPAIRS_COUNT = 100\n\ndef random_pairs_histogram():\n    \"\"\"\n    Generate a number of pairs of random integers within a range, compute the mean of each pair,\n    and display a histogram of the means.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - itertools\n    - matplotlib.pyplot\n    - random\n    - statistics\n\n    Example:\n    >>> random_pairs_histogram()\n    \"\"\"\n    means = []\n    for _ in range(PAIRS_COUNT):\n        pair = random.sample(range(RANGE_START, RANGE_END+1), 2)\n        mean = stats.mean(pair)\n        means.append(mean)\n\n    plt.hist(means, bins=np.arange(RANGE_START, RANGE_END+1), alpha=0.5)\n    plt.title('Histogram of Means')\n    plt.xlabel('Mean')\n    plt.ylabel('Frequency')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, itertools, matplotlib, random, statistics\n```python\nimport numpy as np\nimport itertools\nimport matplotlib.pyplot as plt\nimport random\nimport statistics as stats\n\n# Constants\nRANGE_START = 1\nRANGE_END = 100\nPAIRS_COUNT = 100\n\ndef generate_random_pairs():\n    \"\"\"\n    Generate a number of pairs of random integers within a range and return the mean, \n    median, and standard deviation of the pairs.\n\n    Returns:\n    list: A list of tuples, each containing the mean, median, and standard deviation of a pair.\n\n    Requirements:\n    - numpy\n    - itertools\n    - matplotlib.pyplot\n    - random\n    - statistics\n\n    Example:\n    >>> pairs_stats = generate_random_pairs()\n    >>> print(pairs_stats)\n    \"\"\"\n    pairs_stats = []\n    for _ in range(PAIRS_COUNT):\n        pair = random.sample(range(RANGE_START, RANGE_END+1), 2)\n        mean = stats.mean(pair)\n        median = stats.median(pair)\n        std_dev = stats.stdev(pair)\n        pairs_stats.append((mean, median, std_dev))\n\n    return pairs_stats\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, itertools, numpy, seaborn, statistics\n```python\nimport pandas as pd\nimport itertools\nimport numpy as np\nimport seaborn as sns\nimport statistics as stats\n\ndef analyze_data_pairs_seaborn(list1, list2):\n    \"\"\"\n    Analyze pairs of data from two lists. For each pair, compute the mean, the median, \n    and the standard deviation. Return a DataFrame with the results and a histogram with seaborn.\n\n    Parameters:\n    list1 (list): The first list of data.\n    list2 (list): The second list of data.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the statistical results for each pair.\n\n    Requirements:\n    - pandas\n    - itertools\n    - numpy\n    - seaborn\n    - statistics\n\n    Example:\n    >>> list1 = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n    >>> list2 = [[10, 11, 12], [13, 14, 15], [16, 17, 18]]\n    >>> df = analyze_data_pairs_seaborn(list1, list2)\n    >>> print(df)\n    >>> sns.distplot(df['Mean'])\n    \"\"\"\n    data = []\n    for l1, l2 in zip(list1, list2):\n        pair_data = list(itertools.chain(l1, l2))\n        mean = np.mean(pair_data)\n        median = stats.median(pair_data)\n        std_dev = np.std(pair_data)\n        data.append([mean, median, std_dev])\n\n    df = pd.DataFrame(data, columns=['Mean', 'Median', 'Standard Deviation'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, itertools, matplotlib, random, statistics\n```python\nimport numpy as np\nimport itertools\nimport matplotlib.pyplot as plt\nimport random\nimport statistics as stats\n\n# Constants\nRANGE_START = 1\nRANGE_END = 100\nPAIRS_COUNT = 100\n\ndef plot_random_pairs_distribution():\n    \"\"\"\n    Generate a number of pairs of random integers within a range, compute the mean of each pair,\n    and display a distribution plot of the means.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - itertools\n    - matplotlib.pyplot\n    - random\n    - statistics\n\n    Example:\n    >>> plot_random_pairs_distribution()\n    \"\"\"\n    means = []\n    for _ in range(PAIRS_COUNT):\n        pair = random.sample(range(RANGE_START, RANGE_END+1), 2)\n        mean = stats.mean(pair)\n        means.append(mean)\n\n    plt.hist(means, bins=np.arange(RANGE_START, RANGE_END+1), alpha=0.5, density=True)\n    plt.title('Distribution of Means')\n    plt.xlabel('Mean')\n    plt.ylabel('Density')\n    plt.show()\n```\n"}
{"seed": "{\"task_id\": 61709, \"prompt\": \"def f_61709(nums, target_num):\\n\\t\", \"suffix\": \"\\n\\treturn num\", \"canonical_solution\": \"delta_nums = [abs(target_num - i) for i in nums]\\n\\tnum = nums[delta_nums.index(min(delta_nums))]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([34, 65, 95, 37, 573], 84) == 95\\n\"], \"entry_point\": \"f_61709\", \"intent\": \"\\u041d\\u0430\\u0439\\u0442\\u0438 \\u0441\\u0430\\u043c\\u043e\\u0435 \\u0431\\u043b\\u0438\\u0437\\u043a\\u043e\\u0435 \\u0447\\u0438\\u0441\\u043b\\u043e `n` \\u043a \\u0447\\u0438\\u0441\\u043b\\u0443 `target_num` \\u0432 \\u0441\\u043f\\u0438\\u0441\\u043a\\u0435 `nums`\", \"library\": []}", "generation": "Scenario 1: \nnumpy, pandas, matplotlib, random\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nNUM_CATEGORIES = 5\nTARGET_VALUES = [34, 65, 95, 37, 573]\n\ndef generate_data_and_find_closest(target_num):\n    \"\"\"\n    Generate a DataFrame of random numbers categorized into multiple categories \n    and find the closest number to a target number in each category.\n    \n    Parameters:\n    target_num (int): The target number to find the closest number to.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the closest numbers to the target number in each category.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - random\n    \n    Example:\n    >>> closest_nums = generate_data_and_find_closest(84)\n    >>> print(closest_nums)\n    >>> closest_nums.plot(kind='bar')\n    \"\"\"\n    # Generate random data\n    data = {'Category': [], 'Value': []}\n    for i in range(NUM_CATEGORIES):\n        category = f'Category {i+1}'\n        values = [randint(0, 1000) for _ in range(100)]\n        data['Category'].extend([category]*100)\n        data['Value'].extend(values)\n        \n    df = pd.DataFrame(data)\n    \n    # Find the closest number to the target number in each category\n    closest_nums = df.groupby('Category').apply(lambda x: find_closest_num(x['Value'], target_num))\n    \n    return closest_nums\n\ndef find_closest_num(nums, target_num):\n    \"\"\"\n    Find the closest number to a target number in a list of numbers.\n    \n    Parameters:\n    nums (list): The list of numbers.\n    target_num (int): The target number.\n    \n    Returns:\n    int: The closest number to the target number.\n    \"\"\"\n    delta_nums = [abs(target_num - i) for i in nums]\n    num = nums[delta_nums.index(min(delta_nums))]\n    return num\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nnumpy, pandas, matplotlib, random\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nNUM_CATEGORIES = 5\nTARGET_VALUES = [34, 65, 95, 37, 573]\n\ndef generate_data_and_find_closest(target_nums):\n    \"\"\"\n    Generate a DataFrame of random numbers categorized into multiple categories \n    and find the closest number to each target number in each category.\n    \n    Parameters:\n    target_nums (list): The list of target numbers to find the closest numbers to.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the closest numbers to each target number in each category.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - random\n    \n    Example:\n    >>> closest_nums = generate_data_and_find_closest(TARGET_VALUES)\n    >>> print(closest_nums)\n    >>> closest_nums.plot(kind='bar')\n    \"\"\"\n    # Generate random data\n    data = {'Category': [], 'Value': []}\n    for i in range(NUM_CATEGORIES):\n        category = f'Category {i+1}'\n        values = [randint(0, 1000) for _ in range(100)]\n        data['Category'].extend([category]*100)\n        data['Value'].extend(values)\n        \n    df = pd.DataFrame(data)\n    \n    # Find the closest number to each target number in each category\n    closest_nums = pd.DataFrame(index=target_nums, columns=[f'Category {i+1}' for i in range(NUM_CATEGORIES)])\n    for target_num in target_nums:\n        closest_nums.loc[target_num] = df.groupby('Category').apply(lambda x: find_closest_num(x['Value'], target_num))\n\n    return closest_nums\n\ndef find_closest_num(nums, target_num):\n    \"\"\"\n    Find the closest number to a target number in a list of numbers.\n    \n    Parameters:\n    nums (list): The list of numbers.\n    target_num (int): The target number.\n    \n    Returns:\n    int: The closest number to the target number.\n    \"\"\"\n    delta_nums = [abs(target_num - i) for i in nums]\n    num = nums[delta_nums.index(min(delta_nums))]\n    return num\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnumpy, pandas, matplotlib, random\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nNUM_CATEGORIES = 5\nTARGET_VALUE = 84\n\ndef generate_data_and_find_closest(target_num):\n    \"\"\"\n    Generate a DataFrame of random numbers categorized into multiple categories \n    and find the closest number to a target number in each category.\n    \n    Parameters:\n    target_num (int): The target number to find the closest number to.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the closest numbers to the target number in each category.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - random\n    \n    Example:\n    >>> closest_nums = generate_data_and_find_closest(84)\n    >>> print(closest_nums)\n    >>> closest_nums.plot(kind='bar')\n    \"\"\"\n    # Generate random data\n    data = {'Category': [], 'Value': []}\n    for i in range(NUM_CATEGORIES):\n        category = f'Category {i+1}'\n        values = [randint(0, 1000) for _ in range(100)]\n        data['Category'].extend([category]*100)\n        data['Value'].extend(values)\n        \n    df = pd.DataFrame(data)\n    \n    # Find the closest number to the target number in each category\n    closest_nums = df.groupby('Category').apply(lambda x: find_closest_num(x['Value'], target_num))\n    \n    return closest_nums\n\ndef find_closest_num(nums, target_num):\n    \"\"\"\n    Find the closest number to a target number in a list of numbers.\n    \n    Parameters:\n    nums (list): The list of numbers.\n    target_num (int): The target number.\n    \n    Returns:\n    int: The closest number to the target number.\n    \"\"\"\n    delta_nums = [abs(target_num - i) for i in nums]\n    num = nums[delta_nums.index(min(delta_nums))]\n    return num\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, pandas, matplotlib, random\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nNUM_CATEGORIES = 5\nTARGET_VALUES = [84, 34, 65, 95, 37, 573]\n\ndef generate_data_and_find_closest(target_nums):\n    \"\"\"\n    Generate a DataFrame of random numbers categorized into multiple categories \n    and find the closest number to each target number in each category.\n    \n    Parameters:\n    target_nums (list): The list of target numbers to find the closest numbers to.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the closest numbers to each target number in each category.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - random\n    \n    Example:\n    >>> closest_nums = generate_data_and_find_closest(TARGET_VALUES)\n    >>> print(closest_nums)\n    >>> closest_nums.plot(kind='bar')\n    \"\"\"\n    # Generate random data\n    data = {'Category': [], 'Value': []}\n    for i in range(NUM_CATEGORIES):\n        category = f'Category {i+1}'\n        values = [randint(0, 1000) for _ in range(100)]\n        data['Category'].extend([category]*100)\n        data['Value'].extend(values)\n        \n    df = pd.DataFrame(data)\n    \n    # Find the closest number to each target number in each category\n    closest_nums = pd.DataFrame(index=target_nums, columns=[f'Category {i+1}' for i in range(NUM_CATEGORIES)])\n    for target_num in target_nums:\n        closest_nums.loc[target_num] = df.groupby('Category').apply(lambda x: find_closest_num(x['Value'], target_num))\n\n    return closest_nums\n\ndef find_closest_num(nums, target_num):\n    \"\"\"\n    Find the closest number to a target number in a list of numbers.\n    \n    Parameters:\n    nums (list): The list of numbers.\n    target_num (int): The target number.\n    \n    Returns:\n    int: The closest number to the target number.\n    \"\"\"\n    delta_nums = [abs(target_num - i) for i in nums]\n    num = nums[delta_nums.index(min(delta_nums))]\n    return num\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnumpy, pandas, matplotlib, random\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nNUM_CATEGORIES = 5\nTARGET_VALUE = 84\n\ndef generate_data_and_find_closest(target_num):\n    \"\"\"\n    Generate a DataFrame of random numbers categorized into multiple categories \n    and find the closest number to a target number in each category.\n    \n    Parameters:\n    target_num (int): The target number to find the closest number to.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the closest numbers to the target number in each category.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - random\n    \n    Example:\n    >>> closest_nums = generate_data_and_find_closest(84)\n    >>> print(closest_nums)\n    >>> closest_nums.plot(kind='bar')\n    \"\"\"\n    # Generate random data\n    data = {'Category': [], 'Value': []}\n    for i in range(NUM_CATEGORIES):\n        category = f'Category {i+1}'\n        values = [randint(0, 1000) for _ in range(100)]\n        data['Category'].extend([category]*100)\n        data['Value'].extend(values)\n        \n    df = pd.DataFrame(data)\n    \n    # Find the closest number to the target number in each category\n    closest_nums = df.groupby('Category').apply(lambda x: find_closest_num(x['Value'], target_num))\n    \n    return closest_nums\n\ndef find_closest_num(nums, target_num):\n    \"\"\"\n    Find the closest number to a target number in a list of numbers.\n    \n    Parameters:\n    nums (list): The list of numbers.\n    target_num (int): The target number.\n    \n    Returns:\n    int: The closest number to the target number.\n    \"\"\"\n    delta_nums = [abs(target_num - i) for i in nums]\n    num = nums[delta_nums.index(min(delta_nums))]\n    return num\n```"}
{"seed": "{\"task_id\": 111347, \"prompt\": \"def f_111347(filepath):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"os.access(filepath, os.R_OK)\", \"test_start\": \"\\nimport os\\nfrom unittest.mock import Mock\\n\\ndef check(candidate):\", \"test\": [\"\\n    filepath= '/'\\n    os.access = Mock(return_value = True)\\n    assert candidate(filepath) == True\\n\", \"\\n    filepath= '/'\\n    os.access = Mock(return_value = False)\\n    assert candidate(filepath) == False\\n\"], \"entry_point\": \"f_111347\", \"intent\": \"\\u041f\\u0440\\u043e\\u0432\\u0435\\u0440\\u0438\\u044c \\u043f\\u0440\\u0430\\u0432\\u0430 \\u043d\\u0430 \\u0447\\u0442\\u0435\\u043d\\u0438\\u0435 \\u0444\\u0430\\u0439\\u043b\\u0430 `filepath`\", \"library\": [\"os\"]}", "generation": "Scenario 1:\nos, hashlib, base64\n```python\nimport os\nimport hashlib\nimport base64\n\ndef get_file_hash(filepath):\n    \"\"\"\n    Get the hash of a file using SHA256 algorithm. The hash is encoded in base64 format.\n    \n    Parameters:\n    filepath (str): The path to the file.\n    \n    Returns:\n    str: The base64-encoded SHA256 hash of the file.\n    \n    Requirements:\n    - os\n    - hashlib\n    - base64\n    \n    Example:\n    >>> get_file_hash('/path/to/file')\n    \"\"\"\n    with open(filepath, 'rb') as f:\n        data = f.read()\n        hash_value = hashlib.sha256(data).digest()\n        base64_hash = base64.b64encode(hash_value)\n    return base64_hash.decode()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, shutil, zipfile\n```python\nimport os\nimport shutil\nimport zipfile\n\ndef zip_directory(directory_path, output_path):\n    \"\"\"\n    Zip a directory and save it to the specified output path.\n    \n    Parameters:\n    directory_path (str): The path to the directory to be zipped.\n    output_path (str): The path where the zipped file will be saved.\n    \n    Returns:\n    str: The path to the zipped file.\n    \n    Requirements:\n    - os\n    - shutil\n    - zipfile\n    \n    Example:\n    >>> zip_directory('/path/to/directory', '/path/to/output.zip')\n    \"\"\"\n    with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        for root, dirs, files in os.walk(directory_path):\n            for file in files:\n                zipf.write(os.path.join(root, file), \n                           os.path.relpath(os.path.join(root, file), \n                                           os.path.join(directory_path, '..')))\n    return output_path\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, pandas, matplotlib\n```python\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef analyze_file_sizes(directory_path):\n    \"\"\"\n    Analyze the file sizes in a directory and plot a histogram of the file sizes.\n    \n    Parameters:\n    directory_path (str): The path to the directory.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with file sizes.\n    \n    Requirements:\n    - os\n    - pandas\n    - matplotlib\n    \n    Example:\n    >>> analyze_file_sizes('/path/to/directory')\n    \"\"\"\n    file_sizes = []\n    for root, dirs, files in os.walk(directory_path):\n        for file in files:\n            file_path = os.path.join(root, file)\n            file_size = os.path.getsize(file_path)\n            file_sizes.append(file_size)\n    df = pd.DataFrame(file_sizes, columns=['File Size'])\n    df.hist(bins=50)\n    plt.show()\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, sys, argparse\n```python\nimport os\nimport sys\nimport argparse\n\ndef print_file_info(filepath):\n    \"\"\"\n    Print the information about a file, including its size, creation time, and last modification time.\n    \n    Parameters:\n    filepath (str): The path to the file.\n    \n    Returns:\n    dict: A dictionary with file information.\n    \n    Requirements:\n    - os\n    - sys\n    - argparse\n    \n    Example:\n    >>> print_file_info('/path/to/file')\n    \"\"\"\n    file_info = {\n        'size': os.path.getsize(filepath),\n        'creation_time': os.path.getctime(filepath),\n        'last_modification_time': os.path.getmtime(filepath)\n    }\n    for key, value in file_info.items():\n        print(f'{key}: {value}')\n    return file_info\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, csv, json\n```python\nimport os\nimport csv\nimport json\n\ndef convert_csv_to_json(csv_filepath, json_filepath):\n    \"\"\"\n    Convert a CSV file to a JSON file.\n    \n    Parameters:\n    csv_filepath (str): The path to the CSV file.\n    json_filepath (str): The path to the output JSON file.\n    \n    Returns:\n    str: The path to the output JSON file.\n    \n    Requirements:\n    - os\n    - csv\n    - json\n    \n    Example:\n    >>> convert_csv_to_json('/path/to/file.csv', '/path/to/file.json')\n    \"\"\"\n    with open(csv_filepath, 'r') as csvf, open(json_filepath, 'w') as jsonf:\n        reader = csv.DictReader(csvf)\n        data = list(reader)\n        json.dump(data, jsonf)\n    return json_filepath\n```"}
{"seed": "{\"task_id\": 37423, \"prompt\": \"def f_37423(html):\\n\\t\", \"suffix\": \"\\n\\treturn root\", \"canonical_solution\": \"root = etree.HTML(html)\", \"test_start\": \"\\nfrom lxml import etree\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert etree.tostring(candidate('<p>data</p>')) == b'<html><body><p>data</p></body></html>'\\n\"], \"entry_point\": \"f_37423\", \"intent\": \"\\u0420\\u0430\\u0441\\u043f\\u0430\\u0440\\u0441\\u0438\\u0442\\u044c html \\u0441\\u0442\\u0440\\u043e\\u043a\\u0443 `html` \\u0432 \\u0434\\u0435\\u0440\\u0435\\u0432\\u043e `root`\", \"library\": [\"lxml\"]}", "generation": "Scenario 1:\nlxml, requests, re, csv\n```python\nfrom lxml import html\nimport requests\nimport re\nimport csv\n\n# Constants\nURL = 'https://example.com'\n\ndef scrape_and_store_data():\n    \"\"\"\n    Scrape data from a webpage, extract certain elements using regular expressions, \n    and store the extracted data in a CSV file.\n\n    Requirements:\n    - lxml\n    - requests\n    - re\n    - csv\n\n    Example:\n    >>> scrape_and_store_data()\n    \"\"\"\n    response = requests.get(URL)\n    tree = html.fromstring(response.content)\n\n    data = tree.xpath('//p/text()')\n\n    with open('output.csv', 'w') as f:\n        writer = csv.writer(f)\n        for line in data:\n            matches = re.findall(r'\\d+', line)\n            if matches:\n                writer.writerow(matches)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nlxml, requests, bs4, pandas\n```python\nfrom lxml import html\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\n# Constants\nURL = 'https://example.com'\n\ndef extract_and_convert_table():\n    \"\"\"\n    Extract a table from a webpage and convert it into a pandas DataFrame.\n\n    Requirements:\n    - lxml\n    - requests\n    - bs4\n    - pandas\n\n    Example:\n    >>> df = extract_and_convert_table()\n    >>> print(df)\n    \"\"\"\n    response = requests.get(URL)\n    soup = BeautifulSoup(response.text, 'lxml')\n    table = soup.find_all('table')[0]\n\n    df = pd.read_html(str(table))[0]\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nlxml, requests, json, os\n```python\nfrom lxml import html\nimport requests\nimport json\nimport os\n\n# Constants\nURL = 'https://example.com'\nOUTPUT_DIR = './output'\n\ndef scrape_and_store_json():\n    \"\"\"\n    Scrape data from a webpage, parse it into JSON format, and store it in a JSON file.\n\n    Requirements:\n    - lxml\n    - requests\n    - json\n    - os\n\n    Example:\n    >>> scrape_and_store_json()\n    \"\"\"\n    response = requests.get(URL)\n    tree = html.fromstring(response.content)\n\n    data = tree.xpath('//p/text()')\n\n    if not os.path.exists(OUTPUT_DIR):\n        os.makedirs(OUTPUT_DIR)\n\n    with open(os.path.join(OUTPUT_DIR, 'output.json'), 'w') as f:\n        json.dump(data, f)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nlxml, requests, matplotlib, numpy\n```python\nfrom lxml import html\nimport requests\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nURL = 'https://example.com'\n\ndef scrape_and_plot_data():\n    \"\"\"\n    Scrape data from a webpage, convert the data into a histogram and plot it using matplotlib.\n\n    Requirements:\n    - lxml\n    - requests\n    - matplotlib\n    - numpy\n\n    Example:\n    >>> scrape_and_plot_data()\n    \"\"\"\n    response = requests.get(URL)\n    tree = html.fromstring(response.content)\n\n    data = tree.xpath('//p/text()')\n\n    plt.hist(np.array(data, dtype=float), bins=10)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nlxml, requests, pandas, sqlite3\n```python\nfrom lxml import html\nimport requests\nimport pandas as pd\nimport sqlite3\n\n# Constants\nURL = 'https://example.com'\n\ndef scrape_and_store_sqlite():\n    \"\"\"\n    Scrape data from a webpage, convert the data into a pandas DataFrame and store it in a SQLite database.\n\n    Requirements:\n    - lxml\n    - requests\n    - pandas\n    - sqlite3\n\n    Example:\n    >>> scrape_and_store_sqlite()\n    \"\"\"\n    response = requests.get(URL)\n    tree = html.fromstring(response.content)\n\n    data = tree.xpath('//p/text()')\n\n    df = pd.DataFrame(data, columns=['Data'])\n\n    conn = sqlite3.connect('output.db')\n    df.to_sql('Table', conn, if_exists='replace')\n```"}
{"seed": "{\"task_id\": 93346, \"prompt\": \"def f_93346(url):\\n\\t\", \"suffix\": \"\\n\\treturn Response\", \"canonical_solution\": \"\\n\\twith urllib.request.urlopen(url) as Response:\\n\\t\\tLength = Response.getheader('content-length')\\n\\t\\tBlockSize = 1000000  # default value\\n\\n\\t\\tif Length: \\n\\t\\t\\tLength = int(Length)\\n\\t\\t\\tBlockSize = max(4096, Length // 20)\\n\\t\\tprint(\\\"UrlLib len, blocksize: \\\", Length, BlockSize)\\n        \\n\\t\\tBufferAll = io.BytesIO()\\n\\t\\tSize = 0\\n\\t\\twhile True:\\n\\t\\t\\tBufferNow = Response.read(BlockSize)\\n\\t\\t\\tif not BufferNow:  break\\n\\t\\t\\tBufferAll.write(BufferNow)\\n\\t\\t\\tSize += len(BufferNow)\\n\\t\\t\\tif Length:\\n\\t\\t\\t\\tPercent = int((Size / Length)*100)\\n\\t\\t\\t\\tprint('download: {Percent}% {Url}')\\n\\t\\tprint(\\\"Buffer All len:\\\", len(BufferAll.getvalue()))\\n\", \"test_start\": \"\\nimport io\\nimport urllib \\n\\ndef check(candidate):\", \"test\": [\"\\n    response = candidate('https://en.wikipedia.org/wiki/Pratap_Singh_(Raja_of_Satara)')\\n    assert 'Response' in str(type(response))\\n\"], \"entry_point\": \"f_93346\", \"intent\": \"\\u0421\\u043a\\u0430\\u0447\\u0430\\u0442\\u044c \\u0444\\u0430\\u0439\\u043b \\u0441 \\u0441\\u0430\\u0439\\u0442\\u0430 `url` \\u043f\\u043e\\u043a\\u0430\\u0437\\u044b\\u0432\\u0430\\u044f \\u043f\\u0440\\u043e\\u0433\\u0440\\u0435\\u0441\\u0441\\u0431\\u0430\\u0440\", \"library\": [\"io\", \"urllib\"]}", "generation": "Scenario 3: \nrequests, urllib, os, hashlib, io\n```python\nimport requests\nimport urllib.parse\nimport os\nimport hashlib\nimport io\n\n# Constants\nBUFFER_SIZE = 8192\n\ndef download_and_hash_file(url, destination_folder):\n    \"\"\"\n    Download a file from a URL to a destination folder, display the download progress, \n    and return the SHA256 hash of the downloaded file.\n\n    Parameters:\n    url (str): The URL of the file to download.\n    destination_folder (str): The path to the folder where the file should be saved.\n\n    Returns:\n    str: The SHA256 hash of the downloaded file.\n\n    Requirements:\n    - requests\n    - urllib.parse\n    - os\n    - hashlib\n    - io\n\n    Example:\n    >>> download_and_hash_file('https://example.com/file.txt', '/path/to/destination')\n    \"\"\"\n    response = requests.get(url, stream=True)\n    response.raise_for_status()\n\n    url_path = urllib.parse.urlparse(url).path\n    filename = os.path.basename(url_path)\n    file_path = os.path.join(destination_folder, filename)\n\n    sha256_hash = hashlib.sha256()\n\n    total_size = int(response.headers.get('content-length', 0))\n    block_size = BUFFER_SIZE\n    completed_size = 0\n\n    with io.open(file_path, 'wb') as file:\n        for data in response.iter_content(block_size):\n            completed_size += len(data)\n            file.write(data)\n            sha256_hash.update(data)\n            progress = (completed_size / total_size) * 100\n            print(f'Downloaded {progress:.2f}%')\n\n    file_hash = sha256_hash.hexdigest()\n\n    return file_hash\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nrequests, urllib, os, io, pandas, matplotlib\n```python\nimport requests\nimport urllib.parse\nimport os\nimport io\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nBLOCK_SIZE = 8192\n\ndef download_and_analyze_csv(url, destination_folder):\n    \"\"\"\n    Download a CSV file from a URL to a destination folder, display the download progress, \n    parse the CSV file into a pandas DataFrame, and plot the first column as a histogram.\n\n    Parameters:\n    url (str): The URL of the CSV file to download.\n    destination_folder (str): The path to the folder where the file should be saved.\n\n    Returns:\n    DataFrame: The parsed pandas DataFrame.\n\n    Requirements:\n    - requests\n    - urllib.parse\n    - os\n    - io\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = download_and_analyze_csv('https://example.com/file.csv', '/path/to/destination')\n    >>> df.head()\n    \"\"\"\n    response = requests.get(url, stream=True)\n    response.raise_for_status()\n\n    url_path = urllib.parse.urlparse(url).path\n    filename = os.path.basename(url_path)\n    file_path = os.path.join(destination_folder, filename)\n\n    total_size = int(response.headers.get('content-length', 0))\n    block_size = BLOCK_SIZE\n    completed_size = 0\n\n    with io.open(file_path, 'wb') as file:\n        for data in response.iter_content(block_size):\n            completed_size += len(data)\n            file.write(data)\n            progress = (completed_size / total_size) * 100\n            print(f'Downloaded {progress:.2f}%')\n\n    df = pd.read_csv(file_path)\n    df.hist(column=df.columns[0])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nrequests, urllib, os, io, zipfile\n```python\nimport requests\nimport urllib.parse\nimport os\nimport io\nimport zipfile\n\nBLOCK_SIZE = 8192\n\ndef download_and_unzip(url, destination_folder):\n    \"\"\"\n    Download a ZIP file from a URL to a destination folder, display the download progress, \n    and extract all files from the ZIP file to the destination folder.\n\n    Parameters:\n    url (str): The URL of the ZIP file to download.\n    destination_folder (str): The path to the folder where the file should be saved.\n\n    Returns:\n    None\n\n    Requirements:\n    - requests\n    - urllib.parse\n    - os\n    - io\n    - zipfile\n\n    Example:\n    >>> download_and_unzip('https://example.com/file.zip', '/path/to/destination')\n    \"\"\"\n    response = requests.get(url, stream=True)\n    response.raise_for_status()\n\n    url_path = urllib.parse.urlparse(url).path\n    filename = os.path.basename(url_path)\n    file_path = os.path.join(destination_folder, filename)\n\n    total_size = int(response.headers.get('content-length', 0))\n    block_size = BLOCK_SIZE\n    completed_size = 0\n\n    with io.open(file_path, 'wb') as file:\n        for data in response.iter_content(block_size):\n            completed_size += len(data)\n            file.write(data)\n            progress = (completed_size / total_size) * 100\n            print(f'Downloaded {progress:.2f}%')\n\n    with zipfile.ZipFile(file_path, 'r') as zip_ref:\n        zip_ref.extractall(destination_folder)\n```\n`GPT_ODEX_BREAK`\n\nScenario 6:\nrequests, urllib, os, io, PIL\n```python\nimport requests\nimport urllib.parse\nimport os\nimport io\nfrom PIL import Image\n\nBLOCK_SIZE = 8192\n\ndef download_and_display_image(url, destination_folder):\n    \"\"\"\n    Download an image file from a URL to a destination folder, display the download progress, \n    and display the image using PIL.\n\n    Parameters:\n    url (str): The URL of the image file to download.\n    destination_folder (str): The path to the folder where the file should be saved.\n\n    Returns:\n    None\n\n    Requirements:\n    - requests\n    - urllib.parse\n    - os\n    - io\n    - PIL\n\n    Example:\n    >>> download_and_display_image('https://example.com/image.jpg', '/path/to/destination')\n    \"\"\"\n    response = requests.get(url, stream=True)\n    response.raise_for_status()\n\n    url_path = urllib.parse.urlparse(url).path\n    filename = os.path.basename(url_path)\n    file_path = os.path.join(destination_folder, filename)\n\n    total_size = int(response.headers.get('content-length', 0))\n    block_size = BLOCK_SIZE\n    completed_size = 0\n\n    with io.open(file_path, 'wb') as file:\n        for data in response.iter_content(block_size):\n            completed_size += len(data)\n            file.write(data)\n            progress = (completed_size / total_size) * 100\n            print(f'Downloaded {progress:.2f}%')\n\n    img = Image.open(file_path)\n    img.show()\n```"}
{"seed": "{\"task_id\": 319702, \"prompt\": \"def f_319702(dicts):\\n\\t\", \"suffix\": \"\\n\\treturn dicts\", \"canonical_solution\": \"dicts = [json.dumps(i) for i in dicts]\\n\\n\\tdicts = [json.loads(i) for i in set(dicts)]\\n\", \"test_start\": \"\\nimport json \\n\\ndef check(candidate):\", \"test\": [\"\\n    dicts = [{'a': 1}, {\\\"b\\\": 2}, {\\\"c\\\": 3}, {\\\"b\\\": 2}, {\\\"b\\\": '2'}]\\n    assert sorted(candidate(dicts), key=lambda x: list(x.keys())[0]) ==         [{'a': 1}, {'b': 2}, {'b': '2'}, {'c': 3}]\\n\"], \"entry_point\": \"f_319702\", \"intent\": \"\\u0423\\u0434\\u0430\\u043b\\u0438\\u0442\\u044c \\u043e\\u0434\\u0438\\u043d\\u0430\\u043a\\u043e\\u0432\\u044b\\u0435 \\u0441\\u043b\\u043e\\u0432\\u0430\\u0440\\u0438 \\u0438\\u0437 \\u0441\\u043f\\u0438\\u0441\\u043a\\u0430 \\u0441\\u043b\\u043e\\u0432\\u0430\\u0440\\u0435\\u0439 `dicts`\", \"library\": [\"json\"]}", "generation": "Scenario 1: \npandas, json, matplotlib.pyplot, random\n```python\nimport pandas as pd\nimport json\nimport matplotlib.pyplot as plt\nfrom random import choice\n\n# Constants\nANIMALS = ['Cat', 'Dog', 'Elephant', 'Lion', 'Tiger', 'Giraffe', 'Zebra']\n\ndef plot_animal_frequency(dicts):\n    \"\"\"\n    Remove duplicate dictionaries from the list and plot a bar graph showing\n    the frequency of each animal.\n\n    Parameters:\n    dicts (list): A list of dictionaries.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - json\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> dicts = [{'animal': choice(ANIMALS)} for _ in range(100)]\n    >>> plot_animal_frequency(dicts)\n    \"\"\"\n    unique_dicts = [json.loads(i) for i in set(json.dumps(d) for d in dicts)]\n    df = pd.DataFrame(unique_dicts)\n    df['animal'].value_counts().plot(kind='bar')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, json, random, matplotlib.pyplot\n```python\nimport numpy as np\nimport json\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nNUMBERS = list(range(1, 101))\n\ndef plot_number_frequency(dicts):\n    \"\"\"\n    Remove duplicate dictionaries from the list and plot a histogram showing\n    the frequency of each number.\n\n    Parameters:\n    dicts (list): A list of dictionaries.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - json\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> dicts = [{'number': randint(1, 100)} for _ in range(1000)]\n    >>> plot_number_frequency(dicts)\n    \"\"\"\n    unique_dicts = [json.loads(i) for i in set(json.dumps(d) for d in dicts)]\n    numbers = [d['number'] for d in unique_dicts]\n    plt.hist(numbers, bins=np.arange(1, 102)-0.5, edgecolor='black')\n    plt.xticks(NUMBERS)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\njson, random, collections, matplotlib.pyplot\n```python\nimport json\nfrom random import choice\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\n# Constants\nFRUITS = ['Apple', 'Orange', 'Banana', 'Cherry', 'Mango', 'Pineapple']\n\ndef plot_fruit_frequency(dicts):\n    \"\"\"\n    Remove duplicate dictionaries from the list and plot a pie chart showing\n    the frequency of each fruit.\n\n    Parameters:\n    dicts (list): A list of dictionaries.\n\n    Returns:\n    None\n\n    Requirements:\n    - json\n    - random\n    - collections\n    - matplotlib.pyplot\n\n    Example:\n    >>> dicts = [{'fruit': choice(FRUITS)} for _ in range(100)]\n    >>> plot_fruit_frequency(dicts)\n    \"\"\"\n    unique_dicts = [json.loads(i) for i in set(json.dumps(d) for d in dicts)]\n    fruits = [d['fruit'] for d in unique_dicts]\n    fruit_counter = Counter(fruits)\n    plt.pie(fruit_counter.values(), labels=fruit_counter.keys(), autopct='%1.1f%%')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\njson, random, pandas, numpy, matplotlib.pyplot\n```python\nimport json\nfrom random import randint\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nAGES = list(range(1, 101))\n\ndef plot_age_distribution(dicts):\n    \"\"\"\n    Remove duplicate dictionaries from the list and plot a histogram showing\n    the distribution of ages.\n\n    Parameters:\n    dicts (list): A list of dictionaries.\n\n    Returns:\n    None\n\n    Requirements:\n    - json\n    - random\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> dicts = [{'age': randint(1, 100)} for _ in range(1000)]\n    >>> plot_age_distribution(dicts)\n    \"\"\"\n    unique_dicts = [json.loads(i) for i in set(json.dumps(d) for d in dicts)]\n    df = pd.DataFrame(unique_dicts)\n    df['age'].plot(kind='hist', bins=np.arange(1, 102)-0.5, edgecolor='black')\n    plt.xticks(AGES)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\njson, random, pandas, seaborn\n```python\nimport json\nfrom random import choice\nimport pandas as pd\nimport seaborn as sns\n\n# Constants\nCOLORS = ['Red', 'Blue', 'Green', 'Yellow', 'Purple', 'Orange', 'Pink']\n\ndef plot_color_frequency(dicts):\n    \"\"\"\n    Remove duplicate dictionaries from the list and plot a bar graph showing\n    the frequency of each color.\n\n    Parameters:\n    dicts (list): A list of dictionaries.\n\n    Returns:\n    None\n\n    Requirements:\n    - json\n    - random\n    - pandas\n    - seaborn\n\n    Example:\n    >>> dicts = [{'color': choice(COLORS)} for _ in range(100)]\n    >>> plot_color_frequency(dicts)\n    \"\"\"\n    unique_dicts = [json.loads(i) for i in set(json.dumps(d) for d in dicts)]\n    df = pd.DataFrame(unique_dicts)\n    sns.countplot(x='color', data=df, order=df['color'].value_counts().index)\n    plt.show()\n```"}
{"seed": "{\"task_id\": 151936, \"prompt\": \"def f_151936(url):\\n\\t\", \"suffix\": \"\\n\\treturn query\", \"canonical_solution\": \"\\n\\tparsed_url = urlparse(url)\\n\\tquery = parsed_url.query\\n\", \"test_start\": \"\\nfrom urllib.parse import urlparse\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('https://reqres.in/api/users?page=2') == 'page=2'\\n\"], \"entry_point\": \"f_151936\", \"intent\": \"\\u0420\\u0430\\u0441\\u043f\\u0430\\u0440\\u0441\\u0438\\u0442\\u044c \\u043f\\u0430\\u0440\\u0430\\u043c\\u0435\\u0442\\u0440\\u044b get \\u0437\\u0430\\u043f\\u0440\\u043e\\u0441\\u0430 url `url` \\u0432 \\u0441\\u043b\\u043e\\u0432\\u0430\\u0440\\u044c `query`\", \"library\": [\"urllib\"]}", "generation": "Scenario 3:\nurllib, re, html, requests, bs4 (BeautifulSoup)\n```python\nfrom urllib.parse import urlparse\nimport re\nfrom html import unescape\nimport requests\nfrom bs4 import BeautifulSoup\n\n# Constant\nUSER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36'\n\ndef extract_image_srcs(url):\n    \"\"\"\n    Retrieve and parse an HTML document from the provided url, then find all image sources (img tags with src attribute).\n    \n    Parameters:\n    url (str): The URL of the HTML document.\n    \n    Returns:\n    list: A list of URLs of the images.\n    \n    Requirements:\n    - urllib.parse\n    - re\n    - html\n    - requests\n    - bs4 (BeautifulSoup)\n    \n    Example:\n    >>> extract_image_srcs('https://www.example.com')\n    \"\"\"\n    parsed_url = urlparse(url)\n    response = requests.get(url, headers={'User-Agent': USER_AGENT})\n    soup = BeautifulSoup(response.text, 'html.parser')\n    img_tags = soup.find_all('img')\n    img_srcs = [urlparse(img.get('src')).geturl() for img in img_tags]\n\n    return img_srcs\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nurllib, json, requests, os\n```python\nfrom urllib.parse import urlparse, urljoin\nimport json\nimport requests\nimport os\n\n# Constants\nAPI_KEY = os.getenv('API_KEY')\nBASE_URL = 'https://api.example.com/'\n\ndef get_data_from_api(endpoint):\n    \"\"\"\n    Send a GET request to the specified endpoint of the API, parse the response as JSON, and return the data.\n    \n    Parameters:\n    endpoint (str): The endpoint of the API.\n    \n    Returns:\n    dict: The data returned from the API.\n    \n    Requirements:\n    - urllib.parse\n    - json\n    - requests\n    - os\n    \n    Example:\n    >>> get_data_from_api('users/1')\n    \"\"\"\n    url = urljoin(BASE_URL, endpoint)\n    headers = {'Authorization': f'Bearer {API_KEY}'}\n    response = requests.get(url, headers=headers)\n    data = json.loads(response.text)\n\n    return data\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nurllib, lxml, requests, pandas\n```python\nfrom urllib.parse import urlparse\nfrom lxml import etree\nimport requests\nimport pandas as pd\n\n# Constants\nUSER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36'\n\ndef parse_xml_to_df(url):\n    \"\"\"\n    Retrieve an XML document from the provided url, parse it, and convert it into a pandas DataFrame.\n    \n    Parameters:\n    url (str): The URL of the XML document.\n    \n    Returns:\n    DataFrame: A pandas DataFrame that represents the XML document.\n    \n    Requirements:\n    - urllib.parse\n    - lxml\n    - requests\n    - pandas\n    \n    Example:\n    >>> parse_xml_to_df('https://www.example.com/data.xml')\n    \"\"\"\n    response = requests.get(url, headers={'User-Agent': USER_AGENT})\n    root = etree.fromstring(response.content)\n    data = [{element.tag: element.text for element in child} for child in root]\n    df = pd.DataFrame(data)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 6:\nurllib, json, requests, csv\n```python\nfrom urllib.parse import urlparse\nimport json\nimport requests\nimport csv\n\n# Constants\nAPI_KEY = 'your_api_key'\nBASE_URL = 'https://api.example.com/'\n\ndef fetch_and_write_to_csv(endpoint, filepath):\n    \"\"\"\n    Fetch data from a provided API endpoint and write the data to a CSV file.\n    \n    Parameters:\n    endpoint (str): The API endpoint to fetch data from.\n    filepath (str): The path to the CSV file where the data should be written.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - urllib.parse\n    - json\n    - requests\n    - csv\n    \n    Example:\n    >>> fetch_and_write_to_csv('users', '/path/to/file.csv')\n    \"\"\"\n    url = urlparse(BASE_URL + endpoint)\n    headers = {'Authorization': 'Bearer ' + API_KEY}\n    response = requests.get(url.geturl(), headers=headers)\n    data = json.loads(response.text)\n\n    with open(filepath, 'w', newline='') as csvfile:\n        writer = csv.DictWriter(csvfile, fieldnames=data[0].keys())\n        writer.writeheader()\n        writer.writerows(data)\n\n    return None\n```\n`GPT_ODEX_BREAK`\n\nScenario 7:\nurllib, re, requests, html\n```python\nfrom urllib.parse import urlparse\nimport re\nimport requests\nfrom html import unescape\n\n# Constant\nUSER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36'\n\ndef extract_links(url):\n    \"\"\"\n    Retrieve and parse an HTML document from the provided url, then find all hyperlinks (a tags with href attribute).\n    \n    Parameters:\n    url (str): The URL of the HTML document.\n    \n    Returns:\n    list: A list of URLs of the hyperlinks.\n    \n    Requirements:\n    - urllib.parse\n    - re\n    - requests\n    - html\n    \n    Example:\n    >>> extract_links('https://www.example.com')\n    \"\"\"\n    response = requests.get(url, headers={'User-Agent': USER_AGENT})\n    html = response.text\n    links = re.findall('<a href=\"([^\"]*)\"', html)\n    links = [unescape(link) for link in links]\n\n    return links\n```"}
{"seed": "{\"task_id\": 306957, \"prompt\": \"def f_306957(filepath, newfilepath):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"os.replace(filepath, newfilepath)\", \"test_start\": \"\\nimport os\\nfrom unittest.mock import Mock\\n\\ndef check(candidate):\", \"test\": [\"\\n    os.replace = Mock()\\n    try:\\n        candidate('abc.txt', 'abc.csv')\\n        assert True\\n    except:\\n        assert False\\n\"], \"entry_point\": \"f_306957\", \"intent\": \"\\u041f\\u0435\\u0440\\u0435\\u0438\\u043c\\u0435\\u043d\\u043e\\u0432\\u0430\\u0442\\u044c \\u0444\\u0430\\u0439\\u043b `filepath` \\u0432 \\u0444\\u0430\\u0439\\u043b `newfilepath`\", \"library\": [\"os\"]}", "generation": "Scenario 1: \nos, shutil, csv, pandas\n```python\nimport os\nimport shutil\nimport csv\nimport pandas as pd\n\n# Constants\nBACKUP_DIR = '/path/to/backup'\n\ndef rename_and_backup_file(filepath, newfilepath):\n    \"\"\"\n    Rename a file from `filepath` to `newfilepath` and create a backup of the original file.\n    After renaming, read the file and return its content as a pandas DataFrame.\n    \n    Parameters:\n    filepath (str): The original file path.\n    newfilepath (str): The new file path.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the file's content.\n    \n    Requirements:\n    - os\n    - shutil\n    - csv\n    - pandas\n\n    Example:\n    >>> rename_and_backup_file('data.csv', 'new_data.csv')\n    \"\"\"\n    # Backup the original file\n    shutil.copy2(filepath, BACKUP_DIR)\n    \n    # Rename the file\n    os.rename(filepath, newfilepath)\n    \n    # Read the file into a pandas DataFrame\n    df = pd.read_csv(newfilepath)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, glob, csv, pandas, matplotlib.pyplot\n```python\nimport os\nimport glob\nimport csv\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nDATA_DIR = '/path/to/data'\n\ndef rename_and_analyze_files(old_ext, new_ext):\n    \"\"\"\n    Rename all files in a directory from `old_ext` to `new_ext`, read the files,\n    concatenate their content into a single pandas DataFrame, and plot a histogram of the data.\n    \n    Parameters:\n    old_ext (str): The old file extension.\n    new_ext (str): The new file extension.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the concatenated content of the files.\n    \n    Requirements:\n    - os\n    - glob\n    - csv\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> rename_and_analyze_files('.txt', '.csv')\n    \"\"\"\n    all_data = []\n    \n    # Rename all files\n    for filepath in glob.glob(DATA_DIR + '/*' + old_ext):\n        newfilepath = os.path.splitext(filepath)[0] + new_ext\n        os.rename(filepath, newfilepath)\n        \n        # Read the file into a pandas DataFrame\n        df = pd.read_csv(newfilepath)\n        all_data.append(df)\n    \n    # Concatenate all data\n    all_data_df = pd.concat(all_data, ignore_index=True)\n    \n    # Plot a histogram of the data\n    all_data_df.hist(figsize=(10, 8))\n    plt.show()\n\n    return all_data_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, hashlib, csv, pandas\n```python\nimport os\nimport hashlib\nimport csv\nimport pandas as pd\n\n# Constants\nHASH_ALGORITHM = 'sha256'\n\ndef rename_file_with_hash(filepath, new_dir):\n    \"\"\"\n    Rename a file from `filepath` to its hash value and move it to `new_dir`.\n    After renaming and moving, read the file and return its content as a pandas DataFrame.\n    \n    Parameters:\n    filepath (str): The original file path.\n    new_dir (str): The new directory.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the file's content.\n    \n    Requirements:\n    - os\n    - hashlib\n    - csv\n    - pandas\n\n    Example:\n    >>> rename_file_with_hash('data.csv', '/path/to/new_dir')\n    \"\"\"\n    # Calculate the hash of the file\n    file_hash = hashlib.sha256(filepath.encode()).hexdigest()\n    \n    # Construct the new file path\n    newfilepath = os.path.join(new_dir, file_hash + '.csv')\n    \n    # Rename and move the file\n    os.rename(filepath, newfilepath)\n    \n    # Read the file into a pandas DataFrame\n    df = pd.read_csv(newfilepath)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, csv, pandas, numpy, matplotlib.pyplot\n```python\nimport os\nimport csv\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nDATA_DIR = '/path/to/data'\n\ndef rename_file_and_analyze_data(old_name, new_name):\n    \"\"\"\n    Rename a file from `old_name` to `new_name`, read the file,\n    convert its content into a pandas DataFrame, and perform a statistical analysis of the data.\n    \n    Parameters:\n    old_name (str): The old file name.\n    new_name (str): The new file name.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the file's content.\n    \n    Requirements:\n    - os\n    - csv\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> rename_file_and_analyze_data('old_data.csv', 'new_data.csv')\n    \"\"\"\n    # Rename the file\n    old_filepath = os.path.join(DATA_DIR, old_name)\n    new_filepath = os.path.join(DATA_DIR, new_name)\n    os.rename(old_filepath, new_filepath)\n    \n    # Read the file into a pandas DataFrame\n    df = pd.read_csv(new_filepath)\n    \n    # Perform a statistical analysis\n    mean = df.mean()\n    median = df.median()\n    std = df.std()\n    \n    # Plot a histogram of the data\n    df.hist(figsize=(10, 8))\n    plt.show()\n\n    return df, mean, median, std\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, shutil, csv, pandas, sqlite3\n```python\nimport os\nimport shutil\nimport csv\nimport pandas as pd\nimport sqlite3\n\n# Constants\nDB_PATH = '/path/to/database.db'\n\ndef rename_file_and_import_to_db(filepath, newfilepath):\n    \"\"\"\n    Rename a file from `filepath` to `newfilepath`, read the file,\n    and import its content into a SQLite database.\n    \n    Parameters:\n    filepath (str): The original file path.\n    newfilepath (str): The new file path.\n\n    Returns:\n    None\n    \n    Requirements:\n    - os\n    - shutil\n    - csv\n    - pandas\n    - sqlite3\n\n    Example:\n    >>> rename_file_and_import_to_db('data.csv', 'new_data.csv')\n    \"\"\"\n    # Rename the file\n    os.rename(filepath, newfilepath)\n    \n    # Read the file into a pandas DataFrame\n    df = pd.read_csv(newfilepath)\n    \n    # Connect to the SQLite database\n    conn = sqlite3.connect(DB_PATH)\n    \n    # Import the DataFrame into the SQLite database\n    df.to_sql('data', conn, if_exists='append', index=False)\n    \n    # Close the database connection\n    conn.close()\n```\n"}
{"seed": "{\"task_id\": 249773, \"prompt\": \"def f_249773(a):\\n\\t\", \"suffix\": \"\\n\\treturn nums\", \"canonical_solution\": \"\\n\\tr = range(min(a), max(a) + 1)\\n\\tnums = sorted(list(set(r) - set(a)))\\n\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([1, 3, 5]) == ([2, 4])\\n\"], \"entry_point\": \"f_249773\", \"intent\": \"\\u041d\\u0430\\u0439\\u0442\\u0438 \\u043f\\u0440\\u043e\\u043f\\u0443\\u0449\\u0435\\u043d\\u043d\\u044b\\u0435 \\u0447\\u0438\\u0441\\u043b\\u0430 `nums`  \\u0432 \\u0441\\u043f\\u0438\\u0441\\u043a\\u0435 `a`\", \"library\": []}", "generation": "Scenario 1:\npandas, numpy, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Sample data\n# df = pd.DataFrame(\n#     {\n#         'A': [1, 2, 3, 5, 7],\n#         'B': [3, 4, 6, 8, 10],\n#         'C': [2, 4, 5, 7, 9],\n#         'D': [1, 3, 4, 6, 8]\n#     }\n# )\n\ndef find_missing_data_in_df(df):\n    \"\"\"\n    Find missing values in a pandas DataFrame and visualize the data with a bar chart.\n    \n    Parameters:\n    df (DataFrame): A pandas DataFrame.\n    \n    Returns:\n    dict: A dictionary with column names as keys and missing values as values.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = pd.DataFrame({'A': [1, 2, np.nan, 5, 7], 'B': [3, np.nan, 6, 8, 10]})\n    >>> find_missing_data_in_df(df)\n    \"\"\"\n    missing_data = {}\n\n    for col in df.columns:\n        missing_values = sorted(list(set(range(df[col].min(), df[col].max() + 1)) - set(df[col])))\n        missing_data[col] = missing_values\n\n    plt.bar(missing_data.keys(), [len(v) for v in missing_data.values()])\n    plt.xlabel('Columns')\n    plt.ylabel('Number of missing values')\n    plt.show()\n\n    return missing_data\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, matplotlib, random\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nCATEGORIES = ['A', 'B', 'C', 'D', 'E']\n\n# Sample data\n# df = pd.DataFrame(\n#     {\n#         'Category': ['A', 'B', 'C', 'D', 'E'],\n#         'Values': [1, 3, 5, 7, 9]\n#     }\n# )\n\ndef find_and_visualize_missing_categories(df):\n    \"\"\"\n    Find missing categories in a pandas DataFrame and visualize the data with a pie chart.\n    \n    Parameters:\n    df (DataFrame): A pandas DataFrame.\n    \n    Returns:\n    list: A list of missing categories.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - random\n    \n    Example:\n    >>> df = pd.DataFrame({'Category': ['A', 'B', 'C'], 'Values': [1, 3, 5]})\n    >>> find_and_visualize_missing_categories(df)\n    \"\"\"\n    missing_categories = sorted(list(set(CATEGORIES) - set(df['Category'])))\n\n    present_categories = df['Category'].value_counts()\n    missing_categories_count = [randint(1, 10) for _ in range(len(missing_categories))]\n\n    plt.pie(present_categories.tolist() + missing_categories_count, labels=present_categories.index.tolist() + missing_categories, autopct='%1.1f%%')\n    plt.show()\n\n    return missing_categories\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, seaborn, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef find_and_plot_missing_values(df):\n    \"\"\"\n    Find missing values in a pandas DataFrame and plot the data with a heatmap.\n    \n    Parameters:\n    df (DataFrame): A pandas DataFrame.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with missing values.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - seaborn\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = pd.DataFrame({'A': [1, 2, np.nan, 4, 5], 'B': [np.nan, 2, 3, 4, 5]})\n    >>> find_and_plot_missing_values(df)\n    \"\"\"\n    missing_data = pd.DataFrame(index=COLUMNS)\n\n    for col in COLUMNS:\n        if col in df.columns:\n            missing_data.loc[col, 'Missing Values'] = df[col].isnull().sum()\n        else:\n            missing_data.loc[col, 'Missing Values'] = np.nan\n\n    plt.figure(figsize=(10, 6))\n    sns.heatmap(missing_data, annot=True, cmap='coolwarm')\n    plt.show()\n\n    return missing_data\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, random, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nLETTERS = ['A', 'B', 'C', 'D', 'E']\n\ndef find_missing_letters_and_plot(df):\n    \"\"\"\n    Find missing letters in a pandas DataFrame and plot the data with a histogram.\n    \n    Parameters:\n    df (DataFrame): A pandas DataFrame.\n    \n    Returns:\n    list: A list of missing letters.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = pd.DataFrame({'Letters': ['A', 'B', 'C', 'D', 'E'], 'Values': [1, 3, 5, 7, 9]})\n    >>> find_missing_letters_and_plot(df)\n    \"\"\"\n    missing_letters = sorted(list(set(LETTERS) - set(df['Letters'])))\n\n    present_letters = df['Letters'].value_counts()\n    missing_letters_count = [randint(1, 10) for _ in range(len(missing_letters))]\n\n    plt.hist([present_letters.tolist() + missing_letters_count], bins=len(LETTERS), alpha=0.5)\n    plt.show()\n\n    return missing_letters\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef find_and_plot_missing_columns(df):\n    \"\"\"\n    Find missing columns in a pandas DataFrame and plot the data with a bar chart.\n    \n    Parameters:\n    df (DataFrame): A pandas DataFrame.\n    \n    Returns:\n    list: A list of missing columns.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = pd.DataFrame({'A': [1, 2, 3, 4, 5], 'B': [6, 7, 8, 9, 10]})\n    >>> find_and_plot_missing_columns(df)\n    \"\"\"\n    missing_columns = sorted(list(set(COLUMNS) - set(df.columns)))\n\n    present_columns = df.count()\n    missing_columns_count = [np.nan for _ in range(len(missing_columns))]\n\n    plt.bar(present_columns.index.tolist() + missing_columns, present_columns.tolist() + missing_columns_count)\n    plt.show()\n\n    return missing_columns\n```\n`GPT_ODEX_BREAK`"}
{"seed": "{\"task_id\": 122711, \"prompt\": \"def f_122711(filepath):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"subprocess.check_output(['g++', filepath])\", \"test_start\": \"\\nimport subprocess\\nfrom unittest.mock import Mock\\n\\ndef check(candidate):\", \"test\": [\"\\n    samp_output = '1'\\n    subprocess.check_output = Mock(return_value = samp_output)\\n    assert candidate('a.cpp') == samp_output\\n\"], \"entry_point\": \"f_122711\", \"intent\": \"\\u0421\\u043a\\u043e\\u043c\\u043f\\u0438\\u043b\\u0438\\u0440\\u043e\\u0432\\u0430\\u0442\\u044c c++ \\u0444\\u0430\\u0439\\u043b `filepath`\", \"library\": [\"subprocess\"]}", "generation": "Scenario 1:\nsubprocess, os, sys, pathlib\n```python\nimport subprocess\nimport os\nimport sys\nfrom pathlib import Path\n\ndef compile_and_run(filepath, input_data):\n    \"\"\"\n    Compile a C++ file and run the executable with provided input data.\n    The function returns the output of the executable program.\n\n    Parameters:\n    filepath (str): The path of the C++ file.\n    input_data (str): The input data for the executable program.\n\n    Returns:\n    str: The output of the executable program.\n\n    Requirements:\n    - subprocess\n    - os\n    - sys\n    - pathlib.Path\n\n    Example:\n    >>> compile_and_run('a.cpp', '5')\n    '25\\n'\n    \"\"\"\n    # Compile the C++ file\n    subprocess.check_call(['g++', filepath, '-o', 'a.out'])\n\n    # Run the executable\n    executable = Path('./a.out')\n    output = subprocess.check_output([str(executable), input_data], encoding='utf-8')\n\n    return output\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nsubprocess, os, sys, shutil\n```python\nimport subprocess\nimport os\nimport sys\nimport shutil\n\nOUTPUT_DIR = 'compiled_files'\n\ndef compile_and_move(filepath):\n    \"\"\"\n    Compile a C++ file and move the executable to a specific output directory.\n\n    Parameters:\n    filepath (str): The path of the C++ file.\n\n    Returns:\n    str: The path of the moved executable file.\n\n    Requirements:\n    - subprocess\n    - os\n    - sys\n    - shutil\n\n    Example:\n    >>> compile_and_move('a.cpp')\n    'compiled_files/a.out'\n    \"\"\"\n    # Compile the C++ file\n    subprocess.check_call(['g++', filepath, '-o', 'a.out'])\n\n    # Move the executable to the output directory\n    if not os.path.exists(OUTPUT_DIR):\n        os.mkdir(OUTPUT_DIR)\n\n    shutil.move('a.out', OUTPUT_DIR)\n\n    return os.path.join(OUTPUT_DIR, 'a.out')\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nsubprocess, os, sys, time\n```python\nimport subprocess\nimport os\nimport sys\nimport time\n\ndef compile_and_time(filepath):\n    \"\"\"\n    Compile a C++ file and return the time taken for compilation.\n\n    Parameters:\n    filepath (str): The path of the C++ file.\n\n    Returns:\n    float: The time taken for compilation in seconds.\n\n    Requirements:\n    - subprocess\n    - os\n    - sys\n    - time\n\n    Example:\n    >>> compile_and_time('a.cpp')\n    0.123456\n    \"\"\"\n    # Record the start time\n    start_time = time.time()\n\n    # Compile the C++ file\n    subprocess.check_call(['g++', filepath, '-o', 'a.out'])\n\n    # Record the end time\n    end_time = time.time()\n\n    # Return the time taken for compilation\n    return end_time - start_time\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nsubprocess, os, sys, argparse\n```python\nimport subprocess\nimport os\nimport sys\nimport argparse\n\ndef compile_with_args():\n    \"\"\"\n    Compile a C++ file provided as a command line argument.\n\n    Returns:\n    str: The output of the compile command.\n\n    Requirements:\n    - subprocess\n    - os\n    - sys\n    - argparse\n\n    Example:\n    $ python compile_with_args.py --filepath a.cpp\n    \"\"\"\n    # Parse command line arguments\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--filepath', required=True)\n    args = parser.parse_args()\n\n    # Compile the C++ file\n    output = subprocess.check_output(['g++', args.filepath, '-o', 'a.out'], encoding='utf-8')\n\n    return output\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nsubprocess, os, sys, logging\n```python\nimport subprocess\nimport os\nimport sys\nimport logging\n\ndef compile_with_logging(filepath):\n    \"\"\"\n    Compile a C++ file and log the output of the compile command.\n\n    Parameters:\n    filepath (str): The path of the C++ file.\n\n    Returns:\n    None\n\n    Requirements:\n    - subprocess\n    - os\n    - sys\n    - logging\n\n    Example:\n    >>> compile_with_logging('a.cpp')\n    \"\"\"\n    # Configure logging\n    logging.basicConfig(level=logging.INFO)\n\n    # Try to compile the C++ file\n    try:\n        subprocess.check_call(['g++', filepath, '-o', 'a.out'])\n        logging.info('Successfully compiled %s', filepath)\n    except subprocess.CalledProcessError as e:\n        logging.error('Failed to compile %s: %s', filepath, e)\n```\n"}
{"seed": "{\"task_id\": 249444, \"prompt\": \"def f_249444(s, base_s):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"set(s.split()) & set(base_s.split())\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    s = 'hello hi'\\n    base_s = 'hello God'\\n    assert len(candidate(s, base_s)) > 0\\n\", \"\\n    s = 'hello hi'\\n    base_s = 'llo God'\\n    assert len(candidate(s, base_s)) == 0\\n\"], \"entry_point\": \"f_249444\", \"intent\": \"\\u041f\\u0440\\u043e\\u0432\\u0435\\u0440\\u0438\\u0442\\u044c \\u0441\\u043e\\u0434\\u0435\\u0440\\u0436\\u0438\\u0442 \\u043b\\u0438 \\u0441\\u0442\\u0440\\u043e\\u043a\\u0430 `s` \\u0441\\u043b\\u043e\\u0432\\u0430 \\u0438\\u0437 \\u0441\\u0442\\u0440\\u043e\\u043a\\u0438 `base_s`\", \"library\": []}", "generation": "Scenario 1: \ncollections, nltk, string\n```python\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nimport string\n\n# Constants\nSTOPWORDS = set(stopwords.words('english'))\n\ndef common_words_in_sentences(s1, s2):\n    \"\"\"\n    Find the common words from two sentences excluding the stop words and punctuation.\n\n    Parameters:\n    s1 (str): The first sentence.\n    s2 (str): The second sentence.\n\n    Returns:\n    list: The list of common words.\n\n    Requirements:\n    - collections\n    - nltk.corpus\n    - string\n\n    Example:\n    >>> s1 = 'Hello, how are you today?'\n    >>> s2 = 'Hello, I am fine. How about you?'\n    >>> common_words_in_sentences(s1, s2)\n    ['Hello', 'how', 'you']\n    \"\"\"\n    s1_words = [word for word in s1.translate(str.maketrans('', '', string.punctuation)).split() if word not in STOPWORDS]\n    s2_words = [word for word in s2.translate(str.maketrans('', '', string.punctuation)).split() if word not in STOPWORDS]\n\n    common_words = list((Counter(s1_words) & Counter(s2_words)).elements())\n\n    return common_words\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \ncollections, re\n```python\nfrom collections import Counter\nimport re\n\ndef common_words_in_files(file1, file2):\n    \"\"\"\n    Find the common words from two text files.\n\n    Parameters:\n    file1 (str): The path of the first text file.\n    file2 (str): The path of the second text file.\n\n    Returns:\n    list: The list of common words.\n\n    Requirements:\n    - collections\n    - re\n\n    Example:\n    >>> file1 = 'text1.txt'\n    >>> file2 = 'text2.txt'\n    >>> common_words_in_files(file1, file2)\n    ['word1', 'word2', 'word3']\n    \"\"\"\n    with open(file1, 'r') as f1, open(file2, 'r') as f2:\n        file1_words = re.findall(r'\\b\\w+\\b', f1.read().lower())\n        file2_words = re.findall(r'\\b\\w+\\b', f2.read().lower())\n\n    common_words = list((Counter(file1_words) & Counter(file2_words)).elements())\n\n    return common_words\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \ncollections, nltk, string, pandas\n```python\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nimport string\nimport pandas as pd\n\n# Constants\nSTOPWORDS = set(stopwords.words('english'))\n\ndef common_words_in_dataframe(df1, df2, column):\n    \"\"\"\n    Find the common words from a specific column in two dataframes excluding the stop words and punctuation.\n\n    Parameters:\n    df1 (DataFrame): The first dataframe.\n    df2 (DataFrame): The second dataframe.\n    column (str): The column name.\n\n    Returns:\n    list: The list of common words.\n\n    Requirements:\n    - collections\n    - nltk.corpus\n    - string\n    - pandas\n\n    Example:\n    >>> df1 = pd.DataFrame({'text': ['Hello, how are you today?', 'I am fine.']})\n    >>> df2 = pd.DataFrame({'text': ['Hello, I am fine. How about you?', 'Good to hear.']})\n    >>> common_words_in_dataframe(df1, df2, 'text')\n    ['Hello', 'how', 'you', 'I', 'am', 'fine']\n    \"\"\"\n    df1_words = [word for sentence in df1[column] for word in sentence.translate(str.maketrans('', '', string.punctuation)).split() if word not in STOPWORDS]\n    df2_words = [word for sentence in df2[column] for word in sentence.translate(str.maketrans('', '', string.punctuation)).split() if word not in STOPWORDS]\n\n    common_words = list((Counter(df1_words) & Counter(df2_words)).elements())\n\n    return common_words\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \ncollections, nltk, string, json\n```python\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nimport string\nimport json\n\n# Constants\nSTOPWORDS = set(stopwords.words('english'))\n\ndef common_words_in_json(json1, json2, key):\n    \"\"\"\n    Find the common words from a specific key in two json objects excluding the stop words and punctuation.\n\n    Parameters:\n    json1 (str): The first json object.\n    json2 (str): The second json object.\n    key (str): The key name.\n\n    Returns:\n    list: The list of common words.\n\n    Requirements:\n    - collections\n    - nltk.corpus\n    - string\n    - json\n\n    Example:\n    >>> json1 = '{\"text\": \"Hello, how are you today?\"}'\n    >>> json2 = '{\"text\": \"Hello, I am fine. How about you?\"}'\n    >>> common_words_in_json(json1, json2, 'text')\n    ['Hello', 'how', 'you']\n    \"\"\"\n    data1 = json.loads(json1)\n    data2 = json.loads(json2)\n\n    data1_words = [word for word in data1[key].translate(str.maketrans('', '', string.punctuation)).split() if word not in STOPWORDS]\n    data2_words = [word for word in data2[key].translate(str.maketrans('', '', string.punctuation)).split() if word not in STOPWORDS]\n\n    common_words = list((Counter(data1_words) & Counter(data2_words)).elements())\n\n    return common_words\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \ncollections, nltk, string, csv\n```python\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nimport string\nimport csv\n\n# Constants\nSTOPWORDS = set(stopwords.words('english'))\n\ndef common_words_in_csv(csv_file1, csv_file2, column):\n    \"\"\"\n    Find the common words from a specific column in two csv files excluding the stop words and punctuation.\n\n    Parameters:\n    csv_file1 (str): The path of the first csv file.\n    csv_file2 (str): The path of the second csv file.\n    column (str): The column name.\n\n    Returns:\n    list: The list of common words.\n\n    Requirements:\n    - collections\n    - nltk.corpus\n    - string\n    - csv\n\n    Example:\n    >>> csv_file1 = 'text1.csv'\n    >>> csv_file2 = 'text2.csv'\n    >>> common_words_in_csv(csv_file1, csv_file2, 'text')\n    ['word1', 'word2', 'word3']\n    \"\"\"\n    with open(csv_file1, 'r') as f1, open(csv_file2, 'r') as f2:\n        csv_reader1 = csv.DictReader(f1)\n        csv_reader2 = csv.DictReader(f2)\n\n        csv_file1_words = [word for row in csv_reader1 for word in row[column].translate(str.maketrans('', '', string.punctuation)).split() if word not in STOPWORDS]\n        csv_file2_words = [word for row in csv_reader2 for word in row[column].translate(str.maketrans('', '', string.punctuation)).split() if word not in STOPWORDS]\n\n    common_words = list((Counter(csv_file1_words) & Counter(csv_file2_words)).elements())\n\n    return common_words\n```\n"}
{"seed": "{\"task_id\": 210480, \"prompt\": \"def f_210480(filepath):\\n\\t\", \"suffix\": \"\\n\\treturn line_counts\", \"canonical_solution\": \"\\n\\twith open(filepath) as f:\\n\\t\\tlines = f.readlines()\\n\\t\\tline_counts = collections.Counter(lines)\\n\", \"test_start\": \"\\nimport collections\\n\\ndef check(candidate):\", \"test\": [\"\\n    lines = ['a\\\\n', 'b\\\\n', 'c\\\\n']\\n    filepath = 'a.txt'\\n    with open (filepath, 'a') as f:\\n        for line in lines:\\n            f.write(line)\\n    \\n    line_counts = candidate(filepath)\\n    for i in range (0, len(lines)):\\n        assert line_counts[lines[0]] == 1\\n\"], \"entry_point\": \"f_210480\", \"intent\": \"\\u041f\\u043e\\u0441\\u0447\\u0438\\u0442\\u0430\\u0442\\u044c \\u0441\\u0442\\u0440\\u043e\\u043a\\u0438 \\u0432 \\u0444\\u0430\\u0439\\u043b\\u0435 `filepath` \\u0432 \\u0441\\u043b\\u043e\\u0432\\u0430\\u0440\\u044c line_counts\", \"library\": [\"collections\"]}", "generation": "Scenario 1:\ncollections, json, os, pandas, matplotlib\n```python\nimport collections\nimport json\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nEXTENSIONS = ['.json', '.txt', '.csv']\n\ndef count_lines_directory(directory_path):\n    \"\"\"\n    Count the lines in all files in a directory with specific extensions and return a dataframe with \n    file names and line counts. Also plot a bar graph using matplotlib showing the line counts.\n\n    Parameters:\n    directory_path (str): The path to the directory.\n\n    Returns:\n    DataFrame: A pandas DataFrame with file names and line counts.\n\n    Requirements:\n    - collections\n    - json\n    - os\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> dir_path = '/path/to/directory'\n    >>> df = count_lines_directory(dir_path)\n    >>> print(df)\n    >>> df.plot(x='File Name', y='Line Count', kind='bar')\n    \"\"\"\n    line_counts = []\n\n    for filename in os.listdir(directory_path):\n        if any(filename.endswith(ext) for ext in EXTENSIONS):\n            with open(os.path.join(directory_path, filename)) as f:\n                lines = f.readlines()\n                line_count = len(lines)\n                line_counts.append([filename, line_count])\n\n    df = pd.DataFrame(line_counts, columns=['File Name', 'Line Count'])\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncollections, os, shutil, glob, csv\n```python\nimport collections\nimport os\nimport shutil\nimport glob\nimport csv\n\n# Constants\nEXTENSIONS = ['.txt', '.csv']\n\ndef move_and_count_lines(source_directory, destination_directory):\n    \"\"\"\n    Move files with specific extensions from source directory to destination directory and count the lines in each file.\n\n    Parameters:\n    source_directory (str): The source directory.\n    destination_directory (str): The destination directory.\n\n    Returns:\n    dict: A dictionary with file names as keys and line counts as values.\n\n    Requirements:\n    - collections\n    - os\n    - shutil\n    - glob\n    - csv\n\n    Example:\n    >>> src_dir = '/path/to/source/directory'\n    >>> dest_dir = '/path/to/destination/directory'\n    >>> line_counts = move_and_count_lines(src_dir, dest_dir)\n    >>> print(line_counts)\n    \"\"\"\n    line_counts = {}\n\n    for filename in os.listdir(source_directory):\n        if any(filename.endswith(ext) for ext in EXTENSIONS):\n            shutil.move(os.path.join(source_directory, filename),\n                        os.path.join(destination_directory, filename))\n\n    for filename in glob.glob(os.path.join(destination_directory, '*')):\n        with open(filename) as f:\n            lines = f.readlines()\n            line_counts[os.path.basename(filename)] = len(lines)\n\n    return line_counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ncollections, json, os, nltk, matplotlib\n```python\nimport collections\nimport json\nimport os\nfrom nltk.tokenize import word_tokenize\nimport matplotlib.pyplot as plt\n\n# Constants\nEXTENSIONS = ['.json', '.txt']\n\ndef count_words_directory(directory_path):\n    \"\"\"\n    Count the words in all files in a directory with specific extensions and plot a histogram \n    showing the top 10 most common words using matplotlib.\n\n    Parameters:\n    directory_path (str): The path to the directory.\n\n    Returns:\n    list: A list of tuples with the top 10 most common words and their counts.\n\n    Requirements:\n    - collections\n    - json\n    - os\n    - nltk.tokenize\n    - matplotlib.pyplot\n\n    Example:\n    >>> dir_path = '/path/to/directory'\n    >>> word_counts = count_words_directory(dir_path)\n    >>> print(word_counts)\n    \"\"\"\n    word_counts = collections.Counter()\n    for filename in os.listdir(directory_path):\n        if any(filename.endswith(ext) for ext in EXTENSIONS):\n            with open(os.path.join(directory_path, filename)) as f:\n                text = f.read()\n                words = word_tokenize(text)\n                word_counts.update(words)\n\n    top_10_words = word_counts.most_common(10)\n\n    words, counts = zip(*top_10_words)\n    plt.bar(words, counts)\n    plt.show()\n\n    return top_10_words\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ncollections, os, glob, csv, pandas\n```python\nimport collections\nimport os\nimport glob\nimport csv\nimport pandas as pd\n\n# Constants\nEXTENSIONS = ['.csv']\n\ndef count_columns_directory(directory_path):\n    \"\"\"\n    Count the columns in all CSV files in a directory and return a dataframe with file names and column counts.\n\n    Parameters:\n    directory_path (str): The path to the directory.\n\n    Returns:\n    DataFrame: A pandas DataFrame with file names and column counts.\n\n    Requirements:\n    - collections\n    - os\n    - glob\n    - csv\n    - pandas\n\n    Example:\n    >>> dir_path = '/path/to/directory'\n    >>> df = count_columns_directory(dir_path)\n    >>> print(df)\n    \"\"\"\n    column_counts = []\n\n    for filename in glob.glob(os.path.join(directory_path, '*')):\n        if filename.endswith('.csv'):\n            df = pd.read_csv(filename)\n            column_count = len(df.columns)\n            column_counts.append([os.path.basename(filename), column_count])\n\n    df = pd.DataFrame(column_counts, columns=['File Name', 'Column Count'])\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\ncollections, os, shutil, glob, csv, matplotlib\n```python\nimport collections\nimport os\nimport shutil\nimport glob\nimport csv\nimport matplotlib.pyplot as plt\n\n# Constants\nEXTENSIONS = ['.csv']\n\ndef move_and_count_columns(source_directory, destination_directory):\n    \"\"\"\n    Move CSV files from source directory to destination directory, count the columns in each file, \n    and plot a histogram showing the column counts using matplotlib.\n\n    Parameters:\n    source_directory (str): The source directory.\n    destination_directory (str): The destination directory.\n\n    Returns:\n    dict: A dictionary with file names as keys and column counts as values.\n\n    Requirements:\n    - collections\n    - os\n    - shutil\n    - glob\n    - csv\n    - matplotlib.pyplot\n\n    Example:\n    >>> src_dir = '/path/to/source/directory'\n    >>> dest_dir = '/path/to/destination/directory'\n    >>> column_counts = move_and_count_columns(src_dir, dest_dir)\n    >>> print(column_counts)\n    \"\"\"\n    column_counts = {}\n\n    for filename in os.listdir(source_directory):\n        if filename.endswith('.csv'):\n            shutil.move(os.path.join(source_directory, filename),\n                        os.path.join(destination_directory, filename))\n\n    for filename in glob.glob(os.path.join(destination_directory, '*')):\n        with open(filename) as f:\n            reader = csv.reader(f)\n            columns = next(reader)\n            column_counts[os.path.basename(filename)] = len(columns)\n\n    files, counts = zip(*column_counts.items())\n    plt.bar(files, counts)\n    plt.show()\n\n    return column_counts\n```"}
{"seed": "{\"task_id\": 51160, \"prompt\": \"def f_51160(f, n):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"sympy.integrate(f, (sympy.symbols('x'), n, sympy.oo))\", \"test_start\": \"\\nimport sympy\\n\\ndef check(candidate):\", \"test\": [\"\\n    x = sympy.symbols('x')\\n    f = ( x * x)\\n    n = 1\\n    assert  str(candidate(f, n)) == 'oo'\\n\"], \"entry_point\": \"f_51160\", \"intent\": \"\\u041f\\u043e\\u0441\\u0447\\u0438\\u0442\\u0430\\u0442\\u044c \\u043d\\u0435\\u0441\\u043e\\u0431\\u0441\\u0442\\u0432\\u0435\\u043d\\u043d\\u044b\\u0439 \\u0438\\u043d\\u0442\\u0435\\u0433\\u0440\\u0430\\u043b \\u0437\\u0430\\u0434\\u0430\\u043d\\u043d\\u044b\\u0439 \\u0444\\u0443\\u043d\\u043a\\u0446\\u0438\\u0435\\u0439 `f` \\u043e\\u0442 \\u0447\\u0438\\u0441\\u043b\\u0430 `n` \\u0434\\u043e \\u0431\\u0435\\u0441\\u043a\\u043e\\u043d\\u0435\\u0447\\u043d\\u043e\\u0441\\u0442\\u0438\", \"library\": [\"sympy\"]}", "generation": "Scenario 1:\nsympy, numpy, matplotlib\n```python\nimport sympy\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nX_SYMBOL = sympy.symbols('x')\n\ndef plot_integral(f, start, end, num_points=1000):\n    \"\"\"\n    Plot a function `f` and its definite integral over the interval `[start, end]`.\n\n    Parameters:\n    f (sympy.Function): The function to plot and integrate.\n    start (float): The start of the interval.\n    end (float): The end of the interval.\n    num_points (int, optional): The number of points to consider for the plot. Defaults to 1000.\n\n    Requirements:\n    - sympy\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> f = sympy.sin(X_SYMBOL)\n    >>> plot_integral(f, 0, 2*np.pi)\n    \"\"\"\n    x_values = np.linspace(start, end, num_points)\n    y_values = np.array([f.evalf(subs={X_SYMBOL: x}) for x in x_values])\n\n    integral = sympy.integrate(f, (X_SYMBOL, start, end))\n    integral_value = integral.evalf()\n\n    plt.figure(figsize=(10,6))\n    \n    plt.subplot(1, 2, 1)\n    plt.plot(x_values, y_values, label=str(f))\n    plt.fill_between(x_values, y_values, color='gray', alpha=0.5)\n    plt.legend()\n\n    plt.subplot(1, 2, 2)\n    plt.bar(0, integral_value, width=0.1)\n    plt.text(0, integral_value, f'{integral_value:.2f}', ha='center', va='bottom')\n\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nsympy, numpy, scipy.optimize\n```python\nimport sympy\nimport numpy as np\nfrom scipy.optimize import minimize\n\n# Constants\nX_SYMBOL = sympy.symbols('x')\n\ndef find_minimum_integral(f, start, end, num_points=1000):\n    \"\"\"\n    Find the minimum value of the definite integral of the function `f` over the interval `[start, end]` \n    by shifting the function horizontally.\n\n    Parameters:\n    f (sympy.Function): The function to integrate.\n    start (float): The start of the interval.\n    end (float): The end of the interval.\n    num_points (int, optional): The number of points to consider for the optimization. Defaults to 1000.\n\n    Returns:\n    float: The minimum integral value.\n\n    Requirements:\n    - sympy\n    - numpy\n    - scipy.optimize\n\n    Example:\n    >>> f = sympy.sin(X_SYMBOL)\n    >>> find_minimum_integral(f, 0, 2*np.pi)\n    \"\"\"\n    x_values = np.linspace(start, end, num_points)\n\n    def integral(shift):\n        shifted_values = x_values + shift\n        y_values = np.array([f.evalf(subs={X_SYMBOL: x}) for x in shifted_values])\n        return np.trapz(y_values, shifted_values)\n\n    res = minimize(integral, 0.0)\n\n    return res.fun\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nsympy, numpy, matplotlib, scipy.integrate\n```python\nimport sympy\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.integrate import quad\n\n# Constants\nX_SYMBOL = sympy.symbols('x')\n\ndef compare_integration_methods(f, start, end):\n    \"\"\"\n    Compare the results of numerical integration using the Simpson's rule and \n    symbolic integration for a function `f` over the interval `[start, end]`.\n\n    Parameters:\n    f (sympy.Function): The function to integrate.\n    start (float): The start of the interval.\n    end (float): The end of the interval.\n\n    Requirements:\n    - sympy\n    - numpy\n    - matplotlib.pyplot\n    - scipy.integrate\n\n    Example:\n    >>> f = sympy.sin(X_SYMBOL)\n    >>> compare_integration_methods(f, 0, 2*np.pi)\n    \"\"\"\n    # Symbolic integration\n    integral_symbolic = sympy.integrate(f, (X_SYMBOL, start, end))\n    print(f'Symbolic integration result: {integral_symbolic}')\n\n    # Numeric integration\n    f_numeric = sympy.lambdify(X_SYMBOL, f, 'numpy')\n    integral_numeric, _ = quad(f_numeric, start, end)\n    print(f'Numeric integration result: {integral_numeric}')\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nsympy, numpy, matplotlib\n```python\nimport sympy\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nX_SYMBOL = sympy.symbols('x')\n\ndef plot_derivative_integral(f, start, end, num_points=1000):\n    \"\"\"\n    Plot a function `f`, its derivative and integral over the interval `[start, end]`.\n\n    Parameters:\n    f (sympy.Function): The function to plot, differentiate and integrate.\n    start (float): The start of the interval.\n    end (float): The end of the interval.\n    num_points (int, optional): The number of points to consider for the plot. Defaults to 1000.\n\n    Requirements:\n    - sympy\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> f = sympy.sin(X_SYMBOL)\n    >>> plot_derivative_integral(f, 0, 2*np.pi)\n    \"\"\"\n    x_values = np.linspace(start, end, num_points)\n\n    # Function\n    y_values = np.array([f.evalf(subs={X_SYMBOL: x}) for x in x_values])\n\n    # Derivative\n    derivative = sympy.diff(f, X_SYMBOL)\n    derivative_values = np.array([derivative.evalf(subs={X_SYMBOL: x}) for x in x_values])\n\n    # Integral\n    integral = sympy.integrate(f, X_SYMBOL)\n    integral_values = np.array([integral.evalf(subs={X_SYMBOL: x}) for x in x_values])\n\n    plt.figure(figsize=(10,6))\n    plt.plot(x_values, y_values, label='Function')\n    plt.plot(x_values, derivative_values, label='Derivative')\n    plt.plot(x_values, integral_values, label='Integral')\n    plt.legend()\n\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nsympy, numpy, scipy.optimize\n```python\nimport sympy\nimport numpy as np\nfrom scipy.optimize import minimize\n\n# Constants\nX_SYMBOL = sympy.symbols('x')\n\ndef optimize_integral(f, start, end, num_points=1000):\n    \"\"\"\n    Optimize the definite integral of a function `f` over the interval `[start, end]` \n    by scaling and shifting the function.\n\n    Parameters:\n    f (sympy.Function): The function to integrate.\n    start (float): The start of the interval.\n    end (float): The end of the interval.\n    num_points (int, optional): The number of points to consider for the optimization. Defaults to 1000.\n\n    Returns:\n    float: The minimum integral value.\n\n    Requirements:\n    - sympy\n    - numpy\n    - scipy.optimize\n\n    Example:\n    >>> f = sympy.sin(X_SYMBOL)\n    >>> optimize_integral(f, 0, 2*np.pi)\n    \"\"\"\n    x_values = np.linspace(start, end, num_points)\n\n    def integral(params):\n        scale, shift = params\n        shifted_values = scale * x_values + shift\n        y_values = np.array([f.evalf(subs={X_SYMBOL: x}) for x in shifted_values])\n        return np.trapz(y_values, shifted_values)\n\n    res = minimize(integral, [1.0, 0.0])\n\n    return res.fun\n```"}
{"seed": "{\"task_id\": 560, \"prompt\": \"def f_560(dict1, dict2):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"for key in dict2:\\n\\n\\t\\tif key in dict1:\\n\\n\\t\\t\\tdel dict1[key]\\n\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    x = {'a': 1, 'b': 2}\\n    y = {'b': 1, 'c': 2}\\n    candidate(x, y)\\n    assert x == {'a': 1}\\n\"], \"entry_point\": \"f_560\", \"intent\": \"\\u0423\\u0434\\u0430\\u043b\\u0438\\u0442\\u044c \\u0438\\u0437 \\u0441\\u043b\\u043e\\u0432\\u0430\\u0440\\u044f `dict1` \\u043a\\u043b\\u044e\\u0447\\u0438 \\u043a\\u043e\\u0442\\u043e\\u0440\\u044b\\u0435 \\u0435\\u0441\\u0442\\u044c \\u0432 \\u0441\\u043b\\u043e\\u0432\\u0430\\u0440\\u0435 `dict2`\", \"library\": []}", "generation": "Scenario 1: \ncollections, itertools, operator\n```python\nfrom collections import Counter\nfrom itertools import chain\nfrom operator import itemgetter\n\n# Constants\nDEFAULT_COUNT = 0\n\ndef get_common_elements(dict1, dict2):\n    \"\"\"\n    Get the common keys between two dictionaries and their combined total count.\n    \n    Parameters:\n    dict1 (dict): The first dictionary.\n    dict2 (dict): The second dictionary.\n    \n    Returns:\n    dict: A dictionary with common keys and their total count.\n    \n    Requirements:\n    - collections.Counter\n    - itertools.chain\n    - operator.itemgetter\n    \n    Example:\n    >>> dict1 = {'apple': 10, 'banana': 20, 'orange': 30}\n    >>> dict2 = {'banana': 15, 'orange': 25, 'kiwi': 35}\n    >>> get_common_elements(dict1, dict2)\n    \"\"\"\n    dict1_counter = Counter(dict1)\n    dict2_counter = Counter(dict2)\n    \n    common_keys = set(dict1.keys()) & set(dict2.keys())\n    \n    result = {}\n    for key in common_keys:\n        result[key] = dict1_counter.get(key, DEFAULT_COUNT) + dict2_counter.get(key, DEFAULT_COUNT)\n        \n    return result\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nnumpy, pandas, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef plot_dict_histogram(dict1, dict2):\n    \"\"\"\n    Plot a histogram of the values in two dictionaries.\n    \n    Parameters:\n    dict1 (dict): The first dictionary.\n    dict2 (dict): The second dictionary.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    \n    Example:\n    >>> dict1 = {'one': 1, 'two': 2, 'three': 3}\n    >>> dict2 = {'four': 4, 'five': 5, 'six': 6}\n    >>> plot_dict_histogram(dict1, dict2)\n    \"\"\"\n    data = pd.DataFrame([dict1, dict2]).T\n    data.columns = ['dict1', 'dict2']\n    \n    plt.figure(figsize=(10,5))\n    data['dict1'].plot(kind='bar', color='r', alpha=0.5, align='center')\n    data['dict2'].plot(kind='bar', color='b', alpha=0.5, align='edge')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \njson, os\n```python\nimport json\nimport os\n\ndef save_dict_to_file(dict1, dict2, file_path):\n    \"\"\"\n    Save two dictionaries to a JSON file. If the file already exists, it will be overwritten.\n    \n    Parameters:\n    dict1 (dict): The first dictionary.\n    dict2 (dict): The second dictionary.\n    file_path (str): The path to the file where the dictionaries should be saved.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - json\n    - os\n    \n    Example:\n    >>> dict1 = {'one': 1, 'two': 2, 'three': 3}\n    >>> dict2 = {'four': 4, 'five': 5, 'six': 6}\n    >>> save_dict_to_file(dict1, dict2, './data.json')\n    \"\"\"\n    data = {'dict1': dict1, 'dict2': dict2}\n    \n    with open(file_path, 'w') as f:\n        json.dump(data, f)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \ncollections, operator\n```python\nfrom collections import Counter\nfrom operator import itemgetter\n\ndef merge_dicts_and_sort(dict1, dict2):\n    \"\"\"\n    Merge two dictionaries into one and sort it by value in descending order.\n    \n    Parameters:\n    dict1 (dict): The first dictionary.\n    dict2 (dict): The second dictionary.\n    \n    Returns:\n    list: A list of tuples where each tuple is a key-value pair in the dictionary.\n    \n    Requirements:\n    - collections.Counter\n    - operator.itemgetter\n    \n    Example:\n    >>> dict1 = {'one': 1, 'two': 2, 'three': 3}\n    >>> dict2 = {'one': 4, 'five': 5, 'six': 6}\n    >>> merge_dicts_and_sort(dict1, dict2)\n    \"\"\"\n    merged_counter = Counter(dict1) + Counter(dict2)\n    sorted_dict = sorted(merged_counter.items(), key=itemgetter(1), reverse=True)\n    \n    return sorted_dict\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnumpy, scipy.stats\n```python\nimport numpy as np\nfrom scipy import stats\n\ndef perform_chi_square_test(dict1, dict2):\n    \"\"\"\n    Perform a Chi-Square test of independence between the values of two dictionaries.\n    \n    Parameters:\n    dict1 (dict): The first dictionary.\n    dict2 (dict): The second dictionary.\n    \n    Returns:\n    tuple: A tuple where the first element is the Chi-Square statistic and the second element is the p-value.\n    \n    Requirements:\n    - numpy\n    - scipy.stats\n    \n    Example:\n    >>> dict1 = {'one': 1, 'two': 2, 'three': 3}\n    >>> dict2 = {'one': 4, 'five': 5, 'six': 6}\n    >>> perform_chi_square_test(dict1, dict2)\n    \"\"\"\n    common_keys = set(dict1.keys()) & set(dict2.keys())\n    if not common_keys:\n        return None\n    \n    dict1_values = np.array([dict1[key] for key in common_keys])\n    dict2_values = np.array([dict2[key] for key in common_keys])\n    \n    chi2, p = stats.chisquare(dict1_values, dict2_values)\n    \n    return chi2, p\n```"}
{"seed": "{\"task_id\": 18892, \"prompt\": \"def f_18892(filepath):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"open(filepath,\\\"w\\\", encoding=\\\"utf-8\\\")\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    f = candidate(\\\"tmp.txt\\\")\\n    assert f.encoding == \\\"utf-8\\\"\\n    assert f.name == \\\"tmp.txt\\\"\\n\", \"\\n    f = candidate(\\\"sample_100.pkl\\\")\\n    assert f.encoding == \\\"utf-8\\\"\\n    assert f.name == \\\"sample_100.pkl\\\"\\n\"], \"entry_point\": \"f_18892\", \"intent\": \"\\u0421\\u043e\\u0437\\u0434\\u0430\\u0442\\u044c \\u0444\\u0430\\u0439\\u043b `filepath` \\u0441 \\u043a\\u043e\\u0434\\u0438\\u0440\\u043e\\u0432\\u043a\\u043e\\u0439 utf-8\", \"library\": []}", "generation": "Scenario 1:\nos, csv, collections\n```python\nimport os\nimport csv\nfrom collections import Counter\n\n# Constants\nENCODING = 'utf-8'\n\ndef count_occurrences(filepath):\n    \"\"\"\n    Count the occurrences of each word in a UTF-8 encoded text file.\n\n    Parameters:\n    filepath (str): The path of the text file.\n\n    Returns:\n    dict: A dictionary where keys are the words and values are the counts.\n\n    Requirements:\n    - os\n    - csv\n    - collections.Counter\n\n    Example:\n    >>> count_occurrences('words.txt')\n    \"\"\"\n    with open(filepath, 'r', encoding=ENCODING) as file:\n        text = file.read().lower().split()\n        word_count = dict(Counter(text))\n\n    return word_count\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, json, glob\n```python\nimport os\nimport json\nimport glob\n\n# Constants\nENCODING = 'utf-8'\n\ndef merge_json_files(directory):\n    \"\"\"\n    Merge all JSON files in a directory into a single dictionary.\n\n    Parameters:\n    directory (str): The directory containing the JSON files.\n\n    Returns:\n    dict: A dictionary containing the merged data.\n\n    Requirements:\n    - os\n    - json\n    - glob\n\n    Example:\n    >>> merge_json_files('./json_files/')\n    \"\"\"\n    merged_dict = {}\n\n    for file_path in glob.glob(os.path.join(directory, '*.json')):\n        with open(file_path, 'r', encoding=ENCODING) as file:\n            data = json.load(file)\n            merged_dict.update(data)\n\n    return merged_dict\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, csv, matplotlib\n```python\nimport os\nimport csv\nimport matplotlib.pyplot as plt\n\n# Constants\nENCODING = 'utf-8'\n\ndef plot_csv_data(filepath):\n    \"\"\"\n    Plot the data from a CSV file.\n\n    Parameters:\n    filepath (str): The path of the CSV file.\n\n    Requirements:\n    - os\n    - csv\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_csv_data('data.csv')\n    \"\"\"\n    x = []\n    y = []\n\n    with open(filepath, 'r', encoding=ENCODING) as file:\n        reader = csv.reader(file)\n        next(reader)  # Skip the header\n\n        for row in reader:\n            x.append(float(row[0]))\n            y.append(float(row[1]))\n\n    plt.plot(x, y)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, pandas, seaborn\n```python\nimport os\nimport pandas as pd\nimport seaborn as sns\n\n# Constants\nENCODING = 'utf-8'\n\ndef visualize_data(filepath):\n    \"\"\"\n    Visualize the data from a CSV file using a pairplot.\n\n    Parameters:\n    filepath (str): The path of the CSV file.\n\n    Requirements:\n    - os\n    - pandas\n    - seaborn\n\n    Example:\n    >>> visualize_data('data.csv')\n    \"\"\"\n    df = pd.read_csv(filepath, encoding=ENCODING)\n\n    sns.pairplot(df)\n    sns.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, json, matplotlib\n```python\nimport os\nimport json\nimport matplotlib.pyplot as plt\n\n# Constants\nENCODING = 'utf-8'\n\ndef plot_json_data(filepath):\n    \"\"\"\n    Plot the data from a JSON file.\n\n    Parameters:\n    filepath (str): The path of the JSON file.\n\n    Requirements:\n    - os\n    - json\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_json_data('data.json')\n    \"\"\"\n    with open(filepath, 'r', encoding=ENCODING) as file:\n        data = json.load(file)\n\n    x = list(data.keys())\n    y = list(data.values())\n\n    plt.bar(x, y)\n    plt.show()\n```"}
{"seed": "{\"task_id\": 242841, \"prompt\": \"def f_242841(obj, name):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"getattr(obj, name)()\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(\\\"hello\\\", \\\"encode\\\") == b'hello'\\n\", \"\\n    assert candidate(100, \\\"conjugate\\\") == 100\\n\", \"\\n    assert candidate([1,2,3], \\\"sort\\\") is None\\n\"], \"entry_point\": \"f_242841\", \"intent\": \"\\u0417\\u0430\\u043f\\u0443\\u0441\\u0442\\u0438\\u0442\\u044c \\u043c\\u0435\\u0442\\u043e\\u0434 c \\u0438\\u043c\\u0435\\u043d\\u0435\\u043c `name` \\u043e\\u0431\\u044a\\u0435\\u043a\\u0442\\u0430 `obj`\", \"library\": []}", "generation": "Scenario 1: \nos, sys, pathlib, subprocess\n```python\nimport os\nimport sys\nimport pathlib\nimport subprocess\n\n# Constants\nSCRIPT_EXTENSIONS = ['.py', '.sh', '.rb', '.pl', '.php', '.js']\n\ndef run_script_in_path(scripts_path, script_name):\n    \"\"\"\n    Find a script with the given name in the provided directory path and run it.\n    If multiple scripts with the same name but different extensions are found, run all of them sequentially.\n\n    Parameters:\n    scripts_path (str): The path to the directory containing the scripts.\n    script_name (str): The name of the script to run (without extension).\n\n    Returns:\n    list: A list of tuples, each containing the script's name and its exit code.\n\n    Requirements:\n    - os\n    - sys\n    - pathlib\n    - subprocess\n\n    Example:\n    >>> run_script_in_path('/path/to/scripts', 'my_script')\n    \"\"\"\n    script_path = pathlib.Path(scripts_path)\n    scripts = script_path.glob(f'{script_name}.*')\n\n    results = []\n    for script in scripts:\n        if script.suffix in SCRIPT_EXTENSIONS:\n            exit_code = subprocess.call([sys.executable, str(script)])\n            results.append((str(script), exit_code))\n\n    return results\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncsv, pandas, matplotlib\n```python\nimport csv\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nCSV_HEADER = ['Name', 'Score']\n\ndef generate_score_report(csv_file):\n    \"\"\"\n    Read a CSV file with student names and scores, generate a report showing the distribution of scores,\n    and plot a histogram of the scores.\n\n    Parameters:\n    csv_file (str): The path to the CSV file.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the scores report.\n\n    Requirements:\n    - csv\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> generate_score_report('/path/to/scores.csv')\n    \"\"\"\n    with open(csv_file, 'r') as f:\n        reader = csv.DictReader(f, fieldnames=CSV_HEADER)\n        scores_df = pd.DataFrame(reader)\n\n    scores_df['Score'] = scores_df['Score'].astype(int)\n\n    report = scores_df.describe()\n\n    scores_df['Score'].hist(bins=10)\n    plt.show()\n\n    return report\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, scipy.stats, matplotlib\n```python\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\n# Constants\nSAMPLE_SIZE = 100\n\ndef generate_normal_distribution(mean, std_dev):\n    \"\"\"\n    Generate a normal distribution with the given mean and standard deviation, plot its histogram and \n    calculate its skewness and kurtosis.\n\n    Parameters:\n    mean (float): The mean of the distribution.\n    std_dev (float): The standard deviation of the distribution.\n\n    Returns:\n    tuple: A tuple containing the skewness and kurtosis of the distribution.\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Example:\n    >>> generate_normal_distribution(0, 1)\n    \"\"\"\n    samples = np.random.normal(loc=mean, scale=std_dev, size=SAMPLE_SIZE)\n\n    skewness = stats.skew(samples)\n    kurtosis = stats.kurtosis(samples)\n\n    plt.hist(samples, bins='auto')\n    plt.show()\n\n    return skewness, kurtosis\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\njson, requests, pandas\n```python\nimport json\nimport requests\nimport pandas as pd\n\n# Constants\nAPI_URL = 'https://api.example.com/data'\n\ndef fetch_and_process_data(params):\n    \"\"\"\n    Fetch data from an API with the given parameters, convert the JSON response into a pandas DataFrame,\n    and perform some basic data processing.\n\n    Parameters:\n    params (dict): The parameters to pass to the API.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the processed data.\n\n    Requirements:\n    - json\n    - requests\n    - pandas\n\n    Example:\n    >>> fetch_and_process_data({'type': 'sales', 'year': 2021})\n    \"\"\"\n    response = requests.get(API_URL, params=params)\n    data = json.loads(response.text)\n\n    df = pd.DataFrame(data)\n\n    # Perform some basic data processing\n    df = df.dropna()\n    df = df.sort_values('date')\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nrandom, string, hashlib\n```python\nimport random\nimport string\nimport hashlib\n\n# Constants\nPASSWORD_LENGTH = 10\nSALT = 'mysalt'\n\ndef generate_hashed_password():\n    \"\"\"\n    Generate a random password with a certain length, and return its hashed version using the SHA256 algorithm \n    with a salt.\n\n    Returns:\n    str: The hashed password.\n\n    Requirements:\n    - random\n    - string\n    - hashlib\n\n    Example:\n    >>> generate_hashed_password()\n    \"\"\"\n    password = ''.join(random.choice(string.ascii_letters + string.digits) for _ in range(PASSWORD_LENGTH))\n    password = password + SALT\n\n    hashed_password = hashlib.sha256(password.encode()).hexdigest()\n\n    return hashed_password\n```"}
{"seed": "{\"task_id\": 265219, \"prompt\": \"def f_265219(s, old_data, new_data):\\n\\t\", \"suffix\": \"\\n\\treturn s\", \"canonical_solution\": \"\\n\\tword_dict = dict(zip(old_data, new_data))\\n\\tfor word in word_dict:\\n\\t\\ts = re.sub(r\\\"\\\\b{}\\\\b\\\".format(word), word_dict[word], s)\\n\", \"test_start\": \"\\nimport re\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(\\\"hello, world!\\\", [\\\"happy\\\", \\\"green\\\"], [\\\"angry\\\", \\\"yellow\\\"]) == \\\"hello, world!\\\"\\n\", \"\\n    assert candidate(\\\"hello, green!\\\", [\\\"happy\\\", \\\"green\\\"], [\\\"angry\\\", \\\"yellow\\\"]) == \\\"hello, yellow!\\\"\\n\", \"\\n    assert candidate(\\\"happy, yellow!\\\", [\\\"happy\\\", \\\"green\\\"], [\\\"angry\\\", \\\"yellow\\\"]) == \\\"angry, yellow!\\\"\\n\"], \"entry_point\": \"f_265219\", \"intent\": \"\\u0417\\u0430\\u043c\\u0435\\u043d\\u0438\\u0442\\u044c \\u0432 \\u0441\\u0442\\u0440\\u043e\\u043a\\u0435 `s` \\u0441\\u043b\\u043e\\u0432\\u0430 \\u0438\\u0437 \\u0441\\u043f\\u0438\\u0441\\u043a\\u0430 `old_words` \\u043d\\u0430 \\u0441\\u043b\\u043e\\u0432\\u0430 \\u0432 \\u0441\\u043f\\u0438\\u0441\\u043a\\u0435 `new_words` \", \"library\": [\"re\"]}", "generation": "Scenario 1: \npandas, re, numpy, matplotlib\n```python\nimport pandas as pd\nimport re\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nOLD_WORDS = ['happy', 'green', 'money', 'love', 'work']\nNEW_WORDS = ['angry', 'yellow', 'poverty', 'hate', 'rest']\n\n# Example data\n# DATA = pd.DataFrame({'Text': ['I love green money.', 'Work makes me happy.',\n#                               'Money can't buy love.', 'Work and money are green.']})\n\ndef replace_words_in_dataframe(df, column):\n    \"\"\"\n    Replace certain words in a dataframe column with other words and plot \n    the frequency of the new words.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n    column (str): The column name.\n\n    Returns:\n    DataFrame: The DataFrame with replaced words.\n\n    Requirements:\n    - pandas\n    - re\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = pd.DataFrame({'Text': ['I love green money.', 'Work makes me happy.',\n    ...                             'Money can't buy love.', 'Work and money are green.']})\n    >>> new_df = replace_words_in_dataframe(df, 'Text')\n    >>> print(new_df)\n    >>> new_df['Text'].str.split(expand=True).stack().value_counts().plot(kind='bar')\n    \"\"\"\n    word_dict = dict(zip(OLD_WORDS, NEW_WORDS))\n    \n    for word in word_dict:\n        df[column] = df[column].apply(lambda x: re.sub(r\"\\b{}\\b\".format(word), word_dict[word], x))\n    \n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, re, numpy, sklearn\n```python\nimport pandas as pd\nimport re\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Constants\nOLD_WORDS = ['happy', 'green', 'money', 'love', 'work']\nNEW_WORDS = ['angry', 'yellow', 'poverty', 'hate', 'rest']\n\n# Example data\n# DATA = pd.DataFrame({'Text': ['I love green money.', 'Work makes me happy.',\n#                               'Money can't buy love.', 'Work and money are green.']})\n\ndef replace_words_and_vectorize(df, column):\n    \"\"\"\n    Replace certain words in a dataframe column with other words and return \n    a term document matrix.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n    column (str): The column name.\n\n    Returns:\n    DataFrame: The term document matrix.\n\n    Requirements:\n    - pandas\n    - re\n    - numpy\n    - sklearn.feature_extraction.text.CountVectorizer\n    \n    Example:\n    >>> df = pd.DataFrame({'Text': ['I love green money.', 'Work makes me happy.',\n    ...                             'Money can't buy love.', 'Work and money are green.']})\n    >>> matrix = replace_words_and_vectorize(df, 'Text')\n    >>> print(matrix)\n    \"\"\"\n    word_dict = dict(zip(OLD_WORDS, NEW_WORDS))\n    \n    for word in word_dict:\n        df[column] = df[column].apply(lambda x: re.sub(r\"\\b{}\\b\".format(word), word_dict[word], x))\n        \n    vectorizer = CountVectorizer()\n    X = vectorizer.fit_transform(df[column])\n    \n    matrix = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n\n    return matrix\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, re, os, json\n```python\nimport pandas as pd\nimport re\nimport os\nimport json\n\n# Constants\nOLD_WORDS = ['happy', 'green', 'money', 'love', 'work']\nNEW_WORDS = ['angry', 'yellow', 'poverty', 'hate', 'rest']\n\n# Example data\n# DATA = pd.DataFrame({'Text': ['I love green money.', 'Work makes me happy.',\n#                               'Money can't buy love.', 'Work and money are green.']})\n\ndef replace_words_and_save_to_json(df, column, filename):\n    \"\"\"\n    Replace certain words in a dataframe column with other words and save \n    the dataframe to a JSON file.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n    column (str): The column name.\n    filename (str): The filename.\n\n    Returns:\n    str: The absolute path of the created JSON file.\n\n    Requirements:\n    - pandas\n    - re\n    - os\n    - json\n    \n    Example:\n    >>> df = pd.DataFrame({'Text': ['I love green money.', 'Work makes me happy.',\n    ...                             'Money can't buy love.', 'Work and money are green.']})\n    >>> path = replace_words_and_save_to_json(df, 'Text', 'output.json')\n    >>> print(path)\n    \"\"\"\n    word_dict = dict(zip(OLD_WORDS, NEW_WORDS))\n    \n    for word in word_dict:\n        df[column] = df[column].apply(lambda x: re.sub(r\"\\b{}\\b\".format(word), word_dict[word], x))\n    \n    path = os.path.join(os.getcwd(), filename)\n    df.to_json(path, orient='records')\n\n    return path\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, re, numpy, seaborn\n```python\nimport pandas as pd\nimport re\nimport numpy as np\nimport seaborn as sns\n\n# Constants\nOLD_WORDS = ['happy', 'green', 'money', 'love', 'work']\nNEW_WORDS = ['angry', 'yellow', 'poverty', 'hate', 'rest']\n\n# Example data\n# DATA = pd.DataFrame({'Text': ['I love green money.', 'Work makes me happy.',\n#                               'Money can't buy love.', 'Work and money are green.']})\n\ndef replace_words_and_plot(df, column):\n    \"\"\"\n    Replace certain words in a dataframe column with other words and plot \n    the frequency of the new words using a seaborn countplot.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n    column (str): The column name.\n\n    Returns:\n    DataFrame: The DataFrame with replaced words.\n\n    Requirements:\n    - pandas\n    - re\n    - numpy\n    - seaborn\n    \n    Example:\n    >>> df = pd.DataFrame({'Text': ['I love green money.', 'Work makes me happy.',\n    ...                             'Money can't buy love.', 'Work and money are green.']})\n    >>> new_df = replace_words_and_plot(df, 'Text')\n    >>> print(new_df)\n    >>> sns.countplot(x=\"word\", data=new_df['Text'].str.split(expand=True).stack().reset_index(level=1, name='word'))\n    \"\"\"\n    word_dict = dict(zip(OLD_WORDS, NEW_WORDS))\n    \n    for word in word_dict:\n        df[column] = df[column].apply(lambda x: re.sub(r\"\\b{}\\b\".format(word), word_dict[word], x))\n    \n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, re, string, collections\n```python\nimport pandas as pd\nimport re\nimport string\nfrom collections import Counter\n\n# Constants\nOLD_WORDS = ['happy', 'green', 'money', 'love', 'work']\nNEW_WORDS = ['angry', 'yellow', 'poverty', 'hate', 'rest']\n\n# Example data\n# DATA = pd.DataFrame({'Text': ['I love green money.', 'Work makes me happy.',\n#                               'Money can't buy love.', 'Work and money are green.']})\n\ndef replace_words_and_count(df, column):\n    \"\"\"\n    Replace certain words in a dataframe column with other words and return \n    a dictionary with the counts of each word.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n    column (str): The column name.\n\n    Returns:\n    dict: The dictionary with word counts.\n\n    Requirements:\n    - pandas\n    - re\n    - string\n    - collections.Counter\n    \n    Example:\n    >>> df = pd.DataFrame({'Text': ['I love green money.', 'Work makes me happy.',\n    ...                             'Money can't buy love.', 'Work and money are green.']})\n    >>> word_counts = replace_words_and_count(df, 'Text')\n    >>> print(word_counts)\n    \"\"\"\n    word_dict = dict(zip(OLD_WORDS, NEW_WORDS))\n    \n    for word in word_dict:\n        df[column] = df[column].apply(lambda x: re.sub(r\"\\b{}\\b\".format(word), word_dict[word], x))\n        \n    word_counts = Counter(\" \".join(df[column]).split())\n    \n    return dict(word_counts)\n```\n`GPT_ODEX_BREAK`"}
{"seed": "{\"task_id\": 14314, \"prompt\": \"def f_14314(url):\\n\\t\", \"suffix\": \"\\n\\treturn headers\", \"canonical_solution\": \"\\n\\tresponse = urllib.request.urlopen(url)\\n\\n\\theaders = response.headers\\n\", \"test_start\": \"\\nimport urllib\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert str(candidate(\\\"https://www.google.com\\\").__class__) == \\\"<class 'http.client.HTTPMessage'>\\\"\\n\"], \"entry_point\": \"f_14314\", \"intent\": \"\\u041f\\u043e\\u043b\\u0443\\u0447\\u0438\\u0442\\u044c \\u0441\\u043b\\u043e\\u0432\\u0430\\u0440\\u044c \\u0437\\u0430\\u0433\\u043e\\u043b\\u043e\\u0432\\u043a\\u043e\\u0432 `headers` \\u0441 \\u0441\\u0430\\u0439\\u0442\\u0430 `url`\", \"library\": [\"urllib\"]}", "generation": "Scenario 1:\nurllib, bs4, re, json\n```python\nimport urllib.request\nfrom bs4 import BeautifulSoup\nimport re\nimport json\n\ndef f_1(url):\n    \"\"\"\n    Retrieve the website at the given url, parse it with BeautifulSoup, search for script tags, \n    and return a list of json objects from the script tags' content.\n\n    Parameters:\n    url (str): The website url.\n\n    Returns:\n    list: A list of json objects.\n\n    Requirements:\n    - urllib\n    - BeautifulSoup\n    - re\n    - json\n\n    Example:\n    >>> f_1(\"https://www.example.com\")\n    \"\"\"\n    # Request the website\n    response = urllib.request.urlopen(url)\n\n    # Parse the HTML with BeautifulSoup\n    soup = BeautifulSoup(response)\n\n    # Find all script tags\n    scripts = soup.find_all('script')\n\n    # Extract JSON objects from the scripts' content\n    json_objects = []\n    for script in scripts:\n        json_text = re.search(r'^\\s*window\\.dataLayer\\s*=\\s*({.*?})\\s*;\\s*$', script.string, flags=re.DOTALL | re.MULTILINE)\n        if json_text:\n            data = json.loads(json_text.group(1))\n            json_objects.append(data)\n\n    return json_objects\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nurllib, lxml, pandas, os\n```python\nimport urllib.request\nfrom lxml import etree\nimport pandas as pd\nimport os\n\ndef f_2(url, filename):\n    \"\"\"\n    Download an XML file from a given url, parse it with lxml, convert it to a pandas DataFrame,\n    and save it as a CSV file.\n\n    Parameters:\n    url (str): The file url.\n    filename (str): The output filename.\n\n    Returns:\n    str: The path to the output file.\n\n    Requirements:\n    - urllib\n    - lxml\n    - pandas\n    - os\n\n    Example:\n    >>> f_2(\"https://www.example.com/file.xml\", \"output.csv\")\n    \"\"\"\n    # Download the XML file\n    urllib.request.urlretrieve(url, filename + '.xml')\n\n    # Parse the XML file\n    tree = etree.parse(filename + '.xml')\n    root = tree.getroot()\n\n    # Convert the XML data to a DataFrame\n    data = [{element.tag: element.text for element in child} for child in root]\n    df = pd.DataFrame(data)\n\n    # Save the DataFrame as a CSV file\n    output_path = os.path.join(os.getcwd(), filename + '.csv')\n    df.to_csv(output_path, index=False)\n\n    return output_path\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nurllib, zipfile, io, matplotlib\n```python\nimport urllib.request\nimport zipfile\nimport io\nimport matplotlib.pyplot as plt\n\ndef f_3(url):\n    \"\"\"\n    Download a zip file from a given url, extract it into memory, and plot the first CSV file.\n\n    Parameters:\n    url (str): The file url.\n\n    Returns:\n    None\n\n    Requirements:\n    - urllib\n    - zipfile\n    - io\n    - matplotlib.pyplot\n\n    Example:\n    >>> f_3(\"https://www.example.com/file.zip\")\n    \"\"\"\n    # Download the zip file\n    response = urllib.request.urlopen(url)\n    zip_file = zipfile.ZipFile(io.BytesIO(response.read()))\n\n    # Extract the first CSV file\n    csv_filename = [name for name in zip_file.namelist() if name.endswith('.csv')][0]\n    csv_file = zip_file.open(csv_filename)\n\n    # Read the CSV file into a DataFrame\n    df = pd.read_csv(csv_file)\n\n    # Plot the DataFrame\n    df.plot()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nurllib, PIL, numpy, matplotlib\n```python\nimport urllib.request\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef f_4(url):\n    \"\"\"\n    Download an image from a given url, convert it to a numpy array, and plot it with matplotlib.\n\n    Parameters:\n    url (str): The image url.\n\n    Returns:\n    None\n\n    Requirements:\n    - urllib\n    - PIL\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> f_4(\"https://www.example.com/image.jpg\")\n    \"\"\"\n    # Download the image\n    response = urllib.request.urlopen(url)\n    image = Image.open(io.BytesIO(response.read()))\n\n    # Convert the image to a numpy array\n    image_array = np.array(image)\n\n    # Plot the image\n    plt.imshow(image_array)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nurllib, csv, itertools, collections\n```python\nimport urllib.request\nimport csv\nimport itertools\nimport collections\n\ndef f_5(url):\n    \"\"\"\n    Download a CSV file from a given url, read it into a list of dictionaries,\n    flatten the dictionaries, and return a Counter of the frequencies of each key-value pair.\n\n    Parameters:\n    url (str): The file url.\n\n    Returns:\n    collections.Counter: A Counter object with the frequencies of each key-value pair.\n\n    Requirements:\n    - urllib\n    - csv\n    - itertools\n    - collections\n\n    Example:\n    >>> f_5(\"https://www.example.com/file.csv\")\n    \"\"\"\n    # Download the CSV file\n    response = urllib.request.urlopen(url)\n    csv_file = csv.DictReader(io.StringIO(response.read().decode()))\n\n    # Flatten the dictionaries\n    flattened_dicts = [dict(itertools.chain.from_iterable(d.items() for d in row)) for row in csv_file]\n\n    # Count the frequencies of each key-value pair\n    counter = collections.Counter(tuple(d.items()) for d in flattened_dicts)\n\n    return counter\n```\n"}
{"seed": "{\"task_id\": 25915, \"prompt\": \"def f_25915(s):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"s.encode('unicode_escape')\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(\\\"hello, world!\\\") == b'hello, world!'\\n\", \"\\n    assert candidate(\\\"\\\") == b''\\n\"], \"entry_point\": \"f_25915\", \"intent\": \"\\u042d\\u043a\\u0440\\u0430\\u043d\\u0438\\u0440\\u043e\\u0432\\u0430\\u0442\\u044c \\u0441\\u043f\\u0435\\u0446\\u0441\\u0438\\u043c\\u0432\\u043e\\u043b\\u044b \\u0432 \\u0441\\u0442\\u0440\\u043e\\u043a\\u0435 `s`\", \"library\": []}", "generation": "Scenario 1: \nhashlib, binascii, base64\n```python\nimport hashlib\nimport binascii\nimport base64\n\ndef hash_and_encode(s):\n    \"\"\"\n    This function will take a string, hash it using SHA256, and then encode it using base64.\n    It will return the encoded hash value.\n\n    Parameters:\n    s (str): The string to be hashed and encoded.\n\n    Returns:\n    str: The base64 encoded hash value of the input string.\n\n    Requirements:\n    - hashlib\n    - binascii\n    - base64\n\n    Example:\n    >>> hash_and_encode('hello, world!')\n    '6dcd4ce23d88e2ee95838f7b014b6284f0b9c9cce79e03035fdbc5c65aae783a'\n    \"\"\"\n    hashed_s = hashlib.sha256(s.encode()).digest()\n    encoded_s = base64.b64encode(hashed_s).decode()\n\n    return encoded_s\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 2:\nre, string, random\n```python\nimport re\nimport string\nimport random\n\n# Constants\nPUNCTUATIONS = string.punctuation\n\ndef generate_random_string_and_replace_punctuation(s):\n    \"\"\"\n    This function will generate a random string of the same length as the input string,\n    then replace all punctuation in the original string with the corresponding character\n    in the random string.\n\n    Parameters:\n    s (str): The string in which punctuations will be replaced.\n\n    Returns:\n    str: The modified string.\n\n    Requirements:\n    - re\n    - string\n    - random\n\n    Example:\n    >>> generate_random_string_and_replace_punctuation('hello, world!')\n    'hellox worldx'\n    \"\"\"\n    random_string = \"\".join(random.choices(string.ascii_lowercase + string.digits, k=len(s)))\n    punctuation_positions = [i for i, c in enumerate(s) if c in PUNCTUATIONS]\n\n    s_list = list(s)\n    for pos in punctuation_positions:\n        s_list[pos] = random_string[pos]\n\n    return \"\".join(s_list)\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 3:\nhashlib, binascii, base64, urllib\n```python\nimport hashlib\nimport binascii\nimport base64\nimport urllib.parse\n\ndef hash_encode_and_url_encode(s):\n    \"\"\"\n    This function hashes a string using SHA256, encodes it using base64, and then URL encodes the result.\n\n    Parameters:\n    s (str): The string to be hashed, encoded, and URL encoded.\n\n    Returns:\n    str: The URL encoded hash value.\n\n    Requirements:\n    - hashlib\n    - binascii\n    - base64\n    - urllib.parse\n\n    Example:\n    >>> hash_encode_and_url_encode('hello, world!')\n    'aGVsbG8sIHdvcmxkIQ%3D%3D'\n    \"\"\"\n    hashed_s = hashlib.sha256(s.encode()).digest()\n    encoded_s = base64.b64encode(hashed_s).decode()\n    url_encoded_s = urllib.parse.quote(encoded_s)\n\n    return url_encoded_s\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 4:\nhashlib, binascii, base64, zlib\n```python\nimport hashlib\nimport binascii\nimport base64\nimport zlib\n\ndef hash_encode_and_compress(s):\n    \"\"\"\n    This function will hash a string using SHA256, encode it using base64, and then compress the result using zlib.\n\n    Parameters:\n    s (str): The string to be hashed, encoded, and compressed.\n\n    Returns:\n    bytes: The compressed hash value.\n\n    Requirements:\n    - hashlib\n    - binascii\n    - base64\n    - zlib\n\n    Example:\n    >>> hash_encode_and_compress('hello, world!')\n    b'x\\x9c\\xabVJI\\xcd\\xc9\\xc9W(\\xcf/\\xcaIQ\\xcc\\x00b\\x00\\x12\\x87\\x04\\x97'\n    \"\"\"\n    hashed_s = hashlib.sha256(s.encode()).digest()\n    encoded_s = base64.b64encode(hashed_s)\n    compressed_s = zlib.compress(encoded_s)\n\n    return compressed_s\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 5: \nrandom, string, hashlib, binascii\n```python\nimport random\nimport string\nimport hashlib\nimport binascii\n\ndef generate_random_string_and_hash(s):\n    \"\"\"\n    This function will generate a random string of the same length as the input string,\n    append it to the original string, and return the SHA256 hash value of the result.\n\n    Parameters:\n    s (str): The original string.\n\n    Returns:\n    str: The hex representation of the hash value.\n\n    Requirements:\n    - random\n    - string\n    - hashlib\n    - binascii\n\n    Example:\n    >>> generate_random_string_and_hash('hello, world!')\n    '96d9ff4f5b59db6e7bb8a9fbc94b767525f9157f4ac8e5a2a4c187a5feb23819'\n    \"\"\"\n    random_string = \"\".join(random.choices(string.ascii_lowercase + string.digits, k=len(s)))\n    combined = s + random_string\n    hashed_combined = hashlib.sha256(combined.encode()).digest()\n    hashed_combined_hex = binascii.hexlify(hashed_combined).decode()\n\n    return hashed_combined_hex\n```"}
{"seed": "{\"task_id\": 76183, \"prompt\": \"def f_76183(sign, s):\\n\\t\", \"suffix\": \"\\n\\treturn l\", \"canonical_solution\": \"p = 1 << (len(s)-1)\\n\\tl = []\\n\\tfor i in range(1, p):\\n\\t\\tdef rf(first, sec):\\n\\t\\t\\tc = first[0]\\n\\t\\t\\ts1 = sec\\n\\t\\t\\tif c & i > 0 :\\n\\t\\t\\t\\ts1 = sign+s1\\n\\t\\t\\treturn (c >> 1, first[1]+s1)\\n\\t\\tl.append(functools.reduce(rf, s[1:], (p >> 1, s[0]))[1])\", \"test_start\": \"\\nimport functools\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(\\\"-\\\", \\\"101\\\") == ['10-1', '1-01', '1-0-1']\\n\", \"\\n    assert candidate(\\\".\\\", \\\"101\\\") == ['10.1', '1.01', '1.0.1']\\n\"], \"entry_point\": \"f_76183\", \"intent\": \"\\u041f\\u0435\\u0440\\u0435\\u0431\\u0440\\u0430\\u0442\\u044c \\u0432\\u0441\\u0435 \\u0432\\u043e\\u0437\\u043c\\u043e\\u0436\\u043d\\u044b\\u0435 \\u0441\\u043f\\u043e\\u0441\\u043e\\u0431\\u044b \\u0440\\u0430\\u0441\\u043f\\u043e\\u043b\\u043e\\u0436\\u0435\\u043d\\u0438\\u044f \\u0437\\u043d\\u0430\\u043a\\u0430 `sign` \\u0432 \\u0441\\u0442\\u0440\\u043e\\u043a\\u0435 `s` \\u043a\\u0430\\u043a \\u0432 \\u0431\\u0438\\u0442\\u043e\\u0432\\u043e\\u043c \\u0447\\u0438\\u0441\\u043b\\u0435\", \"library\": [\"functools\"]}", "generation": "Scenario 1: \nnumpy, itertools, re, string\n```python\nimport numpy as np\nimport itertools\nimport re\nfrom string import ascii_lowercase\n\n# Constants\nLETTERS = list(ascii_lowercase)\nVOWELS = ['a', 'e', 'i', 'o', 'u']\n\ndef generate_words(n):\n    \"\"\"\n    Generate all possible words of length n using the English alphabets where vowels and consonants \n    are alternatively placed and the words start with a consonant.\n    \n    Parameters:\n    n (int): The length of the words to generate.\n    \n    Returns:\n    list: A list of all generated words.\n    \n    Requirements:\n    - numpy\n    - itertools\n    - re\n    - string.ascii_lowercase\n    \n    Example:\n    >>> generate_words(3)\n    >>> ['bac', 'baf', 'bah', 'ban', 'bar', ...]\n    \"\"\"\n    consonants = list(set(LETTERS) - set(VOWELS))\n    words = []\n\n    for word in itertools.product(LETTERS, repeat=n):\n        word = ''.join(word)\n        if re.match(r'^(b|c|d|f|g|h|j|k|l|m|n|p|q|r|s|t|v|w|x|y|z)(a|e|i|o|u)(b|c|d|f|g|h|j|k|l|m|n|p|q|r|s|t|v|w|x|y|z)$', word):\n            words.append(word)\n    \n    return words\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nfunctools, random, string\n```python\nimport functools\nimport random\nfrom string import ascii_lowercase, ascii_uppercase, digits\n\n# Constants\nCHARS = ascii_lowercase + ascii_uppercase + digits\nPUNCTUATIONS = ['.', ',', '!', '?', ';']\n\ndef generate_sentences(n, min_length, max_length):\n    \"\"\"\n    Generate n random sentences with a random length between min_length and max_length. \n    The sentences start with a capital letter and end with a punctuation.\n    \n    Parameters:\n    n (int): The number of sentences to generate.\n    min_length (int): The minimum length of the sentences.\n    max_length (int): The maximum length of the sentences.\n    \n    Returns:\n    list: A list of generated sentences.\n    \n    Requirements:\n    - functools\n    - random\n    - string.ascii_lowercase\n    - string.ascii_uppercase\n    - string.digits\n    \n    Example:\n    >>> generate_sentences(2, 5, 10)\n    >>> ['D3b8j,', 'H6i9k;']\n    \"\"\"\n    sentences = []\n\n    for _ in range(n):\n        length = random.randint(min_length, max_length)\n        sentence = random.choices(CHARS, k=length)\n        sentence[0] = sentence[0].upper()\n        sentence[-1] = random.choice(PUNCTUATIONS)\n        sentences.append(''.join(sentence))\n    \n    return sentences\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nsys, os, pathlib, shutil\n```python\nimport sys\nimport os\nfrom pathlib import Path\nimport shutil\n\ndef move_files(source_dir, dest_dir, file_ext):\n    \"\"\"\n    Move all files with a given extension from a source directory to a destination directory.\n    \n    Parameters:\n    source_dir (str): The source directory.\n    dest_dir (str): The destination directory.\n    file_ext (str): The extension of the files to move.\n    \n    Requirements:\n    - sys\n    - os\n    - pathlib.Path\n    - shutil\n    \n    Example:\n    >>> move_files('/path/to/source', '/path/to/destination', '.txt')\n    \"\"\"\n    source_dir = Path(source_dir)\n    dest_dir = Path(dest_dir)\n\n    if not source_dir.is_dir():\n        print(f\"Error: {source_dir} does not exist or is not a directory.\")\n        sys.exit(1)\n\n    if not dest_dir.is_dir():\n        print(f\"Error: {dest_dir} does not exist or is not a directory.\")\n        sys.exit(1)\n\n    for file in source_dir.glob(f'*{file_ext}'):\n        shutil.move(str(file), dest_dir)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, pandas, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nDAYS = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n\ndef plot_weekly_sales(sales_data):\n    \"\"\"\n    Generate a bar chart for weekly sales data. The x-axis represents the days of the week, and the \n    y-axis represents the sales amount.\n    \n    Parameters:\n    sales_data (dict): The sales data where the keys are the days of the week and the values are \n    the sales amounts.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    \n    Example:\n    >>> sales = {'Monday': 100, 'Tuesday': 200, 'Wednesday': 150, 'Thursday': 300, 'Friday': 250, 'Saturday': 400, 'Sunday': 350}\n    >>> plot_weekly_sales(sales)\n    \"\"\"\n    sales_df = pd.DataFrame.from_dict(sales_data, orient='index', columns=['Sales'])\n    sales_df = sales_df.reindex(DAYS)\n    sales_df.plot(kind='bar', legend=False)\n    plt.xlabel('Day of the Week')\n    plt.ylabel('Sales Amount')\n    plt.title('Weekly Sales')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, scipy.stats, matplotlib\n```python\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\ndef plot_normal_distribution(mu, sigma):\n    \"\"\"\n    Plot a normal distribution with a given mean and standard deviation.\n    \n    Parameters:\n    mu (float): The mean of the distribution.\n    sigma (float): The standard deviation of the distribution.\n    \n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - scipy.stats.norm\n    - matplotlib.pyplot\n    \n    Example:\n    >>> plot_normal_distribution(0, 1)\n    \"\"\"\n    x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n    y = norm.pdf(x, mu, sigma)\n\n    plt.plot(x, y)\n    plt.title('Normal Distribution')\n    plt.xlabel('Value')\n    plt.ylabel('Density')\n    plt.grid(True)\n    plt.show()\n```\n"}
