{"seed": "{\"task_id\": 54979, \"prompt\": \"def f_54979(dic):\\n\\t\", \"suffix\": \"\\n\\treturn result\", \"canonical_solution\": \"result = defaultdict(list)\\n\\tfor d in dic.values():\\n\\t    for k,v in d.items():\\n\\t        result[k].append(v)\", \"test_start\": \"\\nfrom collections import defaultdict\\n\\ndef check(candidate):\", \"test\": [\"\\n    dic = {'xx': {0: 0., 1: 1., 2: 2.}, 'yy': {1: '1', 2: '2'}, 'zz': {2: 2}}\\n    assert dict(candidate(dic)) == {0: [0.0], 1: [1.0, '1'], 2: [2.0, '2', 2]}\\n\"], \"entry_point\": \"f_54979\", \"intent\": \"\\u00bfC\\u00f3mo obtener un dictionario `result` con las llaves `keys` y los valores `val` de un diccionario `dic`?\", \"library\": [\"collections\"]}", "generation": "Scenario 1: \npandas, defaultdict, numpy, itertools\n```python\nimport pandas as pd\nfrom collections import defaultdict\nimport numpy as np\nimport itertools\n\n# Constants\nCATEGORIES = ['Fruit', 'Vegetable', 'Grain', 'Dairy', 'Meat']\n\ndef generate_food_summary(food_data):\n    \"\"\"\n    Generate a summary report of food items in different categories with average \n    nutritional values (calories, protein, carbohydrates, and fats).\n\n    Parameters:\n    food_data (dict): A dictionary where keys are food item names and values are \n    dictionaries with nutritional values.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with summary of food items in different categories.\n\n    Requirements:\n    - pandas\n    - collections.defaultdict\n    - numpy\n    - itertools\n    \n    Example:\n    >>> food_data = {\n    ...     'Apple': {'Category': 'Fruit', 'Calories': 52, 'Protein': 0.3, 'Carbohydrates': 14, 'Fat': 0.2},\n    ...     'Banana': {'Category': 'Fruit', 'Calories': 96, 'Protein': 1.3, 'Carbohydrates': 23, 'Fat': 0.3},\n    ...     'Broccoli': {'Category': 'Vegetable', 'Calories': 55, 'Protein': 3.7, 'Carbohydrates': 11, 'Fat': 0.6},\n    ...     'Spinach': {'Category': 'Vegetable', 'Calories': 23, 'Protein': 2.9, 'Carbohydrates': 3.6, 'Fat': 0.4},\n    ...     'Chicken': {'Category': 'Meat', 'Calories': 335, 'Protein': 25, 'Carbohydrates': 0, 'Fat': 3.6},\n    ...     'Beef': {'Category': 'Meat', 'Calories': 250, 'Protein': 17.8, 'Carbohydrates': 0, 'Fat': 17.0},\n    ... }\n    >>> report = generate_food_summary(food_data)\n    >>> print(report)\n    \"\"\"\n    summary = defaultdict(list)\n    for food, nutrition in food_data.items():\n        for category in CATEGORIES:\n            if nutrition['Category'] == category:\n                summary[category].append(nutrition)\n\n    summary_df = pd.DataFrame(columns=['Category', 'Average Calories', 'Average Protein', 'Average Carbohydrates', 'Average Fat'])\n    for category, foods in summary.items():\n        average_calories = np.mean([food['Calories'] for food in foods])\n        average_protein = np.mean([food['Protein'] for food in foods])\n        average_carbohydrates = np.mean([food['Carbohydrates'] for food in foods])\n        average_fat = np.mean([food['Fat'] for food in foods])\n        summary_df = summary_df.append({\n            'Category': category,\n            'Average Calories': average_calories,\n            'Average Protein': average_protein,\n            'Average Carbohydrates': average_carbohydrates,\n            'Average Fat': average_fat\n        }, ignore_index=True)\n        \n    return summary_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncollections, math, itertools, random\n```python\nfrom collections import defaultdict\nimport math\nimport itertools\nimport random\n\n# Constants\nDICE_SIDES = 6\n\ndef simulate_dice_rolls(n):\n    \"\"\"\n    Simulate a series of n dice rolls and return the frequency of each possible \n    sum of the dice.\n\n    Parameters:\n    n (int): The number of dice rolls to simulate.\n    \n    Returns:\n    dict: A dictionary where keys are the possible sums and values are the frequencies.\n\n    Requirements:\n    - collections\n    - math\n    - itertools\n    - random\n    \n    Example:\n    >>> freq = simulate_dice_rolls(10000)\n    >>> print(freq)\n    \"\"\"\n    freq = defaultdict(int)\n    for _ in range(n):\n        dice_roll = sum(random.randint(1, DICE_SIDES) for _ in range(2))\n        freq[dice_roll] += 1\n    return freq\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ncollections, itertools, numpy, pandas\n```python\nfrom collections import defaultdict\nimport itertools\nimport numpy as np\nimport pandas as pd\n\n# Constants\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D']\n\ndef schedule_matches(teams):\n    \"\"\"\n    Generate a round-robin schedule for a list of teams.\n\n    Parameters:\n    teams (list): A list of team names.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the schedule of matches.\n\n    Requirements:\n    - collections\n    - itertools\n    - numpy\n    - pandas\n    \n    Example:\n    >>> schedule = schedule_matches(TEAMS)\n    >>> print(schedule)\n    \"\"\"\n    schedule = defaultdict(list)\n    for home, away in itertools.combinations(teams, 2):\n        schedule[home].append(away)\n        schedule[away].append(home)\n        \n    schedule_df = pd.DataFrame.from_dict(schedule, orient='index').transpose()\n    return schedule_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ncollections, random, itertools, pandas\n```python\nfrom collections import defaultdict\nimport random\nimport itertools\nimport pandas as pd\n\n# Constants\nSTUDENTS = ['Alice', 'Bob', 'Charlie', 'David', 'Eve']\n\ndef assign_project_partners(students):\n    \"\"\"\n    Randomly assign project partners for a list of students.\n\n    Parameters:\n    students (list): A list of student names.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the pairs of project partners.\n\n    Requirements:\n    - collections\n    - random\n    - itertools\n    - pandas\n    \n    Example:\n    >>> partners = assign_project_partners(STUDENTS)\n    >>> print(partners)\n    \"\"\"\n    partners = defaultdict(list)\n    for student1, student2 in itertools.combinations(students, 2):\n        if random.random() < 0.5:\n            partners[student1].append(student2)\n        else:\n            partners[student2].append(student1)\n            \n    partners_df = pd.DataFrame.from_dict(partners, orient='index').transpose()\n    return partners_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\ncollections, itertools, numpy, matplotlib\n```python\nfrom collections import defaultdict\nimport itertools\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nNUMBERS = np.arange(1, 11)\n\ndef plot_number_combinations(numbers):\n    \"\"\"\n    Plot the frequency of sums for all combinations of two numbers from a list.\n\n    Parameters:\n    numbers (np.array): A numpy array of numbers.\n    \n    Returns:\n    None.\n\n    Requirements:\n    - collections\n    - itertools\n    - numpy\n    - matplotlib\n    \n    Example:\n    >>> plot_number_combinations(NUMBERS)\n    \"\"\"\n    sums = defaultdict(int)\n    for num1, num2 in itertools.combinations(numbers, 2):\n        sums[num1 + num2] += 1\n        \n    plt.bar(sums.keys(), sums.values())\n    plt.xlabel('Sum')\n    plt.ylabel('Frequency')\n    plt.title('Frequency of Sums for Combinations of Two Numbers')\n    plt.show()\n```\n"}
{"seed": "{\"task_id\": 39675, \"prompt\": \"def f_39675(x, y):\\n\\t\", \"suffix\": \"\\n\\treturn sol\", \"canonical_solution\": \"\\n\\tdy = diff(y)\\n\\tsol = solve(dy)\\n\", \"test_start\": \"\\nfrom sympy import *\\n\\ndef check(candidate):\", \"test\": [\"\\n    x = symbols('x')\\n    y = -(x-0.4)**2+10\\n    assert candidate(x, y)[0] == 0.4\\n\"], \"entry_point\": \"f_39675\", \"intent\": \"\\u00bfC\\u00f3mo encontrar el m\\u00e1ximo de una funci\\u00f3n `y` que depende de `x`?\", \"library\": [\"sympy\"]}", "generation": "Scenario 1:\nsympy, numpy, matplotlib\n```python\nfrom sympy import symbols, diff, solve\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_and_find_max(func):\n    \"\"\"\n    Plot a function and find its maximum point.\n\n    Parameters:\n    func (str): The function as a string.\n\n    Returns:\n    tuple: The maximum point of the function.\n\n    Requirements:\n    - sympy\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_and_find_max(\"-(x-0.4)**2+10\")\n    \"\"\"\n    x = symbols('x')\n    y = eval(func)\n\n    # Solve the derivative to find the maxima\n    dy = diff(y)\n    sol = solve(dy)\n\n    # Plot the function\n    x_vals = np.linspace(-10, 10, 100)\n    y_func = np.vectorize(lambda x_val: y.subs(x, x_val))\n    y_vals = y_func(x_vals)\n    plt.plot(x_vals, y_vals)\n    plt.title('Function plot')\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.grid(True)\n    plt.show()\n\n    return sol[0], y.subs(x, sol[0])\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nsympy, pandas, scipy.optimize\n```python\nfrom sympy import symbols, diff, solve\nimport pandas as pd\nfrom scipy.optimize import minimize\n\n# Constants\nFUNCTIONS = ['-(x-0.4)**2+10', 'x**3 - 3*x**2 + 1', 'x**4 - x**2 + 2*x - 1']\n\ndef find_maxima_and_minima(functions):\n    \"\"\"\n    Find the maxima and minima of a list of functions.\n\n    Parameters:\n    functions (list): The list of functions as strings.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the maxima and minima of the functions.\n\n    Requirements:\n    - sympy\n    - pandas\n    - scipy.optimize\n\n    Example:\n    >>> find_maxima_and_minima(FUNCTIONS)\n    \"\"\"\n    x = symbols('x')\n    results = []\n\n    for func in functions:\n        y = eval(func)\n\n        # Find maxima\n        dy = diff(y)\n        maxima = solve(dy)\n\n        # Find minima using scipy's minimize function\n        minima = minimize(lambda x_val: y.subs(x, x_val), 0).x[0]\n\n        results.append([func, maxima[0], y.subs(x, maxima[0]), minima, y.subs(x, minima)])\n\n    df = pd.DataFrame(results, columns=['Function', 'Maxima', 'Maxima Value', 'Minima', 'Minima Value'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nsympy, pandas, matplotlib\n```python\nfrom sympy import symbols, diff, solve\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nFUNCTIONS = ['-(x-0.4)**2+10', 'x**3 - 3*x**2 + 1', 'x**4 - x**2 + 2*x - 1']\n\ndef plot_and_compare_functions(functions):\n    \"\"\"\n    Plot and compare a list of functions.\n\n    Parameters:\n    functions (list): The list of functions as strings.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the maxima of the functions.\n\n    Requirements:\n    - sympy\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_and_compare_functions(FUNCTIONS)\n    \"\"\"\n    x = symbols('x')\n    results = []\n\n    for func in functions:\n        y = eval(func)\n\n        # Find maxima\n        dy = diff(y)\n        maxima = solve(dy)\n\n        results.append([func, maxima[0], y.subs(x, maxima[0])])\n\n        # Plot function\n        x_vals = np.linspace(-10, 10, 100)\n        y_func = np.vectorize(lambda x_val: y.subs(x, x_val))\n        y_vals = y_func(x_vals)\n        plt.plot(x_vals, y_vals, label=func)\n\n    plt.title('Function comparison')\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n    df = pd.DataFrame(results, columns=['Function', 'Maxima', 'Maxima Value'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nsympy, numpy, matplotlib, math\n```python\nfrom sympy import symbols, diff, solve\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\n\ndef plot_and_find_roots(func):\n    \"\"\"\n    Plot a function and find its roots.\n\n    Parameters:\n    func (str): The function as a string.\n\n    Returns:\n    list: The roots of the function.\n\n    Requirements:\n    - sympy\n    - numpy\n    - matplotlib.pyplot\n    - math\n\n    Example:\n    >>> plot_and_find_roots(\"x**3 - 3*x**2 + 1\")\n    \"\"\"\n    x = symbols('x')\n    y = eval(func)\n\n    # Solve the function to find the roots\n    roots = solve(y)\n\n    # Plot the function\n    x_vals = np.linspace(-10, 10, 100)\n    y_func = np.vectorize(lambda x_val: y.subs(x, x_val))\n    y_vals = y_func(x_vals)\n    plt.plot(x_vals, y_vals)\n    plt.title('Function plot')\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.grid(True)\n    plt.show()\n\n    return [root.evalf() for root in roots]\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nsympy, pandas, math\n```python\nfrom sympy import symbols, diff, solve\nimport pandas as pd\nimport math\n\n# Constants\nFUNCTIONS = ['-(x-0.4)**2+10', 'x**3 - 3*x**2 + 1', 'x**4 - x**2 + 2*x - 1']\n\ndef find_roots_and_value_at_roots(functions):\n    \"\"\"\n    Find the roots and their corresponding values of a list of functions.\n\n    Parameters:\n    functions (list): The list of functions as strings.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the roots and their corresponding values of the functions.\n\n    Requirements:\n    - sympy\n    - pandas\n    - math\n\n    Example:\n    >>> find_roots_and_value_at_roots(FUNCTIONS)\n    \"\"\"\n    x = symbols('x')\n    results = []\n\n    for func in functions:\n        y = eval(func)\n\n        # Find roots\n        roots = solve(y)\n\n        for root in roots:\n            results.append([func, root, y.subs(x, root)])\n\n    df = pd.DataFrame(results, columns=['Function', 'Root', 'Value at Root'])\n\n    return df\n```"}
{"seed": "{\"task_id\": 19921, \"prompt\": \"def f_19921(file, ruta):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"\\n\\twith open(file, 'rb') as fsrc:\\n\\t\\ta = fsrc.read()\\n\\tshutil.copyfile(file, ruta)\\n\", \"test_start\": \"\\nimport os\\nimport shutil\\n\\ndef check(candidate):\", \"test\": [\"\\n    file = \\\"tmp.txt\\\"\\n    with open(file, 'w') as fw: \\n        fw.write(\\\"hello world!\\\")\\n    ruta = \\\"test/tmp.txt\\\"\\n    os.mkdir(\\\"test\\\")\\n    candidate(file, ruta)\\n    assert os.path.isfile(ruta) == True\\n\"], \"entry_point\": \"f_19921\", \"intent\": \"\\u00bfC\\u00f3mo subir archivos a una carpeta local ubicada en `ruta` despu\\u00e9s de abrirlo?\", \"library\": [\"os\", \"shutil\"]}", "generation": "Scenario 1:\nos, shutil, hashlib, glob, time\n```python\nimport os\nimport shutil\nimport hashlib\nfrom glob import glob\nimport time\n\n# Constants\nFILE_EXTENSIONS = ['.txt', '.jpg', '.png', '.pdf']\n\ndef move_and_hash_files(source_dir, dest_dir):\n    \"\"\"\n    Move files with specific extensions from the source directory to the destination \n    directory. Generate and store the SHA256 hash of each file.\n    \n    Parameters:\n    source_dir (str): The source directory path.\n    dest_dir (str): The destination directory path.\n    \n    Returns:\n    dict: A dictionary with filename as the key and its SHA256 hash as the value.\n    \n    Requirements:\n    - os\n    - shutil\n    - hashlib\n    - glob\n    - time\n    \n    Example:\n    >>> hash_dict = move_and_hash_files('/home/user/source', '/home/user/dest')\n    >>> print(hash_dict)\n    \"\"\"\n    hash_dict = {}\n    \n    for extension in FILE_EXTENSIONS:\n        for file_path in glob(os.path.join(source_dir, '*' + extension)):\n            filename = os.path.basename(file_path)\n            dest_path = os.path.join(dest_dir, filename)\n            \n            shutil.move(file_path, dest_path)\n            time.sleep(1)  # to ensure the file has been moved\n            \n            with open(dest_path, 'rb') as file:\n                file_data = file.read()\n                file_hash = hashlib.sha256(file_data).hexdigest()\n                \n            hash_dict[filename] = file_hash\n    \n    return hash_dict\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, shutil, datetime, pathlib\n```python\nimport os\nimport shutil\nfrom datetime import datetime\nfrom pathlib import Path\n\n# Constants\nDAYS_TO_KEEP = 30\n\ndef clean_old_files(directory):\n    \"\"\"\n    Clean files in a directory that are older than 30 days.\n    \n    Parameters:\n    directory (str): The directory path.\n\n    Returns:\n    int: The number of files deleted.\n\n    Requirements:\n    - os\n    - shutil\n    - datetime\n    - pathlib.Path\n\n    Example:\n    >>> num_files_deleted = clean_old_files('/home/user/documents')\n    >>> print(num_files_deleted)\n    \"\"\"\n    num_files_deleted = 0\n    \n    now = datetime.now()\n    threshold = now.timestamp() - DAYS_TO_KEEP * 24 * 60 * 60  # in seconds\n    \n    for file in Path(directory).iterdir():\n        file_modified_time = file.stat().st_mtime\n        \n        if file_modified_time < threshold:\n            shutil.rmtree(str(file) if file.is_dir() else file)\n            num_files_deleted += 1\n            \n    return num_files_deleted\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, shutil, xml.etree.ElementTree, fnmatch\n```python\nimport os\nimport shutil\nimport xml.etree.ElementTree as ET\nimport fnmatch\n\n# Constants\nXML_FILE_PATTERN = '*.xml'\nXML_ROOT_TAG = 'root'\n\ndef move_xml_files_with_tag(source_dir, dest_dir):\n    \"\"\"\n    Move XML files from the source directory to the destination directory \n    if they contain a specific root tag.\n    \n    Parameters:\n    source_dir (str): The source directory path.\n    dest_dir (str): The destination directory path.\n    \n    Returns:\n    int: The number of files moved.\n\n    Requirements:\n    - os\n    - shutil\n    - xml.etree.ElementTree\n    - fnmatch\n\n    Example:\n    >>> num_files_moved = move_xml_files_with_tag('/home/user/source', '/home/user/dest')\n    >>> print(num_files_moved)\n    \"\"\"\n    num_files_moved = 0\n    \n    for root, dirs, files in os.walk(source_dir):\n        for filename in fnmatch.filter(files, XML_FILE_PATTERN):\n            file_path = os.path.join(root, filename)\n            \n            tree = ET.parse(file_path)\n            root_tag = tree.getroot().tag\n            \n            if root_tag == XML_ROOT_TAG:\n                shutil.move(file_path, dest_dir)\n                num_files_moved += 1\n                \n    return num_files_moved\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, shutil, csv, pandas\n```python\nimport os\nimport shutil\nimport csv\nimport pandas as pd\n\n# Constants\nCSV_FILE_EXTENSION = '.csv'\nCSV_COLUMNS = ['Name', 'Email', 'Phone']\n\ndef merge_csv_files(source_dir, dest_file):\n    \"\"\"\n    Merge CSV files from the source directory into one CSV file in the destination.\n    \n    Parameters:\n    source_dir (str): The source directory path.\n    dest_file (str): The destination file path.\n    \n    Returns:\n    DataFrame: The merged pandas DataFrame.\n\n    Requirements:\n    - os\n    - shutil\n    - csv\n    - pandas\n\n    Example:\n    >>> merged_df = merge_csv_files('/home/user/source', '/home/user/merged.csv')\n    >>> print(merged_df)\n    \"\"\"\n    merged_data = []\n    \n    for root, dirs, files in os.walk(source_dir):\n        for filename in files:\n            if filename.endswith(CSV_FILE_EXTENSION):\n                file_path = os.path.join(root, filename)\n                \n                with open(file_path, 'r') as file:\n                    reader = csv.DictReader(file)\n                    merged_data.extend([row for row in reader if set(row.keys()) == set(CSV_COLUMNS)])\n    \n    merged_df = pd.DataFrame(merged_data)\n    merged_df.to_csv(dest_file, index=False)\n    \n    return merged_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, shutil, zipfile, fnmatch\n```python\nimport os\nimport shutil\nimport zipfile\nimport fnmatch\n\n# Constants\nZIP_FILE_PATTERN = '*.zip'\nEXTRACT_FOLDER_NAME = 'extracted'\n\ndef extract_zip_files(source_dir, dest_dir):\n    \"\"\"\n    Extract zip files from the source directory to the destination directory.\n    \n    Parameters:\n    source_dir (str): The source directory path.\n    dest_dir (str): The destination directory path.\n    \n    Returns:\n    int: The number of files extracted.\n\n    Requirements:\n    - os\n    - shutil\n    - zipfile\n    - fnmatch\n\n    Example:\n    >>> num_files_extracted = extract_zip_files('/home/user/source', '/home/user/dest')\n    >>> print(num_files_extracted)\n    \"\"\"\n    num_files_extracted = 0\n    \n    for root, dirs, files in os.walk(source_dir):\n        for filename in fnmatch.filter(files, ZIP_FILE_PATTERN):\n            file_path = os.path.join(root, filename)\n            extract_folder_path = os.path.join(dest_dir, EXTRACT_FOLDER_NAME)\n            \n            with zipfile.ZipFile(file_path, 'r') as zip_ref:\n                zip_ref.extractall(extract_folder_path)\n                num_files_extracted += len(zip_ref.namelist())\n                \n    return num_files_extracted\n```\n"}
{"seed": "{\"task_id\": 25024, \"prompt\": \"def f_25024(lst):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"list(sum(zip(lst, lst[1:]), ()))\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([1,2,3,4]) == [1,2,2,3,3,4]\\n\"], \"entry_point\": \"f_25024\", \"intent\": \"\\u00bfC\\u00f3mo duplicar los elementos de una lista `lst` excluyendo los extremos?\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, itertools, statistics\n```python\nimport pandas as pd\nimport numpy as np\nimport itertools\nimport statistics\n\n# Constants\nCATEGORIES = ['Fruits', 'Vegetables', 'Dairy', 'Meat', 'Grains']\nITEMS = {\n    'Fruits': ['Apple', 'Banana', 'Orange', 'Pineapple', 'Grapes'],\n    'Vegetables': ['Carrot', 'Broccoli', 'Peas', 'Spinach', 'Tomato'],\n    'Dairy': ['Milk', 'Cheese', 'Butter', 'Yogurt', 'Cream'],\n    'Meat': ['Chicken', 'Beef', 'Pork', 'Lamb', 'Fish'],\n    'Grains': ['Rice', 'Wheat', 'Corn', 'Oats', 'Barley']\n}\n\ndef calculate_stats_from_sales(sales_data):\n    \"\"\"\n    Generate a DataFrame with the average, median, and standard deviation of sales data for a list of items \n    in different categories.\n    \n    Parameters:\n    sales_data (list): The sales data.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the statistics for the sales data.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - itertools\n    - statistics\n    \n    Example:\n    >>> sales_data = [100, 200, 150, 300, 250, 50, 75, 125, 100, 200, 150, 160, 180, 170, 200]\n    >>> stats_df = calculate_stats_from_sales(sales_data)\n    >>> print(stats_df)\n    \"\"\"\n    data = []\n    \n    for category, items in ITEMS.items():\n        for item in items:\n            item_sales = sales_data[:5]\n            sales_data = sales_data[5:]\n            avg_sale = np.mean(item_sales)\n            median_sale = np.median(item_sales)\n            std_dev_sale = statistics.stdev(item_sales)\n            data.append([category, item, avg_sale, median_sale, std_dev_sale])\n\n    df = pd.DataFrame(data, columns=['Category', 'Item', 'Average Sale', 'Median Sale', 'Sale Standard Deviation'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, itertools, collections, math\n```python\nimport numpy as np\nimport itertools\nfrom collections import Counter\nimport math\n\n# Constants\nITEMS = ['A', 'B', 'C', 'D', 'E']\n\ndef calculate_combinations(items_list):\n    \"\"\"\n    Calculate the number of unique combinations of items in a given list, \n    excluding duplicate combinations.\n    \n    Parameters:\n    items_list (list): The list of items.\n    \n    Returns:\n    int: The total number of unique combinations.\n    \n    Requirements:\n    - numpy\n    - itertools\n    - collections\n    - math\n\n    Example:\n    >>> calculate_combinations(['A', 'B', 'A', 'C', 'D', 'B'])\n    \"\"\"\n    unique_items = Counter(items_list)\n    combinations = 0\n\n    for r in range(1, len(unique_items) + 1):\n        combinations += math.factorial(len(unique_items)) / (math.factorial(r) * math.factorial(len(unique_items) - r))\n\n    return int(combinations)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, itertools, random\n```python\nimport pandas as pd\nimport numpy as np\nimport itertools\nimport random\n\n# Constants\nSTUDENTS = ['John', 'Sarah', 'Mike', 'Emma', 'Jake', 'Olivia', 'Sophia', 'James', 'Emily', 'Liam']\n\ndef generate_student_scores(students):\n    \"\"\"\n    Generate a DataFrame with random scores for a list of students in various subjects.\n    \n    Parameters:\n    students (list): The list of students.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the scores for the students.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - itertools\n    - random\n\n    Example:\n    >>> scores_df = generate_student_scores(STUDENTS)\n    >>> print(scores_df)\n    \"\"\"\n    subjects = ['Math', 'English', 'Science', 'History', 'Geography']\n    scores = []\n\n    for student in students:\n        student_scores = [student] + [random.randint(50, 100) for _ in range(len(subjects))]\n        scores.append(student_scores)\n\n    df = pd.DataFrame(scores, columns=['Student'] + subjects)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, itertools, statistics, random\n```python\nimport numpy as np\nimport itertools\nimport statistics\nimport random\n\n# Constants\nNUMBERS = [i for i in range(1, 51)]\n\ndef calculate_statistics(numbers):\n    \"\"\"\n    Calculate the mean, median, and standard deviation of a list of random numbers.\n    \n    Parameters:\n    numbers (list): The list of numbers.\n    \n    Returns:\n    dict: A dictionary with the mean, median, and standard deviation.\n    \n    Requirements:\n    - numpy\n    - itertools\n    - statistics\n    - random\n\n    Example:\n    >>> stats = calculate_statistics(NUMBERS)\n    >>> print(stats)\n    \"\"\"\n    random_numbers = random.sample(numbers, 20)\n    mean = np.mean(random_numbers)\n    median = np.median(random_numbers)\n    std_dev = statistics.stdev(random_numbers)\n    \n    return {'Mean': mean, 'Median': median, 'Standard Deviation': std_dev}\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, itertools, random\n```python\nimport pandas as pd\nimport numpy as np\nimport itertools\nimport random\n\n# Constants\nEMPLOYEES = ['John', 'Sarah', 'Mike', 'Emma', 'Jake', 'Olivia', 'Sophia', 'James', 'Emily', 'Liam']\n\ndef generate_employee_salaries(employees):\n    \"\"\"\n    Generate a DataFrame with random salaries for a list of employees.\n    \n    Parameters:\n    employees (list): The list of employees.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the salaries for the employees.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - itertools\n    - random\n\n    Example:\n    >>> salaries_df = generate_employee_salaries(EMPLOYEES)\n    >>> print(salaries_df)\n    \"\"\"\n    salaries = []\n\n    for employee in employees:\n        salary = random.randint(3000, 8000)\n        salaries.append([employee, salary])\n\n    df = pd.DataFrame(salaries, columns=['Employee', 'Salary'])\n\n    return df\n```"}
{"seed": "{\"task_id\": 774, \"prompt\": \"def f_774():\\n\\t\", \"suffix\": \"\\n\\treturn C\", \"canonical_solution\": \"\\n\\tclass C:\\n\\t\\tdef __init__(self):\\n\\t\\t\\tself.total_renombres = 0\\n\\n\\t\\tdef _incrementa_contador_renombres(self):\\n\\t\\t\\tself.total_renombres += 1\\n\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    sample = candidate()()\\n    sample._incrementa_contador_renombres()\\n    assert sample.total_renombres == 1\\n\"], \"entry_point\": \"f_774\", \"intent\": \"\\u00bfC\\u00f3mo contar las llamadas totales al m\\u00e9todo de una clase `C`?\", \"library\": []}", "generation": "Scenario 1: collections, random, matplotlib.pyplot, pandas\n```python\nfrom collections import Counter\nimport random\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants\nCOLORS = ['Red', 'Green', 'Blue', 'Yellow', 'Black', 'White']\n\nclass ColorCounter:\n    \"\"\"\n    A class used to count the number of times each color is randomly chosen.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        The constructor for ColorCounter class.\n        \"\"\"\n        self.color_counts = Counter()\n\n    def increment_color_count(self):\n        \"\"\"\n        The function to increment the count of a color randomly chosen.\n\n        Returns:\n        None.\n        \"\"\"\n        chosen_color = random.choice(COLORS)\n        self.color_counts[chosen_color] += 1\n\n    def get_color_counts(self):\n        \"\"\"\n        The function to get the counts of all colors.\n\n        Returns:\n        Counter: A Counter object with the counts of all colors.\n        \"\"\"\n        return self.color_counts\n\ndef plot_color_counts():\n    \"\"\"\n    Plot a bar chart of the counts of all colors.\n\n    Returns:\n    None.\n\n    Requirements:\n    - collections\n    - random\n    - matplotlib.pyplot\n    - pandas\n\n    Example:\n    >>> color_counter = ColorCounter()\n    >>> for _ in range(1000):\n    >>>     color_counter.increment_color_count()\n    >>> color_counts = color_counter.get_color_counts()\n    >>> df = pd.DataFrame.from_dict(color_counts, orient='index')\n    >>> df.plot(kind='bar')\n    \"\"\"\n    color_counter = ColorCounter()\n    for _ in range(1000):\n        color_counter.increment_color_count()\n    color_counts = color_counter.get_color_counts()\n    df = pd.DataFrame.from_dict(color_counts, orient='index')\n    df.plot(kind='bar')\n\nplot_color_counts()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: functools, datetime, random\n```python\nfrom functools import wraps\nfrom datetime import datetime\nimport random\n\n# Constants\nMETHODS = ['method_1', 'method_2', 'method_3', 'method_4', 'method_5']\n\nclass MethodCallCounter:\n    \"\"\"\n    A class used to count the number of times each method is called.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        The constructor for MethodCallCounter class.\n        \"\"\"\n        self.method_counts = {method: 0 for method in METHODS}\n\n    def count_method_calls(func):\n        \"\"\"\n        A decorator to count the number of times a method is called.\n\n        Parameters:\n        func (function): The function to be decorated.\n\n        Returns:\n        function: The decorated function.\n        \"\"\"\n        @wraps(func)\n        def wrapper(self, *args, **kwargs):\n            self.method_counts[func.__name__] += 1\n            return func(self, *args, **kwargs)\n        return wrapper\n\n    @count_method_calls\n    def method_1(self):\n        pass\n\n    @count_method_calls\n    def method_2(self):\n        pass\n\n    @count_method_calls\n    def method_3(self):\n        pass\n\n    @count_method_calls\n    def method_4(self):\n        pass\n\n    @count_method_calls\n    def method_5(self):\n        pass\n\ndef random_method_calls():\n    \"\"\"\n    Call methods randomly and return the counts of all method calls.\n\n    Returns:\n    dict: A dictionary with the counts of all method calls.\n\n    Requirements:\n    - functools\n    - datetime\n    - random\n\n    Example:\n    >>> counter = MethodCallCounter()\n    >>> for _ in range(1000):\n    >>>     method = random.choice(METHODS)\n    >>>     getattr(counter, method)()\n    >>> print(counter.method_counts)\n    \"\"\"\n    counter = MethodCallCounter()\n    for _ in range(1000):\n        method = random.choice(METHODS)\n        getattr(counter, method)()\n    return counter.method_counts\n\nprint(random_method_calls())\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: functools, datetime, random, threading\n```python\nfrom functools import wraps\nfrom datetime import datetime, timedelta\nimport random\nimport threading\n\n# Constants\nMETHODS = ['method_1', 'method_2', 'method_3', 'method_4', 'method_5']\n\nclass MethodCallCounter:\n    \"\"\"\n    A class used to count the number of times each method is called within a certain period of time.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        The constructor for MethodCallCounter class.\n        \"\"\"\n        self.method_counts = {method: 0 for method in METHODS}\n        self.start_time = datetime.now()\n\n    def count_method_calls(func):\n        \"\"\"\n        A decorator to count the number of times a method is called within a certain period of time.\n\n        Parameters:\n        func (function): The function to be decorated.\n\n        Returns:\n        function: The decorated function.\n        \"\"\"\n        @wraps(func)\n        def wrapper(self, *args, **kwargs):\n            if datetime.now() - self.start_time <= timedelta(minutes=1):\n                self.method_counts[func.__name__] += 1\n            return func(self, *args, **kwargs)\n        return wrapper\n\n    @count_method_calls\n    def method_1(self):\n        pass\n\n    @count_method_calls\n    def method_2(self):\n        pass\n\n    @count_method_calls\n    def method_3(self):\n        pass\n\n    @count_method_calls\n    def method_4(self):\n        pass\n\n    @count_method_calls\n    def method_5(self):\n        pass\n\ndef random_method_calls():\n    \"\"\"\n    Call methods randomly within a certain period of time and return the counts of all method calls.\n\n    Returns:\n    dict: A dictionary with the counts of all method calls.\n\n    Requirements:\n    - functools\n    - datetime\n    - random\n    - threading\n\n    Example:\n    >>> counter = MethodCallCounter()\n    >>> for _ in range(1000):\n    >>>     threading.Timer(random.random(), getattr(counter, random.choice(METHODS))).start()\n    >>> while datetime.now() - counter.start_time <= timedelta(minutes=1):\n    >>>     pass\n    >>> print(counter.method_counts)\n    \"\"\"\n    counter = MethodCallCounter()\n    for _ in range(1000):\n        threading.Timer(random.random(), getattr(counter, random.choice(METHODS))).start()\n    while datetime.now() - counter.start_time <= timedelta(minutes=1):\n        pass\n    return counter.method_counts\n\nprint(random_method_calls())\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: functools, random, time\n```python\nfrom functools import wraps\nimport random\nimport time\n\n# Constants\nMETHODS = ['method_1', 'method_2', 'method_3', 'method_4', 'method_5']\n\nclass MethodCallCounter:\n    \"\"\"\n    A class used to count the number of times each method is called and the total time spent on each method.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        The constructor for MethodCallCounter class.\n        \"\"\"\n        self.method_counts = {method: 0 for method in METHODS}\n        self.method_times = {method: 0 for method in METHODS}\n\n    def count_method_calls_and_times(func):\n        \"\"\"\n        A decorator to count the number of times a method is called and the total time spent on the method.\n\n        Parameters:\n        func (function): The function to be decorated.\n\n        Returns:\n        function: The decorated function.\n        \"\"\"\n        @wraps(func)\n        def wrapper(self, *args, **kwargs):\n            start_time = time.time()\n            result = func(self, *args, **kwargs)\n            end_time = time.time()\n            self.method_counts[func.__name__] += 1\n            self.method_times[func.__name__] += end_time - start_time\n            return result\n        return wrapper\n\n    @count_method_calls_and_times\n    def method_1(self):\n        time.sleep(random.random())\n\n    @count_method_calls_and_times\n    def method_2(self):\n        time.sleep(random.random())\n\n    @count_method_calls_and_times\n    def method_3(self):\n        time.sleep(random.random())\n\n    @count_method_calls_and_times\n    def method_4(self):\n        time.sleep(random.random())\n\n    @count_method_calls_and_times\n    def method_5(self):\n        time.sleep(random.random())\n\ndef random_method_calls_and_times():\n    \"\"\"\n    Call methods randomly and return the counts and total times of all method calls.\n\n    Returns:\n    tuple: A tuple with two dictionaries, the first one with the counts and the second one with the total times of all method calls.\n\n    Requirements:\n    - functools\n    - random\n    - time\n\n    Example:\n    >>> counter = MethodCallCounter()\n    >>> for _ in range(1000):\n    >>>     getattr(counter, random.choice(METHODS))()\n    >>> print(counter.method_counts)\n    >>> print(counter.method_times)\n    \"\"\"\n    counter = MethodCallCounter()\n    for _ in range(1000):\n        getattr(counter, random.choice(METHODS))()\n    return counter.method_counts, counter.method_times\n\nprint(random_method_calls_and_times())\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: functools, random, time, threading\n```python\nfrom functools import wraps\nimport random\nimport time\nimport threading\n\n# Constants\nMETHODS = ['method_1', 'method_2', 'method_3', 'method_4', 'method_5']\n\nclass MethodCallCounter:\n    \"\"\"\n    A class used to count the number of times each method is called concurrently and the total time spent on each method.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        The constructor for MethodCallCounter class.\n        \"\"\"\n        self.method_counts = {method: 0 for method in METHODS}\n        self.method_times = {method: 0 for method in METHODS}\n        self.lock = threading.Lock()\n\n    def count_method_calls_and_times(func):\n        \"\"\"\n        A decorator to count the number of times a method is called concurrently and the total time spent on the method.\n\n        Parameters:\n        func (function): The function to be decorated.\n\n        Returns:\n        function: The decorated function.\n        \"\"\"\n        @wraps(func)\n        def wrapper(self, *args, **kwargs):\n            start_time = time.time()\n            result = func(self, *args, **kwargs)\n            end_time = time.time()\n            with self.lock:\n                self.method_counts[func.__name__] += 1\n                self.method_times[func.__name__] += end_time - start_time\n            return result\n        return wrapper\n\n    @count_method_calls_and_times\n    def method_1(self):\n        time.sleep(random.random())\n\n    @count_method_calls_and_times\n    def method_2(self):\n        time.sleep(random.random())\n\n    @count_method_calls_and_times\n    def method_3(self):\n        time.sleep(random.random())\n\n    @count_method_calls_and_times\n    def method_4(self):\n        time.sleep(random.random())\n\n    @count_method_calls_and_times\n    def method_5(self):\n        time.sleep(random.random())\n\ndef random_method_calls_and_times():\n    \"\"\"\n    Call methods randomly and concurrently, and return the counts and total times of all method calls.\n\n    Returns:\n    tuple: A tuple with two dictionaries, the first one with the counts and the second one with the total times of all method calls.\n\n    Requirements:\n    - functools\n    - random\n    - time\n    - threading\n\n    Example:\n    >>> counter = MethodCallCounter()\n    >>> threads = []\n    >>> for _ in range(1000):\n    >>>     t = threading.Thread(target=getattr(counter, random.choice(METHODS)))\n    >>>     threads.append(t)\n    >>>     t.start()\n    >>> for t in threads:\n    >>>     t.join()\n    >>> print(counter.method_counts)\n    >>> print(counter.method_times)\n    \"\"\"\n    counter = MethodCallCounter()\n    threads = []\n    for _ in range(1000):\n        t = threading.Thread(target=getattr(counter, random.choice(METHODS)))\n        threads.append(t)\n        t.start()\n    for t in threads:\n        t.join()\n    return counter.method_counts, counter.method_times\n\nprint(random_method_calls_and_times())\n```"}
{"seed": "{\"task_id\": 38658, \"prompt\": \"def f_38658(img, x):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"im = Image.open(img)\\n\\tim.rotate(x).show()\", \"test_start\": \"\\nfrom PIL import Image\\n\\ndef check(candidate):\", \"test\": [\"\\n    im = Image.new('RGBA', (200, 200), (255, 255, 255, 255))\\n    im.save('v.png')\\n    try:\\n        candidate('v.png', 45)\\n    except:\\n        assert False\\n\"], \"entry_point\": \"f_38658\", \"intent\": \"\\u00bfC\\u00f3mo rotar una imagen `img` en un \\u00e1ngulo de `x` grados en Tkinter?\", \"library\": [\"PIL\"]}", "generation": "Scenario 1: \nPIL, numpy, matplotlib, os\n```python\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\n# Constants\nROTATION_ANGLES = [45, 90, 135, 180, 225, 270, 315]\n\ndef rotate_and_save_all_angles(img_path):\n    \"\"\"\n    Open an image file and rotate it by a list of specified angles. \n    Save each rotated image to a new file.\n\n    Parameters:\n    img_path (str): Path to the image file.\n\n    Returns:\n    list: List of paths to the saved images.\n\n    Requirements:\n    - PIL\n    - numpy\n    - matplotlib.pyplot\n    - os\n\n    Example:\n    >>> rotate_and_save_all_angles('sample.png')\n    \"\"\"\n    im = Image.open(img_path)\n    saved_files = []\n\n    for angle in ROTATION_ANGLES:\n        rotated_im = im.rotate(angle)\n        new_file_path = os.path.splitext(img_path)[0] + '_rotated_{}.png'.format(angle)\n        rotated_im.save(new_file_path)\n        saved_files.append(new_file_path)\n\n    return saved_files\n``` \n`GPT_ODEX_BREAK`\n\nScenario 2:\nPIL, numpy, scipy.ndimage\n```python\nfrom PIL import Image\nimport numpy as np\nfrom scipy.ndimage import gaussian_filter\n\n# Constants\nSIGMA_VALUES = [0.5, 1.0, 2.0, 3.0]\n\ndef apply_gaussian_blur(img_path):\n    \"\"\"\n    Open an image file and apply a Gaussian blur with different sigma values. \n    Convert each filtered image back to PIL image.\n\n    Parameters:\n    img_path (str): Path to the image file.\n\n    Returns:\n    list: List of PIL Image objects.\n\n    Requirements:\n    - PIL\n    - numpy\n    - scipy.ndimage\n\n    Example:\n    >>> blurred_images = apply_gaussian_blur('sample.png')\n    >>> for img in blurred_images:\n    ...     img.show()\n    \"\"\"\n    im = Image.open(img_path)\n    img_arr = np.array(im)\n    filtered_images = []\n\n    for sigma in SIGMA_VALUES:\n        filtered_img_arr = gaussian_filter(img_arr, sigma=sigma)\n        filtered_im = Image.fromarray(filtered_img_arr)\n        filtered_images.append(filtered_im)\n\n    return filtered_images\n``` \n`GPT_ODEX_BREAK`\n\nScenario 3:\nPIL, cv2, numpy\n```python\nfrom PIL import Image\nimport cv2\nimport numpy as np\n\n# Constants\nBLUR_KERNEL_SIZES = [(5, 5), (10, 10), (15, 15)]\n\ndef apply_average_blur(img_path):\n    \"\"\"\n    Open an image file and apply an average blur with different kernel sizes. \n    Convert each blurred image back to PIL image.\n\n    Parameters:\n    img_path (str): Path to the image file.\n\n    Returns:\n    list: List of PIL Image objects.\n\n    Requirements:\n    - PIL\n    - cv2\n    - numpy\n\n    Example:\n    >>> blurred_images = apply_average_blur('sample.png')\n    >>> for img in blurred_images:\n    ...     img.show()\n    \"\"\"\n    im = Image.open(img_path)\n    img_arr = cv2.cvtColor(np.array(im), cv2.COLOR_RGB2BGR)\n    blurred_images = []\n\n    for kernel_size in BLUR_KERNEL_SIZES:\n        blurred_img_arr = cv2.blur(img_arr, kernel_size)\n        blurred_im = Image.fromarray(cv2.cvtColor(blurred_img_arr, cv2.COLOR_BGR2RGB))\n        blurred_images.append(blurred_im)\n\n    return blurred_images\n``` \n`GPT_ODEX_BREAK`\n\nScenario 4:\nPIL, numpy, skimage.transform, matplotlib\n```python\nfrom PIL import Image\nimport numpy as np\nfrom skimage.transform import resize\nimport matplotlib.pyplot as plt\n\n# Constants\nSCALE_FACTORS = [0.5, 0.75, 1.5, 2.0]\n\ndef scale_image(img_path):\n    \"\"\"\n    Open an image file and scale it by different scale factors. \n    Display each scaled image using matplotlib.\n\n    Parameters:\n    img_path (str): Path to the image file.\n\n    Requirements:\n    - PIL\n    - numpy\n    - skimage.transform\n    - matplotlib.pyplot\n\n    Example:\n    >>> scale_image('sample.png')\n    \"\"\"\n    im = Image.open(img_path)\n    img_arr = np.array(im)\n\n    for scale_factor in SCALE_FACTORS:\n        scaled_img_arr = resize(img_arr, (int(im.height * scale_factor), int(im.width * scale_factor)),\n                                mode='reflect', anti_aliasing=True)\n        plt.figure()\n        plt.imshow(scaled_img_arr)\n        plt.title('Scale factor: {}'.format(scale_factor))\n        plt.show()\n``` \n`GPT_ODEX_BREAK`\n\nScenario 5:\nPIL, numpy, cv2\n```python\nfrom PIL import Image\nimport numpy as np\nimport cv2\n\ndef convert_image_to_grayscale(img_path):\n    \"\"\"\n    Open an image file, convert it to grayscale using OpenCV, \n    and then convert it back to a PIL image.\n\n    Parameters:\n    img_path (str): Path to the image file.\n\n    Returns:\n    Image: A PIL Image object.\n\n    Requirements:\n    - PIL\n    - numpy\n    - cv2\n\n    Example:\n    >>> grayscale_image = convert_image_to_grayscale('sample.png')\n    >>> grayscale_image.show()\n    \"\"\"\n    im = Image.open(img_path)\n    img_arr = cv2.cvtColor(np.array(im), cv2.COLOR_RGB2BGR)\n    grayscale_img_arr = cv2.cvtColor(img_arr, cv2.COLOR_BGR2GRAY)\n    grayscale_im = Image.fromarray(grayscale_img_arr)\n\n    return grayscale_im\n```"}
{"seed": "{\"task_id\": 49642, \"prompt\": \"def f_49642(date):\\n\\t\", \"suffix\": \"\\n\\treturn mesANumero(date)\", \"canonical_solution\": \"\\n\\tdef mesANumero(date):\\n\\t    m = {\\n\\t        'enero': '01',\\n\\t        'febrero': '02',\\n\\t        'marzo': '03',\\n\\t        'abril': '04',\\n\\t        'mayo': '05',\\n\\t        'junio': '06',\\n\\t        'julio': '07',\\n\\t        'agosto': '08',\\n\\t        'septiembre': '09',\\n\\t        'octubre': '10',\\n\\t        'noviembre': '11',\\n\\t        'diciembre': '12'\\n\\t        }\\n\\t\\n\\t    fecha = date.split('-')\\n\\t    dia =  fecha[0]\\n\\t    mes =  fecha[1]\\n\\t    anio = fecha[2]\\n\\t\\n\\t    try:\\n\\t        out = str(m[mes.lower()])\\n\\t        return dia + '-' +  out + '-' + anio\\n\\t    except:\\n\\t        raise ValueError('No es un mes')\\n\\t\\n\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('13-noviembre-2007') == '13-11-2007'\\n\"], \"entry_point\": \"f_49642\", \"intent\": \"\\u00bfC\\u00f3mo transformar una fecha `date` en palabras a fecha en n\\u00fameros?\", \"library\": []}", "generation": "Scenario 1:\npandas, datetime, matplotlib, seaborn, random\n```python\nimport pandas as pd\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom random import choice\n\n# Constants\nMONTHS = {\n    'enero': '01',\n    'febrero': '02',\n    'marzo': '03',\n    'abril': '04',\n    'mayo': '05',\n    'junio': '06',\n    'julio': '07',\n    'agosto': '08',\n    'septiembre': '09',\n    'octubre': '10',\n    'noviembre': '11',\n    'diciembre': '12'\n}\n\ndef plot_birthdays(birthdays):\n    \"\"\"\n    Plot a histogram of birthdays by month given a list of birthdays in the format \"dd-nombre_mes-aaaa\".\n\n    Parameters:\n    birthdays (list): The list of birthdays.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - datetime\n    - matplotlib.pyplot\n    - seaborn\n    - random\n\n    Example:\n    >>> birthdays = ['13-noviembre-2007', '15-febrero-1996', '20-marzo-1989', '25-julio-2000']\n    >>> plot_birthdays(birthdays)\n    \"\"\"\n    birthday_dates = [datetime.strptime(f'{day}-{MONTHS[month.lower()]}-{year}', '%d-%m-%Y') for day, month, year in (bd.split('-') for bd in birthdays)]\n    birth_months = [bd.month for bd in birthday_dates]\n\n    plt.figure(figsize=(10, 6))\n    sns.countplot(birth_months, palette='viridis')\n    plt.xlabel('Month')\n    plt.ylabel('Count')\n    plt.title('Birthdays by Month')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ndatetime, pandas, matplotlib, seaborn\n```python\nfrom datetime import datetime\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nMONTHS = {\n    'enero': '01',\n    'febrero': '02',\n    'marzo': '03',\n    'abril': '04',\n    'mayo': '05',\n    'junio': '06',\n    'julio': '07',\n    'agosto': '08',\n    'septiembre': '09',\n    'octubre': '10',\n    'noviembre': '11',\n    'diciembre': '12'\n}\n\ndef plot_sales(sales_data):\n    \"\"\"\n    Plot a line graph of sales over time given a list of sales data where each entry is a tuple of\n    (date_in_words, sales) where date_in_words is a date in the format \"dd-nombre_mes-aaaa\".\n\n    Parameters:\n    sales_data (list): The list of sales data.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - datetime\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> sales_data = [('13-noviembre-2007', 100), ('15-febrero-1996', 200), ('20-marzo-1989', 150), ('25-julio-2000', 120)]\n    >>> plot_sales(sales_data)\n    \"\"\"\n    sales_df = pd.DataFrame(sales_data, columns=['Date', 'Sales'])\n\n    # Convert date in words to datetime\n    sales_df['Date'] = sales_df['Date'].apply(lambda x: datetime.strptime(f'{day}-{MONTHS[month.lower()]}-{year}', '%d-%m-%Y') for day, month, year in x.split('-'))\n\n    sales_df = sales_df.sort_values('Date')\n\n    plt.figure(figsize=(10, 6))\n    sns.lineplot(data=sales_df, x='Date', y='Sales')\n    plt.title('Sales Over Time')\n    plt.show()\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 3:\ndatetime, pandas, numpy, random\n```python\nfrom datetime import datetime\nimport pandas as pd\nimport numpy as np\nfrom random import choices\n\n# Constants\nMONTHS = {\n    'enero': '01',\n    'febrero': '02',\n    'marzo': '03',\n    'abril': '04',\n    'mayo': '05',\n    'junio': '06',\n    'julio': '07',\n    'agosto': '08',\n    'septiembre': '09',\n    'octubre': '10',\n    'noviembre': '11',\n    'diciembre': '12'\n}\n\ndef generate_random_dates(start_date, end_date, n):\n    \"\"\"\n    Generate n random dates between start_date and end_date in the format \"dd-nombre_mes-aaaa\".\n\n    Parameters:\n    start_date (str): The start date in the format \"dd-nombre_mes-aaaa\".\n    end_date (str): The end date in the format \"dd-nombre_mes-aaaa\".\n    n (int): The number of random dates to generate.\n\n    Returns:\n    list: The list of generated dates.\n\n    Requirements:\n    - datetime\n    - pandas\n    - numpy\n    - random\n\n    Example:\n    >>> generate_random_dates('01-enero-2000', '31-diciembre-2020', 5)\n    \"\"\"\n    start_date = datetime.strptime(start_date, '%d-%m-%Y')\n    end_date = datetime.strptime(end_date, '%d-%m-%Y')\n\n    date_range = pd.date_range(start_date, end_date).to_pydatetime().tolist()\n    random_dates = choices(date_range, k=n)\n\n    return [f'{rd.day}-{MONTHS[rd.strftime('%m')]}-{rd.year}' for rd in random_dates]\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 4:\ndatetime, pandas, random\n```python\nfrom datetime import datetime\nimport pandas as pd\nfrom random import choice\n\n# Constants\nMONTHS = {\n    'enero': '01',\n    'febrero': '02',\n    'marzo': '03',\n    'abril': '04',\n    'mayo': '05',\n    'junio': '06',\n    'julio': '07',\n    'agosto': '08',\n    'septiembre': '09',\n    'octubre': '10',\n    'noviembre': '11',\n    'diciembre': '12'\n}\n\ndef select_random_dates(dates, n):\n    \"\"\"\n    Select n random dates from a list of dates in the format \"dd-nombre_mes-aaaa\".\n\n    Parameters:\n    dates (list): The list of dates.\n    n (int): The number of random dates to select.\n\n    Returns:\n    list: The list of selected dates.\n\n    Requirements:\n    - datetime\n    - pandas\n    - random\n\n    Example:\n    >>> dates = ['13-noviembre-2007', '15-febrero-1996', '20-marzo-1989', '25-julio-2000']\n    >>> select_random_dates(dates, 2)\n    \"\"\"\n    return choices(dates, k=n)\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 5:\ndatetime, json, requests\n```python\nfrom datetime import datetime\nimport json\nimport requests\n\n# Constants\nMONTHS = {\n    'enero': '01',\n    'febrero': '02',\n    'marzo': '03',\n    'abril': '04',\n    'mayo': '05',\n    'junio': '06',\n    'julio': '07',\n    'agosto': '08',\n    'septiembre': '09',\n    'octubre': '10',\n    'noviembre': '11',\n    'diciembre': '12'\n}\n\ndef get_weather_on_date(city, date):\n    \"\"\"\n    Get the weather in a city on a specific date in the format \"dd-nombre_mes-aaaa\" using a weather API.\n\n    Parameters:\n    city (str): The city.\n    date (str): The date.\n\n    Returns:\n    dict: The weather data.\n\n    Requirements:\n    - datetime\n    - json\n    - requests\n\n    Example:\n    >>> get_weather_on_date('New York', '13-noviembre-2007')\n    \"\"\"\n    date = datetime.strptime(date, '%d-%m-%Y')\n\n    response = requests.get(f'https://api.weatherapi.com/v1/history.json?key=YOUR_API_KEY&q={city}&dt={date.strftime(\"%Y-%m-%d\")}')\n    weather_data = json.loads(response.text)\n\n    return weather_data\n```"}
{"seed": "{\"task_id\": 2611, \"prompt\": \"def f_2611():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"subprocess.Popen('proc')\", \"test_start\": \"\\nimport subprocess\\nfrom unittest.mock import Mock\\n\\ndef check(candidate):\", \"test\": [\"\\n    subprocess.Popen = Mock()\\n    try:\\n        candidate()\\n    except:\\n        assert False\\n\"], \"entry_point\": \"f_2611\", \"intent\": \"\\u00bfC\\u00f3mo crar un subproceso `proc`?\", \"library\": [\"subprocess\"]}", "generation": "Scenario 1:\nsubprocess, os, glob, csv\n```python\nimport subprocess\nimport os\nimport glob\nimport csv\n\n# Constants\nTEXT_FILE_PATH = '/path/to/text/files/*.txt'\nCSV_OUTPUT_PATH = '/path/to/output.csv'\n\ndef convert_txt_to_csv():\n    \"\"\"\n    Convert all text files in a given directory into a single CSV file using a subprocess.\n    \n    Returns:\n    str: The path to the output CSV file.\n    \n    Requirements:\n    - subprocess\n    - os\n    - glob\n    - csv\n    \n    Example:\n    >>> convert_txt_to_csv()\n    '/path/to/output.csv'\n    \"\"\"\n    csv_output = open(CSV_OUTPUT_PATH, 'w')\n    csv_writer = csv.writer(csv_output)\n\n    for txt_file in glob.glob(TEXT_FILE_PATH):\n        subprocess.run(['cat', txt_file], stdout=csv_output)\n\n    csv_output.close()\n\n    return CSV_OUTPUT_PATH\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nsubprocess, shlex, datetime, pytz\n```python\nimport subprocess\nimport shlex\nfrom datetime import datetime\nimport pytz\n\n# Constants\nDATE_FORMAT = '%Y-%m-%d %H:%M:%S %Z%z'\n\ndef get_system_info_and_time(timezone):\n    \"\"\"\n    Get system information and the current datetime in a given timezone using subprocess.\n    \n    Parameters:\n    timezone (str): The timezone.\n    \n    Returns:\n    dict: A dictionary with system information and current datetime.\n    \n    Requirements:\n    - subprocess\n    - shlex\n    - datetime\n    - pytz\n    \n    Example:\n    >>> get_system_info_and_time('Asia/Kolkata')\n    \"\"\"\n    command = shlex.split('uname -a')\n    uname_output = subprocess.check_output(command)\n    uname_info = uname_output.decode('utf-8').strip()\n\n    tz = pytz.timezone(timezone)\n    now = datetime.now(tz)\n\n    return {'System Info': uname_info, 'Current Time': now.strftime(DATE_FORMAT)}\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nsubprocess, sys, os, argparse\n```python\nimport subprocess\nimport sys\nimport os\nimport argparse\n\n# Constants\nBACKUP_DIR = '/path/to/backup'\n\ndef backup_files(file_paths):\n    \"\"\"\n    Backup a list of files to a backup directory using subprocess.\n    \n    Parameters:\n    file_paths (list of str): The list of file paths to backup.\n    \n    Returns:\n    list of str: The list of backup file paths.\n    \n    Requirements:\n    - subprocess\n    - sys\n    - os\n    - argparse\n    \n    Example:\n    >>> backup_files(['/path/to/file1.txt', '/path/to/file2.txt'])\n    \"\"\"\n    backup_paths = []\n\n    for file_path in file_paths:\n        if not os.path.isfile(file_path):\n            print(f\"Error: {file_path} does not exist.\", file=sys.stderr)\n            continue\n\n        backup_path = os.path.join(BACKUP_DIR, os.path.basename(file_path))\n        subprocess.run(['cp', file_path, backup_path])\n        backup_paths.append(backup_path)\n\n    return backup_paths\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nsubprocess, sys, shlex, time\n```python\nimport subprocess\nimport sys\nimport shlex\nimport time\n\ndef run_command_with_timeout(command, timeout):\n    \"\"\"\n    Run a command with a specified timeout using subprocess.\n    \n    Parameters:\n    command (str): The command to run.\n    timeout (int): The timeout in seconds.\n    \n    Returns:\n    bool: True if the command finished successfully, False otherwise.\n    \n    Requirements:\n    - subprocess\n    - sys\n    - shlex\n    - time\n    \n    Example:\n    >>> run_command_with_timeout('sleep 10', 5)\n    \"\"\"\n    start = time.time()\n    process = subprocess.Popen(shlex.split(command))\n\n    while process.poll() is None:\n        if time.time() - start > timeout:\n            process.kill()\n            return False\n        time.sleep(0.1)\n\n    return True\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nsubprocess, shlex, random, string\n```python\nimport subprocess\nimport shlex\nimport random\nimport string\n\ndef generate_random_file(file_path, size):\n    \"\"\"\n    Generate a file with random content of a given size using subprocess.\n    \n    Parameters:\n    file_path (str): The path to the file.\n    size (int): The size of the file in bytes.\n    \n    Returns:\n    str: The path to the generated file.\n    \n    Requirements:\n    - subprocess\n    - shlex\n    - random\n    - string\n    \n    Example:\n    >>> generate_random_file('/path/to/file.txt', 1024)\n    \"\"\"\n    command = f'dd if=/dev/urandom of={file_path} bs=1 count={size}'\n    subprocess.run(shlex.split(command))\n\n    return file_path\n```\n`GPT_ODEX_BREAK`"}
{"seed": "{\"task_id\": 71600, \"prompt\": \"def f_71600(n, m):\\n\\t\", \"suffix\": \"\\n\\treturn array\", \"canonical_solution\": \"\\n\\tarray = []\\n\\tfor count in range(n):\\n\\t\\trow = []\\n\\t\\tfor count in range(m):\\n\\t\\t\\tm = matriz()\\n\\t\\t\\trow.append(m)\\n\\t\\tarray.append(row)\\n\\n\", \"test_start\": \"\\ndef matriz(): return 0\\n\\ndef check(candidate):\", \"test\": [\"\\n    e = candidate(2, 2)\\n    assert len(e) == 2\\n    assert len(e[0]) == 2\\n\"], \"entry_point\": \"f_71600\", \"intent\": \"\\u00bfC\\u00f3mo crear una matriz `array` de `n`x`m` matrices?\", \"library\": []}", "generation": "Scenario 1:\nnumpy, pandas, random, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nROWS = 10\nCOLS = 10\nRANGE = (0, 100)\n\ndef generate_random_matrix(ROWS, COLS, RANGE):\n    \"\"\"\n    Generate a 2D array of random integers within a specified range and plot a \n    heatmap of the array using matplotlib.\n\n    Parameters:\n    ROWS (int): The number of rows in the array.\n    COLS (int): The number of columns in the array.\n    RANGE (tuple): A tuple specifying the inclusive range of random integers.\n\n    Returns:\n    numpy.ndarray: The generated 2D array.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> matrix = generate_random_matrix(ROWS, COLS, RANGE)\n    >>> plt.imshow(matrix, cmap='hot', interpolation='nearest')\n    >>> plt.show()\n    \"\"\"\n    matrix = np.array([[randint(*RANGE) for _ in range(COLS)] for _ in range(ROWS)])\n    return matrix\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, random, itertools\n```python\nimport numpy as np\nfrom random import randint\nimport itertools\n\n# Constants\nROWS = 5\nCOLS = 5\nRANGE = (0, 10)\n\ndef create_matrix_combinations(ROWS, COLS, RANGE):\n    \"\"\"\n    Create a matrix of given dimensions with random integers within a specified range \n    and return all unique combinations of two rows.\n\n    Parameters:\n    ROWS (int): The number of rows in the matrix.\n    COLS (int): The number of columns in the matrix.\n    RANGE (tuple): A tuple specifying the inclusive range of random integers.\n\n    Returns:\n    list: A list of tuples, where each tuple is a unique combination of two rows from the matrix.\n\n    Requirements:\n    - numpy\n    - random\n    - itertools\n\n    Example:\n    >>> combinations = create_matrix_combinations(ROWS, COLS, RANGE)\n    >>> print(combinations)\n    \"\"\"\n    matrix = np.array([[randint(*RANGE) for _ in range(COLS)] for _ in range(ROWS)])\n    combinations = list(itertools.combinations(matrix, 2))\n    return combinations\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, math, matplotlib\n```python\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\n\n# Constants\nROWS = 5\nCOLS = 5\n\ndef generate_sin_cos_matrix(ROWS, COLS):\n    \"\"\"\n    Create a matrix of given dimensions where each element is the sine of its row index \n    and the cosine of its column index, and plot the matrix as a 3D surface plot.\n\n    Parameters:\n    ROWS (int): The number of rows in the matrix.\n    COLS (int): The number of columns in the matrix.\n\n    Returns:\n    numpy.ndarray: The generated matrix.\n\n    Requirements:\n    - numpy\n    - math\n    - matplotlib.pyplot\n\n    Example:\n    >>> matrix = generate_sin_cos_matrix(ROWS, COLS)\n    >>> fig = plt.figure()\n    >>> ax = fig.add_subplot(111, projection='3d')\n    >>> xs = np.arange(ROWS)\n    >>> ys = np.arange(COLS)\n    >>> X, Y = np.meshgrid(xs, ys)\n    >>> ax.plot_surface(X, Y, matrix)\n    >>> plt.show()\n    \"\"\"\n    matrix = np.array([[math.sin(i) * math.cos(j) for j in range(COLS)] for i in range(ROWS)])\n    return matrix\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, random, matplotlib\n```python\nimport numpy as np\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nROWS = 10\nCOLS = 10\nRANGE = (0, 100)\n\ndef generate_and_plot_random_matrix(ROWS, COLS, RANGE):\n    \"\"\"\n    Generate a 2D array of random integers within a specified range and plot a \n    scatter plot of the array using matplotlib.\n\n    Parameters:\n    ROWS (int): The number of rows in the array.\n    COLS (int): The number of columns in the array.\n    RANGE (tuple): A tuple specifying the inclusive range of random integers.\n\n    Returns:\n    numpy.ndarray: The generated 2D array.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> matrix = generate_and_plot_random_matrix(ROWS, COLS, RANGE)\n    >>> x = np.arange(ROWS)\n    >>> y = np.arange(COLS)\n    >>> plt.scatter(x, y, c=matrix.flatten())\n    >>> plt.show()\n    \"\"\"\n    matrix = np.array([[randint(*RANGE) for _ in range(COLS)] for _ in range(ROWS)])\n    return matrix\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, scipy.linalg, matplotlib\n```python\nimport numpy as np\nfrom scipy.linalg import svd\nimport matplotlib.pyplot as plt\n\n# Constants\nROWS = 5\nCOLS = 5\n\ndef perform_svd_and_plot_singular_values(ROWS, COLS):\n    \"\"\"\n    Create a matrix of given dimensions with random floats, perform Singular Value Decomposition \n    (SVD) on the matrix, and plot the singular values.\n\n    Parameters:\n    ROWS (int): The number of rows in the matrix.\n    COLS (int): The number of columns in the matrix.\n\n    Returns:\n    tuple: A tuple (U, s, Vh), where U, Vh are the unitary matrices, and s is a 1-D array of the matrix's singular values.\n\n    Requirements:\n    - numpy\n    - scipy.linalg\n    - matplotlib.pyplot\n\n    Example:\n    >>> U, s, Vh = perform_svd_and_plot_singular_values(ROWS, COLS)\n    >>> plt.plot(s)\n    >>> plt.show()\n    \"\"\"\n    matrix = np.random.rand(ROWS, COLS)\n    U, s, Vh = svd(matrix)\n    return U, s, Vh\n```"}
{"seed": "{\"task_id\": 37964, \"prompt\": \"def f_37964(mi_lista):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"[(x,y) for x in mi_lista for y in mi_lista if x!=y]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([5, 6, 7, 8]) == [(5, 6), (5, 7), (5, 8), (6, 5), (6, 7), (6, 8), (7, 5), (7, 6), (7, 8), (8, 5), (8, 6), (8, 7)]\\n\"], \"entry_point\": \"f_37964\", \"intent\": \"\\u00bfC\\u00f3mo generar permutaciones de una lista `mi_lista` permitiendo repetir elementos y sin repetirlos?\", \"library\": []}", "generation": "Scenario 1: \nnumpy, itertools, scipy.stats\n```python\nimport numpy as np\nimport itertools\nfrom scipy.stats import pearsonr\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef compute_correlations(data):\n    \"\"\"\n    Generate a correlation matrix between the columns of a given numpy array.\n    The correlations are computed using the Pearson correlation coefficient.\n    \n    Parameters:\n    data (np.array): The input data, where each row is a data point and columns are variables.\n    \n    Returns:\n    np.array: A symmetric 2D array with correlation coefficients.\n    \n    Requirements:\n    - numpy\n    - itertools\n    - scipy.stats.pearsonr\n    \n    Example:\n    >>> data = np.random.rand(100, 5)\n    >>> correlations = compute_correlations(data)\n    >>> print(correlations)\n    \"\"\"\n    n_vars = data.shape[1]\n    correlations = np.zeros((n_vars, n_vars))\n\n    for i, j in itertools.combinations(range(n_vars), 2):\n        corr, _ = pearsonr(data[:, i], data[:, j])\n        correlations[i, j] = correlations[j, i] = corr\n\n    return correlations\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nnumpy, pandas, itertools\n```python\nimport numpy as np\nimport pandas as pd\nimport itertools\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef compute_sums(df):\n    \"\"\"\n    Compute all possible sums of two columns in a pandas DataFrame.\n    \n    Parameters:\n    df (pd.DataFrame): The input DataFrame.\n    \n    Returns:\n    pd.DataFrame: A DataFrame with the sums of all pairs of columns.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - itertools\n    \n    Example:\n    >>> df = pd.DataFrame(np.random.rand(100, 5), columns=COLUMNS)\n    >>> sums = compute_sums(df)\n    >>> print(sums)\n    \"\"\"\n    sums_df = pd.DataFrame()\n\n    for col1, col2 in itertools.combinations(df.columns, 2):\n        sums_df[f'{col1}+{col2}'] = df[col1] + df[col2]\n\n    return sums_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnumpy, pandas, matplotlib, seaborn\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef plot_distributions(df):\n    \"\"\"\n    Plot the distributions of all variables in a DataFrame using histograms and density plots.\n    \n    Parameters:\n    df (pd.DataFrame): The input DataFrame.\n    \n    Returns:\n    None: This function does not return anything.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - seaborn\n    \n    Example:\n    >>> df = pd.DataFrame(np.random.rand(100, 5), columns=COLUMNS)\n    >>> plot_distributions(df)\n    \"\"\"\n    for col in df.columns:\n        plt.figure(figsize=(8, 6))\n        sns.histplot(df[col], kde=True)\n        plt.title(f'Distribution of {col}')\n        plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, pandas, scipy.stats\n```python\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import ttest_ind\n\n# Constants\nGROUPS = ['A', 'B', 'C', 'D', 'E']\n\ndef perform_t_tests(df, group_col, value_col):\n    \"\"\"\n    Perform independent sample t-tests between all pairs of groups in a DataFrame.\n    \n    Parameters:\n    df (pd.DataFrame): The input DataFrame.\n    group_col (str): The column name that contains group identifiers.\n    value_col (str): The column name that contains the values to compare.\n    \n    Returns:\n    pd.DataFrame: A DataFrame with the t-test results.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - scipy.stats.ttest_ind\n    \n    Example:\n    >>> df = pd.DataFrame({\n    >>>     'group': np.random.choice(GROUPS, 500),\n    >>>     'value': np.random.rand(500)\n    >>> })\n    >>> t_tests = perform_t_tests(df, 'group', 'value')\n    >>> print(t_tests)\n    \"\"\"\n    results = []\n\n    for group1, group2 in itertools.combinations(df[group_col].unique(), 2):\n        group1_values = df.loc[df[group_col] == group1, value_col]\n        group2_values = df.loc[df[group_col] == group2, value_col]\n        t_stat, p_value = ttest_ind(group1_values, group2_values)\n        results.append([group1, group2, t_stat, p_value])\n\n    results_df = pd.DataFrame(results, columns=['Group1', 'Group2', 'T-Statistic', 'P-Value'])\n\n    return results_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnumpy, pandas, sklearn.preprocessing\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef standardize_variables(df):\n    \"\"\"\n    Standardize all variables in a DataFrame to have zero mean and unit variance.\n    \n    Parameters:\n    df (pd.DataFrame): The input DataFrame.\n    \n    Returns:\n    pd.DataFrame: A DataFrame with standardized variables.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.preprocessing.StandardScaler\n    \n    Example:\n    >>> df = pd.DataFrame(np.random.rand(100, 5), columns=COLUMNS)\n    >>> standardized_df = standardize_variables(df)\n    >>> print(standardized_df)\n    \"\"\"\n    scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(df)\n\n    standardized_df = pd.DataFrame(standardized_data, columns=df.columns)\n\n    return standardized_df\n```"}
{"seed": "{\"task_id\": 54164, \"prompt\": \"def f_54164(dic):\\n\\t\", \"suffix\": \"\\n\\treturn list\", \"canonical_solution\": \"list = [] \\n\\tfor id,mensaje in dic.items():\\n\\t    list.append([mensaje['Lon'],mensaje['Lat']])\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    dic = {3: {'Lat': 32.144, 'Lon':-112.56}, 7: {'Lat': 21.144, 'Lon': 98.56}}\\n    list = candidate(dic)\\n    assert len(list) == 2\\n\"], \"entry_point\": \"f_54164\", \"intent\": \"\\u00bfC\\u00f3mo crear una lista `list` de tuplas con los valores `Lon` y `Lat` de un diccionario `dic`?\", \"library\": []}", "generation": "Scenario 1:\npandas, matplotlib, numpy, random\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom random import uniform\n\n# Constants\nNUM_POINTS = 100\nLON_RANGE = (-180, 180)\nLAT_RANGE = (-90, 90)\n\ndef plot_random_geocoordinates(dic):\n    \"\"\"\n    Plot random geocoordinates on a scatter plot, where the coordinates are \n    generated based on the Lon and Lat ranges in a given dictionary.\n\n    Parameters:\n    dic (dict): A dictionary with 'Lon' and 'Lat' keys, each containing a tuple \n    of min and max range for the coordinate. Example: {'Lon': (-180, 180), 'Lat': (-90, 90)}\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - numpy\n    - random\n\n    Example:\n    >>> dic = {'Lon': (-180, 180), 'Lat': (-90, 90)}\n    >>> plot_random_geocoordinates(dic)\n    \"\"\"\n    lon_min, lon_max = dic['Lon']\n    lat_min, lat_max = dic['Lat']\n\n    lon_data = [uniform(lon_min, lon_max) for _ in range(NUM_POINTS)]\n    lat_data = [uniform(lat_min, lat_max) for _ in range(NUM_POINTS)]\n\n    plt.scatter(lon_data, lat_data)\n    plt.xlabel('Longitude')\n    plt.ylabel('Latitude')\n    plt.title('Random Geocoordinates')\n    plt.grid(True)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, matplotlib, random\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randrange\n\n# Constants\nCITIES = ['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']\n\ndef plot_city_coordinates(dic):\n    \"\"\"\n    Plot the coordinates of a list of cities on a scatter plot, where the coordinates are \n    randomly generated based on the Lon and Lat ranges in a given dictionary.\n\n    Parameters:\n    dic (dict): A dictionary with 'Lon' and 'Lat' keys, each containing a tuple \n    of min and max range for the coordinate. Example: {'Lon': (-180, 180), 'Lat': (-90, 90)}\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> dic = {'Lon': (-180, 180), 'Lat': (-90, 90)}\n    >>> plot_city_coordinates(dic)\n    \"\"\"\n    lon_min, lon_max = dic['Lon']\n    lat_min, lat_max = dic['Lat']\n\n    data = {'City': [], 'Longitude': [], 'Latitude': []}\n    for city in CITIES:\n        data['City'].append(city)\n        data['Longitude'].append(randrange(lon_min, lon_max))\n        data['Latitude'].append(randrange(lat_min, lat_max))\n\n    df = pd.DataFrame(data)\n\n    plt.scatter(df['Longitude'], df['Latitude'])\n    for i, city in enumerate(df['City']):\n        plt.annotate(city, (df['Longitude'][i], df['Latitude'][i]))\n    plt.xlabel('Longitude')\n    plt.ylabel('Latitude')\n    plt.title('City Coordinates')\n    plt.grid(True)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, folium\n```python\nimport pandas as pd\nimport numpy as np\nimport folium\n\n# Constants\nCITIES = ['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']\n\ndef create_map_with_city_markers(dic):\n    \"\"\"\n    Create a map with markers for a list of cities, where the coordinates \n    are randomly generated based on the Lon and Lat ranges in a given dictionary.\n\n    Parameters:\n    dic (dict): A dictionary with 'Lon' and 'Lat' keys, each containing a tuple \n    of min and max range for the coordinate. Example: {'Lon': (-180, 180), 'Lat': (-90, 90)}\n\n    Returns:\n    folium.Map: A folium Map object with markers for the cities.\n\n    Requirements:\n    - pandas\n    - numpy\n    - folium\n\n    Example:\n    >>> dic = {'Lon': (-180, 180), 'Lat': (-90, 90)}\n    >>> map = create_map_with_city_markers(dic)\n    \"\"\"\n    lon_min, lon_max = dic['Lon']\n    lat_min, lat_max = dic['Lat']\n\n    data = {'City': [], 'Longitude': [], 'Latitude': []}\n    for city in CITIES:\n        data['City'].append(city)\n        data['Longitude'].append(np.random.uniform(lon_min, lon_max))\n        data['Latitude'].append(np.random.uniform(lat_min, lat_max))\n\n    df = pd.DataFrame(data)\n\n    m = folium.Map(location=[0, 0], zoom_start=2)\n    for i in range(0, len(df)):\n        folium.Marker([df.iloc[i]['Latitude'], df.iloc[i]['Longitude']], popup=df.iloc[i]['City']).add_to(m)\n\n    return m\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, geopandas, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\n\n# Constants\nCITIES = ['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']\n\ndef create_geopandas_dataframe(dic):\n    \"\"\"\n    Create a GeoPandas DataFrame for a list of cities, where the coordinates \n    are randomly generated based on the Lon and Lat ranges in a given dictionary.\n\n    Parameters:\n    dic (dict): A dictionary with 'Lon' and 'Lat' keys, each containing a tuple \n    of min and max range for the coordinate. Example: {'Lon': (-180, 180), 'Lat': (-90, 90)}\n\n    Returns:\n    GeoDataFrame: A GeoPandas DataFrame with the cities and their coordinates.\n\n    Requirements:\n    - pandas\n    - numpy\n    - geopandas\n    - matplotlib\n\n    Example:\n    >>> dic = {'Lon': (-180, 180), 'Lat': (-90, 90)}\n    >>> gdf = create_geopandas_dataframe(dic)\n    \"\"\"\n    lon_min, lon_max = dic['Lon']\n    lat_min, lat_max = dic['Lat']\n\n    data = {'City': [], 'Coordinates': []}\n    for city in CITIES:\n        data['City'].append(city)\n        data['Coordinates'].append(Point(np.random.uniform(lon_min, lon_max), np.random.uniform(lat_min, lat_max)))\n\n    gdf = gpd.GeoDataFrame(data, geometry='Coordinates')\n\n    return gdf\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, matplotlib, random\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import uniform\n\n# Constants\nNUM_POINTS = 100\nLON_RANGE = (-180, 180)\nLAT_RANGE = (-90, 90)\n\ndef plot_random_geocoordinates_histogram(dic):\n    \"\"\"\n    Plot a histogram of random geocoordinates, where the coordinates are \n    generated based on the Lon and Lat ranges in a given dictionary.\n\n    Parameters:\n    dic (dict): A dictionary with 'Lon' and 'Lat' keys, each containing a tuple \n    of min and max range for the coordinate. Example: {'Lon': (-180, 180), 'Lat': (-90, 90)}\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> dic = {'Lon': (-180, 180), 'Lat': (-90, 90)}\n    >>> plot_random_geocoordinates_histogram(dic)\n    \"\"\"\n    lon_min, lon_max = dic['Lon']\n    lat_min, lat_max = dic['Lat']\n\n    lon_data = [uniform(lon_min, lon_max) for _ in range(NUM_POINTS)]\n    lat_data = [uniform(lat_min, lat_max) for _ in range(NUM_POINTS)]\n\n    plt.hist2d(lon_data, lat_data, bins=30, cmap='plasma')\n    plt.colorbar(label='Counts')\n    plt.xlabel('Longitude')\n    plt.ylabel('Latitude')\n    plt.title('Histogram of Random Geocoordinates')\n    plt.grid(True)\n    plt.show()\n```"}
{"seed": "{\"task_id\": 54164, \"prompt\": \"def f_54164(dic):\\n\\t\", \"suffix\": \"\\n\\treturn list\", \"canonical_solution\": \"list = [[v['Lon'],v['Lat']] for k, v in dic.items()]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    dic = {3: {'Lat': 32.144, 'Lon':-112.56}, 7: {'Lat': 21.144, 'Lon': 98.56}}\\n    list = candidate(dic)\\n    assert len(list) == 2\\n\"], \"entry_point\": \"f_54164\", \"intent\": \"\\u00bfC\\u00f3mo crear una lista `list` de tuplas con los valores `Lon` y `Lat` de un diccionario `dic`?\", \"library\": []}", "generation": "Scenario 1: \npandas, geopy, folium, math\n```python\nimport pandas as pd\nfrom geopy.distance import geodesic\nimport folium\nimport math\n\n# Constants\nEARTH_RADIUS = 6371\n\ndef create_map_and_calculate_distance(dic):\n    \"\"\"\n    Create a map showing the locations specified in a dictionary and calculate the distances \n    between them.\n\n    Parameters:\n    dic (dict): The dictionary with keys as location names and values as dictionaries containing \n                'Lat' and 'Lon' keys.\n\n    Returns:\n    tuple: A tuple containing the created map and a dictionary with keys as location pairs and \n           values as distance between them in kilometers.\n\n    Requirements:\n    - pandas\n    - geopy.distance\n    - folium\n    - math\n\n    Example:\n    >>> dic = {'New York': {'Lat': 40.7128, 'Lon': -74.0060}, 'Los Angeles': {'Lat': 34.0522, 'Lon': -118.2437}}\n    >>> map, distances = create_map_and_calculate_distance(dic)\n    >>> print(distances)\n    >>> map.save(\"map.html\")\n    \"\"\"\n    locations = [(k, v['Lat'], v['Lon']) for k, v in dic.items()]\n    distances = {}\n    \n    # Create a map centered at the first location\n    folium_map = folium.Map(location=[locations[0][1], locations[0][2]], zoom_start=4)\n\n    for i in range(len(locations)):\n        # Add a marker for each location\n        folium.Marker([locations[i][1], locations[i][2]], popup=locations[i][0]).add_to(folium_map)\n        \n        for j in range(i+1, len(locations)):\n            # Calculate distance using the haversine formula\n            lat1, lon1 = math.radians(locations[i][1]), math.radians(locations[i][2])\n            lat2, lon2 = math.radians(locations[j][1]), math.radians(locations[j][2])\n            a = math.sin((lat2-lat1)/2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin((lon2-lon1)/2)**2\n            d = 2 * EARTH_RADIUS * math.asin(math.sqrt(a))\n            \n            distances[(locations[i][0], locations[j][0])] = d\n\n    return folium_map, distances\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, matplotlib, random\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nLOCATIONS = ['City A', 'City B', 'City C', 'City D', 'City E']\nPOPULATION_SIZES = [randint(100000, 1000000) for _ in range(len(LOCATIONS))]\n\ndef create_population_distribution(dic):\n    \"\"\"\n    Create a pie chart showing the population distribution of different locations.\n\n    Parameters:\n    dic (dict): The dictionary with keys as location names and values as dictionaries containing \n                'Lat' and 'Lon' keys.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> dic = {'City A': {'Lat': 51.5074, 'Lon': -0.1278}, 'City B': {'Lat': 34.0522, 'Lon': -118.2437}, 'City C': {'Lat': 48.8566, 'Lon': 2.3522}, 'City D': {'Lat': 35.6895, 'Lon': 139.6917}, 'City E': {'Lat': -33.8688, 'Lon': 151.2093}}\n    >>> create_population_distribution(dic)\n    \"\"\"\n    # Create a DataFrame from the dictionary\n    df = pd.DataFrame(dic.values(), index=dic.keys(), columns=['Lat', 'Lon'])\n    \n    # Add population size to the DataFrame\n    df['Population'] = POPULATION_SIZES\n\n    # Plot a pie chart\n    plt.pie(df['Population'], labels=df.index, autopct='%1.1f%%')\n    plt.title('Population Distribution')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \npandas, folium, geopy\n```python\nimport pandas as pd\nimport folium\nfrom geopy.geocoders import Nominatim\n\n# Constants\nGEOLOCATOR = Nominatim(user_agent=\"geoapiExercises\")\n\ndef create_location_map(dic):\n    \"\"\"\n    Create a map showing the locations from a dictionary with 'Lat' and 'Lon' values.\n\n    Parameters:\n    dic (dict): The dictionary with keys as location names and values as dictionaries containing \n                'Lat' and 'Lon' keys.\n\n    Returns:\n    folium.Map: A folium Map object with the locations from the dictionary marked.\n\n    Requirements:\n    - pandas\n    - folium\n    - geopy.geocoders.Nominatim\n\n    Example:\n    >>> dic = {'New York': {'Lat': 40.7128, 'Lon': -74.0060}, 'Los Angeles': {'Lat': 34.0522, 'Lon': -118.2437}}\n    >>> map = create_location_map(dic)\n    >>> map.save(\"map.html\")\n    \"\"\"\n    # Create a map centered at the first location\n    folium_map = folium.Map(location=list(dic.values())[0], zoom_start=4)\n\n    for location, coord in dic.items():\n        # Add a marker for each location\n        folium.Marker([coord['Lat'], coord['Lon']], popup=location).add_to(folium_map)\n\n    return folium_map\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, matplotlib, collections\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n\n# Constants\nCATEGORIES = ['Restaurant', 'Grocery Store', 'Pharmacy', 'Gas Station', 'Bank']\n\ndef categorize_locations_and_plot(dic):\n    \"\"\"\n    Categorize the places in a dictionary based on their 'Lat' and 'Lon' values and plot the \n    distribution of categories.\n\n    Parameters:\n    dic (dict): The dictionary with keys as location names and values as dictionaries containing \n                'Lat' and 'Lon' keys.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - collections\n\n    Example:\n    >>> dic = {'Restaurant A': {'Lat': 51.5074, 'Lon': -0.1278}, 'Grocery Store B': {'Lat': 34.0522, 'Lon': -118.2437}, 'Pharmacy C': {'Lat': 48.8566, 'Lon': 2.3522}, 'Gas Station D': {'Lat': 35.6895, 'Lon': 139.6917}, 'Bank E': {'Lat': -33.8688, 'Lon': 151.2093}}\n    >>> categorize_locations_and_plot(dic)\n    \"\"\"\n    # Categorize the places\n    categories = [name.split(' ')[0] for name in dic.keys()]\n    category_counts = Counter(categories)\n\n    # Plot the distribution of categories\n    plt.bar(category_counts.keys(), category_counts.values())\n    plt.xlabel('Category')\n    plt.ylabel('Count')\n    plt.title('Distribution of Categories')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nLOCATIONS = ['City A', 'City B', 'City C', 'City D', 'City E']\nTEMPERATURES = [np.random.normal(20, 5, 365) for _ in range(len(LOCATIONS))]\n\ndef plot_temperature_trends(dic):\n    \"\"\"\n    Plot the temperature trends of different locations using a line graph.\n\n    Parameters:\n    dic (dict): The dictionary with keys as location names and values as dictionaries containing \n                'Lat' and 'Lon' keys.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> dic = {'City A': {'Lat': 51.5074, 'Lon': -0.1278}, 'City B': {'Lat': 34.0522, 'Lon': -118.2437}, 'City C': {'Lat': 48.8566, 'Lon': 2.3522}, 'City D': {'Lat': 35.6895, 'Lon': 139.6917}, 'City E': {'Lat': -33.8688, 'Lon': 151.2093}}\n    >>> plot_temperature_trends(dic)\n    \"\"\"\n    for location, temperatures in zip(dic.keys(), TEMPERATURES):\n        plt.plot(temperatures, label=location)\n\n    plt.xlabel('Day of the Year')\n    plt.ylabel('Temperature (\u00b0C)')\n    plt.title('Temperature Trends')\n    plt.legend()\n    plt.show()\n```"}
{"seed": "{\"task_id\": 36164, \"prompt\": \"def f_36164():\\n\\t\", \"suffix\": \"\\n\\treturn indextoentry\", \"canonical_solution\": \"def indextoentry(lista,index):\\n\\t    return [lista[index]]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate()([1,2,3], 0) == [1]\\n\"], \"entry_point\": \"f_36164\", \"intent\": \"\\u00bfC\\u00f3mo crear una funci\\u00f3n `indextoentry` que devuelva una lista al pasarle una lista `lista` y un \\u00ecndice `index`?\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, random, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import randint, choice\nimport matplotlib.pyplot as plt\n\n# Constants\nCITIES = ['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']\nPOPULATION = [8623000, 8136000, 21540000, 9273000, 5106000]\nAGE_GROUPS = ['0-14', '15-24', '25-54', '55-64', '65+']\n\ndef generate_population_distribution(city_index):\n    \"\"\"\n    Generate a distribution of population by age groups for a specific city.\n\n    Parameters:\n    city_index (int): The index of the city in the CITIES list.\n\n    Returns:\n    DataFrame: A pandas DataFrame with population distribution for the city.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> distribution = generate_population_distribution(0)\n    >>> print(distribution)\n    >>> distribution.plot(kind='bar', x='Age Group', y='Population')\n    \"\"\"\n    city = CITIES[city_index]\n    total_population = POPULATION[city_index]\n    population_distribution = []\n\n    for age_group in AGE_GROUPS:\n        population = randint(1, total_population)\n        total_population -= population\n        population_distribution.append([age_group, population])\n\n    distribution_df = pd.DataFrame(population_distribution, columns=['Age Group', 'Population'])\n\n    return distribution_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, matplotlib.pyplot, random\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nCOLORS = ['r', 'g', 'b', 'y', 'm']\n\ndef plot_random_walk(steps, index):\n    \"\"\"\n    Plot a random walk with a given number of steps and color index.\n\n    Parameters:\n    steps (int): The number of steps in the random walk.\n    index (int): The index of the color in the COLORS list.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> plot_random_walk(1000, 0)\n    \"\"\"\n    walk = np.cumsum(np.random.choice([-1, 1], size=steps))\n    plt.plot(walk, color=COLORS[index])\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nrandom, statistics, matplotlib.pyplot\n```python\nfrom random import choice\nimport statistics\nimport matplotlib.pyplot as plt\n\n# Constants\nNUMBERS = list(range(1, 101))\n\ndef sample_and_plot(sample_size):\n    \"\"\"\n    Draw a sample from a list of 100 numbers, calculate the mean and standard deviation,\n    and plot the histogram of the sample.\n\n    Parameters:\n    sample_size (int): The size of the sample to draw from the numbers.\n\n    Returns:\n    tuple: The mean and standard deviation of the sample.\n\n    Requirements:\n    - random\n    - statistics\n    - matplotlib.pyplot\n\n    Example:\n    >>> mean, std_dev = sample_and_plot(50)\n    >>> print(f'Mean: {mean}, Standard Deviation: {std_dev}')\n    \"\"\"\n    sample = [choice(NUMBERS) for _ in range(sample_size)]\n    mean = statistics.mean(sample)\n    std_dev = statistics.stdev(sample)\n    plt.hist(sample, bins=10, alpha=0.5)\n    plt.show()\n\n    return mean, std_dev\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, scipy.optimize, matplotlib.pyplot\n```python\nimport numpy as np\nfrom scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\n\n# Constants\nX = np.linspace(0, 10, 100)\n\ndef fit_and_plot(func, y, index):\n    \"\"\"\n    Fit a function to a set of y-values and plot the original and fitted functions.\n\n    Parameters:\n    func (callable): The function to fit.\n    y (array-like): The y-values to fit the function to.\n    index (int): The index of the subplot to plot on.\n\n    Returns:\n    array: The parameters of the fitted function.\n\n    Requirements:\n    - numpy\n    - scipy.optimize\n    - matplotlib.pyplot\n\n    Example:\n    >>> def func(x, a, b, c):\n    >>>     return a * np.exp(-b * x) + c\n    >>> y = func(X, 2.5, 1.3, 0.5) + 0.2 * np.random.normal(size=len(X))\n    >>> params = fit_and_plot(func, y, 1)\n    >>> print(params)\n    \"\"\"\n    params, _ = curve_fit(func, X, y)\n    plt.subplot(2, 1, index)\n    plt.plot(X, y, 'bo', label='Original')\n    plt.plot(X, func(X, *params), 'r-', label='Fitted')\n    plt.legend()\n\n    return params\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, scipy.stats, matplotlib.pyplot\n```python\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\n# Constants\nMU = 0\nSIGMA = 1\nSAMPLE_SIZE = 1000\n\ndef generate_normal_distribution():\n    \"\"\"\n    Generate a normal distribution with a given mean and standard deviation, \n    draw a sample from it, and plot the histogram of the sample and \n    the probability density function of the distribution.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Example:\n    >>> generate_normal_distribution()\n    \"\"\"\n    sample = np.random.normal(MU, SIGMA, SAMPLE_SIZE)\n    plt.hist(sample, bins=30, density=True, alpha=0.6, color='g')\n\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, MU, SIGMA)\n    plt.plot(x, p, 'k', linewidth=2)\n    plt.show()\n```\n"}
{"seed": "{\"task_id\": 59300, \"prompt\": \"def f_59300():\\n\\t\", \"suffix\": \"\\n\\treturn lista\", \"canonical_solution\": \"lista = []\\n\\twhile True:\\n\\t    inputs = input()\\n\\t    if inputs:\\n\\t        lista.append(inputs)\\n\\t    else:\\n\\t        break\\n\\tprint(lista)\", \"test_start\": \"\\nimport builtins\\nfrom unittest.mock import Mock\\n\\ndef check(candidate):\", \"test\": [\"\\n    builtins.input = Mock(return_value=\\\"hello\\\")\\n    try:\\n        candidate()\\n    except:\\n        return True\\n\"], \"entry_point\": \"f_59300\", \"intent\": \"\\u00bfC\\u00f3mo leer una cadena de varias l\\u00edneas ingresada por consola y almacenar cada l\\u00ednea como un elemento de una lista `lista`?\", \"library\": [\"builtins\"]}", "generation": "Scenario 1: \npandas, matplotlib, random, builtins\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import randint\nimport builtins\nfrom unittest.mock import Mock\n\n# Constants\nCATEGORIES = ['Electronics', 'Clothing', 'Home', 'Books', 'Sports']\n\ndef generate_sales_report():\n    \"\"\"\n    Generate a sales report for different categories of products. The sales data \n    are entered by the user and stored in a list as a string of format \"Category:Sales\". \n    The report is presented as a bar chart.\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - random\n    - builtins\n    \n    Example:\n    >>> generate_sales_report()\n    \"\"\"\n    sales_data = []\n    while True:\n        data = builtins.input()\n        if data:\n            sales_data.append(data.split(':'))\n        else:\n            break\n\n    sales_df = pd.DataFrame(sales_data, columns=['Category', 'Sales'])\n    sales_df['Sales'] = sales_df['Sales'].astype(int)\n\n    sales_df.set_index('Category')['Sales'].plot(kind='bar')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nbuiltins, time, datetime, pytz\n```python\nimport builtins\nimport time\nfrom datetime import datetime\nimport pytz\n\ndef get_utc_time_difference():\n    \"\"\"\n    Get the time difference to the current UTC time from a given timezone that is entered by the user. \n\n    Requirements:\n    - builtins\n    - time\n    - datetime\n    - pytz\n    \n    Example:\n    >>> get_utc_time_difference()\n    \"\"\"\n    user_timezone = builtins.input(\"Enter your timezone: \")\n    user_tz = pytz.timezone(user_timezone)\n    user_time = datetime.now(user_tz)\n    utc_time = datetime.now(pytz.UTC)\n\n    time_difference = (user_time - utc_time).total_seconds()\n    print(f\"The time difference to the current UTC time is {time_difference} seconds.\")\n\n    return time_difference\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nrandom, builtins, datetime, time\n```python\nimport random\nimport builtins\nimport datetime\nimport time\n\n# Constants\nANIMALS = ['Dog', 'Cat', 'Bird', 'Fish', 'Hamster']\n\ndef pet_shop_simulation():\n    \"\"\"\n    Simulate a pet shop where the user enters the number of customers. \n    Each customer will randomly buy one type of animal from the pet shop. \n    The function displays and returns the sales of the day.\n\n    Requirements:\n    - random\n    - builtins\n    - datetime\n    - time\n    \n    Example:\n    >>> pet_shop_simulation()\n    \"\"\"\n    sales = {animal: 0 for animal in ANIMALS}\n    num_customers = int(builtins.input(\"Enter the number of customers: \"))\n\n    for _ in range(num_customers):\n        animal = random.choice(ANIMALS)\n        sales[animal] += 1\n\n    print(f\"Sales of the day: {sales}\")\n    return sales\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, pandas, datetime, builtins\n```python\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\nimport builtins\n\n# Constants\nEMPLOYEES = ['John', 'Mary', 'Peter', 'Lucy', 'Mike']\n\ndef employee_performance():\n    \"\"\"\n    Record the performance ratings of employees. The ratings are entered by the user \n    and stored in a list. The function calculates and returns the average performance rating.\n\n    Requirements:\n    - numpy\n    - pandas\n    - datetime\n    - builtins\n    \n    Example:\n    >>> employee_performance()\n    \"\"\"\n    ratings = []\n    for employee in EMPLOYEES:\n        rating = int(builtins.input(f\"Enter the performance rating of {employee}: \"))\n        ratings.append(rating)\n\n    avg_rating = np.mean(ratings)\n    print(f\"The average performance rating is {avg_rating}\")\n\n    return avg_rating\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nrandom, time, builtins, datetime\n```python\nimport random\nimport time\nimport builtins\nfrom datetime import datetime\n\n# Constants\nWORDS = ['apple', 'banana', 'cherry', 'date', 'elderberry']\n\ndef word_guessing_game():\n    \"\"\"\n    Play a word guessing game where the user guesses a word chosen randomly from a list. \n    The function records the time it takes for the user to guess the word correctly.\n\n    Requirements:\n    - random\n    - time\n    - builtins\n    - datetime\n    \n    Example:\n    >>> word_guessing_game()\n    \"\"\"\n    chosen_word = random.choice(WORDS)\n\n    start_time = time.time()\n    while True:\n        guess = builtins.input(\"Guess the word: \")\n        if guess.lower() == chosen_word:\n            break\n    end_time = time.time()\n\n    time_taken = end_time - start_time\n    print(f\"You guessed the word correctly in {time_taken} seconds.\")\n\n    return time_taken\n```\n"}
{"seed": "{\"task_id\": 2056, \"prompt\": \"def f_2056(rut):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"directory = rut\\n\\tos.chdir(directory)\\n\\tfor filename in glob.glob('**/*.txt', recursive=True):\\n\\t    os.unlink(filename)\", \"test_start\": \"\\nimport os\\nimport glob\\nfrom unittest.mock import Mock\\n\\ndef check(candidate):\", \"test\": [\"\\n    os.chdir = Mock()\\n    os.unlink = Mock()\\n    try:\\n        candidate('/')\\n    except:\\n        assert False\\n\"], \"entry_point\": \"f_2056\", \"intent\": \"\\u00bfC\\u00f3mo borrar todos los archivos .txt dentro de las subcarpetas ubicadas en la ruta `rut`?\", \"library\": [\"glob\", \"os\"]}", "generation": "Scenario 1:\n```python\n# Libraries: os, glob, shutil, time\nimport os\nimport glob\nimport shutil\nimport time\n\n# Constants\nBACKUP_DIR = 'backup'\n\ndef backup_and_delete_txt_files(directory):\n    \"\"\"\n    This function creates a backup of all '.txt' files inside the given directory and its subdirectories \n    in a folder named 'backup'. After the backup process, it deletes all '.txt' files from the original location.\n\n    Parameters:\n    directory (str): The root directory to start the operation.\n\n    Returns:\n    str: The path of the backup directory.\n\n    Requirements:\n    - os\n    - glob\n    - shutil\n    - time\n\n    Example:\n    >>> backup_and_delete_txt_files('/path/to/root/directory')\n    \"\"\"\n    # Create the backup directory if not exists\n    if not os.path.exists(BACKUP_DIR):\n        os.makedirs(BACKUP_DIR)\n\n    for filename in glob.glob(f'{directory}/**/*.txt', recursive=True):\n        # Create a backup of the file\n        backup_filename = f\"{BACKUP_DIR}/{os.path.basename(filename)}_{time.time()}.txt\"\n        shutil.copy2(filename, backup_filename)\n        # Delete the original file\n        os.unlink(filename)\n    \n    return os.path.abspath(BACKUP_DIR)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\n```python\n# Libraries: os, glob, re, csv\nimport os\nimport glob\nimport re\nimport csv\n\n# Constants\nPHONE_PATTERN = r'\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b'\n\ndef extract_phone_numbers(directory):\n    \"\"\"\n    This function searches for phone numbers inside all '.txt' files in a given directory and its subdirectories. \n    It writes all found phone numbers into a CSV file.\n\n    Parameters:\n    directory (str): The root directory to start the operation.\n\n    Returns:\n    str: The path of the CSV file with phone numbers.\n\n    Requirements:\n    - os\n    - glob\n    - re\n    - csv\n\n    Example:\n    >>> extract_phone_numbers('/path/to/root/directory')\n    \"\"\"\n    csv_filename = f\"{directory}/phone_numbers.csv\"\n\n    with open(csv_filename, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n\n        for filename in glob.glob(f'{directory}/**/*.txt', recursive=True):\n            with open(filename, 'r') as f:\n                content = f.read()\n                phone_numbers = re.findall(PHONE_PATTERN, content)\n                \n                for phone_number in phone_numbers:\n                    writer.writerow([phone_number])\n    \n    return os.path.abspath(csv_filename)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\n```python\n# Libraries: os, glob, shutil\nimport os\nimport glob\nimport shutil\n\n# Constants\nARCHIVE_DIR = 'archive'\n\ndef archive_old_files(directory, days_old):\n    \"\"\"\n    This function moves all files in a given directory and its subdirectories which are older than a certain \n    number of days to a separate directory named 'archive'.\n\n    Parameters:\n    directory (str): The root directory to start the operation.\n    days_old (int): The minimum file age in days to qualify for archiving.\n\n    Returns:\n    str: The path of the archive directory.\n\n    Requirements:\n    - os\n    - glob\n    - shutil\n\n    Example:\n    >>> archive_old_files('/path/to/root/directory', 30)\n    \"\"\"\n    # Create the archive directory if not exists\n    if not os.path.exists(ARCHIVE_DIR):\n        os.makedirs(ARCHIVE_DIR)\n\n    for filename in glob.glob(f'{directory}/**/*', recursive=True):\n        # If the file is older than 'days_old' days\n        if os.path.getmtime(filename) < time.time() - days_old * 86400:\n            shutil.move(filename, ARCHIVE_DIR)\n    \n    return os.path.abspath(ARCHIVE_DIR)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\n```python\n# Libraries: os, glob, hashlib\nimport os\nimport glob\nimport hashlib\n\ndef calculate_file_hashes(directory):\n    \"\"\"\n    This function calculates the MD5 hash for all files in a given directory and its subdirectories.\n\n    Parameters:\n    directory (str): The root directory to start the operation.\n\n    Returns:\n    dict: A dictionary mapping from file paths to their MD5 hashes.\n\n    Requirements:\n    - os\n    - glob\n    - hashlib\n\n    Example:\n    >>> calculate_file_hashes('/path/to/root/directory')\n    \"\"\"\n    file_hashes = {}\n\n    for filename in glob.glob(f'{directory}/**/*', recursive=True):\n        if os.path.isfile(filename):\n            with open(filename, 'rb') as f:\n                file_hashes[filename] = hashlib.md5(f.read()).hexdigest()\n    \n    return file_hashes\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\n```python\n# Libraries: os, glob, shutil, re\nimport os\nimport glob\nimport shutil\nimport re\n\ndef sanitize_filenames(directory, pattern):\n    \"\"\"\n    This function renames all files in a given directory and its subdirectories \n    which have names matching a certain pattern by removing special characters and spaces.\n\n    Parameters:\n    directory (str): The root directory to start the operation.\n    pattern (str): Regular expression pattern to match the filenames.\n\n    Requirements:\n    - os\n    - glob\n    - shutil\n    - re\n\n    Example:\n    >>> sanitize_filenames('/path/to/root/directory', r'^[a-zA-Z0-9_]*$')\n    \"\"\"\n    for filename in glob.glob(f'{directory}/**/*', recursive=True):\n        if os.path.isfile(filename) and re.match(pattern, os.path.basename(filename)):\n            sanitized_filename = re.sub(r'\\W+', '_', os.path.basename(filename))\n            shutil.move(filename, os.path.join(os.path.dirname(filename), sanitized_filename))\n```\n"}
{"seed": "{\"task_id\": 59326, \"prompt\": \"def f_59326(proxies, url):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"proxy = request.ProxyHandler(proxies)\\n\\topener = request.build_opener(proxy)\\n\\trequest.install_opener(opener)\\n\\tresponse=request.urlopen(url)\", \"test_start\": \"\\nfrom urllib import request\\nfrom unittest.mock import Mock\\n\\ndef check(candidate):\", \"test\": [\"\\n    proxies = {'http':\\\"218.28.112.114:809\\\"}\\n    url = 'http://www.httpbin.org/ip'\\n    request.ProxyHandler = Mock()\\n    request.build_opener = Mock()\\n    request.install_opener = Mock()\\n    request.urlopen = Mock(return_value = request.urlopen('https://www.google.com'))\\n    try:\\n        candidate(proxies, url)\\n    except:\\n        assert False\\n\"], \"entry_point\": \"f_59326\", \"intent\": \"\\u00bfC\\u00f3mo usar una direcci\\u00f3n proxy `proxies` con una url `url`?\", \"library\": [\"urllib\"]}", "generation": "Scenario 1:\nrequests, BeautifulSoup, os, urllib\n```python\nfrom bs4 import BeautifulSoup\nimport requests\nimport os\nfrom urllib.parse import urljoin\n\n# Constants\nBASE_URL = 'http://books.toscrape.com/'\n\ndef download_books(proxies, base_url=BASE_URL):\n    \"\"\"\n    Download all books from the website 'http://books.toscrape.com' using a proxy.\n\n    Parameters:\n    proxies (dict): The proxies to use.\n    \n    Returns:\n    None\n\n    Requirements:\n    - requests\n    - BeautifulSoup\n    - os\n    - urllib.parse\n    \n    Example:\n    >>> proxies = {'http': '218.28.112.114:809'}\n    >>> download_books(proxies)\n    \"\"\"\n    response = requests.get(base_url, proxies=proxies)\n    soup = BeautifulSoup(response.text, 'lxml')\n    for book in soup.select('article.product_pod'):\n        book_url = urljoin(base_url, book.h3.a.get('href'))\n        book_response = requests.get(book_url, proxies=proxies)\n        book_soup = BeautifulSoup(book_response.text, 'lxml')\n        book_title = book_soup.h1.text\n        with open(os.path.join('books', f'{book_title}.txt'), 'w') as f:\n            f.write(book_soup.select_one('div#product_description').next_sibling.next_sibling.text)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nsocket, urllib, ssl, os\n```python\nimport socket\nimport urllib.request\nimport ssl\nimport os\n\n# Constants\nTARGET_URL = 'https://www.google.com'\nDEFAULT_CERT_FILE = '/etc/ssl/certs/ca-certificates.crt'\n\ndef check_ssl_certificate(proxies, url=TARGET_URL, cert_file=DEFAULT_CERT_FILE):\n    \"\"\"\n    Check the SSL certificate of a URL using a proxy.\n\n    Parameters:\n    proxies (dict): The proxies to use.\n    url (str): The URL to check. Defaults to 'https://www.google.com'.\n    cert_file (str): The path to the CA certificate file. Defaults to '/etc/ssl/certs/ca-certificates.crt'.\n\n    Returns:\n    bool: True if the certificate is valid, False otherwise.\n\n    Requirements:\n    - socket\n    - urllib\n    - ssl\n    - os\n    \n    Example:\n    >>> proxies = {'https': '218.28.112.114:809'}\n    >>> check_ssl_certificate(proxies)\n    \"\"\"\n    proxy_support = urllib.request.ProxyHandler(proxies)\n    opener = urllib.request.build_opener(proxy_support)\n    urllib.request.install_opener(opener)\n\n    hostname = urllib.parse.urlparse(url).hostname\n    context = ssl.create_default_context(cafile=cert_file)\n\n    with socket.create_connection((hostname, 443)) as sock:\n        try:\n            with context.wrap_socket(sock, server_hostname=hostname) as ssock:\n                return ssock.getpeercert() is not None\n        except ssl.SSLCertVerificationError:\n            return False\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nrequests, json, os, urllib\n```python\nimport requests\nimport json\nimport os\nfrom urllib.parse import urljoin\n\n# Constants\nBASE_URL = 'https://api.github.com/users/'\nGITHUB_TOKEN = os.getenv('GITHUB_TOKEN')\n\ndef fetch_github_repos(proxies, username):\n    \"\"\"\n    Fetch all repositories of a GitHub user using a proxy.\n\n    Parameters:\n    proxies (dict): The proxies to use.\n    username (str): The GitHub username.\n\n    Returns:\n    list: A list of repository names.\n\n    Requirements:\n    - requests\n    - json\n    - os\n    - urllib.parse\n\n    Example:\n    >>> proxies = {'https': '218.28.112.114:809'}\n    >>> fetch_github_repos(proxies, 'torvalds')\n    \"\"\"\n    headers = {'Authorization': f'token {GITHUB_TOKEN}'}\n    user_url = urljoin(BASE_URL, username)\n    response = requests.get(user_url + '/repos', proxies=proxies, headers=headers)\n    repos = [repo['name'] for repo in response.json()]\n    return repos\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nrequests, json, os, urllib\n```python\nimport requests\nimport json\nimport os\nfrom urllib.parse import urljoin\n\n# Constants\nBASE_URL = 'https://jsonplaceholder.typicode.com/'\n\ndef fetch_user_posts(proxies, user_id):\n    \"\"\"\n    Fetch all posts of a user from 'https://jsonplaceholder.typicode.com' using a proxy.\n\n    Parameters:\n    proxies (dict): The proxies to use.\n    user_id (int): The user ID.\n\n    Returns:\n    list: A list of posts.\n\n    Requirements:\n    - requests\n    - json\n    - os\n    - urllib.parse\n\n    Example:\n    >>> proxies = {'https': '218.28.112.114:809'}\n    >>> fetch_user_posts(proxies, 1)\n    \"\"\"\n    user_url = urljoin(BASE_URL, f'users/{user_id}/posts')\n    response = requests.get(user_url, proxies=proxies)\n    posts = [post['title'] for post in response.json()]\n    return posts\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nrequests, BeautifulSoup, urllib, lxml\n```python\nfrom bs4 import BeautifulSoup\nimport requests\nfrom urllib.parse import urljoin\n\n# Constants\nBASE_URL = 'https://www.imdb.com/chart/top'\n\ndef fetch_top_movies(proxies, base_url=BASE_URL):\n    \"\"\"\n    Fetch the top 250 movies from IMDB using a proxy.\n\n    Parameters:\n    proxies (dict): The proxies to use.\n    base_url (str): The URL of the top 250 movies page. Defaults to 'https://www.imdb.com/chart/top'.\n\n    Returns:\n    list: A list of movie titles.\n\n    Requirements:\n    - requests\n    - BeautifulSoup\n    - urllib.parse\n    \n    Example:\n    >>> proxies = {'http': '218.28.112.114:809'}\n    >>> fetch_top_movies(proxies)\n    \"\"\"\n    response = requests.get(base_url, proxies=proxies)\n    soup = BeautifulSoup(response.text, 'lxml')\n    movies = [movie.a.get('title') for movie in soup.select('td.titleColumn')]\n    return movies\n```"}
{"seed": "{\"task_id\": 3995, \"prompt\": \"def f_3995(conf):\\n\\t\", \"suffix\": \"\\n\\treturn config\", \"canonical_solution\": \"\\n\\tconfig = configparser.ConfigParser()\\n\\tconfig.read(conf)\\n\", \"test_start\": \"\\nimport configparser\\nfrom unittest.mock import Mock\\n\\ndef check(candidate):\", \"test\": [\"\\n    file_name = 'example.conf'\\n    with open(file_name, 'w') as f:\\n        f.write('[HORAS]\\\\n')\\n        f.write('hora_ini = 14:00\\\\n')\\n        f.write('hora_fin = 22:00\\\\n')\\n    try:\\n        candidate(file_name)\\n    except:\\n        assert False\\n\"], \"entry_point\": \"f_3995\", \"intent\": \"\\u00bfC\\u00f3mo implementar un fichero conf?\", \"library\": [\"configparser\"]}", "generation": "Scenario 1:\nconfigparser, os, csv, pandas\n```python\nimport configparser\nimport os\nimport csv\nimport pandas as pd\n\n# Constants\nCONFIG_SECTIONS = ['DATABASE', 'API']\n\ndef process_config_file(conf):\n    \"\"\"\n    Parse a config file and use its information to load a CSV file from a database \n    path specified in the config and return a pandas DataFrame. The config file \n    also contains API information which is printed.\n\n    Parameters:\n    conf (str): The path of the config file.\n\n    Returns:\n    DataFrame: The dataframe loaded from the CSV file.\n\n    Requirements:\n    - configparser\n    - os\n    - csv\n    - pandas\n\n    Example:\n    >>> df = process_config_file('config.ini')\n    >>> print(df)\n    \"\"\"\n    config = configparser.ConfigParser()\n    config.read(conf)\n\n    # Load CSV file from database path in config\n    db_path = config.get('DATABASE', 'Path')\n    csv_file = os.path.join(db_path, 'data.csv')\n    df = pd.read_csv(csv_file)\n\n    # Print API Information from config\n    api_info = dict(config.items('API'))\n    print(f\"API Information: {api_info}\")\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nconfigparser, datetime, requests, json\n```python\nimport configparser\nimport datetime\nimport requests\nimport json\n\ndef fetch_weather_data(conf):\n    \"\"\"\n    Parse a config file and use its information to fetch weather data from a public API.\n    The config file contains URL, city and API key for the weather API.\n\n    Parameters:\n    conf (str): The path of the config file.\n\n    Returns:\n    dict: The dictionary containing weather data.\n\n    Requirements:\n    - configparser\n    - datetime\n    - requests\n    - json\n\n    Example:\n    >>> data = fetch_weather_data('config.ini')\n    >>> print(data)\n    \"\"\"\n    config = configparser.ConfigParser()\n    config.read(conf)\n\n    # Fetch weather data\n    url = config.get('API', 'URL')\n    city = config.get('API', 'City')\n    api_key = config.get('API', 'Key')\n    response = requests.get(f\"{url}?q={city}&appid={api_key}\")\n    \n    return response.json()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nconfigparser, pandas, matplotlib.pyplot\n```python\nimport configparser\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef plot_data_from_config(conf):\n    \"\"\"\n    Parse a config file and use its information to read a CSV file and plot the data.\n\n    Parameters:\n    conf (str): The path of the config file.\n\n    Returns:\n    None\n\n    Requirements:\n    - configparser\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_data_from_config('config.ini')\n    \"\"\"\n    config = configparser.ConfigParser()\n    config.read(conf)\n\n    # Load CSV file from path in config\n    file_path = config.get('FILE', 'Path')\n    df = pd.read_csv(file_path)\n\n    # Plot the data\n    df.plot()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nconfigparser, smtplib, email.mime.text\n```python\nimport configparser\nimport smtplib\nfrom email.mime.text import MIMEText\n\ndef send_email_from_config(conf):\n    \"\"\"\n    Parse a config file and use its information to send an email.\n\n    Parameters:\n    conf (str): The path of the config file.\n\n    Returns:\n    None\n\n    Requirements:\n    - configparser\n    - smtplib\n    - email.mime.text\n\n    Example:\n    >>> send_email_from_config('config.ini')\n    \"\"\"\n    config = configparser.ConfigParser()\n    config.read(conf)\n\n    # Get email parameters from the config file\n    host = config.get('EMAIL', 'Host')\n    port = config.get('EMAIL', 'Port')\n    username = config.get('EMAIL', 'Username')\n    password = config.get('EMAIL', 'Password')\n    from_addr = config.get('EMAIL', 'From')\n    to_addr = config.get('EMAIL', 'To')\n    subject = config.get('EMAIL', 'Subject')\n    body = config.get('EMAIL', 'Body')\n\n    # Construct the email\n    msg = MIMEText(body)\n    msg['Subject'] = subject\n    msg['From'] = from_addr\n    msg['To'] = to_addr\n\n    # Send the email\n    server = smtplib.SMTP(host, port)\n    server.starttls()\n    server.login(username, password)\n    server.send_message(msg)\n    server.quit()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nconfigparser, matplotlib.pyplot, pandas, seaborn\n```python\nimport configparser\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\ndef create_seaborn_plot_from_config(conf):\n    \"\"\"\n    Parse a config file and use its information to read a CSV file and create a Seaborn plot.\n\n    Parameters:\n    conf (str): The path of the config file.\n\n    Returns:\n    None\n\n    Requirements:\n    - configparser\n    - matplotlib.pyplot\n    - pandas\n    - seaborn\n\n    Example:\n    >>> create_seaborn_plot_from_config('config.ini')\n    \"\"\"\n    config = configparser.ConfigParser()\n    config.read(conf)\n\n    # Load CSV file from path in config\n    file_path = config.get('FILE', 'Path')\n    df = pd.read_csv(file_path)\n\n    # Plot the data using Seaborn\n    sns_plot = sns.pairplot(df)\n    plt.show()\n```\n"}
{"seed": "{\"task_id\": 19685, \"prompt\": \"def f_19685(ingreso):\\n\\t\", \"suffix\": \"\\n\\treturn diferencia\", \"canonical_solution\": \"now = datetime.utcnow() \\n\\tdiferencia = now - ingreso\", \"test_start\": \"\\nfrom datetime import datetime\\n\\ndef check(candidate):\", \"test\": [\"\\n    ingreso = datetime.utcnow()\\n    assert candidate(ingreso).seconds - (datetime.utcnow()-ingreso).seconds < 60\\n\"], \"entry_point\": \"f_19685\", \"intent\": \"\\u00bfC\\u00f3mo calcular la diferencia entre una hora `ingreso` en UTC y este momento?\", \"library\": [\"datetime\"]}", "generation": "Scenario 1:\npandas, pytz, datetime, random, matplotlib\n```python\nimport pandas as pd\nimport pytz\nfrom datetime import datetime\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nCITIES = ['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']\nWEATHER_CONDITIONS = ['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy']\n\n# Time zones for the cities\nTIMEZONES = {\n    'New York': 'America/New_York',\n    'London': 'Europe/London',\n    'Beijing': 'Asia/Shanghai',\n    'Tokyo': 'Asia/Tokyo',\n    'Sydney': 'Australia/Sydney'\n}\n\ndef generate_weather_report(utc_datetime):\n    \"\"\"\n    Generate a report of weather conditions for a list of cities across various \n    time zones at a given time (UTC).\n    \n    Parameters:\n    utc_datetime (datetime): The datetime in UTC.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with weather conditions for the cities.\n    \n    Requirements:\n    - pandas\n    - pytz\n    - datetime\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> utc_time = datetime(2023, 6, 15, 12, 0, 0, tzinfo=pytz.UTC)\n    >>> report = generate_weather_report(utc_time)\n    >>> print(report)\n    >>> report['Weather Condition'].value_counts().plot(kind='bar')\n    \"\"\"\n    report_data = []\n\n    for city in CITIES:\n        city_tz = pytz.timezone(TIMEZONES[city])\n        city_time = utc_datetime.astimezone(city_tz)\n        weather = WEATHER_CONDITIONS[randint(0, len(WEATHER_CONDITIONS)-1)]\n        report_data.append([city, city_time, weather])\n\n    report_df = pd.DataFrame(report_data, columns=['City', 'Local Time', 'Weather Condition'])\n\n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npytz, datetime, numpy, dateutil\n```python\nimport pytz\nfrom datetime import datetime\nimport numpy as np\nfrom dateutil.parser import parse\n\n# Constants\nLEAP_SECONDS = np.array([1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980,\n                         1981, 1982, 1983, 1985, 1988, 1990, 1993, 1994, 1997,\n                         1999, 2006, 2009, 2012, 2015, 2016, 2020])\n\ndef total_seconds_since_date(date_str, from_tz, to_tz):\n    \"\"\"\n    Calculate the total seconds that have passed since a given datetime from the current time \n    in different timezones considering the leap seconds.\n\n    Parameters:\n    date_str (str): The date string in \"yyyy-mm-dd hh:mm:ss\" format.\n    from_tz (str): The timezone of the given date string.\n    to_tz (str): The timezone to which the current time should be converted.\n\n    Returns:\n    int: The total seconds.\n\n    Requirements:\n    - datetime\n    - pytz\n    - numpy\n    - dateutil.parser\n\n    Example:\n    >>> total_seconds_since_date('1970-01-01 00:00:00', 'UTC', 'America/New_York')\n    \"\"\"\n    from_tz = pytz.timezone(from_tz)\n    to_tz = pytz.timezone(to_tz)\n    given_date = parse(date_str).replace(tzinfo=from_tz)\n    current_date = datetime.now().astimezone(to_tz)\n\n    total_seconds = (current_date - given_date).total_seconds()\n\n    leap_years = LEAP_SECONDS[np.logical_and(LEAP_SECONDS >= given_date.year, LEAP_SECONDS <= current_date.year)]\n    leap_seconds = len(leap_years)\n\n    total_seconds += leap_seconds\n\n    return int(total_seconds)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, pytz, datetime, matplotlib\n```python\nimport pandas as pd\nimport pytz\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\n\n# Constants\nCITIES = ['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']\n\n# Time zones for the cities\nTIMEZONES = {\n    'New York': 'America/New_York',\n    'London': 'Europe/London',\n    'Beijing': 'Asia/Shanghai',\n    'Tokyo': 'Asia/Tokyo',\n    'Sydney': 'Australia/Sydney'\n}\n\ndef generate_date_report(utc_datetime):\n    \"\"\"\n    Generate a report of current date for a list of cities across various \n    time zones at a given time (UTC).\n    \n    Parameters:\n    utc_datetime (datetime): The datetime in UTC.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with current date for the cities.\n    \n    Requirements:\n    - pandas\n    - pytz\n    - datetime\n    - matplotlib.pyplot\n    \n    Example:\n    >>> utc_time = datetime(2023, 6, 15, 12, 0, 0, tzinfo=pytz.UTC)\n    >>> report = generate_date_report(utc_time)\n    >>> print(report)\n    \"\"\"\n    report_data = []\n\n    for city in CITIES:\n        city_tz = pytz.timezone(TIMEZONES[city])\n        city_time = utc_datetime.astimezone(city_tz).date()\n        report_data.append([city, city_time])\n\n    report_df = pd.DataFrame(report_data, columns=['City', 'Current Date'])\n\n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, pytz, datetime, matplotlib\n```python\nimport pandas as pd\nimport pytz\nfrom datetime import datetime, timedelta\nimport matplotlib.pyplot as plt\n\n# Constants\nCITIES = ['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']\n\n# Time zones for the cities\nTIMEZONES = {\n    'New York': 'America/New_York',\n    'London': 'Europe/London',\n    'Beijing': 'Asia/Shanghai',\n    'Tokyo': 'Asia/Tokyo',\n    'Sydney': 'Australia/Sydney'\n}\n\ndef generate_future_date_report(utc_datetime, days_in_future):\n    \"\"\"\n    Generate a report of future date for a list of cities across various \n    time zones at a given time (UTC).\n    \n    Parameters:\n    utc_datetime (datetime): The datetime in UTC.\n    days_in_future (int): The number of days in the future.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with future date for the cities.\n    \n    Requirements:\n    - pandas\n    - pytz\n    - datetime\n    - matplotlib.pyplot\n    \n    Example:\n    >>> utc_time = datetime(2023, 6, 15, 12, 0, 0, tzinfo=pytz.UTC)\n    >>> report = generate_future_date_report(utc_time, 7)\n    >>> print(report)\n    \"\"\"\n    report_data = []\n\n    for city in CITIES:\n        city_tz = pytz.timezone(TIMEZONES[city])\n        city_time = (utc_datetime + timedelta(days=days_in_future)).astimezone(city_tz).date()\n        report_data.append([city, city_time])\n\n    report_df = pd.DataFrame(report_data, columns=['City', 'Future Date'])\n\n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, pytz, datetime, matplotlib\n```python\nimport pandas as pd\nimport pytz\nfrom datetime import datetime, timedelta\nimport matplotlib.pyplot as plt\n\n# Constants\nCITIES = ['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']\n\n# Time zones for the cities\nTIMEZONES = {\n    'New York': 'America/New_York',\n    'London': 'Europe/London',\n    'Beijing': 'Asia/Shanghai',\n    'Tokyo': 'Asia/Tokyo',\n    'Sydney': 'Australia/Sydney'\n}\n\ndef generate_past_date_report(utc_datetime, days_in_past):\n    \"\"\"\n    Generate a report of past date for a list of cities across various \n    time zones at a given time (UTC).\n    \n    Parameters:\n    utc_datetime (datetime): The datetime in UTC.\n    days_in_past (int): The number of days in the past.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with past date for the cities.\n    \n    Requirements:\n    - pandas\n    - pytz\n    - datetime\n    - matplotlib.pyplot\n    \n    Example:\n    >>> utc_time = datetime(2023, 6, 15, 12, 0, 0, tzinfo=pytz.UTC)\n    >>> report = generate_past_date_report(utc_time, 7)\n    >>> print(report)\n    \"\"\"\n    report_data = []\n\n    for city in CITIES:\n        city_tz = pytz.timezone(TIMEZONES[city])\n        city_time = (utc_datetime - timedelta(days=days_in_past)).astimezone(city_tz).date()\n        report_data.append([city, city_time])\n\n    report_df = pd.DataFrame(report_data, columns=['City', 'Past Date'])\n\n    return report_df\n```\n`GPT_ODEX_BREAK`"}
{"seed": "{\"task_id\": 19685, \"prompt\": \"def f_19685(ingreso):\\n\\t\", \"suffix\": \"\\n\\treturn diferencia\", \"canonical_solution\": \"now = datetime.utcnow()\\n\\tdiferencia = now.replace(tzinfo=None) - ingreso.replace(tzinfo=None)\", \"test_start\": \"\\nimport datetime\\nfrom unittest.mock import Mock\\n\\ndef check(candidate):\", \"test\": [\"\\n    datetime.utcnow = Mock(return_value = datetime.datetime(2016, 8, 1, 22, 0, 21, 979170))\\n    ingreso = datetime.datetime(2016, 8, 2, 22, 0, 21, 979170)\\n    d = candidate(ingreso)\\n    assert d.days == -1\\n\"], \"entry_point\": \"f_19685\", \"intent\": \"\\u00bfC\\u00f3mo calcular la diferencia entre una hora `ingreso` en UTC y este momento?\", \"library\": [\"datetime\"]}", "generation": "Scenario 1: \npandas, pytz, datetime, numpy, matplotlib\n```python\nimport pandas as pd\nimport pytz\nfrom datetime import datetime\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCITIES = ['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']\n\n# Time zones for the cities\nTIMEZONES = {\n    'New York': 'America/New_York',\n    'London': 'Europe/London',\n    'Beijing': 'Asia/Shanghai',\n    'Tokyo': 'Asia/Tokyo',\n    'Sydney': 'Australia/Sydney'\n}\n\ndef time_difference_to_utc(ingreso):\n    \"\"\"\n    Calculate the time difference to UTC for a list of cities at a given time (UTC).\n    \n    Parameters:\n    ingreso (datetime): The datetime in UTC.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with time differences to UTC for the cities.\n    \n    Requirements:\n    - pandas\n    - pytz\n    - datetime\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> utc_time = datetime(2023, 6, 15, 12, 0, 0, tzinfo=pytz.UTC)\n    >>> df = time_difference_to_utc(utc_time)\n    >>> print(df)\n    >>> df.plot(x='City', y='Time Difference', kind='bar')\n    \"\"\"\n    report_data = []\n\n    for city in CITIES:\n        city_tz = pytz.timezone(TIMEZONES[city])\n        city_time = ingreso.astimezone(city_tz)\n        time_diff = (city_time - ingreso).total_seconds() / 3600\n        report_data.append([city, time_diff])\n\n    df = pd.DataFrame(report_data, columns=['City', 'Time Difference'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npytz, datetime, numpy, dateutil\n```python\nimport pytz\nfrom datetime import datetime\nimport numpy as np\nfrom dateutil.relativedelta import relativedelta\n\n# Constants\nMONTHS = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])\n\ndef months_since_date(ingreso, to_tz):\n    \"\"\"\n    Calculate the total months that have passed since a given datetime from the current time \n    in a different timezone.\n\n    Parameters:\n    ingreso (datetime): The datetime in UTC.\n    to_tz (str): The timezone to which the current time should be converted.\n\n    Returns:\n    int: The total months.\n\n    Requirements:\n    - datetime\n    - pytz\n    - numpy\n    - dateutil.relativedelta\n\n    Example:\n    >>> months_since_date(datetime(2020, 1, 1, tzinfo=pytz.UTC), 'America/New_York')\n    \"\"\"\n    to_tz = pytz.timezone(to_tz)\n    current_date = datetime.now().astimezone(to_tz)\n    \n    months = relativedelta(current_date, ingreso).years * 12 + relativedelta(current_date, ingreso).months\n\n    return months\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, pytz, datetime, numpy, matplotlib\n```python\nimport pandas as pd\nimport pytz\nfrom datetime import datetime\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCITIES = ['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']\n\n# Time zones for the cities\nTIMEZONES = {\n    'New York': 'America/New_York',\n    'London': 'Europe/London',\n    'Beijing': 'Asia/Shanghai',\n    'Tokyo': 'Asia/Tokyo',\n    'Sydney': 'Australia/Sydney'\n}\n\ndef time_difference_to_city(ingreso, city):\n    \"\"\"\n    Calculate the time difference to a specific city for a given time (UTC).\n    \n    Parameters:\n    ingreso (datetime): The datetime in UTC.\n    city (str): The city to calculate the time difference to.\n    \n    Returns:\n    float: The time difference in hours.\n    \n    Requirements:\n    - pandas\n    - pytz\n    - datetime\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> utc_time = datetime(2023, 6, 15, 12, 0, 0, tzinfo=pytz.UTC)\n    >>> time_diff = time_difference_to_city(utc_time, 'New York')\n    >>> print(time_diff)\n    \"\"\"\n    city_tz = pytz.timezone(TIMEZONES[city])\n    city_time = ingreso.astimezone(city_tz)\n    time_diff = (city_time - ingreso).total_seconds() / 3600\n\n    return time_diff\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npytz, datetime, numpy, dateutil\n```python\nimport pytz\nfrom datetime import datetime\nimport numpy as np\nfrom dateutil.relativedelta import relativedelta\n\n# Constants\nYEARS = np.array(list(range(1970, datetime.now().year+1)))\n\ndef years_since_date(ingreso, to_tz):\n    \"\"\"\n    Calculate the total years that have passed since a given datetime from the current time \n    in a different timezone.\n\n    Parameters:\n    ingreso (datetime): The datetime in UTC.\n    to_tz (str): The timezone to which the current time should be converted.\n\n    Returns:\n    int: The total years.\n\n    Requirements:\n    - datetime\n    - pytz\n    - numpy\n    - dateutil.relativedelta\n\n    Example:\n    >>> years_since_date(datetime(2000, 1, 1, tzinfo=pytz.UTC), 'America/New_York')\n    \"\"\"\n    to_tz = pytz.timezone(to_tz)\n    current_date = datetime.now().astimezone(to_tz)\n    \n    years = relativedelta(current_date, ingreso).years\n\n    return years\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, pytz, datetime, numpy, matplotlib\n```python\nimport pandas as pd\nimport pytz\nfrom datetime import datetime\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCITIES = ['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']\n\n# Time zones for the cities\nTIMEZONES = {\n    'New York': 'America/New_York',\n    'London': 'Europe/London',\n    'Beijing': 'Asia/Shanghai',\n    'Tokyo': 'Asia/Tokyo',\n    'Sydney': 'Australia/Sydney'\n}\n\ndef days_since_date_to_city(ingreso, city):\n    \"\"\"\n    Calculate the total days that have passed since a given datetime from the current time \n    in a specific city.\n\n    Parameters:\n    ingreso (datetime): The datetime in UTC.\n    city (str): The city to which the current time should be converted.\n\n    Returns:\n    int: The total days.\n\n    Requirements:\n    - pandas\n    - pytz\n    - datetime\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> days_since_date_to_city(datetime(2020, 1, 1, tzinfo=pytz.UTC), 'New York')\n    \"\"\"\n    city_tz = pytz.timezone(TIMEZONES[city])\n    current_date = datetime.now().astimezone(city_tz)\n    \n    days = (current_date - ingreso).days\n\n    return days\n```\n"}
{"seed": "{\"task_id\": 2256, \"prompt\": \"def f_2256(S):\\n\\t\", \"suffix\": \"\\n\\treturn mayores, menores\", \"canonical_solution\": \"media = stats.mean([len(palabra) for palabra in S.split()])\\n\\tmayores = [palabra for palabra in S.split() if len(palabra) >= media]\\n\\tmenores = [palabra for palabra in S.split() if len(palabra) < media]\\n\\tprint('las palabras menores a la media: ', menores)\\n\\tprint('las palabras mayores o iguales a la media: ', mayores)\", \"test_start\": \"\\nimport statistics as stats\\n\\ndef check(candidate):\", \"test\": [\"\\n    S = \\\"hello world i am a penguin\\\"\\n    x, y = candidate(S)\\n    assert y == ['i', 'am', 'a']\\n    assert x == ['hello', 'world', 'penguin']\\n\"], \"entry_point\": \"f_2256\", \"intent\": \"\\u00bfC\\u00f3mo encontrar las palabras de longitud menor `menores` o mayor `mayores` que la media aritmetica de una cadena `S`?\", \"library\": [\"statistics\"]}", "generation": "Scenario 1:\nstatistics, pandas, numpy, matplotlib\n```python\nimport statistics as stats\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nLETTER_VALUES = {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, \n                 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, \n                 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, \n                 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n\ndef analyze_text(S):\n    \"\"\"\n    Analyze a text string by finding the words with lengths shorter or longer than \n    the average length, and plot the frequency of word lengths and the sum of \n    letter values in each word.\n\n    Parameters:\n    S (str): The text string to analyze.\n\n    Returns:\n    Tuple[list, list]: A tuple where the first element is a list of words shorter \n    than the average length, and the second element is a list of words longer \n    than the average length.\n\n    Requirements:\n    - statistics\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> analyze_text(\"hello world i am a penguin\")\n    \"\"\"\n    words = S.split()\n    word_lengths = [len(word) for word in words]\n    avg_length = stats.mean(word_lengths)\n\n    shorter_words = [word for word in words if len(word) < avg_length]\n    longer_words = [word for word in words if len(word) > avg_length]\n\n    # Create a DataFrame for plotting\n    df = pd.DataFrame({'Word Length': word_lengths, \n                       'Letter Value Sum': [sum(LETTER_VALUES[char] for char in word) for word in words]})\n\n    # Plot word length and letter value sum histograms\n    df.hist(bins=np.arange(1, df['Word Length'].max() + 2) - 0.5, edgecolor='black')\n    plt.show()\n\n    return shorter_words, longer_words\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nstatistics, re, nltk, matplotlib\n```python\nimport statistics as stats\nimport re\nfrom nltk import FreqDist\nimport matplotlib.pyplot as plt\n\n# Constants\nSTOPWORDS = ['a', 'an', 'the', 'and', 'is', 'in', 'it', 'of', 'or']\n\ndef analyze_text_frequency(S):\n    \"\"\"\n    Analyze a text string by finding the words with lengths shorter or longer than \n    the average length, and plot the frequency of non-stopword words.\n\n    Parameters:\n    S (str): The text string to analyze.\n\n    Returns:\n    Tuple[list, list]: A tuple where the first element is a list of words shorter \n    than the average length, and the second element is a list of words longer \n    than the average length.\n\n    Requirements:\n    - statistics\n    - re\n    - nltk\n    - matplotlib.pyplot\n\n    Example:\n    >>> analyze_text_frequency(\"hello world i am a penguin\")\n    \"\"\"\n    words = re.findall(r'\\b\\w+\\b', S)\n    non_stopwords = [word for word in words if word not in STOPWORDS]\n    word_lengths = [len(word) for word in non_stopwords]\n    avg_length = stats.mean(word_lengths)\n\n    shorter_words = [word for word in non_stopwords if len(word) < avg_length]\n    longer_words = [word for word in non_stopwords if len(word) > avg_length]\n\n    # Get word frequency\n    fdist = FreqDist(non_stopwords)\n\n    # Plot word frequency\n    fdist.plot(30, cumulative=False)\n    plt.show()\n\n    return shorter_words, longer_words\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nstatistics, re, collections, matplotlib\n```python\nimport statistics as stats\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef analyze_text_counter(S):\n    \"\"\"\n    Analyze a text string by finding the words with lengths shorter or longer than \n    the average length, and plot the frequency of all words in a bar chart.\n\n    Parameters:\n    S (str): The text string to analyze.\n\n    Returns:\n    Tuple[list, list]: A tuple where the first element is a list of words shorter \n    than the average length, and the second element is a list of words longer \n    than the average length.\n\n    Requirements:\n    - statistics\n    - re\n    - collections\n    - matplotlib.pyplot\n\n    Example:\n    >>> analyze_text_counter(\"hello world i am a penguin\")\n    \"\"\"\n    words = re.findall(r'\\b\\w+\\b', S)\n    word_lengths = [len(word) for word in words]\n    avg_length = stats.mean(word_lengths)\n\n    shorter_words = [word for word in words if len(word) < avg_length]\n    longer_words = [word for word in words if len(word) > avg_length]\n\n    # Count word frequencies\n    word_freq = Counter(words)\n\n    # Plot word frequencies\n    plt.bar(word_freq.keys(), word_freq.values())\n    plt.show()\n\n    return shorter_words, longer_words\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nstatistics, re, string, matplotlib\n```python\nimport statistics as stats\nimport re\nimport string\nimport matplotlib.pyplot as plt\n\ndef analyze_text_punctuation(S):\n    \"\"\"\n    Analyze a text string by finding the words with lengths shorter or longer than \n    the average length, and plot the frequency of punctuation marks.\n\n    Parameters:\n    S (str): The text string to analyze.\n\n    Returns:\n    Tuple[list, list]: A tuple where the first element is a list of words shorter \n    than the average length, and the second element is a list of words longer \n    than the average length.\n\n    Requirements:\n    - statistics\n    - re\n    - string\n    - matplotlib.pyplot\n\n    Example:\n    >>> analyze_text_punctuation(\"hello, world! i am a penguin.\")\n    \"\"\"\n    words = re.findall(r'\\b\\w+\\b', S)\n    word_lengths = [len(word) for word in words]\n    avg_length = stats.mean(word_lengths)\n\n    shorter_words = [word for word in words if len(word) < avg_length]\n    longer_words = [word for word in words if len(word) > avg_length]\n\n    # Count punctuation frequencies\n    punct_freq = {punct: S.count(punct) for punct in string.punctuation}\n\n    # Plot punctuation frequencies\n    plt.bar(punct_freq.keys(), punct_freq.values())\n    plt.show()\n\n    return shorter_words, longer_words\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nstatistics, re, nltk, wordcloud\n```python\nimport statistics as stats\nimport re\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud\n\n# Constants\nSTOPWORDS = stopwords.words('english')\n\ndef generate_wordcloud(S):\n    \"\"\"\n    Analyze a text string by finding the words with lengths shorter or longer than \n    the average length, and generate a word cloud of non-stopword words.\n\n    Parameters:\n    S (str): The text string to analyze.\n\n    Returns:\n    Tuple[list, list]: A tuple where the first element is a list of words shorter \n    than the average length, and the second element is a list of words longer \n    than the average length.\n\n    Requirements:\n    - statistics\n    - re\n    - nltk.corpus\n    - wordcloud\n\n    Example:\n    >>> generate_wordcloud(\"hello world i am a penguin\")\n    \"\"\"\n    words = re.findall(r'\\b\\w+\\b', S)\n    non_stopwords = [word for word in words if word not in STOPWORDS]\n    word_lengths = [len(word) for word in non_stopwords]\n    avg_length = stats.mean(word_lengths)\n\n    shorter_words = [word for word in non_stopwords if len(word) < avg_length]\n    longer_words = [word for word in non_stopwords if len(word) > avg_length]\n\n    # Generate word cloud\n    wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(' '.join(non_stopwords))\n    plt.figure(figsize=(10, 7))\n    plt.imshow(wordcloud, interpolation=\"bilinear\")\n    plt.axis('off')\n    plt.show()\n\n    return shorter_words, longer_words\n```\n`GPT_ODEX_BREAK`"}
{"seed": "{\"task_id\": 2256, \"prompt\": \"def f_2256(S):\\n\\t\", \"suffix\": \"\\n\\treturn mayores, menores\", \"canonical_solution\": \"\\n\\tls, ts = zip(*sorted([(len(x), x) for x in S.split()]))\\n\\tavlen = sum(ls) / len(ls)  \\n\\tbpoint = bisect.bisect_right(ls, avlen)  \\n\\tmayores = ts[bpoint:]\\n\\tmenores = ts[:bpoint]\\n\", \"test_start\": \"\\nimport bisect\\nimport statistics as stats\\n\\ndef check(candidate):\", \"test\": [\"\\n    S = \\\"hello world i am a penguin\\\"\\n    x, y = candidate(S)\\n    assert x == ('hello', 'world', 'penguin')\\n    assert y == ('a', 'i', 'am')\\n\"], \"entry_point\": \"f_2256\", \"intent\": \"\\u00bfC\\u00f3mo encontrar las palabras de longitud menor `menores` o mayor `mayores` que la media aritmetica de una cadena `S`?\", \"library\": [\"bisect\", \"statistics\"]}", "generation": "Scenario 1: \npandas, pytz, datetime, random, matplotlib, bisect\n```python\nimport pandas as pd\nimport pytz\nfrom datetime import datetime\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport bisect\n\n# Constants\nCITIES = ['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']\nWEATHER_CONDITIONS = ['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy']\nTIMEZONES = {\n    'New York': 'America/New_York',\n    'London': 'Europe/London',\n    'Beijing': 'Asia/Shanghai',\n    'Tokyo': 'Asia/Tokyo',\n    'Sydney': 'Australia/Sydney'\n}\n\ndef generate_weather_report_and_analyze(utc_datetime, condition):\n    \"\"\"\n    Generate a report of weather conditions for a list of cities across various \n    time zones at a given time (UTC) and find the cities where the length of \n    their weather condition is greater than the average length of the weather conditions.\n\n    Parameters:\n    utc_datetime (datetime): The datetime in UTC.\n    condition (str): The condition to compare with the average length of the weather conditions.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with weather conditions for the cities.\n    list: A list of cities where the length of their weather condition is greater than the average length.\n    \n    Requirements:\n    - pandas\n    - pytz\n    - datetime\n    - random\n    - matplotlib.pyplot\n    - bisect\n    \n    Example:\n    >>> utc_time = datetime(2023, 6, 15, 12, 0, 0, tzinfo=pytz.UTC)\n    >>> report, cities = generate_weather_report_and_analyze(utc_time, 'Sunny')\n    >>> print(report)\n    >>> print(cities)\n    >>> report['Weather Condition'].value_counts().plot(kind='bar')\n    \"\"\"\n    report_data = []\n\n    for city in CITIES:\n        city_tz = pytz.timezone(TIMEZONES[city])\n        city_time = utc_datetime.astimezone(city_tz)\n        weather = WEATHER_CONDITIONS[randint(0, len(WEATHER_CONDITIONS)-1)]\n        report_data.append([city, city_time, weather])\n\n    report_df = pd.DataFrame(report_data, columns=['City', 'Local Time', 'Weather Condition'])\n\n    lengths, conditions = zip(*sorted([(len(x), x) for x in report_df['Weather Condition'].values]))\n    avg_length = sum(lengths) / len(lengths)  \n    bpoint = bisect.bisect_right(lengths, avg_length)\n    greater_than_avg = conditions[bpoint:]\n\n    return report_df, list(greater_than_avg)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncollections, bisect, statistics, itertools\n```python\nimport bisect\nimport statistics\nfrom collections import Counter\nimport itertools\n\ndef analyze_string(S, word):\n    \"\"\"\n    Find the words in a string S that have a frequency greater than the average \n    frequency of all words, and calculate the number of words that have a length \n    greater than the length of a given word.\n\n    Parameters:\n    S (str): The input string.\n    word (str): The word to compare with the lengths of the words in S.\n    \n    Returns:\n    list: A list of words that have a frequency greater than the average frequency.\n    int: The number of words that have a length greater than the length of the given word.\n    \n    Requirements:\n    - bisect\n    - statistics\n    - collections.Counter\n    - itertools\n    \n    Example:\n    >>> analyze_string('hello world world i am a penguin penguin penguin', 'hello')\n    \"\"\"\n    word_list = S.split()\n    word_count = Counter(word_list)\n    \n    frequencies = list(word_count.values())\n    avg_frequency = statistics.mean(frequencies)\n    higher_freq_words = [word for word, freq in word_count.items() if freq > avg_frequency]\n    \n    lengths, words = zip(*sorted([(len(x), x) for x in word_list]))\n    bpoint = bisect.bisect_right(lengths, len(word))\n    num_greater_length = len(words) - bpoint\n    \n    return higher_freq_words, num_greater_length\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, bisect, statistics, matplotlib\n```python\nimport numpy as np\nimport bisect\nimport statistics\nimport matplotlib.pyplot as plt\n\ndef analyze_data(data, value):\n    \"\"\"\n    Analyze a list of data, find the values that are greater than the average, \n    and count the number of values that are greater than a given value.\n\n    Parameters:\n    data (list): The list of data.\n    value (float): The value to compare with the data.\n    \n    Returns:\n    numpy.ndarray: A numpy array of values that are greater than the average.\n    int: The number of values that are greater than the given value.\n    \n    Requirements:\n    - numpy\n    - bisect\n    - statistics\n    - matplotlib.pyplot\n    \n    Example:\n    >>> analyze_data([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 5)\n    \"\"\"\n    data = np.array(data)\n    avg = statistics.mean(data)\n    greater_avg = data[data > avg]\n    \n    data.sort()\n    bpoint = bisect.bisect_right(data, value)\n    num_greater_value = len(data) - bpoint\n    \n    plt.hist(data, bins=10)\n    plt.show()\n    \n    return greater_avg, num_greater_value\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nrandom, bisect, statistics, matplotlib\n```python\nimport random\nimport bisect\nimport statistics\nimport matplotlib.pyplot as plt\n\ndef analyze_random_numbers(n, value):\n    \"\"\"\n    Generate n random numbers, find the numbers that are greater than the average, \n    and count the number of numbers that are greater than a given value.\n\n    Parameters:\n    n (int): The number of random numbers to generate.\n    value (float): The value to compare with the random numbers.\n    \n    Returns:\n    list: A list of numbers that are greater than the average.\n    int: The number of numbers that are greater than the given value.\n    \n    Requirements:\n    - random\n    - bisect\n    - statistics\n    - matplotlib.pyplot\n    \n    Example:\n    >>> analyze_random_numbers(100, 0.5)\n    \"\"\"\n    numbers = [random.random() for _ in range(n)]\n    avg = statistics.mean(numbers)\n    greater_avg = [x for x in numbers if x > avg]\n    \n    numbers.sort()\n    bpoint = bisect.bisect_right(numbers, value)\n    num_greater_value = len(numbers) - bpoint\n    \n    plt.plot(numbers)\n    plt.show()\n    \n    return greater_avg, num_greater_value\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, bisect, statistics, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport bisect\nimport statistics\nimport matplotlib.pyplot as plt\n\ndef analyze_dataframe(df, column, value):\n    \"\"\"\n    Analyze a column of a pandas DataFrame, find the values that are greater than \n    the average, and count the number of values that are greater than a given value.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n    column (str): The column to analyze.\n    value (float): The value to compare with the data in the column.\n    \n    Returns:\n    numpy.ndarray: A numpy array of values that are greater than the average.\n    int: The number of values that are greater than the given value.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - bisect\n    - statistics\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = pd.DataFrame({'A': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]})\n    >>> analyze_dataframe(df, 'A', 5)\n    \"\"\"\n    data = df[column].values\n    avg = statistics.mean(data)\n    greater_avg = data[data > avg]\n    \n    data.sort()\n    bpoint = bisect.bisect_right(data, value)\n    num_greater_value = len(data) - bpoint\n    \n    df.hist(column=column, bins=10)\n    plt.show()\n    \n    return greater_avg, num_greater_value\n```\n"}
{"seed": "{\"task_id\": 62209, \"prompt\": \"def f_62209(A, B, P):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"\\n\\tif all(A==P) or all(B==P):\\n\\t\\treturn 0\\n\\telif np.arccos(np.dot((P-A)/norm(P-A), (B-A)/norm(B-A))) > np.pi/2:\\n\\t\\treturn norm(P-A)\\n\\telif np.arccos(np.dot((P-B)/norm(P-B), (A-B)/norm(A-B))) > np.pi/2:\\n\\t\\treturn norm(P-B)\\n\\treturn norm(np.cross(B-A, A-P))/norm(B-A)\\n\", \"test_start\": \"\\nimport numpy as np\\nfrom numpy.linalg import norm\\n\\ndef check(candidate):\", \"test\": [\"\\n    A = np.array((5,3))\\n    B = np.array((2,1))\\n    P = np.array((1,2))\\n    assert candidate(A, B, P) == 1.4142135623730951\\n\"], \"entry_point\": \"f_62209\", \"intent\": \"\\u00bfC\\u00f3mo calcular la distancia entre un punto `P` y el segmento acotado por los puntos `A` y `B`?\", \"library\": [\"numpy\"]}", "generation": "Scenario 3: \nnumpy, matplotlib, math, random\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\nfrom random import randint\n\n# Constants\nCOLORS = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n\ndef draw_and_calculate_distance(points, P):\n    \"\"\"\n    Draw a plot with given points and a point P, calculate and display the shortest distance \n    from point P to each line segment formed by the points.\n\n    Parameters:\n    points (list): The list of points. Each point is a tuple of two numbers.\n    P (tuple): The point P.\n\n    Returns:\n    list: The list of shortest distances from point P to each line segment.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - math\n    - random\n\n    Example:\n    >>> draw_and_calculate_distance([(0, 0), (1, 1), (2, 0), (3, 1), (4, 0)], (2, 2))\n    \"\"\"\n    distances = []\n    fig, ax = plt.subplots()\n\n    for i in range(len(points) - 1):\n        A = np.array(points[i])\n        B = np.array(points[i+1])\n        P = np.array(P)\n        if np.all(A==P) or np.all(B==P):\n            distances.append(0)\n        elif np.arccos(np.dot((P-A)/np.linalg.norm(P-A), (B-A)/np.linalg.norm(B-A))) > np.pi/2:\n            distances.append(np.linalg.norm(P-A))\n        elif np.arccos(np.dot((P-B)/np.linalg.norm(P-B), (A-B)/np.linalg.norm(A-B))) > np.pi/2:\n            distances.append(np.linalg.norm(P-B))\n        else:\n            distances.append(np.linalg.norm(np.cross(B-A, A-P))/np.linalg.norm(B-A))\n\n        ax.plot([A[0], B[0]], [A[1], B[1]], color=COLORS[randint(0, len(COLORS)-1)])\n\n    ax.scatter(*P, color='red')\n    ax.text(P[0], P[1], f'P({P[0]}, {P[1]})', fontsize=12, ha='right')\n\n    plt.show()\n\n    return distances\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, scipy.optimize, matplotlib\n```python\nimport numpy as np\nimport scipy.optimize\nimport matplotlib.pyplot as plt\n\n# Constants\nA = np.array([5, 3])\nB = np.array([2, 1])\n\ndef fit_line_and_calculate_distance(P):\n    \"\"\"\n    Fit a line to points A and B, plot the points and the line, and calculate the \n    shortest distance from point P to the line.\n\n    Parameters:\n    P (tuple): The point P.\n\n    Returns:\n    float: The shortest distance from point P to the line.\n\n    Requirements:\n    - numpy\n    - scipy.optimize\n    - matplotlib.pyplot\n\n    Example:\n    >>> fit_line_and_calculate_distance((1, 2))\n    \"\"\"\n    # Fit a line to points A and B\n    def line(x, a, b):\n        return a * x + b\n\n    popt, pcov = scipy.optimize.curve_fit(line, [A[0], B[0]], [A[1], B[1]])\n\n    # Plot the points and the line\n    plt.plot([A[0], B[0]], [A[1], B[1]], 'ko')\n    plt.plot(P[0], P[1], 'ro')\n    x = np.linspace(min([A[0], B[0], P[0]])-1, max([A[0], B[0], P[0]])+1, 100)\n    y = popt[0] * x + popt[1]\n    plt.plot(x, y, 'b-')\n    plt.show()\n\n    # Calculate the shortest distance from point P to the line\n    distance = abs(popt[0]*P[0] - P[1] + popt[1]) / np.sqrt(popt[0]**2 + 1)\n\n    return distance\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, matplotlib, math, sklearn.metrics\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\nfrom sklearn.metrics import pairwise_distances\n\n# Constants\nPOINTS = np.array([[5,3], [2,1], [7,8], [6,9], [1,2], [4,5]])\n\ndef calculate_and_plot_distances(P):\n    \"\"\"\n    Calculate the Euclidean distance from a point P to each point in a given set of points, \n    and plot the point set and point P.\n\n    Parameters:\n    P (tuple): The point P.\n\n    Returns:\n    list: The list of Euclidean distances.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - math\n    - sklearn.metrics.pairwise_distances\n\n    Example:\n    >>> calculate_and_plot_distances((1, 2))\n    \"\"\"\n    distances = pairwise_distances(POINTS, np.array([P])).flatten()\n\n    plt.scatter(POINTS[:, 0], POINTS[:, 1], color='blue')\n    plt.scatter(*P, color='red')\n    plt.show()\n\n    return distances.tolist()\n```\n`GPT_ODEX_BREAK`\n\nScenario 6:\nnumpy, scipy.spatial, matplotlib\n```python\nimport numpy as np\nfrom scipy.spatial import distance\nimport matplotlib.pyplot as plt\n\n# Constants\nPOINTS = np.array([[5,3], [2,1], [7,8], [6,9], [1,2], [4,5]])\n\ndef calculate_distances_and_plot(P):\n    \"\"\"\n    Calculate the Manhattan distance from a point P to each point in a given set of points, \n    and plot the point set and point P.\n\n    Parameters:\n    P (tuple): The point P.\n\n    Returns:\n    list: The list of Manhattan distances.\n\n    Requirements:\n    - numpy\n    - scipy.spatial.distance\n    - matplotlib.pyplot\n\n    Example:\n    >>> calculate_distances_and_plot((1, 2))\n    \"\"\"\n    distances = [distance.cityblock(P, point) for point in POINTS]\n\n    plt.scatter(POINTS[:, 0], POINTS[:, 1], color='blue')\n    plt.scatter(*P, color='red')\n    plt.show()\n\n    return distances\n```\n`GPT_ODEX_BREAK`\n\nScenario 7:\nnumpy, matplotlib, math, itertools\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\nimport itertools\n\n# Constants\nPOINTS = [(5,3), (2,1), (7,8), (6,9), (1,2), (4,5)]\n\ndef calculate_and_plot_polygon_area(P):\n    \"\"\"\n    Create a polygon with a given set of points and a point P, calculate and return its area, \n    and plot the polygon.\n\n    Parameters:\n    P (tuple): The point P.\n\n    Returns:\n    float: The area of the polygon.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - math\n    - itertools\n\n    Example:\n    >>> calculate_and_plot_polygon_area((1, 2))\n    \"\"\"\n    polygon = POINTS + [P]\n    \n    # Calculate the area of the polygon\n    area = 0.5 * abs(sum(x0*y1 - x1*y0 for ((x0, y0), (x1, y1)) in itertools.zip_longest(polygon, polygon[1:]+polygon[:1])))\n\n    # Plot the polygon\n    polygon_np = np.array(polygon)\n    plt.fill(polygon_np[:, 0], polygon_np[:, 1], 'b')\n    plt.show()\n\n    return area\n```"}
{"seed": "{\"task_id\": 62798, \"prompt\": \"def f_62798(life, f, c):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"\\n\\tfs = life[f-1]\\n\\tfi = life[(f+1)%len(life)]\\n\\ti=(c+1)%len(life[0])\\n\\treturn (fs[c-1], fs[c], fs[i], life[f][c-1], life[f][i], fi[c-1], fi[c], fi[i]).count('#')\\n\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate((['.']), 0, 0) == 0\\n\", \"\\n    assert candidate((['..', '..']), 0, 0) == 0\\n\", \"\\n    assert candidate((['..', '..']), 0, 1) == 0\\n\", \"\\n    assert candidate((['##', '..']), 0, 0) == 2\\n\", \"\\n    assert candidate((['##', '..']), 0, 1) == 2\\n\", \"\\n    assert candidate((['#.', '.#']), 0, 0) == 4\\n\", \"\\n    assert candidate((['##', '##']), 0, 0) == 8\\n\", \"\\n    assert candidate((['.#.', '#.#', '.#.']), 1, 1) == 4\\n\", \"\\n    assert candidate((['.#.', '..#', '.#.']), 1, 1) == 3\\n\"], \"entry_point\": \"f_62798\", \"intent\": \"\\u00bfC\\u00f3mo encontrar las celdas adyacentes a la celda en la fila `f` y la columna `c` en una matriz `life` llena de #?\", \"library\": []}", "generation": "Scenario 1: \nnumpy, random, matplotlib.pyplot\n```python\nimport numpy as np\nfrom random import choice\nimport matplotlib.pyplot as plt\n\n# Constants\nSTATES = ['.', '#']\n\ndef random_life_grid(rows, cols):\n    \"\"\"\n    Generate a random life grid and plot it using matplotlib.\n\n    Parameters:\n    rows (int): Number of rows in the grid.\n    cols (int): Number of columns in the grid.\n\n    Returns:\n    2D numpy array: A 2D numpy array representing the life grid.\n\n    Requirements:\n    - numpy\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> grid = random_life_grid(10, 10)\n    >>> plt.imshow(grid, cmap='gray')\n    \"\"\"\n    grid = np.array([[choice(STATES) for _ in range(cols)] for _ in range(rows)])\n\n    return grid\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, scipy.ndimage, matplotlib.pyplot\n```python\nimport numpy as np\nfrom scipy.ndimage import convolve\nimport matplotlib.pyplot as plt\n\n# Constants\nKERNEL = np.array([[1, 1, 1], [1, 0, 1], [1, 1, 1]])\n\ndef next_generation_life_grid(life_grid):\n    \"\"\"\n    Compute the next generation of the life grid according to the game rules.\n\n    Parameters:\n    life_grid (2D numpy array): The current life grid.\n\n    Returns:\n    2D numpy array: The next generation life grid.\n\n    Requirements:\n    - numpy\n    - scipy.ndimage\n    - matplotlib.pyplot\n\n    Example:\n    >>> life_grid = np.array([['.', '#', '.'], ['#', '.', '#'], ['.', '#', '.']])\n    >>> next_gen = next_generation_life_grid(life_grid)\n    >>> plt.imshow(next_gen, cmap='gray')\n    \"\"\"\n    life_grid_binary = (life_grid == '#').astype(int)\n    neighbors_count = convolve(life_grid_binary, KERNEL, mode='wrap')\n\n    next_gen = np.where(((life_grid_binary == 1) & ((neighbors_count < 2) | (neighbors_count > 3))) |\n                        ((life_grid_binary == 0) & (neighbors_count == 3)), '#', '.')\n\n    return next_gen\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, random, matplotlib.pyplot\n```python\nimport numpy as np\nfrom random import choice\nimport matplotlib.pyplot as plt\n\n# Constants\nSTATES = ['.', '#']\n\ndef initialize_and_evolve_life_grid(rows, cols, generations):\n    \"\"\"\n    Initialize a random life grid and evolve it for a given number of generations.\n\n    Parameters:\n    rows (int): Number of rows in the grid.\n    cols (int): Number of columns in the grid.\n    generations (int): Number of generations to evolve.\n\n    Returns:\n    list: A list of 2D numpy arrays representing the life grid at each generation.\n\n    Requirements:\n    - numpy\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> grid = initialize_and_evolve_life_grid(10, 10, 5)\n    >>> for i, g in enumerate(grid):\n    >>>     plt.subplot(1, len(grid), i+1)\n    >>>     plt.imshow(g, cmap='gray')\n    \"\"\"\n    grid = np.array([[choice(STATES) for _ in range(cols)] for _ in range(rows)])\n    grids = [grid]\n\n    for _ in range(generations):\n        grid = next_generation_life_grid(grid)\n        grids.append(grid)\n\n    return grids\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, scipy.ndimage, matplotlib.pyplot, matplotlib.animation\n```python\nimport numpy as np\nfrom scipy.ndimage import convolve\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\n\n# Constants\nKERNEL = np.array([[1, 1, 1], [1, 0, 1], [1, 1, 1]])\n\ndef animate_life_grid(life_grid, frames):\n    \"\"\"\n    Animate the evolution of the life grid for a given number of frames.\n\n    Parameters:\n    life_grid (2D numpy array): The initial life grid.\n    frames (int): Number of frames to animate.\n\n    Returns:\n    None.\n\n    Requirements:\n    - numpy\n    - scipy.ndimage\n    - matplotlib.pyplot\n    - matplotlib.animation\n\n    Example:\n    >>> life_grid = np.array([['.', '#', '.'], ['#', '.', '#'], ['.', '#', '.']])\n    >>> animate_life_grid(life_grid, 50)\n    \"\"\"\n    fig = plt.figure()\n\n    ims = []\n    for _ in range(frames):\n        life_grid = next_generation_life_grid(life_grid)\n        ims.append((plt.imshow(life_grid, cmap='gray'),))\n\n    ani = animation.ArtistAnimation(fig, ims, interval=200, blit=True, repeat_delay=1000)\n\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, scipy.ndimage, matplotlib.pyplot, random\n```python\nimport numpy as np\nfrom scipy.ndimage import convolve\nimport matplotlib.pyplot as plt\nfrom random import choice\n\n# Constants\nSTATES = ['.', '#']\nKERNEL = np.array([[1, 1, 1], [1, 0, 1], [1, 1, 1]])\n\ndef evolve_life_grid_with_random_initial_state(rows, cols, generations):\n    \"\"\"\n    Initialize a random life grid and evolve it for a given number of generations, \n    and then plot the initial and final states.\n\n    Parameters:\n    rows (int): Number of rows in the grid.\n    cols (int): Number of columns in the grid.\n    generations (int): Number of generations to evolve.\n\n    Returns:\n    None.\n\n    Requirements:\n    - numpy\n    - scipy.ndimage\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> evolve_life_grid_with_random_initial_state(10, 10, 5)\n    \"\"\"\n    grid = np.array([[choice(STATES) for _ in range(cols)] for _ in range(rows)])\n\n    plt.subplot(1, 2, 1)\n    plt.imshow(grid, cmap='gray')\n    plt.title('Initial State')\n\n    for _ in range(generations):\n        grid = next_generation_life_grid(grid)\n\n    plt.subplot(1, 2, 2)\n    plt.imshow(grid, cmap='gray')\n    plt.title('Final State')\n\n    plt.show()\n```"}
{"seed": "{\"task_id\": 34346, \"prompt\": \"def f_34346(dbname, user, password, host):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"\\n\\tport = 3306\\n\\tengine = sqlalchemy.create_engine('mysql://'+user+':'+password+'@+'+str(host)+'/'+dbname+'?charset=utf8&use_unicode=0', pool_recycle=port)\\n\\tconnection = engine.connect()\\n\", \"test_start\": \"\\nimport sqlalchemy\\nfrom unittest.mock import Mock\\n\\ndef check(candidate):\", \"test\": [\"\\n    class my_engine:\\n        def __init__(self):\\n            pass\\n        def connect(self):\\n            return 0\\n    engine = my_engine()\\n    sqlalchemy.create_engine = Mock(return_value = engine)\\n    try:\\n        candidate('gaurishankar', 'hyena', 'hyena123', '127.0.0.1')\\n    except:\\n        assert False\\n\"], \"entry_point\": \"f_34346\", \"intent\": \"\\u00bfComo conectarme con SQLALCHEMY a una basededatos `dbname` con usuario `user`, contrase\\u00f1a `password`, host `host` y ?\", \"library\": [\"sqlalchemy\"]}", "generation": "Scenario 1:\nsqlalchemy, pandas, datetime, matplotlib\n```python\nimport pandas as pd\nimport sqlalchemy\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\n\n# Constants\nDATABASE_NAME = 'test_db'\nUSER_NAME = 'test_user'\nPASSWORD = 'password'\nHOST = 'localhost'\nTABLE_NAME = 'sales_data'\n\ndef analyze_sales_data(dbname=DATABASE_NAME, user=USER_NAME, password=PASSWORD, host=HOST):\n    \"\"\"\n    Connect to a MySQL database using sqlalchemy, fetch data from a sales_data table, \n    and perform a simple analysis of the data. The analysis includes calculating the \n    total sales per month and plotting this data.\n\n    Parameters:\n    dbname (str): The name of the database.\n    user (str): The username to connect to the database.\n    password (str): The password to connect to the database.\n    host (str): The host of the database.\n\n    Returns:\n    DataFrame: A pandas DataFrame with total sales per month.\n\n    Requirements:\n    - sqlalchemy\n    - pandas\n    - datetime\n    - matplotlib.pyplot\n\n    Example:\n    >>> analyze_sales_data()\n    \"\"\"\n    engine = sqlalchemy.create_engine(f'mysql://{user}:{password}@{host}/{dbname}')\n    df = pd.read_sql_table(TABLE_NAME, engine)\n\n    df['month'] = df['date'].dt.month\n    sales_by_month = df.groupby('month')['sales'].sum()\n\n    sales_by_month.plot(kind='bar')\n\n    return sales_by_month\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nsqlalchemy, pandas, os, glob\n```python\nimport pandas as pd\nimport sqlalchemy\nimport os\nimport glob\n\n# Constants\nDATABASE_NAME = 'test_db'\nUSER_NAME = 'test_user'\nPASSWORD = 'password'\nHOST = 'localhost'\nDATA_DIRECTORY = './data/'\n\ndef load_data_to_database(dbname=DATABASE_NAME, user=USER_NAME, password=PASSWORD, host=HOST, data_dir=DATA_DIRECTORY):\n    \"\"\"\n    Connect to a MySQL database using sqlalchemy and load all CSV files from a specific directory into a table in the database.\n\n    Parameters:\n    dbname (str): The name of the database.\n    user (str): The username to connect to the database.\n    password (str): The password to connect to the database.\n    host (str): The host of the database.\n    data_dir (str): The directory where the CSV files are located.\n\n    Returns:\n    list: A list of the names of the tables created.\n\n    Requirements:\n    - sqlalchemy\n    - pandas\n    - os\n    - glob\n\n    Example:\n    >>> load_data_to_database()\n    \"\"\"\n    engine = sqlalchemy.create_engine(f'mysql://{user}:{password}@{host}/{dbname}')\n\n    table_names = []\n    csv_files = glob.glob(os.path.join(data_dir, '*.csv'))\n\n    for file in csv_files:\n        df = pd.read_csv(file)\n        table_name = os.path.splitext(os.path.basename(file))[0]\n        df.to_sql(table_name, engine, if_exists='replace')\n        table_names.append(table_name)\n\n    return table_names\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nsqlalchemy, pandas, sklearn, numpy\n```python\nimport pandas as pd\nimport sqlalchemy\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\n\n# Constants\nDATABASE_NAME = 'test_db'\nUSER_NAME = 'test_user'\nPASSWORD = 'password'\nHOST = 'localhost'\nTABLE_NAME = 'sales_data'\n\ndef model_sales_data(dbname=DATABASE_NAME, user=USER_NAME, password=PASSWORD, host=HOST):\n    \"\"\"\n    Connect to a MySQL database using sqlalchemy, fetch data from a sales_data table,\n    split it into training and testing datasets, and fit a linear regression model on it.\n\n    Parameters:\n    dbname (str): The name of the database.\n    user (str): The username to connect to the database.\n    password (str): The password to connect to the database.\n    host (str): The host of the database.\n\n    Returns:\n    float: The R-squared score of the fitted model.\n\n    Requirements:\n    - sqlalchemy\n    - pandas\n    - sklearn\n    - numpy\n\n    Example:\n    >>> model_sales_data()\n    \"\"\"\n    engine = sqlalchemy.create_engine(f'mysql://{user}:{password}@{host}/{dbname}')\n    df = pd.read_sql_table(TABLE_NAME, engine)\n\n    X = df.drop('sales', axis=1)\n    y = df['sales']\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    score = model.score(X_test, y_test)\n\n    return score\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nsqlalchemy, pandas, seaborn, matplotlib\n```python\nimport pandas as pd\nimport sqlalchemy\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nDATABASE_NAME = 'test_db'\nUSER_NAME = 'test_user'\nPASSWORD = 'password'\nHOST = 'localhost'\nTABLE_NAME = 'sales_data'\n\ndef visualize_sales_data(dbname=DATABASE_NAME, user=USER_NAME, password=PASSWORD, host=HOST):\n    \"\"\"\n    Connect to a MySQL database using sqlalchemy, fetch data from a sales_data table, \n    and visualize the data using seaborn.\n\n    Parameters:\n    dbname (str): The name of the database.\n    user (str): The username to connect to the database.\n    password (str): The password to connect to the database.\n    host (str): The host of the database.\n\n    Returns:\n    None\n\n    Requirements:\n    - sqlalchemy\n    - pandas\n    - seaborn\n    - matplotlib.pyplot\n\n    Example:\n    >>> visualize_sales_data()\n    \"\"\"\n    engine = sqlalchemy.create_engine(f'mysql://{user}:{password}@{host}/{dbname}')\n    df = pd.read_sql_table(TABLE_NAME, engine)\n\n    sns.pairplot(df)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nsqlalchemy, pandas, datetime, csv\n```python\nimport pandas as pd\nimport sqlalchemy\nfrom datetime import datetime\nimport csv\n\n# Constants\nDATABASE_NAME = 'test_db'\nUSER_NAME = 'test_user'\nPASSWORD = 'password'\nHOST = 'localhost'\nTABLE_NAME = 'sales_data'\nCSV_FILE = 'sales_data.csv'\n\ndef export_data(dbname=DATABASE_NAME, user=USER_NAME, password=PASSWORD, host=HOST, csv_file=CSV_FILE):\n    \"\"\"\n    Connect to a MySQL database using sqlalchemy, fetch data from a sales_data table, \n    and export the data to a CSV file.\n\n    Parameters:\n    dbname (str): The name of the database.\n    user (str): The username to connect to the database.\n    password (str): The password to connect to the database.\n    host (str): The host of the database.\n    csv_file (str): The CSV file to export the data to.\n\n    Returns:\n    str: The name of the CSV file.\n\n    Requirements:\n    - sqlalchemy\n    - pandas\n    - datetime\n    - csv\n\n    Example:\n    >>> export_data()\n    \"\"\"\n    engine = sqlalchemy.create_engine(f'mysql://{user}:{password}@{host}/{dbname}')\n    df = pd.read_sql_table(TABLE_NAME, engine)\n\n    df.to_csv(csv_file, index=False)\n\n    return csv_file\n```\nAbove is the illustration."}
{"seed": "{\"task_id\": 26959, \"prompt\": \"def f_26959():\\n\\t\", \"suffix\": \"\\n\\treturn mcd\", \"canonical_solution\": \"def mcd(a,b):\\n\\t    return next((i for i in range(min(a,b),0,-1) if a%i==0 and b%i==0), 0)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate()(25, 30) == 5\\n\", \"\\n    assert candidate()(24, 30) == 6\\n\", \"\\n    assert candidate()(23, 30) == 1\\n\"], \"entry_point\": \"f_26959\", \"intent\": \"\\u00bfC\\u00f3mo calcular el m\\u00e1ximo com\\u00fan divisor entre dos n\\u00fameros `a` y `b`?\", \"library\": []}", "generation": "Scenario 1: \nnumpy, functools, itertools\n```python\nimport numpy as np\nfrom functools import reduce\nimport itertools\n\ndef greatest_common_divisor(numbers):\n    \"\"\"\n    Calculate the greatest common divisor (GCD) among a list of numbers.\n\n    Parameters:\n    numbers (list): A list of integers.\n\n    Returns:\n    int: The GCD of the numbers.\n\n    Requirements:\n    - numpy\n    - functools\n    - itertools\n\n    Example:\n    >>> greatest_common_divisor([25, 30, 35])\n    5\n    \"\"\"\n    gcd = lambda a, b: a if b==0 else gcd(b, a%b)\n    return reduce(gcd, numbers)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, matplotlib.pyplot\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nRANGE_START = 1\nRANGE_END = 100\n\ndef plot_gcd_distribution(range_start=RANGE_START, range_end=RANGE_END):\n    \"\"\"\n    Calculate the GCD of all pairs of numbers in a range and plot the distribution of the GCDs.\n\n    Parameters:\n    range_start (int): The start of the range. Default is 1.\n    range_end (int): The end of the range. Default is 100.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_gcd_distribution(1, 100)\n    \"\"\"\n    def gcd(a, b):\n        while b:\n            a, b = b, a % b\n        return a\n\n    gcds = [gcd(a, b) for a, b in itertools.combinations(range(range_start, range_end+1), 2)]\n    \n    plt.hist(gcds, bins=np.arange(1, np.max(gcds)+2)-0.5, edgecolor='black')\n    plt.xlabel('GCD')\n    plt.ylabel('Count')\n    plt.title('Distribution of GCDs')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, math, matplotlib.pyplot\n```python\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\n\n# Constants\nRANGE_START = 1\nRANGE_END = 100\n\ndef plot_relative_prime_distribution(range_start=RANGE_START, range_end=RANGE_END):\n    \"\"\"\n    Calculate the number of relative primes for each number in a range and plot the distribution.\n\n    Parameters:\n    range_start (int): The start of the range. Default is 1.\n    range_end (int): The end of the range. Default is 100.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - math\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_relative_prime_distribution(1, 100)\n    \"\"\"\n    def gcd(a, b):\n        while b:\n            a, b = b, a % b\n        return a\n\n    relative_primes = [sum(1 for b in range(range_start, range_end+1) if gcd(a, b) == 1) for a in range(range_start, range_end+1)]\n    \n    plt.hist(relative_primes, bins=np.arange(np.min(relative_primes)-0.5, np.max(relative_primes)+1.5), edgecolor='black')\n    plt.xlabel('Number of Relative Primes')\n    plt.ylabel('Count')\n    plt.title('Distribution of Relative Primes')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, math, matplotlib.pyplot\n```python\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\n\n# Constants\nRANGE_START = 1\nRANGE_END = 100\n\ndef plot_gcd_vs_numbers(range_start=RANGE_START, range_end=RANGE_END):\n    \"\"\"\n    Calculate the GCD for each number with the maximum number in a range and plot the GCDs versus the numbers.\n\n    Parameters:\n    range_start (int): The start of the range. Default is 1.\n    range_end (int): The end of the range. Default is 100.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - math\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_gcd_vs_numbers(1, 100)\n    \"\"\"\n    def gcd(a, b):\n        while b:\n            a, b = b, a % b\n        return a\n\n    numbers = np.arange(range_start, range_end+1)\n    gcds = [gcd(number, range_end) for number in numbers]\n\n    plt.plot(numbers, gcds)\n    plt.xlabel('Number')\n    plt.ylabel('GCD with '+str(range_end))\n    plt.title('GCDs versus Numbers')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, math, matplotlib.pyplot\n```python\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\n\n# Constants\nRANGE_START = 1\nRANGE_END = 100\n\ndef plot_relative_primes_vs_numbers(range_start=RANGE_START, range_end=RANGE_END):\n    \"\"\"\n    Calculate the number of relative primes for each number in a range and plot the numbers of relative primes versus the numbers.\n\n    Parameters:\n    range_start (int): The start of the range. Default is 1.\n    range_end (int): The end of the range. Default is 100.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - math\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_relative_primes_vs_numbers(1, 100)\n    \"\"\"\n    def gcd(a, b):\n        while b:\n            a, b = b, a % b\n        return a\n\n    numbers = np.arange(range_start, range_end+1)\n    relative_primes = [sum(1 for b in numbers if gcd(a, b) == 1) for a in numbers]\n\n    plt.plot(numbers, relative_primes)\n    plt.xlabel('Number')\n    plt.ylabel('Number of Relative Primes')\n    plt.title('Numbers of Relative Primes versus Numbers')\n    plt.show()\n```"}
{"seed": "{\"task_id\": 44385, \"prompt\": \"def f_44385(command):\\n\\t\", \"suffix\": \"\\n\\treturn output\", \"canonical_solution\": \"process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=None, shell=True)\\n\\t\\n\\toutput = process.communicate()\\n\\t\\n\\tprint(output[0])\", \"test_start\": \"\\nimport subprocess\\nfrom unittest.mock import Mock\\n\\ndef check(candidate):\", \"test\": [\"\\n    class Process:\\n        def __init__(self):\\n            pass\\n        def communicate(self):\\n            return [1, 2, 3]\\n    proc = Process()\\n    subprocess.Popen = Mock(return_value = proc)\\n    assert candidate('python3')[0] == 1\\n\"], \"entry_point\": \"f_44385\", \"intent\": \"\\u00bfC\\u00f3mo capturar la salida del terminal despu\\u00e9s de ejecutar un comando `command`?\", \"library\": [\"subprocess\"]}", "generation": "Scenario 1:\nsubprocess, os, fnmatch, shutil\n```python\nimport subprocess\nimport os\nimport fnmatch\nimport shutil\n\n# Constants\nFILE_PATTERN = '*.txt'\n\ndef move_and_execute(command, source_dir, dest_dir):\n    \"\"\"\n    Move all text files from source_dir to dest_dir and execute a terminal command.\n\n    Parameters:\n    command (str): The terminal command to execute.\n    source_dir (str): The directory from which text files need to be moved.\n    dest_dir (str): The directory to which text files need to be moved.\n\n    Returns:\n    str: The output of the terminal command.\n\n    Requirements:\n    - subprocess\n    - os\n    - fnmatch\n    - shutil\n\n    Example:\n    >>> move_and_execute('ls', '/home/user/documents', '/home/user/Desktop')\n    \"\"\"\n    for root, dirnames, filenames in os.walk(source_dir):\n        for filename in fnmatch.filter(filenames, FILE_PATTERN):\n            shutil.move(os.path.join(root, filename), dest_dir)\n\n    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=None, shell=True)\n    output = process.communicate()\n\n    return output[0]\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nsubprocess, os, time\n```python\nimport subprocess\nimport os\nimport time\n\n# Constants\nCOMMANDS = ['ls', 'pwd', 'date']\n\ndef execute_commands_in_sequence(path):\n    \"\"\"\n    Change to a given directory and execute a sequence of commands with a delay of 1 second between each command.\n\n    Parameters:\n    path (str): The directory path to change to.\n\n    Returns:\n    list: A list of outputs from each command.\n\n    Requirements:\n    - subprocess\n    - os\n    - time\n\n    Example:\n    >>> execute_commands_in_sequence('/home/user/documents')\n    \"\"\"\n    os.chdir(path)\n    outputs = []\n\n    for command in COMMANDS:\n        process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=None, shell=True)\n        output = process.communicate()\n        outputs.append(output[0])\n        time.sleep(1)\n\n    return outputs\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nsubprocess, csv, sys\n```python\nimport subprocess\nimport csv\nimport sys\n\n# Constants\nCSV_FILE = 'commands.csv'\n\ndef execute_commands_from_csv():\n    \"\"\"\n    Read a CSV file with a list of commands and execute each command in sequence.\n\n    Returns:\n    list: A list of outputs from each command.\n\n    Requirements:\n    - subprocess\n    - csv\n    - sys\n\n    Example:\n    >>> execute_commands_from_csv()\n    \"\"\"\n    outputs = []\n\n    with open(CSV_FILE, 'r') as f:\n        reader = csv.reader(f)\n        for row in reader:\n            command = row[0]\n            process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=None, shell=True)\n            output = process.communicate()\n            outputs.append(output[0])\n\n    return outputs\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nsubprocess, os, multiprocessing\n```python\nimport subprocess\nimport os\nfrom multiprocessing import Pool\n\n# Constants\nCOMMANDS = ['ls', 'pwd', 'date']\n\ndef execute_commands_in_parallel():\n    \"\"\"\n    Execute a list of commands in parallel.\n\n    Returns:\n    list: A list of outputs from each command.\n\n    Requirements:\n    - subprocess\n    - os\n    - multiprocessing\n\n    Example:\n    >>> execute_commands_in_parallel()\n    \"\"\"\n    def execute_command(command):\n        process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=None, shell=True)\n        output = process.communicate()\n        return output[0]\n\n    with Pool(processes=len(COMMANDS)) as pool:\n        outputs = pool.map(execute_command, COMMANDS)\n\n    return outputs\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nsubprocess, os, argparse\n```python\nimport subprocess\nimport os\nimport argparse\n\ndef execute_command():\n    \"\"\"\n    Parse command line arguments and execute a given command.\n\n    Returns:\n    str: The output of the command.\n\n    Requirements:\n    - subprocess\n    - os\n    - argparse\n\n    Example:\n    >>> execute_command()\n    \"\"\"\n    parser = argparse.ArgumentParser(description='Execute a command.')\n    parser.add_argument('command', type=str, help='The command to execute')\n\n    args = parser.parse_args()\n\n    process = subprocess.Popen(args.command, stdout=subprocess.PIPE, stderr=None, shell=True)\n    output = process.communicate()\n\n    return output[0]\n```"}
{"seed": "{\"task_id\": 28562, \"prompt\": \"def f_28562(S):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"int(S)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('123') == 123\\n\", \"\\n    assert candidate('10000') == 10000\\n\", \"\\n    assert candidate('0') == 0\\n\"], \"entry_point\": \"f_28562\", \"intent\": \"\\u00bfC\\u00f3mo convertir un string `S` a un entero?\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, matplotlib, sklearn\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n# Constants\nREGRESSION_COEF = 0.5\n\ndef analyze_data(S):\n    \"\"\"\n    Convert a string of comma-separated numbers into a pandas Series, calculate the cumulative sum,\n    perform a simple linear regression with a slope of 0.5 and plot the results.\n\n    Parameters:\n    S (str): The string of comma-separated numbers.\n\n    Returns:\n    float: The coefficient of determination (R^2) of the prediction.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.linear_model.LinearRegression\n\n    Example:\n    >>> analyze_data('1,2,3,4,5,6,7,8,9,10')\n    \"\"\"\n    # Convert string to pandas series and calculate cumulative sum\n    data = pd.Series([int(i) for i in S.split(',')]).cumsum()\n\n    # Perform linear regression\n    X = np.array(data.index).reshape(-1, 1)\n    Y = np.array(data.values).reshape(-1, 1)\n    reg = LinearRegression().fit(X, Y)\n\n    # Plot the data and the regression line\n    plt.scatter(X, Y, color='blue')\n    plt.plot(X, reg.predict(X), color='red')\n    plt.show()\n\n    return reg.score(X, Y)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nnumpy, scipy.stats, matplotlib\n```python\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\n# Constants\nMEAN = 0\nSTANDARD_DEVIATION = 1\n\ndef plot_normal_distribution(S):\n    \"\"\"\n    Convert a string of comma-separated numbers into a numpy array, calculate the z-scores,\n    plot the histogram of the data and the corresponding normal distribution.\n\n    Parameters:\n    S (str): The string of comma-separated numbers.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - scipy.stats.norm\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_normal_distribution('1,2,3,4,5,6,7,8,9,10')\n    \"\"\"\n    # Convert string to numpy array and calculate z-scores\n    data = np.array([int(i) for i in S.split(',')])\n    z_scores = (data - np.mean(data)) / np.std(data)\n\n    # Plot histogram and normal distribution\n    plt.hist(z_scores, density=True, bins=30, color='blue', alpha=0.7)\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    plt.plot(x, norm.pdf(x, MEAN, STANDARD_DEVIATION), 'r', linewidth=2)\n    plt.show()\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 3: \npandas, datetime, matplotlib\n```python\nimport pandas as pd\nfrom datetime import datetime, timedelta\nimport matplotlib.pyplot as plt\n\n# Constants\nSTART_DATE = datetime(2021, 1, 1)\n\ndef analyze_time_series(S):\n    \"\"\"\n    Convert a string of comma-separated numbers into a pandas DataFrame with a DatetimeIndex starting from 2021-01-01,\n    perform a rolling mean with a window size of 3, and plot the original data and the rolling mean.\n\n    Parameters:\n    S (str): The string of comma-separated numbers.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - datetime\n    - matplotlib.pyplot\n\n    Example:\n    >>> analyze_time_series('1,2,3,4,5,6,7,8,9,10')\n    \"\"\"\n    # Convert string to pandas DataFrame with DatetimeIndex\n    data = pd.DataFrame({'Value': [int(i) for i in S.split(',')]})\n    data.index = pd.date_range(start=START_DATE, periods=len(data))\n\n    # Calculate rolling mean\n    rolling_mean = data.rolling(window=3).mean()\n\n    # Plot original data and rolling mean\n    plt.plot(data, label='Original')\n    plt.plot(rolling_mean, label='Rolling Mean')\n    plt.legend(loc='best')\n    plt.show()\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 4: \npandas, numpy, sklearn.cluster, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\n# Constants\nN_CLUSTERS = 3\n\ndef perform_clustering(S):\n    \"\"\"\n    Convert a string of comma-separated numbers into a pandas DataFrame,\n    perform k-means clustering with 3 clusters, and plot the data with different colors for each cluster.\n\n    Parameters:\n    S (str): The string of comma-separated numbers.\n\n    Returns:\n    list: The labels of the clusters.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.cluster.KMeans\n    - matplotlib.pyplot\n\n    Example:\n    >>> perform_clustering('1,2,3,4,5,6,7,8,9,10')\n    \"\"\"\n    # Convert string to pandas DataFrame\n    data = pd.DataFrame({'Value': [int(i) for i in S.split(',')]})\n\n    # Perform k-means clustering\n    kmeans = KMeans(n_clusters=N_CLUSTERS).fit(data)\n\n    # Plot data with different colors for each cluster\n    colors = ['red', 'blue', 'green']\n    for i in range(N_CLUSTERS):\n        plt.scatter(data[kmeans.labels_ == i].index, data[kmeans.labels_ == i]['Value'], color=colors[i])\n    plt.show()\n\n    return kmeans.labels_.tolist()\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 5: \npandas, numpy, sklearn.preprocessing\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef pre_process_data(S):\n    \"\"\"\n    Convert a string of comma-separated numbers into a pandas DataFrame, scale the data using Min-Max scaling,\n    and return the scaled data as a list.\n\n    Parameters:\n    S (str): The string of comma-separated numbers.\n\n    Returns:\n    list: The scaled data.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.preprocessing.MinMaxScaler\n\n    Example:\n    >>> pre_process_data('1,2,3,4,5,6,7,8,9,10')\n    \"\"\"\n    # Convert string to pandas DataFrame\n    data = pd.DataFrame({'Value': [int(i) for i in S.split(',')]})\n\n    # Scale data using Min-Max scaling\n    scaler = MinMaxScaler()\n    scaled_data = scaler.fit_transform(data)\n\n    return scaled_data.flatten().tolist()\n```"}
{"seed": "{\"task_id\": 17560, \"prompt\": \"def f_17560(old, new):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"os.rename(old, new)\", \"test_start\": \"\\nimport os\\nfrom unittest.mock import Mock\\n\\ndef check(candidate):\", \"test\": [\"\\n    os.rename = Mock()\\n    try:\\n        candidate('a.py', 'b.py')\\n    except:\\n        assert False\\n\"], \"entry_point\": \"f_17560\", \"intent\": \"\\u00bfC\\u00f3mo renombrar un archivo con nombre `old` a `new`?\", \"library\": [\"os\"]}", "generation": "Scenario 3:\nos, shutil, glob, re\n```python\nimport os\nimport shutil\nimport glob\nimport re\n\n# Constants\nFILE_TYPES = ['.py', '.txt', '.csv', '.docx', '.xlsx']\n\ndef rename_and_move_files(old_name, new_name, source_dir, dest_dir, file_type):\n    \"\"\"\n    Rename all files with a specific name and file type in a source directory,\n    and move the renamed files to a destination directory.\n\n    Parameters:\n    old_name (str): The old name of the file(s) without file extension.\n    new_name (str): The new name of the file(s) without file extension.\n    source_dir (str): The source directory where the files are located.\n    dest_dir (str): The destination directory where the renamed files will be moved.\n    file_type (str): The file type of the files to be renamed.\n\n    Returns:\n    list: A list of the file paths of the renamed and moved files.\n\n    Requirements:\n    - os\n    - shutil\n    - glob\n    - re\n\n    Example:\n    >>> rename_and_move_files('old', 'new', './source', './destination', '.txt')\n    \"\"\"\n    if file_type not in FILE_TYPES:\n        raise ValueError(f\"Invalid file type {file_type}, valid types are {FILE_TYPES}\")\n\n    old_files = glob.glob(f\"{source_dir}/{old_name}{file_type}\")\n    renamed_files = []\n\n    for old_file in old_files:\n        new_file = re.sub(old_name, new_name, old_file)\n        new_file_path = os.path.join(dest_dir, os.path.basename(new_file))\n        shutil.move(old_file, new_file_path)\n        renamed_files.append(new_file_path)\n\n    return renamed_files\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, glob, hashlib, binascii\n```python\nimport os\nimport glob\nimport hashlib\nimport binascii\n\ndef rename_files_with_hash(directory, file_type, hash_algorithm):\n    \"\"\"\n    Rename all files of a specific type in a directory to their hash value.\n\n    Parameters:\n    directory (str): The directory where the files are located.\n    file_type (str): The type of files to be renamed.\n    hash_algorithm (str): The hash algorithm to be used.\n\n    Returns:\n    list: A list of the file paths of the renamed files.\n\n    Requirements:\n    - os\n    - glob\n    - hashlib\n    - binascii\n\n    Example:\n    >>> rename_files_with_hash('./directory', '.txt', 'sha256')\n    \"\"\"\n    files = glob.glob(f\"{directory}/*{file_type}\")\n    renamed_files = []\n\n    for file_path in files:\n        with open(file_path, 'rb') as file:\n            data = file.read()\n            hash_obj = hashlib.new(hash_algorithm)\n            hash_obj.update(data)\n            new_name = binascii.hexlify(hash_obj.digest()).decode()\n\n        new_file_path = os.path.join(directory, f\"{new_name}{file_type}\")\n        os.rename(file_path, new_file_path)\n        renamed_files.append(new_file_path)\n\n    return renamed_files\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, glob, pathlib, shutil\n```python\nimport os\nimport glob\nimport pathlib\nimport shutil\n\n# Constants\nSUBFOLDERS = ['images', 'documents', 'audio', 'video']\n\ndef sort_files_into_subfolders(directory):\n    \"\"\"\n    Sort all files in a directory into subfolders based on their file extensions.\n\n    Parameters:\n    directory (str): The directory where the files are located.\n\n    Returns:\n    dict: A dictionary with the subfolders as keys and the list of file paths in each subfolder as values.\n\n    Requirements:\n    - os\n    - glob\n    - pathlib\n    - shutil\n\n    Example:\n    >>> sort_files_into_subfolders('./directory')\n    \"\"\"\n    files = glob.glob(f\"{directory}/*\")\n    sorted_files = {subfolder: [] for subfolder in SUBFOLDERS}\n\n    for file_path in files:\n        file_extension = pathlib.Path(file_path).suffix\n        if file_extension in ['.jpg', '.png', '.gif']:\n            subfolder = 'images'\n        elif file_extension in ['.docx', '.xlsx', '.pdf']:\n            subfolder = 'documents'\n        elif file_extension in ['.mp3', '.wav']:\n            subfolder = 'audio'\n        elif file_extension in ['.mp4', '.avi']:\n            subfolder = 'video'\n        else:\n            continue\n\n        new_file_path = shutil.move(file_path, os.path.join(directory, subfolder, os.path.basename(file_path)))\n        sorted_files[subfolder].append(new_file_path)\n\n    return sorted_files\n```\n`GPT_ODEX_BREAK`\n\nScenario 6:\nos, csv, pathlib\n```python\nimport os\nimport csv\nimport pathlib\n\ndef rename_files_from_csv(directory, csv_file):\n    \"\"\"\n    Rename all files in a directory based on the mapping provided in a CSV file.\n\n    Parameters:\n    directory (str): The directory where the files are located.\n    csv_file (str): The CSV file with two columns: 'old_name' and 'new_name'.\n\n    Returns:\n    list: A list of the file paths of the renamed files.\n\n    Requirements:\n    - os\n    - csv\n    - pathlib\n\n    Example:\n    >>> rename_files_from_csv('./directory', 'mapping.csv')\n    \"\"\"\n    with open(csv_file, 'r') as file:\n        reader = csv.DictReader(file)\n        mapping = {row['old_name']: row['new_name'] for row in reader}\n\n    renamed_files = []\n\n    for old_name, new_name in mapping.items():\n        old_file_path = os.path.join(directory, old_name)\n        new_file_path = os.path.join(directory, new_name)\n        os.rename(old_file_path, new_file_path)\n        renamed_files.append(new_file_path)\n\n    return renamed_files\n```\n`GPT_ODEX_BREAK`\n\nScenario 7:\nos, shutil, glob\n```python\nimport os\nimport shutil\nimport glob\n\ndef backup_and_rename_files(directory, backup_directory, new_name):\n    \"\"\"\n    Backup all files in a directory to another directory and rename them.\n\n    Parameters:\n    directory (str): The directory where the files are located.\n    backup_directory (str): The directory where the files will be backed up.\n    new_name (str): The new name of the files without file extension.\n\n    Returns:\n    list: A list of the file paths of the backed up and renamed files.\n\n    Requirements:\n    - os\n    - shutil\n    - glob\n\n    Example:\n    >>> backup_and_rename_files('./directory', './backup', 'new')\n    \"\"\"\n    files = glob.glob(f\"{directory}/*\")\n    backed_up_files = []\n\n    for file_path in files:\n        file_extension = os.path.splitext(file_path)[1]\n        new_file_path = os.path.join(backup_directory, f\"{new_name}{file_extension}\")\n        shutil.copy(file_path, new_file_path)\n        backed_up_files.append(new_file_path)\n\n    return backed_up_files\n```"}
{"seed": "{\"task_id\": 41536, \"prompt\": \"def f_41536(ruta):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"\\n\\tpath = ruta\\n\\textension = '*.ext'\\n\\n\\tdef comparar_linea(file1, file2):\\n\\t\\tdata1 = [line.strip() for line in file1.readlines()]\\n\\t\\tdata2 = [line.strip() for line in file2.readlines()]\\n\\t\\tsame = set(data1).intersection(data2)\\n\\t\\treturn same\\n\\n\\tdef buscarCoincidencias(path, extension):\\n\\t\\twith open('some_output_file.txt', 'w') as file_out:\\n\\t\\t\\tos.chdir(path)\\n\\t\\t\\tfiles = glob.glob(extension)\\n\\t\\t\\tprint(files)\\n\\t\\t\\tfor i in range(len(files)):\\n\\t\\t\\t\\tfor j in range(i+1, len(files)):\\n\\t\\t\\t\\t\\twith open(files[i]) as file1:\\n\\t\\t\\t\\t\\t\\twith open(files[j], 'r') as file2:\\n\\t\\t\\t\\t\\t\\t\\tsame = comparar_linea(file1, file2)\\n\\t\\t\\t\\t\\t\\t\\tfile_out.write(files[i] + \\\"-\\\"+ files[j] + \\\": \\\"+str(same)+\\\"\\\\n\\\")\\n\\n\\tbuscarCoincidencias(path, extension)\\n\", \"test_start\": \"\\nimport os\\nimport glob\\nfrom unittest.mock import Mock\\n\\ndef check(candidate):\", \"test\": [\"\\n    file_name = 'some_output_file.txt'\\n    arr = ['a.ext', 'b.ext', 'c.ext']\\n    for file in arr:\\n        with open (file, 'w') as f:\\n            f.write('1')\\n\\n\\n    os.chdir = Mock()\\n    candidate('/')\\n    f = open('some_output_file.txt')\\n    lines = f.readlines()\\n    assert len(lines) == 3\\n    f.close()\\n\"], \"entry_point\": \"f_41536\", \"intent\": \"\\u00bfC\\u00f3mo buscar todos los archivos con extensi\\u00f3n `.ext` en la ruta `ruta` y buscar las coincidencias entre ellos?\", \"library\": [\"glob\", \"os\"]}", "generation": "Scenario 1:\nos, glob, difflib, string, hashlib\n```python\nimport os\nimport glob\nimport difflib\nimport string\nimport hashlib\n\n# Constants\nALLOWED_CHARACTERS = string.ascii_letters + string.digits + '_.'\n\ndef file_similarity_check(path, extension, minimum_similarity):\n    \"\"\"\n    Search all files with a certain extension in a specified directory and \n    calculate the similarity ratio between each pair of files. If the similarity\n    ratio is greater than or equal to a minimum similarity threshold, print the \n    filenames and the similarity ratio.\n\n    Parameters:\n    path (str): The directory path.\n    extension (str): The file extension.\n    minimum_similarity (float): The minimum similarity ratio to consider.\n\n    Returns:\n    None\n\n    Requirements:\n    - os\n    - glob\n    - difflib\n    - string\n    - hashlib\n\n    Example:\n    >>> file_similarity_check('/my_directory', '.txt', 0.75)\n    \"\"\"\n    os.chdir(path)\n    files = glob.glob(f\"*{extension}\")\n\n    for i in range(len(files)):\n        with open(files[i], 'r') as file1:\n            data1 = file1.read().translate(str.maketrans('', '', string.punctuation))\n            data1_hash = hashlib.md5(data1.encode()).hexdigest()\n            for j in range(i+1, len(files)):\n                with open(files[j], 'r') as file2:\n                    data2 = file2.read().translate(str.maketrans('', '', string.punctuation))\n                    data2_hash = hashlib.md5(data2.encode()).hexdigest()\n                    if data1_hash != data2_hash:\n                        similarity = difflib.SequenceMatcher(None, data1, data2).ratio()\n                        if similarity >= minimum_similarity:\n                            print(f\"{files[i]} and {files[j]} have a similarity ratio of {similarity}.\")\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, glob, pandas, re, collections\n```python\nimport os\nimport glob\nimport pandas as pd\nimport re\nimport collections\n\ndef word_count_in_files(path, extension):\n    \"\"\"\n    Search all files with a certain extension in a specified directory. For each file, \n    count the number of occurrences of each word and store the result in a pandas DataFrame.\n\n    Parameters:\n    path (str): The directory path.\n    extension (str): The file extension.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the word counts.\n\n    Requirements:\n    - os\n    - glob\n    - pandas\n    - re\n    - collections\n\n    Example:\n    >>> word_count = word_count_in_files('/my_directory', '.txt')\n    >>> print(word_count)\n    \"\"\"\n    os.chdir(path)\n    files = glob.glob(f\"*{extension}\")\n\n    word_counts = []\n\n    for file in files:\n        with open(file, 'r') as f:\n            text = f.read().lower()\n            words = re.findall(r'\\b\\w+\\b', text)\n            count = collections.Counter(words)\n            word_counts.append(pd.DataFrame.from_dict(count, orient='index', columns=[file]))\n\n    word_counts_df = pd.concat(word_counts, axis=1).fillna(0)\n    \n    return word_counts_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, glob, csv, operator, itertools\n```python\nimport os\nimport glob\nimport csv\nimport operator\nimport itertools\n\n# Constants\nFIELDNAMES = ['Name', 'Age', 'City']\n\ndef merge_sort_csv_files(path, extension, output_filename):\n    \"\"\"\n    Search all CSV files with a certain extension in a specified directory. Merge \n    all files into a single CSV file and sort the file by a specific field.\n\n    Parameters:\n    path (str): The directory path.\n    extension (str): The file extension.\n    output_filename (str): The filename of the output file.\n\n    Returns:\n    None\n\n    Requirements:\n    - os\n    - glob\n    - csv\n    - operator\n    - itertools\n\n    Example:\n    >>> merge_sort_csv_files('/my_directory', '.csv', 'merged.csv')\n    \"\"\"\n    os.chdir(path)\n    files = glob.glob(f\"*{extension}\")\n\n    data = []\n\n    for file in files:\n        with open(file, 'r') as f:\n            reader = csv.DictReader(f)\n            data.extend(list(reader))\n\n    data.sort(key=operator.itemgetter('Name'))\n    \n    with open(output_filename, 'w', newline='') as f:\n        writer = csv.DictWriter(f, fieldnames=FIELDNAMES)\n        writer.writeheader()\n        writer.writerows(data)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, glob, shutil, fnmatch, pathlib\n```python\nimport os\nimport glob\nimport shutil\nimport fnmatch\nimport pathlib\n\ndef copy_files_with_extension_to_new_directory(path, extension, new_directory):\n    \"\"\"\n    Search all files with a certain extension in a specified directory and copy \n    them to a new directory.\n\n    Parameters:\n    path (str): The directory path.\n    extension (str): The file extension.\n    new_directory (str): The path of the new directory.\n\n    Returns:\n    None\n\n    Requirements:\n    - os\n    - glob\n    - shutil\n    - fnmatch\n    - pathlib\n\n    Example:\n    >>> copy_files_with_extension_to_new_directory('/my_directory', '.txt', '/new_directory')\n    \"\"\"\n    os.chdir(path)\n    files = glob.glob(f\"*{extension}\")\n\n    pathlib.Path(new_directory).mkdir(parents=True, exist_ok=True)\n\n    for file in files:\n        shutil.copy(file, new_directory)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, glob, zipfile, pathlib, fnmatch\n```python\nimport os\nimport glob\nimport zipfile\nimport pathlib\nimport fnmatch\n\ndef zip_files_with_extension(path, extension, zip_filename):\n    \"\"\"\n    Search all files with a certain extension in a specified directory and zip \n    them into a single .zip file.\n\n    Parameters:\n    path (str): The directory path.\n    extension (str): The file extension.\n    zip_filename (str): The filename of the .zip file.\n\n    Returns:\n    None\n\n    Requirements:\n    - os\n    - glob\n    - zipfile\n    - pathlib\n    - fnmatch\n\n    Example:\n    >>> zip_files_with_extension('/my_directory', '.txt', 'my_zipfile.zip')\n    \"\"\"\n    os.chdir(path)\n    files = glob.glob(f\"*{extension}\")\n\n    with zipfile.ZipFile(zip_filename, 'w') as zip_f:\n        for file in files:\n            zip_f.write(file)\n```\n"}
{"seed": "{\"task_id\": 40594, \"prompt\": \"def f_40594(M, x):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"M.transpose() + x.transpose()\", \"test_start\": \"\\nimport numpy as np\\n\\ndef check(candidate):\", \"test\": [\"\\n    M = np.array([[1,2],[3,4],[5,6]])\\n    x = np.array([10, 100, 1000])\\n    assert np.all(candidate(M, x) == np.array([[ 11, 103, 1005],[12, 104, 1006]]))\\n\"], \"entry_point\": \"f_40594\", \"intent\": \"\\u00bfC\\u00f3mo sumar los valores del vector `x` a todas las columnas de la matriz `M`?\", \"library\": [\"numpy\"]}", "generation": "Scenario 1: \nnumpy, pandas, matplotlib, scipy.stats\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef analyze_correlation_and_distribution(matrix, vector):\n    \"\"\"\n    Add the values of the vector to all columns of the matrix, calculate the correlation \n    of the resulting matrix, and plot the distribution of each column.\n\n    Parameters:\n    matrix (numpy array): The input matrix.\n    vector (numpy array): The input vector.\n\n    Returns:\n    DataFrame: The correlation matrix.\n    None: Plots the distribution of each column.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - scipy.stats\n    \n    Example:\n    >>> M = np.array([[1,2,3,4,5],[6,7,8,9,10],[11,12,13,14,15]])\n    >>> x = np.array([10, 20, 30])\n    >>> analyze_correlation_and_distribution(M, x)\n    \"\"\"\n    matrix = matrix.transpose() + vector.transpose()\n    df = pd.DataFrame(matrix, columns=COLUMNS)\n    correlation_matrix = df.corr()\n\n    for column in COLUMNS:\n        plt.figure()\n        plt.title(f'Distribution of {column}')\n        plt.hist(df[column], density=True, bins=30)\n        xmin, xmax = plt.xlim()\n        x = np.linspace(xmin, xmax, 100)\n        p = stats.norm.pdf(x, np.mean(df[column]), np.std(df[column]))\n        plt.plot(x, p, 'k', linewidth=2)\n\n    return correlation_matrix\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, pandas, seaborn, sklearn.decomposition\n```python\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef apply_pca_and_visualize(matrix, vector):\n    \"\"\"\n    Add the values of the vector to all columns of the matrix, apply PCA to the resulting data, \n    and visualize the first two principal components.\n\n    Parameters:\n    matrix (numpy array): The input matrix.\n    vector (numpy array): The input vector.\n\n    Returns:\n    None: Plots the first two principal components.\n\n    Requirements:\n    - numpy\n    - pandas\n    - seaborn\n    - sklearn.decomposition.PCA\n\n    Example:\n    >>> M = np.array([[1,2,3,4,5],[6,7,8,9,10],[11,12,13,14,15]])\n    >>> x = np.array([10, 20, 30])\n    >>> apply_pca_and_visualize(M, x)\n    \"\"\"\n    matrix = matrix.transpose() + vector.transpose()\n    df = pd.DataFrame(matrix, columns=COLUMNS)\n    pca = PCA(n_components=2)\n    principalComponents = pca.fit_transform(df)\n    principalDf = pd.DataFrame(data = principalComponents, columns = ['PC 1', 'PC 2'])\n    sns.scatterplot(x = 'PC 1', y = 'PC 2', data=principalDf)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, pandas, matplotlib, sklearn.cluster\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef perform_clustering(matrix, vector, n_clusters):\n    \"\"\"\n    Add the values of the vector to all columns of the matrix, perform K-Means clustering on \n    the resulting data, and plot the clusters.\n\n    Parameters:\n    matrix (numpy array): The input matrix.\n    vector (numpy array): The input vector.\n    n_clusters (int): The number of clusters to form.\n\n    Returns:\n    None: Plots the clusters.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - sklearn.cluster.KMeans\n\n    Example:\n    >>> M = np.array([[1,2,3,4,5],[6,7,8,9,10],[11,12,13,14,15]])\n    >>> x = np.array([10, 20, 30])\n    >>> perform_clustering(M, x, 3)\n    \"\"\"\n    matrix = matrix.transpose() + vector.transpose()\n    df = pd.DataFrame(matrix, columns=COLUMNS)\n\n    kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(df)\n    df['Cluster'] = kmeans.labels_\n\n    plt.scatter(df['A'], df['B'], c=df['Cluster'])\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, pandas, matplotlib, scipy.spatial.distance\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.spatial.distance import cdist\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef calculate_distance_matrix(matrix, vector):\n    \"\"\"\n    Add the values of the vector to all columns of the matrix, calculate the \n    Euclidean distance matrix of the resulting data, and visualize it as a heatmap.\n\n    Parameters:\n    matrix (numpy array): The input matrix.\n    vector (numpy array): The input vector.\n\n    Returns:\n    None: Plots the distance matrix as a heatmap.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - scipy.spatial.distance.cdist\n\n    Example:\n    >>> M = np.array([[1,2,3,4,5],[6,7,8,9,10],[11,12,13,14,15]])\n    >>> x = np.array([10, 20, 30])\n    >>> calculate_distance_matrix(M, x)\n    \"\"\"\n    matrix = matrix.transpose() + vector.transpose()\n    df = pd.DataFrame(matrix, columns=COLUMNS)\n    distance_matrix = cdist(df.values, df.values, 'euclidean')\n    plt.imshow(distance_matrix, cmap='hot', interpolation='nearest')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, pandas, seaborn, sklearn.preprocessing\n```python\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef standardize_and_plot(matrix, vector):\n    \"\"\"\n    Add the values of the vector to all columns of the matrix, standardize the resulting data \n    (mean=0, std=1), and plot a pairplot of the standardized data.\n\n    Parameters:\n    matrix (numpy array): The input matrix.\n    vector (numpy array): The input vector.\n\n    Returns:\n    None: Plots a pairplot of the standardized data.\n\n    Requirements:\n    - numpy\n    - pandas\n    - seaborn\n    - sklearn.preprocessing.StandardScaler\n\n    Example:\n    >>> M = np.array([[1,2,3,4,5],[6,7,8,9,10],[11,12,13,14,15]])\n    >>> x = np.array([10, 20, 30])\n    >>> standardize_and_plot(M, x)\n    \"\"\"\n    matrix = matrix.transpose() + vector.transpose()\n    df = pd.DataFrame(matrix, columns=COLUMNS)\n    scaler = StandardScaler()\n    df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n    sns.pairplot(df_scaled)\n```"}
{"seed": "{\"task_id\": 61810, \"prompt\": \"def f_61810(A, B):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"[[sum(x * B[i][col] for i,x in enumerate(fila)) for col in range(len(B[0]))] for fila in A]\", \"test_start\": \"\\nimport numpy as np\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert (candidate([[1,2],[3,4]], [[5,6],[7,8]]) == [[19, 22], [43, 50]]) or            (candidate(np.array([[1,2],[3,4]]), np.array([[5,6],[7,8]]))) == np.array([[19, 22], [43, 50]])\\n\"], \"entry_point\": \"f_61810\", \"intent\": \"\\u00bfC\\u00f3mo multiplicar la matriz `A` y la matriz `B`?\", \"library\": [\"numpy\"]}", "generation": "Scenario 1:\nnumpy, random, matplotlib, pandas\n```python\nimport numpy as np\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants\nMATRIX_SIZE = 10\n\ndef create_and_multiply_random_matrices():\n    \"\"\"\n    Create two random matrices of size MATRIX_SIZE x MATRIX_SIZE, multiply them \n    and then plot the resulting matrix as a heatmap.\n\n    Returns:\n    numpy.ndarray: The result of the matrix multiplication.\n\n    Requirements:\n    - numpy\n    - random\n    - matplotlib\n    - pandas\n\n    Example:\n    >>> result = create_and_multiply_random_matrices()\n    >>> plt.imshow(result, cmap='hot', interpolation='nearest')\n    >>> plt.show()\n    \"\"\"\n    A = np.random.randint(0, 100, size=(MATRIX_SIZE, MATRIX_SIZE))\n    B = np.random.randint(0, 100, size=(MATRIX_SIZE, MATRIX_SIZE))\n\n    result = np.dot(A, B)\n\n    return result\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, scipy, matplotlib\n```python\nfrom scipy import linalg\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nMATRIX_SIZE = 5\n\ndef compute_and_plot_eigenvalues():\n    \"\"\"\n    Create a random symmetric matrix of size MATRIX_SIZE x MATRIX_SIZE, compute \n    its eigenvalues and plot them.\n\n    Returns:\n    numpy.ndarray: The eigenvalues of the matrix.\n\n    Requirements:\n    - numpy\n    - scipy\n    - matplotlib\n\n    Example:\n    >>> eigenvalues = compute_and_plot_eigenvalues()\n    >>> plt.plot(eigenvalues, 'bo')\n    >>> plt.show()\n    \"\"\"\n    A = np.random.rand(MATRIX_SIZE, MATRIX_SIZE)\n    A = A + A.T  # Make A symmetric\n\n    eigenvalues = linalg.eigvalsh(A)\n\n    return eigenvalues\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, scipy, matplotlib\n```python\nfrom scipy import linalg\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nMATRIX_SIZE = 3\n\ndef solve_linear_system_and_plot_solution():\n    \"\"\"\n    Create a random system of linear equations and solve it. The system is \n    represented as AX = B, where A is a MATRIX_SIZE x MATRIX_SIZE matrix and B \n    is a MATRIX_SIZE x 1 matrix. Then plot the solution.\n\n    Returns:\n    numpy.ndarray: The solution of the linear system.\n\n    Requirements:\n    - numpy\n    - scipy\n    - matplotlib\n\n    Example:\n    >>> solution = solve_linear_system_and_plot_solution()\n    >>> plt.plot(solution, 'bo')\n    >>> plt.show()\n    \"\"\"\n    A = np.random.rand(MATRIX_SIZE, MATRIX_SIZE)\n    B = np.random.rand(MATRIX_SIZE, 1)\n\n    X = linalg.solve(A, B)\n\n    return X\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, scipy, pandas, seaborn\n```python\nfrom scipy import stats\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n# Constants\nDATA_SIZE = 1000\n\ndef generate_and_plot_normal_distribution():\n    \"\"\"\n    Generate a normal distribution with random mean and standard deviation, \n    create a pandas DataFrame from the data and plot a histogram.\n\n    Returns:\n    pandas.DataFrame: A DataFrame with the generated data.\n\n    Requirements:\n    - numpy\n    - scipy\n    - pandas\n    - seaborn\n\n    Example:\n    >>> df = generate_and_plot_normal_distribution()\n    >>> sns.histplot(data=df, x=\"Value\", kde=True)\n    \"\"\"\n    mean = np.random.randint(-10, 10)\n    std_dev = np.random.randint(1, 5)\n\n    data = stats.norm.rvs(loc=mean, scale=std_dev, size=DATA_SIZE)\n\n    df = pd.DataFrame(data, columns=['Value'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, pandas, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nDATA_SIZE = 1000\n\ndef generate_and_plot_cumulative_distribution():\n    \"\"\"\n    Generate a random dataset of size DATA_SIZE, create a pandas DataFrame from \n    the data, calculate the cumulative distribution and plot it.\n\n    Returns:\n    pandas.DataFrame: A DataFrame with the generated data and the cumulative distribution.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib\n\n    Example:\n    >>> df = generate_and_plot_cumulative_distribution()\n    >>> df.plot(x='Value', y='Cumulative Distribution')\n    \"\"\"\n    data = np.random.randn(DATA_SIZE)\n\n    df = pd.DataFrame(data, columns=['Value'])\n    df = df.sort_values('Value')\n    df['Cumulative Distribution'] = np.linspace(0., 1., len(df))\n\n    return df\n```\n"}
{"seed": "{\"task_id\": 63234, \"prompt\": \"def f_63234(x, list):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"list.index(x)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(3, [1,2,3,4]) == 2\\n\", \"\\n    assert candidate(0, [0,0,0,0]) == 0\\n\"], \"entry_point\": \"f_63234\", \"intent\": \"\\u00bfC\\u00f3mo obtener la posici\\u00f3n de un dato `x` en una lista `list`?\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, itertools, statistics\n```python\nimport pandas as pd\nimport numpy as np\nfrom itertools import combinations\nfrom statistics import mean\n\n# Constants\nDATA = [1,2,3,4]\n\ndef average_combinations(x, data):\n    \"\"\"\n    Generate all combinations of a list of numbers taken 'x' at a time and return \n    the average of each combination in a pandas Series.\n\n    Parameters:\n    x (int): The number of elements in each combination.\n    data (list): The list of numbers.\n\n    Returns:\n    Series: A pandas Series with the averages of the combinations.\n\n    Requirements:\n    - pandas\n    - numpy\n    - itertools\n    - statistics\n\n    Example:\n    >>> average_combinations(2, [1,2,3,4])\n    \"\"\"\n    comb = list(combinations(data, x))\n    averages = [mean(c) for c in comb]\n\n    return pd.Series(averages)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, collections, random\n```python\nimport pandas as pd\nimport numpy as np\nfrom collections import Counter\nimport random\n\n# Constants\nDATA = [1,2,3,4]\n\ndef frequency_distribution(x, data, num_samples):\n    \"\"\"\n    Randomly generate 'num_samples' samples of size 'x' from a list of data. \n    Calculate the frequency distribution of the samples and return as a pandas DataFrame.\n\n    Parameters:\n    x (int): The size of each sample.\n    data (list): The list of data.\n    num_samples (int): The number of samples.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the frequency distribution.\n\n    Requirements:\n    - pandas\n    - numpy\n    - collections\n    - random\n\n    Example:\n    >>> frequency_distribution(2, [1,2,3,4], 1000)\n    \"\"\"\n    samples = [random.sample(data, x) for _ in range(num_samples)]\n\n    flat_list = [item for sublist in samples for item in sublist]\n\n    freq_dict = dict(Counter(flat_list))\n\n    return pd.DataFrame(list(freq_dict.items()), columns=['Value', 'Frequency'])\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, math, statistics\n```python\nimport numpy as np\nimport math\nfrom statistics import stdev\n\n# Constants\nDATA = [1,2,3,4]\n\ndef calculate_z_score(x, data):\n    \"\"\"\n    Calculate the Z-score of a number 'x' from a given list of data.\n\n    Parameters:\n    x (int): The number for which the Z-score is calculated.\n    data (list): The list of data.\n\n    Returns:\n    float: The Z-score of 'x'.\n\n    Requirements:\n    - numpy\n    - math\n    - statistics\n\n    Example:\n    >>> calculate_z_score(2, [1,2,3,4])\n    \"\"\"\n    mean = np.mean(data)\n    std_dev = stdev(data)\n\n    z_score = (x - mean) / std_dev\n\n    return z_score\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, scipy.stats, matplotlib\n```python\nimport numpy as np\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\n\n# Constants\nDATA = [1,2,3,4]\n\ndef plot_normal_distribution(x, data):\n    \"\"\"\n    Fit a normal distribution to a given list of data and plot the distribution and where 'x' falls on it.\n\n    Parameters:\n    x (int): The number to be plotted on the distribution.\n    data (list): The list of data.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib\n\n    Example:\n    >>> plot_normal_distribution(2, [1,2,3,4])\n    \"\"\"\n    mu, std = stats.norm.fit(data)\n\n    plt.hist(data, bins=25, density=True, alpha=0.6, color='g')\n\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, mu, std)\n    plt.plot(x, p, 'k', linewidth=2)\n    plt.title(\"Fit results: mu = %.2f,  std = %.2f\" % (mu, std))\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, scipy.optimize, matplotlib\n```python\nimport numpy as np\nimport scipy.optimize as optimize\nimport matplotlib.pyplot as plt\n\n# Constants\nDATA = [1,2,3,4]\n\ndef find_nearest_value(x, data):\n    \"\"\"\n    Find the nearest value to 'x' in a given list of data and plot both 'x' and the nearest value on a line plot.\n\n    Parameters:\n    x (int): The target number.\n    data (list): The list of data.\n\n    Returns:\n    int: The nearest value to 'x' in the data.\n\n    Requirements:\n    - numpy\n    - scipy.optimize\n    - matplotlib\n\n    Example:\n    >>> find_nearest_value(2, [1,2,3,4])\n    \"\"\"\n    data = np.array(data)\n    idx = (np.abs(data - x)).argmin()\n    nearest = data[idx]\n\n    plt.plot(data)\n    plt.plot(idx, nearest, 'ro')\n    plt.plot(x, 'bo')\n    plt.show()\n\n    return nearest\n```"}
{"seed": "{\"task_id\": 57263, \"prompt\": \"def f_57263(INT, STR):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"repr(INT) + STR\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(2, 'a') == '2a'\\n\", \"\\n    assert candidate(-1, ' ') == '-1 '\\n\"], \"entry_point\": \"f_57263\", \"intent\": \"\\u00bfC\\u00f3mo concatenar un `INT` y un `STR`?\", \"library\": []}", "generation": "Scenario 1: \npandas, random, matplotlib, numpy\n```python\nimport pandas as pd\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nPRODUCTS = ['Apple', 'Banana', 'Orange', 'Pear', 'Grape']\nSTORES = ['Store1', 'Store2', 'Store3', 'Store4', 'Store5']\n\ndef generate_sales_report(INT, STR):\n    \"\"\"\n    Generate a sales report for an integer number of days for a specific Store.\n    \n    Parameters:\n    INT (int): The number of days for which the report should be generated.\n    STR (str): The name of the store.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with sales information.\n    \n    Requirements:\n    - pandas\n    - random\n    - matplotlib.pyplot\n    - numpy\n    \n    Example:\n    >>> report = generate_sales_report(7, 'Store1')\n    >>> print(report)\n    >>> report['Product'].value_counts().plot(kind='bar')\n    \"\"\"\n    report_data = []\n\n    for i in range(INT):\n        product = PRODUCTS[randint(0, len(PRODUCTS)-1)]\n        sales = np.random.poisson(100)  # simulate sales with a Poisson distribution\n        report_data.append([STR, i, product, sales])\n\n    report_df = pd.DataFrame(report_data, columns=['Store', 'Day', 'Product', 'Sales'])\n\n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ndatetime, random, matplotlib, os\n```python\nimport os\nfrom datetime import datetime, timedelta\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nLOG_LEVELS = ['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL']\n\ndef generate_log_files(INT, STR):\n    \"\"\"\n    Generate INT number of log files with STR as the filename prefix. Each log file \n    contains random log messages with timestamps.\n\n    Parameters:\n    INT (int): The number of log files to generate.\n    STR (str): The filename prefix for the log files.\n\n    Returns:\n    None\n\n    Requirements:\n    - datetime\n    - random\n    - matplotlib.pyplot\n    - os\n\n    Example:\n    >>> generate_log_files(5, 'server_log')\n    \"\"\"\n    for i in range(INT):\n        filename = f\"{STR}_{i+1}.log\"\n        with open(filename, 'w') as f:\n            for _ in range(100):  # each file contains 100 log messages\n                log_level = LOG_LEVELS[randint(0, len(LOG_LEVELS)-1)]\n                timestamp = datetime.now() + timedelta(seconds=randint(-1000, 1000))  # random timestamps\n                message = f\"{timestamp} - {log_level} - This is a log message.\"\n                f.write(message + \"\\n\")\n        print(f\"Log file {filename} has been generated.\")\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, matplotlib, random\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\n\ndef plot_random_walk(INT, STR):\n    \"\"\"\n    Plot a random walk of INT steps with STR as the title of the plot.\n\n    Parameters:\n    INT (int): The number of steps in the random walk.\n    STR (str): The title of the plot.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_random_walk(1000, 'Random Walk Example')\n    \"\"\"\n    steps = np.random.choice([-1, 1], INT)\n    path = np.cumsum(steps)\n    plt.plot(path)\n    plt.title(STR)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, matplotlib, random\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nCOLOURS = ['red', 'green', 'blue', 'yellow', 'purple']\n\ndef generate_random_scatter_plot(INT, STR):\n    \"\"\"\n    Generate a scatter plot with INT number of points and STR as the title.\n    \n    Parameters:\n    INT (int): The number of points in the scatter plot.\n    STR (str): The title of the scatter plot.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> generate_random_scatter_plot(500, 'Random Scatter Plot')\n    \"\"\"\n    x = np.random.rand(INT)\n    y = np.random.rand(INT)\n    colours = [COLOURS[randint(0, len(COLOURS)-1)] for _ in range(INT)]\n    sizes = np.random.rand(INT) * 100\n\n    plt.scatter(x, y, c=colours, s=sizes, alpha=0.5)\n    plt.title(STR)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, csv, random, datetime\n```python\nimport os\nimport csv\nfrom random import randint\nfrom datetime import datetime, timedelta\n\n# Constants\nUSER_IDS = list(range(1000, 2000))\n\ndef generate_user_log(INT, STR):\n    \"\"\"\n    Generate a CSV file with INT number of user logs. STR is used as the file name.\n\n    Parameters:\n    INT (int): The number of user logs.\n    STR (str): The name of the CSV file.\n\n    Returns:\n    None\n\n    Requirements:\n    - os\n    - csv\n    - random\n    - datetime\n\n    Example:\n    >>> generate_user_log(500, 'user_log.csv')\n    \"\"\"\n    with open(STR, 'w', newline='') as csvfile:\n        fieldnames = ['user_id', 'login_time', 'logout_time']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n\n        writer.writeheader()\n        for _ in range(INT):\n            user_id = USER_IDS[randint(0, len(USER_IDS)-1)]\n            login_time = datetime.now() + timedelta(seconds=randint(-1000, 1000))  # random login times\n            logout_time = login_time + timedelta(seconds=randint(1, 1000))  # random logout times\n            writer.writerow({'user_id': user_id, 'login_time': login_time, 'logout_time': logout_time})\n\n    print(f\"User log has been written to {STR}.\")\n```"}
{"seed": "{\"task_id\": 87813, \"prompt\": \"def f_87813(L):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"list(set(L))\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([1,1,1,1]) == [1]\\n\", \"\\n    assert candidate([1,2,3,4]) == [1,2,3,4]\\n\", \"\\n    assert candidate([]) == []\\n\"], \"entry_point\": \"f_87813\", \"intent\": \"\\u00bfC\\u00f3mo eliminar duplicados en una lista `L`?\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, matplotlib.pyplot, random\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nCATEGORIES = ['A', 'B', 'C', 'D', 'E']\n\ndef generate_sales_report(n=100):\n    \"\"\"\n    Generate a sales report for n number of products with categories from the provided list. \n    The function should also plot a bar graph using matplotlib showing the total sales for each category.\n    \n    Parameters:\n    n (int): The number of products. Default is 100.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with sales report.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - random\n    \n    Example:\n    >>> report = generate_sales_report(500)\n    >>> print(report)\n    >>> report.groupby('Category')['Sales'].sum().plot(kind='bar')\n    \"\"\"\n    data = []\n\n    for _ in range(n):\n        category = CATEGORIES[randint(0, len(CATEGORIES)-1)]\n        sales = np.random.randint(1, 100)\n        data.append([category, sales])\n\n    df = pd.DataFrame(data, columns=['Category', 'Sales'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncollections, operator, itertools, random\n```python\nfrom collections import Counter\nimport operator\nimport itertools\nfrom random import randint\n\n# Constants\nELEMENTS = ['apple', 'banana', 'cherry', 'date', 'elderberry']\n\ndef most_common_combinations(n=100, k=2):\n    \"\"\"\n    Generate a list of n random elements from the provided list, calculate combinations of size k, \n    and find the most common combinations.\n\n    Parameters:\n    n (int): The size of the list. Default is 100.\n    k (int): The size of the combinations. Default is 2.\n\n    Returns:\n    list: A list of tuples with the most common combinations and their counts.\n\n    Requirements:\n    - collections\n    - operator\n    - itertools\n    - random\n\n    Example:\n    >>> most_common_combinations(500, 3)\n    \"\"\"\n    lst = [ELEMENTS[randint(0, len(ELEMENTS)-1)] for _ in range(n)]\n    combinations = list(itertools.combinations(lst, k))\n    counter = Counter(combinations)\n\n    most_common = sorted(counter.items(), key=operator.itemgetter(1), reverse=True)\n\n    return most_common\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, sklearn.preprocessing, numpy\n```python\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\n\n# Constants\nFEATURES = ['Feature 1', 'Feature 2', 'Feature 3', 'Feature 4', 'Feature 5']\n\ndef scale_features(df):\n    \"\"\"\n    Scale the features in a pandas DataFrame using sklearn's StandardScaler.\n\n    Parameters:\n    df (DataFrame): The DataFrame with features to scale.\n\n    Returns:\n    DataFrame: A new DataFrame with the scaled features.\n\n    Requirements:\n    - pandas\n    - sklearn.preprocessing\n    - numpy\n\n    Example:\n    >>> df = pd.DataFrame(np.random.randint(0,100,size=(100, 5)), columns=FEATURES)\n    >>> scaled_df = scale_features(df)\n    >>> print(scaled_df)\n    \"\"\"\n    scaler = StandardScaler()\n    scaled_features = scaler.fit_transform(df[FEATURES])\n\n    scaled_df = pd.DataFrame(scaled_features, columns=FEATURES)\n\n    return scaled_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ncollections, random, string\n```python\nfrom collections import defaultdict\nfrom random import choice\nimport string\n\n# Constants\nLETTERS = string.ascii_lowercase\n\ndef generate_letter_frequency(n=100):\n    \"\"\"\n    Generate a list of n random letters from the alphabet, and calculate the frequency of each letter.\n\n    Parameters:\n    n (int): The number of letters. Default is 100.\n\n    Returns:\n    dict: A dictionary with the frequency of each letter.\n\n    Requirements:\n    - collections\n    - random\n    - string\n\n    Example:\n    >>> generate_letter_frequency(500)\n    \"\"\"\n    lst = [choice(LETTERS) for _ in range(n)]\n    frequency = defaultdict(int)\n\n    for letter in lst:\n        frequency[letter] += 1\n\n    return dict(frequency)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, pandas, sklearn.linear_model, sklearn.model_selection\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\n# Constants\nFEATURES = ['Feature 1', 'Feature 2', 'Feature 3', 'Feature 4', 'Feature 5']\nTARGET = 'Target'\n\ndef train_linear_regression(df):\n    \"\"\"\n    Train a linear regression model using the features and target in a pandas DataFrame.\n\n    Parameters:\n    df (DataFrame): The DataFrame with features and target.\n\n    Returns:\n    LinearRegression: The trained linear regression model.\n\n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.linear_model\n    - sklearn.model_selection\n\n    Example:\n    >>> df = pd.DataFrame(np.random.randint(0,100,size=(100, 6)), columns=FEATURES+[TARGET])\n    >>> model = train_linear_regression(df)\n    \"\"\"\n    X = df[FEATURES]\n    y = df[TARGET]\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    return model\n```"}
{"seed": "{\"task_id\": 15622, \"prompt\": \"def f_15622(x):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"type(x)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([1,2,3]) == list\\n\", \"\\n    assert candidate({'a': None}) == dict\\n\", \"\\n    assert candidate(123) == int\\n\", \"\\n    assert candidate(12.34) == float\\n\"], \"entry_point\": \"f_15622\", \"intent\": \"\\u00bfC\\u00f3mo obtener el tipo de una variable `x`?\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, matplotlib, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nCOLUMN_NAMES = ['Temperature', 'Humidity', 'Pressure', 'Wind Speed', 'Rainfall']\n\ndef generate_and_visualize_weather_data(n_rows):\n    \"\"\"\n    Create a DataFrame of simulated weather data and visualize the data using a pairplot.\n    \n    Parameters:\n    n_rows (int): The number of rows of data to generate.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the generated data.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> df = generate_and_visualize_weather_data(500)\n    >>> print(df)\n    >>> sns.pairplot(df)\n    \"\"\"\n    data = np.random.rand(n_rows, len(COLUMN_NAMES)) * 100\n    df = pd.DataFrame(data, columns=COLUMN_NAMES)\n    \n    sns.pairplot(df)\n    \n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\njson, os, matplotlib, numpy \n```python\nimport json\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nFILE_PATH = 'data.json'\nBINS = 20\n\ndef read_json_and_plot_histogram():\n    \"\"\"\n    Read a JSON file containing a list of numbers and plot a histogram of the data.\n    \n    Requirements:\n    - json\n    - os\n    - matplotlib.pyplot\n    - numpy\n\n    Example:\n    # Assume that data.json contains: [1, 3, 4, 2, 2, 1, 7, 8, 9, 2, 1, 4, 5]\n    >>> read_json_and_plot_histogram()\n    \"\"\"\n    if not os.path.exists(FILE_PATH):\n        return None\n\n    with open(FILE_PATH, 'r') as f:\n        data = json.load(f)\n\n    plt.hist(data, bins=BINS)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \npandas, numpy, random, sklearn\n```python\nimport pandas as pd\nimport numpy as np\nimport random\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\n# Constants\nLABELS = ['A', 'B', 'C', 'D', 'E']\nFEATURES = ['Feature1', 'Feature2', 'Feature3', 'Feature4', 'Feature5']\n\ndef perform_logistic_regression(n_samples):\n    \"\"\"\n    Generate a DataFrame with random data, split it into train and test sets, \n    perform logistic regression and return the accuracy score.\n    \n    Parameters:\n    n_samples (int): The number of samples to generate.\n    \n    Returns:\n    float: The accuracy score of the logistic regression model.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - random\n    - sklearn.model_selection\n    - sklearn.linear_model\n    - sklearn.metrics\n\n    Example:\n    >>> perform_logistic_regression(1000)\n    \"\"\"\n    data = {\n        feature: np.random.rand(n_samples) for feature in FEATURES\n    }\n    data['Label'] = [random.choice(LABELS) for _ in range(n_samples)]\n    \n    df = pd.DataFrame(data)\n    \n    X_train, X_test, y_train, y_test = train_test_split(df[FEATURES], df['Label'], test_size=0.2)\n    \n    model = LogisticRegression()\n    model.fit(X_train, y_train)\n    \n    predictions = model.predict(X_test)\n    \n    return accuracy_score(y_test, predictions)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, scipy, matplotlib\n```python\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\n# Constants\nMEAN = 0\nSTD_DEV = 1\nNUM_SAMPLES = 1000\n\ndef calculate_and_visualize_normal_distribution():\n    \"\"\"\n    Create a normal distribution with a given mean and standard deviation, \n    calculate its PDF and CDF, and plot them.\n    \n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Example:\n    >>> calculate_and_visualize_normal_distribution()\n    \"\"\"\n    data = np.random.normal(MEAN, STD_DEV, NUM_SAMPLES)\n    \n    pdf = stats.norm.pdf(data)\n    cdf = stats.norm.cdf(data)\n    \n    plt.figure(figsize=(12, 6))\n    \n    plt.subplot(1, 2, 1)\n    plt.plot(data, pdf, 'o')\n    plt.title('PDF')\n    \n    plt.subplot(1, 2, 2)\n    plt.plot(data, cdf, 'o')\n    plt.title('CDF')\n    \n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \npandas, matplotlib, os, numpy\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport numpy as np\n\n# Constants\nCSV_FILE_PATH = 'data.csv'\nCOLUMN_NAME = 'Value'\n\ndef read_csv_and_plot_histogram():\n    \"\"\"\n    Read a CSV file containing a column of numbers and plot a histogram of the data.\n    \n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - os\n    - numpy\n\n    Example:\n    # Assume that data.csv contains a column named 'Value'\n    >>> read_csv_and_plot_histogram()\n    \"\"\"\n    if not os.path.exists(CSV_FILE_PATH):\n        return None\n\n    df = pd.read_csv(CSV_FILE_PATH)\n    data = df[COLUMN_NAME]\n    \n    plt.hist(data.dropna(), bins='auto')  \n    plt.show()\n```"}
{"seed": "{\"task_id\": 201210, \"prompt\": \"def f_201210(lista):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"sum(lista)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert (candidate([1,2,3]) - 6) < 1e-6\\n\", \"\\n    assert (candidate([0.0, 3.5, 2.7, 1.8, 2.0]) - 10.0) < 1e-6\\n\"], \"entry_point\": \"f_201210\", \"intent\": \"\\u00bfC\\u00f3mo sumar todos los elementos de una `lista`?\", \"library\": []}", "generation": "Scenario 1:\npandas, numpy, random, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nPRODUCTS = ['Product A', 'Product B', 'Product C', 'Product D', 'Product E']\nCATEGORIES = ['Electronics', 'Clothing', 'Home & Kitchen', 'Books', 'Toys']\n\ndef generate_sales_report():\n    \"\"\"\n    Generate a sales report for a list of products across various categories.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with sales data for the products.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> report = generate_sales_report()\n    >>> print(report)\n    >>> report['Sales'].sum()\n    \"\"\"\n    report_data = []\n\n    for product in PRODUCTS:\n        category = CATEGORIES[randint(0, len(CATEGORIES)-1)]\n        sales = np.round(np.random.uniform(100, 1000), 2)\n        report_data.append([product, category, sales])\n\n    report_df = pd.DataFrame(report_data, columns=['Product', 'Category', 'Sales'])\n\n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, scipy.stats, matplotlib\n```python\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\ndef plot_normal_distribution(mu, sigma):\n    \"\"\"\n    Plot a normal distribution with a given mean and standard deviation.\n    \n    Parameters:\n    mu (float): The mean of the distribution.\n    sigma (float): The standard deviation of the distribution.\n    \n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_normal_distribution(0, 1)\n    \"\"\"\n    x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n    y = norm.pdf(x, mu, sigma)\n\n    plt.plot(x, y)\n    plt.title('Normal Distribution with mu = {} and sigma = {}'.format(mu, sigma))\n    plt.xlabel('x')\n    plt.ylabel('Probability Density')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nMONTHS = ['January', 'February', 'March', 'April', 'May', 'June',\n          'July', 'August', 'September', 'October', 'November', 'December']\n\ndef generate_temperature_report():\n    \"\"\"\n    Generate a temperature report for a year and plot the temperature trend.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with temperature data for the months.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> report = generate_temperature_report()\n    >>> print(report)\n    >>> report['Temperature'].plot(kind='line')\n    \"\"\"\n    temperature_data = []\n\n    for month in MONTHS:\n        temperature = np.round(np.random.uniform(-30, 30), 2)\n        temperature_data.append([month, temperature])\n\n    temperature_df = pd.DataFrame(temperature_data, columns=['Month', 'Temperature'])\n\n    return temperature_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, sklearn.cluster, matplotlib\n```python\nimport numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\ndef perform_kmeans_clustering(n_clusters, n_points):\n    \"\"\"\n    Perform KMeans clustering on a random 2D dataset and plot the result.\n    \n    Parameters:\n    n_clusters (int): The number of clusters to form.\n    n_points (int): The number of points in the dataset.\n    \n    Returns:\n    ndarray: The cluster labels for each point in the dataset.\n    \n    Requirements:\n    - numpy\n    - sklearn.cluster\n    - matplotlib.pyplot\n\n    Example:\n    >>> labels = perform_kmeans_clustering(3, 100)\n    \"\"\"\n    X = np.random.rand(n_points, 2)\n\n    kmeans = KMeans(n_clusters=n_clusters)\n    kmeans.fit(X)\n\n    labels = kmeans.labels_\n\n    plt.scatter(X[:, 0], X[:, 1], c=labels)\n    plt.show()\n\n    return labels\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, sklearn.datasets, sklearn.linear_model, matplotlib\n```python\nimport numpy as np\nfrom sklearn.datasets import make_regression\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\n\ndef perform_linear_regression(n_samples, n_features):\n    \"\"\"\n    Perform linear regression on a randomly generated dataset and plot the result.\n    \n    Parameters:\n    n_samples (int): The number of samples.\n    n_features (int): The number of features.\n    \n    Returns:\n    ndarray: The coefficients of the linear regression model.\n    \n    Requirements:\n    - numpy\n    - sklearn.datasets\n    - sklearn.linear_model\n    - matplotlib.pyplot\n\n    Example:\n    >>> coeffs = perform_linear_regression(100, 1)\n    \"\"\"\n    X, y = make_regression(n_samples=n_samples, n_features=n_features, noise=0.1)\n\n    model = LinearRegression()\n    model.fit(X, y)\n\n    y_pred = model.predict(X)\n\n    plt.scatter(X, y, color='blue')\n    plt.plot(X, y_pred, color='red', linewidth=2)\n    plt.show()\n\n    return model.coef_\n```"}
{"seed": "{\"task_id\": 124063, \"prompt\": \"def f_124063(n):\\n\\t\", \"suffix\": \"\\n\\treturn L\", \"canonical_solution\": \"L = [random.randint(0, 1000) for i in range(n)]\", \"test_start\": \"\\nimport random\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert len(candidate(100)) == 100\\n\", \"\\n    assert len(candidate(0)) == 0\\n\"], \"entry_point\": \"f_124063\", \"intent\": \"\\u00bfC\\u00f3mo crear una lista `L` con `n` n\\u00fameros aleatorios?\", \"library\": [\"random\"]}", "generation": "Scenario 1: \nrandom, statistics, collections\n```python\nimport random\nimport statistics\nimport collections\n\n# Constants\nRANGE_LOW = 0\nRANGE_HIGH = 1000\n\ndef generate_stats(n):\n    \"\"\"\n    Generate a list of 'n' random numbers and return their mean, median, mode and frequency distribution.\n\n    Parameters:\n    n (int): The number of random numbers to generate.\n\n    Returns:\n    dict: A dictionary with mean, median, mode and frequency distribution of the numbers.\n\n    Requirements:\n    - random\n    - statistics\n    - collections\n\n    Example:\n    >>> stats = generate_stats(100)\n    >>> print(stats)\n    \"\"\"\n    L = [random.randint(RANGE_LOW, RANGE_HIGH) for i in range(n)]\n    mean = statistics.mean(L)\n    median = statistics.median(L)\n    mode = statistics.mode(L)\n    freq_dist = collections.Counter(L)\n\n    return {'mean': mean, 'median': median, 'mode': mode, 'frequency_distribution': freq_dist}\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, matplotlib.pyplot, random\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\n\n# Constants\nRANGE_LOW = 0\nRANGE_HIGH = 1000\nNUM_BINS = 50\n\ndef plot_histogram(n):\n    \"\"\"\n    Generate a list of 'n' random numbers and plot their histogram.\n\n    Parameters:\n    n (int): The number of random numbers to generate.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> plot_histogram(1000)\n    \"\"\"\n    L = np.array([random.randint(RANGE_LOW, RANGE_HIGH) for i in range(n)])\n    plt.hist(L, bins=NUM_BINS)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nrandom, numpy, scipy.stats\n```python\nimport random\nimport numpy as np\nimport scipy.stats as stats\n\n# Constants\nRANGE_LOW = 0\nRANGE_HIGH = 1000\n\ndef calculate_skewness_kurtosis(n):\n    \"\"\"\n    Generate a list of 'n' random numbers and calculate their skewness and kurtosis.\n\n    Parameters:\n    n (int): The number of random numbers to generate.\n\n    Returns:\n    tuple: A tuple with skewness and kurtosis of the numbers.\n\n    Requirements:\n    - random\n    - numpy\n    - scipy.stats\n\n    Example:\n    >>> skewness, kurtosis = calculate_skewness_kurtosis(1000)\n    >>> print(f'Skewness: {skewness}, Kurtosis: {kurtosis}')\n    \"\"\"\n    L = np.array([random.randint(RANGE_LOW, RANGE_HIGH) for i in range(n)])\n    skewness = stats.skew(L)\n    kurtosis = stats.kurtosis(L)\n\n    return skewness, kurtosis\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nrandom, numpy, pandas\n```python\nimport random\nimport numpy as np\nimport pandas as pd\n\n# Constants\nRANGE_LOW = 0\nRANGE_HIGH = 1000\n\ndef generate_dataframe(n):\n    \"\"\"\n    Generate a list of 'n' random numbers and create a pandas DataFrame from it.\n\n    Parameters:\n    n (int): The number of random numbers to generate.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the numbers.\n\n    Requirements:\n    - random\n    - numpy\n    - pandas\n\n    Example:\n    >>> df = generate_dataframe(100)\n    >>> print(df.head())\n    \"\"\"\n    L = np.array([random.randint(RANGE_LOW, RANGE_HIGH) for i in range(n)])\n    df = pd.DataFrame(L, columns=['Random Numbers'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nrandom, numpy, cv2, matplotlib.pyplot\n```python\nimport random\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\n# Constants\nRANGE_LOW = 0\nRANGE_HIGH = 255\nIMAGE_SIZE = (100, 100, 3)\n\ndef generate_random_image():\n    \"\"\"\n    Generate a random RGB image and display it.\n\n    Requirements:\n    - random\n    - numpy\n    - cv2\n    - matplotlib.pyplot\n\n    Example:\n    >>> generate_random_image()\n    \"\"\"\n    image = np.zeros(IMAGE_SIZE, dtype=np.uint8)\n\n    for i in range(IMAGE_SIZE[0]):\n        for j in range(IMAGE_SIZE[1]):\n            for k in range(IMAGE_SIZE[2]):\n                image[i, j, k] = random.randint(RANGE_LOW, RANGE_HIGH)\n\n    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n    plt.show()\n```\n"}
{"seed": "{\"task_id\": 8510, \"prompt\": \"def f_8510(num):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"math.ceil(num)\", \"test_start\": \"\\nimport math\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(3.8) == 4\\n\", \"\\n    assert candidate(4.0) == 4\\n\", \"\\n    assert candidate(76.4) == 77\\n\"], \"entry_point\": \"f_8510\", \"intent\": \"\\u00bfC\\u00f3mo redondear decimales hacia arriba de un n\\u00famero `num`?\", \"library\": [\"math\"]}", "generation": "Scenario 1: \nnumpy, math, matplotlib\n```python\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\n\n# Constants\nNUM_SAMPLES = 100\n\ndef plot_ceil_distribution(random_seed):\n    \"\"\"\n    Generate NUM_SAMPLES random float numbers, apply math.ceil() to each of them\n    and plot the distribution of the results.\n\n    Parameters:\n    random_seed (int): The random seed for reproducibility.\n\n    Returns:\n    None: This function plots the distribution of ceil values.\n\n    Requirements:\n    - numpy\n    - math\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_ceil_distribution(42)\n    \"\"\"\n    np.random.seed(random_seed)\n    samples = np.random.rand(NUM_SAMPLES)\n    ceil_samples = [math.ceil(sample) for sample in samples]\n\n    plt.hist(ceil_samples, bins=range(int(samples.min()), int(samples.max())+2), align='left')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nnumpy, math, statistics\n```python\nimport numpy as np\nimport math\nimport statistics\n\n# Constants\nDATA = np.array([3.8, 4.0, 76.4, 5.6, 7.1, 8.3, 9.9])\nDECIMAL_PLACES = 2\n\ndef average_ceil(data=DATA):\n    \"\"\"\n    Calculate the average of math.ceil() applied to each number in a numpy array.\n\n    Parameters:\n    data (numpy.array): The numpy array of floats.\n\n    Returns:\n    float: The average of ceil values.\n\n    Requirements:\n    - numpy\n    - math\n    - statistics\n\n    Example:\n    >>> average_ceil(DATA)\n    \"\"\"\n    ceil_data = [math.ceil(num) for num in data]\n    avg_ceil = statistics.mean(ceil_data)\n\n    return round(avg_ceil, DECIMAL_PLACES)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnumpy, math, pandas\n```python\nimport numpy as np\nimport math\nimport pandas as pd\n\n# Constants\nDATA = {'Value': [3.8, 4.0, 76.4, 5.6, 7.1, 8.3, 9.9]}\n\ndef apply_ceil_to_dataframe(data=DATA):\n    \"\"\"\n    Apply math.ceil() to each number in a pandas DataFrame column.\n\n    Parameters:\n    data (dict): Dictionary of data to create the DataFrame.\n\n    Returns:\n    DataFrame: The DataFrame with ceil applied to 'Value' column.\n\n    Requirements:\n    - numpy\n    - math\n    - pandas\n\n    Example:\n    >>> apply_ceil_to_dataframe(DATA)\n    \"\"\"\n    df = pd.DataFrame(data)\n    df['Value'] = df['Value'].apply(math.ceil)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, math, scipy.stats\n```python\nimport numpy as np\nimport math\nfrom scipy import stats\n\n# Constants\nDATA = np.array([3.8, 4.0, 76.4, 5.6, 7.1, 8.3, 9.9])\nDECIMAL_PLACES = 2\n\ndef median_ceil(data=DATA):\n    \"\"\"\n    Calculate the median of math.ceil() applied to each number in a numpy array.\n\n    Parameters:\n    data (numpy.array): The numpy array of floats.\n\n    Returns:\n    float: The median of ceil values.\n\n    Requirements:\n    - numpy\n    - math\n    - scipy.stats\n\n    Example:\n    >>> median_ceil(DATA)\n    \"\"\"\n    ceil_data = [math.ceil(num) for num in data]\n    median_ceil = stats.median(ceil_data)\n\n    return round(median_ceil, DECIMAL_PLACES)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnumpy, math, matplotlib, sklearn.preprocessing\n```python\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Constants\nDATA = np.array([3.8, 4.0, 76.4, 5.6, 7.1, 8.3, 9.9]).reshape(-1, 1)\n\ndef plot_ceil_scaled_distribution(random_seed, data=DATA):\n    \"\"\"\n    Apply math.ceil() to each number in a numpy array, scale the ceil values\n    using MinMaxScaler and plot the distribution of the scaled results.\n\n    Parameters:\n    random_seed (int): The random seed for reproducibility.\n    data (numpy.array): The numpy array of floats.\n\n    Returns:\n    None: This function plots the distribution of scaled ceil values.\n\n    Requirements:\n    - numpy\n    - math\n    - matplotlib.pyplot\n    - sklearn.preprocessing.MinMaxScaler\n\n    Example:\n    >>> plot_ceil_scaled_distribution(42, DATA)\n    \"\"\"\n    np.random.seed(random_seed)\n    ceil_data = [math.ceil(num) for num in data.flatten()]\n\n    scaler = MinMaxScaler()\n    scaled_ceil_data = scaler.fit_transform(np.array(ceil_data).reshape(-1, 1))\n\n    plt.hist(scaled_ceil_data, bins=np.linspace(0, 1, num=11))\n    plt.show()\n```"}
{"seed": "{\"task_id\": 114782, \"prompt\": \"def f_114782(s):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"sum([int(c in 'aeiouAEIOU') for c in s])\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('mvfbgndvsh') == 0\\n\", \"\\n    assert candidate('vowel') == 2\\n\", \"\\n    assert candidate('AAAAABBC') == 5\\n\"], \"entry_point\": \"f_114782\", \"intent\": \"\\u00bfC\\u00f3mo contar las vocales en una cadena `s`?\", \"library\": []}", "generation": "Scenario 1: \nstring, re, itertools, collections\n```python\nimport string\nimport re\nfrom itertools import chain\nfrom collections import Counter\n\n# Constants\nVOWELS = 'aeiouAEIOU'\n\ndef count_vowels_and_consonants(text):\n    \"\"\"\n    Count the number of vowels, consonants, and other characters in a text.\n    The text can contain multiple lines. \n    The function should be case-insensitive and consider only English alphabets.\n\n    Parameters:\n    text (str): The text to be analyzed.\n\n    Returns:\n    dict: A dictionary with keys 'Vowels', 'Consonants', 'Others' and their respective counts.\n\n    Requirements:\n    - string\n    - re\n    - itertools\n    - collections\n\n    Example:\n    >>> count_vowels_and_consonants(\"Hello, World!\\nHow are you?\")\n    {'Vowels': 7, 'Consonants': 10, 'Others': 7}\n    \"\"\"\n    lines = text.split('\\n')\n    words = list(chain.from_iterable([line.split() for line in lines]))\n    chars = list(chain.from_iterable(words))\n\n    counts = Counter(chars)\n\n    vowels = sum(counts[v] for v in VOWELS if v in counts)\n    consonants = sum(counts[c] for c in string.ascii_letters if c not in VOWELS and c in counts)\n    others = len(chars) - vowels - consonants\n\n    return {'Vowels': vowels, 'Consonants': consonants, 'Others': others}\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nstring, collections, random\n```python\nimport string\nfrom collections import Counter\nimport random\n\n# Constants\nVOWELS = 'aeiouAEIOU'\n\ndef random_string_without_vowels(length):\n    \"\"\"\n    Generate a random string of a specified length that does not contain any vowels.\n\n    Parameters:\n    length (int): The length of the string to be generated.\n\n    Returns:\n    str: A string of the specified length without any vowels.\n\n    Requirements:\n    - string\n    - collections\n    - random\n\n    Example:\n    >>> random_string_without_vowels(10)\n    \"\"\"\n    consonants = [c for c in string.ascii_letters if c not in VOWELS]\n    return ''.join(random.choice(consonants) for _ in range(length))\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nstring, collections, re\n```python\nimport string\nfrom collections import Counter\nimport re\n\n# Constants\nVOWELS = 'aeiouAEIOU'\n\ndef replace_vowels_with_space(text):\n    \"\"\"\n    Replace all vowels in a text with space.\n\n    Parameters:\n    text (str): The text in which vowels are to be replaced with spaces.\n\n    Returns:\n    str: The text after replacing all vowels with spaces.\n\n    Requirements:\n    - string\n    - collections\n    - re\n\n    Example:\n    >>> replace_vowels_with_space(\"Hello, World!\")\n    'H ll , W rld!'\n    \"\"\"\n    return re.sub(f'[{VOWELS}]', ' ', text)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nstring, re, collections\n```python\nimport string\nimport re\nfrom collections import Counter\n\n# Constants\nVOWELS = 'aeiouAEIOU'\n\ndef count_vowels_in_words(text):\n    \"\"\"\n    Count the number of vowels in each word of a text.\n\n    Parameters:\n    text (str): The text containing words.\n\n    Returns:\n    dict: A dictionary with words as keys and the number of vowels in each word as values.\n\n    Requirements:\n    - string\n    - re\n    - collections\n\n    Example:\n    >>> count_vowels_in_words(\"Hello, World!\")\n    {'Hello': 2, 'World': 1}\n    \"\"\"\n    words = re.findall(r'\\b\\w+\\b', text)\n    return {word: sum(c in VOWELS for c in word) for word in words}\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nstring, collections, re, random\n```python\nimport string\nimport re\nfrom collections import Counter\nimport random\n\n# Constants\nVOWELS = 'aeiouAEIOU'\n\ndef scramble_vowels(text):\n    \"\"\"\n    Scramble all vowels in a text while keeping other characters in their original positions.\n\n    Parameters:\n    text (str): The text in which vowels are to be scrambled.\n\n    Returns:\n    str: The text after scrambling all vowels.\n\n    Requirements:\n    - string\n    - collections\n    - re\n    - random\n\n    Example:\n    >>> scramble_vowels(\"Hello, World!\")\n    'Holla, Werld!'\n    \"\"\"\n    vowels = re.findall(f'[{VOWELS}]', text)\n    random.shuffle(vowels)\n    return re.sub(f'[{VOWELS}]', lambda m: vowels.pop(), text)\n```"}
{"seed": "{\"task_id\": 159996, \"prompt\": \"def f_159996(n, m):\\n\\t\", \"suffix\": \"\\n\\treturn matriz\", \"canonical_solution\": \"matriz = []\\n\\t\\n\\tfor i in range(n):\\n\\t    matriz.append([])\\n\\t    for j in range(m):\\n\\t        matriz[i].append(random.randint(0, 100))\", \"test_start\": \"\\nimport random \\n\\ndef check(candidate):\", \"test\": [\"\\n    matriz = candidate(2,3)\\n    assert len(matriz) == 2 and all(len(row)==3 for row in matriz)\\n\"], \"entry_point\": \"f_159996\", \"intent\": \"\\u00bfC\\u00f3mo crear una matriz `matriz` con `n` filas y `m` columnas?\", \"library\": [\"random\"]}", "generation": "Scenario 1: \nrandom, numpy, seaborn, matplotlib.pyplot\n```python\nimport random\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nMIN_VAL = 0\nMAX_VAL = 100\n\ndef generate_heatmap(n, m):\n    \"\"\"\n    Generate a heatmap for a matrix with `n` rows and `m` columns, each cell is randomly filled with a value \n    between MIN_VAL and MAX_VAL.\n\n    Parameters:\n    n (int): The number of rows.\n    m (int): The number of columns.\n\n    Returns:\n    ndarray: A numpy ndarray with the matrix values.\n    \n    Requirements:\n    - numpy\n    - random\n    - seaborn\n    - matplotlib.pyplot\n    \n    Example:\n    >>> matrix = generate_heatmap(10, 10)\n    >>> sns.heatmap(matrix)\n    \"\"\"\n    matrix = np.random.randint(MIN_VAL, MAX_VAL, (n, m))\n\n    return matrix\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nrandom, pandas, matplotlib.pyplot, seaborn\n```python\nimport random\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nMIN_VAL = 0\nMAX_VAL = 100\n\ndef generate_scatter_plot(n, m):\n    \"\"\"\n    Generate a scatter plot for a matrix with `n` rows and `m` columns, each cell is randomly filled with a value \n    between MIN_VAL and MAX_VAL.\n\n    Parameters:\n    n (int): The number of data points.\n    m (int): The number of features/dimensions.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the matrix values.\n    \n    Requirements:\n    - pandas\n    - random\n    - matplotlib.pyplot\n    - seaborn\n    \n    Example:\n    >>> df = generate_scatter_plot(100, 2)\n    >>> sns.scatterplot(data=df, x=\"Feature 1\", y=\"Feature 2\")\n    \"\"\"\n    data = []\n    for _ in range(n):\n        data.append([random.randint(MIN_VAL, MAX_VAL) for _ in range(m)])\n\n    df = pd.DataFrame(data, columns=[f'Feature {i+1}' for i in range(m)])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nrandom, numpy, matplotlib.pyplot, pandas\n```python\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants\nMIN_VAL = 0\nMAX_VAL = 100\n\ndef plot_histogram(n, m):\n    \"\"\"\n    Generate a histogram for a matrix with `n` rows and `m` columns, each cell is randomly filled with a value \n    between MIN_VAL and MAX_VAL.\n\n    Parameters:\n    n (int): The number of rows.\n    m (int): The number of columns.\n\n    Returns:\n    ndarray: A numpy ndarray with the matrix values.\n    \n    Requirements:\n    - numpy\n    - random\n    - matplotlib.pyplot\n    - pandas\n    \n    Example:\n    >>> matrix = plot_histogram(1000, 1)\n    >>> plt.hist(matrix, bins=20)\n    \"\"\"\n    matrix = np.random.randint(MIN_VAL, MAX_VAL, (n, m))\n\n    return matrix\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nrandom, numpy, matplotlib.pyplot, csv\n```python\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport csv\n\n# Constants\nMIN_VAL = 0\nMAX_VAL = 100\nFILE_NAME = 'matrix.csv'\n\ndef generate_csv(n, m):\n    \"\"\"\n    Generate a CSV file for a matrix with `n` rows and `m` columns, each cell is randomly filled with a value \n    between MIN_VAL and MAX_VAL.\n\n    Parameters:\n    n (int): The number of rows.\n    m (int): The number of columns.\n    \n    Requirements:\n    - numpy\n    - random\n    - matplotlib.pyplot\n    - csv\n    \n    Example:\n    >>> generate_csv(100, 2)\n    \"\"\"\n    matrix = np.random.randint(MIN_VAL, MAX_VAL, (n, m))\n\n    with open(FILE_NAME, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerows(matrix)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nrandom, numpy, matplotlib.pyplot, scipy.spatial\n```python\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.spatial import distance\n\n# Constants\nMIN_VAL = 0\nMAX_VAL = 100\n\ndef calculate_distances(n, m):\n    \"\"\"\n    Generate a matrix with `n` rows and `m` columns, each cell is randomly filled with a value \n    between MIN_VAL and MAX_VAL. Then calculate the Euclidean distance between each pair of rows.\n\n    Parameters:\n    n (int): The number of rows.\n    m (int): The number of columns.\n\n    Returns:\n    ndarray: A numpy ndarray with the distance values.\n    \n    Requirements:\n    - numpy\n    - random\n    - matplotlib.pyplot\n    - scipy.spatial.distance\n    \n    Example:\n    >>> distances = calculate_distances(5, 2)\n    \"\"\"\n    matrix = np.random.randint(MIN_VAL, MAX_VAL, (n, m))\n    distances = distance.cdist(matrix, matrix, 'euclidean')\n\n    return distances\n```\n"}
{"seed": "{\"task_id\": 159996, \"prompt\": \"def f_159996(n, m):\\n\\t\", \"suffix\": \"\\n\\treturn matriz\", \"canonical_solution\": \"matriz = np.random.randint(0, 100, size=(n, m))\", \"test_start\": \"\\nimport numpy as np\\n\\ndef check(candidate):\", \"test\": [\"\\n    matriz = candidate(2,3)\\n    assert len(matriz) == 2 and all(len(row)==3 for row in matriz)\\n\"], \"entry_point\": \"f_159996\", \"intent\": \"\\u00bfC\\u00f3mo crear una matriz `matriz` con `n` filas y `m` columnas?\", \"library\": [\"numpy\"]}", "generation": "Scenario 1: \nnumpy, pandas, matplotlib, random\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\n\n# Constants\nCOLORS = ['red', 'blue', 'green', 'yellow', 'purple']\n\ndef create_and_plot_matrix(n, m):\n    \"\"\"\n    Create a random integer matrix with n rows and m columns and plot a bar chart \n    for the mean of each row color-coded randomly.\n    \n    Parameters:\n    n (int): Number of rows.\n    m (int): Number of columns.\n    \n    Returns:\n    tuple: A tuple containing the numpy array and the plotted bar chart.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - random\n    \n    Example:\n    >>> matrix, plot = create_and_plot_matrix(5, 10)\n    >>> print(matrix)\n    >>> plt.show()\n    \"\"\"\n    matrix = np.random.randint(0, 100, size=(n, m))\n    df = pd.DataFrame(matrix)\n    means = df.mean(axis=1)\n    colors = random.choices(COLORS, k=n)\n    plot = means.plot(kind='bar', color=colors)\n    return matrix, plot\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nnumpy, scipy.stats, matplotlib\n```python\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef create_normal_distribution(n, mean, std_dev):\n    \"\"\"\n    Create a normal distribution with n samples, a given mean, and standard deviation. \n    Then plot the histogram and probability density function (PDF) of the distribution.\n    \n    Parameters:\n    n (int): Number of samples.\n    mean (float): The mean of the normal distribution.\n    std_dev (float): The standard deviation of the normal distribution.\n    \n    Returns:\n    tuple: A tuple containing the numpy array and the plotted histogram and PDF.\n    \n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n    \n    Example:\n    >>> samples, plot = create_normal_distribution(1000, 0, 1)\n    >>> print(samples)\n    >>> plt.show()\n    \"\"\"\n    samples = np.random.normal(mean, std_dev, n)\n    plt.hist(samples, bins=30, density=True, alpha=0.6, color='g')\n\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, mean, std_dev)\n    plt.plot(x, p, 'k', linewidth=2)\n    title = \"Fit results: mean = %.2f,  std_dev = %.2f\" % (mean, std_dev)\n    plt.title(title)\n\n    return samples, plt\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnumpy, sklearn.decomposition, matplotlib\n```python\nimport numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\ndef create_and_decompose_matrix(n, m):\n    \"\"\"\n    Create a random integer matrix with n rows and m columns, perform Principal \n    Component Analysis (PCA) on it, and plot the explained variance ratio of each component.\n    \n    Parameters:\n    n (int): Number of rows.\n    m (int): Number of columns.\n    \n    Returns:\n    tuple: A tuple containing the numpy array and the plotted explained variance ratio.\n    \n    Requirements:\n    - numpy\n    - sklearn.decomposition\n    - matplotlib.pyplot\n    \n    Example:\n    >>> matrix, plot = create_and_decompose_matrix(100, 5)\n    >>> print(matrix)\n    >>> plt.show()\n    \"\"\"\n    matrix = np.random.randint(0, 100, size=(n, m))\n    pca = PCA(n_components=min(n, m))\n    pca.fit(matrix)\n    plt.bar(range(1, min(n, m) + 1), pca.explained_variance_ratio_)\n    plt.xlabel('Principal Component')\n    plt.ylabel('Explained Variance Ratio')\n    plt.title('PCA Explained Variance Ratio')\n    return matrix, plt\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, seaborn, pandas\n```python\nimport numpy as np\nimport seaborn as sns\nimport pandas as pd\n\ndef create_and_visualize_matrix(n, m):\n    \"\"\"\n    Create a random integer matrix with n rows and m columns and visualize \n    the correlation matrix using a heatmap.\n    \n    Parameters:\n    n (int): Number of rows.\n    m (int): Number of columns.\n    \n    Returns:\n    tuple: A tuple containing the numpy array and the plotted heatmap.\n    \n    Requirements:\n    - numpy\n    - seaborn\n    - pandas\n    \n    Example:\n    >>> matrix, plot = create_and_visualize_matrix(5, 5)\n    >>> print(matrix)\n    >>> plt.show()\n    \"\"\"\n    matrix = np.random.randint(0, 100, size=(n, m))\n    df = pd.DataFrame(matrix)\n    corr = df.corr()\n    ax = sns.heatmap(corr, annot=True)\n    return matrix, ax\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnumpy, scipy.sparse, matplotlib\n```python\nimport numpy as np\nfrom scipy import sparse\nimport matplotlib.pyplot as plt\n\ndef create_and_visualize_sparse_matrix(n, m, density):\n    \"\"\"\n    Create a random sparse matrix with n rows, m columns, and a given density. \n    Then visualize the sparse matrix.\n    \n    Parameters:\n    n (int): Number of rows.\n    m (int): Number of columns.\n    density (float): The density of the sparse matrix.\n    \n    Returns:\n    tuple: A tuple containing the sparse matrix and the plotted matrix.\n    \n    Requirements:\n    - numpy\n    - scipy.sparse\n    - matplotlib.pyplot\n    \n    Example:\n    >>> matrix, plot = create_and_visualize_sparse_matrix(10, 10, 0.2)\n    >>> print(matrix)\n    >>> plt.show()\n    \"\"\"\n    matrix = sparse.random(n, m, density=density, format='csr')\n    plt.figure(figsize=(5, 5))\n    plt.spy(matrix, markersize=5)\n    return matrix, plt\n```\nAbove is the illustration."}
{"seed": "{\"task_id\": 14454, \"prompt\": \"def f_14454(l):\\n\\t\", \"suffix\": \"\\n\\treturn sel\", \"canonical_solution\": \"shuffle(l)\\n\\tsel = l[0]\", \"test_start\": \"\\nimport random\\nfrom random import shuffle\\n\\ndef check(candidate):\", \"test\": [\"\\n    results = [candidate([1,2,3,4,5]) for i in range(10)]\\n    assert all([(x in [1,2,3,4,5]) for x in results])\\n    assert len(set(results)) > 1\\n\"], \"entry_point\": \"f_14454\", \"intent\": \"\\u00bfC\\u00f3mo seleccionar un elemento aleatorio de una lista `l`?\", \"library\": [\"random\"]}", "generation": "Scenario 1: \nrandom, collections, os, hashlib\n```python\nimport random\nimport os\nimport hashlib\nfrom collections import Counter\n\n# Constants\nFREQUENCY_THRESHOLD = 5\n\ndef check_file_frequency(directory):\n    \"\"\"\n    Check the frequency of file types in a directory. If a file type appears more than a threshold, \n    select a random file of this type, and return its MD5 hash.\n    \n    Parameters:\n    directory (str): The directory to check.\n    \n    Returns:\n    str: The MD5 hash of a randomly selected file.\n    \n    Requirements:\n    - random\n    - os\n    - hashlib\n    - collections.Counter\n    \n    Example:\n    >>> check_file_frequency('/home/user/Documents/')\n    \"\"\"\n    files = os.listdir(directory)\n    file_types = [os.path.splitext(file)[1] for file in files]\n    frequency = Counter(file_types)\n\n    common_files = [file_type for file_type, count in frequency.items() if count > FREQUENCY_THRESHOLD]\n    \n    if common_files:\n        common_files_files = [file for file in files if os.path.splitext(file)[1] in common_files]\n        selected_file = random.choice(common_files_files)\n        with open(os.path.join(directory, selected_file), 'rb') as f:\n            file_hash = hashlib.md5(f.read()).hexdigest()\n\n        return file_hash\n    else:\n        return None\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nrandom, itertools, math, matplotlib\n```python\nimport random\nimport math\nimport matplotlib.pyplot as plt\nfrom itertools import combinations\n\n# Constants\nDISTANCE_THRESHOLD = 5\n\ndef plot_close_points(points):\n    \"\"\"\n    From a list of points, find pairs of points that are close to each other (distance less than a threshold),\n    select a random pair and plot all points highlighting the selected pair.\n    \n    Parameters:\n    points (list): A list of tuples representing the points.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - random\n    - itertools\n    - math\n    - matplotlib.pyplot\n    \n    Example:\n    >>> plot_close_points([(1, 2), (3, 4), (5, 6), (1, 3), (2, 4), (3, 5)])\n    \"\"\"\n    close_pairs = [pair for pair in combinations(points, 2) \n                   if math.dist(pair[0], pair[1]) < DISTANCE_THRESHOLD]\n\n    if close_pairs:\n        selected_pair = random.choice(close_pairs)\n    \n        x, y = zip(*points)\n        plt.scatter(x, y, color='blue')\n\n        x, y = zip(*selected_pair)\n        plt.scatter(x, y, color='red')\n\n        plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nrandom, re, string\n```python\nimport random\nimport re\nimport string\n\n# Constants\nPUNCTUATION = string.punctuation\n\ndef mask_random_word(sentence):\n    \"\"\"\n    From a sentence, select a word at random and mask it with asterisks, keeping punctuation intact.\n    \n    Parameters:\n    sentence (str): The sentence to mask a word from.\n    \n    Returns:\n    str: The sentence with a word masked.\n    \n    Requirements:\n    - random\n    - re\n    - string\n    \n    Example:\n    >>> mask_random_word(\"Hello, world!\")\n    \"\"\"\n    words = re.findall(r'\\b\\w+\\b', sentence)\n    selected_word = random.choice(words)\n\n    masked_sentence = re.sub(r'\\b'+selected_word+r'\\b', '*'*len(selected_word), sentence)\n\n    return masked_sentence\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nrandom, pandas, numpy\n```python\nimport random\nimport pandas as pd\nimport numpy as np\n\n# Constants\nDATA = pd.DataFrame(np.random.randint(0,100,size=(100, 4)), columns=list('ABCD'))\n\ndef replace_random_value():\n    \"\"\"\n    From a pandas DataFrame, select a random value and replace it with NaN.\n    \n    Returns:\n    DataFrame: The DataFrame with a value replaced.\n    \n    Requirements:\n    - random\n    - pandas\n    - numpy\n    \n    Example:\n    >>> replace_random_value()\n    \"\"\"\n    random_index = random.choice(DATA.index)\n    random_column = random.choice(DATA.columns)\n\n    DATA.at[random_index, random_column] = np.nan\n\n    return DATA\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nrandom, json, os, pathlib\n```python\nimport random\nimport json\nimport os\nfrom pathlib import Path\n\n# Constants\nDIRECTORY = '/home/user/Documents/'\n\ndef select_random_json_key():\n    \"\"\"\n    From a directory containing JSON files, select a random file, then select a random key from the file, \n    and return the key and its corresponding value.\n    \n    Returns:\n    tuple: A tuple containing the selected key and its value.\n    \n    Requirements:\n    - random\n    - json\n    - os\n    - pathlib.Path\n    \n    Example:\n    >>> select_random_json_key()\n    \"\"\"\n    files = list(Path(DIRECTORY).rglob('*.json'))\n    random_file = random.choice(files)\n\n    with open(random_file, 'r') as f:\n        data = json.load(f)\n\n    random_key = random.choice(list(data.keys()))\n    random_value = data[random_key]\n\n    return (random_key, random_value)\n```"}
{"seed": "{\"task_id\": 14454, \"prompt\": \"def f_14454(l):\\n\\t\", \"suffix\": \"\\n\\treturn sel\", \"canonical_solution\": \"sel = random.sample(l, 1)\", \"test_start\": \"\\nimport random\\n\\ndef check(candidate):\", \"test\": [\"\\n    results = [candidate([1,2,3,4,5])[0] for i in range(10)]\\n    assert all([(x in [1,2,3,4,5]) for x in results])\\n    assert len(set(results)) > 0\\n\"], \"entry_point\": \"f_14454\", \"intent\": \"\\u00bfC\\u00f3mo seleccionar un elemento aleatorio de una lista `l`?\", \"library\": [\"random\"]}", "generation": "Scenario 1: \npandas, random, matplotlib, numpy, math\n```python\nimport pandas as pd\nfrom random import choice\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport math\n\n# Constants\nCOUNTRIES = ['USA', 'UK', 'China', 'Japan', 'Germany']\nPRODUCTS = ['Shoes', 'Shirts', 'Pants', 'Watches', 'Bags']\n\ndef sales_forecast():\n    \"\"\"\n    Generate a sales forecast for different products in different countries.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the forecasted sales.\n    \n    Requirements:\n    - pandas\n    - random.choice\n    - matplotlib.pyplot\n    - numpy\n    - math\n\n    Example:\n    >>> forecast = sales_forecast()\n    >>> print(forecast)\n    >>> forecast.plot(kind='bar', stacked=True)\n    \"\"\"\n    forecast_data = []\n\n    for country in COUNTRIES:\n        for product in PRODUCTS:\n            sales = math.ceil(np.random.normal(10000, 2000))\n            forecast_data.append([country, product, sales])\n\n    forecast_df = pd.DataFrame(forecast_data, columns=['Country', 'Product', 'Forecasted Sales'])\n\n    return forecast_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nrandom, os, pathlib, zipfile\n```python\nimport os\nfrom random import choice\nfrom pathlib import Path\nfrom zipfile import ZipFile\n\n# Constants\nDIRECTORY = './files'\n\ndef choose_and_extract_random_zipfile():\n    \"\"\"\n    Select a random zipfile from a directory and extract it.\n\n    Returns:\n    str: The name of the selected zipfile.\n\n    Requirements:\n    - os\n    - random.choice\n    - pathlib.Path\n    - zipfile.ZipFile\n\n    Example:\n    >>> choose_and_extract_random_zipfile()\n    'selected_file.zip'\n    \"\"\"\n    files = os.listdir(DIRECTORY)\n    zipfiles = [file for file in files if file.endswith('.zip')]\n    selected_file = choice(zipfiles)\n\n    with ZipFile(Path(DIRECTORY) / selected_file, 'r') as zip_ref:\n        zip_ref.extractall(DIRECTORY)\n\n    return selected_file\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nrandom, itertools, numpy, matplotlib\n```python\nfrom random import choice\nimport itertools\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLORS = ['red', 'blue', 'green', 'yellow', 'purple']\n\ndef plot_random_walk():\n    \"\"\"\n    Generate a random walk and plot it.\n\n    Returns:\n    None.\n\n    Requirements:\n    - random.choice\n    - itertools\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_random_walk()\n    \"\"\"\n    walk = [0]\n    for i in range(1000):\n        step = choice([-1, 1])\n        walk.append(walk[-1] + step)\n\n    plt.plot(walk, color=choice(COLORS))\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nrandom, hashlib, binascii\n```python\nfrom random import choice\nimport hashlib\nimport binascii\n\n# Constants\nCHARSET = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789'\n\ndef generate_random_hash():\n    \"\"\"\n    Generate a random string and compute its SHA256 hash.\n\n    Returns:\n    str: The SHA256 hash of the random string.\n\n    Requirements:\n    - random.choice\n    - hashlib\n    - binascii\n\n    Example:\n    >>> generate_random_hash()\n    '29c3eea3f305d6b823f562ac4be35217adfd18644fb24696a07e5e76f2ce9d00'\n    \"\"\"\n    random_string = ''.join([choice(CHARSET) for _ in range(10)])\n    sha256 = hashlib.sha256(random_string.encode())\n    return binascii.hexlify(sha256.digest()).decode()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nrandom, turtle, time\n```python\nfrom random import choice\nimport turtle\nimport time\n\n# Constants\nCOLORS = ['red', 'blue', 'green', 'yellow', 'purple']\n\ndef draw_random_squares():\n    \"\"\"\n    Draw random squares with the turtle graphics module.\n\n    Returns:\n    None.\n\n    Requirements:\n    - random.choice\n    - turtle\n    - time\n\n    Example:\n    >>> draw_random_squares()\n    \"\"\"\n    window = turtle.Screen()\n    window.bgcolor('white')\n\n    t = turtle.Turtle()\n    t.speed(1)\n\n    for _ in range(5):\n        t.color(choice(COLORS))\n        for _ in range(4):\n            t.forward(100)\n            t.right(90)\n        time.sleep(1)\n\n    window.mainloop()\n```\n"}
{"seed": "{\"task_id\": 46558, \"prompt\": \"def f_46558(l, n):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"max(l, key=itemgetter(n))[n]\", \"test_start\": \"\\nfrom operator import itemgetter \\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([[1,2,3],[4,5,6], [-1,-2,0]], 0) == 4\\n\"], \"entry_point\": \"f_46558\", \"intent\": \"\\u00bfC\\u00f3mo encontrar el mayor elemento en la posici\\u00f3n `n` una lista de listas `l`?\", \"library\": [\"operator\"]}", "generation": "Scenario 1: \npandas, numpy, matplotlib, operator\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom operator import itemgetter\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef plot_max_in_position(dataframe, n):\n    \"\"\"\n    Find the maximum element in the position `n` of a pandas DataFrame and plot the column.\n    \n    Parameters:\n    dataframe (DataFrame): The pandas DataFrame.\n    n (int): The position in the DataFrame.\n    \n    Returns:\n    float: The maximum element in position `n`.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib\n    - operator\n\n    Example:\n    >>> df = pd.DataFrame(np.random.randint(0,100,size=(100, 5)), columns=COLUMNS)\n    >>> max_value = plot_max_in_position(df, 2)\n    >>> print(max_value)\n    \"\"\"\n    max_value = max(dataframe.values, key=itemgetter(n))[n]\n    dataframe.iloc[:, n].plot(kind='bar')\n    plt.show()\n    return max_value\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, operator, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nfrom operator import itemgetter\nimport seaborn as sns\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef find_and_visualize_max_position(df, n):\n    \"\"\"\n    Find the maximum element in the position `n` of a pandas DataFrame and visualize the distribution of the column.\n    \n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n    n (int): The position in the DataFrame.\n    \n    Returns:\n    float: The maximum element in position `n`.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - operator\n    - seaborn\n\n    Example:\n    >>> df = pd.DataFrame(np.random.randint(0,100,size=(100, 5)), columns=COLUMNS)\n    >>> max_value = find_and_visualize_max_position(df, 2)\n    >>> print(max_value)\n    \"\"\"\n    max_value = max(df.values, key=itemgetter(n))[n]\n    sns.displot(df.iloc[:, n])\n    plt.show()\n    return max_value\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, operator, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom operator import itemgetter\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef max_position_and_histogram(df, n):\n    \"\"\"\n    Find the maximum element in the position `n` of a pandas DataFrame and display a histogram of the column.\n    \n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n    n (int): The position in the DataFrame.\n    \n    Returns:\n    float: The maximum element in position `n`.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - operator\n    - matplotlib\n\n    Example:\n    >>> df = pd.DataFrame(np.random.randint(0,100,size=(100, 5)), columns=COLUMNS)\n    >>> max_value = max_position_and_histogram(df, 2)\n    >>> print(max_value)\n    \"\"\"\n    max_value = max(df.values, key=itemgetter(n))[n]\n    df.iloc[:, n].hist()\n    plt.show()\n    return max_value\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, operator, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom operator import itemgetter\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef max_position_and_pie_chart(df, n):\n    \"\"\"\n    Find the maximum element in the position `n` of a pandas DataFrame and display a pie chart of the column values.\n    \n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n    n (int): The position in the DataFrame.\n    \n    Returns:\n    float: The maximum element in position `n`.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - operator\n    - matplotlib\n\n    Example:\n    >>> df = pd.DataFrame(np.random.randint(0,5,size=(100, 5)), columns=COLUMNS)\n    >>> max_value = max_position_and_pie_chart(df, 2)\n    >>> print(max_value)\n    \"\"\"\n    max_value = max(df.values, key=itemgetter(n))[n]\n    df.iloc[:, n].value_counts().plot(kind='pie')\n    plt.show()\n    return max_value\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, operator, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nfrom operator import itemgetter\nimport seaborn as sns\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef max_position_and_box_plot(df, n):\n    \"\"\"\n    Find the maximum element in the position `n` of a pandas DataFrame and display a box plot of the column.\n    \n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n    n (int): The position in the DataFrame.\n    \n    Returns:\n    float: The maximum element in position `n`.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - operator\n    - seaborn\n\n    Example:\n    >>> df = pd.DataFrame(np.random.randint(0,100,size=(100, 5)), columns=COLUMNS)\n    >>> max_value = max_position_and_box_plot(df, 2)\n    >>> print(max_value)\n    \"\"\"\n    max_value = max(df.values, key=itemgetter(n))[n]\n    sns.boxplot(y=df.iloc[:, n])\n    plt.show()\n    return max_value\n```"}
{"seed": "{\"task_id\": 213125, \"prompt\": \"def f_213125(list):\\n\\t\", \"suffix\": \"\\n\\treturn list\", \"canonical_solution\": \"for i in range(len(list)):\\n\\t    list[i] = int(list[i])\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(['0','1','2','3','4']) == [0,1,2,3,4]\\n\", \"\\n    assert candidate(['100','444','45']) == [100, 444, 45]\\n\"], \"entry_point\": \"f_213125\", \"intent\": \"\\\"C\\u00f3mo convertir una lista `list` de tipo `str` a una de tipo `int`\\\"\", \"library\": []}", "generation": "Scenario 1: \npandas, random, matplotlib\n```python\nimport pandas as pd\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nPRODUCT_CATEGORIES = ['Electronics', 'Toys', 'Furniture', 'Grocery', 'Books']\n\ndef generate_sales_report():\n    \"\"\"\n    Generate a report of sales by product category over the last 12 months.\n\n    Returns:\n    DataFrame: A pandas DataFrame with sales data for each product category.\n\n    Requirements:\n    - pandas\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> report = generate_sales_report()\n    >>> print(report)\n    >>> report.plot(kind='bar', stacked=True)\n    \"\"\"\n    report_data = []\n\n    for month in range(1, 13):\n        monthly_sales = [randint(100, 1000) for _ in PRODUCT_CATEGORIES]\n        report_data.append([month] + monthly_sales)\n\n    columns = ['Month'] + PRODUCT_CATEGORIES\n    report_df = pd.DataFrame(report_data, columns=columns)\n\n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy\n```python\nimport numpy as np\n\ndef f_213125_v2(list):\n    \"\"\"\n    Convert a list of string to numpy array of integers.\n\n    Parameters:\n    list (list): The list of string.\n\n    Returns:\n    numpy.array: The numpy array of integers.\n\n    Requirements:\n    - numpy\n\n    Example:\n    >>> f_213125_v2(['0','1','2','3','4'])\n    \"\"\"\n    return np.array(list, dtype=int)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, datetime\n```python\nimport pandas as pd\nfrom datetime import datetime\n\ndef generate_date_range(start_date, end_date):\n    \"\"\"\n    Generate a DataFrame with a range of dates between two given dates.\n\n    Parameters:\n    start_date (str): The start date in \"yyyy-mm-dd\" format.\n    end_date (str): The end date in \"yyyy-mm-dd\" format.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the date range.\n\n    Requirements:\n    - pandas\n    - datetime\n\n    Example:\n    >>> generate_date_range('2020-01-01', '2020-01-05')\n    \"\"\"\n    start_date = datetime.strptime(start_date, '%Y-%m-%d')\n    end_date = datetime.strptime(end_date, '%Y-%m-%d')\n    date_range = pd.date_range(start_date, end_date, freq='D')\n\n    return pd.DataFrame({'Date': date_range})\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, random\n```python\nimport numpy as np\nfrom random import randint\n\ndef generate_random_matrix(n):\n    \"\"\"\n    Generate a nxn matrix with random integer values between 0 and 100.\n\n    Parameters:\n    n (int): The size of the matrix.\n\n    Returns:\n    numpy.array: The nxn matrix.\n\n    Requirements:\n    - numpy\n    - random\n\n    Example:\n    >>> generate_random_matrix(3)\n    \"\"\"\n    return np.array([[randint(0, 100) for _ in range(n)] for _ in range(n)])\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, pandas\n```python\nimport numpy as np\nimport pandas as pd\n\ndef convert_to_dataframe(list):\n    \"\"\"\n    Convert a list of string to a pandas DataFrame of integers.\n\n    Parameters:\n    list (list): The list of string.\n\n    Returns:\n    DataFrame: The pandas DataFrame of integers.\n\n    Requirements:\n    - numpy\n    - pandas\n\n    Example:\n    >>> convert_to_dataframe(['0','1','2','3','4'])\n    \"\"\"\n    array = np.array(list, dtype=int)\n    return pd.DataFrame(array, columns=['Value'])\n```"}
{"seed": "{\"task_id\": 151572, \"prompt\": \"def f_151572(lista):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"Counter(lista).most_common()[0][0]\", \"test_start\": \"\\nfrom collections import Counter\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([1,1,2,3,4]) == 1\\n\", \"\\n    assert candidate([8,2,5,7,3,7]) == 7\\n\"], \"entry_point\": \"f_151572\", \"intent\": \"\\u00bfC\\u00f3mo hallar el numero mas repetido de una `lista`?\", \"library\": [\"collections\"]}", "generation": "Scenario 1: \ncollections, random, matplotlib, numpy\n```python\nimport collections\nimport random\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nNAMES = ['John', 'Jane', 'Alex', 'Alice', 'Steve', 'Emma']\n\ndef generate_name_frequency(names_list):\n    \"\"\"\n    Generate a frequency distribution of names in a list and plot the distribution.\n\n    Parameters:\n    names_list (list of str): The list of names.\n\n    Returns:\n    dict: A dictionary with the frequency of each name.\n    \n    Requirements:\n    - collections\n    - random\n    - matplotlib.pyplot\n    - numpy\n\n    Example:\n    >>> names_list = [random.choice(NAMES) for _ in range(1000)]\n    >>> freq = generate_name_frequency(names_list)\n    >>> print(freq)\n    >>> plt.bar(freq.keys(), freq.values())\n    >>> plt.show()\n    \"\"\"\n    counter = collections.Counter(names_list)\n    freq_dict = dict(counter)\n\n    return freq_dict\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncollections, os, glob\n```python\nimport collections\nimport os\nimport glob\n\ndef count_file_types(directory):\n    \"\"\"\n    Count the number of files of each type in a directory.\n\n    Parameters:\n    directory (str): The directory to scan.\n\n    Returns:\n    dict: A dictionary with the count of each file type.\n\n    Requirements:\n    - collections\n    - os\n    - glob\n\n    Example:\n    >>> directory = '/home/user/Documents'\n    >>> file_types = count_file_types(directory)\n    >>> print(file_types)\n    \"\"\"\n    files = glob.glob(os.path.join(directory, \"*\"))\n    file_types = [os.path.splitext(file)[1] for file in files]\n    counter = collections.Counter(file_types)\n    file_type_counts = dict(counter)\n\n    return file_type_counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ncollections, string, random\n```python\nimport collections\nimport string\nimport random\n\ndef generate_random_text(length):\n    \"\"\"\n    Generate a random text of a given length and count the frequency of each letter.\n\n    Parameters:\n    length (int): The length of the text to generate.\n\n    Returns:\n    str, dict: The generated text and a dictionary with the frequency of each letter.\n\n    Requirements:\n    - collections\n    - string\n    - random\n\n    Example:\n    >>> text_length = 1000\n    >>> text, letter_freq = generate_random_text(text_length)\n    >>> print(text)\n    >>> print(letter_freq)\n    \"\"\"\n    text = ''.join(random.choice(string.ascii_lowercase) for _ in range(length))\n    counter = collections.Counter(text)\n    letter_freq = dict(counter)\n\n    return text, letter_freq\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ncollections, pandas, numpy\n```python\nimport collections\nimport pandas as pd\nimport numpy as np\n\n# Sample Data\n# df = pd.DataFrame({'A': np.random.choice(list('abcdef'), 1000),\n#                    'B': np.random.randint(1, 10, 1000)})\n\ndef most_common_in_dataframe(df, column):\n    \"\"\"\n    Find the most common value in a column of a pandas DataFrame.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n    column (str): The column name.\n\n    Returns:\n    tuple: The most common value and its count.\n\n    Requirements:\n    - collections\n    - pandas\n    - numpy \n\n    Example:\n    >>> most_common_in_dataframe(df, 'A')\n    \"\"\"\n    counter = collections.Counter(df[column])\n    most_common = counter.most_common(1)[0]\n\n    return most_common\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\ncollections, matplotlib, numpy\n```python\nimport collections\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nVALUES = np.arange(1, 11)\n\ndef plot_value_counts(values):\n    \"\"\"\n    Plot a histogram of the counts of values in a list.\n\n    Parameters:\n    values (list of int): The list of values.\n\n    Returns:\n    None\n\n    Requirements:\n    - collections\n    - matplotlib.pyplot\n    - numpy\n\n    Example:\n    >>> values = np.random.choice(VALUES, 1000)\n    >>> plot_value_counts(values)\n    \"\"\"\n    counter = collections.Counter(values)\n    values, counts = zip(*counter.items())\n\n    plt.bar(values, counts)\n    plt.show()\n```\n"}
{"seed": "{\"task_id\": 115010, \"prompt\": \"def f_115010(c, m):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"np.array(m)[:, c]\", \"test_start\": \"\\nimport numpy as np \\n\\ndef check(candidate):\", \"test\": [\"\\n    assert all(candidate(1, np.array([[1,2],[3,4],[5,6]])) == np.array([2, 4, 6]))\\n\"], \"entry_point\": \"f_115010\", \"intent\": \"\\u00bfC\\u00f3mo obtener toda una columna `c` de una matriz `m`?\", \"library\": [\"numpy\"]}", "generation": "Scenario 1:\nnumpy, random, matplotlib.pyplot, seaborn\n```python\nimport numpy as np\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nROWS = 10\nCOLS = 10\n\ndef display_heatmap(c, m):\n    \"\"\"\n    Generate a random matrix of given size, select a column, and draw a heatmap \n    using seaborn for the selected column.\n\n    Parameters:\n    c (int): The column index.\n    m (tuple): The size of the matrix.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - random\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> display_heatmap(2, (ROWS, COLS))\n    \"\"\"\n    matrix = np.random.randint(0, 100, size=m)\n    column = matrix[:, c]\n    column = column.reshape(-1, 1)\n    \n    plt.figure(figsize=(2, 10))\n    sns.heatmap(column, annot=True, cmap='coolwarm')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, pandas, matplotlib.pyplot, sklearn.preprocessing\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Constants\nROWS = 100\nCOLS = 5\n\ndef plot_column_distribution(c, m):\n    \"\"\"\n    Generate a random matrix of given size, select a column, normalize its values \n    using MinMaxScaler and plot its distribution using Pandas.\n\n    Parameters:\n    c (int): The column index.\n    m (tuple): The size of the matrix.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - sklearn.preprocessing\n\n    Example:\n    >>> plot_column_distribution(2, (ROWS, COLS))\n    \"\"\"\n    matrix = np.random.randint(0, 100, size=m)\n    column = matrix[:, c]\n    \n    scaler = MinMaxScaler()\n    column_scaled = scaler.fit_transform(column.reshape(-1, 1))\n\n    df = pd.DataFrame(column_scaled, columns=['Column'])\n    df['Column'].plot(kind='hist')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, sklearn.decomposition, matplotlib.pyplot, scipy.spatial.distance\n```python\nimport numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nfrom scipy.spatial.distance import cdist\n\n# Constants\nROWS = 100\nCOLS = 5\n\ndef calculate_euclidean_distance(c, m):\n    \"\"\"\n    Generate a random matrix of given size, select a column, perform PCA to reduce \n    its dimension to 2, and calculate the Euclidean distance between the reduced \n    column data and the origin.\n\n    Parameters:\n    c (int): The column index.\n    m (tuple): The size of the matrix.\n\n    Returns:\n    ndarray: An array of distances.\n\n    Requirements:\n    - numpy\n    - sklearn.decomposition\n    - matplotlib.pyplot\n    - scipy.spatial.distance\n\n    Example:\n    >>> calculate_euclidean_distance(2, (ROWS, COLS))\n    \"\"\"\n    matrix = np.random.randint(0, 100, size=m)\n    column = matrix[:, c]\n    \n    pca = PCA(n_components=2)\n    column_reduced = pca.fit_transform(column.reshape(-1, 1))\n    \n    origin = np.zeros((1, 2))\n    distances = cdist(column_reduced, origin, 'euclidean')\n\n    return distances\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, scipy.stats, matplotlib.pyplot\n```python\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\n# Constants\nROWS = 100\nCOLS = 5\n\ndef plot_column_density(c, m):\n    \"\"\"\n    Generate a random matrix of given size, select a column, and plot its \n    kernel density estimation (KDE) using scipy and matplotlib.\n\n    Parameters:\n    c (int): The column index.\n    m (tuple): The size of the matrix.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_column_density(2, (ROWS, COLS))\n    \"\"\"\n    matrix = np.random.randn(m[0], m[1])\n    column = matrix[:, c]\n\n    density = stats.kde.gaussian_kde(column)\n\n    x = np.linspace(np.min(column), np.max(column), 100)\n    plt.plot(x, density(x))\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, scipy.spatial, sklearn.metrics\n```python\nimport numpy as np\nfrom scipy.spatial import ConvexHull\nfrom sklearn.metrics import pairwise_distances_argmin_min\n\n# Constants\nROWS = 100\nCOLS = 2\n\ndef find_farthest_points(c, m):\n    \"\"\"\n    Generate a random matrix of given size, select a column, construct a convex hull \n    and find the farthest point from the centroid of the convex hull.\n\n    Parameters:\n    c (int): The column index.\n    m (tuple): The size of the matrix.\n\n    Returns:\n    tuple: The index and distance of the farthest point.\n\n    Requirements:\n    - numpy\n    - scipy.spatial\n    - sklearn.metrics\n\n    Example:\n    >>> find_farthest_points(0, (ROWS, COLS))\n    \"\"\"\n    matrix = np.random.randint(0, 100, size=m)\n    points = matrix[:, c].reshape(-1, 1)\n\n    hull = ConvexHull(points)\n    centroid = np.mean(points[hull.vertices], axis=0)\n\n    farthest_index, distance = pairwise_distances_argmin_min(centroid.reshape(1, -1), points)\n\n    return farthest_index[0], distance[0]\n```"}
{"seed": "{\"task_id\": 206450, \"prompt\": \"def f_206450(L, n):\\n\\t\", \"suffix\": \"\\n\\treturn L\", \"canonical_solution\": \"L.remove(n)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    L = [i for i in range(8)]\\n    assert candidate(L, 3) == [0,1,2,4,5,6,7]\\n\", \"\\n    L = [0,1,2,4,5,6,7]\\n    assert candidate(L, 5) == [0,1,2,4,6,7]\\n\", \"\\n    L = [0,1,2,4,6,7]\\n    assert candidate(L, 0) == [1,2,4,6,7]\\n\"], \"entry_point\": \"f_206450\", \"intent\": \"\\u00bfC\\u00f3mo crear una funci\\u00f3n para eliminar un elemento `n` de una lista `L`?\", \"library\": []}", "generation": "Scenario 1:\npandas, numpy, random\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import randint\n\n# Constants\nVALUES_RANGE = (1, 100)\n\ndef remove_value_from_dataframe(df, val):\n    \"\"\"\n    Remove rows from a pandas DataFrame where a given value exists.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n    val (int): The value to be removed.\n\n    Returns:\n    DataFrame: The updated pandas DataFrame.\n\n    Requirements:\n    - pandas\n    - numpy\n    - random\n\n    Example:\n    >>> df = pd.DataFrame(np.random.randint(1, 100, size=(10, 10)))\n    >>> df = remove_value_from_dataframe(df, 50)\n    \"\"\"\n    return df[~df.apply(lambda row: val in row.values, axis=1)]\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, random, matplotlib.pyplot\n```python\nimport numpy as np\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nVALUES_RANGE = (1, 100)\n\ndef remove_and_plot_values(values, remove_val):\n    \"\"\"\n    Remove a value from a numpy array and plot the remaining values.\n\n    Parameters:\n    values (numpy array): The numpy array of values.\n    remove_val (int): The value to be removed.\n\n    Returns:\n    numpy array: The updated numpy array.\n\n    Requirements:\n    - numpy\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> values = np.random.randint(1, 100, size=100)\n    >>> values = remove_and_plot_values(values, 50)\n    >>> plt.hist(values)\n    \"\"\"\n    values = values[values != remove_val]\n    return values\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, random, pandas\n```python\nimport numpy as np\nfrom random import randint\nimport pandas as pd\n\n# Constants\nVALUES_RANGE = (1, 100)\n\ndef remove_value_and_get_stats(values, remove_val):\n    \"\"\"\n    Remove a value from a numpy array and get statistical info.\n\n    Parameters:\n    values (numpy array): The numpy array of values.\n    remove_val (int): The value to be removed.\n\n    Returns:\n    Series: A pandas Series with statistical info.\n\n    Requirements:\n    - numpy\n    - random\n    - pandas\n\n    Example:\n    >>> values = np.random.randint(1, 100, size=100)\n    >>> stats = remove_value_and_get_stats(values, 50)\n    >>> print(stats)\n    \"\"\"\n    values = values[values != remove_val]\n    return pd.Series(values).describe()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, random, matplotlib.pyplot, pandas\n```python\nimport numpy as np\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants\nVALUES_RANGE = (1, 100)\n\ndef remove_value_and_analyze(values, remove_val):\n    \"\"\"\n    Remove a value from a numpy array, get statistics and plot the remaining values.\n\n    Parameters:\n    values (numpy array): The numpy array of values.\n    remove_val (int): The value to be removed.\n\n    Returns:\n    Series: A pandas Series with statistical info.\n\n    Requirements:\n    - numpy\n    - random\n    - matplotlib.pyplot\n    - pandas\n\n    Example:\n    >>> values = np.random.randint(1, 100, size=100)\n    >>> stats = remove_value_and_analyze(values, 50)\n    >>> print(stats)\n    >>> plt.hist(values)\n    \"\"\"\n    values = values[values != remove_val]\n    plt.hist(values)\n    return pd.Series(values).describe()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, random, pandas, matplotlib.pyplot\n```python\nimport numpy as np\nfrom random import randint\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nVALUES_RANGE = (1, 100)\n\ndef remove_value_and_visualize(df, remove_val):\n    \"\"\"\n    Remove rows from a pandas DataFrame where a given value exists and plot the remaining values.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n    remove_val (int): The value to be removed.\n\n    Returns:\n    DataFrame: The updated pandas DataFrame.\n\n    Requirements:\n    - numpy\n    - random\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame(np.random.randint(1, 100, size=(10, 10)))\n    >>> df = remove_value_and_visualize(df, 50)\n    >>> df.hist()\n    \"\"\"\n    df = df[~df.apply(lambda row: remove_val in row.values, axis=1)]\n    df.hist()\n    return df\n```\n"}
{"seed": "{\"task_id\": 148791, \"prompt\": \"def f_148791(L):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"sum(L)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([1,2,3,4,5]) == 15\\n\", \"\\n    assert candidate([0, 1, -1, 2, 2]) == 4\\n\", \"\\n    assert candidate([3.4, 5.8, 6.6, 19.2]) == 35\\n\"], \"entry_point\": \"f_148791\", \"intent\": \"\\u00bfC\\u00f3mo realizar una sumatoria de una lista `L` sin utilizar ciclos?\", \"library\": []}", "generation": "Scenario 3:\nnumpy, json, math\n```python\nimport numpy as np\nimport json\nimport math\n\n# Constants\nJSON_FILE = 'data.json'\n\ndef json_summation(json_file=JSON_FILE):\n    \"\"\"\n    Read a JSON file where each object has a key 'data' with an array of numbers.\n    Perform the summation of all numbers in the 'data' arrays and return the result.\n    \n    Parameters:\n    json_file (str): The path to the JSON file. Default is 'data.json'.\n\n    Returns:\n    float: The sum of all numbers in the 'data' arrays.\n\n    Requirements:\n    - numpy\n    - json\n    - math\n\n    Example:\n    Given a JSON file 'data.json' with the following content:\n    [\n        {\"data\": [1, 2, 3, 4, 5]},\n        {\"data\": [0, 1, -1, 2, 2]},\n        {\"data\": [3.4, 5.8, 6.6, 19.2]}\n    ]\n\n    >>> json_summation('data.json')\n    54.0\n    \"\"\"\n    with open(json_file, 'r') as file:\n        data = json.load(file)\n    \n    sum = 0\n    for obj in data:\n        sum += np.sum(obj['data'])\n    return round(sum, 2)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, pandas, matplotlib.pyplot\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef df_sum_and_plot(data):\n    \"\"\"\n    Given a pandas DataFrame, calculate the sum of each column and plot the sums.\n    \n    Parameters:\n    data (DataFrame): The pandas DataFrame.\n\n    Returns:\n    Series: A pandas Series with the sum of each column.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> data = pd.DataFrame(np.random.rand(10, 5), columns=COLUMNS)\n    >>> df_sum_and_plot(data)\n    \"\"\"\n    column_sums = data.sum()\n    column_sums.plot(kind='bar')\n    plt.show()\n    return column_sums\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, itertools.product, math\n```python\nimport numpy as np\nfrom itertools import product\nimport math\n\ndef sum_of_products(L):\n    \"\"\"\n    Given a list of lists `L`, calculate the sum of the products of all possible \n    combinations of elements taken one from each list.\n\n    Parameters:\n    L (list): The list of lists.\n\n    Returns:\n    float: The sum of the products.\n\n    Requirements:\n    - numpy\n    - itertools.product\n    - math\n\n    Example:\n    >>> L = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n    >>> sum_of_products(L)\n    \"\"\"\n    product_sum = sum(np.prod(combination) for combination in product(*L))\n    return round(product_sum, 2)\n```\n`GPT_ODEX_BREAK`\n\nScenario 6:\nnumpy, scipy.stats, matplotlib.pyplot\n```python\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\n# Constants\nMU = 0\nSTD_DEV = 1\n\ndef plot_normal_distribution(mu=MU, std_dev=STD_DEV):\n    \"\"\"\n    Plot the normal distribution with mean `mu` and standard deviation `std_dev`.\n\n    Parameters:\n    mu (float): The mean of the normal distribution. Default is 0.\n    std_dev (float): The standard deviation of the normal distribution. Default is 1.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_normal_distribution()\n    \"\"\"\n    x = np.linspace(mu - 3*std_dev, mu + 3*std_dev, 100)\n    plt.plot(x, norm.pdf(x, mu, std_dev))\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 7:\nnumpy, sklearn.linear_model, pandas\n```python\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nimport pandas as pd\n\n# Constants\nTARGET_COLUMN = 'target'\n\ndef fit_linear_model(data, target_column=TARGET_COLUMN):\n    \"\"\"\n    Fit a linear regression model on a given pandas DataFrame, with a specified target column.\n\n    Parameters:\n    data (DataFrame): The pandas DataFrame.\n    target_column (str): The column in the DataFrame to use as the target variable.\n\n    Returns:\n    LinearRegression: The fitted linear regression model.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.linear_model\n\n    Example:\n    >>> data = pd.DataFrame(np.random.rand(100, 5), columns=list('ABCDE'))\n    >>> data['target'] = np.random.rand(100)\n    >>> model = fit_linear_model(data)\n    \"\"\"\n    X = data.drop(columns=[target_column])\n    y = data[target_column]\n\n    model = LinearRegression()\n    model.fit(X, y)\n\n    return model\n```"}
{"seed": "{\"task_id\": 8239, \"prompt\": \"def f_8239(L, M, N):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"np.array(L).reshape(M,N)\", \"test_start\": \"\\nimport numpy as np \\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([1,2,3,4],2,2).tolist() == [[1,2],[3,4]]\\n\", \"\\n    assert np.all(candidate([i for i in range(100)], 10, 10) == np.array([[i*10+j for j in range(10)] for i in range(10)]))\\n\"], \"entry_point\": \"f_8239\", \"intent\": \"\\u00bfC\\u00f3mo generar una matr\\u00edz de `M`x`N` con una lista `L`?\", \"library\": [\"numpy\"]}", "generation": "Scenario 1:\nnumpy, pandas, matplotlib, scipy.stats\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n# Constants\nMEAN = 0\nSTD_DEV = 1\n\ndef generate_normal_distribution(L, M, N):\n    \"\"\"\n    Generate a MxN matrix from a list L, calculate the normal distribution with \n    the generated matrix, and plot the histogram with the normal distribution curve.\n    \n    Parameters:\n    L (list): The list of numbers.\n    M (int): The number of rows.\n    N (int): The number of columns.\n    \n    Returns:\n    numpy.ndarray: The generated matrix.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - scipy.stats\n    \n    Example:\n    >>> generate_normal_distribution([i for i in range(100)], 10, 10)\n    >>> plt.show()\n    \"\"\"\n    matrix = np.array(L).reshape(M,N)\n\n    # Calculate the normal distribution\n    mu, std = stats.norm.fit(matrix.flatten())\n    p = stats.norm.pdf(matrix.flatten(), mu, std)\n\n    # Plot the histogram\n    plt.hist(matrix.flatten(), bins=30, density=True, alpha=0.6, color='g')\n    \n    # Plot the PDF\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, mu, std)\n    plt.plot(x, p, 'k', linewidth=2)\n    plt.title(\"Fit results: mu = %.2f,  std = %.2f\" % (mu, std))\n\n    return matrix\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, os, soundfile, librosa\n```python\nimport numpy as np\nimport os\nimport soundfile as sf\nimport librosa\n\ndef generate_spectrogram(L, M, N, audio_file):\n    \"\"\"\n    Generate a MxN matrix from a list L, normalize the matrix to match the \n    sound pressure level (SPL) of an audio file and generate a spectrogram.\n    \n    Parameters:\n    L (list): The list of numbers.\n    M (int): The number of rows.\n    N (int): The number of columns.\n    audio_file (str): The path to the audio file.\n    \n    Returns:\n    numpy.ndarray: The generated matrix.\n    \n    Requirements:\n    - numpy\n    - os\n    - soundfile\n    - librosa\n    \n    Example:\n    >>> generate_spectrogram([i for i in range(100)], 10, 10, 'audio.wav')\n    \"\"\"\n    # Ensure the audio file exists\n    if os.path.isfile(audio_file):\n        # Read the audio file\n        data, samplerate = sf.read(audio_file)\n        # Calculate the sound pressure level (SPL)\n        spl = 20 * np.log10(np.sqrt(np.mean(data**2)))\n        \n        # Generate the matrix\n        matrix = np.array(L).reshape(M,N)\n        \n        # Normalize the matrix to match the SPL\n        matrix = matrix / np.max(matrix) * spl\n        \n        # Generate the spectrogram\n        D = librosa.amplitude_to_db(np.abs(librosa.stft(matrix)), ref=np.max)\n        librosa.display.specshow(D, sr=samplerate, x_axis='time', y_axis='log')\n        plt.colorbar(format='%+2.0f dB')\n        plt.title('Spectrogram')\n\n        return matrix\n    else:\n        raise FileNotFoundError(f\"{audio_file} does not exist.\")\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, pandas, sklearn.preprocessing, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\n\ndef generate_normalized_data(L, M, N):\n    \"\"\"\n    Generate a MxN matrix from a list L, normalize the data with MinMaxScaler and \n    plot the original and normalized data.\n    \n    Parameters:\n    L (list): The list of numbers.\n    M (int): The number of rows.\n    N (int): The number of columns.\n    \n    Returns:\n    numpy.ndarray: The normalized matrix.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.preprocessing\n    - matplotlib.pyplot\n    \n    Example:\n    >>> generate_normalized_data([i for i in range(100)], 10, 10)\n    \"\"\"\n    # Generate the matrix\n    matrix = np.array(L).reshape(M,N)\n\n    # Normalize the data\n    scaler = MinMaxScaler()\n    scaled_matrix = scaler.fit_transform(matrix)\n\n    # Plot the original and normalized data\n    plt.figure(figsize=(12, 6))\n    plt.subplot(1, 2, 1)\n    plt.imshow(matrix, cmap='hot', interpolation='nearest')\n    plt.title('Original Data')\n    plt.subplot(1, 2, 2)\n    plt.imshow(scaled_matrix, cmap='hot', interpolation='nearest')\n    plt.title('Normalized Data')\n\n    return scaled_matrix\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, scipy.spatial, matplotlib\n```python\nimport numpy as np\nfrom scipy.spatial import distance_matrix\nimport matplotlib.pyplot as plt\n\ndef generate_distance_matrix(L, M, N):\n    \"\"\"\n    Generate a MxN matrix from a list L and calculate the distance matrix \n    using Euclidean distance.\n    \n    Parameters:\n    L (list): The list of numbers.\n    M (int): The number of rows.\n    N (int): The number of columns.\n    \n    Returns:\n    numpy.ndarray: The distance matrix.\n    \n    Requirements:\n    - numpy\n    - scipy.spatial\n    - matplotlib.pyplot\n    \n    Example:\n    >>> generate_distance_matrix([i for i in range(100)], 10, 10)\n    \"\"\"\n    # Generate the matrix\n    matrix = np.array(L).reshape(M,N)\n    \n    # Calculate the distance matrix\n    dist_matrix = distance_matrix(matrix, matrix)\n\n    # Plot the distance matrix\n    plt.imshow(dist_matrix, cmap='hot', interpolation='nearest')\n    plt.title('Distance Matrix')\n\n    return dist_matrix\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, matplotlib, sklearn.decomposition\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\ndef generate_pca_components(L, M, N):\n    \"\"\"\n    Generate a MxN matrix from a list L, perform PCA (Principal Component Analysis) \n    and plot the first two principal components.\n    \n    Parameters:\n    L (list): The list of numbers.\n    M (int): The number of rows.\n    N (int): The number of columns.\n    \n    Returns:\n    numpy.ndarray: The transformed matrix with the first two principal components.\n    \n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.decomposition\n    \n    Example:\n    >>> generate_pca_components([i for i in range(100)], 10, 10)\n    \"\"\"\n    # Generate the matrix\n    matrix = np.array(L).reshape(M,N)\n    \n    # Perform PCA\n    pca = PCA(n_components=2)\n    transformed_matrix = pca.fit_transform(matrix)\n\n    # Plot the first two principal components\n    plt.scatter(transformed_matrix[:, 0], transformed_matrix[:, 1])\n    plt.title('First Two Principal Components')\n\n    return transformed_matrix\n```"}
{"seed": "{\"task_id\": 83439, \"prompt\": \"def f_83439(L):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"[i for i,x in enumerate(L) if L.count(x) > 1]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([1,1,2,2,3,3]) == [0,1,2,3,4,5]\\n\", \"\\n    assert candidate([1,2,3,4,5,6]) == []\\n\", \"\\n    assert candidate([100, 101, 102, 100]) == [0, 3]\\n\"], \"entry_point\": \"f_83439\", \"intent\": \"\\u00bfC\\u00f3mo mostrar la posici\\u00f3n de los elementos repetidos en una lista `L`?\", \"library\": []}", "generation": "Scenario 1: \ncollections, itertools, random\n```python\nimport collections\nimport itertools\nfrom random import randint\n\n# Constants\nLETTERS = 'abcdefghijklmnopqrstuvwxyz'\n\ndef generate_random_strings(n, length):\n    \"\"\"\n    Generate a list of n random strings of a given length and return a list of \n    indices of strings that are repeated in the list.\n    \n    Parameters:\n    n (int): The number of strings to generate.\n    length (int): The length of each string.\n    \n    Returns:\n    list: A list of indices of strings that are repeated in the list.\n    \n    Requirements:\n    - collections\n    - itertools\n    - random\n    \n    Example:\n    >>> generate_random_strings(10, 5)\n    \"\"\"\n    random_strings = [''.join([LETTERS[randint(0, len(LETTERS)-1)] for _ in range(length)]) for _ in range(n)]\n    counter = collections.Counter(random_strings)\n    duplicates = [item for item, count in counter.items() if count > 1]\n    duplicate_indices = [i for i, string in enumerate(random_strings) if string in duplicates]\n\n    return duplicate_indices\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, pandas, matplotlib.pyplot\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef generate_dataframe_and_plot(n_rows):\n    \"\"\"\n    Generate a DataFrame with n rows and 5 columns filled with random numbers in \n    the range [0, 1). Then, plot the distribution of values in each column.\n    \n    Parameters:\n    n_rows (int): The number of rows in the DataFrame.\n    \n    Returns:\n    DataFrame: The generated DataFrame.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = generate_dataframe_and_plot(100)\n    >>> plt.show()\n    \"\"\"\n    df = pd.DataFrame(np.random.rand(n_rows, len(COLUMNS)), columns=COLUMNS)\n    df.plot(kind='density')\n    \n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnumpy, matplotlib.pyplot, sklearn.decomposition, sklearn.datasets\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.datasets import load_digits\n\ndef pca_on_digits():\n    \"\"\"\n    Load the digits dataset, perform PCA on it to reduce the dimensionality to 2, \n    and plot the result.\n    \n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.decomposition\n    - sklearn.datasets\n    \n    Example:\n    >>> pca_on_digits()\n    >>> plt.show()\n    \"\"\"\n    digits = load_digits()\n    pca = PCA(2)  # project from 64 to 2 dimensions\n    projected = pca.fit_transform(digits.data)\n    plt.scatter(projected[:, 0], projected[:, 1], c=digits.target, edgecolor='none', alpha=0.5,\n                cmap=plt.cm.get_cmap('tab10', 10))\n    plt.xlabel('component 1')\n    plt.ylabel('component 2')\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \npandas, sklearn.model_selection, sklearn.linear_model, sklearn.metrics\n```python\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n# Constants\nDATASET_URL = 'https://raw.githubusercontent.com/justmarkham/DAT8/master/data/bikeshare.csv'\n\ndef linear_regression_on_bikeshare():\n    \"\"\"\n    Load the Bikeshare dataset, split it into a training set and a testing set, train a linear regression \n    model on the training set, make predictions on the testing set, and return the mean squared error of the \n    predictions.\n    \n    Returns:\n    float: The mean squared error of the predictions.\n    \n    Requirements:\n    - pandas\n    - sklearn.model_selection\n    - sklearn.linear_model\n    - sklearn.metrics\n    \n    Example:\n    >>> linear_regression_on_bikeshare()\n    \"\"\"\n    df = pd.read_csv(DATASET_URL, index_col='datetime', parse_dates=True)\n    X = df.drop('count', axis=1)\n    y = df['count']\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    predictions = model.predict(X_test)\n\n    mse = mean_squared_error(y_test, predictions)\n\n    return mse\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnumpy, matplotlib.pyplot, scipy.stats\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\ndef plot_normal_distribution(mu, sigma):\n    \"\"\"\n    Plot a normal distribution with a given mean and standard deviation.\n    \n    Parameters:\n    mu (float): The mean of the distribution.\n    sigma (float): The standard deviation of the distribution.\n    \n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - scipy.stats\n    \n    Example:\n    >>> plot_normal_distribution(0, 1)\n    >>> plt.show()\n    \"\"\"\n    x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n    plt.plot(x, norm.pdf(x, mu, sigma))\n```"}
{"seed": "{\"task_id\": 188925, \"prompt\": \"def f_188925(S):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"Counter(S)\", \"test_start\": \"\\nfrom collections import Counter\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('apple') == Counter({'a': 1, 'e': 1, 'l': 1, 'p': 2})\\n\", \"\\n    assert candidate('') == Counter({})\\n\"], \"entry_point\": \"f_188925\", \"intent\": \"\\u00bfC\\u00f3mo contar cu\\u00e1ntas veces se repite un car\\u00e1cter en una cadena `S`?\", \"library\": [\"collections\"]}", "generation": "Scenario 1: \ncollections, os, glob, re\n```python\nfrom collections import Counter\nimport os\nimport glob\nimport re\n\n# Constants\nFILE_TYPES = ['.txt', '.md', '.csv']\n\ndef count_words_in_files(directory):\n    \"\"\"\n    Count the occurrence of each word across all files in a given directory.\n\n    Parameters:\n    directory (str): The path to the directory.\n\n    Returns:\n    Counter: A Counter object with word frequencies.\n\n    Requirements:\n    - collections\n    - os\n    - glob\n    - re\n\n    Example:\n    >>> count_words_in_files('./documents/')\n    \"\"\"\n    all_files = []\n    for file_type in FILE_TYPES:\n        all_files.extend(glob.glob(f'{directory}/*{file_type}'))\n    \n    word_counter = Counter()\n    for file in all_files:\n        with open(file, 'r') as f:\n            words = re.findall(r'\\b\\w+\\b', f.read())\n            word_counter.update(words)\n    \n    return word_counter\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \ncollections, matplotlib, pandas, numpy\n```python\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\n# Constants\nLETTER_VALUES = {'A': 1, 'B': 3, 'C': 3, 'D': 2, 'E': 1, 'F': 4, 'G': 2, 'H': 4, \n                 'I': 1, 'J': 8, 'K': 5, 'L': 1, 'M': 3, 'N': 1, 'O': 1, 'P': 3, \n                 'Q': 10, 'R': 1, 'S': 1, 'T': 1, 'U': 1, 'V': 4, 'W': 4, 'X': 8, \n                 'Y': 4, 'Z': 10}\n\ndef scrabble_score_distribution(words):\n    \"\"\"\n    Display the distribution of Scrabble scores for a list of words.\n\n    Parameters:\n    words (list): A list of words.\n\n    Returns:\n    None\n\n    Requirements:\n    - collections\n    - matplotlib\n    - pandas\n    - numpy\n\n    Example:\n    >>> scrabble_score_distribution(['apple', 'orange', 'banana', 'kiwi'])\n    \"\"\"\n    scores = []\n    for word in words:\n        scores.append(sum(LETTER_VALUES.get(char.upper(), 0) for char in word))\n    \n    score_counter = Counter(scores)\n    score_df = pd.DataFrame.from_dict(score_counter, orient='index').reset_index()\n    score_df.columns = ['Score', 'Frequency']\n    \n    plt.figure(figsize=(10, 6))\n    plt.bar(score_df['Score'], score_df['Frequency'], color='navy')\n    plt.xlabel('Score')\n    plt.ylabel('Frequency')\n    plt.title('Scrabble Score Distribution')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \ncollections, json, requests, itertools\n```python\nfrom collections import Counter\nimport json\nimport requests\nfrom itertools import chain\n\n# Constants\nAPI_URL = \"https://api.github.com/users/\"\n\ndef count_languages_for_user(username):\n    \"\"\"\n    Count the occurrence of each programming language used by a GitHub user across all their repositories.\n\n    Parameters:\n    username (str): The GitHub username.\n\n    Returns:\n    Counter: A Counter object with language frequencies.\n\n    Requirements:\n    - collections\n    - json\n    - requests\n    - itertools\n\n    Example:\n    >>> count_languages_for_user('octocat')\n    \"\"\"\n    repos = requests.get(API_URL + username + \"/repos\")\n    repos_json = json.loads(repos.text)\n    \n    languages = list(chain.from_iterable([repo['language'] for repo in repos_json if repo['language']]))\n    \n    return Counter(languages)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \ncollections, nltk, string\n```python\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nimport string\n\n# Constants\nSTOPWORDS = set(stopwords.words('english'))\n\ndef count_non_stopwords(text):\n    \"\"\"\n    Count the occurrence of each non-stopword in a given text.\n\n    Parameters:\n    text (str): The text string.\n\n    Returns:\n    Counter: A Counter object with non-stopword frequencies.\n\n    Requirements:\n    - collections\n    - nltk.corpus\n    - string\n\n    Example:\n    >>> count_non_stopwords(\"This is a simple test. This test is simple.\")\n    \"\"\"\n    text = text.translate(str.maketrans('', '', string.punctuation)).lower()\n    words = [word for word in text.split() if word not in STOPWORDS]\n    \n    return Counter(words)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\ncollections, os, sys\n```python\nfrom collections import Counter\nimport os\nimport sys\n\n# Constants\nBUFFER_SIZE = 65536\n\ndef count_byte_frequency(file_path):\n    \"\"\"\n    Count the frequency of each byte in a file.\n\n    Parameters:\n    file_path (str): The path to the file.\n\n    Returns:\n    Counter: A Counter object with byte frequencies.\n\n    Requirements:\n    - collections\n    - os\n    - sys\n\n    Example:\n    >>> count_byte_frequency('./file.bin')\n    \"\"\"\n    if not os.path.isfile(file_path):\n        print(f'{file_path} does not exist.')\n        sys.exit()\n\n    byte_counter = Counter()\n    with open(file_path, 'rb') as f:\n        while True:\n            chunk = f.read(BUFFER_SIZE)\n            if not chunk:\n                break\n            byte_counter.update(chunk)\n    \n    return byte_counter\n```"}
{"seed": "{\"task_id\": 134908, \"prompt\": \"def f_134908(L, n):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"[L[i:i + n] for i in range(0, len(L), n)]\", \"test_start\": \"\\nimport os\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([1,2,3,4,5,6],3) == [[1,2,3],[4,5,6]]\\n\", \"\\n    assert candidate([1,2,3,4],1) == [[1],[2],[3],[4]]\\n\"], \"entry_point\": \"f_134908\", \"intent\": \"\\u00bfC\\u00f3mo dividir una lista `L` en una lista de listas cada `n` datos?\", \"library\": [\"os\"]}", "generation": "Scenario 1:\nnumpy, pandas, sklearn, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\n# Constants\nN_FEATURES = 10\n\ndef generate_and_scale_dataset(L, n):\n    \"\"\"\n    Generate a dataset with `n` samples from a list `L` and scale the features to have zero mean \n    and unit variance.\n\n    Parameters:\n    L (list): The list of numbers.\n    n (int): The number of samples.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the scaled dataset.\n\n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.preprocessing.StandardScaler\n    - matplotlib.pyplot\n\n    Example:\n    >>> L = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n    >>> generate_and_scale_dataset(L, 5)\n    \"\"\"\n    data = np.array([L[i:i + N_FEATURES] for i in range(0, len(L), N_FEATURES)]).reshape(n, N_FEATURES)\n\n    scaler = StandardScaler()\n    scaled_data = scaler.fit_transform(data)\n\n    df = pd.DataFrame(scaled_data, columns=[f'feature_{i+1}' for i in range(N_FEATURES)])\n\n    df.plot(kind='box')\n\n    return df\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, numpy, matplotlib, PIL\n```python\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\n# Constants\nIMAGE_SIZE = 64\n\ndef batch_resize_images(L, n):\n    \"\"\"\n    Resize a batch of `n` images from a list `L` of image paths to a fixed size.\n\n    Parameters:\n    L (list): The list of image paths.\n    n (int): The number of images in a batch.\n\n    Returns:\n    list: A list of numpy arrays representing the resized images.\n\n    Requirements:\n    - os\n    - numpy\n    - matplotlib.pyplot\n    - PIL.Image\n\n    Example:\n    >>> L = ['/path/to/image1.jpg', '/path/to/image2.jpg', '/path/to/image3.jpg']\n    >>> batch_resize_images(L, 2)\n    \"\"\"\n    batches = [L[i:i + n] for i in range(0, len(L), n)]\n    resized_images = []\n\n    for batch in batches:\n        for img_path in batch:\n            img = Image.open(img_path)\n            img = img.resize((IMAGE_SIZE, IMAGE_SIZE))\n            img_array = np.array(img)\n            resized_images.append(img_array)\n\n            plt.imshow(img_array)\n            plt.show()\n\n    return resized_images\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 3:\nstring, re, nltk, collections\n```python\nimport string\nimport re\nfrom nltk.tokenize import word_tokenize\nfrom collections import Counter\n\n# Constants\nN_WORDS = 100\n\ndef count_words_in_text(L, n):\n    \"\"\"\n    Count the occurrence of words in a list `L` of `n` strings.\n\n    Parameters:\n    L (list): The list of strings.\n    n (int): The number of strings in a batch.\n\n    Returns:\n    Counter: A Counter object with word counts.\n\n    Requirements:\n    - string\n    - re\n    - nltk.tokenize.word_tokenize\n    - collections.Counter\n\n    Example:\n    >>> L = ['This is a test.', 'This is another test.']\n    >>> count_words_in_text(L, 2)\n    \"\"\"\n    batches = [L[i:i + n] for i in range(0, len(L), n)]\n    word_counts = Counter()\n\n    for batch in batches:\n        for text in batch:\n            text = text.lower()\n            text = re.sub('['+string.punctuation+']', '', text)\n            words = word_tokenize(text)\n            word_counts.update(words)\n\n    return word_counts.most_common(N_WORDS)\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, subprocess, shlex\n```python\nimport os\nimport subprocess\nimport shlex\n\n# Constants\nCOMMAND = 'ls -l'\n\ndef run_command_in_dirs(L, n):\n    \"\"\"\n    Run a shell command in a list `L` of `n` directories.\n\n    Parameters:\n    L (list): The list of directories.\n    n (int): The number of directories in a batch.\n\n    Returns:\n    dict: A dictionary with directory names as keys and command outputs as values.\n\n    Requirements:\n    - os\n    - subprocess\n    - shlex\n\n    Example:\n    >>> L = ['/path/to/dir1', '/path/to/dir2']\n    >>> run_command_in_dirs(L, 2)\n    \"\"\"\n    batches = [L[i:i + n] for i in range(0, len(L), n)]\n    results = {}\n\n    for batch in batches:\n        for dir_path in batch:\n            command = shlex.split(COMMAND)\n            output = subprocess.check_output(command, cwd=dir_path)\n            results[dir_path] = output.decode('utf-8')\n\n    return results\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, shutil, zipfile\n```python\nimport os\nimport shutil\nfrom zipfile import ZipFile\n\n# Constants\nDEST_DIR = '/path/to/dest'\n\ndef batch_unzip_files(L, n):\n    \"\"\"\n    Unzip a batch of `n` zip files from a list `L` of zip file paths to a destination directory.\n\n    Parameters:\n    L (list): The list of zip file paths.\n    n (int): The number of zip files in a batch.\n\n    Returns:\n    list: A list of unzipped directory paths.\n\n    Requirements:\n    - os\n    - shutil\n    - zipfile.ZipFile\n\n    Example:\n    >>> L = ['/path/to/file1.zip', '/path/to/file2.zip']\n    >>> batch_unzip_files(L, 2)\n    \"\"\"\n    batches = [L[i:i + n] for i in range(0, len(L), n)]\n    unzipped_dirs = []\n\n    for batch in batches:\n        for zip_file in batch:\n            with ZipFile(zip_file, 'r') as zip_ref:\n                zip_ref.extractall(DEST_DIR)\n                unzipped_dirs.append(os.path.join(DEST_DIR, os.path.splitext(os.path.basename(zip_file))[0]))\n\n    return unzipped_dirs\n```\n"}
{"seed": "{\"task_id\": 52839, \"prompt\": \"def f_52839(S):\\n\\t\", \"suffix\": \"\\n\\treturn M, m, n\", \"canonical_solution\": \"M = len([c for c in S if c.isupper()])\\n\\tm = len([c for c in S if c.islower()])\\n\\tn = len([c for c in S if c.isdigit()])\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('ceg242AABd  ') == (3,4,3)\\n\", \"\\n    assert candidate('12233457') == (0,0,8)\\n\"], \"entry_point\": \"f_52839\", \"intent\": \"\\u00bfC\\u00f3mo saber cu\\u00e1ntas may\\u00fasculas `M`, cu\\u00e1ntas min\\u00fasculas `m` y cu\\u00e1ntos n\\u00fameros `n` tiene un string `S`?\", \"library\": []}", "generation": "Scenario 1:\npandas, re, nltk, sklearn\n```python\nimport pandas as pd\nimport re\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Constants\nSTOPWORDS = set(stopwords.words('english'))\n\ndef analyze_text_data(text_data):\n    \"\"\"\n    Analyze the text data and return the count of uppercase letters, lowercase letters,\n    digits, words, and the word frequency distribution.\n\n    Parameters:\n    text_data (str): The text data to analyze.\n\n    Returns:\n    dict: A dictionary with counts of uppercase letters, lowercase letters, digits, words, and word frequency distribution.\n\n    Requirements:\n    - pandas\n    - re\n    - nltk.corpus.stopwords\n    - sklearn.feature_extraction.text.CountVectorizer\n\n    Example:\n    >>> analyze_text_data('Hello World! 123')\n    \"\"\"\n    # Count uppercase, lowercase and digits\n    uppercase_count = len([c for c in text_data if c.isupper()])\n    lowercase_count = len([c for c in text_data if c.islower()])\n    digit_count = len([c for c in text_data if c.isdigit()])\n\n    # Count words\n    words = re.sub(r'\\W', ' ', text_data).split()\n    word_count = len(words)\n\n    # Word frequency distribution\n    vectorizer = CountVectorizer(stop_words=STOPWORDS)\n    word_frequency = vectorizer.fit_transform(words).toarray().sum(axis=0)\n    word_frequency_dict = dict(zip(vectorizer.get_feature_names(), word_frequency))\n\n    return {\n        'uppercase_count': uppercase_count,\n        'lowercase_count': lowercase_count,\n        'digit_count': digit_count,\n        'word_count': word_count,\n        'word_frequency_distribution': word_frequency_dict\n    }\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncollections, re, string, matplotlib\n```python\nimport collections\nimport re\nimport string\nimport matplotlib.pyplot as plt\n\ndef analyze_string(S):\n    \"\"\"\n    Count the number of uppercase, lowercase, digits, and punctuation in a string.\n    Create a pie chart of the counts.\n\n    Parameters:\n    S (str): The string to analyze.\n\n    Returns:\n    dict: A dictionary with counts of uppercase letters, lowercase letters, digits, and punctuation.\n\n    Requirements:\n    - collections\n    - re\n    - string\n    - matplotlib.pyplot\n\n    Example:\n    >>> analyze_string('Hello, World! 123')\n    \"\"\"\n    counts = collections.Counter(S)\n    uppercase_count = sum(counts[c] for c in string.ascii_uppercase)\n    lowercase_count = sum(counts[c] for c in string.ascii_lowercase)\n    digit_count = sum(counts[c] for c in string.digits)\n    punctuation_count = sum(counts[c] for c in string.punctuation)\n\n    results = {\n        'Uppercase': uppercase_count,\n        'Lowercase': lowercase_count,\n        'Digits': digit_count,\n        'Punctuation': punctuation_count\n    }\n\n    plt.pie(results.values(), labels=results.keys(), autopct='%1.1f%%')\n    plt.title('String Composition')\n    plt.show()\n\n    return results\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, seaborn, nltk\n```python\nimport pandas as pd\nimport seaborn as sns\nimport nltk\n\n# Constants\nSTOPWORDS = nltk.corpus.stopwords.words('english')\n\ndef analyze_text(S):\n    \"\"\"\n    Count the number of uppercase, lowercase, digits, and stopwords in a string.\n    Display the counts in a bar plot.\n\n    Parameters:\n    S (str): The string to analyze.\n\n    Returns:\n    dict: A dictionary with counts of uppercase letters, lowercase letters, digits, and stopwords.\n\n    Requirements:\n    - pandas\n    - seaborn\n    - nltk\n\n    Example:\n    >>> analyze_text('Hello, World! This is a test string. 123')\n    \"\"\"\n    uppercase_count = len([c for c in S if c.isupper()])\n    lowercase_count = len([c for c in S if c.islower()])\n    digit_count = len([c for c in S if c.isdigit()])\n    stopwords_count = len([word for word in S.split() if word.lower() in STOPWORDS])\n\n    results = {\n        'Uppercase': uppercase_count,\n        'Lowercase': lowercase_count,\n        'Digits': digit_count,\n        'Stopwords': stopwords_count\n    }\n\n    df = pd.DataFrame(list(results.items()), columns=['Type', 'Count'])\n    sns.barplot(x='Type', y='Count', data=df)\n\n    return results\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, nltk, matplotlib\n```python\nimport pandas as pd\nimport nltk\nimport matplotlib.pyplot as plt\n\n# Constants\nSTOPWORDS = nltk.corpus.stopwords.words('english')\n\ndef analyze_sentence(S):\n    \"\"\"\n    Count the number of uppercase, lowercase, digits, and stopwords in a string S.\n    Create a histogram of the word lengths in the string.\n\n    Parameters:\n    S (str): The string to analyze.\n\n    Returns:\n    dict: A dictionary with counts of uppercase letters, lowercase letters, digits, and stopwords.\n\n    Requirements:\n    - pandas\n    - nltk\n    - matplotlib.pyplot\n\n    Example:\n    >>> analyze_sentence('Hello, World! This is a test string. 123')\n    \"\"\"\n    uppercase_count = len([c for c in S if c.isupper()])\n    lowercase_count = len([c for c in S if c.islower()])\n    digit_count = len([c for c in S if c.isdigit()])\n    stopwords_count = len([word for word in S.split() if word.lower() in STOPWORDS])\n\n    results = {\n        'Uppercase': uppercase_count,\n        'Lowercase': lowercase_count,\n        'Digits': digit_count,\n        'Stopwords': stopwords_count\n    }\n\n    # Create a histogram of the word lengths\n    word_lengths = [len(word) for word in S.split() if word.lower() not in STOPWORDS]\n    plt.hist(word_lengths, bins=range(1, max(word_lengths)+1))\n    plt.title('Word Lengths')\n    plt.xlabel('Length')\n    plt.ylabel('Frequency')\n    plt.show()\n\n    return results\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\ncollections, string, matplotlib\n```python\nimport collections\nimport string\nimport matplotlib.pyplot as plt\n\ndef char_frequency_analysis(S):\n    \"\"\"\n    Count the frequency of each character in a string S.\n    Create a bar plot of the character frequencies.\n\n    Parameters:\n    S (str): The string to analyze.\n\n    Returns:\n    collections.Counter: A Counter object with character frequencies.\n\n    Requirements:\n    - collections\n    - string\n    - matplotlib.pyplot\n\n    Example:\n    >>> char_frequency_analysis('Hello, World! 123')\n    \"\"\"\n    counts = collections.Counter(S)\n\n    # Create a bar plot of the character frequencies\n    letters = list(string.ascii_letters + string.digits + string.punctuation + ' ')\n    frequencies = [counts[c] for c in letters]\n    plt.bar(letters, frequencies)\n    plt.title('Character Frequencies')\n    plt.xlabel('Character')\n    plt.ylabel('Frequency')\n    plt.show()\n\n    return counts\n```\n"}
{"seed": "{\"task_id\": 50423, \"prompt\": \"def f_50423(n):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"int(n, 2)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('1100') == 12\\n\", \"\\n    assert candidate('001010111') == 87\\n\", \"\\n    assert candidate('0') == 0\\n\"], \"entry_point\": \"f_50423\", \"intent\": \"\\u00bfC\\u00f3mo convertir un n\\u00famero binario `n` a entero?\", \"library\": []}", "generation": "Scenario 1:\nnumpy, pandas, random, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nimport random\nimport matplotlib.pyplot as plt\n\n# Constants\nELEMENTS = [\"Hydrogen\", \"Helium\", \"Lithium\", \"Beryllium\", \"Boron\"]\n\ndef create_periodic_table(elements):\n    \"\"\"\n    Create a random periodic table with binary values for each element's presence and \n    then return a dataframe where the binary values are converted to integers.\n    \n    Parameters:\n    elements (list): A list of element names.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with integer values for each element's presence.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> table = create_periodic_table(ELEMENTS)\n    >>> print(table)\n    >>> table.sum().plot(kind='bar')\n    \"\"\"\n    data = {element: [random.choice(['0', '1']) for _ in range(10)] for element in elements}\n\n    df = pd.DataFrame(data)\n    df = df.applymap(lambda n: int(n, 2))\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, pandas, itertools, random\n```python\nimport numpy as np\nimport pandas as pd\nfrom itertools import product\nimport random\n\ndef generate_combinations(elements, binary_to_int=True):\n    \"\"\"\n    Generate all possible combinations of given elements with binary values and \n    then convert these binary values to integers if binary_to_int is set to True.\n    \n    Parameters:\n    elements (list): A list of element names.\n    binary_to_int (bool): A flag to convert binary values to integers.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with all possible combinations.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - itertools\n    - random\n    \n    Example:\n    >>> combinations = generate_combinations(['A', 'B', 'C'])\n    >>> print(combinations)\n    \"\"\"\n    combinations = list(product(['0', '1'], repeat=len(elements)))\n    df = pd.DataFrame(combinations, columns=elements)\n    \n    if binary_to_int:\n        df = df.applymap(lambda n: int(n, 2))\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, pandas, itertools, random\n```python\nimport numpy as np\nimport pandas as pd\nfrom itertools import product\nimport random\n\n# Constants\nSEEDS = [\"Tomato\", \"Cucumber\", \"Carrot\", \"Pepper\", \"Corn\"]\n\ndef plant_seeds(seeds):\n    \"\"\"\n    Generate a planting plan for given seeds with binary values where 1 represents \n    the seed is planted and 0 represents it is not. Then convert these binary values \n    to integers.\n    \n    Parameters:\n    seeds (list): A list of seed names.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the planting plan.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - itertools\n    - random\n    \n    Example:\n    >>> planting_plan = plant_seeds(SEEDS)\n    >>> print(planting_plan)\n    \"\"\"\n    data = {seed: [random.choice(['0', '1']) for _ in range(10)] for seed in seeds}\n\n    df = pd.DataFrame(data)\n    df = df.applymap(lambda n: int(n, 2))\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, pandas, random, seaborn\n```python\nimport numpy as np\nimport pandas as pd\nimport random\nimport seaborn as sns\n\n# Constants\nCOUNTRIES = [\"USA\", \"China\", \"India\", \"Russia\", \"Brazil\"]\n\ndef generate_population_data(countries):\n    \"\"\"\n    Generate a random population data for given countries with binary values \n    and then convert these binary values to integers representing population in millions.\n    \n    Parameters:\n    countries (list): A list of country names.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with population data for each country.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - random\n    - seaborn\n    \n    Example:\n    >>> population_data = generate_population_data(COUNTRIES)\n    >>> print(population_data)\n    >>> sns.heatmap(population_data)\n    \"\"\"\n    data = {country: [random.choice(['0', '1']) for _ in range(10)] for country in countries}\n\n    df = pd.DataFrame(data)\n    df = df.applymap(lambda n: int(n, 2))\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, pandas, random, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nimport random\nimport matplotlib.pyplot as plt\n\n# Constants\nPRODUCTS = [\"Apple\", \"Banana\", \"Cherry\", \"Date\", \"Elderberry\"]\n\ndef generate_sales_data(products):\n    \"\"\"\n    Generate a random sales data for given products with binary values \n    and then convert these binary values to integers representing sales in thousands.\n    \n    Parameters:\n    products (list): A list of product names.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with sales data for each product.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> sales_data = generate_sales_data(PRODUCTS)\n    >>> print(sales_data)\n    >>> sales_data.sum().plot(kind='bar')\n    \"\"\"\n    data = {product: [random.choice(['0', '1']) for _ in range(10)] for product in products}\n\n    df = pd.DataFrame(data)\n    df = df.applymap(lambda n: int(n, 2)*1000)\n\n    return df\n```\nAbove is the illustration."}
{"seed": "{\"task_id\": 39861, \"prompt\": \"def f_39861(S):\\n\\t\", \"suffix\": \"\\n\\treturn L1, L2, L3, L4\", \"canonical_solution\": \"L1, L2, L3, L4 = random.sample(S, 4)\", \"test_start\": \"\\nimport random \\n\\ndef check(candidate):\", \"test\": [\"\\n    L1, L2, L3, L4 = candidate([1,2,3,4,5,6,7,8])\\n    s = set([L1,L2,L3,L4])\\n    assert len(s) == 4\\n    assert s < set([i+1 for i in range(8)])\\n\"], \"entry_point\": \"f_39861\", \"intent\": \"\\u00bfC\\u00f3mo escoger cuatro valores aleatorios `L1`, `L2`, `L3`, `L4` de una cadena `S` sin repetir?\", \"library\": [\"random\"]}", "generation": "Scenario 1:\nnumpy, random, matplotlib.pyplot, pandas\n```python\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants\nCOLORS = ['Red', 'Blue', 'Green', 'Yellow', 'Black']\n\ndef f_39861_scenario_1(S):\n    \"\"\"\n    The function picks four random colors from a given list and generates a random dataset of 100 rows.\n    It then plots the distribution of these colors in the dataset.\n\n    Parameters:\n    S (list): The list of colors.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - random\n    - matplotlib.pyplot\n    - pandas\n\n    Example:\n    >>> f_39861_scenario_1(COLORS)\n    \"\"\"\n    L1, L2, L3, L4 = random.sample(S, 4)\n    data = np.random.choice([L1, L2, L3, L4], 100)\n    df = pd.DataFrame(data, columns=['Color'])\n    df['Color'].value_counts().plot(kind='bar')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nrandom, datetime, pytz\n```python\nimport random\nfrom datetime import datetime, timedelta\nimport pytz\n\ndef f_39861_scenario_2(S):\n    \"\"\"\n    The function picks four random dates from a given list, and finds the one that is closest to the current date.\n\n    Parameters:\n    S (list): The list of dates.\n\n    Returns:\n    datetime: The date closest to the current date.\n\n    Requirements:\n    - random\n    - datetime\n    - pytz\n\n    Example:\n    >>> dates = [datetime.now(pytz.UTC) + timedelta(days=i) for i in range(10)]\n    >>> f_39861_scenario_2(dates)\n    \"\"\"\n    L1, L2, L3, L4 = random.sample(S, 4)\n    closest_date = min([L1, L2, L3, L4], key=lambda x: abs(x - datetime.now(pytz.UTC)))\n    return closest_date\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nrandom, pandas, matplotlib.pyplot\n```python\nimport random\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nCOUNTRIES = ['USA', 'UK', 'China', 'India', 'Japan', 'Brazil', 'Canada', 'Australia', 'Germany', 'France']\n\ndef f_39861_scenario_3(S):\n    \"\"\"\n    The function picks four random countries from a given list and generates a dataset of 1000 rows.\n    It then plots the distribution of these countries in the dataset.\n\n    Parameters:\n    S (list): The list of countries.\n\n    Returns:\n    None\n\n    Requirements:\n    - random\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> f_39861_scenario_3(COUNTRIES)\n    \"\"\"\n    L1, L2, L3, L4 = random.sample(S, 4)\n    data = [random.choice([L1, L2, L3, L4]) for _ in range(1000)]\n    df = pd.DataFrame(data, columns=['Country'])\n    df['Country'].value_counts().plot(kind='bar')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nrandom, numpy, matplotlib.pyplot\n```python\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef f_39861_scenario_4(S):\n    \"\"\"\n    The function picks four random numbers from a given list and generates a random dataset of 100 rows.\n    It then plots the histogram of these numbers in the dataset.\n\n    Parameters:\n    S (list): The list of numbers.\n\n    Returns:\n    None\n\n    Requirements:\n    - random\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> f_39861_scenario_4(list(range(100)))\n    \"\"\"\n    L1, L2, L3, L4 = random.sample(S, 4)\n    data = np.random.choice([L1, L2, L3, L4], 100)\n    plt.hist(data, bins=[L1, L2, L3, L4, max([L1, L2, L3, L4])+1], edgecolor='black')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nrandom, datetime, matplotlib.dates, matplotlib.pyplot\n```python\nimport random\nfrom datetime import datetime, timedelta\nimport matplotlib.dates as mdates\nimport matplotlib.pyplot as plt\n\ndef f_39861_scenario_5(S):\n    \"\"\"\n    The function picks four random dates from a given list and plots them on a timeline.\n\n    Parameters:\n    S (list): The list of dates.\n\n    Returns:\n    None\n\n    Requirements:\n    - random\n    - datetime\n    - matplotlib.dates\n    - matplotlib.pyplot\n\n    Example:\n    >>> dates = [datetime.now() + timedelta(days=i) for i in range(20)]\n    >>> f_39861_scenario_5(dates)\n    \"\"\"\n    L1, L2, L3, L4 = random.sample(S, 4)\n    dates = [L1, L2, L3, L4]\n    dates.sort()\n\n    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%m/%d/%Y'))\n    plt.gca().xaxis.set_major_locator(mdates.DayLocator())\n    plt.plot(dates, [1, 2, 3, 4], 'o-')\n    plt.gcf().autofmt_xdate()\n    plt.show()\n```"}
{"seed": "{\"task_id\": 22582, \"prompt\": \"def f_22582(lista):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"list(zip(*[iter(lista)]*2))\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([1,2,3,4]) == [(1,2),(3,4)]\\n\", \"\\n    assert candidate([1,'a',33.5,None]) == [(1,'a'),(33.5,None)]\\n\"], \"entry_point\": \"f_22582\", \"intent\": \"\\u00bfC\\u00f3mo crear una lista de tuplas de 2 valores a partir de una lista `lista`?\", \"library\": []}", "generation": "Scenario 1:\nitertools, random, pandas, matplotlib\n```python\nimport pandas as pd\nimport random\nimport matplotlib.pyplot as plt\nfrom itertools import zip_longest\n\n# Constants\nLETTERS = list('ABCDEFGHIJKLMNOPQRSTUVWXYZ')\n\ndef create_random_data_pairs(n_pairs, fillvalue=None):\n    \"\"\"\n    Generate a DataFrame of random pairs of integers and letters with the \n    possibility of uneven lists.\n\n    Parameters:\n    n_pairs (int): The number of pairs to generate.\n\n    Returns:\n    DataFrame: A pandas DataFrame with columns 'Integers' and 'Letters'.\n\n    Requirements:\n    - pandas\n    - random\n    - matplotlib.pyplot\n    - itertools.zip_longest\n\n    Example:\n    >>> df = create_random_data_pairs(10)\n    >>> print(df)\n    >>> df.plot(x='Integers', y='Letters', kind='scatter')\n    \"\"\"\n    random_integers = [random.randint(0, 100) for _ in range(n_pairs)]\n    random_letters = random.sample(LETTERS, n_pairs)\n\n    data_pairs = list(zip_longest(random_integers, random_letters, fillvalue=fillvalue))\n\n    df = pd.DataFrame(data_pairs, columns=['Integers', 'Letters'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, pandas, matplotlib, itertools\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom itertools import zip_longest\n\n# Constants\nRANGES = [(0, 10), (10, 20), (20, 30), (30, 40), (40, 50)]\n\ndef generate_random_data_and_plot_histogram(n_elements, fillvalue=None):\n    \"\"\"\n    Generate random data within specified ranges and plot a histogram of the \n    data.\n\n    Parameters:\n    n_elements (int): The number of elements to generate.\n\n    Returns:\n    list: A list of tuples containing the generated data.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - itertools.zip_longest\n\n    Example:\n    >>> data = generate_random_data_and_plot_histogram(10)\n    >>> print(data)\n    \"\"\"\n    random_data = [np.random.randint(low, high, size=n_elements) for low, high in RANGES]\n    data_pairs = list(zip_longest(*random_data, fillvalue=fillvalue))\n\n    df = pd.DataFrame(data_pairs, columns=['Range {}'.format(range_) for range_ in RANGES])\n    df.hist(bins=10, figsize=(10, 8))\n    plt.tight_layout()\n\n    return data_pairs\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ncsv, pandas, matplotlib, itertools\n```python\nimport csv\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom itertools import zip_longest\n\n# Constants\nCSV_FILE = 'sample_data.csv'\n\ndef read_csv_and_plot_data(fillvalue=None):\n    \"\"\"\n    Read data from a CSV file and plot the data.\n\n    Parameters:\n    None\n\n    Returns:\n    DataFrame: A pandas DataFrame of the CSV data.\n\n    Requirements:\n    - csv\n    - pandas\n    - matplotlib.pyplot\n    - itertools.zip_longest\n\n    Example:\n    >>> df = read_csv_and_plot_data()\n    >>> print(df)\n    >>> df.plot(kind='bar')\n    \"\"\"\n    with open(CSV_FILE, 'r') as f:\n        reader = csv.reader(f)\n        data = list(zip_longest(*reader, fillvalue=fillvalue))\n\n    df = pd.DataFrame(data)\n    df.plot(kind='bar')\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, pandas, matplotlib, itertools\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom itertools import zip_longest\n\n# Constants\nCATEGORIES = ['Category {}'.format(i) for i in range(1, 6)]\n\ndef generate_categorized_data_and_plot(n_elements, fillvalue=None):\n    \"\"\"\n    Generate a DataFrame of categorized data and plot the data.\n\n    Parameters:\n    n_elements (int): The number of elements to generate.\n\n    Returns:\n    DataFrame: A pandas DataFrame of the generated data.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - itertools.zip_longest\n\n    Example:\n    >>> df = generate_categorized_data_and_plot(10)\n    >>> print(df)\n    >>> df.plot(kind='bar')\n    \"\"\"\n    random_data = [np.random.normal(loc=i, scale=1, size=n_elements) for i in range(len(CATEGORIES))]\n    data_pairs = list(zip_longest(*random_data, fillvalue=fillvalue))\n\n    df = pd.DataFrame(data_pairs, columns=CATEGORIES)\n    df.plot(kind='bar')\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, pandas, matplotlib, itertools\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom itertools import zip_longest\n\n# Constants\nRANGES = [(0, 10), (10, 20), (20, 30), (30, 40), (40, 50)]\n\ndef generate_random_data_and_plot_scatter(n_elements, fillvalue=None):\n    \"\"\"\n    Generate random data within specified ranges and plot a scatter plot of the \n    data.\n\n    Parameters:\n    n_elements (int): The number of elements to generate.\n\n    Returns:\n    list: A list of tuples containing the generated data.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - itertools.zip_longest\n\n    Example:\n    >>> data = generate_random_data_and_plot_scatter(10)\n    >>> print(data)\n    \"\"\"\n    random_data = [np.random.randint(low, high, size=n_elements) for low, high in RANGES]\n    data_pairs = list(zip_longest(*random_data, fillvalue=fillvalue))\n\n    df = pd.DataFrame(data_pairs, columns=['Range {}'.format(range_) for range_ in RANGES])\n    df.plot(kind='scatter', x='Range (0, 10)', y='Range (40, 50)')\n\n    return data_pairs\n```"}
{"seed": "{\"task_id\": 1651, \"prompt\": \"def f_1651(num):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"sum([int(c) for c in str(num)])\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(10) == 1\\n\", \"\\n    assert candidate(100) == 1\\n\", \"\\n    assert candidate(1234) == 10\\n\"], \"entry_point\": \"f_1651\", \"intent\": \"\\u00bfC\\u00f3mo obtener la ra\\u00edz digital de un n\\u00famero `num`?\", \"library\": []}", "generation": "Scenario 1: \nnumpy, itertools, math, matplotlib\n```python\nimport numpy as np\nimport itertools\nimport math\nimport matplotlib.pyplot as plt\n\n# Constants\nBASE_NUMBERS = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n\ndef plot_digit_sum_distribution(num):\n    \"\"\"\n    Generate a histogram of the digital sum distribution of a large number of permutations \n    of a given number's digits.\n    \n    Parameters:\n    num (int): The number.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - numpy\n    - itertools\n    - math\n    - matplotlib.pyplot\n    \n    Example:\n    >>> plot_digit_sum_distribution(12345)\n    \"\"\"\n    num_digits = [int(c) for c in str(num)]\n    permutations = list(itertools.permutations(num_digits))\n    digit_sums = [sum(p) for p in permutations]\n\n    plt.hist(digit_sums, bins=np.arange(min(digit_sums)-0.5, max(digit_sums)+1.5), edgecolor='black')\n    plt.title('Digital Sum Distribution')\n    plt.xlabel('Digital Sum')\n    plt.ylabel('Frequency')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncollections, itertools, math\n```python\nfrom collections import Counter\nimport itertools\nimport math\n\ndef most_common_digit_sum(num):\n    \"\"\"\n    Find the most common digital sum of all permutations of a given number's digits.\n\n    Parameters:\n    num (int): The number.\n\n    Returns:\n    int: The most common digital sum.\n\n    Requirements:\n    - collections\n    - itertools\n    - math\n\n    Example:\n    >>> most_common_digit_sum(12345)\n    \"\"\"\n    num_digits = [int(c) for c in str(num)]\n    permutations = list(itertools.permutations(num_digits))\n    digit_sums = [sum(p) for p in permutations]\n    digit_sum_counter = Counter(digit_sums)\n\n    return digit_sum_counter.most_common(1)[0][0]\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, math, matplotlib\n```python\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\n\ndef plot_digit_sum_vs_num(num_range):\n    \"\"\"\n    Plot the digital sum of numbers in a given range.\n\n    Parameters:\n    num_range (int): The range of numbers.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - math\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_digit_sum_vs_num(1000)\n    \"\"\"\n    nums = np.arange(num_range+1)\n    digit_sums = [sum(int(c) for c in str(num)) for num in nums]\n\n    plt.plot(nums, digit_sums)\n    plt.title('Digital Sum vs Number')\n    plt.xlabel('Number')\n    plt.ylabel('Digital Sum')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nrandom, collections, math\n```python\nimport random\nfrom collections import Counter\nimport math\n\ndef random_num_most_common_digit_sum(num_samples, num_digits):\n    \"\"\"\n    Find the most common digital sum of a sample of random numbers with a given number of digits.\n\n    Parameters:\n    num_samples (int): The number of samples.\n    num_digits (int): The number of digits in each sample.\n\n    Returns:\n    int: The most common digital sum.\n\n    Requirements:\n    - random\n    - collections\n    - math\n\n    Example:\n    >>> random_num_most_common_digit_sum(1000, 5)\n    \"\"\"\n    nums = [random.randint(10**(num_digits-1), 10**num_digits-1) for _ in range(num_samples)]\n    digit_sums = [sum(int(c) for c in str(num)) for num in nums]\n    digit_sum_counter = Counter(digit_sums)\n\n    return digit_sum_counter.most_common(1)[0][0]\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, matplotlib\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_digit_sum_distribution(num_samples, num_digits):\n    \"\"\"\n    Generate a histogram of the digital sum distribution of a sample of random numbers \n    with a given number of digits.\n\n    Parameters:\n    num_samples (int): The number of samples.\n    num_digits (int): The number of digits in each sample.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_digit_sum_distribution(1000, 5)\n    \"\"\"\n    nums = np.random.randint(10**(num_digits-1), 10**num_digits-1, num_samples)\n    digit_sums = [sum(int(c) for c in str(num)) for num in nums]\n\n    plt.hist(digit_sums, bins=np.arange(min(digit_sums)-0.5, max(digit_sums)+1.5), edgecolor='black')\n    plt.title('Digital Sum Distribution')\n    plt.xlabel('Digital Sum')\n    plt.ylabel('Frequency')\n    plt.show()\n```"}
{"seed": "{\"task_id\": 1139, \"prompt\": \"def f_1139(date):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"date.strftime('%y%j')\", \"test_start\": \"\\nimport datetime\\n\\ndef check(candidate):\", \"test\": [\"\\n    date = datetime.datetime.now()\\n    assert candidate(date) == date.strftime('%y%j')\\n\"], \"entry_point\": \"f_1139\", \"intent\": \"\\u00bfC\\u00f3mo obtener los dos \\u00faltimos d\\u00edgitos del a\\u00f1o seguidos del d\\u00eda del a\\u00f1o desde un datetime `date`?\", \"library\": [\"datetime\"]}", "generation": "Scenario 1:\npandas, datetime, os, random\n```python\nimport pandas as pd\nfrom datetime import datetime\nimport os\nimport random\n\n# Constants\nFILE_PATH = os.path.join(os.getcwd(), 'sales.csv')\n\n# Sample data for sales.csv in comment\n# Date,Product,Units Sold\n# 2023-01-01,Apple,100\n# 2023-01-01,Banana,150\n# 2023-01-02,Apple,120\n# 2023-01-02,Banana,130\n\ndef compute_sales_summary(date):\n    \"\"\"\n    Compute the total units sold and the most sold product on a given date.\n\n    Parameters:\n    date (datetime): The date for the sales data.\n\n    Returns:\n    dict: A dictionary with total units sold and the most sold product.\n\n    Requirements:\n    - pandas\n    - datetime\n    - os\n    - random\n\n    Example:\n    >>> date = datetime(2023, 1, 1)\n    >>> summary = compute_sales_summary(date)\n    >>> print(summary)\n    {'Total Units Sold': 250, 'Most Sold Product': 'Banana'}\n    \"\"\"\n    df = pd.read_csv(FILE_PATH)\n    df['Date'] = pd.to_datetime(df['Date'])\n    sales_data = df[df['Date'] == pd.Timestamp(date.date())]\n\n    total_units_sold = sales_data['Units Sold'].sum()\n    most_sold_product = sales_data.groupby('Product')['Units Sold'].sum().idxmax()\n\n    return {'Total Units Sold': total_units_sold, 'Most Sold Product': most_sold_product}\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ndatetime, matplotlib.pyplot, numpy, math\n```python\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport math\n\n# Constants\nSTART_DATE = datetime(2023, 1, 1)\nEND_DATE = datetime(2023, 12, 31)\n\ndef plot_sin_wave(date):\n    \"\"\"\n    Plot a sine wave for the year starting from a given date with the frequency \n    proportional to the day of the year of the given date.\n\n    Parameters:\n    date (datetime): The date.\n\n    Returns:\n    None: The function does not return a value.\n\n    Requirements:\n    - datetime\n    - matplotlib.pyplot\n    - numpy\n    - math\n\n    Example:\n    >>> date = datetime(2023, 6, 15)\n    >>> plot_sin_wave(date)\n    \"\"\"\n    days = (date - START_DATE).days + 1\n    frequency = days / 365.0\n    x = np.linspace(0, 2 * math.pi, 1000)\n    y = np.sin(2 * math.pi * frequency * x)\n\n    plt.plot(x, y)\n    plt.title('Sine Wave for Year with Frequency Proportional to Day of Year')\n    plt.grid(True)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ndatetime, sqlite3, pandas, matplotlib.pyplot\n```python\nfrom datetime import datetime\nimport sqlite3\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nDB_PATH = 'my_database.db'\n\n# Sample data for my_database.db in comment\n# CREATE TABLE logs (\n#   timestamp datetime,\n#   level text,\n#   message text\n# );\n# INSERT INTO logs VALUES ('2023-01-01 00:00:00', 'INFO', 'Starting system...');\n# INSERT INTO logs VALUES ('2023-01-01 00:01:00', 'ERROR', 'System failed to start.');\n\ndef plot_log_counts(date):\n    \"\"\"\n    Plot the counts of logs for each level on a given date.\n\n    Parameters:\n    date (datetime): The date for the logs.\n\n    Returns:\n    None: The function does not return a value.\n\n    Requirements:\n    - datetime\n    - sqlite3\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> date = datetime(2023, 1, 1)\n    >>> plot_log_counts(date)\n    \"\"\"\n    conn = sqlite3.connect(DB_PATH)\n\n    query = f\"\"\"\n    SELECT level, COUNT(*) AS count\n    FROM logs\n    WHERE DATE(timestamp) = '{date.date()}'\n    GROUP BY level\n    \"\"\"\n\n    df = pd.read_sql_query(query, conn)\n    df.plot(kind='bar', x='level', y='count', rot=0)\n\n    plt.title('Log Counts by Level on ' + date.strftime('%Y-%m-%d'))\n    plt.xlabel('Level')\n    plt.ylabel('Count')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ndatetime, glob, os, hashlib\n```python\nfrom datetime import datetime\nimport glob\nimport os\nimport hashlib\n\n# Constants\nFOLDER_PATH = os.path.join(os.getcwd(), 'documents')\n\ndef get_file_hashes(date):\n    \"\"\"\n    Get the MD5 hashes of all files modified on a given date in a folder.\n\n    Parameters:\n    date (datetime): The date for the files.\n\n    Returns:\n    dict: A dictionary with file names as keys and MD5 hashes as values.\n\n    Requirements:\n    - datetime\n    - glob\n    - os\n    - hashlib\n\n    Example:\n    >>> date = datetime(2023, 1, 1)\n    >>> hashes = get_file_hashes(date)\n    >>> print(hashes)\n    {'document1.txt': '5d41402abc4b2a76b9719d911017c592', 'document2.txt': '6dcd4ce23d88e2ee95838f7b014b628e'}\n    \"\"\"\n    file_hashes = {}\n\n    for file_path in glob.glob(FOLDER_PATH + '/*'):\n        mtime = os.path.getmtime(file_path)\n        mtime_date = datetime.fromtimestamp(mtime).date()\n\n        if mtime_date == date.date():\n            with open(file_path, 'rb') as file:\n                data = file.read()\n                hash_md5 = hashlib.md5(data).hexdigest()\n                file_hashes[os.path.basename(file_path)] = hash_md5\n\n    return file_hashes\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\ndatetime, requests, bs4, pandas\n```python\nfrom datetime import datetime\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\n# Constants\nURL = 'https://www.example.com/news'\n\ndef extract_news_titles(date):\n    \"\"\"\n    Extract the titles of news articles published on a given date from a website.\n\n    Parameters:\n    date (datetime): The date for the news articles.\n\n    Returns:\n    list: A list of news article titles.\n\n    Requirements:\n    - datetime\n    - requests\n    - bs4.BeautifulSoup\n    - pandas\n\n    Example:\n    >>> date = datetime(2023, 1, 1)\n    >>> titles = extract_news_titles(date)\n    >>> print(titles)\n    ['News Title 1', 'News Title 2', 'News Title 3']\n    \"\"\"\n    response = requests.get(URL)\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    articles = soup.find_all('article', {'class': 'news-article'})\n    titles = [article.find('h2').text for article in articles if pd.to_datetime(article.find('time')['datetime']).date() == date.date()]\n\n    return titles\n```"}
{"seed": "{\"task_id\": 15727, \"prompt\": \"def f_15727(S):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"S.upper()\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('mystring') == 'MYSTRING'\\n\", \"\\n    assert candidate('myString') == 'MYSTRING'\\n\", \"\\n    assert candidate('MySTRinG') == 'MYSTRING'\\n\", \"\\n    assert candidate('MYSTRING') == 'MYSTRING'\\n\"], \"entry_point\": \"f_15727\", \"intent\": \"\\u00bfC\\u00f3mo convertir de min\\u00fasculas a may\\u00fasculas un string `S`?\", \"library\": []}", "generation": "Scenario 1:\npandas, re, numpy, matplotlib\n```python\nimport pandas as pd\nimport re\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nLETTERS = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n\ndef count_letters_frequency(text):\n    \"\"\"\n    Count the frequency of each uppercase letter in a given text and plot the histogram.\n    \n    Parameters:\n    text (str): The given text.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the frequency of each uppercase letter.\n    \n    Requirements:\n    - pandas\n    - re\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> text = \"Hello, World!\"\n    >>> df = count_letters_frequency(text)\n    >>> print(df)\n    >>> df.plot(kind='bar')\n    \"\"\"\n    text = text.upper()\n    letter_counts = {letter: text.count(letter) for letter in LETTERS}\n    df = pd.DataFrame.from_dict(letter_counts, orient='index', columns=['Frequency'])\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, os, shutil\n```python\nimport pandas as pd\nimport os\nimport shutil\n\n# Constants\nCSV_DIRECTORY = './csv_files/'\n\ndef move_csv_files(df):\n    \"\"\"\n    Move csv files from current directory to a specific directory based on a DataFrame.\n    The DataFrame should have 'Filename' and 'Should Move' columns.\n    \n    Parameters:\n    df (DataFrame): The DataFrame with 'Filename' and 'Should Move' columns.\n    \n    Returns:\n    list: A list of moved files.\n    \n    Requirements:\n    - pandas\n    - os\n    - shutil\n    \n    Example:\n    >>> df = pd.DataFrame({'Filename': ['file1.csv', 'file2.csv'], 'Should Move': [True, False]})\n    >>> moved_files = move_csv_files(df)\n    >>> print(moved_files)\n    \"\"\"\n    moved_files = []\n    for index, row in df.iterrows():\n        filename = row['Filename']\n        should_move = row['Should Move']\n        if should_move and filename.endswith('.csv') and os.path.isfile(filename):\n            shutil.move(filename, CSV_DIRECTORY)\n            moved_files.append(filename)\n    return moved_files\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ncollections, string, random\n```python\nfrom collections import Counter\nimport string\nimport random\n\n# Constants\nLETTERS = string.ascii_uppercase\n\ndef generate_random_text(length):\n    \"\"\"\n    Generate a random text with a given length and calculate the frequency of each letter.\n    \n    Parameters:\n    length (int): The length of the random text.\n    \n    Returns:\n    dict: A dictionary with the frequency of each letter.\n    \n    Requirements:\n    - collections\n    - string\n    - random\n    \n    Example:\n    >>> text_length = 100\n    >>> frequencies = generate_random_text(text_length)\n    >>> print(frequencies)\n    \"\"\"\n    text = ''.join(random.choice(LETTERS) for _ in range(length))\n    frequencies = dict(Counter(text))\n    return frequencies\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, os, csv\n```python\nimport pandas as pd\nimport os\nimport csv\n\n# Constants\nDATA_DIRECTORY = './data/'\n\ndef convert_to_uppercase(filename):\n    \"\"\"\n    Convert all strings in a csv file to uppercase.\n    \n    Parameters:\n    filename (str): The filename of the csv file.\n    \n    Returns:\n    str: The filename of the new csv file.\n    \n    Requirements:\n    - pandas\n    - os\n    - csv\n    \n    Example:\n    >>> filename = 'data.csv'\n    >>> new_filename = convert_to_uppercase(filename)\n    >>> print(new_filename)\n    \"\"\"\n    df = pd.read_csv(filename)\n    df = df.applymap(lambda s: s.upper() if type(s) == str else s)\n    new_filename = os.path.join(DATA_DIRECTORY, f'uppercase_{os.path.basename(filename)}')\n    df.to_csv(new_filename, index=False)\n    return new_filename\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nLETTERS = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\nNUM_SAMPLES = 1000\n\ndef plot_random_letters_distribution():\n    \"\"\"\n    Generate a random sequence of uppercase letters and plot the letters distribution.\n    \n    Returns:\n    Series: A pandas Series with the letters distribution.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> s = plot_random_letters_distribution()\n    >>> print(s)\n    >>> s.plot(kind='bar')\n    \"\"\"\n    letters = np.random.choice(list(LETTERS), NUM_SAMPLES)\n    s = pd.Series(letters).value_counts()\n    return s\n```"}
{"seed": "{\"task_id\": 47500, \"prompt\": \"def f_47500(num, num1):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"num / num1\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert (candidate(1, 1.0) - 1.0) < 1e-6\\n\", \"\\n    assert (candidate(10, 3.3) - 3.0303) < 0.01\\n\"], \"entry_point\": \"f_47500\", \"intent\": \"\\u00bfC\\u00f3mo dividir un n\\u00famero entero `num` por un n\\u00famero `num1` decimal?\", \"library\": []}", "generation": "Scenario 1:\nnumpy, scipy.stats, pandas, matplotlib\n```python\nimport numpy as np\nfrom scipy.stats import norm\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nNUM_SAMPLES = 1000\n\ndef generate_and_analyze_normal_distribution(mean, std_dev):\n    \"\"\"\n    Generate a normal distribution with a given mean and standard deviation, \n    calculate its mean, standard deviation, and plot its histogram.\n\n    Parameters:\n    mean (int): The mean of the normal distribution.\n    std_dev (float): The standard deviation of the normal distribution.\n\n    Returns:\n    tuple: A tuple containing the calculated mean, standard deviation, and a \n    pandas Series of the generated normal distribution.\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> mean, std_dev, distribution = generate_and_analyze_normal_distribution(10, 3.3)\n    >>> print(f'Mean: {mean}, Standard Deviation: {std_dev}')\n    >>> distribution.plot(kind='hist', rwidth=0.8)\n    \"\"\"\n    distribution = np.random.normal(mean, std_dev, NUM_SAMPLES)\n\n    calculated_mean = np.mean(distribution)\n    calculated_std_dev = np.std(distribution)\n\n    distribution_series = pd.Series(distribution)\n\n    return calculated_mean, calculated_std_dev, distribution_series\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nmath, numpy, matplotlib\n```python\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nX_VALUES = np.linspace(-10, 10, 400)\nY_VALUES = np.linspace(-10, 10, 400)\n\ndef plot_complex_function(real_part, imaginary_part):\n    \"\"\"\n    Plot a complex function given its real and imaginary parts.\n\n    Parameters:\n    real_part (str): The real part of the complex function.\n    imaginary_part (str): The imaginary part of the complex function.\n\n    Returns:\n    None\n\n    Requirements:\n    - math\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_complex_function('x**2 - y**2', '2*x*y')\n    \"\"\"\n    X, Y = np.meshgrid(X_VALUES, Y_VALUES)\n\n    Z = eval(real_part) + 1j * eval(imaginary_part)\n\n    plt.figure(figsize=(8, 8))\n    plt.imshow(np.angle(Z), extent=[-10, 10, -10, 10], cmap='hsv')\n    plt.colorbar()\n    plt.title(f'Plot of {real_part} + i{imaginary_part}')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, sklearn.preprocessing\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Constants\nDATA = pd.DataFrame({\n    'A': [1, 2, 3, 4, 5],\n    'B': [10, 20, 30, 40, 50],\n    'C': [100, 200, 300, 400, 500]\n})\n\ndef scale_dataframe(df, num, num1):\n    \"\"\"\n    Scale a DataFrame using MinMaxScaler with a given range, and then divide each \n    value in the DataFrame by a given number.\n\n    Parameters:\n    df (DataFrame): The DataFrame to scale.\n    num (int): The lower bound of the scaling range.\n    num1 (int): The upper bound of the scaling range.\n\n    Returns:\n    DataFrame: The scaled DataFrame.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.preprocessing\n\n    Example:\n    >>> scaled_df = scale_dataframe(DATA, 1, 10)\n    >>> print(scaled_df)\n    \"\"\"\n    scaler = MinMaxScaler(feature_range=(num, num1))\n    scaled_data = scaler.fit_transform(df)\n    scaled_df = pd.DataFrame(scaled_data, columns=df.columns)\n\n    return scaled_df / num1\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nsympy, numpy, matplotlib\n```python\nfrom sympy import symbols, diff\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nX_RANGE = np.linspace(-10, 10, 400)\n\ndef plot_function_and_derivative(expr):\n    \"\"\"\n    Plot a function and its derivative given the function's expression.\n\n    Parameters:\n    expr (str): The expression of the function.\n\n    Returns:\n    None\n\n    Requirements:\n    - sympy\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_function_and_derivative('x**3 - 3*x')\n    \"\"\"\n    x = symbols('x')\n    function = eval(expr)\n    derivative = diff(function, x)\n\n    function_lambdified = lambdify(x, function, \"numpy\")\n    derivative_lambdified = lambdify(x, derivative, \"numpy\")\n\n    plt.figure(figsize=(8, 8))\n    plt.plot(X_RANGE, function_lambdified(X_RANGE), label='Function')\n    plt.plot(X_RANGE, derivative_lambdified(X_RANGE), label='Derivative')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, sklearn.linear_model\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\n# Constants\nDATA = pd.DataFrame({\n    'X': [1, 2, 3, 4, 5],\n    'Y': [2, 4, 6, 8, 10]\n})\n\ndef linear_regression(df, x_column, y_column):\n    \"\"\"\n    Perform a linear regression on a DataFrame given the X and Y columns, and return \n    the slope and intercept of the regression line.\n\n    Parameters:\n    df (DataFrame): The DataFrame to perform the regression on.\n    x_column (str): The name of the X column.\n    y_column (str): The name of the Y column.\n\n    Returns:\n    tuple: A tuple containing the slope and intercept of the regression line.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.linear_model\n\n    Example:\n    >>> slope, intercept = linear_regression(DATA, 'X', 'Y')\n    >>> print(f'Slope: {slope}, Intercept: {intercept}')\n    \"\"\"\n    X = df[x_column].values.reshape(-1, 1)\n    Y = df[y_column].values.reshape(-1, 1)\n\n    reg = LinearRegression().fit(X, Y)\n\n    return reg.coef_[0][0], reg.intercept_[0]\n```"}
{"seed": "{\"task_id\": 49118, \"prompt\": \"def f_49118(S):\\n\\t\", \"suffix\": \"\\n\\treturn num\", \"canonical_solution\": \"num = sum([int('ch' in w) for w in S.split()])\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('chain of words color cook zero chaos') == 2\\n\"], \"entry_point\": \"f_49118\", \"intent\": \"\\u00bfC\\u00f3mo calcular el n\\u00famero de palabras `num` que contengan la letra `ch` en un texto `S`?\", \"library\": []}", "generation": "Scenario 1:\npandas, re, nltk, matplotlib\n```python\nimport pandas as pd\nimport re\nfrom nltk.corpus import stopwords\nimport matplotlib.pyplot as plt\n\n# Constants\nSTOPWORDS = set(stopwords.words('english'))\n\ndef plot_word_frequency(text, target_word):\n    \"\"\"\n    Plot the frequency of a target word and all other words (excluding stopwords) in a given text.\n\n    Parameters:\n    text (str): The input text.\n    target_word (str): The target word\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - re\n    - nltk.corpus\n    - matplotlib.pyplot\n    \n    Example:\n    >>> plot_word_frequency('This is a test. This is only a test.', 'test')\n    \"\"\"\n    words = re.findall(r'\\b\\w+\\b', text)\n    words_df = pd.DataFrame(words, columns=['word'])\n    words_df['word'] = words_df['word'].str.lower()\n    words_df = words_df[~words_df['word'].isin(STOPWORDS)]\n    word_counts = words_df['word'].value_counts()\n\n    plt.figure(figsize=(10, 5))\n    word_counts.plot(kind='bar', color=['red' if word == target_word else 'blue' for word in word_counts.index])\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nre, collections, matplotlib, string\n```python\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nimport string\n\ndef plot_letter_frequency(text):\n    \"\"\"\n    Plot the frequency of each letter in a given text.\n\n    Parameters:\n    text (str): The input text.\n\n    Returns:\n    None\n\n    Requirements:\n    - re\n    - collections\n    - matplotlib.pyplot\n    - string\n\n    Example:\n    >>> plot_letter_frequency('This is a test. This is only a test.')\n    \"\"\"\n    text = text.lower()\n    letters = re.findall(r'\\b[a-z]\\b', text)\n    letter_counts = Counter(letters)\n    letter_counts = {letter: letter_counts.get(letter, 0) for letter in string.ascii_lowercase}\n\n    plt.figure(figsize=(10, 5))\n    plt.bar(letter_counts.keys(), letter_counts.values())\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnltk, re, collections, matplotlib\n```python\nfrom nltk import FreqDist, word_tokenize\nimport re\nimport collections\nimport matplotlib.pyplot as plt\n\ndef plot_top_n_words(text, n):\n    \"\"\"\n    Plot the top n most frequent words in a given text.\n\n    Parameters:\n    text (str): The input text.\n    n (int): The number of top words to plot.\n\n    Returns:\n    None\n\n    Requirements:\n    - nltk\n    - re\n    - collections\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_top_n_words('This is a test. This is only a test.', 5)\n    \"\"\"\n    words = word_tokenize(text)\n    words = [word.lower() for word in words if word.isalpha()]\n    fd = FreqDist(words)\n    most_common = fd.most_common(n)\n\n    plt.figure(figsize=(10, 5))\n    plt.bar(*zip(*most_common))\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnltk, matplotlib, numpy, string\n```python\nfrom nltk import FreqDist, word_tokenize\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport string\n\ndef plot_word_length_distribution(text):\n    \"\"\"\n    Plot the distribution of word lengths in a given text.\n\n    Parameters:\n    text (str): The input text.\n\n    Returns:\n    None\n\n    Requirements:\n    - nltk\n    - matplotlib.pyplot\n    - numpy\n    - string\n\n    Example:\n    >>> plot_word_length_distribution('This is a test. This is only a test.')\n    \"\"\"\n    words = word_tokenize(text)\n    words = [word for word in words if word not in string.punctuation]\n    word_lengths = [len(word) for word in words]\n\n    plt.figure(figsize=(10, 5))\n    plt.hist(word_lengths, bins=np.arange(1, max(word_lengths)+2) - 0.5, edgecolor='black')\n    plt.xticks(np.arange(max(word_lengths)+1))\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, matplotlib, scipy\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\ndef plot_normal_distribution(mu, sigma):\n    \"\"\"\n    Plot a normal distribution with a given mean and standard deviation.\n\n    Parameters:\n    mu (float): The mean of the distribution.\n    sigma (float): The standard deviation of the distribution.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - scipy.stats\n\n    Example:\n    >>> plot_normal_distribution(0, 1)\n    \"\"\"\n    x = np.linspace(mu - 3.5*sigma, mu + 3.5*sigma, 100)\n    y = norm.pdf(x, mu, sigma)\n\n    plt.figure(figsize=(10, 5))\n    plt.plot(x, y)\n    plt.show()\n```"}
{"seed": "{\"task_id\": 96261, \"prompt\": \"def f_96261(L1, L2):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"list(map(operator.mul, L1, L2))\", \"test_start\": \"\\nimport operator\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([3,4,2,6],[98,80,86,78]) == [294, 320, 172, 468]\\n\", \"\\n    assert candidate([i for i in range(10)], [0 for i in range(10)]) == [0,0,0,0,0,0,0,0,0,0]\\n\"], \"entry_point\": \"f_96261\", \"intent\": \"\\u00bfC\\u00f3mo puedo multiplicar dos listas de `INT` `L1` y `L2` ?\", \"library\": [\"operator\"]}", "generation": "Scenario 1: \npandas, operator, matplotlib, random\n```python\nimport pandas as pd\nimport operator\nimport matplotlib.pyplot as plt\nfrom random import randint, seed\n\n# Constants\nCOLUMNS = ['Sales', 'Cost', 'Profit']\n\ndef calculate_profit(data):\n    \"\"\"\n    Take a DataFrame of sales and cost data for multiple products, calculate the profit for each \n    product, and visualize the profit distribution.\n\n    Parameters:\n    data (DataFrame): A DataFrame with 'Sales' and 'Cost' columns.\n\n    Returns:\n    DataFrame: A DataFrame with 'Profit' column added.\n\n    Requirements:\n    - pandas\n    - operator\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> data = pd.DataFrame({\n    ...     'Sales': [randint(100, 200) for _ in range(10)],\n    ...     'Cost': [randint(50, 150) for _ in range(10)],\n    ... })\n    >>> profit_data = calculate_profit(data)\n    >>> print(profit_data)\n    >>> profit_data['Profit'].hist(bins=10)\n    \"\"\"\n    data['Profit'] = list(map(operator.sub, data['Sales'], data['Cost']))\n\n    return data\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nnumpy, operator, itertools\n```python\nimport numpy as np\nimport operator\nfrom itertools import combinations\n\n# Constants\nCOMBINATION_SIZE = 3\n\ndef max_sum_combination(arr):\n    \"\"\"\n    Find the combination of three elements in the array that gives the maximum sum.\n\n    Parameters:\n    arr (numpy.ndarray): A numpy array of integers.\n\n    Returns:\n    tuple: A tuple of three integers that gives the maximum sum.\n\n    Requirements:\n    - numpy\n    - operator\n    - itertools\n\n    Example:\n    >>> arr = np.array([1, 2, 3, 4, 5])\n    >>> max_sum_combination(arr)\n    \"\"\"\n    max_sum = -np.inf\n    max_comb = None\n\n    for comb in combinations(arr, COMBINATION_SIZE):\n        comb_sum = operator.add.reduce(comb)\n        if comb_sum > max_sum:\n            max_sum = comb_sum\n            max_comb = comb\n\n    return max_comb\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nopencv, numpy, operator, matplotlib\n```python\nimport cv2\nimport numpy as np\nimport operator\nimport matplotlib.pyplot as plt\n\ndef blur_image(image_path, kernel_size):\n    \"\"\"\n    Blur an image using a kernel of a specified size and then visualize the original and blurred image.\n\n    Parameters:\n    image_path (str): The path to the image file.\n    kernel_size (int): The size of the kernel to use for blurring.\n\n    Requirements:\n    - opencv-python (cv2)\n    - numpy\n    - operator\n    - matplotlib.pyplot\n\n    Example:\n    >>> blur_image('path_to_image.jpg', 5)\n    \"\"\"\n    image = cv2.imread(image_path)\n    blurred_image = cv2.blur(image, (kernel_size, kernel_size))\n\n    plt.subplot(121),plt.imshow(image),plt.title('Original')\n    plt.xticks([]), plt.yticks([])\n    plt.subplot(122),plt.imshow(blurred_image),plt.title('Blurred')\n    plt.xticks([]), plt.yticks([])\n    plt.show()\n\n    return blurred_image\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, operator, scipy\n```python\nimport numpy as np\nimport operator\nfrom scipy import optimize\n\n# Constants\nFUNC = lambda x: x**4 - 3*x**3 + 2\n\ndef find_minimum(func, initial_guess):\n    \"\"\"\n    Find the minimum of a function using the BFGS method.\n\n    Parameters:\n    func (function): The function to minimize.\n    initial_guess (float): The initial guess for the minimum.\n\n    Returns:\n    OptimizeResult: The result of the optimization.\n\n    Requirements:\n    - numpy\n    - operator\n    - scipy.optimize\n\n    Example:\n    >>> find_minimum(FUNC, 2.0)\n    \"\"\"\n    result = optimize.minimize(func, initial_guess, method='BFGS')\n\n    return result\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnumpy, operator, matplotlib\n```python\nimport numpy as np\nimport operator\nimport matplotlib.pyplot as plt\n\n# Constants\nPERIOD = 2*np.pi\nAMPLITUDE = 1\nPHASE = 0\n\ndef plot_wave(amplitude, period, phase):\n    \"\"\"\n    Plot a sine wave with a given amplitude, period, and phase.\n\n    Parameters:\n    amplitude (float): The amplitude of the wave.\n    period (float): The period of the wave.\n    phase (float): The phase of the wave.\n\n    Requirements:\n    - numpy\n    - operator\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_wave(AMPLITUDE, PERIOD, PHASE)\n    \"\"\"\n    x = np.linspace(0, 4 * np.pi, 1000)\n    y = amplitude * np.sin((2 * np.pi / period) * x + phase)\n\n    plt.plot(x, y)\n    plt.show()\n\n    return y\n```\n`GPT_ODEX_BREAK`"}
{"seed": "{\"task_id\": 96261, \"prompt\": \"def f_96261(L1, L2):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"np.multiply(L1,L2)\", \"test_start\": \"\\nimport numpy as np\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert all(candidate(np.array([3,4,2,6]),np.array([98,80,86,78])) == np.array([294, 320, 172, 468]))\\n\", \"\\n    assert all(candidate(np.array([i for i in range(10)]), np.array([0 for i in range(10)])) == np.array([0,0,0,0,0,0,0,0,0,0]))\\n\"], \"entry_point\": \"f_96261\", \"intent\": \"\\u00bfC\\u00f3mo puedo multiplicar dos listas de `INT` `L1` y `L2` ?\", \"library\": [\"numpy\"]}", "generation": "Scenario 1:\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom functools import reduce\n\n# Constants\nX_LABEL = 'Number'\nY_LABEL = 'Squared Value'\n\ndef plot_squared_values(L1, L2):\n    \"\"\"\n    Multiply two lists of integers L1 and L2, square the results, and then plot the squared values.\n\n    Parameters:\n    L1, L2 (list): Lists of integers.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - pandas\n    - functools.reduce\n\n    Example:\n    >>> plot_squared_values([3,4,2,6], [98,80,86,78])\n    \"\"\"\n    # Multiply the lists and square the results\n    squared_values = np.square(np.multiply(L1,L2))\n    \n    # Prepare the data for plotting\n    data = pd.DataFrame({X_LABEL: reduce(lambda x, y: x + y, [L1, L2]), \n                         Y_LABEL: squared_values})\n    \n    # Create a bar plot\n    plt.bar(data[X_LABEL], data[Y_LABEL])\n    plt.xlabel(X_LABEL)\n    plt.ylabel(Y_LABEL)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\n```python\nimport numpy as np\nfrom scipy import stats\n\n# Constants\nALPHA = 0.05\n\ndef check_normal_distribution(L1, L2):\n    \"\"\"\n    Multiply two lists of integers L1 and L2, and then check if the results follow a normal distribution.\n\n    Parameters:\n    L1, L2 (list): Lists of integers.\n\n    Returns:\n    bool: True if the results follow a normal distribution, False otherwise.\n\n    Requirements:\n    - numpy\n    - scipy.stats\n\n    Example:\n    >>> check_normal_distribution([3,4,2,6], [98,80,86,78])\n    \"\"\"\n    # Multiply the lists\n    results = np.multiply(L1,L2)\n\n    # Perform the Shapiro-Wilk test for normality\n    stat, p_value = stats.shapiro(results)\n    \n    # Return if the results are normally distributed or not\n    return p_value > ALPHA\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\n```python\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\n\ndef scale_and_multiply(L1, L2):\n    \"\"\"\n    Standardize two lists of integers L1 and L2, and then multiply the standardized lists.\n\n    Parameters:\n    L1, L2 (list): Lists of integers.\n\n    Returns:\n    numpy.ndarray: The result of the multiplication.\n\n    Requirements:\n    - numpy\n    - sklearn.preprocessing.StandardScaler\n\n    Example:\n    >>> scale_and_multiply([3,4,2,6], [98,80,86,78])\n    \"\"\"\n    # Standardize the lists\n    scaler = StandardScaler()\n    L1_scaled = scaler.fit_transform(np.array(L1).reshape(-1, 1))\n    L2_scaled = scaler.transform(np.array(L2).reshape(-1, 1))\n\n    # Multiply the standardized lists\n    result = np.multiply(L1_scaled, L2_scaled)\n\n    return result\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n# Constants\nX_LABEL = 'L1'\nY_LABEL = 'Product of L1 and L2'\n\ndef plot_linear_regression(L1, L2):\n    \"\"\"\n    Multiply two lists of integers L1 and L2, and then plot a linear regression line on the scatter plot.\n\n    Parameters:\n    L1, L2 (list): Lists of integers.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.linear_model.LinearRegression\n\n    Example:\n    >>> plot_linear_regression([3,4,2,6], [98,80,86,78])\n    \"\"\"\n    # Multiply the lists\n    y = np.multiply(L1,L2)\n    X = np.array(L1).reshape(-1, 1)\n\n    # Fit the linear regression model\n    model = LinearRegression().fit(X, y)\n\n    # Predict the values\n    y_pred = model.predict(X)\n\n    # Plot the scatter plot and the regression line\n    plt.scatter(X, y, color='blue')\n    plt.plot(X, y_pred, color='red')\n    plt.xlabel(X_LABEL)\n    plt.ylabel(Y_LABEL)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\n```python\nimport numpy as np\nfrom scipy.spatial import distance\n\ndef calculate_euclidean_distance(L1, L2):\n    \"\"\"\n    Multiply two lists of integers L1 and L2, and then calculate the Euclidean distance between the multiplied list and the original list L1.\n\n    Parameters:\n    L1, L2 (list): Lists of integers.\n\n    Returns:\n    float: The Euclidean distance.\n\n    Requirements:\n    - numpy\n    - scipy.spatial.distance\n\n    Example:\n    >>> calculate_euclidean_distance([3,4,2,6], [98,80,86,78])\n    \"\"\"\n    # Multiply the lists\n    multiplied_list = np.multiply(L1,L2)\n\n    # Calculate the Euclidean distance\n    euclidean_distance = distance.euclidean(L1, multiplied_list)\n\n    return euclidean_distance\n```"}
{"seed": "{\"task_id\": 24577, \"prompt\": \"def f_24577(L, x):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"np.where(L == x)\", \"test_start\": \"\\nimport numpy as np\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(np.array([1,2,3,4,5]), 4)[0].tolist() == [3]\\n\", \"\\n    assert candidate(np.array([1,2,3,4,5]), 100)[0].tolist() == []\\n\"], \"entry_point\": \"f_24577\", \"intent\": \"\\u00bfC\\u00f3mo obtener las posiciones del array `L` donde encuentre un valor `x`?\", \"library\": [\"numpy\"]}", "generation": "Scenario 1: \nnumpy, pandas, matplotlib.pyplot, sklearn.preprocessing\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Constants\nCOLUMNS = ['Temperature', 'Humidity', 'Pressure']\n\ndef find_and_plot(x, df):\n    \"\"\"\n    Find the positions of a given value in a pandas DataFrame's columns \n    and plot the findings with normalized values.\n\n    Parameters:\n    x (float): The value to find.\n    df (pandas.DataFrame): The DataFrame.\n\n    Returns:\n    dict: A dictionary with column names as keys and list of indices as values.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - sklearn.preprocessing\n\n    Example:\n    >>> df = pd.DataFrame({'Temperature': [20, 21, 19, 22, 23, 20],\n    ...                    'Humidity': [30, 35, 40, 45, 50, 55],\n    ...                    'Pressure': [1013, 1014, 1015, 1016, 1017, 1018]})\n    >>> find_and_plot(20, df)\n    \"\"\"\n    results = {}\n    scaler = MinMaxScaler()\n\n    for col in COLUMNS:\n        pos = np.where(df[col] == x)[0].tolist()\n        if pos:\n            results[col] = pos\n\n    df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n\n    df_scaled.plot(kind='line')\n    plt.show()\n\n    return results\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nnumpy, pandas, seaborn, sklearn.decomposition\n```python\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\n\n# Constants\nCOLUMNS = ['Feature1', 'Feature2', 'Feature3', 'Feature4']\n\ndef find_and_decompose(x, df):\n    \"\"\"\n    Find the positions of a given value in a pandas DataFrame's columns \n    and perform PCA decomposition on the DataFrame.\n\n    Parameters:\n    x (float): The value to find.\n    df (pandas.DataFrame): The DataFrame.\n\n    Returns:\n    tuple: A tuple containing a dictionary with column names as keys and \n        list of indices as values, and a PCA object.\n\n    Requirements:\n    - numpy\n    - pandas\n    - seaborn\n    - sklearn.decomposition\n\n    Example:\n    >>> df = pd.DataFrame({'Feature1': [1, 2, 3, 4, 5, 6],\n    ...                    'Feature2': [2, 3, 4, 5, 6, 7],\n    ...                    'Feature3': [3, 4, 5, 6, 7, 8],\n    ...                    'Feature4': [4, 5, 6, 7, 8, 9]})\n    >>> find_and_decompose(4, df)\n    \"\"\"\n    results = {}\n    pca = PCA(n_components=2)\n\n    for col in COLUMNS:\n        pos = np.where(df[col] == x)[0].tolist()\n        if pos:\n            results[col] = pos\n\n    pca_result = pca.fit_transform(df)\n\n    pca_df = pd.DataFrame(data = pca_result, columns = ['Principal Component 1', 'Principal Component 2'])\n    sns.heatmap(pca_df.corr(), annot = True)\n\n    return results, pca\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnumpy, pandas, matplotlib.pyplot, sklearn.cluster\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\n\n# Constants\nCOLUMNS = ['X', 'Y']\n\ndef find_and_cluster(x, df):\n    \"\"\"\n    Find the positions of a given value in a pandas DataFrame's columns \n    and perform KMeans clustering on the DataFrame.\n\n    Parameters:\n    x (float): The value to find.\n    df (pandas.DataFrame): The DataFrame.\n\n    Returns:\n    tuple: A tuple containing a dictionary with column names as keys and \n        list of indices as values, and a KMeans object.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - sklearn.cluster\n\n    Example:\n    >>> df = pd.DataFrame({'X': [1, 2, 3, 4, 5, 6],\n    ...                    'Y': [2, 3, 4, 5, 6, 7]})\n    >>> find_and_cluster(4, df)\n    \"\"\"\n    results = {}\n    kmeans = KMeans(n_clusters=2)\n\n    for col in COLUMNS:\n        pos = np.where(df[col] == x)[0].tolist()\n        if pos:\n            results[col] = pos\n\n    kmeans.fit(df)\n\n    plt.scatter(df['X'], df['Y'], c=kmeans.labels_)\n    plt.show()\n\n    return results, kmeans\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, pandas, seaborn, sklearn.preprocessing\n```python\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\n\n# Constants\nCOLUMNS = ['Feature1', 'Feature2', 'Feature3']\n\ndef find_and_standardize(x, df):\n    \"\"\"\n    Find the positions of a given value in a pandas DataFrame's columns \n    and standardize the DataFrame.\n\n    Parameters:\n    x (float): The value to find.\n    df (pandas.DataFrame): The DataFrame.\n\n    Returns:\n    tuple: A tuple containing a dictionary with column names as keys and \n        list of indices as values, and a StandardScaler object.\n\n    Requirements:\n    - numpy\n    - pandas\n    - seaborn\n    - sklearn.preprocessing\n\n    Example:\n    >>> df = pd.DataFrame({'Feature1': [1, 2, 3, 4, 5, 6],\n    ...                    'Feature2': [2, 3, 4, 5, 6, 7],\n    ...                    'Feature3': [3, 4, 5, 6, 7, 8]})\n    >>> find_and_standardize(4, df)\n    \"\"\"\n    results = {}\n    scaler = StandardScaler()\n\n    for col in COLUMNS:\n        pos = np.where(df[col] == x)[0].tolist()\n        if pos:\n            results[col] = pos\n\n    df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n\n    sns.heatmap(df_scaled.corr(), annot=True)\n\n    return results, scaler\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnumpy, pandas, matplotlib.pyplot, sklearn.linear_model\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n# Constants\nCOLUMNS = ['X', 'Y']\n\ndef find_and_fit(x, df):\n    \"\"\"\n    Find the positions of a given value in a pandas DataFrame's columns \n    and fit a linear regression model to the DataFrame.\n\n    Parameters:\n    x (float): The value to find.\n    df (pandas.DataFrame): The DataFrame.\n\n    Returns:\n    tuple: A tuple containing a dictionary with column names as keys and \n        list of indices as values, and a LinearRegression object.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - sklearn.linear_model\n\n    Example:\n    >>> df = pd.DataFrame({'X': [1, 2, 3, 4, 5, 6],\n    ...                    'Y': [2, 3, 4, 5, 6, 7]})\n    >>> find_and_fit(4, df)\n    \"\"\"\n    results = {}\n    lr = LinearRegression()\n\n    for col in COLUMNS:\n        pos = np.where(df[col] == x)[0].tolist()\n        if pos:\n            results[col] = pos\n\n    X = df['X'].values.reshape(-1,1)\n    Y = df['Y'].values.reshape(-1,1)\n\n    lr.fit(X, Y)\n\n    Y_pred = lr.predict(X)\n\n    plt.scatter(X, Y, color = 'red')\n    plt.plot(X, Y_pred, color = 'blue')\n\n    return results, lr\n```\n"}
{"seed": "{\"task_id\": 148266, \"prompt\": \"def f_148266(df, type):\\n\\t\", \"suffix\": \"\\n\\treturn df1\", \"canonical_solution\": \"df1 = df.select_dtypes(include=[type])\", \"test_start\": \"\\nimport numpy as np\\nimport pandas as pd\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(pd.DataFrame({'a':[1,2,3], 'b':[1.,2.,3.]}), np.float).equals(pd.DataFrame({'b':[1.,2.,3.]}))\\n\"], \"entry_point\": \"f_148266\", \"intent\": \"\\u00bfC\\u00f3mo crear un dataframe `df1` a partir de otro dataframe `df` filtrando columnas que contengan \\u00fanicamente datos de tipo \\u00b4type\\u00b4?\", \"library\": [\"numpy\", \"pandas\"]}", "generation": "Scenario 1:\npandas, numpy, random, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nCATEGORIES = ['Electronics', 'Fashion', 'Grocery', 'Toys', 'Books']\nRATINGS = [1, 2, 3, 4, 5]\n\ndef generate_sales_report(df):\n    \"\"\"\n    Generate a report of sales and customer ratings for a list of product categories \n    from a given DataFrame and plot the average ratings for each category.\n    \n    Parameters:\n    df (DataFrame): The input DataFrame with columns - 'Category', 'Sales', 'Rating'.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with sales and average ratings for each category.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = pd.DataFrame({\n    ...     'Category': [CATEGORIES[randint(0, len(CATEGORIES)-1)] for _ in range(1000)],\n    ...     'Sales': [randint(100, 500) for _ in range(1000)],\n    ...     'Rating': [RATINGS[randint(0, len(RATINGS)-1)] for _ in range(1000)]\n    ... })\n    >>> report = generate_sales_report(df)\n    >>> print(report)\n    >>> report['Average Rating'].plot(kind='bar')\n    \"\"\"\n    report_df = df.groupby('Category').agg({'Sales': 'sum', 'Rating': 'mean'}).rename(columns={'Rating': 'Average Rating'})\n\n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, scikit-learn, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport seaborn as sns\n\n# Constants\nFEATURES = ['Age', 'Income', 'Education', 'Occupation']\n\ndef preprocess_and_visualize(df):\n    \"\"\"\n    Preprocess a DataFrame by scaling the numerical features and visualize the \n    correlation matrix of the features.\n    \n    Parameters:\n    df (DataFrame): The input DataFrame with columns - 'Age', 'Income', 'Education', 'Occupation'.\n    \n    Returns:\n    DataFrame: The preprocessed DataFrame.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.preprocessing\n    - seaborn\n\n    Example:\n    >>> df = pd.DataFrame({\n    ...     'Age': np.random.randint(20, 60, 100),\n    ...     'Income': np.random.randint(30000, 80000, 100),\n    ...     'Education': np.random.randint(1, 5, 100),\n    ...     'Occupation': np.random.randint(1, 5, 100)\n    ... })\n    >>> df_preprocessed = preprocess_and_visualize(df)\n    >>> print(df_preprocessed)\n    \"\"\"\n    scaler = MinMaxScaler()\n    df_scaled = scaler.fit_transform(df[FEATURES])\n    df_scaled = pd.DataFrame(df_scaled, columns=FEATURES)\n\n    sns.heatmap(df_scaled.corr(), annot=True, cmap='coolwarm')\n\n    return df_scaled\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, sklearn, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\n\n# Constants\nFEATURES = ['Age', 'Experience', 'Education', 'Skills']\n\ndef predict_salary(df):\n    \"\"\"\n    Predict the salary of employees based on their age, experience, education, and skills \n    using a simple linear regression model and plot the predicted vs actual values.\n    \n    Parameters:\n    df (DataFrame): The input DataFrame with columns - 'Age', 'Experience', 'Education', 'Skills', 'Salary'.\n    \n    Returns:\n    (LinearRegression, Series): The trained LinearRegression model and the predictions.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.model_selection\n    - sklearn.linear_model\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame({\n    ...     'Age': np.random.randint(20, 60, 100),\n    ...     'Experience': np.random.randint(1, 40, 100),\n    ...     'Education': np.random.randint(1, 5, 100),\n    ...     'Skills': np.random.randint(1, 5, 100),\n    ...     'Salary': np.random.randint(30000, 80000, 100)\n    ... })\n    >>> model, predictions = predict_salary(df)\n    >>> plt.scatter(df['Salary'], predictions)\n    \"\"\"\n    X = df[FEATURES]\n    y = df['Salary']\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    predictions = model.predict(X_test)\n\n    return model, pd.Series(predictions, index=X_test.index)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, sklearn, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cluster import KMeans\nimport seaborn as sns\n\n# Constants\nFEATURES = ['Income', 'Spending Score']\n\ndef customer_segmentation(df):\n    \"\"\"\n    Segment customers into different groups based on their income and spending score \n    using K-means clustering and visualize the clusters.\n    \n    Parameters:\n    df (DataFrame): The input DataFrame with columns - 'Income', 'Spending Score'.\n    \n    Returns:\n    (KMeans, Series): The trained KMeans model and the cluster labels.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.cluster\n    - seaborn\n\n    Example:\n    >>> df = pd.DataFrame({\n    ...     'Income': np.random.randint(30000, 80000, 100),\n    ...     'Spending Score': np.random.randint(1, 100, 100)\n    ... })\n    >>> model, labels = customer_segmentation(df)\n    >>> sns.scatterplot(x=df['Income'], y=df['Spending Score'], hue=labels, palette='viridis')\n    \"\"\"\n    kmeans = KMeans(n_clusters=5, random_state=42)\n    kmeans.fit(df[FEATURES])\n    labels = pd.Series(kmeans.labels_, index=df.index)\n\n    return kmeans, labels\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, sklearn, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\n# Constants\nFEATURES = ['Feature1', 'Feature2', 'Feature3', 'Feature4', 'Feature5']\n\ndef dimension_reduction(df):\n    \"\"\"\n    Reduce the dimensionality of a DataFrame with PCA and visualize the data in \n    the two principal components.\n    \n    Parameters:\n    df (DataFrame): The input DataFrame with columns - 'Feature1', 'Feature2', 'Feature3', 'Feature4', 'Feature5'.\n    \n    Returns:\n    (PCA, DataFrame): The PCA object and the DataFrame with the two principal components.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.decomposition\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame(np.random.rand(100, 5), columns=FEATURES)\n    >>> pca, df_pca = dimension_reduction(df)\n    >>> plt.scatter(df_pca['PC1'], df_pca['PC2'])\n    \"\"\"\n    pca = PCA(n_components=2)\n    pca.fit(df[FEATURES])\n    df_pca = pd.DataFrame(pca.transform(df[FEATURES]), columns=['PC1', 'PC2'])\n\n    return pca, df_pca\n```"}
{"seed": "{\"task_id\": 104440, \"prompt\": \"def f_104440(df):\\n\\t\", \"suffix\": \"\\n\\treturn new_df\", \"canonical_solution\": \"new_df = df.astype('str')\\n\\tnew_df[new_df == \\\"1\\\"] = 'Presente'\\n\\tnew_df[new_df == \\\"0\\\"] = 'Falta'\", \"test_start\": \"\\nimport pandas as pd \\n\\ndef check(candidate):\", \"test\": [\"\\n    df1 = pd.DataFrame({'a':[1,0,3], 'b': [0,1,4]})\\n    df2 = pd.DataFrame({'a':['sobra','falta',3], 'b': ['falta','sobra',4]})\\n    assert all(candidate(df1) == df2)\\n\"], \"entry_point\": \"f_104440\", \"intent\": \"\\u00bfC\\u00f3mo puedo sustituir valores `falta` si el valor es 0 y `sobra` si el valor es 1 '' en un dataframe `df`?\", \"library\": [\"pandas\"]}", "generation": "Scenario 1:\npandas, numpy, matplotlib, sklearn.preprocessing\n\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\n\n# Constants\nCATEGORIES = ['Presente', 'Falta', 'Other']\n\ndef preprocess_data(df):\n    \"\"\"\n    Preprocess the given DataFrame by replacing values 'Falta' if the value is 0, \n    'Presente' if the value is 1 and 'Other' for other values. Also, plot the count \n    of each category and encode them using LabelEncoder.\n\n    Parameters:\n    df (DataFrame): A pandas DataFrame.\n\n    Returns:\n    DataFrame: The preprocessed DataFrame.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.preprocessing.LabelEncoder\n\n    Example:\n    >>> df = pd.DataFrame({'a':[1, 0, 3], 'b': [0, 1, 4]})\n    >>> preprocess_data(df)\n    \"\"\"\n    new_df = df.astype('str')\n    new_df[new_df == \"1\"] = 'Presente'\n    new_df[new_df == \"0\"] = 'Falta'\n    new_df[(new_df != 'Presente') & (new_df != 'Falta')] = 'Other'\n    \n    plt.figure(figsize=(10,6))\n    new_df.apply(pd.Series.value_counts).sum(axis=1)[CATEGORIES].plot(kind='bar')\n    plt.show()\n    \n    le = LabelEncoder()\n    for col in new_df.columns:\n        new_df[col] = le.fit_transform(new_df[col])\n        \n    return new_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, seaborn, sklearn.preprocessing\n\n```python\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\n\n# Constants\nCATEGORIES = ['Presente', 'Falta', 'Other']\n\ndef analyze_data(df):\n    \"\"\"\n    Analyze the given DataFrame by replacing values 'Falta' if the value is 0, \n    'Presente' if the value is 1 and 'Other' for other values. Also, plot the \n    distribution of each category and encode them using LabelEncoder.\n\n    Parameters:\n    df (DataFrame): A pandas DataFrame.\n\n    Returns:\n    DataFrame: The analyzed DataFrame.\n\n    Requirements:\n    - pandas\n    - numpy\n    - seaborn\n    - sklearn.preprocessing.LabelEncoder\n\n    Example:\n    >>> df = pd.DataFrame({'a':[1, 0, 3], 'b': [0, 1, 4]})\n    >>> analyze_data(df)\n    \"\"\"\n    new_df = df.astype('str')\n    new_df[new_df == \"1\"] = 'Presente'\n    new_df[new_df == \"0\"] = 'Falta'\n    new_df[(new_df != 'Presente') & (new_df != 'Falta')] = 'Other'\n    \n    plt.figure(figsize=(10,6))\n    sns.countplot(x = 'variable', hue = 'value', data = pd.melt(new_df))\n    plt.show()\n    \n    le = LabelEncoder()\n    for col in new_df.columns:\n        new_df[col] = le.fit_transform(new_df[col])\n        \n    return new_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, matplotlib, sklearn.preprocessing, sklearn.model_selection\n\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\n# Constants\nCATEGORIES = ['Presente', 'Falta', 'Other']\n\ndef prepare_data(df):\n    \"\"\"\n    Prepare the given DataFrame for machine learning by replacing values 'Falta' \n    if the value is 0, 'Presente' if the value is 1 and 'Other' for other values. \n    Also, plot the count of each category, encode them using LabelEncoder and split \n    the DataFrame into train and test sets.\n\n    Parameters:\n    df (DataFrame): A pandas DataFrame.\n\n    Returns:\n    DataFrame, DataFrame: The train and test DataFrames.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.preprocessing.LabelEncoder\n    - sklearn.model_selection.train_test_split\n\n    Example:\n    >>> df = pd.DataFrame({'a':[1, 0, 3], 'b': [0, 1, 4]})\n    >>> train, test = prepare_data(df)\n    \"\"\"\n    new_df = df.astype('str')\n    new_df[new_df == \"1\"] = 'Presente'\n    new_df[new_df == \"0\"] = 'Falta'\n    new_df[(new_df != 'Presente') & (new_df != 'Falta')] = 'Other'\n    \n    plt.figure(figsize=(10,6))\n    new_df.apply(pd.Series.value_counts).sum(axis=1)[CATEGORIES].plot(kind='bar')\n    plt.show()\n    \n    le = LabelEncoder()\n    for col in new_df.columns:\n        new_df[col] = le.fit_transform(new_df[col])\n        \n    train, test = train_test_split(new_df, test_size=0.2, random_state=42)\n    \n    return train, test\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, seaborn, sklearn.preprocessing, sklearn.cluster\n\n```python\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.cluster import KMeans\n\n# Constants\nCATEGORIES = ['Presente', 'Falta', 'Other']\n\ndef cluster_data(df):\n    \"\"\"\n    Cluster the given DataFrame by replacing values 'Falta' if the value is 0, \n    'Presente' if the value is 1 and 'Other' for other values. Also, plot the \n    distribution of each category, encode them using LabelEncoder and perform \n    K-Means clustering.\n\n    Parameters:\n    df (DataFrame): A pandas DataFrame.\n\n    Returns:\n    DataFrame: The clustered DataFrame.\n\n    Requirements:\n    - pandas\n    - numpy\n    - seaborn\n    - sklearn.preprocessing.LabelEncoder\n    - sklearn.cluster.KMeans\n\n    Example:\n    >>> df = pd.DataFrame({'a':[1, 0, 3], 'b': [0, 1, 4]})\n    >>> cluster_data(df)\n    \"\"\"\n    new_df = df.astype('str')\n    new_df[new_df == \"1\"] = 'Presente'\n    new_df[new_df == \"0\"] = 'Falta'\n    new_df[(new_df != 'Presente') & (new_df != 'Falta')] = 'Other'\n    \n    plt.figure(figsize=(10,6))\n    sns.countplot(x = 'variable', hue = 'value', data = pd.melt(new_df))\n    plt.show()\n    \n    le = LabelEncoder()\n    for col in new_df.columns:\n        new_df[col] = le.fit_transform(new_df[col])\n        \n    kmeans = KMeans(n_clusters=3, random_state=42)\n    new_df['Cluster'] = kmeans.fit_predict(new_df)\n    \n    return new_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, matplotlib, sklearn.preprocessing, sklearn.decomposition\n\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.decomposition import PCA\n\n# Constants\nCATEGORIES = ['Presente', 'Falta', 'Other']\n\ndef reduce_dimension(df):\n    \"\"\"\n    Reduce the dimension of the given DataFrame by replacing values 'Falta' \n    if the value is 0, 'Presente' if the value is 1 and 'Other' for other values. \n    Also, plot the count of each category, encode them using LabelEncoder and perform \n    Principal Component Analysis (PCA).\n\n    Parameters:\n    df (DataFrame): A pandas DataFrame.\n\n    Returns:\n    DataFrame: The DataFrame with reduced dimension.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.preprocessing.LabelEncoder\n    - sklearn.decomposition.PCA\n\n    Example:\n    >>> df = pd.DataFrame({'a':[1, 0, 3], 'b': [0, 1, 4]})\n    >>> reduce_dimension(df)\n    \"\"\"\n    new_df = df.astype('str')\n    new_df[new_df == \"1\"] = 'Presente'\n    new_df[new_df == \"0\"] = 'Falta'\n    new_df[(new_df != 'Presente') & (new_df != 'Falta')] = 'Other'\n    \n    plt.figure(figsize=(10,6))\n    new_df.apply(pd.Series.value_counts).sum(axis=1)[CATEGORIES].plot(kind='bar')\n    plt.show()\n    \n    le = LabelEncoder()\n    for col in new_df.columns:\n        new_df[col] = le.fit_transform(new_df[col])\n        \n    pca = PCA(n_components=1)\n    new_df['PCA'] = pca.fit_transform(new_df)\n    \n    return new_df\n```"}
{"seed": "{\"task_id\": 33325, \"prompt\": \"def f_33325(num):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"num == 0 or num % ((10**int(log10(abs(num))+1) - 1) // 9) == 0\", \"test_start\": \"\\nimport math\\nfrom math import log10\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(1111) == True\\n\", \"\\n    assert candidate(1211) == False\\n\", \"\\n    assert candidate(0) == True\\n\"], \"entry_point\": \"f_33325\", \"intent\": \"\\u00bfC\\u00f3mo determinar si todos los d\\u00edgitos de un n\\u00famero entero `num` son iguales?\", \"library\": [\"math\"]}", "generation": "Scenario 1: \nnumpy, math, matplotlib, random\n```python\nimport numpy as np\nimport math\nfrom matplotlib import pyplot as plt\nfrom random import randint\n\n# Constants\nNUM_SAMPLES = 1000\n\ndef plot_numbers_with_equal_digits():\n    \"\"\"\n    Generate a random set of numbers and plot a histogram of the numbers where all \n    digits are the same.\n\n    Requirements:\n    - numpy\n    - math\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> plot_numbers_with_equal_digits()\n    \"\"\"\n    numbers = [randint(0, 99999) for _ in range(NUM_SAMPLES)]\n    equal_digit_numbers = [num for num in numbers if num == 0 or num % ((10**int(math.log10(abs(num))+1) - 1) // 9) == 0]\n    plt.hist(equal_digit_numbers, bins=50)\n    plt.show()\n\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, pandas, math, seaborn\n```python\nimport numpy as np\nimport pandas as pd\nimport math\nimport seaborn as sns\n\n# Constants\nNUM_SAMPLES = 1000\nCOLUMN_NAME = 'Number'\n\ndef create_equal_digit_df():\n    \"\"\"\n    Generate a random set of numbers and create a pandas DataFrame of the numbers where all \n    digits are the same.\n\n    Returns:\n    DataFrame: A pandas DataFrame with numbers with equal digits.\n\n    Requirements:\n    - pandas\n    - numpy\n    - math\n    - seaborn\n\n    Example:\n    >>> df = create_equal_digit_df()\n    >>> sns.countplot(x=COLUMN_NAME, data=df)\n    \"\"\"\n    numbers = [np.random.randint(0, 99999) for _ in range(NUM_SAMPLES)]\n    equal_digit_numbers = [num for num in numbers if num == 0 or num % ((10**int(math.log10(abs(num))+1) - 1) // 9) == 0]\n    df = pd.DataFrame(equal_digit_numbers, columns=[COLUMN_NAME])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, math, matplotlib, seaborn\n```python\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nNUM_SAMPLES = 1000\n\ndef generate_and_plot_equal_digit_numbers():\n    \"\"\"\n    Generate a random set of numbers and plot a histogram of the numbers where all \n    digits are the same using seaborn.\n\n    Requirements:\n    - numpy\n    - math\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> generate_and_plot_equal_digit_numbers()\n    \"\"\"\n    numbers = [np.random.randint(0, 99999) for _ in range(NUM_SAMPLES)]\n    equal_digit_numbers = [num for num in numbers if num == 0 or num % ((10**int(math.log10(abs(num))+1) - 1) // 9) == 0]\n    sns.histplot(equal_digit_numbers, bins=50)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, math, pandas, seaborn\n```python\nimport numpy as np\nimport math\nimport pandas as pd\nimport seaborn as sns\n\n# Constants\nNUM_SAMPLES = 1000\nCOLUMN_NAME = 'Number'\n\ndef create_and_plot_equal_digit_df():\n    \"\"\"\n    Generate a random set of numbers and create a pandas DataFrame of the numbers where all \n    digits are the same and plot it using seaborn.\n\n    Returns:\n    DataFrame: A pandas DataFrame with numbers with equal digits.\n\n    Requirements:\n    - pandas\n    - numpy\n    - math\n    - seaborn\n\n    Example:\n    >>> df = create_and_plot_equal_digit_df()\n    \"\"\"\n    numbers = [np.random.randint(0, 99999) for _ in range(NUM_SAMPLES)]\n    equal_digit_numbers = [num for num in numbers if num == 0 or num % ((10**int(math.log10(abs(num))+1) - 1) // 9) == 0]\n    df = pd.DataFrame(equal_digit_numbers, columns=[COLUMN_NAME])\n    sns.countplot(x=COLUMN_NAME, data=df)\n    plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, math, matplotlib, random\n```python\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nNUM_SAMPLES = 1000\n\ndef plot_random_numbers_with_equal_digits():\n    \"\"\"\n    Generate a histogram of a random set of numbers where all digits are the same.\n\n    Requirements:\n    - numpy\n    - math\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> plot_random_numbers_with_equal_digits()\n    \"\"\"\n    numbers = [randint(0, 99999) for _ in range(NUM_SAMPLES)]\n    equal_digit_numbers = [num for num in numbers if num == 0 or num % ((10**int(math.log10(abs(num))+1) - 1) // 9) == 0]\n    plt.hist(equal_digit_numbers, bins=50)\n    plt.show()\n```\n"}
{"seed": "{\"task_id\": 314, \"prompt\": \"def f_314(a, b):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"tuple(sum(x) for x in zip(a, b))\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate((1,2,3),(4,5,6)) == (5,7,9)\\n\", \"\\n    assert candidate((1,2,3),(4,5,6)) != [5,7,9]\\n\"], \"entry_point\": \"f_314\", \"intent\": \"\\u00bfC\\u00f3mo sumar dos vectores `a` y `b` representados por tuplas?\", \"library\": []}", "generation": "Scenario 1:\nnumpy, itertools, math, random\n```python\nimport numpy as np\nimport itertools\nimport math\nimport random\n\n# Constants\nVECTOR_DIMENSION = 3\n\ndef generate_random_vectors_and_calculate_dot_product(n):\n    \"\"\"\n    Generate n random vectors of a certain dimension and calculate the dot product \n    for every pair of vectors.\n\n    Parameters:\n    n (int): The number of vectors to generate.\n\n    Returns:\n    dict: A dictionary where the keys are pairs of vectors and the values are their dot products.\n\n    Requirements:\n    - numpy\n    - itertools\n    - math\n    - random\n\n    Example:\n    >>> dot_products = generate_random_vectors_and_calculate_dot_product(5)\n    >>> print(dot_products)\n    \"\"\"\n    vectors = [tuple(random.uniform(-1, 1) for _ in range(VECTOR_DIMENSION)) for _ in range(n)]\n\n    dot_products = {}\n\n    for pair in itertools.combinations(vectors, 2):\n        dot_products[pair] = np.dot(pair[0], pair[1])\n\n    return dot_products\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, matplotlib.pyplot\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nVECTORS = [(1,2,3), (4,5,6), (7,8,9), (10,11,12), (13,14,15)]\n\ndef plot_vectors_and_their_sum(vectors):\n    \"\"\"\n    Plot the given vectors and their sum on a 3D plot.\n\n    Parameters:\n    vectors (list): A list of 3D vectors.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_vectors_and_their_sum(VECTORS)\n    \"\"\"\n    vectors = np.array(vectors)\n    vector_sum = np.sum(vectors, axis=0)\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n\n    for vector in vectors:\n        ax.quiver(0, 0, 0, vector[0], vector[1], vector[2], color='b')\n\n    ax.quiver(0, 0, 0, vector_sum[0], vector_sum[1], vector_sum[2], color='r')\n    ax.set_xlim([0, vector_sum[0]])\n    ax.set_ylim([0, vector_sum[1]])\n    ax.set_zlim([0, vector_sum[2]])\n\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, functools\n```python\nimport numpy as np\nimport functools\n\n# Constants\nVECTORS = [(1,2,3), (4,5,6), (7,8,9), (10,11,12), (13,14,15)]\n\ndef calculate_cumulative_sum_of_vectors(vectors):\n    \"\"\"\n    Calculate the cumulative sum of a list of vectors.\n\n    Parameters:\n    vectors (list): A list of vectors.\n\n    Returns:\n    list: A list of cumulative sums.\n\n    Requirements:\n    - numpy\n    - functools\n\n    Example:\n    >>> cumulative_sums = calculate_cumulative_sum_of_vectors(VECTORS)\n    >>> print(cumulative_sums)\n    \"\"\"\n    vectors = np.array(vectors)\n    cumulative_sums = np.cumsum(vectors, axis=0)\n\n    return cumulative_sums.tolist()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, matplotlib.pyplot\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nVECTORS = [(1,2,3), (4,5,6), (7,8,9), (10,11,12), (13,14,15)]\n\ndef plot_vectors_and_their_average(vectors):\n    \"\"\"\n    Plot the given vectors and their average on a 3D plot.\n\n    Parameters:\n    vectors (list): A list of 3D vectors.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_vectors_and_their_average(VECTORS)\n    \"\"\"\n    vectors = np.array(vectors)\n    vector_avg = np.mean(vectors, axis=0)\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n\n    for vector in vectors:\n        ax.quiver(0, 0, 0, vector[0], vector[1], vector[2], color='b')\n\n    ax.quiver(0, 0, 0, vector_avg[0], vector_avg[1], vector_avg[2], color='r')\n    ax.set_xlim([0, vector_avg[0]])\n    ax.set_ylim([0, vector_avg[1]])\n    ax.set_zlim([0, vector_avg[2]])\n\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, math, random\n```python\nimport numpy as np\nimport math\nimport random\n\n# Constants\nVECTOR_DIMENSION = 3\n\ndef generate_random_vectors_and_calculate_angle(n):\n    \"\"\"\n    Generate n random vectors of a certain dimension and calculate the angle \n    between every pair of vectors.\n\n    Parameters:\n    n (int): The number of vectors to generate.\n\n    Returns:\n    dict: A dictionary where the keys are pairs of vectors and the values are the angles between them.\n\n    Requirements:\n    - numpy\n    - math\n    - random\n\n    Example:\n    >>> angles = generate_random_vectors_and_calculate_angle(5)\n    >>> print(angles)\n    \"\"\"\n    vectors = [tuple(random.uniform(-1, 1) for _ in range(VECTOR_DIMENSION)) for _ in range(n)]\n\n    angles = {}\n\n    for i in range(n):\n        for j in range(i+1, n):\n            angle = math.acos(np.dot(vectors[i], vectors[j]) / (np.linalg.norm(vectors[i]) * np.linalg.norm(vectors[j])))\n            angles[(vectors[i], vectors[j])] = np.degrees(angle)\n\n    return angles\n```"}
{"seed": "{\"task_id\": 87732, \"prompt\": \"def f_87732(dic):\\n\\t\", \"suffix\": \"\\n\\treturn dic2\", \"canonical_solution\": \"dic2 = {key: [sum(value)] for key, value in dic.items()}\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate({'11': [1,2,3], '22': [34.0, 1]}) == {'11': [6], '22': [35.0]}\\n\", \"\\n    assert candidate({0: [10,20,-10,-20]}) == {0: [0]}\\n\", \"\\n    assert candidate({'random_key': [10,20,-10,-20], 'random_key1': [-1,-2]})            == {'random_key': [0], 'random_key1': [-3]}\\n\", \"\\n    assert candidate({1: [0]}) == {1: [0]}\\n\", \"\\n    assert candidate({1: [], 2:[2]}) == {1: [0], 2: [2]}\\n\"], \"entry_point\": \"f_87732\", \"intent\": \"\\u00bfC\\u00f3mo crear un diccionario `dic2` con la suma de los valores de cada una de las llaves en un diccionario `dic`?\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, matplotlib, random\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import choice\n\n# Constants\nCATEGORIES = ['Electronics', 'Furniture', 'Apparel', 'Toys', 'Groceries']\nPRODUCTS = ['Product1', 'Product2', 'Product3', 'Product4', 'Product5']\n\ndef generate_sales_report(sales_dict):\n    \"\"\"\n    Generate a report of sales for a list of products in different categories and visualize \n    the sales using a bar chart.\n    \n    Parameters:\n    sales_dict (dict): The dictionary of product sales with categories as keys \n                       and list of sales as values.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with sales for the products.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - random\n    \n    Example:\n    >>> sales = {'Electronics': [1000, 2000, 1500, 1800, 1700], 'Furniture': [1200, 2300, 1600, 1700, 2100]}\n    >>> report = generate_sales_report(sales)\n    >>> print(report)\n    >>> report.plot.bar()\n    \"\"\"\n    report_data = []\n\n    for category in CATEGORIES:\n        sales = sales_dict.get(category, [0]*len(PRODUCTS))\n        for product, sale in zip(PRODUCTS, sales):\n            report_data.append([category, product, sale])\n\n    report_df = pd.DataFrame(report_data, columns=['Category', 'Product', 'Sales'])\n\n    report_df = report_df.pivot(index='Product', columns='Category', values='Sales')\n\n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, matplotlib, random\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nTEAMS = ['Team1', 'Team2', 'Team3', 'Team4', 'Team5']\n\ndef generate_match_results(match_dict):\n    \"\"\"\n    Generate a DataFrame of match results for different teams and visualize \n    the results using a bar chart.\n    \n    Parameters:\n    match_dict (dict): The dictionary of match results with teams as keys \n                       and list of runs as values.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with match results for the teams.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - random\n    \n    Example:\n    >>> match_results = {'Team1': [200, 150, 180, 220, 210], 'Team2': [180, 230, 160, 170, 210]}\n    >>> report = generate_match_results(match_results)\n    >>> print(report)\n    >>> report.plot.bar()\n    \"\"\"\n    report_data = []\n\n    for team in TEAMS:\n        runs = match_dict.get(team, [0]*5)\n        for match, run in enumerate(runs, start=1):\n            report_data.append([team, f'Match {match}', run])\n\n    report_df = pd.DataFrame(report_data, columns=['Team', 'Match', 'Runs'])\n\n    report_df = report_df.pivot(index='Match', columns='Team', values='Runs')\n\n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \npandas, matplotlib, random\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nSTUDENTS = ['Student1', 'Student2', 'Student3', 'Student4', 'Student5']\nSUBJECTS = ['Math', 'Science', 'English', 'History', 'Geography']\n\ndef generate_student_report(grades_dict):\n    \"\"\"\n    Generate a report of student grades for different subjects and visualize \n    the grades using a bar chart.\n    \n    Parameters:\n    grades_dict (dict): The dictionary of student grades with students as keys \n                        and list of grades as values.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with student grades.\n    \n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - random\n    \n    Example:\n    >>> grades = {'Student1': [90, 85, 88, 92, 86], 'Student2': [78, 82, 80, 76, 84]}\n    >>> report = generate_student_report(grades)\n    >>> print(report)\n    >>> report.plot.bar()\n    \"\"\"\n    report_data = []\n\n    for student in STUDENTS:\n        grades = grades_dict.get(student, [0]*len(SUBJECTS))\n        for subject, grade in zip(SUBJECTS, grades):\n            report_data.append([student, subject, grade])\n\n    report_df = pd.DataFrame(report_data, columns=['Student', 'Subject', 'Grade'])\n\n    report_df = report_df.pivot(index='Subject', columns='Student', values='Grade')\n\n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \npandas, matplotlib, random\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nEMPLOYEES = ['Employee1', 'Employee2', 'Employee3', 'Employee4', 'Employee5']\nMONTHS = ['January', 'February', 'March', 'April', 'May']\n\ndef generate_sales_report(sales_dict):\n    \"\"\"\n    Generate a report of sales for different employees across various months and visualize \n    the sales using a bar chart.\n    \n    Parameters:\n    sales_dict (dict): The dictionary of employee sales with employees as keys \n                       and list of sales as values.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with sales for the employees.\n    \n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - random\n    \n    Example:\n    >>> sales = {'Employee1': [1000, 2000, 1500, 1800, 1700], 'Employee2': [1200, 2300, 1600, 1700, 2100]}\n    >>> report = generate_sales_report(sales)\n    >>> print(report)\n    >>> report.plot.bar()\n    \"\"\"\n    report_data = []\n\n    for employee in EMPLOYEES:\n        sales = sales_dict.get(employee, [0]*len(MONTHS))\n        for month, sale in zip(MONTHS, sales):\n            report_data.append([employee, month, sale])\n\n    report_df = pd.DataFrame(report_data, columns=['Employee', 'Month', 'Sales'])\n\n    report_df = report_df.pivot(index='Month', columns='Employee', values='Sales')\n\n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \npandas, matplotlib, random\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nCARS = ['Car1', 'Car2', 'Car3', 'Car4', 'Car5']\nMONTHS = ['January', 'February', 'March', 'April', 'May']\n\ndef generate_sales_report(sales_dict):\n    \"\"\"\n    Generate a report of car sales for different car models across various months and visualize \n    the sales using a bar chart.\n    \n    Parameters:\n    sales_dict (dict): The dictionary of car sales with car models as keys \n                       and list of sales as values.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with sales for the car models.\n    \n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - random\n    \n    Example:\n    >>> sales = {'Car1': [1000, 2000, 1500, 1800, 1700], 'Car2': [1200, 2300, 1600, 1700, 2100]}\n    >>> report = generate_sales_report(sales)\n    >>> print(report)\n    >>> report.plot.bar()\n    \"\"\"\n    report_data = []\n\n    for car in CARS:\n        sales = sales_dict.get(car, [0]*len(MONTHS))\n        for month, sale in zip(MONTHS, sales):\n            report_data.append([car, month, sale])\n\n    report_df = pd.DataFrame(report_data, columns=['Car Model', 'Month', 'Sales'])\n\n    report_df = report_df.pivot(index='Month', columns='Car Model', values='Sales')\n\n    return report_df\n```\n"}
{"seed": "{\"task_id\": 87732, \"prompt\": \"def f_87732(dic):\\n\\t\", \"suffix\": \"\\n\\treturn dic2\", \"canonical_solution\": \"\\n\\tdef suma_valores_dic(dic):\\n\\t\\tdic2 = {}\\n\\t\\tfor key, value in dic.items():\\n\\t\\t\\tsuma = 0\\n\\t\\t\\tfor v in value:\\n\\t\\t\\t\\tsuma += v\\n\\t\\t\\tdic2[key] = suma\\n\\t\\treturn dic2\\n\\tdic2 = suma_valores_dic(dic)\\n\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate({'11': [1,2,3], '22': [34.0, 1]}) == {'11': 6, '22': 35.0}\\n\", \"\\n    assert candidate({0: [10,20,-10,-20]}) == {0: 0}\\n\", \"\\n    assert candidate({'random_key': [10,20,-10,-20], 'random_key1': [-1,-2]})            == {'random_key': 0, 'random_key1': -3}\\n\", \"\\n    assert candidate({1: [0]}) == {1: 0}\\n\", \"\\n    assert candidate({1: [], 2:[2]}) == {1: 0, 2: 2}\\n\"], \"entry_point\": \"f_87732\", \"intent\": \"\\u00bfC\\u00f3mo crear un diccionario `dic2` con la suma de los valores de cada una de las llaves en un diccionario `dic`?\", \"library\": []}", "generation": "Scenario 1:\npandas, numpy, scipy.stats, collections\n```python\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nfrom collections import Counter\n\n# Constants\nNUMERIC_TYPES = [np.int, np.int8, np.int16, np.int32, np.int64, np.float, np.float16, np.float32, np.float64]\n\ndef summarize_numeric_data(dic):\n    \"\"\"\n    Create a dictionary 'dic2' that summarizes numeric data in a dictionary 'dic'.\n    For each key in the input dictionary, the function computes the sum, mean, median, mode, variance and standard deviation.\n\n    Parameters:\n    dic (dict): The dictionary that contains the data. Each key has a list of numeric values as value.\n\n    Returns:\n    dict: A dictionary where each key is the same as in the input dictionary and the value is another dictionary with\n    the sum, mean, median, mode, variance and standard deviation of the values.\n\n    Requirements:\n    - pandas\n    - numpy\n    - scipy.stats\n    - collections\n\n    Example:\n    >>> summarize_numeric_data({'key1': [1, 2, 3, 4, 5], 'key2': [6, 7, 8, 9, 10]})\n    \"\"\"\n    dic2 = {}\n\n    for key, values in dic.items():\n        if all(isinstance(v, tuple(NUMERIC_TYPES)) for v in values):\n            dic2[key] = {\n                'sum': np.sum(values),\n                'mean': np.mean(values),\n                'median': np.median(values),\n                'mode': stats.mode(values).mode[0],\n                'variance': np.var(values),\n                'std_dev': np.std(values)\n            }\n\n    return dic2\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, statistics, collections\n```python\nimport pandas as pd\nimport numpy as np\nimport statistics\nfrom collections import Counter\n\n# Constants\nNUMERIC_TYPES = [np.int, np.int8, np.int16, np.int32, np.int64, np.float, np.float16, np.float32, np.float64]\n\ndef summarize_numeric_data(dic):\n    \"\"\"\n    Create a dictionary 'dic2' that summarizes numeric data in a dictionary 'dic'.\n    For each key in the input dictionary, the function computes the sum, mean, median, mode, variance and standard deviation.\n\n    Parameters:\n    dic (dict): The dictionary that contains the data. Each key has a list of numeric values as value.\n\n    Returns:\n    dict: A dictionary where each key is the same as in the input dictionary and the value is another dictionary with\n    the sum, mean, median, mode, variance and standard deviation of the values.\n\n    Requirements:\n    - pandas\n    - numpy\n    - statistics\n    - collections\n\n    Example:\n    >>> summarize_numeric_data({'key1': [1, 2, 3, 4, 5], 'key2': [6, 7, 8, 9, 10]})\n    \"\"\"\n    dic2 = {}\n\n    for key, values in dic.items():\n        if all(isinstance(v, tuple(NUMERIC_TYPES)) for v in values):\n            dic2[key] = {\n                'sum': sum(values),\n                'mean': statistics.mean(values),\n                'median': statistics.median(values),\n                'mode': Counter(values).most_common(1)[0][0],\n                'variance': statistics.variance(values),\n                'std_dev': statistics.stdev(values)\n            }\n\n    return dic2\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, sklearn.preprocessing, collections\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nfrom collections import Counter\n\n# Constants\nNUMERIC_TYPES = [np.int, np.int8, np.int16, np.int32, np.int64, np.float, np.float16, np.float32, np.float64]\n\ndef normalize_and_summarize(dic):\n    \"\"\"\n    Create a dictionary 'dic2' that normalizes and summarizes numeric data in a dictionary 'dic'.\n    For each key in the input dictionary, the function computes the sum, mean, min, max, range of normalized values.\n\n    Parameters:\n    dic (dict): The dictionary that contains the data. Each key has a list of numeric values as value.\n\n    Returns:\n    dict: A dictionary where each key is the same as in the input dictionary and the value is another dictionary with\n    the sum, mean, min, max, range of normalized values.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.preprocessing\n    - collections\n\n    Example:\n    >>> normalize_and_summarize({'key1': [1, 2, 3, 4, 5], 'key2': [6, 7, 8, 9, 10]})\n    \"\"\"\n    dic2 = {}\n    scaler = MinMaxScaler()\n\n    for key, values in dic.items():\n        if all(isinstance(v, tuple(NUMERIC_TYPES)) for v in values):\n            normalized_values = scaler.fit_transform(np.array(values).reshape(-1, 1)).flatten()\n            dic2[key] = {\n                'sum': np.sum(normalized_values),\n                'mean': np.mean(normalized_values),\n                'min': np.min(normalized_values),\n                'max': np.max(normalized_values),\n                'range': np.ptp(normalized_values)\n            }\n\n    return dic2\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, scipy.stats, collections\n```python\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nfrom collections import Counter\n\n# Constants\nNUMERIC_TYPES = [np.int, np.int8, np.int16, np.int32, np.int64, np.float, np.float16, np.float32, np.float64]\n\ndef compute_skewness_kurtosis(dic):\n    \"\"\"\n    Create a dictionary 'dic2' that computes skewness and kurtosis of numeric data in a dictionary 'dic'.\n    For each key in the input dictionary, the function computes the skewness and kurtosis.\n\n    Parameters:\n    dic (dict): The dictionary that contains the data. Each key has a list of numeric values as value.\n\n    Returns:\n    dict: A dictionary where each key is the same as in the input dictionary and the value is another dictionary with\n    the skewness and kurtosis of the values.\n\n    Requirements:\n    - pandas\n    - numpy\n    - scipy.stats\n    - collections\n\n    Example:\n    >>> compute_skewness_kurtosis({'key1': [1, 2, 3, 4, 5], 'key2': [6, 7, 8, 9, 10]})\n    \"\"\"\n    dic2 = {}\n\n    for key, values in dic.items():\n        if all(isinstance(v, tuple(NUMERIC_TYPES)) for v in values):\n            dic2[key] = {\n                'skewness': stats.skew(values),\n                'kurtosis': stats.kurtosis(values)\n            }\n\n    return dic2\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, scipy.stats, collections\n```python\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nfrom collections import Counter\n\n# Constants\nNUMERIC_TYPES = [np.int, np.int8, np.int16, np.int32, np.int64, np.float, np.float16, np.float32, np.float64]\n\ndef compute_percentiles(dic):\n    \"\"\"\n    Create a dictionary 'dic2' that computes percentiles of numeric data in a dictionary 'dic'.\n    For each key in the input dictionary, the function computes the 25th, 50th, and 75th percentiles.\n\n    Parameters:\n    dic (dict): The dictionary that contains the data. Each key has a list of numeric values as value.\n\n    Returns:\n    dict: A dictionary where each key is the same as in the input dictionary and the value is another dictionary with\n    the 25th, 50th, and 75th percentiles of the values.\n\n    Requirements:\n    - pandas\n    - numpy\n    - scipy.stats\n    - collections\n\n    Example:\n    >>> compute_percentiles({'key1': [1, 2, 3, 4, 5], 'key2': [6, 7, 8, 9, 10]})\n    \"\"\"\n    dic2 = {}\n\n    for key, values in dic.items():\n        if all(isinstance(v, tuple(NUMERIC_TYPES)) for v in values):\n            dic2[key] = {\n                '25th percentile': np.percentile(values, 25),\n                '50th percentile': np.percentile(values, 50),\n                '75th percentile': np.percentile(values, 75)\n            }\n\n    return dic2\n```\nAbove is the illustration."}
{"seed": "{\"task_id\": 107612, \"prompt\": \"def f_107612(file1, file2, file3):\\n\\t\", \"suffix\": \"\\n\\treturn process1, process2, process3\", \"canonical_solution\": \"process1 = subprocess.Popen(['python', file1])\\n\\tprocess2 = subprocess.Popen(['python', file2])\\n\\tprocess3 = subprocess.Popen(['python', file3])\", \"test_start\": \"\\nimport subprocess\\nfrom unittest.mock import Mock\\n\\ndef check(candidate):\", \"test\": [\"\\n    subprocess.Popen = Mock(return_value = 0)\\n    assert candidate(\\\"archive1.py\\\", \\\"archive2.py\\\", \\\"archive3.py\\\") == (0, 0, 0)\\n\"], \"entry_point\": \"f_107612\", \"intent\": \"\\u00bfC\\u00f3mo ejecutar los archivos `file1`, `file2` y `file3` en secuencia?\", \"library\": [\"subprocess\"]}", "generation": "Scenario 1: \nsubprocess, os, shutil, glob\n```python\nimport subprocess\nimport os\nimport shutil\nimport glob\n\n# Constants\nPYTHON_EXE = \"python\"\nARCHIVE_FILES = \"*.py\"\n\ndef archive_and_run_files(directory):\n    \"\"\"\n    Archive all .py files in a directory, move them to a new directory, and run them in a sequential manner.\n    \n    Parameters:\n    directory (str): The directory where the .py files are located.\n    \n    Returns:\n    List[str]: A list of outputs from each script.\n    \n    Requirements:\n    - subprocess\n    - os\n    - shutil\n    - glob\n    \n    Example:\n    >>> archive_and_run_files('/path/to/directory')\n    \"\"\"\n    # Archive files\n    files = glob.glob(os.path.join(directory, ARCHIVE_FILES))\n    for file in files:\n        shutil.move(file, 'archived_scripts')\n        \n    # Run files\n    outputs = []\n    for file in os.listdir('archived_scripts'):\n        if file.endswith('.py'):\n            process = subprocess.Popen([PYTHON_EXE, os.path.join('archived_scripts', file)], stdout=subprocess.PIPE)\n            output, _ = process.communicate()\n            outputs.append(output.decode('utf-8'))\n            \n    return outputs\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nsubprocess, csv, pandas, matplotlib.pyplot\n```python\nimport subprocess\nimport csv\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nPYTHON_EXE = \"python\"\nSCRIPTS = ['script1.py', 'script2.py', 'script3.py']\n\ndef run_scripts_and_plot_results():\n    \"\"\"\n    Execute a series of Python scripts, each returning a CSV file with two columns: 'x' and 'y'. \n    Read the results and plot the 'y' values against the 'x' values.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - subprocess\n    - csv\n    - pandas\n    - matplotlib.pyplot\n    \n    Example:\n    >>> run_scripts_and_plot_results()\n    \"\"\"\n    for script in SCRIPTS:\n        subprocess.run([PYTHON_EXE, script])\n        \n    fig, ax = plt.subplots()\n    \n    for script in SCRIPTS:\n        df = pd.read_csv(script.replace('.py', '.csv'))\n        ax.plot(df['x'], df['y'], label=script)\n        \n    plt.legend()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nsubprocess, os, time, pathlib\n```python\nimport subprocess\nimport os\nimport time\nfrom pathlib import Path\n\n# Constants\nPYTHON_EXE = \"python\"\nSCRIPTS = ['script1.py', 'script2.py', 'script3.py']\n\ndef run_scripts_time_log():\n    \"\"\"\n    Execute a series of Python scripts and write the execution time of each to a log file.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - subprocess\n    - os\n    - time\n    - pathlib.Path\n    \n    Example:\n    >>> run_scripts_time_log()\n    \"\"\"\n    with open('time_log.txt', 'w') as f:\n        for script in SCRIPTS:\n            start_time = time.time()\n            subprocess.run([PYTHON_EXE, script])\n            end_time = time.time()\n            f.write(f\"{Path(script).name} executed in {end_time - start_time} seconds.\\n\")\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nsubprocess, multiprocessing, time, random\n```python\nimport subprocess\nimport multiprocessing\nimport time\nimport random\n\n# Constants\nPYTHON_EXE = \"python\"\nSCRIPTS = ['script1.py', 'script2.py', 'script3.py']\n\ndef run_scripts_parallel():\n    \"\"\"\n    Execute a series of Python scripts in parallel and return the execution time.\n    \n    Returns:\n    float: The execution time in seconds.\n    \n    Requirements:\n    - subprocess\n    - multiprocessing\n    - time\n    - random\n    \n    Example:\n    >>> run_scripts_parallel()\n    \"\"\"\n    start_time = time.time()\n    \n    processes = []\n    for script in SCRIPTS:\n        p = multiprocessing.Process(target=subprocess.run, args=([PYTHON_EXE, script],))\n        p.start()\n        processes.append(p)\n        \n    for p in processes:\n        p.join()\n        \n    end_time = time.time()\n    \n    return end_time - start_time\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nsubprocess, os, sys, fnmatch\n```python\nimport subprocess\nimport os\nimport sys\nimport fnmatch\n\n# Constants\nPYTHON_EXE = \"python\"\n\ndef run_all_py_files(directory):\n    \"\"\"\n    Navigate to a directory and run all Python files in it.\n    \n    Parameters:\n    directory (str): The directory where the .py files are located.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - subprocess\n    - os\n    - sys\n    - fnmatch\n    \n    Example:\n    >>> run_all_py_files('/path/to/directory')\n    \"\"\"\n    for file in os.listdir(directory):\n        if fnmatch.fnmatch(file, '*.py'):\n            subprocess.run([PYTHON_EXE, os.path.join(directory, file)])\n```\nAbove is the illustration."}
{"seed": "{\"task_id\": 129553, \"prompt\": \"def f_129553(a, b):\\n\\t\", \"suffix\": \"\\n\\treturn listas(a, b)\", \"canonical_solution\": \"\\n\\tdef listas(a, b):\\n\\t\\tlista_final = []\\n\\t\\tfor i in a:\\n\\t\\t\\tif (i not in lista_final) and (i in b):\\n\\t\\t\\t\\tlista_final.append(i)\\n\\t\\treturn lista_final\\n\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([1,2,3,2,4,5], [7,2,5,8,10]) == [2, 5]\\n\"], \"entry_point\": \"f_129553\", \"intent\": \"\\u00bfC\\u00f3mo obtener los elementos comunes de dos listas `a` y `b`?\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, random, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nDATA_COLUMNS = ['A', 'B', 'C', 'D', 'E']\nDATA_SIZE = 100\n\ndef generate_data_and_plot_common_elements():\n    \"\"\"\n    Generate two random dataframes and plot the common elements in 'A' column of both dataframes. \n\n    Requirements:\n    - pandas\n    - numpy\n    - random\n    - matplotlib.pyplot\n    \n    Returns:\n    tuple: A tuple containing the two dataframes and a matplotlib figure.\n\n    Example:\n    >>> df1, df2, fig = generate_data_and_plot_common_elements()\n    >>> plt.show()\n    \"\"\"\n    # Generate two random DataFrames\n    df1 = pd.DataFrame(np.random.randint(0,100,size=(DATA_SIZE, len(DATA_COLUMNS))), columns=DATA_COLUMNS)\n    df2 = pd.DataFrame(np.random.randint(0,100,size=(DATA_SIZE, len(DATA_COLUMNS))), columns=DATA_COLUMNS)\n\n    # Find common elements\n    common_elements = np.intersect1d(df1['A'].values, df2['A'].values)\n\n    # Plot common elements\n    fig, ax = plt.subplots()\n    ax.hist(common_elements, bins=10, edgecolor='black')\n    ax.set_title('Common Elements in Column A')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    return df1, df2, fig\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \ncollections, itertools, string\n```python\nfrom collections import Counter\nfrom itertools import chain\nimport string\n\n# Constants\nPUNCTUATION = string.punctuation\n\ndef count_common_elements(str1, str2):\n    \"\"\"\n    Count the common character elements (excluding punctuation) in two strings and return the five most common.\n\n    Parameters:\n    str1 (str): The first string.\n    str2 (str): The second string.\n\n    Returns:\n    list: A list of tuples with the five most common characters and their counts.\n\n    Requirements:\n    - collections\n    - itertools\n    - string\n\n    Example:\n    >>> count_common_elements('Hello, World!', 'Python is great!')\n    \"\"\"\n    # Remove punctuation and convert to lowercase\n    str1 = ''.join(ch for ch in str1 if ch not in PUNCTUATION).lower()\n    str2 = ''.join(ch for ch in str2 if ch not in PUNCTUATION).lower()\n\n    # Count elements\n    count1 = Counter(str1)\n    count2 = Counter(str2)\n\n    # Find common elements\n    common_counts = (count1 & count2).most_common(5)\n\n    return common_counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnumpy, pandas, sklearn.preprocessing, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['A', 'B']\n\ndef scale_and_plot_common_elements(df1, df2):\n    \"\"\"\n    Scale the columns 'A' and 'B' in two dataframes, find the common elements, and plot them.\n\n    Parameters:\n    df1 (DataFrame): The first dataframe.\n    df2 (DataFrame): The second dataframe.\n\n    Returns:\n    figure: A matplotlib figure.\n\n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.preprocessing\n    - matplotlib.pyplot\n\n    Example:\n    >>> df1 = pd.DataFrame(np.random.rand(50, 2), columns=['A', 'B'])\n    >>> df2 = pd.DataFrame(np.random.rand(50, 2), columns=['A', 'B'])\n    >>> fig = scale_and_plot_common_elements(df1, df2)\n    >>> plt.show()\n    \"\"\"\n    # Scale the dataframes\n    scaler = StandardScaler()\n    df1_scaled = pd.DataFrame(scaler.fit_transform(df1), columns=COLUMNS)\n    df2_scaled = pd.DataFrame(scaler.transform(df2), columns=COLUMNS)\n\n    # Find common elements\n    common_elements = pd.merge(df1_scaled, df2_scaled, how='inner')\n\n    # Plot common elements\n    fig, ax = plt.subplots()\n    ax.scatter(common_elements['A'], common_elements['B'])\n    ax.set_title('Common Scaled Elements')\n    ax.set_xlabel('A')\n    ax.set_ylabel('B')\n\n    return fig\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \ncollections, random, statistics\n```python\nfrom collections import Counter\nfrom random import choices\nimport statistics\n\n# Constants\nPOPULATION = list('abcdefghijklmnopqrstuvwxyz')\nWEIGHTS = [1]*26\nK = 1000\n\ndef get_common_elements():\n    \"\"\"\n    Generate two random lists from a population and return the mode of the common elements.\n\n    Returns:\n    str: The mode of the common elements.\n\n    Requirements:\n    - collections\n    - random\n    - statistics\n\n    Example:\n    >>> get_common_elements()\n    \"\"\"\n    # Generate two random lists\n    list1 = choices(POPULATION, weights=WEIGHTS, k=K)\n    list2 = choices(POPULATION, weights=WEIGHTS, k=K)\n\n    # Get common elements\n    common_elements = list((Counter(list1) & Counter(list2)).elements())\n\n    # Find and return the mode\n    mode = statistics.mode(common_elements)\n\n    return mode\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnumpy, pandas, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['A', 'B']\n\ndef plot_common_elements(df1, df2):\n    \"\"\"\n    Plot the common elements in columns 'A' and 'B' from two dataframes.\n\n    Parameters:\n    df1 (DataFrame): The first dataframe.\n    df2 (DataFrame): The second dataframe.\n\n    Returns:\n    figure: A matplotlib figure.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> df1 = pd.DataFrame(np.random.rand(50, 2), columns=['A', 'B'])\n    >>> df2 = pd.DataFrame(np.random.rand(50, 2), columns=['A', 'B'])\n    >>> fig = plot_common_elements(df1, df2)\n    >>> plt.show()\n    \"\"\"\n    # Find common elements\n    common_elements = pd.merge(df1, df2, how='inner')\n\n    # Plot common elements\n    fig, ax = plt.subplots()\n    ax.scatter(common_elements['A'], common_elements['B'])\n    ax.set_title('Common Elements')\n    ax.set_xlabel('A')\n    ax.set_ylabel('B')\n\n    return fig\n```"}
{"seed": "{\"task_id\": 129553, \"prompt\": \"def f_129553(a, b):\\n\\t\", \"suffix\": \"\\n\\treturn intersection(a, b)\", \"canonical_solution\": \"\\n\\tdef intersection(a,b):\\n\\t\\tc = []\\n\\t\\tfor i in a:\\n\\t\\t\\tfor j in b:\\n\\t\\t\\t\\tif i == j:\\n\\t\\t\\t\\t\\tc.append(i)\\n\\t\\tc = list(tuple(c))\\n\\t\\treturn c  \\n\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([1, 2, 3, 4, 5], [1, 3, 5]) == [1, 3, 5]\\n\"], \"entry_point\": \"f_129553\", \"intent\": \"\\u00bfC\\u00f3mo obtener los elementos comunes de dos listas `a` y `b`?\", \"library\": []}", "generation": "Scenario 1:\nnumpy, pandas, random, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef generate_random_dataframe_and_plot_histogram(a, b):\n    \"\"\"\n    Generate a random DataFrame of size (a, b) where a is number of rows and b is number of columns.\n    Plot a histogram of the values in the DataFrame.\n    \n    Parameters:\n    a (int): The number of rows.\n    b (int): The number of columns.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with random integers.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = generate_random_dataframe_and_plot_histogram(10, 5)\n    >>> print(df)\n    >>> df.plot.hist(bins=12, alpha=0.5)\n    \"\"\"\n    df = pd.DataFrame(np.random.randint(0,100,size=(a, b)), columns=COLUMNS[:b])\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, matplotlib, seaborn, pandas\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\ndef plot_heatmap_of_intersection(a, b):\n    \"\"\"\n    Create a DataFrame that represents the intersection of two lists a and b.\n    Then plot a heatmap of the DataFrame.\n    \n    Parameters:\n    a (list): The first list.\n    b (list): The second list.\n    \n    Returns:\n    DataFrame: A pandas DataFrame that represents the intersection of a and b.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - seaborn\n    \n    Example:\n    >>> df = plot_heatmap_of_intersection([1, 2, 3, 4, 5], [1, 3, 5])\n    >>> print(df)\n    >>> sns.heatmap(df, annot=True, cmap='YlGnBu')\n    \"\"\"\n    intersection = list(set(a) & set(b))\n    df = pd.DataFrame([intersection]*len(intersection), columns=intersection)\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, pandas, matplotlib, random\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import randint\n\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef generate_random_dataframe_and_plot_scatter(a, b):\n    \"\"\"\n    Generate a random DataFrame of size (a, b) where a is number of rows and b is number of columns.\n    Plot a scatter plot of the values in the DataFrame.\n    \n    Parameters:\n    a (int): The number of rows.\n    b (int): The number of columns.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with random integers.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - random\n    \n    Example:\n    >>> df = generate_random_dataframe_and_plot_scatter(10, 5)\n    >>> print(df)\n    >>> df.plot.scatter(x='A', y='B')\n    \"\"\"\n    df = pd.DataFrame(np.random.randint(0,100,size=(a, b)), columns=COLUMNS[:b])\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, pandas, matplotlib, seaborn\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef plot_density_of_intersection(a, b):\n    \"\"\"\n    Create a DataFrame that represents the intersection of two lists a and b.\n    Then plot a density plot of the DataFrame.\n    \n    Parameters:\n    a (list): The first list.\n    b (list): The second list.\n    \n    Returns:\n    DataFrame: A pandas DataFrame that represents the intersection of a and b.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - seaborn\n    \n    Example:\n    >>> df = plot_density_of_intersection([1, 2, 3, 4, 5], [1, 3, 5])\n    >>> print(df)\n    >>> sns.kdeplot(df)\n    \"\"\"\n    intersection = list(set(a) & set(b))\n    df = pd.DataFrame([intersection]*len(intersection), columns=intersection)\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, pandas, random, seaborn\n```python\nimport numpy as np\nimport pandas as pd\nfrom random import randint\nimport seaborn as sns\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef generate_random_dataframe_and_plot_boxplot(a, b):\n    \"\"\"\n    Generate a random DataFrame of size (a, b) where a is number of rows and b is number of columns.\n    Plot a boxplot of the values in the DataFrame.\n    \n    Parameters:\n    a (int): The number of rows.\n    b (int): The number of columns.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with random integers.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - random\n    - seaborn\n    \n    Example:\n    >>> df = generate_random_dataframe_and_plot_boxplot(10, 5)\n    >>> print(df)\n    >>> sns.boxplot(data=df)\n    \"\"\"\n    df = pd.DataFrame(np.random.randint(0,100,size=(a, b)), columns=COLUMNS[:b])\n    return df\n```\n"}
{"seed": "{\"task_id\": 98725, \"prompt\": \"def f_98725(lst):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"sorted(enumerate(lst), key=itemgetter(1),  reverse=True)\", \"test_start\": \"\\nfrom operator import itemgetter\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([2,1,9,3,6,4,5,10,8,7]) ==         [(7, 10),(2, 9),(8, 8),(9, 7),(4, 6),(6, 5),(5, 4),(3, 3),(0, 2),(1, 1)]\\n\"], \"entry_point\": \"f_98725\", \"intent\": \"\\u00bfC\\u00f3mo obtener la posici\\u00f3n y el valor del segundo mayor elemento de una lista `list`?\", \"library\": [\"operator\"]}", "generation": "Scenario 1: \npandas, numpy, operator, random\n```python\nimport pandas as pd\nimport numpy as np\nfrom operator import itemgetter\nfrom random import sample\n\n# Constants\nCOLUMNS = ['Name', 'Age', 'Height', 'Weight', 'Score']\nPEOPLE = ['John', 'Mike', 'Anna', 'Ella', 'Tom', 'Sophie', 'Ryan', 'Grace']\n\n# Sample data\nnp.random.seed(0)\ndata = np.random.randint(20, 50, size=(8, 5))\ndf = pd.DataFrame(data, columns=COLUMNS)\ndf['Name'] = PEOPLE\n\ndef find_second_max_score(df):\n    \"\"\"\n    Find the name and score of the person with the second highest score in a DataFrame.\n\n    Parameters:\n    df (DataFrame): The input DataFrame.\n\n    Returns:\n    str: The name of the person with the second highest score.\n\n    Requirements:\n    - pandas\n    - numpy\n    - operator\n    - random\n\n    Example:\n    >>> print(df)\n    >>> find_second_max_score(df)\n    \"\"\"\n    scores = sorted(list(df['Score'].items()), key=itemgetter(1), reverse=True)\n    second_max_score = scores[1]\n    return df.loc[second_max_score[0], 'Name']\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \ncollections, math, itertools, operator\n```python\nfrom collections import Counter\nimport math\nfrom itertools import combinations\nfrom operator import itemgetter\n\n# Constants\nWORDS = ['apple', 'banana', 'cherry', 'date', 'elderberry', 'fig', 'grape']\n\ndef word_combinations(words, n):\n    \"\"\"\n    Generate all combinations of 'n' words from a list and calculate the total number of characters \n    in each combination. Return the combination with the second largest number of characters.\n\n    Parameters:\n    words (list): The list of words.\n    n (int): The number of words in each combination.\n\n    Returns:\n    tuple: The combination of words with the second largest number of characters.\n\n    Requirements:\n    - collections\n    - math\n    - itertools\n    - operator\n\n    Example:\n    >>> word_combinations(WORDS, 3)\n    \"\"\"\n    combs = list(combinations(words, n))\n    lengths = [(comb, sum([len(word) for word in comb])) for comb in combs]\n    lengths.sort(key=itemgetter(1), reverse=True)\n    return lengths[1][0]\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \npandas, numpy, datetime, operator\n```python\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nfrom operator import itemgetter\n\n# Constants\nCOLUMNS = ['Date', 'Price']\n\n# Sample data\nDATES = pd.date_range(start='2021-01-01', end='2021-12-31', freq='D')\nnp.random.seed(0)\nPRICES = np.random.uniform(low=50, high=200, size=(len(DATES),))\ndf = pd.DataFrame({'Date': DATES, 'Price': PRICES})\n\ndef find_date_with_second_highest_price(df):\n    \"\"\"\n    Find the date with the second highest price in a DataFrame.\n\n    Parameters:\n    df (DataFrame): The input DataFrame.\n\n    Returns:\n    datetime: The date with the second highest price.\n\n    Requirements:\n    - pandas\n    - numpy\n    - datetime\n    - operator\n\n    Example:\n    >>> print(df.head())\n    >>> find_date_with_second_highest_price(df)\n    \"\"\"\n    prices = sorted(list(df['Price'].items()), key=itemgetter(1), reverse=True)\n    second_highest_price = prices[1]\n    return df.loc[second_highest_price[0], 'Date']\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, skimage, matplotlib, operator\n```python\nimport numpy as np\nfrom skimage import data, filters\nimport matplotlib.pyplot as plt\nfrom operator import itemgetter\n\n# Constants\nIMAGE = data.camera()\n\ndef find_second_largest_intensity(image):\n    \"\"\"\n    Find the second largest intensity value in a grayscale image.\n\n    Parameters:\n    image (ndarray): The grayscale image.\n\n    Returns:\n    int: The second largest intensity value.\n\n    Requirements:\n    - numpy\n    - skimage\n    - matplotlib\n    - operator\n\n    Example:\n    >>> plt.imshow(IMAGE, cmap='gray')\n    >>> plt.show()\n    >>> find_second_largest_intensity(IMAGE)\n    \"\"\"\n    intensities = sorted(list(np.ndenumerate(image)), key=itemgetter(1), reverse=True)\n    second_largest_intensity = intensities[1]\n    return second_largest_intensity[1]\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nrandom, numpy, scipy, operator\n```python\nfrom random import sample\nimport numpy as np\nfrom scipy import stats\nfrom operator import itemgetter\n\n# Constants\nPOPULATION = list(range(1000))\n\ndef find_second_most_common(sample_size):\n    \"\"\"\n    Draw a random sample from a population, calculate the frequencies of the values, \n    and find the second most common value.\n\n    Parameters:\n    sample_size (int): The size of the sample to draw from the population.\n\n    Returns:\n    int: The second most common value in the sample.\n\n    Requirements:\n    - random\n    - numpy\n    - scipy\n    - operator\n\n    Example:\n    >>> find_second_most_common(500)\n    \"\"\"\n    sample_ = sample(POPULATION, sample_size)\n    freqs = sorted(list(stats.itemfreq(sample_)), key=itemgetter(1), reverse=True)\n    second_most_common = freqs[1]\n    return second_most_common[0]\n```"}
{"seed": "{\"task_id\": 1732, \"prompt\": \"def f_1732(S):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"sorted(S.split(), key=lambda palabra: len(palabra), reverse=True)\", \"test_start\": \"\\nfrom operator import itemgetter\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('hola me llamo Cesar y soy de Peru') ==         ['llamo', 'Cesar', 'hola', 'Peru', 'soy', 'me', 'de', 'y']\\n\"], \"entry_point\": \"f_1732\", \"intent\": \"\\u00bfC\\u00f3mo ordenar una cadena `S` desde su palabra m\\u00e1s larga a la m\\u00e1s corta?\", \"library\": [\"operator\"]}", "generation": "Scenario 1: \ncollections, nltk, operator, re\n```python\nimport collections\nimport nltk\nfrom nltk.corpus import stopwords\nfrom operator import itemgetter\nimport re\n\n# Constants\nSTOPWORDS = set(stopwords.words('english'))\n\ndef count_non_stopwords(text):\n    \"\"\"\n    Count the non-stopwords in a text and return the words and their counts in descending order of counts.\n    If there are multiple words with the same count, order them alphabetically.\n\n    Parameters:\n    text (str): The text to count words in.\n\n    Returns:\n    list: A list of tuples, each containing a word and its count.\n\n    Requirements:\n    - collections\n    - nltk.corpus.stopwords\n    - operator.itemgetter\n    - re\n\n    Example:\n    >>> count_non_stopwords(\"Hello, hello world! This is a test. This is only a test.\")\n    [('test', 2), ('hello', 2), ('world', 1), ('This', 2), ('is', 2), ('a', 2), ('only', 1)]\n    \"\"\"\n    words = re.findall(r'\\b\\w+\\b', text)\n    word_counts = collections.Counter(word for word in words if word.lower() not in STOPWORDS)\n    return sorted(word_counts.items(), key=itemgetter(1, 0), reverse=True)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncollections, string, operator\n```python\nimport collections\nfrom operator import itemgetter\nimport string\n\n# Constants\nALPHABET = string.ascii_lowercase\n\ndef letter_frequency(text):\n    \"\"\"\n    Calculate the frequency of each letter in the English alphabet in a text and return a \n    sorted list of tuples containing the letter and its frequency. The list should be sorted \n    in descending order of frequency, and in ascending order of the letter in case of a tie.\n\n    Parameters:\n    text (str): The text to analyze.\n\n    Returns:\n    list: A list of tuples, each containing a letter and its frequency.\n\n    Requirements:\n    - collections\n    - string.ascii_lowercase\n    - operator.itemgetter\n\n    Example:\n    >>> letter_frequency(\"Hello, world!\")\n    [('l', 3), ('o', 2), ('h', 1), ('e', 1), ('w', 1), ('r', 1), ('d', 1)]\n    \"\"\"\n    letter_counts = collections.Counter(letter for letter in text.lower() if letter in ALPHABET)\n    return sorted(letter_counts.items(), key=itemgetter(1, 0), reverse=True)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nrandom, operator, string\n```python\nimport random\nfrom operator import itemgetter\nimport string\n\n# Constants\nCHARS = string.ascii_letters + string.digits\n\ndef generate_random_strings(n, length):\n    \"\"\"\n    Generate n random strings of a given length, then return a list of the strings sorted by \n    their alphanumeric value.\n\n    Parameters:\n    n (int): The number of strings to generate.\n    length (int): The length of the strings.\n\n    Returns:\n    list: A list of sorted random strings.\n\n    Requirements:\n    - random\n    - operator.itemgetter\n    - string\n\n    Example:\n    >>> generate_random_strings(5, 10)\n    ['3jkl2z7v9x', '4ghij5k6lm', 'a1bcd2efg3', 'h8ij9kl0mn', 'zxywvutsrq']\n    \"\"\"\n    strings = [''.join(random.choice(CHARS) for _ in range(length)) for _ in range(n)]\n    return sorted(strings, key=itemgetter(0))\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnltk, operator, collections\n```python\nimport nltk\nfrom operator import itemgetter\nimport collections\nfrom nltk.tokenize import word_tokenize\n\ndef count_bigrams(text):\n    \"\"\"\n    Count the frequency of each bigram in a text, and return a sorted list of tuples \n    containing the bigram and its frequency. The list should be sorted in descending order \n    of frequency, and in ascending order of the bigram in case of a tie.\n\n    Parameters:\n    text (str): The text to analyze.\n\n    Returns:\n    list: A list of tuples, each containing a bigram and its frequency.\n\n    Requirements:\n    - nltk\n    - operator.itemgetter\n    - collections\n    - nltk.tokenize.word_tokenize\n\n    Example:\n    >>> count_bigrams(\"Hello, world! This is a test. This is only a test.\")\n    [(('This', 'is'), 2), (('a', 'test'), 2), (('is', 'a'), 2), (('is', 'only'), 1), (('only', 'a'), 1), (('hello', 'world'), 1)]\n    \"\"\"\n    words = word_tokenize(text)\n    bigrams = nltk.bigrams(words)\n    bigram_counts = collections.Counter(bigrams)\n    return sorted(bigram_counts.items(), key=itemgetter(1, 0), reverse=True)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nstring, operator, collections\n```python\nimport string\nfrom operator import itemgetter\nimport collections\n\n# Constants\nPUNCTUATION = string.punctuation\n\ndef count_punctuation(text):\n    \"\"\"\n    Count the frequency of each punctuation mark in a text, and return a sorted list of tuples \n    containing the punctuation mark and its frequency. The list should be sorted in descending \n    order of frequency, and in ascending order of the punctuation mark in case of a tie.\n\n    Parameters:\n    text (str): The text to analyze.\n\n    Returns:\n    list: A list of tuples, each containing a punctuation mark and its frequency.\n\n    Requirements:\n    - string.punctuation\n    - operator.itemgetter\n    - collections\n\n    Example:\n    >>> count_punctuation(\"Hello, world! This is a test. This is only a test.\")\n    [(' ', 9), ('.', 2), ('!', 1), (',', 1)]\n    \"\"\"\n    punctuation_counts = collections.Counter(char for char in text if char in PUNCTUATION)\n    return sorted(punctuation_counts.items(), key=itemgetter(1, 0), reverse=True)\n```\n"}
{"seed": "{\"task_id\": 1732, \"prompt\": \"def f_1732(S):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"[i for i in reversed(sorted(S.split(), key=lambda item: len(item)))]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    S = \\\"In addition to facilitating the new recognition process\\\"\\n    actual_result = candidate(S)\\n    expected_result = ['facilitating', 'recognition', 'addition', 'process', 'new', 'the', 'to', 'In']\\n    assert actual_result == expected_result\\n\"], \"entry_point\": \"f_1732\", \"intent\": \"\\u00bfC\\u00f3mo ordenar una cadena `S` desde su palabra m\\u00e1s larga a la m\\u00e1s corta?\", \"library\": []}", "generation": "Scenario 1:\ncollections, string, itertools, random\n```python\nimport collections\nimport string\nimport itertools\nimport random\n\n# Constants\nLETTERS = string.ascii_lowercase\n\ndef generate_most_common_letters(text, n):\n    \"\"\"\n    Generate a list of n most common letters in a given text, if there are ties, \n    sort the letters in lexicographical order. If there are less than n distinct \n    letters in the text, fill the list with random letters.\n    \n    Parameters:\n    text (str): The input text.\n    n (int): The number of most common letters to return.\n    \n    Returns:\n    list: A list of n most common letters.\n    \n    Requirements:\n    - collections\n    - string\n    - itertools\n    - random\n\n    Example:\n    >>> generate_most_common_letters('hello world', 5)\n    ['l', 'o', 'r', 'd', 'h']\n    \"\"\"\n    text = text.lower()\n    counter = collections.Counter(text)\n    common_letters = [letter for letter, _ in counter.most_common(n) if letter in LETTERS]\n\n    if len(common_letters) < n:\n        remaining_letters = list(set(LETTERS) - set(common_letters))\n        random.shuffle(remaining_letters)\n        common_letters.extend(remaining_letters[:n-len(common_letters)])\n\n    common_letters.sort()\n\n    return common_letters[:n]\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnltk, collections, string, re\n```python\nimport nltk\nimport collections\nimport string\nimport re\n\n# Constants\nSTOPWORDS = nltk.corpus.stopwords.words('english')\n\ndef filter_most_common_words(text, n):\n    \"\"\"\n    Filter out the n most common words in a given text excluding the stopwords \n    and punctuation, return the remaining text.\n    \n    Parameters:\n    text (str): The input text.\n    n (int): The number of most common words to filter.\n    \n    Returns:\n    str: The remaining text after filtering.\n    \n    Requirements:\n    - nltk\n    - collections\n    - string\n    - re\n\n    Example:\n    >>> filter_most_common_words('the quick brown fox jumps over the lazy dog', 3)\n    'the quick brown fox jumps over the'\n    \"\"\"\n    words = nltk.word_tokenize(text)\n    counter = collections.Counter(words)\n    common_words = {word for word, _ in counter.most_common(n) if word not in STOPWORDS and word not in string.punctuation}\n\n    filtered_text = ' '.join([word for word in words if word not in common_words])\n\n    return filtered_text\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, string, numpy\n```python\nimport pandas as pd\nimport string\nimport numpy as np\n\n# Constants\nLETTERS = string.ascii_lowercase\n\ndef calculate_letter_frequency(df, column_name):\n    \"\"\"\n    Calculate the frequency of each letter in a given column of a pandas DataFrame. \n    Return a DataFrame with letters as the index and their frequencies as the column.\n    \n    Parameters:\n    df (DataFrame): The input DataFrame.\n    column_name (str): The column name in the DataFrame.\n    \n    Returns:\n    DataFrame: A DataFrame with letters as the index and their frequencies as the column.\n    \n    Requirements:\n    - pandas\n    - string\n    - numpy\n\n    Example:\n    >>> df = pd.DataFrame({'text': ['hello', 'world', 'foo', 'bar']})\n    >>> calculate_letter_frequency(df, 'text')\n    \"\"\"\n    letter_freq = pd.Series(index=list(LETTERS), data=np.zeros(len(LETTERS)))\n\n    for text in df[column_name]:\n        for letter in text:\n            if letter in LETTERS:\n                letter_freq[letter] += 1\n\n    letter_freq = letter_freq.sort_values(ascending=False)\n\n    return letter_freq\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nstring, random, collections\n```python\nimport string\nimport random\nimport collections\n\n# Constants\nLETTERS = string.ascii_lowercase\n\ndef generate_random_string(n, freq_dict):\n    \"\"\"\n    Generate a random string of length n from a frequency dictionary of letters.\n    \n    Parameters:\n    n (int): The length of the random string.\n    freq_dict (dict): The frequency dictionary of letters.\n    \n    Returns:\n    str: The generated random string.\n    \n    Requirements:\n    - string\n    - random\n    - collections\n\n    Example:\n    >>> generate_random_string(10, {'a': 1, 'b': 2, 'c': 3})\n    \"\"\"\n    letter_freq = collections.Counter(freq_dict)\n    letters = list(itertools.chain.from_iterable([[letter]*freq for letter, freq in letter_freq.items()]))\n    return ''.join(random.choice(letters) for _ in range(n))\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnltk, collections, string, re\n```python\nimport nltk\nimport collections\nimport string\nimport re\n\n# Constants\nSTOPWORDS = nltk.corpus.stopwords.words('english')\n\ndef find_most_common_phrases(text, n, phrase_length):\n    \"\"\"\n    Find the n most common phrases of a certain length in a given text excluding the stopwords \n    and punctuation.\n    \n    Parameters:\n    text (str): The input text.\n    n (int): The number of most common phrases to return.\n    phrase_length (int): The length of the phrases.\n    \n    Returns:\n    list: A list of n most common phrases.\n    \n    Requirements:\n    - nltk\n    - collections\n    - string\n    - re\n\n    Example:\n    >>> find_most_common_phrases('the quick brown fox jumps over the lazy dog', 3, 2)\n    ['quick brown', 'brown fox', 'fox jumps']\n    \"\"\"\n    words = [word for word in nltk.word_tokenize(text) if word not in STOPWORDS and word not in string.punctuation]\n    phrases = [' '.join(words[i:i+phrase_length]) for i in range(len(words)-phrase_length+1)]\n    \n    counter = collections.Counter(phrases)\n    common_phrases = [phrase for phrase, _ in counter.most_common(n)]\n\n    return common_phrases\n```"}
{"seed": "{\"task_id\": 9367, \"prompt\": \"def f_9367(list):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"[item2 for i, item in enumerate(list) for item2 in list[i:]]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    array = [1, 3, 4, 6]\\n    assert candidate(array) == [1, 3, 4, 6, 3, 4, 6, 4, 6, 6]\\n\"], \"entry_point\": \"f_9367\", \"intent\": \"\\u00bfC\\u00f3mo realizar una iteraci\\u00f3n anidada del valor de una lista `list` y luego de los valores siguientes hasta el final?\", \"library\": []}", "generation": "Scenario 1:\nnumpy, itertools, math, random\n```python\nimport numpy as np\nfrom itertools import combinations\nfrom math import sqrt\nfrom random import randint\n\n# Constants\nPOINTS = [(randint(1, 100), randint(1, 100)) for _ in range(20)]\n\ndef find_closest_points(points):\n    \"\"\"\n    Find the two closest points in a set of points in a 2D space.\n    \n    Parameters:\n    points (list): A list of tuples, each representing a point in 2D space.\n    \n    Returns:\n    tuple: A tuple of two points that are the closest to each other.\n    \n    Requirements:\n    - numpy\n    - itertools\n    - math\n    - random\n    \n    Example:\n    >>> closest_points = find_closest_points(POINTS)\n    >>> print(closest_points)\n    \"\"\"\n    min_distance = float('inf')\n    closest_pair = None\n\n    for point1, point2 in combinations(points, 2):\n        distance = sqrt((point1[0] - point2[0])**2 + (point1[1] - point2[1])**2)\n        if distance < min_distance:\n            min_distance = distance\n            closest_pair = (point1, point2)\n\n    return closest_pair\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, itertools, random, matplotlib.pyplot\n```python\nimport numpy as np\nfrom itertools import combinations\nfrom random import randrange\nimport matplotlib.pyplot as plt\n\n# Constants\nLIST_SIZE = 10\nRANGE_START = 1\nRANGE_END = 100\n\ndef plot_combinations_histogram():\n    \"\"\"\n    Generate a list of random integers, calculate all combinations of the list, \n    and plot a histogram of the combinations' sums.\n    \n    Requirements:\n    - numpy\n    - itertools\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> plot_combinations_histogram()\n    \"\"\"\n    list_numbers = [randrange(RANGE_START, RANGE_END) for _ in range(LIST_SIZE)]\n    sums = [sum(combination) for combination in combinations(list_numbers, 2)]\n    plt.hist(sums, bins='auto')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, itertools, random, pandas\n```python\nimport numpy as np\nfrom itertools import combinations\nfrom random import randrange\nimport pandas as pd\n\n# Constants\nLIST_SIZE = 10\nRANGE_START = 1\nRANGE_END = 100\n\ndef generate_combinations_dataframe():\n    \"\"\"\n    Generate a list of random integers, calculate all combinations of the list, \n    and create a pandas DataFrame of the combinations and their sums.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the combinations and their sums.\n    \n    Requirements:\n    - numpy\n    - itertools\n    - random\n    - pandas\n    \n    Example:\n    >>> df = generate_combinations_dataframe()\n    >>> print(df)\n    \"\"\"\n    list_numbers = [randrange(RANGE_START, RANGE_END) for _ in range(LIST_SIZE)]\n    combinations_sums = [{'Combination': combination, 'Sum': sum(combination)} for combination in combinations(list_numbers, 2)]\n    df = pd.DataFrame(combinations_sums)\n    \n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, itertools, random, matplotlib.pyplot\n```python\nimport numpy as np\nfrom itertools import combinations\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nLIST_SIZE = 10\nRANGE_START = 1\nRANGE_END = 100\n\ndef plot_combination_pairs():\n    \"\"\"\n    Generate a list of random integers, calculate all combinations of the list, \n    and plot the pairs of combinations on a scatter plot.\n    \n    Requirements:\n    - numpy\n    - itertools\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> plot_combination_pairs()\n    \"\"\"\n    list_numbers = [randint(RANGE_START, RANGE_END) for _ in range(LIST_SIZE)]\n    pairs = list(combinations(list_numbers, 2))\n    plt.scatter(*zip(*pairs))\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, itertools, random, scipy.stats\n```python\nimport numpy as np\nfrom itertools import combinations\nfrom random import randint\nfrom scipy.stats import pearsonr\n\n# Constants\nLIST_SIZE = 10\nRANGE_START = 1\nRANGE_END = 100\n\ndef calculate_combinations_correlation():\n    \"\"\"\n    Generate a list of random integers, calculate all combinations of the list, \n    and calculate the Pearson correlation coefficient of the combinations' sums.\n    \n    Returns:\n    float: The Pearson correlation coefficient of the combinations' sums.\n    \n    Requirements:\n    - numpy\n    - itertools\n    - random\n    - scipy.stats\n    \n    Example:\n    >>> correlation = calculate_combinations_correlation()\n    >>> print(correlation)\n    \"\"\"\n    list_numbers = [randint(RANGE_START, RANGE_END) for _ in range(LIST_SIZE)]\n    sums = [sum(combination) for combination in combinations(list_numbers, 2)]\n    correlation, _ = pearsonr(range(len(sums)), sums)\n    \n    return correlation\n```"}
{"seed": "{\"task_id\": 9367, \"prompt\": \"def f_9367(list):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"[list[j] for i in range(len(list)) for j in range((i+1), len(list))]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    array = [4, 1, 3, 0, 9]\\n    assert candidate(array) == [1, 3, 0, 9, 3, 0, 9, 0, 9, 9]\\n\"], \"entry_point\": \"f_9367\", \"intent\": \"\\u00bfC\\u00f3mo realizar una iteraci\\u00f3n anidada del valor de una lista `list` y luego de los valores siguientes hasta el final?\", \"library\": []}", "generation": "Scenario 1: \npandas, itertools, numpy, matplotlib\n```python\nimport pandas as pd\nfrom itertools import combinations\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef generate_combinations(dataframe):\n    \"\"\"\n    Take a pandas DataFrame, generate all possible combinations of 2 columns, \n    calculate the correlation for each combination, and plot a histogram of the correlation coefficients.\n    \n    Parameters:\n    dataframe (DataFrame): The input pandas DataFrame.\n    \n    Returns:\n    dict: A dictionary with column pairs as keys and their correlation coefficients as values.\n    \n    Requirements:\n    - pandas\n    - itertools\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = pd.DataFrame(np.random.randint(0,100,size=(100, 5)), columns=list('ABCDE'))\n    >>> correlations = generate_combinations(df)\n    >>> plt.hist(correlations.values(), bins=10)\n    \"\"\"\n    correlations = {}\n\n    for pair in combinations(dataframe.columns, 2):\n        corr = dataframe[list(pair)].corr().iloc[0, 1]\n        correlations[pair] = corr\n\n    return correlations\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, itertools, numpy, matplotlib\n```python\nimport pandas as pd\nfrom itertools import permutations\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef generate_permutations(dataframe):\n    \"\"\"\n    Take a pandas DataFrame, generate all possible permutations of the columns, \n    calculate the sum for each permutation and plot a histogram of the sums.\n    \n    Parameters:\n    dataframe (DataFrame): The input pandas DataFrame.\n    \n    Returns:\n    dict: A dictionary with column permutations as keys and their sums as values.\n    \n    Requirements:\n    - pandas\n    - itertools\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = pd.DataFrame(np.random.randint(0,100,size=(100, 5)), columns=list('ABCDE'))\n    >>> sums = generate_permutations(df)\n    >>> plt.hist(sums.values(), bins=10)\n    \"\"\"\n    sums = {}\n\n    for perm in permutations(dataframe.columns):\n        sum_val = dataframe[list(perm)].sum().sum()\n        sums[perm] = sum_val\n\n    return sums\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, itertools, numpy, matplotlib\n```python\nimport pandas as pd\nfrom itertools import combinations\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef generate_combinations_and_plot(dataframe):\n    \"\"\"\n    Take a pandas DataFrame, generate all possible combinations of 2 columns, \n    calculate the mean of each combination and plot a bar chart of the means.\n    \n    Parameters:\n    dataframe (DataFrame): The input pandas DataFrame.\n    \n    Returns:\n    dict: A dictionary with column pairs as keys and their means as values.\n    \n    Requirements:\n    - pandas\n    - itertools\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = pd.DataFrame(np.random.randint(0,100,size=(100, 5)), columns=list('ABCDE'))\n    >>> means = generate_combinations_and_plot(df)\n    >>> plt.bar(range(len(means)), list(means.values()), align='center')\n    >>> plt.xticks(range(len(means)), list(means.keys()))\n    \"\"\"\n    means = {}\n\n    for pair in combinations(dataframe.columns, 2):\n        mean = dataframe[list(pair)].mean().mean()\n        means[pair] = mean\n\n    return means\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, itertools, numpy, matplotlib\n```python\nimport pandas as pd\nfrom itertools import combinations\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef generate_combinations_and_plot(dataframe):\n    \"\"\"\n    Take a pandas DataFrame, generate all possible combinations of 3 columns, \n    calculate the standard deviation of each combination and plot a pie chart of the standard deviations.\n    \n    Parameters:\n    dataframe (DataFrame): The input pandas DataFrame.\n    \n    Returns:\n    dict: A dictionary with column triplets as keys and their standard deviations as values.\n    \n    Requirements:\n    - pandas\n    - itertools\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = pd.DataFrame(np.random.randint(0,100,size=(100, 5)), columns=list('ABCDE'))\n    >>> stds = generate_combinations_and_plot(df)\n    >>> plt.pie(list(stds.values()), labels = list(stds.keys()))\n    \"\"\"\n    stds = {}\n\n    for triple in combinations(dataframe.columns, 3):\n        std = dataframe[list(triple)].std().mean()\n        stds[triple] = std\n\n    return stds\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, itertools, numpy, matplotlib\n```python\nimport pandas as pd\nfrom itertools import combinations\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef generate_combinations_and_plot(dataframe):\n    \"\"\"\n    Take a pandas DataFrame, generate all possible combinations of 2 columns, \n    calculate the product of each combination and plot a line chart of the products.\n    \n    Parameters:\n    dataframe (DataFrame): The input pandas DataFrame.\n    \n    Returns:\n    dict: A dictionary with column pairs as keys and their products as values.\n    \n    Requirements:\n    - pandas\n    - itertools\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = pd.DataFrame(np.random.randint(0,100,size=(100, 5)), columns=list('ABCDE'))\n    >>> products = generate_combinations_and_plot(df)\n    >>> plt.plot(list(products.values()))\n    \"\"\"\n    products = {}\n\n    for pair in combinations(dataframe.columns, 2):\n        product = dataframe[list(pair)].product().product()\n        products[pair] = product\n\n    return products\n```\n"}
{"seed": "{\"task_id\": 138290, \"prompt\": \"def f_138290(S):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"S[::-1]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(\\\"palindrome\\\") == \\\"emordnilap\\\"\\n\", \"\\n    assert candidate(\\\"\\\") == \\\"\\\"\\n\"], \"entry_point\": \"f_138290\", \"intent\": \"\\u00bfC\\u00f3mo recorrer una cadena `S` de derecha a izquierda?\", \"library\": []}", "generation": "Scenario 1:\npandas, random, string, matplotlib\n```python\nimport pandas as pd\nimport random\nimport string\nimport matplotlib.pyplot as plt\n\n# Constants\nLETTERS = string.ascii_lowercase\n\ndef generate_random_data(n, m):\n    \"\"\"\n    Generate a DataFrame containing n rows and m columns of random lowercase letters. \n    Then sort each row from right to left and plot the frequency of the letters in the \n    first column.\n\n    Parameters:\n    n (int): The number of rows.\n    m (int): The number of columns.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the sorted random letters.\n\n    Requirements:\n    - pandas\n    - random\n    - string\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = generate_random_data(5, 3)\n    >>> print(df)\n    >>> df[0].value_counts().plot(kind='bar')\n    \"\"\"\n    data = []\n\n    for _ in range(n):\n        row = [random.choice(LETTERS) for _ in range(m)]\n        row.sort(reverse=True)\n        data.append(row)\n\n    df = pd.DataFrame(data)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nitertools, string, random\n```python\nimport itertools\nimport string\nimport random\n\n# Constants\nLETTERS = string.ascii_lowercase\nNUMBERS = string.digits\n\ndef generate_permutations(n):\n    \"\"\"\n    Generate all permutations of a string of length n containing random lowercase letters \n    and numbers. Then reverse each permutation and return them as a list.\n\n    Parameters:\n    n (int): The length of the string.\n\n    Returns:\n    list: A list of all reversed permutations.\n\n    Requirements:\n    - itertools\n    - string\n    - random\n\n    Example:\n    >>> permutations = generate_permutations(2)\n    >>> print(permutations)\n    \"\"\"\n    s = ''.join(random.choice(LETTERS + NUMBERS) for _ in range(n))\n    permutations = [''.join(p)[::-1] for p in itertools.permutations(s)]\n\n    return permutations\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, string, random\n```python\nimport numpy as np\nimport string\nimport random\n\n# Constants\nLETTERS = string.ascii_lowercase\n\ndef create_random_matrix(n, m):\n    \"\"\"\n    Create a numpy matrix with shape (n, m) containing random lowercase letters. \n    Then reverse each row from right to left and return the matrix.\n\n    Parameters:\n    n (int): The number of rows.\n    m (int): The number of columns.\n\n    Returns:\n    ndarray: A numpy ndarray with the reversed rows.\n\n    Requirements:\n    - numpy\n    - string\n    - random\n\n    Example:\n    >>> matrix = create_random_matrix(5, 3)\n    >>> print(matrix)\n    \"\"\"\n    matrix = np.array([[random.choice(LETTERS) for _ in range(m)] for _ in range(n)])\n    matrix = np.apply_along_axis(lambda x: x[::-1], 1, matrix)\n\n    return matrix\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nrandom, string, collections\n```python\nimport random\nimport string\nfrom collections import Counter\n\n# Constants\nLETTERS = string.ascii_lowercase\n\ndef count_letter_frequencies(n):\n    \"\"\"\n    Generate a string of length n containing random lowercase letters. Then count the \n    frequency of each letter in the reversed string and return a Counter object.\n\n    Parameters:\n    n (int): The length of the string.\n\n    Returns:\n    Counter: A Counter object with the letter frequencies.\n\n    Requirements:\n    - random\n    - string\n    - collections\n\n    Example:\n    >>> frequencies = count_letter_frequencies(100)\n    >>> print(frequencies)\n    \"\"\"\n    s = ''.join(random.choice(LETTERS) for _ in range(n))\n    frequencies = Counter(s[::-1])\n\n    return frequencies\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nrandom, string, re\n```python\nimport random\nimport string\nimport re\n\n# Constants\nLETTERS = string.ascii_lowercase\nPATTERN = r'[aeiou]'\n\ndef count_vowels_in_reversed_string(n):\n    \"\"\"\n    Generate a string of length n containing random lowercase letters. Then count the \n    number of vowels in the reversed string and return the count.\n\n    Parameters:\n    n (int): The length of the string.\n\n    Returns:\n    int: The number of vowels.\n\n    Requirements:\n    - random\n    - string\n    - re\n\n    Example:\n    >>> count = count_vowels_in_reversed_string(100)\n    >>> print(count)\n    \"\"\"\n    s = ''.join(random.choice(LETTERS) for _ in range(n))\n    count = len(re.findall(PATTERN, s[::-1]))\n\n    return count\n```"}
{"seed": "{\"task_id\": 364966, \"prompt\": \"def f_364966(a):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"int(a)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(\\\"1\\\") == 1\\n\", \"\\n    assert candidate(\\\"12\\\") == 12\\n\", \"\\n    assert candidate(\\\"123\\\") == 123\\n\", \"\\n    assert candidate(\\\"0000\\\") == 0\\n\"], \"entry_point\": \"f_364966\", \"intent\": \"\\u00bfC\\u00f3mo convertir un dato `a` de `string` a `int`?\", \"library\": []}", "generation": "Scenario 1:\nnumpy, pandas, matplotlib, random, sklearn\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import randint\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Constants\nCATEGORIES = ['Electronics', 'Books', 'Grocery', 'Toys', 'Fashion']\nYEARS = ['2018', '2019', '2020', '2021', '2022']\n\ndef generate_sales_report():\n    \"\"\"\n    Generate a sales report of different categories over the years and plot the \n    normalized sales trend.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with sales data for the categories.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - random\n    - sklearn.preprocessing.MinMaxScaler\n    \n    Example:\n    >>> report = generate_sales_report()\n    >>> print(report)\n    >>> report.plot(x='Year', y=CATEGORIES, kind='line')\n    \"\"\"\n    sales_data = []\n    scaler = MinMaxScaler()\n\n    for year in YEARS:\n        sales = [randint(1000, 5000) for _ in CATEGORIES]\n        sales_data.append([year] + sales)\n\n    report_df = pd.DataFrame(sales_data, columns=['Year'] + CATEGORIES)\n    report_df[CATEGORIES] = scaler.fit_transform(report_df[CATEGORIES])\n\n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ndatetime, re, json\n```python\nfrom datetime import datetime\nimport re\nimport json\n\n# Constants\nDATE_FORMAT = \"%Y-%m-%dT%H:%M:%S\"\nJSON_FILE_PATH = \"sample.json\"\n\ndef filter_json_by_date(json_str, date_str):\n    \"\"\"\n    Filter a JSON string by a given date. Only return the items that have a \n    \"date\" field that is later than the given date.\n\n    Parameters:\n    json_str (str): The JSON string.\n    date_str (str): The date string in \"yyyy-mm-ddThh:mm:ss\" format.\n\n    Returns:\n    str: The filtered JSON string.\n\n    Requirements:\n    - datetime\n    - re\n    - json\n\n    Example:\n    >>> json_str = '[{\"id\": 1, \"date\": \"2022-01-01T00:00:00\"}, {\"id\": 2, \"date\": \"2022-02-01T00:00:00\"}]'\n    >>> filter_json_by_date(json_str, \"2022-01-01T00:00:00\")\n    \"\"\"\n    given_date = datetime.strptime(date_str, DATE_FORMAT)\n    json_data = json.loads(json_str)\n\n    filtered_data = [item for item in json_data if datetime.strptime(item[\"date\"], DATE_FORMAT) > given_date]\n\n    return json.dumps(filtered_data)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nbs4, requests, re\n```python\nfrom bs4 import BeautifulSoup\nimport requests\nimport re\n\n# Constants\nURL = \"https://www.example.com\"\n\ndef scrape_website(url=URL):\n    \"\"\"\n    Scrape a website and return all the URLs in it.\n\n    Returns:\n    list: The list of URLs.\n\n    Requirements:\n    - bs4.BeautifulSoup\n    - requests\n    - re\n\n    Example:\n    >>> urls = scrape_website()\n    >>> print(urls)\n    \"\"\"\n    response = requests.get(url)\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    urls = [a.get('href') for a in soup.find_all('a', href=re.compile(\"^http\"))]\n\n    return urls\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, datetime, hashlib\n```python\nimport os\nfrom datetime import datetime\nimport hashlib\n\n# Constants\nDIRECTORY_PATH = \".\"\n\ndef get_file_info(directory_path=DIRECTORY_PATH):\n    \"\"\"\n    Get the metadata and MD5 hash of all files in a directory.\n\n    Returns:\n    list: The list of file info.\n\n    Requirements:\n    - os\n    - datetime\n    - hashlib\n\n    Example:\n    >>> file_info = get_file_info()\n    >>> print(file_info)\n    \"\"\"\n    file_info = []\n\n    for file_name in os.listdir(directory_path):\n        file_path = os.path.join(directory_path, file_name)\n        \n        if os.path.isfile(file_path):\n            file_size = os.path.getsize(file_path)\n            file_mtime = datetime.fromtimestamp(os.path.getmtime(file_path)).strftime('%Y-%m-%d %H:%M:%S')\n\n            with open(file_path, 'rb') as file:\n                file_hash = hashlib.md5(file.read()).hexdigest()\n\n            file_info.append([file_name, file_size, file_mtime, file_hash])\n\n    return file_info\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nrandom, string, hashlib\n```python\nimport random\nimport string\nimport hashlib\n\ndef generate_random_string_and_hash(length):\n    \"\"\"\n    Generate a random string of a given length and its MD5 hash.\n\n    Parameters:\n    length (int): The length of the string.\n\n    Returns:\n    tuple: The random string and its MD5 hash.\n\n    Requirements:\n    - random\n    - string\n    - hashlib\n\n    Example:\n    >>> random_string, hash = generate_random_string_and_hash(10)\n    >>> print(random_string)\n    >>> print(hash)\n    \"\"\"\n    random_string = ''.join(random.choice(string.ascii_letters + string.digits) for _ in range(length))\n    hash = hashlib.md5(random_string.encode()).hexdigest()\n\n    return random_string, hash\n```"}
{"seed": "{\"task_id\": 51410, \"prompt\": \"def f_51410(val1, val2):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"val1, val2\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(\\\"1\\\", 2) == (\\\"1\\\", 2)\\n\"], \"entry_point\": \"f_51410\", \"intent\": \"\\u00bfC\\u00f3mo retornar dos valores `val1` y `val2` en una funci\\u00f3n?\", \"library\": []}", "generation": "Scenario 1: \nnumpy, matplotlib, scipy.stats, pandas\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nimport pandas as pd\n\n# Constants\nNUM_SAMPLES = 1000\n\ndef plot_normal_dist(val1, val2):\n    \"\"\"\n    Generate a plot of a normal distribution with mean and standard deviation \n    provided by the parameters val1 (mean) and val2 (stddev).\n\n    Parameters:\n    val1 (float): The mean of the distribution.\n    val2 (float): The standard deviation of the distribution.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the sample data. \n    \n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - scipy.stats\n    - pandas\n\n    Example:\n    >>> plot_normal_dist(0, 1)\n    \"\"\"\n    # Generate samples\n    samples = np.random.normal(loc=val1, scale=val2, size=NUM_SAMPLES)\n    df = pd.DataFrame(samples, columns=['Sample Data'])\n\n    # Generate plot\n    plt.hist(samples, bins=30, density=True, alpha=0.5, color='g')\n\n    # Plot expected normal distribution\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, val1, val2)\n    plt.plot(x, p, 'k', linewidth=2)\n\n    plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, matplotlib, sklearn.linear_model, pandas\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nimport pandas as pd\n\n# Constants\nNUM_SAMPLES = 100\n\ndef generate_linear_regression(val1, val2):\n    \"\"\"\n    Generate a linear regression model and plot based on the input parameters \n    val1 (coefficient) and val2 (intercept).\n\n    Parameters:\n    val1 (float): The coefficient of the linear regression.\n    val2 (float): The intercept of the linear regression.\n\n    Returns:\n    LinearRegression: The trained linear regression model.\n    \n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.linear_model\n    - pandas\n\n    Example:\n    >>> generate_linear_regression(2, 3)\n    \"\"\"\n    # Generate data\n    X = np.array([i for i in range(NUM_SAMPLES)]).reshape(-1, 1)\n    y = val1 * X + val2 + np.random.rand(NUM_SAMPLES, 1)\n\n    # Fit the model\n    model = LinearRegression()\n    model.fit(X, y)\n\n    # Plot the data and the line\n    plt.scatter(X, y)\n    plt.plot(X, model.predict(X), color='red')\n    plt.show()\n\n    return model\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, matplotlib, scipy.optimize, pandas\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import curve_fit\nimport pandas as pd\n\n# Constants\nNUM_SAMPLES = 100\n\ndef fit_exponential_curve(val1, val2):\n    \"\"\"\n    Fit an exponential curve to randomly generated data based on the input parameters \n    val1 (coefficient) and val2 (exponent).\n\n    Parameters:\n    val1 (float): The coefficient of the exponential curve.\n    val2 (float): The exponent of the exponential curve.\n\n    Returns:\n    array: The optimal values for the parameters of the curve.\n    \n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - scipy.optimize\n    - pandas\n\n    Example:\n    >>> fit_exponential_curve(2, 3)\n    \"\"\"\n    # Exponential function\n    def func(x, a, b):\n        return a * np.exp(-b * x)\n\n    # Generate data\n    x = np.linspace(0, 4, NUM_SAMPLES)\n    y = func(x, val1, val2)\n\n    # Add noise\n    yn = y + 0.2 * np.random.normal(size=len(x))\n\n    # Fit the curve\n    popt, pcov = curve_fit(func, x, yn)\n\n    # Plot the data and the curve\n    plt.figure()\n    plt.plot(x, yn, 'b-', label='data')\n    plt.plot(x, func(x, *popt), 'r-', label='fit: a=%5.3f, b=%5.3f' % tuple(popt))\n\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.legend()\n    plt.show()\n\n    return popt\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, matplotlib, sklearn.cluster, pandas\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nimport pandas as pd\n\n# Constants\nNUM_SAMPLES = 1000\nNUM_CLUSTERS = 3\n\ndef perform_kmeans_clustering(val1, val2):\n    \"\"\"\n    Perform KMeans clustering on randomly generated 2D data based on the input parameters \n    val1 and val2 (coordinates of the center of the clusters).\n\n    Parameters:\n    val1 (float): The x-coordinate of the center of the clusters.\n    val2 (float): The y-coordinate of the center of the clusters.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the cluster labels.\n    \n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.cluster\n    - pandas\n\n    Example:\n    >>> perform_kmeans_clustering(2, 3)\n    \"\"\"\n    # Generate data\n    X = np.random.rand(NUM_SAMPLES, 2) + [val1, val2]\n\n    # Perform KMeans clustering\n    kmeans = KMeans(n_clusters=NUM_CLUSTERS)\n    kmeans.fit(X)\n\n    # Plot the data and the clusters\n    plt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_, cmap='viridis')\n    plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red')\n    plt.show()\n\n    df = pd.DataFrame(X, columns=['x', 'y'])\n    df['label'] = kmeans.labels_\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, matplotlib, sklearn.preprocessing\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Constants\nNUM_SAMPLES = 100\n\ndef generate_and_scale_data(val1, val2):\n    \"\"\"\n    Generate a DataFrame of random data and scale it using MinMaxScaler based on \n    the input parameters val1 and val2 (the lower and upper bounds for the scaling).\n\n    Parameters:\n    val1 (float): The lower bound for the scaling.\n    val2 (float): The upper bound for the scaling.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the scaled data.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.preprocessing\n\n    Example:\n    >>> generate_and_scale_data(0, 1)\n    \"\"\"\n    # Generate data\n    df = pd.DataFrame(np.random.randint(0,100,size=(NUM_SAMPLES, 4)), columns=list('ABCD'))\n\n    # Scale the data\n    scaler = MinMaxScaler(feature_range=(val1, val2))\n    df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n\n    # Plot the data before and after scaling\n    df.plot(kind='box', title='Before Scaling')\n    df_scaled.plot(kind='box', title='After Scaling')\n    plt.show()\n\n    return df_scaled\n```\n"}
{"seed": "{\"task_id\": 30666, \"prompt\": \"def f_30666(file):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"os.stat(file).st_size == 0\", \"test_start\": \"\\nimport os\\n\\ndef check(candidate):\", \"test\": [\"\\n    f = open('test.txt', 'w')\\n    f.close()\\n    assert candidate('test.txt') == True \\n\", \"\\n    with open('test.txt', 'w') as fw: fw.write(\\\"hello world!\\\")\\n    assert candidate('test.txt') == False\\n\"], \"entry_point\": \"f_30666\", \"intent\": \"\\u00bfComo verifico si un archivo `file` tiene contenido o est\\u00e1 vac\\u00edo?\", \"library\": [\"os\"]}", "generation": "Scenario 1: \nos, csv, random\n```python\nimport os\nimport csv\nfrom random import choice\n\n# Constants\nANIMALS = ['Dog', 'Cat', 'Bird', 'Fish', 'Rabbit']\n\ndef populate_and_check_file(file_path):\n    \"\"\"\n    Populate a file with random animal names from a list and then check if the file \n    is empty or not.\n\n    Parameters:\n    file_path (str): The path to the file.\n\n    Returns:\n    bool: True if the file is not empty, False otherwise.\n\n    Requirements:\n    - os\n    - csv\n    - random.choice\n\n    Example:\n    >>> populate_and_check_file('animals.csv')\n    \"\"\"\n    with open(file_path, 'w') as f:\n        writer = csv.writer(f)\n        for _ in range(10):\n            animal = choice(ANIMALS)\n            writer.writerow([animal])\n\n    return os.stat(file_path).st_size != 0\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, glob, hashlib\n```python\nimport os\nimport glob\nimport hashlib\n\ndef find_duplicate_files(start_dir):\n    \"\"\"\n    Find duplicate files in a directory and its subdirectories based on their checksums.\n\n    Parameters:\n    start_dir (str): The directory to start the search.\n\n    Returns:\n    list: A list of duplicate files.\n\n    Requirements:\n    - os\n    - glob\n    - hashlib\n\n    Example:\n    >>> find_duplicate_files('/home/user/Documents')\n    \"\"\"\n    checksums = {}\n    duplicates = []\n\n    for dirpath, dirnames, filenames in os.walk(start_dir):\n        for filename in filenames:\n            filepath = os.path.join(dirpath, filename)\n            with open(filepath, 'rb') as f:\n                checksum = hashlib.md5(f.read()).hexdigest()\n            if checksum in checksums:\n                duplicates.append((filepath, checksums[checksum]))\n            else:\n                checksums[checksum] = filepath\n\n    return duplicates\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, pathlib, shutil\n```python\nimport os\nfrom pathlib import Path\nimport shutil\n\ndef move_files_based_on_extension(start_dir, dest_dir):\n    \"\"\"\n    Move files from one directory to another based on their file extension.\n\n    Parameters:\n    start_dir (str): The directory from which the files will be moved.\n    dest_dir (str): The directory to which the files will be moved.\n\n    Requirements:\n    - os\n    - pathlib.Path\n    - shutil\n\n    Example:\n    >>> move_files_based_on_extension('/home/user/Documents', '/home/user/Pictures')\n    \"\"\"\n    start_path = Path(start_dir)\n    dest_path = Path(dest_dir)\n\n    for file in start_path.glob('*'):\n        if file.suffix in ['.jpg', '.png', '.gif']:\n            shutil.move(str(file), dest_path)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, time, datetime\n```python\nimport os\nimport time\nfrom datetime import datetime, timedelta\n\ndef delete_old_files(path, days):\n    \"\"\"\n    Delete files older than a certain number of days.\n\n    Parameters:\n    path (str): The directory path.\n    days (int): The number of days.\n\n    Requirements:\n    - os\n    - time\n    - datetime, timedelta\n\n    Example:\n    >>> delete_old_files('/home/user/Downloads', 30)\n    \"\"\"\n    now = time.time()\n    cutoff = now - days * 86400\n\n    files = os.listdir(path)\n    for file in files:\n        file_path = os.path.join(path, file)\n        if os.path.isfile(file_path):\n            if os.path.getmtime(file_path) < cutoff:\n                os.remove(file_path)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, csv, pandas\n```python\nimport os\nimport csv\nimport pandas as pd\n\ndef write_dataframe_to_csv(file_path, df):\n    \"\"\"\n    Write a pandas DataFrame to a CSV file and then check if the file is empty or not.\n\n    Parameters:\n    file_path (str): The path to the file.\n    df (pandas.DataFrame): The DataFrame to write.\n\n    Returns:\n    bool: True if the file is not empty, False otherwise.\n\n    Requirements:\n    - os\n    - csv\n    - pandas\n\n    Example:\n    >>> df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n    >>> write_dataframe_to_csv('output.csv', df)\n    \"\"\"\n    df.to_csv(file_path, index=False)\n\n    return os.stat(file_path).st_size != 0\n```"}
{"seed": "{\"task_id\": 86430, \"prompt\": \"def f_86430(list, file):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"mi_path = file\\n\\tf = open(mi_path, 'a+')\\n\\t\\n\\tfor i in list:\\n\\t    f.write(i)\\n\\t\\n\\tf.close()\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    path = 'a.txt'\\n    list = [\\\"abc\\\", \\\"def\\\"]\\n    candidate(list, path)\\n    f = open(path, 'r')\\n    lines = f.readlines()\\n    line = lines[0]\\n    assert line == \\\"abcdef\\\"        \\n    f.close()\\n\"], \"entry_point\": \"f_86430\", \"intent\": \"\\u00bfC\\u00f3mo iterar una lista `list` y guardar sus valores en un archivo `file`?\", \"library\": []}", "generation": "Scenario 1:\nos, datetime, pytz, pandas, numpy, matplotlib\n```python\nimport os\nfrom datetime import datetime\nimport pytz\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['Date', 'Time', 'Temperature', 'Humidity', 'Pressure']\n\ndef f_86430_1(dirname):\n    \"\"\"\n    Iterate over a directory of CSV files, each representing weather data for a \n    specific day. For each file, load the data into a pandas DataFrame, and \n    resample the time series data to a one-hour frequency (from a one-minute\n    frequency). Then, plot the temperature, humidity, and pressure over time.\n\n    Parameters:\n    dirname (str): The directory containing the CSV files.\n\n    Returns:\n    None.\n\n    Requirements:\n    - os\n    - datetime\n    - pytz\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> f_86430_1('./weather_data/')\n    \"\"\"\n    files = os.listdir(dirname)\n    df_list = []\n\n    for file in files:\n        filepath = os.path.join(dirname, file)\n        df = pd.read_csv(filepath, names=COLUMNS, parse_dates={'Datetime': ['Date', 'Time']}, index_col='Datetime')\n        df = df.resample('1H').mean()\n        df_list.append(df)\n\n    df_all = pd.concat(df_list)\n\n    df_all['Temperature'].plot()\n    df_all['Humidity'].plot()\n    df_all['Pressure'].plot()\n\n    plt.legend()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, csv, json, random, string\n```python\nimport os\nimport csv\nimport json\nimport random\nimport string\n\n# Constants\nLETTERS = string.ascii_letters + string.digits\n\ndef f_86430_2(dirname, outfile):\n    \"\"\"\n    Iterate over a directory of CSV files, each containing user data (names and email addresses).\n    For each file, read the data and generate a unique identifier for each user.\n    Save the updated data (names, email addresses, and unique identifiers) as a JSON file.\n\n    Parameters:\n    dirname (str): The directory containing the CSV files.\n    outfile (str): The output JSON file.\n\n    Returns:\n    None.\n\n    Requirements:\n    - os\n    - csv\n    - json\n    - random\n    - string\n\n    Example:\n    >>> f_86430_2('./user_data/', 'updated_user_data.json')\n    \"\"\"\n    files = os.listdir(dirname)\n    user_data = []\n\n    for file in files:\n        filepath = os.path.join(dirname, file)\n        with open(filepath, 'r') as f:\n            reader = csv.reader(f)\n            next(reader)  # skip the header\n            for row in reader:\n                name, email = row\n                uid = ''.join(random.choices(LETTERS, k=10))\n                user_data.append({'name': name, 'email': email, 'uid': uid})\n\n    with open(outfile, 'w') as f:\n        json.dump(user_data, f)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, csv, pandas, seaborn, matplotlib\n```python\nimport os\nimport csv\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['Date', 'Close', 'Volume']\n\ndef f_86430_3(dirname):\n    \"\"\"\n    Iterate over a directory of CSV files, each containing stock market data (date, closing price, and volume).\n    For each file, load the data into a pandas DataFrame, and then visualize the closing price and volume over time \n    with a line plot and a bar plot, respectively.\n\n    Parameters:\n    dirname (str): The directory containing the CSV files.\n\n    Returns:\n    None.\n\n    Requirements:\n    - os\n    - csv\n    - pandas\n    - seaborn\n    - matplotlib.pyplot\n\n    Example:\n    >>> f_86430_3('./stock_data/')\n    \"\"\"\n    files = os.listdir(dirname)\n    df_list = []\n\n    for file in files:\n        filepath = os.path.join(dirname, file)\n        df = pd.read_csv(filepath, names=COLUMNS, parse_dates=['Date'], index_col='Date')\n        df_list.append(df)\n\n    df_all = pd.concat(df_list)\n\n    plt.figure(figsize=(12, 6))\n    sns.lineplot(data=df_all, x='Date', y='Close')\n    plt.figure(figsize=(12, 6))\n    sns.barplot(data=df_all, x='Date', y='Volume')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, csv, pandas, numpy, matplotlib\n```python\nimport os\nimport csv\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['Date', 'Sales']\n\ndef f_86430_4(dirname):\n    \"\"\"\n    Iterate over a directory of CSV files, each containing sales data (date and sales).\n    For each file, load the data into a pandas DataFrame, and then calculate the moving average \n    of sales with a window size of 7. Finally, plot the original sales and the moving average over time.\n\n    Parameters:\n    dirname (str): The directory containing the CSV files.\n\n    Returns:\n    None.\n\n    Requirements:\n    - os\n    - csv\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> f_86430_4('./sales_data/')\n    \"\"\"\n    files = os.listdir(dirname)\n    df_list = []\n\n    for file in files:\n        filepath = os.path.join(dirname, file)\n        df = pd.read_csv(filepath, names=COLUMNS, parse_dates=['Date'], index_col='Date')\n        df_list.append(df)\n\n    df_all = pd.concat(df_list)\n    df_all['Moving Average'] = df_all['Sales'].rolling(window=7).mean()\n\n    df_all[['Sales', 'Moving Average']].plot()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, csv, pandas, folium\n```python\nimport os\nimport csv\nimport pandas as pd\nimport folium\n\n# Constants\nCOLUMNS = ['Latitude', 'Longitude', 'Value']\n\ndef f_86430_5(dirname, outfile):\n    \"\"\"\n    Iterate over a directory of CSV files, each containing geographic data (latitude, longitude, and a value).\n    For each file, load the data into a pandas DataFrame, and then plot the locations on a map with \n    a circle marker, where the size of the circle represents the value.\n\n    Parameters:\n    dirname (str): The directory containing the CSV files.\n    outfile (str): The output HTML file.\n\n    Returns:\n    None.\n\n    Requirements:\n    - os\n    - csv\n    - pandas\n    - folium\n\n    Example:\n    >>> f_86430_5('./geo_data/', 'map.html')\n    \"\"\"\n    files = os.listdir(dirname)\n    df_list = []\n\n    for file in files:\n        filepath = os.path.join(dirname, file)\n        df = pd.read_csv(filepath, names=COLUMNS)\n        df_list.append(df)\n\n    df_all = pd.concat(df_list)\n\n    m = folium.Map(location=[df_all['Latitude'].mean(), df_all['Longitude'].mean()], zoom_start=3)\n\n    for i, row in df_all.iterrows():\n        folium.Circle(\n            location=[row['Latitude'], row['Longitude']],\n            radius=row['Value'],\n            color='blue',\n            fill=True,\n            fill_color='blue'\n        ).add_to(m)\n\n    m.save(outfile)\n```"}
{"seed": "{\"task_id\": 86430, \"prompt\": \"def f_86430(list, file):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"f = open(file, \\\"a\\\")\\n\\t\\n\\tfor i in list:\\n\\t    f.write(i)\\n\\t\\n\\tf.close()\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    path = 'a.txt'\\n    list = [\\\"abc\\\", \\\"def\\\"]\\n    candidate(list, path)\\n    f = open(path, 'r')\\n    lines = f.readlines()\\n    line = lines[0]\\n    assert line == \\\"abcdef\\\"        \\n    f.close()\\n\"], \"entry_point\": \"f_86430\", \"intent\": \"\\u00bfC\\u00f3mo iterar una lista `list` y guardar sus valores en un archivo `file`?\", \"library\": []}", "generation": "Scenario 1: \nos, pandas, numpy, random\n```python\nimport os\nimport pandas as pd\nimport numpy as np\nfrom random import randint\n\n# Constants\nAVAILABLE_PRODUCTS = ['TV', 'Laptop', 'Phone', 'Headphones', 'Mouse', 'Keyboard', 'Monitor', 'Printer', 'Speakers', 'Camera']\n\ndef generate_sales_report(file_path, sales_list):\n    \"\"\"\n    Iterate over a list of sales data where each entry is a tuple in the format \n    (product, quantity). Write the data to a CSV file. Also add a column for the \n    total sales (quantity * random price between 50 and 500).\n\n    Parameters:\n    file_path (str): Path to the output CSV file.\n    sales_list (list): A list of tuples where each tuple is in the format (product, quantity).\n\n    Returns:\n    str: The file path of the generated report.\n\n    Requirements:\n    - os\n    - pandas\n    - numpy\n    - random\n\n    Example:\n    >>> sales = [('TV', 100), ('Laptop', 50), ('Phone', 200), ('Headphones', 300)]\n    >>> generate_sales_report('/path/to/output.csv', sales)\n    \"\"\"\n    if not os.path.exists(os.path.dirname(file_path)):\n        os.makedirs(os.path.dirname(file_path))\n\n    report_data = []\n    for product, quantity in sales_list:\n        price = randint(50, 500)\n        total = quantity * price\n        report_data.append([product, quantity, price, total])\n\n    df = pd.DataFrame(report_data, columns=['Product', 'Quantity', 'Price', 'Total Sales'])\n    df.to_csv(file_path, index=False)\n\n    return file_path\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, json, random, time\n```python\nimport os\nimport json\nimport random\nimport time\n\n# Constants\nAVAILABLE_COLORS = ['Red', 'Green', 'Blue', 'Yellow', 'Black', 'White']\n\ndef create_car_data_json(file_path, car_list):\n    \"\"\"\n    Iterate over a list of car data where each car is a tuple in the format \n    (make, model, year). Write the data to a JSON file. Also add a random color \n    from a predefined list and current timestamp to each car.\n\n    Parameters:\n    file_path (str): Path to the output JSON file.\n    car_list (list): A list of tuples where each tuple is in the format (make, model, year).\n\n    Returns:\n    str: The file path of the generated JSON file.\n\n    Requirements:\n    - os\n    - json\n    - random\n    - time\n\n    Example:\n    >>> cars = [('Toyota', 'Corolla', 2020), ('Ford', 'Mustang', 2022), ('BMW', '3 Series', 2021)]\n    >>> create_car_data_json('/path/to/output.json', cars)\n    \"\"\"\n    if not os.path.exists(os.path.dirname(file_path)):\n        os.makedirs(os.path.dirname(file_path))\n\n    car_data = []\n    for make, model, year in car_list:\n        color = random.choice(AVAILABLE_COLORS)\n        timestamp = int(time.time())\n        car_data.append({'Make': make, 'Model': model, 'Year': year, 'Color': color, 'Timestamp': timestamp})\n\n    with open(file_path, 'w') as json_file:\n        json.dump(car_data, json_file)\n\n    return file_path\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, csv, random, datetime\n```python\nimport os\nimport csv\nimport random\nfrom datetime import datetime, timedelta\n\n# Constants\nEVENTS = ['Login', 'Logout', 'Purchase', 'View Product', 'Add to Cart', 'Remove from Cart']\n\ndef generate_user_activity_log(file_path, user_list, start_date, end_date):\n    \"\"\"\n    Iterate over a list of user IDs and generate a log of user activity between two dates. \n    Write the log to a CSV file.\n\n    Parameters:\n    file_path (str): Path to the output CSV file.\n    user_list (list): A list of user IDs.\n    start_date (datetime): The start date of the log.\n    end_date (datetime): The end date of the log.\n\n    Returns:\n    str: The file path of the generated log.\n\n    Requirements:\n    - os\n    - csv\n    - random\n    - datetime\n\n    Example:\n    >>> users = [1, 2, 3, 4, 5]\n    >>> start = datetime(2022, 1, 1)\n    >>> end = datetime(2022, 12, 31)\n    >>> generate_user_activity_log('/path/to/output.csv', users, start, end)\n    \"\"\"\n    if not os.path.exists(os.path.dirname(file_path)):\n        os.makedirs(os.path.dirname(file_path))\n\n    with open(file_path, 'w', newline='') as csv_file:\n        writer = csv.writer(csv_file)\n        writer.writerow(['UserID', 'Event', 'Timestamp'])\n\n        current_date = start_date\n        while current_date <= end_date:\n            user = random.choice(user_list)\n            event = random.choice(EVENTS)\n            writer.writerow([user, event, current_date])\n\n            current_date += timedelta(minutes=randint(1, 60))\n\n    return file_path\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, pandas, random, statistics\n```python\nimport os\nimport pandas as pd\nimport random\nimport statistics\n\n# Constants\nSTUDENTS = ['Alice', 'Bob', 'Charlie', 'David', 'Eve', 'Frank', 'Grace', 'Heidi', 'Ivan', 'Judy']\n\ndef generate_student_scores(file_path, student_list, num_exams):\n    \"\"\"\n    Iterate over a list of student names and generate a list of random scores \n    for a given number of exams. Write the scores to a CSV file. Also calculate \n    and write the average score for each student.\n\n    Parameters:\n    file_path (str): Path to the output CSV file.\n    student_list (list): A list of student names.\n    num_exams (int): The number of exams.\n\n    Returns:\n    str: The file path of the generated scores.\n\n    Requirements:\n    - os\n    - pandas\n    - random\n    - statistics\n\n    Example:\n    >>> generate_student_scores('/path/to/output.csv', STUDENTS, 5)\n    \"\"\"\n    if not os.path.exists(os.path.dirname(file_path)):\n        os.makedirs(os.path.dirname(file_path))\n\n    scores_data = []\n    for student in student_list:\n        scores = [random.randint(50, 100) for _ in range(num_exams)]\n        average = statistics.mean(scores)\n        scores_data.append([student] + scores + [average])\n\n    df = pd.DataFrame(scores_data, columns=['Student'] + [f'Exam {i+1}' for i in range(num_exams)] + ['Average'])\n    df.to_csv(file_path, index=False)\n\n    return file_path\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, csv, random, datetime\n```python\nimport os\nimport csv\nimport random\nfrom datetime import datetime, timedelta\n\n# Constants\nSENSORS = ['Temperature', 'Humidity', 'Pressure', 'Light', 'Sound', 'Motion']\n\ndef generate_sensor_data(file_path, sensor_list, start_datetime, end_datetime, interval_minutes):\n    \"\"\"\n    Iterate over a list of sensor names and generate sensor readings at regular \n    intervals between two datetimes. Write the readings to a CSV file.\n\n    Parameters:\n    file_path (str): Path to the output CSV file.\n    sensor_list (list): A list of sensor names.\n    start_datetime (datetime): The start datetime of the readings.\n    end_datetime (datetime): The end datetime of the readings.\n    interval_minutes (int): The interval in minutes between each reading.\n\n    Returns:\n    str: The file path of the generated readings.\n\n    Requirements:\n    - os\n    - csv\n    - random\n    - datetime\n\n    Example:\n    >>> start = datetime(2022, 1, 1, 0, 0, 0)\n    >>> end = datetime(2022, 1, 1, 23, 59, 59)\n    >>> generate_sensor_data('/path/to/output.csv', SENSORS, start, end, 15)\n    \"\"\"\n    if not os.path.exists(os.path.dirname(file_path)):\n        os.makedirs(os.path.dirname(file_path))\n\n    with open(file_path, 'w', newline='') as csv_file:\n        writer = csv.writer(csv_file)\n        writer.writerow(['Sensor', 'Timestamp', 'Reading'])\n\n        current_datetime = start_datetime\n        while current_datetime <= end_datetime:\n            for sensor in sensor_list:\n                reading = random.uniform(0, 100)\n                writer.writerow([sensor, current_datetime, reading])\n\n            current_datetime += timedelta(minutes=interval_minutes)\n\n    return file_path\n```\n"}
{"seed": "{\"task_id\": 183061, \"prompt\": \"def f_183061(df, col):\\n\\t\", \"suffix\": \"\\n\\treturn df\", \"canonical_solution\": \"df['col_1'] = df[col].str.replace(',', '.').astype(float)\", \"test_start\": \"\\nimport pandas as pd\\n\\ndef check(candidate):\", \"test\": [\"\\n    df = pd.DataFrame([\\\"4,99\\\", \\\"5,99\\\"], columns = ['vals'])\\n    candidate(df, 'vals')\\n    \\n    for i in range (0, len(df['vals'])):\\n        assert float(str(df['vals'][i]).replace(',', '.')) - df['col_1'][i] < 1e-6\\n\"], \"entry_point\": \"f_183061\", \"intent\": \"\\u00bfC\\u00f3mo reemplazar el separador decimal de comas por puntos en la columna `col` de un DataFrame `df`?\", \"library\": [\"pandas\"]}", "generation": "Scenario 1:\npandas, numpy, matplotlib, random\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nCITIES = ['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']\nWEATHER_CONDITIONS = ['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy']\nTEMPERATURE_RANGE = (-10, 50)  # in Celsius\n\ndef generate_weather_and_temp_report():\n    \"\"\"\n    Generate a report of weather conditions and temperatures for a list of cities.\n\n    Returns:\n    DataFrame: A pandas DataFrame with weather conditions and temperatures for the cities.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> report = generate_weather_and_temp_report()\n    >>> print(report)\n    >>> report['Temperature'].plot(kind='bar')\n    \"\"\"\n    weather_data = []\n    for city in CITIES:\n        weather = WEATHER_CONDITIONS[randint(0, len(WEATHER_CONDITIONS)-1)]\n        temperature = np.random.uniform(TEMPERATURE_RANGE[0], TEMPERATURE_RANGE[1])\n        weather_data.append([city, weather, temperature])\n\n    weather_df = pd.DataFrame(weather_data, columns=['City', 'Weather Condition', 'Temperature'])\n\n    return weather_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, matplotlib, numpy, random\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom random import shuffle\n\n# Constants\nPRODUCTS = ['Product A', 'Product B', 'Product C', 'Product D', 'Product E']\nSALE_PRICE_RANGE = (10, 200)  # in dollars\n\ndef generate_sales_report():\n    \"\"\"\n    Generate a sales report for a list of products with shuffled sale prices.\n\n    Returns:\n    DataFrame: A pandas DataFrame with products and their sale prices.\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - numpy\n    - random\n\n    Example:\n    >>> report = generate_sales_report()\n    >>> print(report)\n    >>> report['Sale Price'].plot(kind='bar')\n    \"\"\"\n    sale_prices = list(np.linspace(SALE_PRICE_RANGE[0], SALE_PRICE_RANGE[1], num=len(PRODUCTS)))\n    shuffle(sale_prices)\n\n    sales_data = []\n    for product in PRODUCTS:\n        sales_data.append([product, sale_prices.pop()])\n\n    sales_df = pd.DataFrame(sales_data, columns=['Product', 'Sale Price'])\n\n    return sales_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, datetime, matplotlib, random\n```python\nimport pandas as pd\nfrom datetime import datetime, timedelta\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nUSERS = ['User1', 'User2', 'User3', 'User4', 'User5']\nACTIVITY_TYPES = ['Login', 'Logout', 'Purchase', 'Review', 'Search']\n\ndef generate_user_activity_report():\n    \"\"\"\n    Generate a user activity report for a list of users over the past week.\n\n    Returns:\n    DataFrame: A pandas DataFrame with user activities.\n\n    Requirements:\n    - pandas\n    - datetime\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> report = generate_user_activity_report()\n    >>> print(report)\n    >>> report['Activity Type'].value_counts().plot(kind='bar')\n    \"\"\"\n    end_date = datetime.now()\n    start_date = end_date - timedelta(days=7)\n\n    activity_data = []\n    for user in USERS:\n        activity_date = start_date + timedelta(days=randint(0, 7))\n        activity_type = ACTIVITY_TYPES[randint(0, len(ACTIVITY_TYPES)-1)]\n        activity_data.append([user, activity_date, activity_type])\n\n    activity_df = pd.DataFrame(activity_data, columns=['User', 'Activity Date', 'Activity Type'])\n\n    return activity_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, matplotlib, numpy, random\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom random import shuffle\n\n# Constants\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nSCORE_RANGE = (50, 150)  # in points\n\ndef generate_team_scores():\n    \"\"\"\n    Generate a report of team scores with shuffled scores.\n\n    Returns:\n    DataFrame: A pandas DataFrame with teams and their scores.\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - numpy\n    - random\n\n    Example:\n    >>> report = generate_team_scores()\n    >>> print(report)\n    >>> report['Score'].plot(kind='bar')\n    \"\"\"\n    scores = list(np.linspace(SCORE_RANGE[0], SCORE_RANGE[1], num=len(TEAMS)))\n    shuffle(scores)\n\n    score_data = []\n    for team in TEAMS:\n        score_data.append([team, scores.pop()])\n\n    score_df = pd.DataFrame(score_data, columns=['Team', 'Score'])\n\n    return score_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, matplotlib, numpy, random\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom random import randint\n\n# Constants\nSTUDENTS = ['Student1', 'Student2', 'Student3', 'Student4', 'Student5']\nGRADE_RANGE = (0, 100)  # in percentage\n\ndef generate_student_grades():\n    \"\"\"\n    Generate a report of student grades with randomized grades.\n\n    Returns:\n    DataFrame: A pandas DataFrame with students and their grades.\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - numpy\n    - random\n\n    Example:\n    >>> report = generate_student_grades()\n    >>> print(report)\n    >>> report['Grade'].plot(kind='bar')\n    \"\"\"\n    grade_data = []\n    for student in STUDENTS:\n        grade = np.random.uniform(GRADE_RANGE[0], GRADE_RANGE[1])\n        grade_data.append([student, grade])\n\n    grade_df = pd.DataFrame(grade_data, columns=['Student', 'Grade'])\n\n    return grade_df\n```"}
{"seed": "{\"task_id\": 183061, \"prompt\": \"def f_183061(df, col):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"[float(x.replace(',','.')) for x in df[col]]\", \"test_start\": \"\\nimport pandas as pd\\n\\ndef check(candidate):\", \"test\": [\"\\n    df = pd.DataFrame([\\\"4,99\\\", \\\"5,99\\\"], columns = ['vals'])\\n    comas_por_puntos = candidate(df, 'vals')\\n    for i in range (0, len(df['vals'])):\\n        assert float(str(df['vals'][i]).replace(',', '.')) - comas_por_puntos[i] < 1e-6\\n\"], \"entry_point\": \"f_183061\", \"intent\": \"\\u00bfC\\u00f3mo reemplazar el separador decimal de comas por puntos en la columna `col` de un DataFrame `df`?\", \"library\": [\"pandas\"]}", "generation": "Scenario 1: \npandas, matplotlib, seaborn, numpy\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Constants\nCOLUMNS = ['Price', 'Sales', 'Profit']\n\ndef plot_distribution(df, col):\n    \"\"\"\n    Replace comma decimal separator with a dot in a DataFrame column,\n    convert the column to float, calculate basic statistics,\n    and plot the distribution of the data.\n\n    Parameters:\n    df (DataFrame): The input DataFrame.\n    col (str): The target column.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - seaborn\n    - numpy\n\n    Example:\n    >>> df = pd.DataFrame({\"Price\": [\"4,99\", \"5,99\", \"6,49\", \"7,99\", \"3,49\"]})\n    >>> plot_distribution(df, \"Price\")\n    \"\"\"\n    df[col] = [float(x.replace(',', '.')) for x in df[col]]\n    \n    print(f'Mean: {np.mean(df[col])}')\n    print(f'Standard Deviation: {np.std(df[col])}')\n    print(f'Median: {np.median(df[col])}')\n\n    sns.distplot(df[col])\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, sklearn.preprocessing\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Constants\nCOLUMNS = ['Price', 'Sales', 'Profit']\n\ndef normalize_dataframe(df, col):\n    \"\"\"\n    Replace comma decimal separator with a dot in a DataFrame column,\n    convert the column to float, and normalize the column using MinMaxScaler.\n\n    Parameters:\n    df (DataFrame): The input DataFrame.\n    col (str): The target column.\n\n    Returns:\n    DataFrame: the DataFrame with the normalized column.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.preprocessing\n\n    Example:\n    >>> df = pd.DataFrame({\"Price\": [\"4,99\", \"5,99\", \"6,49\", \"7,99\", \"3,49\"]})\n    >>> normalize_dataframe(df, \"Price\")\n    \"\"\"\n    df[col] = [float(x.replace(',', '.')) for x in df[col]]\n    scaler = MinMaxScaler()\n\n    df[col] = scaler.fit_transform(df[[col]])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['Price', 'Sales', 'Profit']\n\ndef plot_boxplot(df, col):\n    \"\"\"\n    Replace comma decimal separator with a dot in a DataFrame column,\n    convert the column to float, and plot a boxplot for the column.\n\n    Parameters:\n    df (DataFrame): The input DataFrame.\n    col (str): The target column.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib\n\n    Example:\n    >>> df = pd.DataFrame({\"Price\": [\"4,99\", \"5,99\", \"6,49\", \"7,99\", \"3,49\"]})\n    >>> plot_boxplot(df, \"Price\")\n    \"\"\"\n    df[col] = [float(x.replace(',', '.')) for x in df[col]]\n    plt.boxplot(df[col])\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, scipy.stats\n```python\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\n\n# Constants\nCOLUMNS = ['Price', 'Sales', 'Profit']\n\ndef calculate_zscore(df, col):\n    \"\"\"\n    Replace comma decimal separator with a dot in a DataFrame column,\n    convert the column to float, and calculate the Z-Score for the column.\n\n    Parameters:\n    df (DataFrame): The input DataFrame.\n    col (str): The target column.\n\n    Returns:\n    Series: the Z-Score for the column.\n\n    Requirements:\n    - pandas\n    - numpy\n    - scipy.stats\n\n    Example:\n    >>> df = pd.DataFrame({\"Price\": [\"4,99\", \"5,99\", \"6,49\", \"7,99\", \"3,49\"]})\n    >>> calculate_zscore(df, \"Price\")\n    \"\"\"\n    df[col] = [float(x.replace(',', '.')) for x in df[col]]\n    zscore = stats.zscore(df[col])\n\n    return zscore\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n\n# Constants\nCOLUMNS = ['Price', 'Sales', 'Profit']\n\ndef plot_heatmap(df):\n    \"\"\"\n    Replace comma decimal separator with a dot in a DataFrame column,\n    convert the column to float, and plot a heatmap for the correlation matrix of the DataFrame.\n\n    Parameters:\n    df (DataFrame): The input DataFrame.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - numpy\n    - seaborn\n\n    Example:\n    >>> df = pd.DataFrame({\"Price\": [\"4,99\", \"5,99\", \"6,49\", \"7,99\", \"3,49\"], \n                           \"Sales\": [\"100,00\", \"150,00\", \"200,00\", \"250,00\", \"300,00\"], \n                           \"Profit\": [\"50,00\", \"75,00\", \"100,00\", \"125,00\", \"150,00\"]})\n    >>> plot_heatmap(df)\n    \"\"\"\n    for col in COLUMNS:\n        df[col] = [float(x.replace(',', '.')) for x in df[col]]\n    \n    correlation = df.corr()\n    sns.heatmap(correlation, annot=True)\n    plt.show()\n```"}
{"seed": "{\"task_id\": 61046, \"prompt\": \"def f_61046(M):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"\\n\\twith open('file.csv', 'w', newline='', encoding='utf-8') as csvfile:\\n\\t\\twriter = csv.writer(csvfile)\\n\\t\\twriter.writerows(M)\\n\", \"test_start\": \"\\nimport csv \\n\\ndef check(candidate):\", \"test\": [\"\\n    f = open ('file.csv', 'w')\\n    f.close()\\n\\n    candidate([[1, 2, 3], [4, 5, 6]])\\n    with open ('file.csv', 'r') as f:\\n        lines = f.readlines()\\n        assert lines[0].strip() == '1,2,3'\\n        assert lines[1].strip() == '4,5,6'\\n\"], \"entry_point\": \"f_61046\", \"intent\": \"\\u00bfC\\u00f3mo guardar una matriz `M` en un archivo `file.csv`?\", \"library\": [\"csv\"]}", "generation": "Scenario 1:\npandas, csv, random, numpy, matplotlib\n```python\nimport pandas as pd\nimport csv\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCATEGORIES = ['Fruit', 'Vegetable', 'Meat', 'Dairy', 'Bakery', 'Beverages']\nFILE_NAME = \"store_inventory.csv\"\n\ndef generate_store_inventory(M):\n    \"\"\"\n    Generate a report of a store inventory with a list of categories and save it in a csv file.\n    Then load the csv file as a pandas DataFrame and plot the distribution of the inventory.\n    \n    Parameters:\n    M (list of lists): The 2D list containing the inventory data.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the inventory data.\n    \n    Requirements:\n    - pandas\n    - csv\n    - random\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> M = [[random.choice(CATEGORIES), random.randint(1,1000)] for _ in range(1000)]\n    >>> df = generate_store_inventory(M)\n    >>> print(df)\n    >>> df['Category'].value_counts().plot(kind='bar')\n    \"\"\"\n    with open(FILE_NAME, 'w', newline='', encoding='utf-8') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerows(M)\n        \n    df = pd.read_csv(FILE_NAME, names=['Category', 'Inventory'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \ncsv, random, os, matplotlib, pandas\n```python\nimport csv\nimport random\nimport os\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants\nFILE_NAME = \"random_numbers.csv\"\nN = 1000\nMIN_VAL = 1\nMAX_VAL = 100\n\ndef generate_and_plot_random_numbers():\n    \"\"\"\n    Generate a csv file with N random numbers between MIN_VAL and MAX_VAL,\n    then load the file and plot a histogram of the distribution of the numbers.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - csv\n    - random\n    - os\n    - matplotlib.pyplot\n    - pandas\n\n    Example:\n    >>> generate_and_plot_random_numbers()\n    \"\"\"\n    with open(FILE_NAME, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow([random.randint(MIN_VAL, MAX_VAL) for _ in range(N)])\n\n    df = pd.read_csv(FILE_NAME, header=None)\n    plt.hist(df.iloc[0], bins=20)\n    plt.show()\n\n    os.remove(FILE_NAME)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \ncsv, numpy, matplotlib, pandas\n```python\nimport csv\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants\nFILE_NAME = \"matrix.csv\"\n\ndef save_and_plot_matrix(M):\n    \"\"\"\n    Save a 2D numpy array (matrix) to a csv file, then load the file and plot a heatmap of the matrix.\n    \n    Parameters:\n    M (numpy array): The 2D numpy array to be saved and plotted.\n    \n    Returns:\n    None\n\n    Requirements:\n    - csv\n    - numpy\n    - matplotlib.pyplot\n    - pandas\n\n    Example:\n    >>> M = np.random.rand(10, 10)\n    >>> save_and_plot_matrix(M)\n    \"\"\"\n    np.savetxt(FILE_NAME, M, delimiter=\",\")\n    df = pd.read_csv(FILE_NAME, header=None)\n    plt.imshow(df, cmap='hot', interpolation='nearest')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ncsv, random, datetime, pandas\n```python\nimport csv\nimport random\nfrom datetime import datetime, timedelta\nimport pandas as pd\n\n# Constants\nFILE_NAME = \"temperature_data.csv\"\n\ndef generate_and_plot_temperature_data():\n    \"\"\"\n    Generate a csv file with random temperature data for the last 30 days, \n    then load the file and plot the temperature data.\n    \n    Returns:\n    None\n\n    Requirements:\n    - csv\n    - random\n    - datetime\n    - pandas\n\n    Example:\n    >>> generate_and_plot_temperature_data()\n    \"\"\"\n    with open(FILE_NAME, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        for i in range(30):\n            writer.writerow([(datetime.now() - timedelta(days=i)).strftime('%Y-%m-%d'), random.randint(-20, 40)])\n\n    df = pd.read_csv(FILE_NAME, names=['Date', 'Temperature'], parse_dates=['Date'])\n    df = df.sort_values('Date')\n    df.plot(x='Date', y='Temperature')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\ncsv, random, numpy, matplotlib, pandas\n```python\nimport csv\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants\nFILE_NAME = \"histogram_data.csv\"\n\ndef generate_and_plot_histogram_data():\n    \"\"\"\n    Generate a csv file with 1000 random numbers between 1 and 100, \n    then load the file and plot a histogram of the data.\n    \n    Returns:\n    None\n\n    Requirements:\n    - csv\n    - random\n    - numpy\n    - matplotlib.pyplot\n    - pandas\n\n    Example:\n    >>> generate_and_plot_histogram_data()\n    \"\"\"\n    with open(FILE_NAME, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow([random.randint(1, 100) for _ in range(1000)])\n\n    df = pd.read_csv(FILE_NAME, header=None)\n    plt.hist(df.iloc[0], bins=20)\n    plt.show()\n```\n"}
{"seed": "{\"task_id\": 109768, \"prompt\": \"def f_109768(oracion):\\n\\t\", \"suffix\": \"\\n\\treturn mayor, menor\", \"canonical_solution\": \"mayor=''\\n\\tmenor=''\\n\\tpalabra=''\\n\\t\\n\\tfor c in oracion:\\n\\t  if c == ' ':\\n\\t    if palabra != '':\\n\\t      mayor = palabra if len(mayor)<len(palabra) or mayor == '' else mayor\\n\\t      menor = palabra if len(menor)>len(palabra) or menor == '' else menor\\n\\t\\n\\t    palabra = ''\\n\\t\\n\\t  palabra = palabra + c\\n\\t\\n\\tprint(mayor)\\n\\tprint(menor)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(\\\"Maria had a little lamb named peepeepoopoo\\\") == (' little', ' a')\\n\", \"\\n    assert candidate(\\\"12345 123 1\\\") == ('12345', ' 123')\\n\"], \"entry_point\": \"f_109768\", \"intent\": \"\\u00bfC\\u00f3mo determinar la palabra m\\u00e1s larga `mayor` y la m\\u00e1s corta `menor` de un String `S`?\", \"library\": []}", "generation": "Scenario 1: \nnumpy, pandas, matplotlib, random\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import sample\n\n# Constants\nWORDS = ['Maria', 'had', 'a', 'little', 'lamb', 'named', 'peepeepoopoo']\n\ndef word_length_analysis(n_samples):\n    \"\"\"\n    Generate n_samples number of sentences by randomly selecting words from a list. \n    For each sentence, determine the longest and shortest word, and their lengths.\n    Finally, visualize the length distribution of the longest and shortest words.\n    \n    Parameters:\n    n_samples (int): The number of sentences to generate.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with length of longest and shortest words for each sentence.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - random\n    \n    Example:\n    >>> word_length_analysis(1000)\n    \"\"\"\n    analysis_data = []\n\n    for _ in range(n_samples):\n        sentence = sample(WORDS, len(WORDS))\n        longest_word = max(sentence, key=len)\n        shortest_word = min(sentence, key=len)\n        analysis_data.append([len(longest_word), len(shortest_word)])\n\n    analysis_df = pd.DataFrame(analysis_data, columns=['Longest Word', 'Shortest Word'])\n\n    analysis_df.hist(bins=np.arange(1, len(max(WORDS, key=len)) + 2) - 0.5, edgecolor='black')\n    plt.show()\n\n    return analysis_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nrandom, collections, re\n```python\nfrom random import choice\nfrom collections import Counter\nimport re\n\n# Constants\nLETTERS = \"abcdefghijklmnopqrstuvwxyz\"\n\ndef string_analysis(n_strings, string_length):\n    \"\"\"\n    Generate n_strings number of strings with string_length characters. \n    For each string, count the frequency of each character and return the most common character.\n    \n    Parameters:\n    n_strings (int): The number of strings to generate.\n    string_length (int): The length of each string.\n    \n    Returns:\n    list: A list of tuples containing the most common character and its count for each string.\n    \n    Requirements:\n    - random\n    - collections.Counter\n    - re\n    \n    Example:\n    >>> string_analysis(5, 10)\n    \"\"\"\n    common_chars = []\n\n    for _ in range(n_strings):\n        string = \"\".join(choice(LETTERS) for _ in range(string_length))\n        char_counts = Counter(re.findall(r'\\w', string))\n        common_chars.append(char_counts.most_common(1)[0])\n\n    return common_chars\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nrandom, re, nltk\n```python\nfrom random import sample\nimport re\nfrom nltk.corpus import words\n\n# Constants\nMIN_WORD_LENGTH = 3\nMAX_WORD_LENGTH = 7\n\ndef word_search(n_words):\n    \"\"\"\n    Select n_words number of words with lengths between MIN_WORD_LENGTH and MAX_WORD_LENGTH (inclusive) from the nltk corpus. \n    For each word, search for occurrences of the word in a predefined list and return their indices.\n    \n    Parameters:\n    n_words (int): The number of words to select.\n    \n    Returns:\n    dict: A dictionary with words as keys and list of indices as values.\n    \n    Requirements:\n    - random\n    - re\n    - nltk.corpus\n    \n    Example:\n    >>> word_search(5)\n    \"\"\"\n    selected_words = [word for word in words.words() if MIN_WORD_LENGTH <= len(word) <= MAX_WORD_LENGTH]\n    selected_words = sample(selected_words, n_words)\n\n    word_indices = {}\n    for word in selected_words:\n        word_indices[word] = [m.start() for m in re.finditer(word, \" \".join(words.words()))]\n\n    return word_indices\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nrandom, collections, string\n```python\nfrom random import choice\nfrom collections import Counter\nimport string\n\ndef random_string_analysis(n_strings, string_length):\n    \"\"\"\n    Generate n_strings number of strings with string_length characters. \n    For each string, count the frequency of each punctuation and return the most common punctuation.\n    \n    Parameters:\n    n_strings (int): The number of strings to generate.\n    string_length (int): The length of each string.\n    \n    Returns:\n    list: A list of tuples containing the most common punctuation and its count for each string.\n    \n    Requirements:\n    - random\n    - collections.Counter\n    - string\n    \n    Example:\n    >>> random_string_analysis(5, 10)\n    \"\"\"\n    common_punctuations = []\n\n    for _ in range(n_strings):\n        string = \"\".join(choice(string.punctuation) for _ in range(string_length))\n        punctuation_counts = Counter(string)\n        common_punctuations.append(punctuation_counts.most_common(1)[0])\n\n    return common_punctuations\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nrandom, re, nltk\n```python\nfrom random import sample\nimport re\nfrom nltk.corpus import words\n\n# Constants\nLETTERS = \"abcdefghijklmnopqrstuvwxyz\"\n\ndef word_substring_search(n_words, substring_length):\n    \"\"\"\n    Select n_words number of words from the nltk corpus. \n    For each word, search for occurrences of all possible substrings with substring_length characters in a predefined list and return their indices.\n    \n    Parameters:\n    n_words (int): The number of words to select.\n    substring_length (int): The length of the substring.\n    \n    Returns:\n    dict: A dictionary with substrings as keys and list of indices as values.\n    \n    Requirements:\n    - random\n    - re\n    - nltk.corpus\n    \n    Example:\n    >>> word_substring_search(5, 3)\n    \"\"\"\n    selected_words = sample(words.words(), n_words)\n\n    substring_indices = {}\n    for word in selected_words:\n        for i in range(len(word) - substring_length + 1):\n            substring = word[i:i+substring_length]\n            substring_indices[substring] = [m.start() for m in re.finditer(substring, \" \".join(words.words()))]\n\n    return substring_indices\n```\nAbove is the illustration."}
{"seed": "{\"task_id\": 191740, \"prompt\": \"def f_191740(df, col):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"df[col].std()\", \"test_start\": \"\\nimport pandas as pd\\nimport numpy as np\\n\\ndef check(candidate):\", \"test\": [\"\\n    data = {\\\"A\\\": (8.65, 24.53, 13.25, np.NaN, 4.03, 4.55, 14.31, 4.54, 8.57, 4.72),\\n            \\\"B\\\":  pd.timedelta_range('1s', periods=10),\\n            \\\"C\\\": (3, 12, 13, 4, 5, 7, 9, np.NaN, 17, 19)\\n            }\\n    df = pd.DataFrame(data)\\n    assert candidate(df, 'A') == 6.766256350449634\\n\"], \"entry_point\": \"f_191740\", \"intent\": \"\\u00bfC\\u00f3mo calcular la desviaci\\u00f3n est\\u00e1ndar de la columna `col` en un DataFrame `df`?\", \"library\": [\"numpy\", \"pandas\"]}", "generation": "Scenario 1:\npandas, numpy, matplotlib, math, random\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\nimport random\n\n# Constants\nCATEGORIES = ['Electronics', 'Fashion', 'Home & Garden', 'Sports', 'Automotive']\n\ndef calculate_and_plot_sales_std(df, col, category_col):\n    \"\"\"\n    Calculate the standard deviation of sales for each category in a DataFrame and plot a bar chart\n    for the standard deviations.\n\n    Parameters:\n    df (DataFrame): The DataFrame containing the sales data.\n    col (str): The name of the column containing the sales data.\n    category_col (str): The name of the column containing the category data.\n\n    Returns:\n    dict: A dictionary with categories as keys and standard deviations as values.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - math\n    - random\n\n    Example:\n    >>> data = {\n    ...     'Category': [random.choice(CATEGORIES) for _ in range(1000)],\n    ...     'Sales': [math.ceil(random.gauss(50, 15)) for _ in range(1000)]\n    ... }\n    >>> df = pd.DataFrame(data)\n    >>> stds = calculate_and_plot_sales_std(df, 'Sales', 'Category')\n    >>> print(stds)\n    \"\"\"\n    stds = df.groupby(category_col)[col].std().to_dict()\n\n    plt.bar(stds.keys(), stds.values())\n    plt.xlabel('Category')\n    plt.ylabel('Sales Standard Deviation')\n    plt.title('Standard Deviation of Sales by Category')\n    plt.show()\n\n    return stds\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, scipy.stats, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\n# Constants\nCATEGORIES = ['Electronics', 'Fashion', 'Home & Garden', 'Sports', 'Automotive']\n\ndef calculate_and_plot_confidence_interval(df, col, category_col, confidence=0.95):\n    \"\"\"\n    Calculate the confidence interval of the mean for each category in a DataFrame and plot a bar chart\n    with error bars for the confidence intervals.\n\n    Parameters:\n    df (DataFrame): The DataFrame containing the data.\n    col (str): The name of the column containing the data.\n    category_col (str): The name of the column containing the category data.\n    confidence (float): The confidence level to calculate the interval. Defaults to 0.95.\n\n    Returns:\n    dict: A dictionary with categories as keys and tuples of (mean, lower_bound, upper_bound) as values.\n\n    Requirements:\n    - pandas\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Example:\n    >>> data = {\n    ...     'Category': [random.choice(CATEGORIES) for _ in range(1000)],\n    ...     'Sales': [math.ceil(random.gauss(50, 15)) for _ in range(1000)]\n    ... }\n    >>> df = pd.DataFrame(data)\n    >>> intervals = calculate_and_plot_confidence_interval(df, 'Sales', 'Category')\n    >>> print(intervals)\n    \"\"\"\n    intervals = {}\n\n    for category in df[category_col].unique():\n        data = df[df[category_col] == category][col]\n        mean = np.mean(data)\n        se = stats.sem(data)\n        ci = stats.t.interval(confidence, len(data)-1, loc=mean, scale=se)\n        intervals[category] = (mean, *ci)\n\n    plt.bar(intervals.keys(), [mean for mean, _, _ in intervals.values()], \n            yerr=[(upper - lower) / 2 for _, lower, upper in intervals.values()])\n    plt.xlabel('Category')\n    plt.ylabel('Mean with Confidence Interval')\n    plt.title('Confidence Interval of Mean by Category')\n    plt.show()\n\n    return intervals\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, scipy.stats, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport seaborn as sns\n\ndef calculate_and_plot_correlation(df, col1, col2):\n    \"\"\"\n    Calculate the correlation coefficient between two columns in a DataFrame and plot a scatterplot\n    with a regression line.\n\n    Parameters:\n    df (DataFrame): The DataFrame containing the data.\n    col1, col2 (str): The names of the columns to calculate the correlation.\n\n    Returns:\n    float: The correlation coefficient.\n\n    Requirements:\n    - pandas\n    - numpy\n    - scipy.stats\n    - seaborn\n\n    Example:\n    >>> data = {\n    ...     'Age': [np.random.randint(20, 60) for _ in range(1000)],\n    ...     'Income': [np.random.randint(30000, 80000) for _ in range(1000)]\n    ... }\n    >>> df = pd.DataFrame(data)\n    >>> corr = calculate_and_plot_correlation(df, 'Age', 'Income')\n    >>> print(corr)\n    \"\"\"\n    corr, _ = stats.pearsonr(df[col1], df[col2])\n\n    sns.regplot(x=col1, y=col2, data=df, line_kws={'color': 'red'})\n    plt.title(f'Correlation between {col1} and {col2}: {corr:.2f}')\n    plt.show()\n\n    return corr\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, sklearn.preprocessing, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\n\ndef scale_and_plot_distribution(df, col):\n    \"\"\"\n    Scale a column in a DataFrame using Min-Max scaling and plot the distribution of the scaled data.\n\n    Parameters:\n    df (DataFrame): The DataFrame containing the data.\n    col (str): The name of the column to scale.\n\n    Returns:\n    Series: A pandas Series of the scaled data.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.preprocessing\n    - matplotlib.pyplot\n\n    Example:\n    >>> data = {\n    ...     'Income': [np.random.randint(30000, 80000) for _ in range(1000)]\n    ... }\n    >>> df = pd.DataFrame(data)\n    >>> scaled_data = scale_and_plot_distribution(df, 'Income')\n    >>> print(scaled_data)\n    \"\"\"\n    scaler = MinMaxScaler()\n    scaled_data = scaler.fit_transform(df[[col]])\n\n    plt.hist(scaled_data, bins=20, color='skyblue')\n    plt.xlabel('Scaled Income')\n    plt.ylabel('Frequency')\n    plt.title('Distribution of Scaled Income')\n    plt.show()\n\n    return pd.Series(scaled_data.flatten())\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, sklearn.decomposition, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\ndef perform_and_plot_pca(df, n_components):\n    \"\"\"\n    Perform Principal Component Analysis (PCA) on a DataFrame and plot the first two principal components.\n\n    Parameters:\n    df (DataFrame): The DataFrame containing the data.\n    n_components (int): The number of principal components to keep.\n\n    Returns:\n    DataFrame: A DataFrame of the principal components.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.decomposition\n    - matplotlib.pyplot\n\n    Example:\n    >>> data = {\n    ...     'Age': [np.random.randint(20, 60) for _ in range(1000)],\n    ...     'Income': [np.random.randint(30000, 80000) for _ in range(1000)],\n    ...     'Debt': [np.random.randint(0, 50000) for _ in range(1000)],\n    ...     'Savings': [np.random.randint(0, 50000) for _ in range(1000)]\n    ... }\n    >>> df = pd.DataFrame(data)\n    >>> pca_data = perform_and_plot_pca(df, 2)\n    >>> print(pca_data)\n    \"\"\"\n    pca = PCA(n_components=n_components)\n    principalComponents = pca.fit_transform(df)\n\n    pca_df = pd.DataFrame(data=principalComponents, \n                          columns=[f'Principal Component {i}' for i in range(1, n_components + 1)])\n\n    plt.figure(figsize=(8, 8))\n    plt.scatter(pca_df['Principal Component 1'], pca_df['Principal Component 2'])\n    plt.xlabel('Principal Component 1')\n    plt.ylabel('Principal Component 2')\n    plt.title('First Two Principal Components')\n    plt.show()\n\n    return pca_df\n```"}
