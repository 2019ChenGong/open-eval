{"seed": "{\"task_id\": 3283984, \"prompt\": \"def f_3283984():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"bytes.fromhex('4a4b4c').decode('utf-8')\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate() == \\\"JKL\\\"\\n\"], \"entry_point\": \"f_3283984\", \"intent\": \"decode a hex string '4a4b4c' to UTF-8.\", \"library\": []}", "generation": "Scenario 1: \nbinascii, base64, codecs, os\n```python\nimport binascii\nimport base64\nimport codecs\nimport os\n\n# Constants\nSAMPLE_HEX = '4a4b4c'\n\ndef generate_various_string_encodings(hex_string):\n    \"\"\"\n    Generate various string encodings (hex, base64, utf-8, utf-16, utf-32) \n    from a given hexadecimal string.\n\n    Parameters:\n    hex_string (str): The hexadecimal string.\n\n    Returns:\n    dict: A dictionary with various encoded strings.\n\n    Requirements:\n    - binascii\n    - base64\n    - codecs\n    - os\n\n    Example:\n    >>> generate_various_string_encodings(SAMPLE_HEX)\n    \"\"\"\n    encodings = {}\n\n    # Hexadecimal encoding\n    encodings['hex'] = binascii.hexlify(hex_string.encode()).decode()\n\n    # Base64 encoding\n    encodings['base64'] = base64.b64encode(hex_string.encode()).decode()\n\n    # UTF-8 encoding\n    encodings['utf-8'] = hex_string.encode('utf-8').decode()\n\n    # UTF-16 encoding\n    encodings['utf-16'] = hex_string.encode('utf-16').decode('utf-16')\n\n    # UTF-32 encoding\n    encodings['utf-32'] = hex_string.encode('utf-32').decode('utf-32')\n\n    return encodings\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nbinascii, string, random\n```python\nimport binascii\nimport string\nimport random\n\n# Constants\nHEX_CHARS = string.hexdigits.lower()\n\ndef generate_and_decode_random_hex_string(length):\n    \"\"\"\n    Generate a random hexadecimal string of a given length and then decode it to ASCII.\n\n    Parameters:\n    length (int): The length of the hexadecimal string.\n\n    Returns:\n    str: The decoded ASCII string.\n\n    Requirements:\n    - binascii\n    - string\n    - random\n\n    Example:\n    >>> generate_and_decode_random_hex_string(6)\n    \"\"\"\n    hex_string = ''.join(random.choice(HEX_CHARS) for _ in range(length))\n    ascii_string = binascii.unhexlify(hex_string).decode('utf-8', 'ignore')\n\n    return ascii_string\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nbinascii, os, hashlib\n```python\nimport binascii\nimport os\nimport hashlib\n\n# Constants\nSAMPLE_STRING = '4a4b4c'\n\ndef hash_and_encode_string(input_string):\n    \"\"\"\n    Hash an input string using SHA256, and then encode the hash in hexadecimal.\n\n    Parameters:\n    input_string (str): The input string.\n\n    Returns:\n    str: The hexadecimal encoded hash.\n\n    Requirements:\n    - binascii\n    - os\n    - hashlib\n\n    Example:\n    >>> hash_and_encode_string(SAMPLE_STRING)\n    \"\"\"\n    hashed_bytes = hashlib.sha256(input_string.encode()).digest()\n    hex_encoded_hash = binascii.hexlify(hashed_bytes).decode()\n\n    return hex_encoded_hash\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nbinascii, urllib.parse, codecs\n```python\nimport binascii\nimport urllib.parse\nimport codecs\n\n# Constants\nURL = 'https://www.example.com?q=4a4b4c'\n\ndef decode_query_from_url(url):\n    \"\"\"\n    Extract the query parameter from a URL, assume it's a hexadecimal string, \n    decode it to UTF-8 and return the result.\n\n    Parameters:\n    url (str): The URL.\n\n    Returns:\n    str: The decoded query parameter.\n\n    Requirements:\n    - binascii\n    - urllib.parse\n    - codecs\n\n    Example:\n    >>> decode_query_from_url(URL)\n    \"\"\"\n    parsed_url = urllib.parse.urlparse(url)\n    query = urllib.parse.parse_qs(parsed_url.query).get('q', [None])\n\n    if query[0] is None:\n        return None\n\n    decoded_query = binascii.unhexlify(query[0]).decode('utf-8', 'ignore')\n\n    return decoded_query\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nbinascii, io, gzip\n```python\nimport binascii\nimport io\nimport gzip\n\n# Constants\nCOMPRESSED_HEX = '1f8b08000000000002ff0b49494e55560304000000ffff8b202d0b000000'\n\ndef decompress_and_decode_hex_string(compressed_hex):\n    \"\"\"\n    Decompress a gzip-compressed hexadecimal string and decode the result to UTF-8.\n\n    Parameters:\n    compressed_hex (str): The gzip-compressed hexadecimal string.\n\n    Returns:\n    str: The decoded and decompressed string.\n\n    Requirements:\n    - binascii\n    - io\n    - gzip\n\n    Example:\n    >>> decompress_and_decode_hex_string(COMPRESSED_HEX)\n    \"\"\"\n    compressed_bytes = binascii.unhexlify(compressed_hex)\n    decompressed_bytes = gzip.GzipFile(fileobj=io.BytesIO(compressed_bytes)).read()\n    decoded_string = decompressed_bytes.decode('utf-8')\n\n    return decoded_string\n```"}
{"seed": "{\"task_id\": 3844801, \"prompt\": \"def f_3844801(myList):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"all(x == myList[0] for x in myList)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([1,2,3]) == False\\n\", \"\\n    assert candidate([1,1,1,1,1,1]) == True\\n\", \"\\n    assert candidate([1]) == True\\n\", \"\\n    assert candidate(['k','k','k','k','k']) == True\\n\", \"\\n    assert candidate([None,'%$#ga',3]) == False\\n\"], \"entry_point\": \"f_3844801\", \"intent\": \"check if all elements in list `myList` are identical\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, matplotlib.pyplot\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCATEGORIES = ['A', 'B', 'C', 'D', 'E']\n\ndef check_distribution(data_list):\n    \"\"\"\n    Check if the distribution of categories in the given list is uniform by generating a histogram.\n\n    Parameters:\n    data_list (list): The list of categories.\n\n    Returns:\n    matplotlib.pyplot: The histogram of the category distribution.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> data = ['A', 'A', 'B', 'B', 'B', 'C', 'C', 'C', 'C', 'D', 'E', 'E']\n    >>> check_distribution(data)\n    \"\"\"\n    data_series = pd.Series(data_list)\n    category_counts = data_series.value_counts()\n    \n    if not all(x == category_counts[0] for x in category_counts):\n        print(\"The distribution of categories is not uniform.\")\n        \n    plt.hist(data_list, bins=np.arange(len(CATEGORIES)+1)-0.5, rwidth=0.8)\n    plt.xticks(CATEGORIES)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncollections, numpy, matplotlib.pyplot\n```python\nimport collections\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef check_distribution_dict(data_dict):\n    \"\"\"\n    Check if the distribution of values in the given dictionary is uniform by generating a histogram.\n\n    Parameters:\n    data_dict (dict): The dictionary of categories and their counts.\n\n    Returns:\n    matplotlib.pyplot: The histogram of the category distribution.\n\n    Requirements:\n    - collections\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> data = {'A': 2, 'B': 3, 'C': 4, 'D': 1, 'E': 2}\n    >>> check_distribution_dict(data)\n    \"\"\"\n    data_counter = collections.Counter(data_dict)\n    counts = list(data_counter.values())\n\n    if not all(x == counts[0] for x in counts):\n        print(\"The distribution of categories is not uniform.\")\n        \n    plt.hist(counts, bins=np.arange(len(counts)+1)-0.5, rwidth=0.8)\n    plt.xticks(list(data_counter.keys()))\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, matplotlib.pyplot\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef check_uniformity_dataframe(df, column_name):\n    \"\"\"\n    Check if the distribution of values in the specified column of the given DataFrame is uniform by \n    generating a histogram.\n\n    Parameters:\n    df (DataFrame): The DataFrame.\n    column_name (str): The column name.\n\n    Returns:\n    matplotlib.pyplot: The histogram of the value distribution.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame({'Category': ['A', 'A', 'B', 'B', 'B', 'C', 'C', 'C', 'C', 'D', 'E', 'E']})\n    >>> check_uniformity_dataframe(df, 'Category')\n    \"\"\"\n    column_values = df[column_name].value_counts()\n    \n    if not all(x == column_values[0] for x in column_values):\n        print(\"The distribution of values is not uniform.\")\n        \n    plt.hist(df[column_name], bins=np.arange(len(column_values)+1)-0.5, rwidth=0.8)\n    plt.xticks(column_values.index)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, matplotlib.pyplot\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef check_distribution_dataframe(df):\n    \"\"\"\n    Check if the distribution of values in all columns of the given DataFrame is uniform by \n    generating a histogram for each column.\n\n    Parameters:\n    df (DataFrame): The DataFrame.\n\n    Returns:\n    matplotlib.pyplot: The histograms of the value distributions.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame({'Category1': ['A', 'A', 'B', 'B', 'B', 'C', 'C', 'C', 'C', 'D', 'E', 'E'],\n    ...                    'Category2': ['X', 'Y', 'Y', 'Z', 'Z', 'Z', 'Z', 'W', 'W', 'W', 'W', 'W']})\n    >>> check_distribution_dataframe(df)\n    \"\"\"\n    for column in df.columns:\n        column_values = df[column].value_counts()\n        \n        if not all(x == column_values[0] for x in column_values):\n            print(f\"The distribution of values in column {column} is not uniform.\")\n            \n        plt.hist(df[column], bins=np.arange(len(column_values)+1)-0.5, rwidth=0.8)\n        plt.xticks(column_values.index)\n        plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, matplotlib.pyplot\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef check_distribution_array(arr):\n    \"\"\"\n    Check if the distribution of values in the given numpy array is uniform by generating a histogram.\n\n    Parameters:\n    arr (numpy array): The numpy array of values.\n\n    Returns:\n    matplotlib.pyplot: The histogram of the value distribution.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> arr = np.array(['A', 'A', 'B', 'B', 'B', 'C', 'C', 'C', 'C', 'D', 'E', 'E'])\n    >>> check_distribution_array(arr)\n    \"\"\"\n    unique, counts = np.unique(arr, return_counts=True)\n    value_counts = dict(zip(unique, counts))\n    \n    if not all(x == counts[0] for x in counts):\n        print(\"The distribution of values is not uniform.\")\n        \n    plt.hist(arr, bins=np.arange(len(value_counts)+1)-0.5, rwidth=0.8)\n    plt.xticks(unique)\n    plt.show()\n```"}
{"seed": "{\"task_id\": 4302166, \"prompt\": \"def f_4302166():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"'%*s : %*s' % (20, 'Python', 20, 'Very Good')\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate() == '              Python :            Very Good'\\n\"], \"entry_point\": \"f_4302166\", \"intent\": \"format number of spaces between strings `Python`, `:` and `Very Good` to be `20`\", \"library\": []}", "generation": "Scenario 1:\n```python\nimport os\nimport sys\nimport json\nimport re\n\n# Constants\nLOG_FILE = '/path/to/log_file.log'\nKEYWORDS = ['ERROR', 'CRITICAL', 'WARNING']\n\ndef format_log_file():\n    \"\"\"\n    Check a log file and format the lines containing certain keywords.\n    Each line is formatted to display the keyword, the timestamp and the message \n    separated by 20 spaces.\n    \n    Requirements:\n    - os\n    - sys\n    - json\n    - re\n    \n    Example:\n    >>> format_log_file()\n    \"\"\"\n    if not os.path.exists(LOG_FILE):\n        print(f\"Log file {LOG_FILE} does not exist.\")\n        sys.exit(1)\n\n    with open(LOG_FILE, 'r') as log:\n        for line in log:\n            for keyword in KEYWORDS:\n                if keyword in line:\n                    parts = re.split(r'\\s+', line.strip(), 2)\n                    print('%*s : %*s : %*s' % (20, parts[0], 20, parts[1], 20, parts[2]))\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\n```python\nimport requests\nimport bs4\nimport csv\n\n# Constants\nURL = 'https://www.example.com'\nCSV_FILE = '/path/to/data.csv'\n\ndef scrape_and_format():\n    \"\"\"\n    Scrape a webpage, extract certain text and write it to a CSV file.\n    Each line in the CSV file has three columns separated by 20 spaces.\n    \n    Requirements:\n    - requests\n    - bs4.BeautifulSoup\n    - csv\n    \n    Example:\n    >>> scrape_and_format()\n    \"\"\"\n    response = requests.get(URL)\n    soup = bs4.BeautifulSoup(response.text, 'html.parser')\n    data = []\n    for div in soup.find_all('div', class_='container'):\n        title = div.find('h1').text\n        date = div.find('span', class_='date').text\n        author = div.find('span', class_='author').text\n        data.append([title, date, author])\n\n    with open(CSV_FILE, 'w') as csv_file:\n        writer = csv.writer(csv_file)\n        for row in data:\n            formatted_row = '%*s : %*s : %*s' % (20, row[0], 20, row[1], 20, row[2])\n            writer.writerow([formatted_row])\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCSV_FILE = '/path/to/data.csv'\n\ndef analyze_and_plot():\n    \"\"\"\n    Load a CSV file into a pandas DataFrame, perform some analysis on the data and \n    plot the results. The plot title, x-label and y-label are formatted to have 20 spaces \n    on either side.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> analyze_and_plot()\n    \"\"\"\n    df = pd.read_csv(CSV_FILE)\n    mean = df['column1'].mean()\n    std = df['column1'].std()\n    df['column1_normalized'] = (df['column1'] - mean) / std\n\n    plt.figure(figsize=(10, 6))\n    plt.plot(df['column1_normalized'], label='Normalized Column 1')\n    plt.title('%*s : %*s' % (20, 'Plot Title', 20, 'Normalized Column 1'))\n    plt.xlabel('%*s : %*s' % (20, 'Index', 20, 'Normalized Value'))\n    plt.ylabel('%*s : %*s' % (20, 'Frequency', 20, 'Normalized Value'))\n    plt.legend()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCSV_FILE = '/path/to/data.csv'\n\ndef plot_graph():\n    \"\"\"\n    Load a CSV file into a pandas DataFrame, perform a groupby operation on the data and \n    plot the results. The plot title, x-label and y-label are formatted to have 20 spaces \n    on either side.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> plot_graph()\n    \"\"\"\n    df = pd.read_csv(CSV_FILE)\n    groupby_data = df.groupby('column1')['column2'].mean()\n\n    plt.figure(figsize=(10, 6))\n    plt.bar(groupby_data.index, groupby_data.values)\n    plt.title('%*s : %*s' % (20, 'Bar Plot', 20, 'Grouped by Column 1'))\n    plt.xlabel('%*s : %*s' % (20, 'Column 1', 20, 'Group Value'))\n    plt.ylabel('%*s : %*s' % (20, 'Mean', 20, 'Column 2'))\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\n```python\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\n\n# Constants\nCSV_FILE = '/path/to/data.csv'\n\ndef train_and_evaluate():\n    \"\"\"\n    Load a CSV file into a pandas DataFrame, split the data into training and test sets, \n    train a random forest classifier and print the classification report. The output is \n    formatted to have 20 spaces on either side of each line.\n    \n    Requirements:\n    - pandas\n    - sklearn.model_selection.train_test_split\n    - sklearn.ensemble.RandomForestClassifier\n    - sklearn.metrics.classification_report\n    \n    Example:\n    >>> train_and_evaluate()\n    \"\"\"\n    df = pd.read_csv(CSV_FILE)\n    X = df.drop('target', axis=1)\n    y = df['target']\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    clf = RandomForestClassifier(n_estimators=100, random_state=42)\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n\n    report = classification_report(y_test, y_pred)\n    formatted_report = '\\n'.join(['%*s : %*s' % (20, line.split(':')[0], 20, line.split(':')[1]) for line in report.split('\\n') if ':' in line])\n    print(formatted_report)\n```"}
{"seed": "{\"task_id\": 7555335, \"prompt\": \"def f_7555335(d):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"d.decode('cp1251').encode('utf8')\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('hello world!'.encode('cp1251')) == b'hello world!'\\n\", \"\\n    assert candidate('%*(^O*'.encode('cp1251')) == b'%*(^O*'\\n\", \"\\n    assert candidate(''.encode('cp1251')) == b''\\n\", \"\\n    assert candidate('hello world!'.encode('cp1251')) != 'hello world!'\\n\"], \"entry_point\": \"f_7555335\", \"intent\": \"convert a string `d` from CP-1251 to UTF-8\", \"library\": []}", "generation": "Scenario 1: \ncodecs, os, sys, glob\n```python\nimport codecs\nimport os\nimport sys\nimport glob\n\n# Constants\nDIRECTORY_PATH = './files/'\n\ndef convert_files_encoding(directory=DIRECTORY_PATH, from_encoding='cp1251', to_encoding='utf8'):\n    \"\"\"\n    Convert the encoding of all text files in a directory from one encoding to another.\n    \n    Parameters:\n    directory (str): The directory where the text files are located.\n    from_encoding (str): The original encoding of the text files.\n    to_encoding (str): The encoding to which the text files should be converted.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - codecs\n    - os\n    - sys\n    - glob\n    \n    Example:\n    >>> convert_files_encoding('./files/', 'cp1251', 'utf8')\n    \"\"\"\n    for filename in glob.glob(os.path.join(directory, '*.txt')):\n        with codecs.open(filename, 'r', from_encoding) as file:\n            content = file.read()\n\n        with codecs.open(filename, 'w', to_encoding) as file:\n            file.write(content)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \ncsv, codecs\n```python\nimport csv\nimport codecs\n\n# Constants\nCSV_FILE = 'sample.csv'\n\ndef convert_csv_encoding(filename=CSV_FILE, from_encoding='cp1251', to_encoding='utf8'):\n    \"\"\"\n    Convert the encoding of a CSV file from one encoding to another and return a list of dictionaries.\n    \n    Parameters:\n    filename (str): The name of the CSV file.\n    from_encoding (str): The original encoding of the CSV file.\n    to_encoding (str): The encoding to which the CSV file should be converted.\n    \n    Returns:\n    list: A list of dictionaries. Each dictionary represents a row in the CSV file.\n    \n    Requirements:\n    - csv\n    - codecs\n    \n    Example:\n    >>> data = convert_csv_encoding('sample.csv', 'cp1251', 'utf8')\n    >>> print(data)\n    \"\"\"\n    with codecs.open(filename, 'r', from_encoding) as file:\n        reader = csv.DictReader(file)\n        data = list(reader)\n\n    with codecs.open(filename, 'w', to_encoding) as file:\n        writer = csv.DictWriter(file, fieldnames=data[0].keys())\n        writer.writeheader()\n        writer.writerows(data)\n\n    return data\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \ncodecs, BeautifulSoup, requests\n```python\nimport codecs\nfrom bs4 import BeautifulSoup\nimport requests\n\n# Constants\nURL = 'http://example.com'\n\ndef convert_webpage_encoding(url=URL, from_encoding='cp1251', to_encoding='utf8'):\n    \"\"\"\n    Fetch a webpage, convert the encoding from one encoding to another and return the parsed HTML.\n    \n    Parameters:\n    url (str): The URL of the webpage.\n    from_encoding (str): The original encoding of the webpage.\n    to_encoding (str): The encoding to which the webpage should be converted.\n    \n    Returns:\n    BeautifulSoup: The parsed HTML of the webpage.\n    \n    Requirements:\n    - codecs\n    - BeautifulSoup\n    - requests\n    \n    Example:\n    >>> html = convert_webpage_encoding('http://example.com', 'cp1251', 'utf8')\n    >>> print(html.prettify())\n    \"\"\"\n    response = requests.get(url)\n    content = response.content.decode(from_encoding).encode(to_encoding)\n\n    soup = BeautifulSoup(content, 'html.parser')\n\n    return soup\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \ncodecs, PIL, io\n```python\nimport codecs\nfrom PIL import Image\nimport io\n\n# Constants\nIMAGE_PATH = 'image.png'\n\ndef convert_image_text_encoding(filename=IMAGE_PATH, from_encoding='cp1251', to_encoding='utf8'):\n    \"\"\"\n    Open an image, extract the text, convert the encoding from one encoding to another and return the converted text.\n    \n    Parameters:\n    filename (str): The path to the image file.\n    from_encoding (str): The original encoding of the text.\n    to_encoding (str): The encoding to which the text should be converted.\n    \n    Returns:\n    str: The converted text.\n    \n    Requirements:\n    - codecs\n    - PIL.Image\n    - io\n    \n    Example:\n    >>> text = convert_image_text_encoding('image.png', 'cp1251', 'utf8')\n    >>> print(text)\n    \"\"\"\n    image = Image.open(filename)\n    text = image.info.get('comment', '').decode(from_encoding).encode(to_encoding)\n\n    return text.decode(to_encoding)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \ncodecs, json, requests\n```python\nimport codecs\nimport json\nimport requests\n\n# Constants\nAPI_URL = 'http://api.example.com/data'\n\ndef convert_api_response_encoding(url=API_URL, from_encoding='cp1251', to_encoding='utf8'):\n    \"\"\"\n    Fetch data from a REST API, convert the encoding from one encoding to another and return the data.\n    \n    Parameters:\n    url (str): The URL of the REST API.\n    from_encoding (str): The original encoding of the data.\n    to_encoding (str): The encoding to which the data should be converted.\n    \n    Returns:\n    dict: The converted data.\n    \n    Requirements:\n    - codecs\n    - json\n    - requests\n    \n    Example:\n    >>> data = convert_api_response_encoding('http://api.example.com/data', 'cp1251', 'utf8')\n    >>> print(data)\n    \"\"\"\n    response = requests.get(url)\n    content = response.content.decode(from_encoding).encode(to_encoding)\n\n    data = json.loads(content)\n\n    return data\n```"}
{"seed": "{\"task_id\": 2544710, \"prompt\": \"def f_2544710(kwargs):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"{k: v for k, v in list(kwargs.items()) if v is not None}\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate({i: None for i in range(10)}) == {}\\n\", \"\\n    assert candidate({i: min(i,4) for i in range(6)}) == {0:0,1:1,2:2,3:3,4:4,5:4}\\n\", \"\\n    assert candidate({'abc': 'abc'})['abc'] == 'abc'\\n\", \"\\n    assert candidate({'x': None, 'yy': 234}) == {'yy': 234}\\n\"], \"entry_point\": \"f_2544710\", \"intent\": \"get rid of None values in dictionary `kwargs`\", \"library\": []}", "generation": "Scenario 1:\ncollections, json, os\n```python\nimport collections\nimport json\nimport os\n\n# Constants\nPREFIXES = [\"is_\", \"has_\", \"can_\", \"should_\"]\n\ndef generate_statistics_from_json_files(directory):\n    \"\"\"\n    Read all JSON files from a directory, count the occurrence of keys starting with certain prefixes\n    and return a dictionary with the statistics.\n\n    Parameters:\n    directory (str): The directory path where the JSON files are located.\n\n    Returns:\n    dict: A dictionary with keys as prefixes and values as counts.\n\n    Requirements:\n    - collections\n    - json\n    - os\n\n    Example:\n    >>> generate_statistics_from_json_files('/path/to/json/files')\n    \"\"\"\n    stats = collections.defaultdict(int)\n\n    for filename in os.listdir(directory):\n        if filename.endswith('.json'):\n            with open(f'{directory}/{filename}', 'r') as f:\n                data = json.load(f)\n\n            for key in data.keys():\n                for prefix in PREFIXES:\n                    if key.startswith(prefix):\n                        stats[prefix] += 1\n\n    return dict(stats)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncsv, pandas, datetime\n```python\nimport csv\nimport pandas as pd\nfrom datetime import datetime\n\n# Constants\nCSV_FILE_PATH = 'path/to/csvfile.csv'\nDATE_FORMAT = '%Y-%m-%d'\n\ndef filter_and_sort_csv_by_date(column_name):\n    \"\"\"\n    Filter and sort a CSV file using a date column.\n\n    Parameters:\n    column_name (str): The column name in the CSV file.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the filtered and sorted data.\n\n    Requirements:\n    - csv\n    - pandas\n    - datetime\n\n    Example:\n    >>> filter_and_sort_csv_by_date('Date')\n    \"\"\"\n    df = pd.read_csv(CSV_FILE_PATH)\n    df[column_name] = pd.to_datetime(df[column_name], format=DATE_FORMAT)\n    df = df[df[column_name] >= datetime.now()]\n    df = df.sort_values(by=column_name)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nmath, random, statistics\n```python\nimport math\nimport random\nimport statistics\n\n# Constants\nRADIUS = 5\n\ndef generate_random_points_within_circle(n):\n    \"\"\"\n    Generate n random points within a circle of a certain radius and return their average distance from the center.\n\n    Parameters:\n    n (int): The number of points to be generated.\n\n    Returns:\n    float: The average distance from the center.\n\n    Requirements:\n    - math\n    - random\n    - statistics\n\n    Example:\n    >>> generate_random_points_within_circle(100)\n    \"\"\"\n    distances = []\n\n    for _ in range(n):\n        theta = 2 * math.pi * random.random()\n        r = RADIUS * math.sqrt(random.random())\n        x = r * math.cos(theta)\n        y = r * math.sin(theta)\n        distance = math.sqrt(x**2 + y**2)\n        distances.append(distance)\n\n    return statistics.mean(distances)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, shutil, fnmatch\n```python\nimport os\nimport shutil\nimport fnmatch\n\n# Constants\nSOURCE_DIRECTORY = '/path/to/source_directory'\nDESTINATION_DIRECTORY = '/path/to/destination_directory'\nFILE_PATTERN = '*.txt'\n\ndef move_files_from_directory():\n    \"\"\"\n    Move all files matching a certain pattern from one directory to another.\n\n    Requirements:\n    - os\n    - shutil\n    - fnmatch\n\n    Example:\n    >>> move_files_from_directory()\n    \"\"\"\n    for path, dirs, files in os.walk(SOURCE_DIRECTORY):\n        for filename in fnmatch.filter(files, FILE_PATTERN):\n            shutil.move(os.path.join(path, filename), DESTINATION_DIRECTORY)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nDATAFRAME = pd.DataFrame({\n    'A': np.random.rand(100),\n    'B': np.random.rand(100),\n    'C': np.random.rand(100)\n})\n\ndef plot_highest_correlation_pairs():\n    \"\"\"\n    Calculate the correlation matrix of a DataFrame, find the pairs of columns with the highest correlation,\n    and plot their scatter plots.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_highest_correlation_pairs()\n    \"\"\"\n    corr = DATAFRAME.corr()\n    corr_abs = corr.abs()\n\n    sol = (corr.where(np.triu(np.ones(corr.shape), k=1).astype(np.bool))\n                 .stack()\n                 .sort_values(ascending=False))\n\n    first_pair = sol.index[0]\n\n    DATAFRAME.plot(kind='scatter', x=first_pair[0], y=first_pair[1])\n\n    plt.show()\n```"}
{"seed": "{\"task_id\": 2544710, \"prompt\": \"def f_2544710(kwargs):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"dict((k, v) for k, v in kwargs.items() if v is not None)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate({i: None for i in range(10)}) == {}\\n\", \"\\n    assert candidate({i: min(i,4) for i in range(6)}) == {0:0,1:1,2:2,3:3,4:4,5:4}\\n\", \"\\n    assert candidate({'abc': 'abc'})['abc'] == 'abc'\\n\", \"\\n    assert candidate({'x': None, 'yy': 234}) == {'yy': 234}\\n\"], \"entry_point\": \"f_2544710\", \"intent\": \"get rid of None values in dictionary `kwargs`\", \"library\": []}", "generation": "Scenario 1: \njson, re, collections\n```python\nimport json\nimport re\nfrom collections import Counter\n\n# Constants\nREPLACE_NONE = \"None\"\n\ndef process_json(json_str):\n    \"\"\"\n    Process a JSON string by removing None values, counting the frequency of each unique value,\n    and replacing any email addresses with a placeholder.\n\n    Parameters:\n    json_str (str): The JSON string.\n\n    Returns:\n    dict: A dictionary with the processed JSON data.\n\n    Requirements:\n    - json\n    - re\n    - collections.Counter\n\n    Example:\n    >>> json_str = '{\"name\": \"John\", \"age\": None, \"email\": \"john@example.com\"}'\n    >>> process_json(json_str)\n    \"\"\"\n    data = json.loads(json_str)\n    data = {k: v for k, v in data.items() if v is not None}\n\n    value_counts = Counter(data.values())\n    data = {k: REPLACE_NONE if isinstance(v, str) and re.match(r\"[^@]+@[^@]+\\.[^@]+\", v) else v for k, v in data.items()}\n\n    return {\"data\": data, \"value_counts\": value_counts}\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, pandas, seaborn\n```python\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n# Constants\nPLOT_TITLE = \"Value Distribution\"\n\ndef analyze_data(kwargs):\n    \"\"\"\n    Analyze the data from a dictionary by creating a Pandas DataFrame, removing None values \n    and plotting the distribution of values.\n\n    Parameters:\n    kwargs (dict): The input dictionary.\n\n    Returns:\n    DataFrame, Plot: A pandas DataFrame with the data and a Seaborn plot.\n\n    Requirements:\n    - pandas\n    - numpy\n    - seaborn\n\n    Example:\n    >>> data = {'a': [1, 2, None, 4], 'b': [5, None, 7, 8]}\n    >>> df, plot = analyze_data(data)\n    \"\"\"\n    df = pd.DataFrame(kwargs)\n    df = df.dropna()\n\n    plot = sns.distplot(df.values.flatten(), bins=10)\n    plot.set_title(PLOT_TITLE)\n\n    return df, plot\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, pathlib, shutil\n```python\nimport os\nfrom pathlib import Path\nimport shutil\n\n# Constants\nTARGET_DIR = \"non_none_files\"\n\ndef process_files(kwargs):\n    \"\"\"\n    Process files from a dictionary by checking if the file exists, if its content is not None,\n    and then copying it to a target directory.\n\n    Parameters:\n    kwargs (dict): A dictionary where keys are file names and values are the file content.\n\n    Returns:\n    list: A list of file paths that were copied.\n\n    Requirements:\n    - os\n    - pathlib.Path\n    - shutil\n\n    Example:\n    >>> files = {'file1.txt': 'Hello', 'file2.txt': None, 'file3.txt': 'World'}\n    >>> process_files(files)\n    \"\"\"\n    copied_files = []\n\n    for file, content in kwargs.items():\n        if content is not None and os.path.isfile(file):\n            target_file = Path(TARGET_DIR) / file\n            shutil.copyfile(file, target_file)\n            copied_files.append(target_file)\n\n    return copied_files\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, matplotlib.pyplot, sklearn.preprocessing\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Constants\nPLOT_TITLE = \"Scaled Values\"\n\ndef scale_and_plot(kwargs):\n    \"\"\"\n    Scale the values in a dictionary using MinMaxScaler, create a pandas DataFrame,\n    and plot the scaled values.\n\n    Parameters:\n    kwargs (dict): The input dictionary.\n\n    Returns:\n    DataFrame, Plot: A pandas DataFrame with the scaled data and a plot.\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - sklearn.preprocessing.MinMaxScaler\n\n    Example:\n    >>> data = {'a': [1, 2, None, 4], 'b': [5, None, 7, 8]}\n    >>> df, plot = scale_and_plot(data)\n    \"\"\"\n    df = pd.DataFrame(kwargs)\n    df = df.dropna()\n\n    scaler = MinMaxScaler()\n    scaled_data = scaler.fit_transform(df)\n    df_scaled = pd.DataFrame(scaled_data, columns=df.columns)\n\n    plot = df_scaled.plot()\n    plot.set_title(PLOT_TITLE)\n\n    return df_scaled, plot\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, scipy.stats, matplotlib.pyplot\n```python\nimport numpy as np\nfrom scipy.stats import ttest_ind\nimport matplotlib.pyplot as plt\n\n# Constants\nALPHA = 0.05\n\ndef perform_ttest(kwargs):\n    \"\"\"\n    Perform a t-test between two groups of values from a dictionary to test if they have \n    significantly different means.\n\n    Parameters:\n    kwargs (dict): The input dictionary.\n\n    Returns:\n    bool, Plot: A boolean indicating if the means are significantly different and a boxplot.\n\n    Requirements:\n    - numpy\n    - scipy.stats.ttest_ind\n    - matplotlib.pyplot\n\n    Example:\n    >>> data = {'group1': [1, 2, None, 4], 'group2': [5, None, 7, 8]}\n    >>> result, plot = perform_ttest(data)\n    \"\"\"\n    group1 = np.array(kwargs.get('group1'))\n    group2 = np.array(kwargs.get('group2'))\n\n    group1 = group1[~np.isnan(group1)]\n    group2 = group2[~np.isnan(group2)]\n\n    t_stat, p_val = ttest_ind(group1, group2)\n    significant = p_val < ALPHA\n\n    plt.boxplot([group1, group2], labels=['group1', 'group2'])\n\n    return significant, plt\n```"}
{"seed": "{\"task_id\": 14971373, \"prompt\": \"def f_14971373():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"subprocess.check_output('ps -ef | grep something | wc -l', shell=True)\", \"test_start\": \"\\nimport subprocess\\nfrom unittest.mock import Mock\\n\\ndef check(candidate):\", \"test\": [\"\\n    output = b'  PID TTY          TIME CMD\\\\n  226 pts/1    00:00:00 bash\\\\n  285 pts/1    00:00:00 python3\\\\n  352 pts/1    00:00:00 ps\\\\n'\\n    subprocess.check_output = Mock(return_value = output)\\n    assert candidate() == output\\n\"], \"entry_point\": \"f_14971373\", \"intent\": \"capture final output of a chain of system commands `ps -ef | grep something | wc -l`\", \"library\": [\"subprocess\"]}", "generation": "Scenario 1:\nsubprocess, os, shutil, time\n```python\nimport subprocess\nimport os\nimport shutil\nimport time\n\n# Constants\nBACKUP_DIR = '/tmp/backup'\n\ndef backup_and_clean(directory):\n    \"\"\"\n    Create a backup of a directory and then clean the directory.\n    \n    Parameters:\n    directory (str): The directory path.\n    \n    Returns:\n    str: The backup directory path.\n    \n    Requirements:\n    - subprocess\n    - os\n    - shutil\n    - time\n    \n    Example:\n    >>> backup_and_clean('/tmp/my_data')\n    \n    Note: This function will return the backup directory path.\n    \"\"\"\n    # Create backup directory if it does not exist\n    if not os.path.exists(BACKUP_DIR):\n        os.makedirs(BACKUP_DIR)\n\n    # Create a timestamped backup directory\n    timestamp = time.strftime('%Y%m%d%H%M%S')\n    backup_subdir = os.path.join(BACKUP_DIR, f'backup_{timestamp}')\n    os.makedirs(backup_subdir)\n    \n    # Copy the directory to backup directory\n    shutil.copytree(directory, os.path.join(backup_subdir, os.path.basename(directory)))\n\n    # Clean the directory\n    for filename in os.listdir(directory):\n        file_path = os.path.join(directory, filename)\n        try:\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)\n        except Exception as e:\n            print(f'Failed to delete {file_path}. Reason: {e}')\n    \n    return backup_subdir\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nsubprocess, os, shutil, glob\n```python\nimport subprocess\nimport os\nimport shutil\nimport glob\n\n# Constants\nARCHIVE_DIR = '/tmp/archive'\n\ndef archive_and_delete_files(pattern):\n    \"\"\"\n    Archive all files matching a given pattern and then delete the original files.\n    \n    Parameters:\n    pattern (str): The pattern to match files.\n    \n    Returns:\n    str: The archive file path.\n    \n    Requirements:\n    - subprocess\n    - os\n    - shutil\n    - glob\n    \n    Example:\n    >>> archive_and_delete_files('*.txt')\n    \n    Note: This function will return the archive file path.\n    \"\"\"\n    # Create archive directory if it does not exist\n    if not os.path.exists(ARCHIVE_DIR):\n        os.makedirs(ARCHIVE_DIR)\n\n    # Get the list of files matching the pattern\n    file_list = glob.glob(pattern)\n\n    # Create an archive file\n    archive_file = os.path.join(ARCHIVE_DIR, 'archive.tar.gz')\n    with open(archive_file, 'w') as f:\n        for file in file_list:\n            subprocess.run(['tar', '-czf', archive_file, file])\n    \n    # Delete the original files\n    for file in file_list:\n        os.remove(file)\n    \n    return archive_file\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nsubprocess, time, json, os\n```python\nimport subprocess\nimport time\nimport json\nimport os\n\n# Constants\nLOGFILE_PATH = '/tmp/logfile.log'\n\ndef monitor_cpu_usage(interval, duration):\n    \"\"\"\n    Monitor CPU usage at regular intervals for a certain duration and save the data to a log file.\n    \n    Parameters:\n    interval (int): The interval in seconds at which to monitor CPU usage.\n    duration (int): The duration in seconds for which to monitor CPU usage.\n    \n    Returns:\n    str: The log file path.\n    \n    Requirements:\n    - subprocess\n    - time\n    - json\n    - os\n    \n    Example:\n    >>> monitor_cpu_usage(5, 60)\n    \n    Note: This function will return the log file path.\n    \"\"\"\n    start_time = time.time()\n\n    with open(LOGFILE_PATH, 'w') as logfile:\n\n        while True:\n            # Get CPU usage using the `top` command\n            output = subprocess.check_output(['top', '-b', '-n1'])\n            output_lines = output.decode('utf-8').split('\\n')\n            cpu_line = output_lines[2]  # The line containing CPU usage information\n            cpu_usage = cpu_line.split(':')[1].split(',')[0].strip()  # Extract the CPU usage\n\n            # Log the CPU usage with a timestamp\n            log_data = {\n                'timestamp': time.time(),\n                'cpu_usage': cpu_usage\n            }\n            json.dump(log_data, logfile)\n            logfile.write('\\n')\n\n            if time.time() - start_time > duration:\n                break\n\n            time.sleep(interval)\n\n    return LOGFILE_PATH\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nsubprocess, re, datetime, time\n```python\nimport subprocess\nimport re\nimport datetime\nimport time\n\n# Constants\nLOGFILE_PATH = '/tmp/logfile.log'\n\ndef monitor_http_requests(logfile):\n    \"\"\"\n    Monitor a HTTP server's access log file and count the number of 500 errors every minute.\n    \n    Parameters:\n    logfile (str): The path to the access log file.\n    \n    Returns:\n    int: The number of 500 errors.\n    \n    Requirements:\n    - subprocess\n    - re\n    - datetime\n    - time\n    \n    Example:\n    >>> monitor_http_requests('/var/log/apache2/access.log')\n    \n    Note: This function will return the number of 500 errors.\n    \"\"\"\n    start_time = datetime.datetime.now()\n\n    error_count = 0\n\n    while True:\n        with open(logfile, 'r') as f:\n            lines = f.readlines()\n        \n        for line in lines:\n            if re.search(r' 500 ', line):\n                error_count += 1\n\n        if datetime.datetime.now() - start_time > datetime.timedelta(minutes=1):\n            break\n\n        time.sleep(1)\n\n    return error_count\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nsubprocess, os, shutil, sys\n```python\nimport subprocess\nimport os\nimport shutil\nimport sys\n\n# Constants\nBACKUP_DIR = '/tmp/backup'\n\ndef rollback_update(directory):\n    \"\"\"\n    Rollback the update of a directory by restoring it from a backup.\n    \n    Parameters:\n    directory (str): The directory path.\n    \n    Returns:\n    str: The restored directory path.\n    \n    Requirements:\n    - subprocess\n    - os\n    - shutil\n    - sys\n    \n    Example:\n    >>> rollback_update('/tmp/my_data')\n    \n    Note: This function will return the restored directory path.\n    \"\"\"\n    # Check if the backup directory exists\n    if not os.path.exists(BACKUP_DIR):\n        print(f'Backup directory {BACKUP_DIR} does not exist. Cannot rollback update.')\n        sys.exit(1)\n\n    # Get the latest backup\n    backups = sorted(os.listdir(BACKUP_DIR))\n    latest_backup = backups[-1] if backups else None\n\n    if not latest_backup:\n        print(f'No backups found in {BACKUP_DIR}. Cannot rollback update.')\n        sys.exit(1)\n\n    # Remove the current directory\n    shutil.rmtree(directory)\n\n    # Restore the backup\n    shutil.copytree(os.path.join(BACKUP_DIR, latest_backup), directory)\n\n    return directory\n```"}
{"seed": "{\"task_id\": 6726636, \"prompt\": \"def f_6726636():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"\\\"\\\"\\\"\\\"\\\"\\\".join(['a', 'b', 'c'])\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate() == \\\"abc\\\"\\n\", \"\\n    assert candidate() == 'a' + 'b' + 'c'\\n\"], \"entry_point\": \"f_6726636\", \"intent\": \"concatenate a list of strings `['a', 'b', 'c']`\", \"library\": []}", "generation": "Scenario 1: \npandas, random, numpy, matplotlib\n```python\nimport pandas as pd\nfrom random import randint\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['a', 'b', 'c']\nVALUES = list('abcdefghijklmnopqrstuvwxyz')\nROWS = 100\n\ndef generate_random_dataframe():\n    \"\"\"\n    Generate a pandas DataFrame with random alphabets in each cell. Each column is named with a string from the list ['a', 'b', 'c'].\n\n    Returns:\n    DataFrame: A pandas DataFrame with random alphabets.\n\n    Requirements:\n    - pandas\n    - random\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = generate_random_dataframe()\n    >>> print(df)\n    >>> df['a'].value_counts().plot(kind='bar')\n    \"\"\"\n    data = np.random.choice(VALUES, size=(ROWS, len(COLUMNS)))\n\n    df = pd.DataFrame(data, columns=COLUMNS)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nitertools, string, pandas\n```python\nimport itertools\nimport string\nimport pandas as pd\n\n# Constants\nLETTERS = list(string.ascii_lowercase)\n\ndef get_all_combinations():\n    \"\"\"\n    Generate all possible combinations of three letters in the alphabet and store them in a pandas DataFrame.\n\n    Returns:\n    DataFrame: A pandas DataFrame with all combinations.\n\n    Requirements:\n    - itertools\n    - string\n    - pandas\n\n    Example:\n    >>> df = get_all_combinations()\n    >>> print(df)\n    \"\"\"\n    combinations = list(itertools.combinations(LETTERS, 3))\n\n    df = pd.DataFrame(combinations, columns=['a', 'b', 'c'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nrandom, string, pandas, matplotlib\n```python\nimport random\nimport string\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nLETTERS = list(string.ascii_lowercase)\nROWS = 1000\n\ndef generate_and_plot_histogram():\n    \"\"\"\n    Generate a series of random strings and plot a histogram of the frequency of each alphabet.\n\n    Requirements:\n    - random\n    - string\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> generate_and_plot_histogram()\n    \"\"\"\n    data = [''.join(random.choice(LETTERS) for _ in range(3)) for _ in range(ROWS)]\n    df = pd.DataFrame(data, columns=['String'])\n    df['String'] = df['String'].apply(lambda x: ''.join(sorted(x)))\n    df['String'].value_counts().plot(kind='bar')\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nrandom, string, pandas, seaborn\n```python\nimport random\nimport string\nimport pandas as pd\nimport seaborn as sns\n\n# Constants\nLETTERS = list(string.ascii_lowercase)\nROWS = 1000\n\ndef generate_and_plot_heatmap():\n    \"\"\"\n    Generate a series of random strings and plot a heatmap of the frequency of each alphabet.\n\n    Requirements:\n    - random\n    - string\n    - pandas\n    - seaborn\n\n    Example:\n    >>> generate_and_plot_heatmap()\n    \"\"\"\n    data = [''.join(random.choice(LETTERS) for _ in range(3)) for _ in range(ROWS)]\n    df = pd.DataFrame(data, columns=['String'])\n    df['String'] = df['String'].apply(lambda x: ''.join(sorted(x)))\n    df = df['String'].str.get_dummies(sep='')\n    corr = df.corr()\n    sns.heatmap(corr)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nitertools, string, pandas, matplotlib\n```python\nimport itertools\nimport string\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nLETTERS = list(string.ascii_lowercase)\n\ndef get_all_permutations_and_plot():\n    \"\"\"\n    Generate all possible permutations of three letters in the alphabet, store them in a pandas DataFrame, and plot a histogram of the first letters.\n\n    Returns:\n    DataFrame: A pandas DataFrame with all permutations.\n\n    Requirements:\n    - itertools\n    - string\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = get_all_permutations_and_plot()\n    >>> print(df)\n    \"\"\"\n    permutations = list(itertools.permutations(LETTERS, 3))\n\n    df = pd.DataFrame(permutations, columns=['a', 'b', 'c'])\n\n    df['a'].value_counts().plot(kind='bar')\n\n    return df\n```\n"}
{"seed": "{\"task_id\": 18079563, \"prompt\": \"def f_18079563(s1, s2):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"pd.Series(list(set(s1).intersection(set(s2))))\", \"test_start\": \"\\nimport pandas as pd\\n\\ndef check(candidate):\", \"test\": [\"\\n    x1, x2 = pd.Series([1,2]), pd.Series([1,3])\\n    assert candidate(x1, x2).equals(pd.Series([1]))\\n\", \"\\n    x1, x2 = pd.Series([1,2]), pd.Series([1,3, 10, 4, 5, 9])\\n    assert candidate(x1, x2).equals(pd.Series([1]))\\n\", \"\\n    x1, x2 = pd.Series([1,2]), pd.Series([1,2, 10])\\n    assert candidate(x1, x2).equals(pd.Series([1, 2]))\\n\"], \"entry_point\": \"f_18079563\", \"intent\": \"find intersection data between series `s1` and series `s2`\", \"library\": [\"pandas\"]}", "generation": "Scenario 3:\npandas, numpy, matplotlib, random\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import choice\n\n# Constants\nCATEGORIES = ['Electronics', 'Clothing', 'Home Decor', 'Automotive', 'Books']\n\ndef compare_sales(s1, s2):\n    \"\"\"\n    Compare the sales data of two different stores and plot the categories \n    where both stores have sales over a certain threshold.\n\n    Parameters:\n    s1 (Series): Series of sales data for store 1.\n    s2 (Series): Series of sales data for store 2.\n\n    Returns:\n    None.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> s1 = pd.Series(np.random.randint(100, 500, size=5), index=CATEGORIES)\n    >>> s2 = pd.Series(np.random.randint(150, 600, size=5), index=CATEGORIES)\n    >>> compare_sales(s1, s2)\n    \"\"\"\n    # Find the intersection of sales\n    intersection = pd.Series(list(set(s1[s1 > 200]).intersection(set(s2[s2 > 200]))))\n\n    # Create a dataframe from the intersection series\n    df = pd.DataFrame(intersection, columns=['Sales'])\n\n    # Assign random categories to the Sales\n    df['Category'] = [choice(CATEGORIES) for _ in range(len(df))]\n\n    # Plot the Sales by Category\n    df.groupby('Category')['Sales'].sum().plot(kind='bar', title='Sales by Category')\n    plt.show()\n\n    return None\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, sklearn, numpy, matplotlib\n```python\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef predict_churn(s1, s2):\n    \"\"\"\n    Predict customer churn using logistic regression based on two series of \n    customer data. Split the data into train and test datasets, train the model,\n    and evaluate it using a confusion matrix.\n\n    Parameters:\n    s1 (Series): Series of customer data for feature 1.\n    s2 (Series): Series of customer data for feature 2.\n\n    Returns:\n    ndarray: The confusion matrix.\n\n    Requirements:\n    - pandas\n    - sklearn.model_selection\n    - sklearn.linear_model\n    - sklearn.metrics\n    - numpy\n    - matplotlib\n\n    Example:\n    >>> s1 = pd.Series(np.random.rand(1000)) # feature 1\n    >>> s2 = pd.Series(np.random.randint(0, 2, size=1000)) # target (churn: yes or no)\n    >>> predict_churn(s1, s2)\n    \"\"\"\n    # Create a DataFrame from the series\n    df = pd.concat([s1, s2], axis=1)\n    \n    # Split the data into train and test datasets\n    X_train, X_test, y_train, y_test = train_test_split(df[s1.name], df[s2.name], test_size=0.2, random_state=42)\n\n    # Initialize and train the Logistic Regression model\n    model = LogisticRegression()\n    model.fit(X_train.values.reshape(-1, 1), y_train)\n\n    # Make predictions\n    y_pred = model.predict(X_test.values.reshape(-1, 1))\n\n    # Compute the confusion matrix\n    cm = confusion_matrix(y_test, y_pred)\n\n    # Plot the confusion matrix\n    plt.imshow(cm, cmap='Blues')\n    plt.colorbar()\n    plt.xticks([0, 1], ['Churn: No', 'Churn: Yes'])\n    plt.yticks([0, 1], ['Churn: No', 'Churn: Yes'])\n    plt.show()\n\n    return cm\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, seaborn, numpy, matplotlib\n```python\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef visualize_overlap(s1, s2):\n    \"\"\"\n    Visualize the overlapping data points between two series using a swarm plot.\n\n    Parameters:\n    s1 (Series): Series of data 1.\n    s2 (Series): Series of data 2.\n\n    Returns:\n    None.\n\n    Requirements:\n    - pandas\n    - seaborn\n    - numpy\n    - matplotlib\n\n    Example:\n    >>> s1 = pd.Series(np.random.randn(1000))\n    >>> s2 = pd.Series(np.random.randn(1000))\n    >>> visualize_overlap(s1, s2)\n    \"\"\"\n    # Find the intersection data points\n    intersection = pd.Series(list(set(s1).intersection(set(s2))))\n\n    # Create a DataFrame for visualization\n    df = pd.DataFrame({s1.name: s1, s2.name: s2, \"Intersection\": intersection})\n\n    # Create a swarm plot\n    plt.figure(figsize=(10, 6))\n    sns.swarmplot(data=df)\n    plt.title('Overlap Between Two Series')\n    plt.show()\n\n    return None\n```\n`GPT_ODEX_BREAK`\n\nScenario 6:\npandas, numpy, scipy.stats, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import ttest_ind\nimport matplotlib.pyplot as plt\n\ndef compare_distributions(s1, s2):\n    \"\"\"\n    Compare the distributions of two series, visualize them using a histogram, \n    and perform a t-test to check if they are significantly different.\n\n    Parameters:\n    s1 (Series): Series of data 1.\n    s2 (Series): Series of data 2.\n\n    Returns:\n    float: t-statistic.\n    float: p-value.\n\n    Requirements:\n    - pandas\n    - numpy\n    - scipy.stats\n    - matplotlib\n\n    Example:\n    >>> s1 = pd.Series(np.random.normal(0, 1, 1000))\n    >>> s2 = pd.Series(np.random.normal(1, 1, 1000))\n    >>> t_stat, p_value = compare_distributions(s1, s2)\n    \"\"\"\n    # Visualize the distributions\n    plt.hist(s1, bins=30, alpha=0.5, label=s1.name)\n    plt.hist(s2, bins=30, alpha=0.5, label=s2.name)\n    plt.legend()\n    plt.show()\n\n    # Perform a t-test\n    t_stat, p_value = ttest_ind(s1, s2)\n\n    return t_stat, p_value\n```\n`GPT_ODEX_BREAK`\n\nScenario 7:\npandas, numpy, sklearn, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\ndef cluster_data(s1, s2):\n    \"\"\"\n    Cluster the data points from two series using K-Means clustering and \n    visualize the clusters.\n\n    Parameters:\n    s1 (Series): Series of data 1.\n    s2 (Series): Series of data 2.\n\n    Returns:\n    ndarray: Cluster labels.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.cluster\n    - matplotlib\n\n    Example:\n    >>> s1 = pd.Series(np.random.rand(100)) # feature 1\n    >>> s2 = pd.Series(np.random.rand(100)) # feature 2\n    >>> labels = cluster_data(s1, s2)\n    \"\"\"\n    # Create a DataFrame from the series\n    df = pd.concat([s1, s2], axis=1)\n\n    # Perform K-Means clustering\n    kmeans = KMeans(n_clusters=3, random_state=42)\n    labels = kmeans.fit_predict(df)\n\n    # Visualize the clusters\n    plt.scatter(df[s1.name], df[s2.name], c=labels)\n    plt.title('K-Means Clustering')\n    plt.show()\n\n    return labels\n```\n"}
{"seed": "{\"task_id\": 8315209, \"prompt\": \"def f_8315209(client):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"client.send('HTTP/1.0 200 OK\\\\r\\\\n')\", \"test_start\": \"\\nimport socket\\nfrom unittest.mock import Mock\\nimport mock\\n\\ndef check(candidate):\", \"test\": [\"\\n    with mock.patch('socket.socket') as mock_socket:\\n        mock_socket.return_value.recv.return_value = ''\\n        mock_socket.bind(('', 8080))\\n        mock_socket.listen(5)\\n        mock_socket.accept = Mock(return_value = mock_socket)\\n        mock_socket.send = Mock()\\n        try:\\n            candidate(mock_socket)\\n        except:\\n            assert False\\n\"], \"entry_point\": \"f_8315209\", \"intent\": \"sending http headers to `client`\", \"library\": [\"socket\"]}", "generation": "Scenario 1:\nsocket, time, threading, datetime\n```python\nimport socket\nimport time\nimport threading\nfrom datetime import datetime\n\nSERVER_ADDRESS = 'localhost'\nSERVER_PORT = 8080\nBUFFER_SIZE = 1024\n\ndef handle_client(client_socket):\n    \"\"\"\n    Handle a client connection by sending a greeting message with the current server time.\n\n    Parameters:\n    client_socket (socket.socket): The client socket.\n\n    Requirements:\n    - socket\n    - time\n    - threading\n    - datetime\n\n    Example:\n    >>> server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    >>> server_socket.bind((SERVER_ADDRESS, SERVER_PORT))\n    >>> server_socket.listen(5)\n    >>> client_socket, addr = server_socket.accept()\n    >>> handle_client(client_socket)\n    \"\"\"\n    request = client_socket.recv(BUFFER_SIZE)\n\n    print(f\"Received: {request}\")\n\n    response = f\"Hello! Server time is {datetime.now()}\"\n\n    client_socket.send(response.encode('utf-8'))\n    client_socket.close()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nsocket, ssl, os, hashlib\n```python\nimport socket\nimport ssl\nimport os\nimport hashlib\n\nSERVER_ADDRESS = 'localhost'\nSERVER_PORT = 443\nBUFFER_SIZE = 1024\nCERT_FILE = 'server.crt'\nKEY_FILE = 'server.key'\n\ndef handle_secure_client(client_socket):\n    \"\"\"\n    Handle a client connection securely using SSL/TLS. Send a file's SHA256 hash to the client.\n\n    Parameters:\n    client_socket (socket.socket): The client socket. \n\n    Requirements:\n    - socket\n    - ssl\n    - os\n    - hashlib\n\n    Example:\n    >>> context = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER)\n    >>> context.load_cert_chain(certfile=CERT_FILE, keyfile=KEY_FILE)\n    >>> server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    >>> server_socket.bind((SERVER_ADDRESS, SERVER_PORT))\n    >>> server_socket.listen(5)\n    >>> client_socket, addr = server_socket.accept()\n    >>> handle_secure_client(client_socket)\n    \"\"\"\n    secure_socket = context.wrap_socket(client_socket, server_side=True)\n    request = secure_socket.recv(BUFFER_SIZE).decode('utf-8')\n\n    print(f\"Received: {request}\")\n\n    if os.path.exists(request):\n        with open(request, 'rb') as file:\n            sha256_hash = hashlib.sha256()\n            for byte_block in iter(lambda: file.read(4096), b\"\"):\n                sha256_hash.update(byte_block)\n            response = sha256_hash.hexdigest()\n    else:\n        response = \"File not found.\"\n\n    secure_socket.send(response.encode('utf-8'))\n    secure_socket.close()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nsocket, select, queue, datetime\n```python\nimport socket\nimport select\nimport queue\nfrom datetime import datetime\n\nSERVER_ADDRESS = 'localhost'\nSERVER_PORT = 12345\nBUFFER_SIZE = 1024\n\ndef run_echo_server():\n    \"\"\"\n    Run an echo server that sends back any received data to client with server time.\n\n    Requirements:\n    - socket\n    - select\n    - queue\n    - datetime\n\n    Example:\n    >>> run_echo_server()\n    \"\"\"\n    server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    server.setblocking(0)\n    server.bind((SERVER_ADDRESS, SERVER_PORT))\n    server.listen(5)\n    inputs = [server]\n    outputs = []\n    message_queues = {}\n    \n    while inputs:\n        readable, writable, exceptional = select.select(inputs, outputs, inputs)\n        for s in readable:\n            if s is server:\n                connection, client_address = s.accept()\n                connection.setblocking(0)\n                inputs.append(connection)\n                message_queues[connection] = queue.Queue()\n            else:\n                data = s.recv(BUFFER_SIZE)\n                if data:\n                    message_queues[s].put(data)\n                    if s not in outputs:\n                        outputs.append(s)\n                else:\n                    if s in outputs:\n                        outputs.remove(s)\n                    inputs.remove(s)\n                    s.close()\n                    del message_queues[s]\n        for s in writable:\n            try:\n                next_msg = message_queues[s].get_nowait()\n            except queue.Empty:\n                outputs.remove(s)\n            else:\n                s.sendall(f\"{datetime.now()}: {next_msg}\".encode('utf-8'))\n        for s in exceptional:\n            inputs.remove(s)\n            if s in outputs:\n                outputs.remove(s)\n            s.close()\n            del message_queues[s]\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nsocket, re, os, time\n```python\nimport socket\nimport re\nimport os\nimport time\n\nSERVER_ADDRESS = 'localhost'\nSERVER_PORT = 80\nBUFFER_SIZE = 1024\n\ndef serve_static_files(client_socket):\n    \"\"\"\n    Serve static files from a directory based on the HTTP GET request.\n\n    Parameters:\n    client_socket (socket.socket): The client socket.\n\n    Requirements:\n    - socket\n    - re\n    - os\n    - time\n\n    Example:\n    >>> server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    >>> server_socket.bind((SERVER_ADDRESS, SERVER_PORT))\n    >>> server_socket.listen(5)\n    >>> client_socket, addr = server_socket.accept()\n    >>> serve_static_files(client_socket)\n    \"\"\"\n    request = client_socket.recv(BUFFER_SIZE).decode('utf-8')\n\n    print(f\"Received: {request}\")\n\n    match = re.search(r'GET /(.*) HTTP', request)\n\n    if match:\n        file_name = match.group(1)\n        if os.path.exists(file_name):\n            with open(file_name, 'rb') as file:\n                response = f\"HTTP/1.1 200 OK\\r\\nContent-Length: {os.path.getsize(file_name)}\\r\\n\\r\\n\"\n                client_socket.send(response.encode('utf-8'))\n                client_socket.send(file.read())\n        else:\n            response = \"HTTP/1.1 404 NOT FOUND\\r\\n\\r\\nFile Not Found\"\n            client_socket.send(response.encode('utf-8'))\n    else:\n        response = \"HTTP/1.1 400 BAD REQUEST\\r\\n\\r\\nBad Request\"\n        client_socket.send(response.encode('utf-8'))\n\n    client_socket.close()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nsocket, smtplib, email.message, getpass\n```python\nimport socket\nimport smtplib\nfrom email.message import EmailMessage\nimport getpass\n\nSERVER_ADDRESS = 'localhost'\nSERVER_PORT = 25\nBUFFER_SIZE = 1024\nSMTP_SERVER = 'smtp.gmail.com'\nSMTP_PORT = 587\n\ndef handle_email_client(client_socket):\n    \"\"\"\n    Handle a client connection by sending an email with the received message.\n\n    Parameters:\n    client_socket (socket.socket): The client socket.\n\n    Requirements:\n    - socket\n    - smtplib\n    - email.message\n    - getpass\n\n    Example:\n    >>> server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    >>> server_socket.bind((SERVER_ADDRESS, SERVER_PORT))\n    >>> server_socket.listen(5)\n    >>> client_socket, addr = server_socket.accept()\n    >>> handle_email_client(client_socket)\n    \"\"\"\n    request = client_socket.recv(BUFFER_SIZE).decode('utf-8')\n\n    print(f\"Received: {request}\")\n\n    email = EmailMessage()\n    email['From'] = getpass.getpass('Email: ')\n    email['To'] = getpass.getpass('Recipient: ')\n    email['Subject'] = 'Message from socket client'\n    email.set_content(request)\n\n    with smtplib.SMTP(SMTP_SERVER, SMTP_PORT) as smtp:\n        smtp.starttls()\n        smtp.login(email['From'], getpass.getpass('Password: '))\n        smtp.send_message(email)\n\n    response = \"Message sent.\"\n    client_socket.send(response.encode('utf-8'))\n    client_socket.close()\n```\n"}
{"seed": "{\"task_id\": 26153795, \"prompt\": \"def f_26153795(when):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"datetime.datetime.strptime(when, '%Y-%m-%d').date()\", \"test_start\": \"\\nimport datetime\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('2013-05-07') == datetime.date(2013, 5, 7)\\n\", \"\\n    assert candidate('2000-02-29') == datetime.date(2000, 2, 29)\\n\", \"\\n    assert candidate('1990-01-08') == datetime.date(1990, 1, 8)\\n\", \"\\n    assert candidate('1990-1-08') == datetime.date(1990, 1, 8)\\n\", \"\\n    assert candidate('1990-1-8') == datetime.date(1990, 1, 8)\\n\", \"\\n    assert candidate('1990-01-8') == datetime.date(1990, 1, 8)\\n\"], \"entry_point\": \"f_26153795\", \"intent\": \"Format a datetime string `when` to extract date only\", \"library\": [\"datetime\"]}", "generation": "Scenario 1: \npandas, datetime, random, matplotlib\n```python\nimport pandas as pd\nfrom datetime import datetime\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nROOMS = ['Room1', 'Room2', 'Room3', 'Room4', 'Room5']\nBOOKING_STATUS = ['Booked', 'Available']\n\ndef generate_room_booking_status(date_str):\n    \"\"\"\n    Generate a status report of room bookings for a list of rooms on a given date.\n    \n    Parameters:\n    date_str (str): The date string in \"yyyy-mm-dd\" format.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with booking status for the rooms.\n    \n    Requirements:\n    - pandas\n    - datetime\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> report = generate_room_booking_status('2023-6-15')\n    >>> print(report)\n    >>> report['Booking Status'].value_counts().plot(kind='bar')\n    \"\"\"\n    report_data = []\n\n    for room in ROOMS:\n        status = BOOKING_STATUS[randint(0, len(BOOKING_STATUS)-1)]\n        report_data.append([room, status])\n\n    report_df = pd.DataFrame(report_data, columns=['Room', 'Booking Status'])\n\n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ndatetime, numpy, dateutil\n```python\nfrom datetime import datetime\nimport numpy as np\nfrom dateutil.parser import parse\n\n# Constants\nLEAP_SECONDS = np.array([1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980,\n                         1981, 1982, 1983, 1985, 1988, 1990, 1993, 1994, 1997,\n                         1999, 2006, 2009, 2012, 2015, 2016, 2020])\n\ndef total_seconds_since_date(date_str):\n    \"\"\"\n    Calculate the total seconds that have passed since a given datetime till now \n    considering the leap seconds.\n\n    Parameters:\n    date_str (str): The date string in \"yyyy-mm-dd hh:mm:ss\" format.\n\n    Returns:\n    int: The total seconds.\n\n    Requirements:\n    - datetime\n    - numpy\n    - dateutil.parser\n\n    Example:\n    >>> total_seconds_since_date('1970-01-01 00:00:00')\n    \"\"\"\n    given_date = parse(date_str)\n    current_date = datetime.now()\n\n    total_seconds = (current_date - given_date).total_seconds()\n\n    leap_years = LEAP_SECONDS[np.logical_and(LEAP_SECONDS >= given_date.year, LEAP_SECONDS <= current_date.year)]\n    leap_seconds = len(leap_years)\n\n    total_seconds += leap_seconds\n\n    return int(total_seconds)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ndatetime, numpy, pandas\n```python\nfrom datetime import datetime\nimport numpy as np\nimport pandas as pd\n\n# Constants\nEMPLOYEES = ['John', 'Alice', 'Bob', 'Charlie', 'Dave']\n\ndef generate_employee_dates(date_str):\n    \"\"\"\n    Generate a pandas DataFrame that contains the dates for the next 10 days \n    for a list of employees starting from a given date.\n\n    Parameters:\n    date_str (str): The date string in \"yyyy-mm-dd\" format.\n\n    Returns:\n    DataFrame: A pandas DataFrame with dates for the next 10 days for each employee.\n\n    Requirements:\n    - datetime\n    - numpy\n    - pandas\n\n    Example:\n    >>> df = generate_employee_dates('2023-06-15')\n    >>> print(df)\n    \"\"\"\n    start_date = datetime.strptime(date_str, '%Y-%m-%d')\n    dates = pd.date_range(start_date, periods=10).tolist()\n\n    data = {employee: dates for employee in EMPLOYEES}\n    df = pd.DataFrame(data)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ndatetime, random, matplotlib.pyplot\n```python\nfrom datetime import datetime\nimport random\nimport matplotlib.pyplot as plt\n\ndef plot_random_values(date_str):\n    \"\"\"\n    Generate a list of random numbers and plot them. The number of random numbers \n    is determined by the day of the month from a given date.\n\n    Parameters:\n    date_str (str): The date string in \"yyyy-mm-dd\" format.\n\n    Requirements:\n    - datetime\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_random_values('2023-06-15')\n    \"\"\"\n    date = datetime.strptime(date_str, '%Y-%m-%d')\n    random_values = [random.randint(1, 100) for _ in range(date.day)]\n    plt.plot(random_values)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\ndatetime, numpy, matplotlib.pyplot\n```python\nfrom datetime import datetime\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_sin_wave(date_str):\n    \"\"\"\n    Generate and plot a sin wave. The frequency of the sin wave is determined by \n    the day of the month from a given date.\n\n    Parameters:\n    date_str (str): The date string in \"yyyy-mm-dd\" format.\n\n    Requirements:\n    - datetime\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_sin_wave('2023-06-15')\n    \"\"\"\n    date = datetime.strptime(date_str, '%Y-%m-%d')\n    x = np.linspace(0, 2 * np.pi, 1000)\n    y = np.sin(x * date.day)\n    plt.plot(x, y)\n    plt.show()\n```\n"}
{"seed": "{\"task_id\": 172439, \"prompt\": \"def f_172439(inputString):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"inputString.split('\\\\n')\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('line a\\\\nfollows by line b\\t...bye\\\\n') ==         ['line a', 'follows by line b\\t...bye', '']\\n\", \"\\n    assert candidate('no new line in this sentence. ') == ['no new line in this sentence. ']\\n\", \"\\n    assert candidate('a\\tbfs hhhdf\\tsfdas') == ['a\\tbfs hhhdf\\tsfdas']\\n\", \"\\n    assert candidate('') == ['']\\n\"], \"entry_point\": \"f_172439\", \"intent\": \"split a multi-line string `inputString` into separate strings\", \"library\": []}", "generation": "Scenario 1: \nre, os, string, random\n```python\nimport re\nimport os\nimport string\nimport random\n\n# Constants\nDIRECTORY = './text_files'\nSPECIAL_CHARS = string.punctuation\n\ndef split_and_save(input_string):\n    \"\"\"\n    Split a multi-line string into separate strings, \n    remove special characters, and save each string as a separate text file.\n\n    Parameters:\n    input_string (str): The multi-line string.\n\n    Requirements:\n    - re\n    - os\n    - string\n    - random\n\n    Example:\n    >>> split_and_save('line a\\nfollows by line b\\n...bye\\n')\n    \"\"\"\n    lines = input_string.split('\\n')\n    for line in lines:\n        line = re.sub('['+SPECIAL_CHARS+']', '', line)\n        filename = str(random.randint(10000, 99999)) + '.txt'\n        filepath = os.path.join(DIRECTORY, filename)\n        with open(filepath, 'w') as file:\n            file.write(line)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nre, json, pandas\n```python\nimport re\nimport json\nimport pandas as pd\n\ndef split_and_to_dataframe(input_string):\n    \"\"\"\n    Split a multi-line string into separate strings and \n    transform the strings into a pandas DataFrame.\n\n    Parameters:\n    input_string (str): The multi-line string.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the separate strings.\n\n    Requirements:\n    - re\n    - json\n    - pandas \n\n    Example:\n    >>> split_and_to_dataframe('line a\\nfollows by line b\\n...bye\\n')\n    \"\"\"\n    lines = input_string.split('\\n')\n    lines = [re.sub('\\t', ' ', line) for line in lines]\n    df = pd.DataFrame(lines, columns=['Text'])\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nre, nltk, collections\n```python\nimport re\nfrom nltk.corpus import stopwords\nfrom collections import Counter\n\nSTOPWORDS = set(stopwords.words('english'))\n\ndef split_and_word_frequency(input_string):\n    \"\"\"\n    Split a multi-line string into separate strings, \n    remove stopwords and count the frequency of each word.\n\n    Parameters:\n    input_string (str): The multi-line string.\n\n    Returns:\n    dict: A dictionary with word frequencies.\n\n    Requirements:\n    - re\n    - nltk.corpus\n    - collections\n\n    Example:\n    >>> split_and_word_frequency('line a\\nfollows by line b\\n...bye\\n')\n    \"\"\"\n    lines = input_string.split('\\n')\n    word_count = Counter()\n    for line in lines:\n        words = re.findall(r'\\b\\w+\\b', line)\n        words = [word for word in words if word not in STOPWORDS]\n        word_count.update(words)\n    return dict(word_count)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nre, os, hashlib\n```python\nimport re\nimport os\nimport hashlib\n\n# Constants\nDIRECTORY = './hashed_files'\n\ndef split_and_hash(input_string):\n    \"\"\"\n    Split a multi-line string into separate strings, \n    hash each string and save the hash as a separate file.\n\n    Parameters:\n    input_string (str): The multi-line string.\n\n    Requirements:\n    - re\n    - os\n    - hashlib\n\n    Example:\n    >>> split_and_hash('line a\\nfollows by line b\\n...bye\\n')\n    \"\"\"\n    lines = input_string.split('\\n')\n    for line in lines:\n        line_hash = hashlib.sha256(line.encode()).hexdigest()\n        filename = line_hash[:10] + '.txt'\n        filepath = os.path.join(DIRECTORY, filename)\n        with open(filepath, 'w') as file:\n            file.write(line_hash)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nre, textwrap\n```python\nimport re\nimport textwrap\n\ndef split_and_wrap(input_string, width):\n    \"\"\"\n    Split a multi-line string into separate strings, \n    and wrap each line to a specified width.\n\n    Parameters:\n    input_string (str): The multi-line string.\n    width (int): The width to wrap each line to.\n\n    Returns:\n    str: The wrapped string.\n\n    Requirements:\n    - re\n    - textwrap\n\n    Example:\n    >>> split_and_wrap('line a\\nfollows by line b\\n...bye\\n', 10)\n    \"\"\"\n    lines = input_string.split('\\n')\n    wrapped_lines = [textwrap.fill(line, width) for line in lines]\n    return '\\n'.join(wrapped_lines)\n```\n"}
{"seed": "{\"task_id\": 172439, \"prompt\": \"def f_172439():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"' a \\\\n b \\\\r\\\\n c '.split('\\\\n')\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate() == [' a ', ' b \\\\r', ' c ']\\n\"], \"entry_point\": \"f_172439\", \"intent\": \"Split a multi-line string ` a \\\\n b \\\\r\\\\n c ` by new line character `\\\\n`\", \"library\": []}", "generation": "Scenario 1:\ncsv, pandas, numpy, sklearn, matplotlib\n```python\nimport csv\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\n\n# Constants\nSTOP_WORDS = ['a', 'an', 'the', 'in', 'on', 'at', 'and', 'or']\n\ndef analyze_text_data(file_path):\n    \"\"\"\n    Read a CSV file where each row is a multi-line string. Split each string by new line character,\n    convert to a pandas DataFrame, vectorize the text (excluding stop words), \n    and plot a histogram of the top 10 most common words.\n\n    Parameters:\n    file_path (str): The path to the CSV file.\n    \n    Returns:\n    None\n\n    Requirements:\n    - csv\n    - pandas\n    - numpy\n    - sklearn.feature_extraction.text.CountVectorizer\n    - matplotlib.pyplot\n\n    Example:\n    >>> analyze_text_data('text_data.csv')\n    \"\"\"\n    with open(file_path, 'r') as file:\n        reader = csv.reader(file)\n        data = [row[0].split('\\\\n') for row in reader]\n\n    df = pd.DataFrame(data, columns=['Text'])\n\n    vectorizer = CountVectorizer(stop_words=STOP_WORDS)\n    word_count = vectorizer.fit_transform(df['Text'])\n\n    sum_words = word_count.sum(axis=0)\n    words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n\n    top_words = words_freq[:10]\n    df = pd.DataFrame(top_words, columns=['Word', 'Count'])\n\n    df.plot.bar(x='Word', y='Count', rot=0)\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 2:\nre, string, pandas, matplotlib, nltk\n```python\nimport re\nimport string\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\n\n# Constants\nSTOPWORDS = set(stopwords.words('english'))\n\ndef count_words_in_paragraph(paragraph):\n    \"\"\"\n    Split a paragraph into sentences by new line character, remove punctuation and \n    stop words, and count the frequency of each word. Return the result as a pandas DataFrame\n    and plot a histogram of the word counts.\n\n    Parameters:\n    paragraph (str): The paragraph to analyze.\n\n    Returns:\n    DataFrame: A pandas DataFrame with word counts.\n\n    Requirements:\n    - re\n    - string\n    - pandas\n    - matplotlib.pyplot\n    - nltk.corpus.stopwords\n\n    Example:\n    >>> paragraph = 'This is a sample paragraph.\\\\nIt contains several sentences.\\\\nSome sentences have stop words.'\n    >>> df = count_words_in_paragraph(paragraph)\n    >>> print(df)\n    >>> df.plot.bar(x='Word', y='Count', rot=0)\n    \"\"\"\n    sentences = paragraph.split('\\\\n')\n\n    word_counts = {}\n    for sentence in sentences:\n        sentence = re.sub('['+string.punctuation+']', '', sentence)\n        words = sentence.split()\n        for word in words:\n            if word not in STOPWORDS:\n                if word in word_counts:\n                    word_counts[word] += 1\n                else:\n                    word_counts[word] = 1\n\n    df = pd.DataFrame(list(word_counts.items()), columns=['Word', 'Count'])\n    df = df.sort_values('Count', ascending=False)\n\n    return df\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 3:\ncsv, random, numpy, scipy.stats, matplotlib\n```python\nimport csv\nimport random\nimport numpy as np\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\n\n# Constants\nPOPULATION = list(range(1, 101))\n\ndef sample_and_analyze_population(file_path):\n    \"\"\"\n    Read a CSV file where each row is a number representing an individual in a population.\n    Randomly sample 30 individuals without replacement, calculate the sample mean and \n    standard deviation, and plot a histogram of the sample with a normal distribution overlay.\n\n    Parameters:\n    file_path (str): The path to the CSV file.\n\n    Returns:\n    Tuple: The sample mean and standard deviation.\n\n    Requirements:\n    - csv\n    - random\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Example:\n    >>> sample_and_analyze_population('population_data.csv')\n    \"\"\"\n    with open(file_path, 'r') as file:\n        reader = csv.reader(file)\n        population = [int(row[0]) for row in reader]\n\n    sample = random.sample(population, 30)\n    mean = np.mean(sample)\n    std_dev = np.std(sample)\n\n    plt.hist(sample, bins='auto', density=True, alpha=0.7, rwidth=0.85)\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, mean, std_dev)\n    plt.plot(x, p, 'k', linewidth=2)\n    plt.show()\n\n    return mean, std_dev\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, shutil, re, string\n```python\nimport os\nimport shutil\nimport re\nimport string\n\n# Constants\nINVALID_CHARACTERS = string.punctuation + string.whitespace\n\ndef organize_files_by_extension(directory_path):\n    \"\"\"\n    Scan a directory and organize the files by their extensions. Files with invalid characters \n    in their names are moved to a new directory called \"Invalid\". \n\n    Parameters:\n    directory_path (str): The path to the directory.\n\n    Returns:\n    None\n\n    Requirements:\n    - os\n    - shutil\n    - re\n    - string\n\n    Example:\n    >>> organize_files_by_extension('path_to_directory')\n    \"\"\"\n    for filename in os.listdir(directory_path):\n        if any(char in INVALID_CHARACTERS for char in filename):\n            if not os.path.exists(os.path.join(directory_path, 'Invalid')):\n                os.mkdir(os.path.join(directory_path, 'Invalid'))\n            shutil.move(os.path.join(directory_path, filename), os.path.join(directory_path, 'Invalid'))\n\n        else:\n            extension = os.path.splitext(filename)[-1].strip('.')\n            if not os.path.exists(os.path.join(directory_path, extension)):\n                os.mkdir(os.path.join(directory_path, extension))\n            shutil.move(os.path.join(directory_path, filename), os.path.join(directory_path, extension))\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 5:\nre, random, string, collections\n```python\nimport re\nimport random\nimport string\nimport collections\n\n# Constants\nVALID_CHARACTERS = string.ascii_letters + string.digits\n\ndef generate_random_strings_and_count_characters(n_strings, string_length):\n    \"\"\"\n    Generate n random strings of a given length, split each string by new line character, \n    count the frequency of each character, and return the result as a dictionary.\n\n    Parameters:\n    n_strings (int): The number of strings to generate.\n    string_length (int): The length of each string.\n\n    Returns:\n    Dict: A dictionary with character counts.\n\n    Requirements:\n    - re\n    - random\n    - string\n    - collections\n\n    Example:\n    >>> generate_random_strings_and_count_characters(1000, 10)\n    \"\"\"\n    strings = [''.join(random.choice(VALID_CHARACTERS) for _ in range(string_length)) for _ in range(n_strings)]\n    strings = '\\\\n'.join(strings)\n    \n    strings = strings.split('\\\\n')\n    character_counts = collections.Counter(''.join(strings))\n\n    return dict(character_counts)\n```"}
{"seed": "{\"task_id\": 13954222, \"prompt\": \"def f_13954222(b):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"\\\"\\\"\\\":\\\"\\\"\\\".join(str(x) for x in b)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(['x','y','zzz']) == 'x:y:zzz'\\n\", \"\\n    assert candidate(['111','22','3']) == '111:22:3'\\n\", \"\\n    assert candidate(['']) == ''\\n\", \"\\n    assert candidate([':',':']) == ':::'\\n\", \"\\n    assert candidate([',','#','#$%']) == ',:#:#$%'\\n\", \"\\n    assert candidate(['a','b','c']) != 'abc'\\n\"], \"entry_point\": \"f_13954222\", \"intent\": \"concatenate elements of list `b` by a colon \\\":\\\"\", \"library\": []}", "generation": "Scenario 1:\npandas, numpy, itertools, random\n```python\nimport pandas as pd\nimport numpy as np\nimport itertools\nimport random\n\n# Constants\nCOLORS = ['Red', 'Blue', 'Green', 'Yellow', 'Pink', 'Orange', 'Purple', 'White', 'Black', 'Brown']\nSTATES = ['Solid', 'Liquid', 'Gas']\n\ndef generate_color_state_table():\n    \"\"\"\n    Generate a table of color-state pairs. Each row is a color concatenated with a state by a colon.\n\n    Returns:\n    DataFrame: A pandas DataFrame with color-state pairs.\n\n    Requirements:\n    - pandas\n    - numpy\n    - itertools\n    - random\n\n    Example:\n    >>> color_state_table = generate_color_state_table()\n    >>> print(color_state_table)\n    \"\"\"\n    combinations = list(itertools.product(COLORS, STATES))\n    random.shuffle(combinations)\n\n    data = {f'Color:State {i+1}': [f\"{comb[0]}:{comb[1]}\" for comb in combinations[i::3]] for i in range(3)}\n    df = pd.DataFrame(data)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, itertools, random, matplotlib.pyplot\n```python\nimport numpy as np\nimport itertools\nimport random\nimport matplotlib.pyplot as plt\n\n# Constants\nLETTERS = list('abcdefghijklmnopqrstuvwxyz')\nNUMBERS = list(range(1, 27))\n\ndef plot_random_letter_number_pairs():\n    \"\"\"\n    Plot a bar chart of random letter-number pairs. Each pair is a letter concatenated with a number by a colon.\n\n    Requirements:\n    - numpy\n    - itertools\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_random_letter_number_pairs()\n    \"\"\"\n    pairs = [f\"{l}:{n}\" for l, n in zip(LETTERS, NUMBERS)]\n    random.shuffle(pairs)\n\n    counts = np.random.randint(1, 10, size=len(pairs))\n    plt.bar(pairs, counts)\n    plt.xlabel('Letter:Number Pairs')\n    plt.ylabel('Counts')\n    plt.title('Random Letter:Number Pairs')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, pandas, itertools, random\n```python\nimport numpy as np\nimport pandas as pd\nimport itertools\nimport random\n\n# Constants\nANIMALS = ['Dog', 'Cat', 'Elephant', 'Tiger', 'Lion', 'Zebra', 'Giraffe', 'Bear', 'Monkey', 'Kangaroo']\nFOODS = ['Meat', 'Fish', 'Grass', 'Fruits', 'Insects', 'Seeds', 'Leaves']\n\ndef create_animal_food_pairs():\n    \"\"\"\n    Create a table of animal-food pairs. Each pair is an animal concatenated with a food by a colon.\n\n    Returns:\n    pandas.DataFrame: A pandas DataFrame with animal-food pairs.\n\n    Requirements:\n    - numpy\n    - pandas\n    - itertools\n    - random\n\n    Example:\n    >>> animal_food_pairs = create_animal_food_pairs()\n    >>> print(animal_food_pairs)\n    \"\"\"\n    pairs = [f\"{a}:{f}\" for a, f in itertools.product(ANIMALS, FOODS)]\n    random.shuffle(pairs)\n\n    data = np.array(pairs).reshape(-1, len(FOODS))\n    df = pd.DataFrame(data, columns=FOODS)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, itertools, random, seaborn\n```python\nimport numpy as np\nimport itertools\nimport random\nimport seaborn as sns\n\n# Constants\nSHAPES = ['Circle', 'Square', 'Triangle', 'Rectangle', 'Pentagon', 'Hexagon', 'Heptagon', 'Octagon', 'Nonagon', 'Decagon']\nCOLORS = ['Red', 'Blue', 'Green', 'Yellow', 'Black', 'White', 'Purple', 'Orange', 'Pink', 'Brown']\n\ndef plot_shapes_colors_distribution():\n    \"\"\"\n    Plot a countplot of random shape-color pairs. Each pair is a shape concatenated with a color by a colon.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - itertools\n    - random\n    - seaborn\n\n    Example:\n    >>> plot_shapes_colors_distribution()\n    \"\"\"\n    pairs = [f\"{s}:{c}\" for s, c in itertools.product(SHAPES, COLORS)]\n    random.shuffle(pairs)\n\n    sns.countplot(x=pairs, palette='Set3')\n    plt.xticks(rotation=90)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, random, itertools, pandas\n```python\nimport numpy as np\nimport random\nimport itertools\nimport pandas as pd\n\n# Constants\nPLANETS = ['Mercury', 'Venus', 'Earth', 'Mars', 'Jupiter', 'Saturn', 'Uranus', 'Neptune']\nELEMENTS = ['Hydrogen', 'Helium', 'Oxygen', 'Carbon', 'Nitrogen', 'Magnesium', 'Silicon', 'Iron', 'Nickel']\n\ndef generate_planet_elements_table():\n    \"\"\"\n    Generate a table of planet-element pairs. Each pair is a planet concatenated with an element by a colon.\n\n    Returns:\n    pandas.DataFrame: A pandas DataFrame with planet-element pairs.\n\n    Requirements:\n    - numpy\n    - random\n    - itertools\n    - pandas\n\n    Example:\n    >>> planet_elements_table = generate_planet_elements_table()\n    >>> print(planet_elements_table)\n    \"\"\"\n    pairs = [f\"{p}:{e}\" for p, e in itertools.product(PLANETS, ELEMENTS)]\n    random.shuffle(pairs)\n\n    data = np.array(pairs).reshape(-1, len(ELEMENTS))\n    df = pd.DataFrame(data, columns=ELEMENTS)\n\n    return df\n```\n"}
{"seed": "{\"task_id\": 13567345, \"prompt\": \"def f_13567345(a):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"a.sum(axis=1)\", \"test_start\": \"\\nimport numpy as np \\n\\ndef check(candidate):\", \"test\": [\"\\n    a1 = np.array([[i for i in range(3)] for j in range(5)])\\n    assert np.array_equal(candidate(a1), np.array([3, 3, 3, 3, 3]))\\n\", \"\\n    a2 = np.array([[i+j for i in range(3)] for j in range(5)])\\n    assert np.array_equal(candidate(a2), np.array([ 3,  6,  9, 12, 15]))\\n\", \"\\n    a3 = np.array([[i*j for i in range(3)] for j in range(5)])\\n    assert np.array_equal(candidate(a3), np.array([ 0,  3,  6,  9, 12]))\\n\"], \"entry_point\": \"f_13567345\", \"intent\": \"Calculate sum over all rows of 2D numpy array `a`\", \"library\": [\"numpy\"]}", "generation": "Scenario 1: \nnumpy, scipy, matplotlib\n```python\nimport numpy as np\nfrom scipy.stats import norm\nfrom matplotlib import pyplot as plt\n\ndef plot_normal_distribution(arr):\n    \"\"\"\n    Given a 2D numpy array, calculate the sum of all rows, normalize the result to a standard normal distribution \n    and plot the histogram of the normalized data along with the probability density function (PDF) of a standard normal distribution.\n\n    Parameters:\n    arr (numpy.array): The 2D numpy array.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Example:\n    >>> arr = np.array([[i+j for i in range(3)] for j in range(5)])\n    >>> plot_normal_distribution(arr)\n    \"\"\"\n    row_sums = arr.sum(axis=1)\n    normalized_data = (row_sums - np.mean(row_sums)) / np.std(row_sums)\n\n    plt.hist(normalized_data, bins=30, density=True)\n\n    x = np.linspace(norm.ppf(0.01), norm.ppf(0.99), 100)\n    plt.plot(x, norm.pdf(x), 'r-', lw=2)\n\n    plt.title(\"Histogram of normalized data and PDF of a standard normal distribution\")\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nnumpy, pandas, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n\ndef plot_timeseries(arr):\n    \"\"\"\n    Given a 2D numpy array, calculate the sum of all rows and create a pandas DataFrame with the sums as a time series data.\n    Then plot the time series data.\n\n    Parameters:\n    arr (numpy.array): The 2D numpy array.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> arr = np.array([[i+j for i in range(3)] for j in range(5)])\n    >>> plot_timeseries(arr)\n    \"\"\"\n    row_sums = arr.sum(axis=1)\n    df = pd.DataFrame(row_sums, columns=['Value'])\n    df.index = pd.date_range(start='1/1/2020', periods=len(row_sums))\n\n    df.plot()\n    plt.title(\"Time Series of Row Sums\")\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnumpy, sklearn, matplotlib\n```python\nimport numpy as np\nfrom sklearn.decomposition import PCA\nfrom matplotlib import pyplot as plt\n\ndef perform_pca(arr):\n    \"\"\"\n    Given a 2D numpy array, calculate the sum of all rows and perform Principal Component Analysis (PCA) on the sums.\n    Then plot the explained variance ratio of each principal component.\n\n    Parameters:\n    arr (numpy.array): The 2D numpy array.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - sklearn.decomposition\n    - matplotlib.pyplot\n\n    Example:\n    >>> arr = np.array([[i+j for i in range(3)] for j in range(5)])\n    >>> perform_pca(arr)\n    \"\"\"\n    row_sums = arr.sum(axis=1)\n\n    pca = PCA(n_components=2)\n    pca.fit(row_sums.reshape(-1, 1))\n\n    plt.bar(range(pca.n_components_), pca.explained_variance_ratio_)\n    plt.title(\"Explained Variance Ratio of Principal Components\")\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, pandas, seaborn\n```python\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\ndef create_heatmap(arr):\n    \"\"\"\n    Given a 2D numpy array, calculate the sum of all rows and create a correlation matrix. \n    Then plot a heatmap of the correlation matrix.\n\n    Parameters:\n    arr (numpy.array): The 2D numpy array.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - pandas\n    - seaborn\n\n    Example:\n    >>> arr = np.array([[i+j for i in range(3)] for j in range(5)])\n    >>> create_heatmap(arr)\n    \"\"\"\n    row_sums = arr.sum(axis=1)\n    df = pd.DataFrame(row_sums, columns=['Value'])\n    correlation_matrix = df.corr()\n\n    sns.heatmap(correlation_matrix, annot=True)\n    plt.title(\"Correlation Matrix Heatmap\")\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnumpy, scipy, matplotlib\n```python\nimport numpy as np\nfrom scipy import fftpack\nfrom matplotlib import pyplot as plt\n\ndef plot_fft(arr):\n    \"\"\"\n    Given a 2D numpy array, calculate the sum of all rows and perform a Fast Fourier Transform (FFT) on the sums.\n    Then plot the absolute values of the FFT coefficients.\n\n    Parameters:\n    arr (numpy.array): The 2D numpy array.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - scipy.fftpack\n    - matplotlib.pyplot\n\n    Example:\n    >>> arr = np.array([[i+j for i in range(3)] for j in range(5)])\n    >>> plot_fft(arr)\n    \"\"\"\n    row_sums = arr.sum(axis=1)\n    fft_coefficients = fftpack.fft(row_sums)\n\n    plt.plot(np.abs(fft_coefficients))\n    plt.title(\"Absolute values of FFT coefficients\")\n    plt.show()\n```"}
{"seed": "{\"task_id\": 29784889, \"prompt\": \"def f_29784889():\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"warnings.simplefilter('always')\", \"test_start\": \"\\nimport warnings \\n\\ndef check(candidate):\", \"test\": [\"\\n    candidate() \\n    assert any([(wf[0] == 'always') for wf in warnings.filters])\\n\"], \"entry_point\": \"f_29784889\", \"intent\": \"enable warnings using action 'always'\", \"library\": [\"warnings\"]}", "generation": "Scenario 1: \nwarnings, os, glob, shutil, time\n```python\nimport warnings\nimport os\nimport glob\nimport shutil\nimport time\n\n# Constants\nSOURCE_DIR = '/path/to/source_dir'\nDEST_DIR = '/path/to/dest_dir'\nEXTENSIONS = ['.txt', '.csv', '.xlsx']\n\ndef transfer_files_with_warnings():\n    \"\"\"\n    Transfer files from one directory to another and raise warnings for any files that \n    do not match the given file extensions. The warning action is set to 'always'.\n\n    Returns:\n    list: The list of files successfully transferred.\n\n    Requirements:\n    - warnings\n    - os\n    - glob\n    - shutil\n    - time\n\n    Example:\n    >>> transfer_files_with_warnings()\n    \"\"\"\n    warnings.simplefilter('always')\n\n    transferred_files = []\n\n    for ext in EXTENSIONS:\n        for src_file in glob.glob(os.path.join(SOURCE_DIR, '*' + ext)):\n            try:\n                shutil.move(src_file, DEST_DIR)\n                transferred_files.append(os.path.basename(src_file))\n            except Exception as e:\n                warnings.warn(f\"Unable to move file {src_file}: {str(e)}\")\n\n    time.sleep(1)  # To ensure all warnings are processed\n\n    return transferred_files\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nwarnings, pandas, numpy, matplotlib.pyplot\n```python\nimport warnings\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nNUM_SAMPLES = 100\nNUM_OUTLIERS = 5\n\ndef generate_data_and_plot_with_warnings():\n    \"\"\"\n    Generate a dataset with a number of outliers and plot a histogram of the data. Warnings are raised for any outliers \n    in the data. The warning action is set to 'always'.\n\n    Returns:\n    None\n\n    Requirements:\n    - warnings\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> generate_data_and_plot_with_warnings()\n    \"\"\"\n    warnings.simplefilter('always')\n\n    # Generate data with outliers\n    data = np.random.normal(size=NUM_SAMPLES)\n    outliers = np.random.uniform(low=-10, high=10, size=NUM_OUTLIERS)\n    data = np.concatenate([data, outliers])\n\n    # Identify and warn about outliers\n    q75, q25 = np.percentile(data, [75 ,25])\n    iqr = q75 - q25\n    lower_bound = q25 - (iqr*1.5)\n    upper_bound = q75 + (iqr*1.5)\n\n    for outlier in data[(data < lower_bound) | (data > upper_bound)]:\n        warnings.warn(f\"Outlier detected: {outlier}\")\n\n    # Plot histogram\n    plt.hist(data, bins=30)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nwarnings, json, requests, sys\n```python\nimport warnings\nimport json\nimport requests\nimport sys\n\n# Constants\nURL = 'https://api.github.com/repos/psf/requests'\n\ndef get_github_repo_info_with_warnings():\n    \"\"\"\n    Fetch information about a GitHub repository and raise a warning if the repository has more than 10000 open issues. \n    The warning action is set to 'always'.\n\n    Returns:\n    dict: The information about the GitHub repository.\n\n    Requirements:\n    - warnings\n    - json\n    - requests\n    - sys\n\n    Example:\n    >>> get_github_repo_info_with_warnings()\n    \"\"\"\n    warnings.simplefilter('always')\n\n    try:\n        response = requests.get(URL)\n        repo_info = json.loads(response.text)\n\n        if repo_info['open_issues_count'] > 10000:\n            warnings.warn(\"The repository has more than 10000 open issues.\")\n\n        return repo_info\n\n    except Exception as e:\n        sys.exit(f\"Error fetching repo info: {str(e)}\")\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nwarnings, sqlite3, pandas, sys\n```python\nimport warnings\nimport sqlite3\nimport pandas as pd\nimport sys\n\n# Constants\nDB_PATH = '/path/to/sqlite.db'\nQUERY = 'SELECT * FROM table_name'\n\ndef fetch_db_data_with_warnings():\n    \"\"\"\n    Fetch data from a SQLite database and raise a warning if the data contains more than 10000 rows. \n    The warning action is set to 'always'.\n\n    Returns:\n    DataFrame: The fetched data as a pandas DataFrame.\n\n    Requirements:\n    - warnings\n    - sqlite3\n    - pandas\n    - sys\n\n    Example:\n    >>> fetch_db_data_with_warnings()\n    \"\"\"\n    warnings.simplefilter('always')\n\n    try:\n        conn = sqlite3.connect(DB_PATH)\n        data = pd.read_sql_query(QUERY, conn)\n\n        if data.shape[0] > 10000:\n            warnings.warn(\"The data contains more than 10000 rows.\")\n\n        return data\n\n    except Exception as e:\n        sys.exit(f\"Error fetching data from the database: {str(e)}\")\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nwarnings, sklearn.model_selection, sklearn.svm, sklearn.datasets, sklearn.metrics\n```python\nimport warnings\nimport sklearn.model_selection as model_selection\nimport sklearn.svm as svm\nimport sklearn.datasets as datasets\nimport sklearn.metrics as metrics\n\ndef svm_classification_with_warnings():\n    \"\"\"\n    Perform SVM classification on the iris dataset and raise a warning if the accuracy is below 0.9. \n    The warning action is set to 'always'.\n\n    Returns:\n    float: The accuracy of the SVM classification.\n\n    Requirements:\n    - warnings\n    - sklearn.model_selection\n    - sklearn.svm\n    - sklearn.datasets\n    - sklearn.metrics\n\n    Example:\n    >>> svm_classification_with_warnings()\n    \"\"\"\n    warnings.simplefilter('always')\n\n    iris = datasets.load_iris()\n    X_train, X_test, y_train, y_test = model_selection.train_test_split(iris.data, iris.target, test_size=0.33)\n\n    clf = svm.SVC()\n    clf.fit(X_train, y_train)\n    predictions = clf.predict(X_test)\n\n    accuracy = metrics.accuracy_score(y_test, predictions)\n\n    if accuracy < 0.9:\n        warnings.warn(\"The accuracy of the SVM classification is below 0.9.\")\n\n    return accuracy\n```"}
{"seed": "{\"task_id\": 13550423, \"prompt\": \"def f_13550423(l):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"' '.join(map(str, l))\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(['x','y','zzz']) == 'x y zzz'\\n\", \"\\n    assert candidate(['111','22','3']) == '111 22 3'\\n\", \"\\n    assert candidate(['']) == ''\\n\", \"\\n    assert candidate([':',':']) == ': :'\\n\", \"\\n    assert candidate([',','#','#$%']) == ', # #$%'\\n\", \"\\n    assert candidate(['a','b','c']) != 'abc'\\n\"], \"entry_point\": \"f_13550423\", \"intent\": \"concatenate items of list `l` with a space ' '\", \"library\": []}", "generation": "Scenario 1: \npandas, itertools, numpy, random\n```python\nimport pandas as pd\nimport itertools\nimport numpy as np\nfrom random import shuffle\n\n# Constants\nPOSSIBLE_VALUES = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']\n\ndef generate_random_dataframes(list_of_lists):\n    \"\"\"\n    For each list in list_of_lists, generate a random DataFrame with columns \n    named after the elements of the list and filled with random values from \n    POSSIBLE_VALUES.\n\n    Parameters:\n    list_of_lists (list): A list of lists. Each list contains the column names for a DataFrame.\n\n    Returns:\n    list: A list of pandas DataFrames.\n\n    Requirements:\n    - pandas\n    - itertools\n    - numpy\n    - random\n\n    Example:\n    >>> dfs = generate_random_dataframes([['x', 'y', 'z'], ['a', 'b', 'c']])\n    >>> for df in dfs:\n    >>>     print(df)\n    \"\"\"\n    dataframes = []\n\n    for list_ in list_of_lists:\n        df_dict = {}\n        for col in list_:\n            shuffle(POSSIBLE_VALUES)\n            df_dict[col] = POSSIBLE_VALUES.copy()\n        df = pd.DataFrame(df_dict)\n        dataframes.append(df)\n\n    return dataframes\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncollections, itertools, random\n```python\nfrom collections import Counter\nfrom itertools import combinations\nfrom random import choice\n\n# Constants\nPOSSIBLE_ITEMS = ['apple', 'banana', 'cherry', 'date', 'elderberry']\n\ndef create_baskets(list_of_lists):\n    \"\"\"\n    For each list in list_of_lists, create a 'basket' (Counter) with items \n    from POSSIBLE_ITEMS. The count of each item is the number of occurrences \n    in the list.\n\n    Parameters:\n    list_of_lists (list): A list of lists. Each list represents a 'basket'.\n\n    Returns:\n    list: A list of Counters.\n\n    Requirements:\n    - collections\n    - itertools\n    - random\n\n    Example:\n    >>> baskets = create_baskets([['x', 'y', 'z'], ['a', 'b', 'c']])\n    >>> for basket in baskets:\n    >>>     print(basket)\n    \"\"\"\n    baskets = []\n\n    for list_ in list_of_lists:\n        basket = Counter()\n        for _ in list_:\n            basket[choice(POSSIBLE_ITEMS)] += 1\n        baskets.append(basket)\n\n    return baskets\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnumpy, math, itertools\n```python\nimport numpy as np\nimport math\nfrom itertools import chain\n\n# Constants\nPOSSIBLE_NUMBERS = np.arange(1, 11)\n\ndef calculate_list_sums(list_of_lists):\n    \"\"\"\n    For each list in list_of_lists, calculate the sum of the squares of the \n    elements. The elements are replaced by numbers from POSSIBLE_NUMBERS.\n\n    Parameters:\n    list_of_lists (list): A list of lists. Each list represents a set of numbers.\n\n    Returns:\n    list: A list of sums.\n\n    Requirements:\n    - numpy\n    - math\n    - itertools\n\n    Example:\n    >>> sums = calculate_list_sums([['x', 'y', 'z'], ['a', 'b', 'c']])\n    >>> print(sums)\n    \"\"\"\n    sums = []\n\n    for list_ in list_of_lists:\n        sum_ = sum(math.pow(x, 2) for x in POSSIBLE_NUMBERS[:len(list_)])\n        sums.append(sum_)\n\n    return sums\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nmatplotlib, itertools, numpy, random\n```python\nimport matplotlib.pyplot as plt\nfrom itertools import cycle\nimport numpy as np\nfrom random import shuffle\n\n# Constants\nCOLORS = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n\ndef plot_lists(list_of_lists):\n    \"\"\"\n    For each list in list_of_lists, plot the elements as y-values against \n    their indices as x-values. The elements are replaced by numbers from \n    a shuffled range.\n\n    Parameters:\n    list_of_lists (list): A list of lists. Each list represents a set of y-values.\n\n    Returns:\n    None\n\n    Requirements:\n    - matplotlib.pyplot\n    - itertools\n    - numpy\n    - random\n\n    Example:\n    >>> plot_lists([['x', 'y', 'z'], ['a', 'b', 'c']])\n    \"\"\"\n    color_cycle = cycle(COLORS)\n\n    for list_ in list_of_lists:\n        y_values = np.arange(1, len(list_)+1)\n        shuffle(y_values)\n        plt.plot(y_values, next(color_cycle))\n\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \npandas, itertools, numpy, random\n```python\nimport pandas as pd\nimport itertools\nimport numpy as np\nfrom random import shuffle\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef generate_random_series(list_of_lists):\n    \"\"\"\n    For each list in list_of_lists, generate a random Series with index named \n    after the elements of the list and filled with random values from a shuffled range.\n\n    Parameters:\n    list_of_lists (list): A list of lists. Each list contains the index names for a Series.\n\n    Returns:\n    list: A list of pandas Series.\n\n    Requirements:\n    - pandas\n    - itertools\n    - numpy\n    - random\n\n    Example:\n    >>> series = generate_random_series([['x', 'y', 'z'], ['a', 'b', 'c']])\n    >>> for s in series:\n    >>>     print(s)\n    \"\"\"\n    series_list = []\n\n    for list_ in list_of_lists:\n        values = np.arange(1, len(list_)+1)\n        shuffle(values)\n        s = pd.Series(values, index=list_)\n        series_list.append(s)\n\n    return series_list\n```"}
{"seed": "{\"task_id\": 698223, \"prompt\": \"def f_698223():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"time.strptime('30/03/09 16:31:32.123', '%d/%m/%y %H:%M:%S.%f')\", \"test_start\": \"\\nimport time \\n\\ndef check(candidate):\", \"test\": [\"\\n    answer = time.strptime('30/03/09 16:31:32.123', '%d/%m/%y %H:%M:%S.%f')\\n    assert candidate() == answer\\n    false_1 = time.strptime('30/03/09 17:31:32.123', '%d/%m/%y %H:%M:%S.%f')\\n    assert candidate() != false_1\\n    false_2 = time.strptime('20/03/09 17:31:32.123', '%d/%m/%y %H:%M:%S.%f')\\n    assert candidate() != false_2\\n\"], \"entry_point\": \"f_698223\", \"intent\": \"parse a time string '30/03/09 16:31:32.123' containing milliseconds in it\", \"library\": [\"time\"]}", "generation": "Scenario 1:\ntime, datetime, pandas, matplotlib\n```python\nimport time\nfrom datetime import datetime\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nTIME_FORMAT = '%d/%m/%y %H:%M:%S.%f'\n\ndef plot_seconds_distribution(time_strings):\n    \"\"\"\n    Parse a list of time strings and plot a distribution of the seconds part.\n\n    Parameters:\n    time_strings (list): A list of time strings in the format 'dd/mm/yy HH:MM:SS.fff'.\n\n    Returns:\n    None\n\n    Requirements:\n    - time\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> time_strings = ['30/03/09 16:31:32.123', '15/04/10 14:25:46.789', '20/12/11 12:34:56.000']\n    >>> plot_seconds_distribution(time_strings)\n    \"\"\"\n    seconds = []\n\n    for time_string in time_strings:\n        time_struct = time.strptime(time_string, TIME_FORMAT)\n        seconds.append(time_struct.tm_sec)\n\n    pd.Series(seconds).plot(kind='hist', rwidth=0.8, bins=60)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ntime, datetime, pytz, dateutil\n```python\nimport time\nfrom datetime import datetime\nimport pytz\nfrom dateutil.parser import parse\n\n# Constants\nTIME_FORMAT = '%d/%m/%y %H:%M:%S.%f'\n\ndef convert_time_string(time_string, from_tz, to_tz):\n    \"\"\"\n    Convert a time string from one timezone to another.\n\n    Parameters:\n    time_string (str): A time string in the format 'dd/mm/yy HH:MM:SS.fff'.\n    from_tz (str): The timezone of the given time string.\n    to_tz (str): The timezone to which the time string should be converted.\n\n    Returns:\n    str: The converted time string in the format 'dd/mm/yy HH:MM:SS.fff'.\n\n    Requirements:\n    - time\n    - datetime\n    - pytz\n    - dateutil.parser\n\n    Example:\n    >>> convert_time_string('30/03/09 16:31:32.123', 'UTC', 'America/New_York')\n    \"\"\"\n    from_tz = pytz.timezone(from_tz)\n    to_tz = pytz.timezone(to_tz)\n    dt = parse(time_string)\n    dt = dt.replace(tzinfo=from_tz)\n    dt = dt.astimezone(to_tz)\n\n    return dt.strftime(TIME_FORMAT)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ntime, datetime, numpy, matplotlib\n```python\nimport time\nimport datetime\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nTIME_FORMAT = '%d/%m/%y %H:%M:%S.%f'\n\ndef plot_time_string_difference(time_strings):\n    \"\"\"\n    Calculate the time difference between each consecutive pair of time strings \n    and plots a bar graph for the differences.\n\n    Parameters:\n    time_strings (list): A list of time strings in the format 'dd/mm/yy HH:MM:SS.fff'.\n\n    Returns:\n    None\n\n    Requirements:\n    - time\n    - datetime\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> time_strings = ['30/03/09 16:31:32.123', '30/03/09 16:32:33.123', '30/03/09 16:33:34.123']\n    >>> plot_time_string_difference(time_strings)\n    \"\"\"\n    differences = []\n\n    for i in range(len(time_strings)-1):\n        t1 = datetime.datetime.strptime(time_strings[i], TIME_FORMAT)\n        t2 = datetime.datetime.strptime(time_strings[i+1], TIME_FORMAT)\n        difference = (t2 - t1).total_seconds()\n        differences.append(difference)\n\n    plt.bar(np.arange(len(differences)), differences)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ntime, datetime, pytz, pandas\n```python\nimport time\nfrom datetime import datetime\nimport pytz\nimport pandas as pd\n\n# Constants\nTIME_FORMAT = '%d/%m/%y %H:%M:%S.%f'\n\ndef create_timezone_dataframe(time_strings, tz):\n    \"\"\"\n    Parse a list of time strings and create a dataframe with the time in a \n    specified timezone and the original time string.\n\n    Parameters:\n    time_strings (list): A list of time strings in the format 'dd/mm/yy HH:MM:SS.fff'.\n    tz (str): The timezone to which the time strings should be converted.\n\n    Returns:\n    DataFrame: A pandas DataFrame with two columns: 'Original Time' and 'Converted Time'.\n\n    Requirements:\n    - time\n    - datetime\n    - pytz\n    - pandas\n\n    Example:\n    >>> time_strings = ['30/03/09 16:31:32.123', '15/04/10 14:25:46.789', '20/12/11 12:34:56.000']\n    >>> df = create_timezone_dataframe(time_strings, 'America/New_York')\n    >>> print(df)\n    \"\"\"\n    data = []\n\n    for time_string in time_strings:\n        dt = datetime.strptime(time_string, TIME_FORMAT)\n        dt = dt.replace(tzinfo=pytz.UTC)\n        dt = dt.astimezone(pytz.timezone(tz))\n        data.append([time_string, dt.strftime(TIME_FORMAT)])\n\n    df = pd.DataFrame(data, columns=['Original Time', 'Converted Time'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\ntime, datetime, pytz, numpy\n```python\nimport time\nfrom datetime import datetime\nimport pytz\nimport numpy as np\n\n# Constants\nTIME_FORMAT = '%d/%m/%y %H:%M:%S.%f'\n\ndef calculate_mean_time_difference(time_strings, tz):\n    \"\"\"\n    Calculate the mean time difference in seconds between each consecutive pair \n    of time strings in a specified timezone.\n\n    Parameters:\n    time_strings (list): A list of time strings in the format 'dd/mm/yy HH:MM:SS.fff'.\n    tz (str): The timezone to which the time strings should be converted.\n\n    Returns:\n    float: The mean time difference in seconds.\n\n    Requirements:\n    - time\n    - datetime\n    - pytz\n    - numpy\n\n    Example:\n    >>> time_strings = ['30/03/09 16:31:32.123', '30/03/09 16:32:33.123', '30/03/09 16:33:34.123']\n    >>> mean_difference = calculate_mean_time_difference(time_strings, 'America/New_York')\n    >>> print(mean_difference)\n    \"\"\"\n    differences = []\n\n    for i in range(len(time_strings)-1):\n        t1 = datetime.strptime(time_strings[i], TIME_FORMAT)\n        t1 = t1.replace(tzinfo=pytz.UTC)\n        t1 = t1.astimezone(pytz.timezone(tz))\n        \n        t2 = datetime.strptime(time_strings[i+1], TIME_FORMAT)\n        t2 = t2.replace(tzinfo=pytz.UTC)\n        t2 = t2.astimezone(pytz.timezone(tz))\n        \n        difference = (t2 - t1).total_seconds()\n        differences.append(difference)\n\n    mean_difference = np.mean(differences)\n\n    return mean_difference\n```"}
{"seed": "{\"task_id\": 6633523, \"prompt\": \"def f_6633523(my_string):\\n\\t\", \"suffix\": \"\\n\\treturn my_float\", \"canonical_solution\": \"my_float = float(my_string.replace(',', ''))\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert (candidate('1,234.00') - 1234.0) < 1e-6\\n\", \"\\n    assert (candidate('0.00') - 0.00) < 1e-6\\n\", \"\\n    assert (candidate('1,000,000.00') - 1000000.00) < 1e-6\\n\", \"\\n    assert (candidate('1,000,000.00') - 999999.98) > 1e-6\\n\", \"\\n    assert (candidate('1') - 1.00) < 1e-6\\n\"], \"entry_point\": \"f_6633523\", \"intent\": \"convert a string `my_string` with dot and comma into a float number `my_float`\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nDATA = {'Product': ['Apple', 'Banana', 'Orange', 'Pear', 'Peach'],\n        'Price_String': ['1,234.00', '567.89', '1,000,000.00', '0.00', '1,000.12']}\n\ndef analyze_product_prices(data=DATA):\n    \"\"\"\n    Convert the string price with dot and comma into a float number, \n    calculate the mean, median, and standard deviation of the prices, plot a histogram of the prices.\n    \n    Parameters:\n    data (dict): The dictionary containing product and their prices in string format.\n    \n    Returns:\n    dict: A dictionary containing mean, median, and standard deviation of the prices.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> analyze_product_prices()\n    \"\"\"\n    df = pd.DataFrame(data)\n    df['Price_Float'] = df['Price_String'].apply(lambda x: float(x.replace(',', '')))\n\n    mean_price = np.mean(df['Price_Float'])\n    median_price = np.median(df['Price_Float'])\n    std_dev_price = np.std(df['Price_Float'])\n\n    plt.hist(df['Price_Float'], bins=10, alpha=0.5)\n    plt.title('Product Prices Distribution')\n    plt.xlabel('Price')\n    plt.ylabel('Frequency')\n    plt.show()\n\n    return {'mean': mean_price, 'median': median_price, 'std_dev': std_dev_price}\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, sklearn.linear_model\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\n# Constants\nDATA = {'Area_String': ['1,000', '2,000', '3,000', '4,000', '5,000'],\n        'Price': [100, 200, 300, 400, 500]}\n\ndef predict_price(area_string, data=DATA):\n    \"\"\"\n    Convert the string area with comma into a float number, \n    train a linear regression model based on area and price, \n    predict the price for a given area.\n    \n    Parameters:\n    area_string (str): The string of area for which the price is to be predicted.\n    data (dict): The dictionary containing area and their corresponding prices.\n    \n    Returns:\n    float: The predicted price for the given area.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.linear_model\n    \n    Example:\n    >>> predict_price('6,000')\n    \"\"\"\n    df = pd.DataFrame(data)\n    df['Area_Float'] = df['Area_String'].apply(lambda x: float(x.replace(',', '')))\n    \n    X = np.array(df['Area_Float']).reshape((-1, 1))\n    Y = np.array(df['Price'])\n\n    model = LinearRegression()\n    model.fit(X, Y)\n\n    area_float = float(area_string.replace(',', ''))\n    price_predicted = model.predict(np.array(area_float).reshape((-1, 1)))\n\n    return float(price_predicted)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \npandas, seaborn, matplotlib\n```python\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nDATA = {'Weight_String': ['60.5', '65.7', '70.2', '75.9', '80.1'],\n        'Height': [160, 165, 170, 175, 180]}\n\ndef plot_weight_vs_height(data=DATA):\n    \"\"\"\n    Convert the string weight with dot into a float number, \n    plot a scatter plot of weight against height.\n    \n    Parameters:\n    data (dict): The dictionary containing weight and height.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - pandas\n    - seaborn\n    - matplotlib.pyplot\n    \n    Example:\n    >>> plot_weight_vs_height()\n    \"\"\"\n    df = pd.DataFrame(data)\n    df['Weight_Float'] = df['Weight_String'].apply(lambda x: float(x))\n\n    sns.scatterplot(data=df, x='Weight_Float', y='Height')\n    plt.title('Weight vs Height')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, scipy.stats\n```python\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\n\n# Constants\nDATA = {'Score_String': ['80.5', '85.7', '90.2', '95.9', '100.1'],\n        'Grade': ['B', 'B+', 'A-', 'A', 'A+']}\n\ndef calculate_corr(data=DATA):\n    \"\"\"\n    Convert the string score with dot into a float number, \n    calculate the correlation between score and grade.\n    \n    Parameters:\n    data (dict): The dictionary containing score and grade.\n    \n    Returns:\n    float: The correlation coefficient between score and grade.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - scipy.stats\n    \n    Example:\n    >>> calculate_corr()\n    \"\"\"\n    df = pd.DataFrame(data)\n    df['Score_Float'] = df['Score_String'].apply(lambda x: float(x))\n    df['Grade_Encoded'] = df['Grade'].astype('category').cat.codes\n\n    correlation, _ = stats.pearsonr(df['Score_Float'], df['Grade_Encoded'])\n\n    return correlation\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, matplotlib, sklearn.preprocessing\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Constants\nDATA = {'Salary_String': ['1,000', '2,000', '3,000', '4,000', '5,000'],\n        'Experience': [1, 2, 3, 4, 5]}\n\ndef plot_normalized_salary(data=DATA):\n    \"\"\"\n    Convert the string salary with comma into a float number, \n    normalize the salary, and plot a line graph of normalized salary against experience.\n    \n    Parameters:\n    data (dict): The dictionary containing salary and experience.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - sklearn.preprocessing\n    \n    Example:\n    >>> plot_normalized_salary()\n    \"\"\"\n    df = pd.DataFrame(data)\n    df['Salary_Float'] = df['Salary_String'].apply(lambda x: float(x.replace(',', '')))\n    \n    scaler = MinMaxScaler()\n    df['Normalized_Salary'] = scaler.fit_transform(df[['Salary_Float']])\n\n    plt.plot(df['Experience'], df['Normalized_Salary'])\n    plt.title('Normalized Salary vs Experience')\n    plt.xlabel('Experience')\n    plt.ylabel('Normalized Salary')\n    plt.show()\n```"}
{"seed": "{\"task_id\": 6633523, \"prompt\": \"def f_6633523():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"float('123,456.908'.replace(',', ''))\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert (candidate() - 123456.908) < 1e-6\\n    assert (candidate() - 123456.9) > 1e-6\\n    assert (candidate() - 1234.908) > 1e-6\\n    assert type(candidate()) == float\\n    assert int(candidate()) == 123456\\n\"], \"entry_point\": \"f_6633523\", \"intent\": \"convert a string `123,456.908` with dot and comma into a floating number\", \"library\": []}", "generation": "Scenario 1: \n\ncsv, pandas, numpy, matplotlib\n```python\nimport csv\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nDATA_FILE = 'data.csv'  # Sample data file\n\ndef analyze_numeric_data():\n    \"\"\"\n    Analyze numeric data from a CSV file: convert strings with commas into floating numbers, \n    calculate mean and standard deviation, and plot a histogram.\n    \n    Returns:\n    float: Mean of the data.\n    float: Standard deviation of the data.\n    \n    Requirements:\n    - csv\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> mean, std_dev = analyze_numeric_data()\n    >>> print(f'Mean: {mean}, Standard Deviation: {std_dev}')\n    \"\"\"\n    df = pd.read_csv(DATA_FILE)\n    df = df.applymap(lambda x: float(x.replace(',', '')) if isinstance(x, str) else x)\n\n    mean = df.mean()\n    std_dev = df.std()\n\n    df.hist(bins=50)\n    plt.show()\n\n    return mean, std_dev\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\n\nre, nltk, string, collections, matplotlib\n```python\nimport re\nimport nltk\nfrom string import punctuation\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\n# Constants\nTEXT = \"\"\"...\"\"\"  # Sample text\n\ndef analyze_text():\n    \"\"\"\n    Analyze a text: remove punctuation, tokenize into words, \n    count the frequency of each word, and plot a bar chart of the 10 most common words.\n    \n    Returns:\n    list: The 10 most common words and their counts.\n    \n    Requirements:\n    - re\n    - nltk\n    - string\n    - collections\n    - matplotlib.pyplot\n    \n    Example:\n    >>> common_words = analyze_text()\n    >>> print(common_words)\n    \"\"\"\n    text = re.sub('['+punctuation+']', '', TEXT).lower()\n    words = nltk.word_tokenize(text)\n    word_counts = Counter(words)\n\n    most_common_words = word_counts.most_common(10)\n\n    plt.bar(*zip(*most_common_words))\n    plt.show()\n\n    return most_common_words\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\n\nstring, random, pandas, numpy\n```python\nimport string\nimport random\nimport pandas as pd\nimport numpy as np\n\n# Constants\nNUM_SAMPLES = 1000  # Number of samples\n\ndef generate_random_dataset():\n    \"\"\"\n    Generate a random dataset with a string field and a floating number field. \n    The string field consists of random letters and the float field is a random number \n    with a comma as the thousands separator.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the generated dataset.\n    \n    Requirements:\n    - string\n    - random\n    - pandas\n    - numpy\n    \n    Example:\n    >>> dataset = generate_random_dataset()\n    >>> print(dataset)\n    \"\"\"\n    data = {\n        'String Field': [''.join(random.choices(string.ascii_letters, k=10)) for _ in range(NUM_SAMPLES)],\n        'Float Field': [f'{x:,.2f}' for x in np.random.uniform(0, 10000, NUM_SAMPLES)]\n    }\n\n    df = pd.DataFrame(data)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\n\nnumpy, matplotlib, scipy\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n# Constants\nDISTRIBUTION_PARAMS = (123456.908, 1.2)  # Parameters of the distribution\n\ndef analyze_distribution():\n    \"\"\"\n    Generate a random sample from a normal distribution with given mean and standard deviation, \n    plot a histogram and a QQ-plot to analyze the distribution.\n    \n    Returns:\n    float: The skewness of the sample.\n    float: The kurtosis of the sample.\n    \n    Requirements:\n    - numpy\n    - matplotlib\n    - scipy\n    \n    Example:\n    >>> skewness, kurtosis = analyze_distribution()\n    >>> print(f'Skewness: {skewness}, Kurtosis: {kurtosis}')\n    \"\"\"\n    sample = np.random.normal(*DISTRIBUTION_PARAMS, 1000)\n\n    plt.hist(sample, bins=50)\n    plt.show()\n\n    stats.probplot(sample, plot=plt)\n    plt.show()\n\n    skewness = stats.skew(sample)\n    kurtosis = stats.kurtosis(sample)\n\n    return skewness, kurtosis\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\n\nnumpy, pandas, sklearn\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\n# Constants\nDATA = np.random.rand(100, 5)  # Sample data\n\ndef preprocess_data():\n    \"\"\"\n    Preprocess a dataset: convert it to a pandas DataFrame, \n    replace values less than 0.5 with zeros, and standardize the data.\n    \n    Returns:\n    DataFrame: The preprocessed dataset.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.preprocessing\n    \n    Example:\n    >>> dataset = preprocess_data()\n    >>> print(dataset)\n    \"\"\"\n    df = pd.DataFrame(DATA)\n    df[df < 0.5] = 0\n\n    scaler = StandardScaler()\n    df = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n\n    return df\n```"}
{"seed": "{\"task_id\": 3108285, \"prompt\": \"def f_3108285():\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"sys.path.append('/path/to/whatever')\", \"test_start\": \"\\nimport sys \\n\\ndef check(candidate):\", \"test\": [\"\\n    original_paths = [sp for sp in sys.path]\\n    candidate()\\n    assert '/path/to/whatever' in sys.path\\n\"], \"entry_point\": \"f_3108285\", \"intent\": \"set python path '/path/to/whatever' in python script\", \"library\": [\"sys\"]}", "generation": "Scenario 1: \nos, sys, pathlib, shutil\n```python\nimport os\nimport sys\nfrom pathlib import Path\nimport shutil\n\n# Constants\nPATH_TO_APPEND = '/path/to/whatever'\n\ndef append_path_and_create_directory(path_to_append=PATH_TO_APPEND):\n    \"\"\"\n    Append a specific path to the sys.path and create a directory at that path if it doesn't exist.\n\n    Parameters:\n    path_to_append (str): The path to append to sys.path and to create a directory. Default is '/path/to/whatever'.\n\n    Returns:\n    str: The path that was appended and where the directory was created.\n\n    Requirements:\n    - os\n    - sys\n    - pathlib.Path\n    - shutil\n\n    Example:\n    >>> append_path_and_create_directory('/path/to/new_directory')\n    \"\"\"\n    sys.path.append(path_to_append)\n    Path(path_to_append).mkdir(parents=True, exist_ok=True)\n\n    return path_to_append\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nsys, os, subprocess\n```python\nimport sys\nimport os\nimport subprocess\n\n# Constants\nPYTHON_VERSION = '3.8'\nPATH_TO_APPEND = '/path/to/whatever'\n\ndef switch_python_version_and_append_path(python_version=PYTHON_VERSION, path_to_append=PATH_TO_APPEND):\n    \"\"\"\n    Switch to a specific Python version and append a specific path to the sys.path.\n\n    Parameters:\n    python_version (str): The Python version to switch to. Default is '3.8'.\n    path_to_append (str): The path to append to sys.path. Default is '/path/to/whatever'.\n\n    Returns:\n    str: The Python version that was switched to.\n\n    Requirements:\n    - sys\n    - os\n    - subprocess\n\n    Example:\n    >>> switch_python_version_and_append_path('3.7', '/path/to/new_directory')\n    \"\"\"\n    subprocess.run(['pyenv', 'global', python_version], check=True)\n    sys.path.append(path_to_append)\n\n    return python_version\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nos, sys, json, datetime\n```python\nimport os\nimport sys\nimport json\nfrom datetime import datetime\n\n# Constants\nPATH_TO_APPEND = '/path/to/whatever'\nJSON_FILE = '/path/to/json_file.json'\n\ndef append_path_and_update_json(path_to_append=PATH_TO_APPEND, json_file=JSON_FILE):\n    \"\"\"\n    Append a specific path to the sys.path and update a JSON file with the current date and time.\n\n    Parameters:\n    path_to_append (str): The path to append to sys.path. Default is '/path/to/whatever'.\n    json_file (str): The path to the JSON file to update. Default is '/path/to/json_file.json'.\n\n    Returns:\n    dict: The updated JSON data.\n\n    Requirements:\n    - os\n    - sys\n    - json\n    - datetime.datetime\n\n    Example:\n    >>> append_path_and_update_json('/path/to/new_directory', '/path/to/new_json_file.json')\n    \"\"\"\n    sys.path.append(path_to_append)\n\n    with open(json_file, 'r+') as file:\n        json_data = json.load(file)\n        json_data['last_updated'] = str(datetime.now())\n        file.seek(0)\n        json.dump(json_data, file, indent=4)\n        file.truncate()\n\n    return json_data\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nos, sys, configparser\n```python\nimport os\nimport sys\nfrom configparser import ConfigParser\n\n# Constants\nPATH_TO_APPEND = '/path/to/whatever'\nCONFIG_FILE = '/path/to/config.ini'\n\ndef append_path_and_update_config(path_to_append=PATH_TO_APPEND, config_file=CONFIG_FILE):\n    \"\"\"\n    Append a specific path to the sys.path and update a config file with that path.\n\n    Parameters:\n    path_to_append (str): The path to append to sys.path. Default is '/path/to/whatever'.\n    config_file (str): The path to the config file to update. Default is '/path/to/config.ini'.\n\n    Returns:\n    ConfigParser: The updated ConfigParser object.\n\n    Requirements:\n    - os\n    - sys\n    - configparser.ConfigParser\n\n    Example:\n    >>> append_path_and_update_config('/path/to/new_directory', '/path/to/new_config.ini')\n    \"\"\"\n    sys.path.append(path_to_append)\n\n    config = ConfigParser()\n    config.read(config_file)\n    config.set('DEFAULT', 'path_to_append', path_to_append)\n    with open(config_file, 'w') as file:\n        config.write(file)\n\n    return config\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nos, sys, sqlite3\n```python\nimport os\nimport sys\nimport sqlite3\n\n# Constants\nPATH_TO_APPEND = '/path/to/whatever'\nDATABASE = '/path/to/database.db'\n\ndef append_path_and_update_database(path_to_append=PATH_TO_APPEND, database=DATABASE):\n    \"\"\"\n    Append a specific path to the sys.path and update a SQLite database with that path.\n\n    Parameters:\n    path_to_append (str): The path to append to sys.path. Default is '/path/to/whatever'.\n    database (str): The path to the SQLite database to update. Default is '/path/to/database.db'.\n\n    Returns:\n    str: The path that was appended and updated in the database.\n\n    Requirements:\n    - os\n    - sys\n    - sqlite3\n\n    Example:\n    >>> append_path_and_update_database('/path/to/new_directory', '/path/to/new_database.db')\n    \"\"\"\n    sys.path.append(path_to_append)\n\n    conn = sqlite3.connect(database)\n    cur = conn.cursor()\n    cur.execute(\"INSERT INTO paths (path) VALUES (?)\", (path_to_append,))\n    conn.commit()\n    conn.close()\n\n    return path_to_append\n```"}
{"seed": "{\"task_id\": 2195340, \"prompt\": \"def f_2195340():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"re.split('(\\\\\\\\W+)', 'Words, words, words.')\", \"test_start\": \"\\nimport re\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate() == ['Words', ', ', 'words', ', ', 'words', '.', '']\\n    assert candidate() == ['Words', ', '] + ['words', ', ', 'words', '.', '']\\n\"], \"entry_point\": \"f_2195340\", \"intent\": \"split string 'Words, words, words.' using a regex '(\\\\\\\\W+)'\", \"library\": [\"re\"]}", "generation": "Scenario 1:\nre, nltk, collections, string\n```python\nimport re\nimport nltk\nfrom collections import Counter\nfrom string import punctuation\n\n# Constants\nSTOPWORDS = nltk.corpus.stopwords.words('english')\n\ndef word_frequency(text):\n    \"\"\"\n    Calculate the frequency of non-stopwords in a text string, not considering punctuation.\n    \n    Parameters:\n    text (str): The text string.\n    \n    Returns:\n    dict: A dictionary with each non-stopword and its frequency.\n    \n    Requirements:\n    - re\n    - nltk\n    - collections.Counter\n    - string.punctuation\n\n    Example:\n    >>> word_frequency('Words, words, words. And more words!')\n    \"\"\"\n    words = re.split(r'\\W+', text.lower())\n    words = [word for word in words if word and word not in STOPWORDS and word not in punctuation]\n    frequency = Counter(words)\n    return dict(frequency)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nre, pandas, seaborn, matplotlib.pyplot\n```python\nimport re\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef plot_word_count_distribution(text):\n    \"\"\"\n    Plot the distribution of word counts in a text string.\n    \n    Parameters:\n    text (str): The text string.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - re\n    - pandas\n    - seaborn\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_word_count_distribution('Words, words, words. And more words!')\n    \"\"\"\n    words = re.split(r'\\W+', text)\n    word_counts = pd.Series([len(word) for word in words if word])\n    sns.histplot(word_counts, kde=True)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nre, random, string\n```python\nimport re\nimport random\nimport string\n\n# Constants\nPUNCTUATION = list(string.punctuation)\n\ndef random_punctuation_replacement(text):\n    \"\"\"\n    Replace all punctuation in a text string with a random punctuation mark.\n    \n    Parameters:\n    text (str): The text string.\n    \n    Returns:\n    str: The text with replaced punctuation.\n    \n    Requirement:\n    - re\n    - random\n    - string.punctuation\n    \n    Example:\n    >>> random_punctuation_replacement('Words, words, words. And more words!')\n    \"\"\"\n    return re.sub(r'(\\W+)', lambda x: random.choice(PUNCTUATION), text)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nre, numpy, scipy.stats\n```python\nimport re\nimport numpy as np\nfrom scipy.stats import ttest_rel\n\ndef t_test_word_counts(text1, text2):\n    \"\"\"\n    Perform a paired t-test on the word counts of two text strings.\n    \n    Parameters:\n    text1 (str), text2 (str): The two text strings.\n    \n    Returns:\n    float: The t-statistic.\n    float: The p-value.\n    \n    Requirements:\n    - re\n    - numpy\n    - scipy.stats.ttest_rel\n    \n    Example:\n    >>> t_test_word_counts('Words, words, words.', 'And more words!')\n    \"\"\"\n    word_counts1 = np.array([len(word) for word in re.split(r'\\W+', text1) if word])\n    word_counts2 = np.array([len(word) for word in re.split(r'\\W+', text2) if word])\n    t_statistic, p_value = ttest_rel(word_counts1, word_counts2)\n    return t_statistic, p_value\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nre, os, glob\n```python\nimport re\nimport os\nimport glob\n\ndef count_files_with_word(directory, word):\n    \"\"\"\n    Count the number of files in a directory that contain a specific word.\n    \n    Parameters:\n    directory (str): The directory path.\n    word (str): The word to search for.\n    \n    Returns:\n    int: The number of files.\n    \n    Requirements:\n    - re\n    - os\n    - glob\n    \n    Example:\n    >>> count_files_with_word('./documents', 'word')\n    \"\"\"\n    count = 0\n    for filename in glob.glob(os.path.join(directory, '*')):\n        with open(filename, 'r') as f:\n            text = f.read()\n            if re.search(r'\\b' + word + r'\\b', text):\n                count += 1\n    return count\n```"}
{"seed": "{\"task_id\": 17977584, \"prompt\": \"def f_17977584():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"open('Output.txt', 'a')\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    f = candidate()\\n    assert str(f.__class__) == \\\"<class '_io.TextIOWrapper'>\\\"\\n    assert f.name == 'Output.txt'\\n    assert f.mode == 'a'\\n\"], \"entry_point\": \"f_17977584\", \"intent\": \"open a file `Output.txt` in append mode\", \"library\": []}", "generation": "Scenario 1:\nos, csv, random, datetime\n```python\nimport os\nimport csv\nimport random\nfrom datetime import datetime\n\n# Constants\nFILE_NAME = 'Output.txt'\nFIELDS = ['Timestamp', 'Temperature', 'Humidity']\n\ndef generate_sensor_data():\n    \"\"\"\n    Generate a CSV file 'Output.txt' containing sensor data for temperature and humidity. \n    The data is generated randomly and written in append mode.\n\n    Requirements:\n    - os\n    - csv\n    - random\n    - datetime\n\n    Example:\n    >>> generate_sensor_data()\n    \"\"\"\n    temperature = random.uniform(20, 30)  # temperature between 20 and 30\n    humidity = random.uniform(50, 60)  # humidity between 50 and 60\n    timestamp = datetime.now()\n\n    # Check if file exists and write headers if not\n    if not os.path.isfile(FILE_NAME):\n        with open(FILE_NAME, 'w') as f:\n            csv_writer = csv.writer(f)\n            csv_writer.writerow(FIELDS)\n\n    # Append data\n    with open(FILE_NAME, 'a') as f:\n        csv_writer = csv.writer(f)\n        csv_writer.writerow([timestamp, temperature, humidity])\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, pandas, matplotlib, numpy\n```python\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nFILE_NAME = 'Output.txt'\n\ndef analyze_data():\n    \"\"\"\n    Open 'Output.txt' file, clean the data, perform some basic analysis and create a plot.\n\n    Requirements:\n    - os\n    - pandas\n    - matplotlib\n    - numpy\n\n    Example:\n    >>> analyze_data()\n    \"\"\"\n    # Check if file exists\n    if not os.path.isfile(FILE_NAME):\n        print(\"File does not exist.\")\n        return\n\n    # Load data\n    data = pd.read_csv(FILE_NAME)\n\n    # Clean data\n    data = data.dropna()\n\n    # Perform analysis\n    mean = np.mean(data)\n    median = np.median(data)\n\n    # Create plot\n    plt.plot(data)\n    plt.show()\n\n    return mean, median\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, json, requests, bs4\n```python\nimport os\nimport json\nimport requests\nfrom bs4 import BeautifulSoup\n\n# Constants\nFILE_NAME = 'Output.txt'\nURL = 'http://example.com'\n\ndef web_scrape():\n    \"\"\"\n    Scrape a webpage, extract some data, convert the data into JSON and append it to 'Output.txt' file.\n\n    Requirements:\n    - os\n    - json\n    - requests\n    - bs4\n\n    Example:\n    >>> web_scrape()\n    \"\"\"\n    response = requests.get(URL)\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    # Extract data\n    data = {'title': soup.title.string}\n\n    # Convert data to JSON\n    json_data = json.dumps(data)\n\n    # Append to file\n    with open(FILE_NAME, 'a') as f:\n        f.write(json_data + '\\n')\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, shutil, zipfile\n```python\nimport os\nimport shutil\nimport zipfile\n\n# Constants\nFILE_NAME = 'Output.txt'\nBACKUP_DIR = 'backup'\n\ndef backup_file():\n    \"\"\"\n    Create a backup of 'Output.txt' file by compressing it into a ZIP file.\n\n    Requirements:\n    - os\n    - shutil\n    - zipfile\n\n    Example:\n    >>> backup_file()\n    \"\"\"\n    # Create backup directory if it doesn't exist\n    if not os.path.exists(BACKUP_DIR):\n        os.makedirs(BACKUP_DIR)\n\n    # Create ZIP file\n    with zipfile.ZipFile(os.path.join(BACKUP_DIR, FILE_NAME + '.zip'), 'w') as zipf:\n        zipf.write(FILE_NAME)\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, csv, collections\n```python\nimport os\nimport csv\nfrom collections import Counter\n\n# Constants\nFILE_NAME = 'Output.txt'\n\ndef count_words():\n    \"\"\"\n    Open 'Output.txt' file, count the frequency of each word and return the most common word.\n\n    Requirements:\n    - os\n    - csv\n    - collections\n\n    Example:\n    >>> count_words()\n    \"\"\"\n    # Check if file exists\n    if not os.path.isfile(FILE_NAME):\n        print(\"File does not exist.\")\n        return\n\n    word_counter = Counter()\n\n    with open(FILE_NAME, 'r') as f:\n        csv_reader = csv.reader(f)\n        for row in csv_reader:\n            for word in row:\n                word_counter[word] += 1\n\n    most_common_word, frequency = word_counter.most_common(1)[0]\n\n    return most_common_word, frequency\n```"}
{"seed": "{\"task_id\": 22676, \"prompt\": \"def f_22676():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"urllib.request.urlretrieve('https://github.com/zorazrw/multilingual-conala/blob/master/dataset/test/es_test.json', 'mp3.mp3')\", \"test_start\": \"\\nimport urllib \\n\\ndef check(candidate):\", \"test\": [\"\\n    results = candidate()\\n    assert len(results) == 2\\n    assert results[0] == \\\"mp3.mp3\\\"\\n    assert results[1].values()[0] == \\\"GitHub.com\\\"\\n\"], \"entry_point\": \"f_22676\", \"intent\": \"download a file \\\"http://www.example.com/songs/mp3.mp3\\\" over HTTP and save to \\\"mp3.mp3\\\"\", \"library\": [\"urllib\"]}", "generation": "Scenario 1:\nurllib, os, zipfile, shutil\n```python\nimport urllib.request\nimport os\nimport zipfile\nimport shutil\n\n# Constants\nTARGET_DIR = 'downloaded_files'\nTARGET_ZIP_FILE = 'downloaded_files.zip'\n\ndef download_extract_zip_file(url):\n    \"\"\"\n    Download a zip file from the specified url and extract it to a target directory.\n\n    Parameters:\n    url (str): The url of the zip file to be downloaded.\n\n    Returns:\n    str: The target directory path where the zip file is extracted.\n\n    Requirements:\n    - urllib.request\n    - os\n    - zipfile\n    - shutil\n\n    Example:\n    >>> download_extract_zip_file('http://example.com/files.zip')\n    \"\"\"\n    if not os.path.exists(TARGET_DIR):\n        os.makedirs(TARGET_DIR)\n\n    urllib.request.urlretrieve(url, TARGET_ZIP_FILE)\n\n    with zipfile.ZipFile(TARGET_ZIP_FILE, 'r') as zip_ref:\n        zip_ref.extractall(TARGET_DIR)\n\n    os.remove(TARGET_ZIP_FILE)\n\n    return TARGET_DIR\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nurllib, os, hashlib, tarfile\n```python\nimport urllib.request\nimport os\nimport hashlib\nimport tarfile\n\n# Constants\nTARGET_TAR_FILE = 'downloaded_files.tar.gz'\nEXPECTED_MD5_CHECKSUM = 'd41d8cd98f00b204e9800998ecf8427e'\n\ndef download_extract_tar_file(url):\n    \"\"\"\n    Download a tar.gz file from the specified url, validate it using MD5 checksum, and extract it.\n\n    Parameters:\n    url (str): The url of the tar.gz file to be downloaded.\n\n    Returns:\n    bool: True if the tar.gz file is downloaded, validated, and extracted successfully, False otherwise.\n\n    Requirements:\n    - urllib.request\n    - os\n    - hashlib\n    - tarfile\n\n    Example:\n    >>> download_extract_tar_file('http://example.com/files.tar.gz')\n    \"\"\"\n    urllib.request.urlretrieve(url, TARGET_TAR_FILE)\n\n    md5_hash = hashlib.md5()\n    with open(TARGET_TAR_FILE, 'rb') as f:\n        for byte_block in iter(lambda: f.read(4096), b\"\"):\n            md5_hash.update(byte_block)\n    if md5_hash.hexdigest() != EXPECTED_MD5_CHECKSUM:\n        os.remove(TARGET_TAR_FILE)\n        return False\n\n    with tarfile.open(TARGET_TAR_FILE, 'r:gz') as tar_ref:\n        tar_ref.extractall()\n\n    os.remove(TARGET_TAR_FILE)\n\n    return True\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nurllib, os, mmap, re\n```python\nimport urllib.request\nimport os\nimport mmap\nimport re\n\n# Constants\nTARGET_FILE = 'downloaded_file.txt'\nSEARCH_PATTERN = r'\\bERROR\\b'\n\ndef download_file_search_errors(url):\n    \"\"\"\n    Download a text file from the specified url and search for occurrences of the word 'ERROR'.\n\n    Parameters:\n    url (str): The url of the text file to be downloaded.\n\n    Returns:\n    int: The number of occurrences of the word 'ERROR'.\n\n    Requirements:\n    - urllib.request\n    - os\n    - mmap\n    - re\n\n    Example:\n    >>> download_file_search_errors('http://example.com/log.txt')\n    \"\"\"\n    urllib.request.urlretrieve(url, TARGET_FILE)\n\n    with open(TARGET_FILE, 'r') as f:\n        data = mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ)\n        occurrences = len(re.findall(SEARCH_PATTERN, data))\n\n    os.remove(TARGET_FILE)\n\n    return occurrences\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nurllib, os, csv, collections\n```python\nimport urllib.request\nimport os\nimport csv\nimport collections\n\n# Constants\nTARGET_CSV_FILE = 'downloaded_file.csv'\n\ndef download_csv_count_values(url, column_name):\n    \"\"\"\n    Download a csv file from the specified url and count the occurrences of each value in a specific column.\n\n    Parameters:\n    url (str): The url of the csv file to be downloaded.\n    column_name (str): The name of the column to count the values.\n\n    Returns:\n    dict: A dictionary with the values and their occurrences.\n\n    Requirements:\n    - urllib.request\n    - os\n    - csv\n    - collections\n\n    Example:\n    >>> download_csv_count_values('http://example.com/data.csv', 'category')\n    \"\"\"\n    urllib.request.urlretrieve(url, TARGET_CSV_FILE)\n\n    with open(TARGET_CSV_FILE, 'r') as f:\n        reader = csv.DictReader(f)\n        values = [row[column_name] for row in reader]\n\n    os.remove(TARGET_CSV_FILE)\n\n    return collections.Counter(values)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nurllib, os, json, pandas\n```python\nimport urllib.request\nimport os\nimport json\nimport pandas as pd\n\n# Constants\nTARGET_JSON_FILE = 'downloaded_file.json'\n\ndef download_json_to_dataframe(url):\n    \"\"\"\n    Download a json file from the specified url and convert it into a pandas DataFrame.\n\n    Parameters:\n    url (str): The url of the json file to be downloaded.\n\n    Returns:\n    DataFrame: A pandas DataFrame created from the downloaded json file.\n\n    Requirements:\n    - urllib.request\n    - os\n    - json\n    - pandas\n\n    Example:\n    >>> download_json_to_dataframe('http://example.com/data.json')\n    \"\"\"\n    urllib.request.urlretrieve(url, TARGET_JSON_FILE)\n\n    with open(TARGET_JSON_FILE, 'r') as f:\n        data = json.load(f)\n\n    os.remove(TARGET_JSON_FILE)\n\n    return pd.DataFrame(data)\n```"}
{"seed": "{\"task_id\": 22676, \"prompt\": \"def f_22676(url):\\n\\t\", \"suffix\": \"\\n\\treturn html\", \"canonical_solution\": \"html = urllib.request.urlopen(url).read()\", \"test_start\": \"\\nimport urllib \\n\\ndef check(candidate):\", \"test\": [\"\\n    html = candidate(\\\"https://github.com/zorazrw/multilingual-conala/blob/master/dataset/test/es_test.json\\\")\\n    assert b\\\"zorazrw/multilingual-conala\\\" in html\\n\"], \"entry_point\": \"f_22676\", \"intent\": \"download a file 'http://www.example.com/' over HTTP\", \"library\": [\"urllib\"]}", "generation": "Scenario 1: \nurllib, bs4, csv, os\n```python\nimport urllib.request\nfrom bs4 import BeautifulSoup\nimport csv\nimport os\n\n# Constants\nCSV_FILE_PATH = 'scraped_data.csv'\n\ndef scrape_and_save_data(url):\n    \"\"\"\n    Scrape data from a given URL and save the scraped data into a CSV file.\n\n    Parameters:\n    url (str): The URL to scrape data from.\n\n    Returns:\n    str: The path of the CSV file where the scraped data is saved.\n\n    Requirements:\n    - urllib.request\n    - bs4.BeautifulSoup\n    - csv\n    - os\n\n    Example:\n    >>> scrape_and_save_data('http://www.example.com/')\n    'scraped_data.csv'\n    \"\"\"\n    html = urllib.request.urlopen(url).read()\n    soup = BeautifulSoup(html, 'html.parser')\n\n    data = []\n    table = soup.find('table', attrs={'class':'data-table'})\n    table_rows = table.find_all('tr')\n\n    for tr in table_rows:\n        td = tr.find_all('td')\n        row = [tr.text for tr in td]\n        data.append(row)\n    \n    if os.path.exists(CSV_FILE_PATH):\n        os.remove(CSV_FILE_PATH)\n\n    with open(CSV_FILE_PATH, 'w') as f:\n        writer = csv.writer(f)\n        writer.writerows(data)\n    \n    return CSV_FILE_PATH\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nurllib, json, pandas, matplotlib\n```python\nimport urllib.request\nimport json\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef download_and_visualize_json(url):\n    \"\"\"\n    Download a JSON file from a given URL, convert it to a pandas DataFrame and plot a histogram\n    of a given column.\n\n    Parameters:\n    url (str): The URL to download the JSON file from.\n\n    Returns:\n    DataFrame: The pandas DataFrame converted from the JSON file.\n\n    Requirements:\n    - urllib.request\n    - json\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = download_and_visualize_json('http://www.example.com/data.json')\n    >>> print(df)\n    \"\"\"\n    with urllib.request.urlopen(url) as url:\n        data = json.loads(url.read().decode())\n        df = pd.DataFrame(data)\n        df.hist(column='target_column')\n        return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nurllib, lxml, requests, pandas\n```python\nimport urllib.request\nfrom lxml import etree\nimport requests\nimport pandas as pd\n\ndef download_and_parse_xml(url):\n    \"\"\"\n    Download an XML file from a given URL, parse it using lxml and convert it to a pandas DataFrame.\n\n    Parameters:\n    url (str): The URL to download the XML file from.\n\n    Returns:\n    DataFrame: The pandas DataFrame converted from the XML file.\n\n    Requirements:\n    - urllib.request\n    - lxml.etree\n    - requests\n    - pandas\n\n    Example:\n    >>> df = download_and_parse_xml('http://www.example.com/data.xml')\n    >>> print(df)\n    \"\"\"\n    with urllib.request.urlopen(url) as url:\n        xml_data = url.read()\n        xml_tree = etree.XML(xml_data)\n        data = []\n        for item in xml_tree.findall('item'):\n            data_item = {}\n            for child in item:\n                data_item[child.tag] = child.text\n            data.append(data_item)\n        df = pd.DataFrame(data)\n        return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nurllib, re, collections, matplotlib\n```python\nimport urllib.request\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef count_and_visualize_word_frequency(url):\n    \"\"\"\n    Download a text file from a given URL, count the frequency of each word and visualize the top 10 most common words.\n\n    Parameters:\n    url (str): The URL to download the text file from.\n\n    Returns:\n    Counter: A counter object of word frequencies.\n\n    Requirements:\n    - urllib.request\n    - re\n    - collections.Counter\n    - matplotlib.pyplot\n\n    Example:\n    >>> word_freq = count_and_visualize_word_frequency('http://www.example.com/data.txt')\n    >>> print(word_freq)\n    \"\"\"\n    with urllib.request.urlopen(url) as url:\n        text = url.read().decode()\n        words = re.findall(r'\\b\\w+\\b', text)\n        word_freq = Counter(words)\n        top_words = word_freq.most_common(10)\n        plt.bar([word[0] for word in top_words], [word[1] for word in top_words])\n        plt.show()\n        return word_freq\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nurllib, zipfile, os\n```python\nimport urllib.request\nimport zipfile\nimport os\n\n# Constants\nSAVE_PATH = 'downloaded_file.zip'\nEXTRACT_PATH = 'extracted_files'\n\ndef download_and_extract_zip(url):\n    \"\"\"\n    Download a ZIP file from a given URL and extract it to a local directory.\n\n    Parameters:\n    url (str): The URL to download the ZIP file from.\n\n    Returns:\n    str: The path of the directory where the ZIP file is extracted.\n\n    Requirements:\n    - urllib.request\n    - zipfile\n    - os\n\n    Example:\n    >>> extract_path = download_and_extract_zip('http://www.example.com/data.zip')\n    'extracted_files'\n    \"\"\"\n    urllib.request.urlretrieve(url, SAVE_PATH)\n    with zipfile.ZipFile(SAVE_PATH, 'r') as zip_ref:\n        zip_ref.extractall(EXTRACT_PATH)\n    os.remove(SAVE_PATH)\n    return EXTRACT_PATH\n```\n"}
{"seed": "{\"task_id\": 22676, \"prompt\": \"def f_22676(url):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"requests.get(url)\", \"test_start\": \"\\nimport requests \\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(\\\"https://github.com/\\\").url == \\\"https://github.com/\\\"\\n\", \"\\n    assert candidate(\\\"https://google.com/\\\").url == \\\"https://www.google.com/\\\"\\n\"], \"entry_point\": \"f_22676\", \"intent\": \"download a file `url` over HTTP\", \"library\": [\"requests\"]}", "generation": "Scenario 1:\nrequests, os, urllib, zipfile\n```python\nimport requests\nimport os\nimport urllib.parse\nfrom zipfile import ZipFile\n\nDOWNLOAD_PATH = '/path/to/download/directory/'\n\ndef download_and_extract_zip(url):\n    \"\"\"\n    Downloads a ZIP file from a given URL and extracts it.\n\n    Parameters:\n    url (str): The URL of the ZIP file.\n\n    Returns:\n    str: The path to the extracted folder.\n\n    Requirements:\n    - requests\n    - os\n    - urllib.parse\n    - zipfile\n\n    Example:\n    >>> download_and_extract_zip('https://example.com/file.zip')\n    \"\"\"\n    response = requests.get(url)\n    file_name = os.path.join(DOWNLOAD_PATH, urllib.parse.basename(url))\n    \n    with open(file_name, 'wb') as f:\n        f.write(response.content)\n    \n    with ZipFile(file_name, 'r') as zip_ref:\n        zip_ref.extractall(DOWNLOAD_PATH)\n    \n    return os.path.splitext(file_name)[0]\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nrequests, json, pandas\n```python\nimport requests\nimport json\nimport pandas as pd\n\ndef fetch_and_convert_to_dataframe(url):\n    \"\"\"\n    Fetches a JSON from a given URL and converts it into a pandas DataFrame.\n\n    Parameters:\n    url (str): The URL of the JSON file.\n\n    Returns:\n    DataFrame: The pandas DataFrame.\n\n    Requirements:\n    - requests\n    - json\n    - pandas\n\n    Example:\n    >>> fetch_and_convert_to_dataframe('https://example.com/data.json')\n    \"\"\"\n    response = requests.get(url)\n    data = json.loads(response.text)\n    df = pd.DataFrame(data)\n    \n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nrequests, bs4, pandas\n```python\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\ndef scrape_table_to_dataframe(url, table_id):\n    \"\"\"\n    Scrapes a HTML table from a given URL and converts it into a pandas DataFrame.\n\n    Parameters:\n    url (str): The URL of the webpage.\n    table_id (str): The id of the table in the webpage.\n\n    Returns:\n    DataFrame: The pandas DataFrame.\n\n    Requirements:\n    - requests\n    - bs4.BeautifulSoup\n    - pandas\n\n    Example:\n    >>> scrape_table_to_dataframe('https://example.com/data.html', 'table1')\n    \"\"\"\n    response = requests.get(url)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    table = soup.find('table', {'id': table_id})\n    df = pd.read_html(str(table))[0]\n    \n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nrequests, lxml.etree, csv\n```python\nimport requests\nimport lxml.etree as ET\nimport csv\n\ndef download_and_parse_xml(url, output_csv_path):\n    \"\"\"\n    Downloads an XML file from a given URL and parses it into a CSV file.\n\n    Parameters:\n    url (str): The URL of the XML file.\n    output_csv_path (str): The path to save the CSV file.\n\n    Returns:\n    None\n\n    Requirements:\n    - requests\n    - lxml.etree\n    - csv\n\n    Example:\n    >>> download_and_parse_xml('https://example.com/data.xml', '/path/to/output.csv')\n    \"\"\"\n    response = requests.get(url)\n    root = ET.fromstring(response.content)\n    data = [[elem.tag, elem.text] for elem in root.iter()]\n\n    with open(output_csv_path, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerows(data)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nrequests, PIL.Image, io\n```python\nimport requests\nfrom PIL import Image\nimport io\n\ndef download_and_open_image(url):\n    \"\"\"\n    Downloads an image from a given URL and opens it using PIL.\n\n    Parameters:\n    url (str): The URL of the image.\n\n    Returns:\n    Image: The PIL Image object.\n\n    Requirements:\n    - requests\n    - PIL.Image\n    - io\n\n    Example:\n    >>> download_and_open_image('https://example.com/image.jpg')\n    \"\"\"\n    response = requests.get(url)\n    image = Image.open(io.BytesIO(response.content))\n    \n    return image\n```"}
{"seed": "{\"task_id\": 22676, \"prompt\": \"def f_22676(url):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"\\n\\tresponse = requests.get(url, stream=True)\\n\\twith open('10MB', 'wb') as handle:\\n\\t\\tfor data in response.iter_content():\\n\\t\\t\\thandle.write(data)\\n\\t\", \"test_start\": \"\\nimport requests \\n\\ndef check(candidate):\", \"test\": [\"\\n    candidate(\\\"https://github.com/\\\")\\n    with open(\\\"10MB\\\", 'rb') as fr: \\n        all_data = [data for data in fr]\\n    assert all_data[: 2] == [b'\\\\n', b'\\\\n']\\n\"], \"entry_point\": \"f_22676\", \"intent\": \"download a file `url` over HTTP and save to \\\"10MB\\\"\", \"library\": [\"requests\"]}", "generation": "Scenario 3:\nrequests, os, shutil, zipfile\n```python\nimport requests\nimport os\nimport shutil\nimport zipfile\n\n# Constants\nDOWNLOAD_DIR = 'downloads'\nZIP_DIR = 'unzipped_files'\n\ndef download_and_unzip(url, filename):\n    \"\"\"\n    Download a zip file from a given URL and save it to a specified filename. Then, unzip the file \n    and save the contents to a directory.\n    \n    Parameters:\n    url (str): The URL of the zip file.\n    filename (str): The filename to save the downloaded zip file.\n    \n    Returns:\n    list: A list of filenames in the unzipped directory.\n\n    Requirements:\n    - requests\n    - os\n    - shutil\n    - zipfile\n    \n    Example:\n    >>> download_and_unzip('http://example.com/myfile.zip', 'myfile.zip')\n    \"\"\"\n    # Download the file\n    response = requests.get(url, stream=True)\n    filepath = os.path.join(DOWNLOAD_DIR, filename)\n    with open(filepath, 'wb') as handle:\n        for data in response.iter_content():\n            handle.write(data)\n\n    # Unzip the file\n    zip_dir = os.path.join(ZIP_DIR, filename[:-4])  # Remove .zip extension\n    os.makedirs(zip_dir, exist_ok=True)\n    with zipfile.ZipFile(filepath, 'r') as zip_ref:\n        zip_ref.extractall(zip_dir)\n\n    # Return the list of filenames in the unzipped directory\n    return os.listdir(zip_dir)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nrequests, urllib.parse, bs4, csv\n```python\nimport requests\nfrom urllib.parse import urljoin\nfrom bs4 import BeautifulSoup\nimport csv\n\n# Constants\nBASE_URL = 'https://www.example.com'\nCSV_FILE = 'scraped_data.csv'\n\ndef scrape_and_save(url):\n    \"\"\"\n    Scrape a webpage, extract links from it, and save the links to a CSV file.\n    \n    Parameters:\n    url (str): The relative URL of the webpage to scrape.\n    \n    Returns:\n    int: The number of links scraped.\n\n    Requirements:\n    - requests\n    - urllib.parse\n    - bs4 (BeautifulSoup)\n    - csv\n    \n    Example:\n    >>> scrape_and_save('/mywebpage')\n    \"\"\"\n    # Get the webpage\n    response = requests.get(urljoin(BASE_URL, url))\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    # Scrape the links\n    links = [a['href'] for a in soup.find_all('a', href=True)]\n\n    # Save the links to a CSV file\n    with open(CSV_FILE, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        for link in links:\n            writer.writerow([link])\n\n    return len(links)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nrequests, json, pandas, matplotlib\n```python\nimport requests\nimport json\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nAPI_URL = 'https://api.example.com/data'\n\ndef get_data_and_plot():\n    \"\"\"\n    Send a GET request to an API, parse the JSON response, and plot the data using matplotlib.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the parsed data.\n\n    Requirements:\n    - requests\n    - json\n    - pandas\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = get_data_and_plot()\n    >>> df.plot()\n    \"\"\"\n    # Send the GET request\n    response = requests.get(API_URL)\n\n    # Parse the JSON response\n    data = json.loads(response.text)\n\n    # Convert the data to a pandas DataFrame\n    df = pd.DataFrame(data)\n\n    # Plot the data\n    df.plot()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 6:\nrequests, lxml, pandas, sqlite3\n```python\nimport requests\nfrom lxml import html\nimport pandas as pd\nimport sqlite3\n\n# Constants\nWEBPAGE_URL = 'https://www.example.com'\n\ndef scrape_webpage_and_save_to_db():\n    \"\"\"\n    Scrape a webpage, convert the scraped data to a pandas DataFrame, and save the DataFrame to a SQLite database.\n    \n    Returns:\n    int: The number of rows saved to the database.\n\n    Requirements:\n    - requests\n    - lxml\n    - pandas\n    - sqlite3\n    \n    Example:\n    >>> scrape_webpage_and_save_to_db()\n    \"\"\"\n    # Scrape the webpage\n    response = requests.get(WEBPAGE_URL)\n    tree = html.fromstring(response.content)\n\n    # Extract the data and convert it to a DataFrame\n    data = tree.xpath('//tr')\n    df = pd.DataFrame(data)\n\n    # Save the DataFrame to a SQLite database\n    conn = sqlite3.connect('my_database.db')\n    df.to_sql('my_table', conn, if_exists='replace', index=False)\n\n    return len(df)\n```\n`GPT_ODEX_BREAK`\n\nScenario 7:\nrequests, PIL, numpy, matplotlib\n```python\nimport requests\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nIMAGE_URL = 'https://www.example.com/myimage.jpg'\n\ndef download_image_and_display_histogram(url):\n    \"\"\"\n    Download an image from a URL, convert it to a grayscale numpy array, and display its histogram using matplotlib.\n\n    Parameters:\n    url (str): The URL of the image.\n\n    Requirements:\n    - requests\n    - PIL (Pillow)\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> download_image_and_display_histogram(IMAGE_URL)\n    \"\"\"\n    # Download the image\n    response = requests.get(url, stream=True)\n    img = Image.open(response.raw).convert('L')\n\n    # Convert the image to a numpy array\n    img_array = np.array(img)\n\n    # Display the histogram\n    plt.hist(img_array.ravel(), bins=256, color='gray', alpha=0.7)\n    plt.title('Grayscale Histogram')\n    plt.show()\n```\n`GPT_ODEX_BREAK`"}
{"seed": "{\"task_id\": 15405636, \"prompt\": \"def f_15405636(parser):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"parser.add_argument('--version', action='version', version='%(prog)s 2.0')\", \"test_start\": \"\\nimport argparse \\n\\ndef check(candidate):\", \"test\": [\"\\n    parser = argparse.ArgumentParser()\\n    output = candidate(parser)\\n    assert output.option_strings == ['--version']\\n    assert output.dest == 'version'\\n    assert output.nargs == 0\\n\"], \"entry_point\": \"f_15405636\", \"intent\": \"argparse add argument with flag '--version' and version action of '%(prog)s 2.0' to parser `parser`\", \"library\": [\"argparse\"]}", "generation": "Scenario 1:\nargparse, os, sys, json\n```python\nimport argparse\nimport os\nimport sys\nimport json\n\ndef create_config_parser():\n    \"\"\"\n    Create a command line argument parser that takes in a configuration file path,\n    reads the JSON configuration file, and returns the configuration dictionary.\n    \n    Requirements:\n    - argparse\n    - os\n    - sys\n    - json\n\n    Example:\n    ```bash\n    $ python script.py --config config.json\n    ```\n    \"\"\"\n    parser = argparse.ArgumentParser(description='Process configuration file.')\n    parser.add_argument('--config', type=str, help='Path to the configuration file.')\n    args = parser.parse_args()\n\n    if not os.path.isfile(args.config):\n        print('The configuration file does not exist.')\n        sys.exit(1)\n\n    with open(args.config) as f:\n        config = json.load(f)\n\n    return config\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nargparse, logging, shutil\n```python\nimport argparse\nimport logging\nimport shutil\n\ndef setup_logging():\n    \"\"\"\n    Set up logging based on command line arguments. The function should set up a logging file\n    if the --log argument is passed, otherwise it should print logs to the console.\n\n    Requirements:\n    - argparse\n    - logging\n    - shutil\n\n    Example:\n    ```bash\n    $ python script.py --log log.txt\n    ```\n    \"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--log', type=str, help='Path to the log file.')\n    args = parser.parse_args()\n\n    if args.log:\n        logging.basicConfig(filename=args.log, level=logging.INFO)\n    else:\n        logging.basicConfig(level=logging.INFO)\n\n    return logging\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nargparse, random, matplotlib.pyplot\n```python\nimport argparse\nimport random\nimport matplotlib.pyplot as plt\n\ndef generate_random_graph():\n    \"\"\"\n    Generate a graph of random numbers based on command line arguments. The function should take \n    in the number of points as an argument and generate a graph of that many random points.\n\n    Requirements:\n    - argparse\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    ```bash\n    $ python script.py --points 100\n    ```\n    \"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--points', type=int, help='Number of points to generate.')\n    args = parser.parse_args()\n\n    x = list(range(args.points))\n    y = [random.random() for _ in range(args.points)]\n\n    plt.plot(x, y)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nargparse, sqlite3, pandas\n```python\nimport argparse\nimport sqlite3\nimport pandas as pd\n\ndef query_database():\n    \"\"\"\n    Query a SQLite database based on command line arguments. The function should take in a \n    database file and a SQL query as arguments, execute the query, and print the results.\n\n    Requirements:\n    - argparse\n    - sqlite3\n    - pandas\n\n    Example:\n    ```bash\n    $ python script.py --db database.db --query \"SELECT * FROM table\"\n    ```\n    \"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--db', type=str, help='Path to the SQLite database file.')\n    parser.add_argument('--query', type=str, help='SQL query to execute.')\n    args = parser.parse_args()\n\n    conn = sqlite3.connect(args.db)\n    df = pd.read_sql_query(args.query, conn)\n\n    print(df)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nargparse, requests, bs4, csv\n```python\nimport argparse\nimport requests\nfrom bs4 import BeautifulSoup\nimport csv\n\ndef scrape_website():\n    \"\"\"\n    Scrape a website based on command line arguments. The function should take in a URL and a \n    CSS selector, scrape the website, and write the results to a CSV file.\n\n    Requirements:\n    - argparse\n    - requests\n    - bs4\n    - csv\n\n    Example:\n    ```bash\n    $ python script.py --url \"https://example.com\" --selector \".class\" --output output.csv\n    ```\n    \"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--url', type=str, help='URL to scrape.')\n    parser.add_argument('--selector', type=str, help='CSS selector to use.')\n    parser.add_argument('--output', type=str, help='Path to the output CSV file.')\n    args = parser.parse_args()\n\n    response = requests.get(args.url)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    elements = soup.select(args.selector)\n\n    with open(args.output, 'w', newline='') as f:\n        writer = csv.writer(f)\n        for element in elements:\n            writer.writerow([element.text])\n```\n"}
{"seed": "{\"task_id\": 17665809, \"prompt\": \"def f_17665809(d):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"{i: d[i] for i in d if i != 'c'}\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate({'a': 1 , 'b': 2, 'c': 3}) == {'a': 1 , 'b': 2}\\n\", \"\\n    assert candidate({'c': None}) == {}\\n\", \"\\n    assert candidate({'a': 1 , 'b': 2, 'c': 3}) != {'a': 1 , 'b': 2, 'c': 3}\\n\", \"\\n    assert candidate({'c': 1, 'cc': 2, 'ccc':3}) == {'cc': 2, 'ccc':3}\\n\", \"\\n    assert 'c' not in candidate({'c':i for i in range(10)})\\n\"], \"entry_point\": \"f_17665809\", \"intent\": \"remove key 'c' from dictionary `d`\", \"library\": []}", "generation": "Scenario 1: \npandas, random, json, os\n```python\nimport pandas as pd\nimport random\nimport json\nimport os\n\n# Constants\nFILE_PATH = './data.json'\n\ndef remove_key_and_save(df):\n    \"\"\"\n    Remove a column 'c' from a DataFrame, convert it to a dictionary and save to a JSON file.\n\n    Parameters:\n    df (DataFrame): The input DataFrame.\n\n    Returns:\n    str: The path of the saved file.\n    \n    Requirements:\n    - pandas\n    - random\n    - json\n    - os\n\n    Example:\n    >>> df = pd.DataFrame({'a': [1, 2], 'b': [3, 4], 'c': [5, 6]})\n    >>> remove_key_and_save(df)\n    './data.json'\n    \"\"\"\n    if 'c' in df.columns:\n        df = df.drop(columns='c')\n    \n    data_dict = df.to_dict()\n\n    with open(FILE_PATH, 'w') as f:\n        json.dump(data_dict, f)\n\n    return FILE_PATH\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, matplotlib, numpy\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nCOLUMN_TO_REMOVE = 'c'\n\ndef plot_after_removal(df):\n    \"\"\"\n    Remove a column 'c' from DataFrame and plot the remaining DataFrame.\n\n    Parameters:\n    df (DataFrame): The input DataFrame.\n\n    Returns:\n    None\n    \n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - numpy\n\n    Example:\n    >>> df = pd.DataFrame({'a': np.random.randn(100), 'b': np.random.randn(100), 'c': np.random.randn(100)})\n    >>> plot_after_removal(df)\n    \"\"\"\n    if COLUMN_TO_REMOVE in df.columns:\n        df = df.drop(columns=COLUMN_TO_REMOVE)\n    \n    df.plot()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, seaborn, matplotlib\n```python\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMN_TO_REMOVE = 'c'\n\ndef heatmap_after_removal(df):\n    \"\"\"\n    Remove a column 'c' from DataFrame and create a heatmap of the correlation matrix of the remaining DataFrame.\n\n    Parameters:\n    df (DataFrame): The input DataFrame.\n\n    Returns:\n    None\n    \n    Requirements:\n    - pandas\n    - seaborn\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6], 'c': [7, 8, 9]})\n    >>> heatmap_after_removal(df)\n    \"\"\"\n    if COLUMN_TO_REMOVE in df.columns:\n        df = df.drop(columns=COLUMN_TO_REMOVE)\n    \n    corr = df.corr()\n    sns.heatmap(corr, annot=True)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, sklearn.preprocessing\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\n\n# Constants\nCOLUMN_TO_REMOVE = 'c'\n\ndef scale_after_removal(df):\n    \"\"\"\n    Remove a column 'c' from DataFrame and scale the remaining DataFrame using StandardScaler from sklearn.\n\n    Parameters:\n    df (DataFrame): The input DataFrame.\n\n    Returns:\n    DataFrame: The scaled DataFrame.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.preprocessing.StandardScaler\n\n    Example:\n    >>> df = pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6], 'c': [7, 8, 9]})\n    >>> scale_after_removal(df)\n    \"\"\"\n    if COLUMN_TO_REMOVE in df.columns:\n        df = df.drop(columns=COLUMN_TO_REMOVE)\n    \n    scaler = StandardScaler()\n    scaled_df = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n\n    return scaled_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, sklearn.model_selection\n```python\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Constants\nCOLUMN_TO_REMOVE = 'c'\n\ndef split_after_removal(df, target_column):\n    \"\"\"\n    Remove a column 'c' from DataFrame and split the remaining DataFrame into training set and test set.\n\n    Parameters:\n    df (DataFrame): The input DataFrame.\n    target_column (str): The target column for train_test_split.\n\n    Returns:\n    tuple: The training set and test set.\n    \n    Requirements:\n    - pandas\n    - sklearn.model_selection.train_test_split\n\n    Example:\n    >>> df = pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6], 'c': [7, 8, 9]})\n    >>> split_after_removal(df, 'b')\n    \"\"\"\n    if COLUMN_TO_REMOVE in df.columns:\n        df = df.drop(columns=COLUMN_TO_REMOVE)\n    \n    X_train, X_test, y_train, y_test = train_test_split(df.drop(columns=target_column), df[target_column], test_size=0.2)\n\n    return X_train, X_test, y_train, y_test\n```"}
{"seed": "{\"task_id\": 41861705, \"prompt\": \"def f_41861705(split_df, csv_df):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"pd.merge(split_df, csv_df, on=['key'], suffixes=('_left', '_right'))\", \"test_start\": \"\\nimport pandas as pd\\n\\ndef check(candidate):\", \"test\": [\"\\n    split_df = pd.DataFrame({'key': ['foo', 'bar'], 'value': [1, 2]})\\n    csv_df = pd.DataFrame({'key': ['foo', 'baz'], 'value': [3, 4]})\\n    result = pd.DataFrame({'key': ['foo'], 'value_left': [1],'value_right': [3]})\\n    assert all(candidate(csv_df, split_df) == result)\\n\"], \"entry_point\": \"f_41861705\", \"intent\": \"Create new DataFrame object by merging columns \\\"key\\\" of  dataframes `split_df` and `csv_df` and rename the columns from dataframes `split_df` and `csv_df` with suffix `_left` and `_right` respectively\", \"library\": [\"pandas\"]}", "generation": "Scenario 1: \npandas, numpy, sklearn, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\n\n# Constants\nFEATURES = ['feature1', 'feature2', 'feature3']\nTARGET = 'target'\n\ndef linear_regression_analysis(df1, df2):\n    \"\"\"\n    Perform linear regression analysis using specified features and target. \n    The function should merge two dataframes on the 'id' column, perform \n    linear regression, and plot the residuals.\n\n    Parameters:\n    df1 (DataFrame): The first dataframe.\n    df2 (DataFrame): The second dataframe.\n\n    Returns:\n    dict: A dictionary containing the regression coefficients and intercept.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.linear_model.LinearRegression\n    - matplotlib.pyplot\n\n    Example:\n    >>> df1 = pd.DataFrame({'id': [1, 2, 3], 'feature1': [1.2, 3.4, 5.6], 'feature2': [2.3, 4.5, 6.7], 'feature3': [3.4, 5.6, 7.8]})\n    >>> df2 = pd.DataFrame({'id': [1, 2, 3], 'target': [4.5, 6.7, 8.9]})\n    >>> linear_regression_analysis(df1, df2)\n    \"\"\"\n    df = pd.merge(df1, df2, on='id')\n\n    X = df[FEATURES]\n    y = df[TARGET]\n\n    model = LinearRegression()\n    model.fit(X, y)\n\n    y_pred = model.predict(X)\n    residuals = y - y_pred\n\n    plt.scatter(y_pred, residuals)\n    plt.xlabel('Predicted Values')\n    plt.ylabel('Residuals')\n    plt.show()\n\n    return {'coefficients': model.coef_, 'intercept': model.intercept_}\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \npandas, seaborn, matplotlib, sklearn\n```python\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\n# Constants\nFEATURES = ['feature1', 'feature2', 'feature3']\n\ndef scale_and_plot(df1, df2):\n    \"\"\"\n    Scale the features of the merged dataframe and plot a pairplot of \n    the scaled features.\n\n    Parameters:\n    df1 (DataFrame): The first dataframe.\n    df2 (DataFrame): The second dataframe.\n\n    Returns:\n    DataFrame: A dataframe of the scaled features.\n    \n    Requirements:\n    - pandas\n    - seaborn\n    - matplotlib.pyplot\n    - sklearn.preprocessing.StandardScaler\n\n    Example:\n    >>> df1 = pd.DataFrame({'id': [1, 2, 3], 'feature1': [1.2, 3.4, 5.6], 'feature2': [2.3, 4.5, 6.7]})\n    >>> df2 = pd.DataFrame({'id': [1, 2, 3], 'feature3': [3.4, 5.6, 7.8]})\n    >>> scale_and_plot(df1, df2)\n    \"\"\"\n    df = pd.merge(df1, df2, on='id')\n\n    scaler = StandardScaler()\n    df_scaled = pd.DataFrame(scaler.fit_transform(df[FEATURES]), columns=FEATURES)\n\n    sns.pairplot(df_scaled)\n    plt.show()\n\n    return df_scaled\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \npandas, numpy, sklearn, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_selection import SelectKBest, f_classif\nimport seaborn as sns\n\n# Constants\nFEATURES = ['feature1', 'feature2', 'feature3']\nTARGET = 'target'\n\ndef feature_selection(df1, df2):\n    \"\"\"\n    Perform feature selection using SelectKBest and plot a heatmap of\n    the feature correlations.\n\n    Parameters:\n    df1 (DataFrame): The first dataframe.\n    df2 (DataFrame): The second dataframe.\n\n    Returns:\n    list: A list of the selected features.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.feature_selection.SelectKBest\n    - sklearn.feature_selection.f_classif\n    - seaborn\n\n    Example:\n    >>> df1 = pd.DataFrame({'id': [1, 2, 3], 'feature1': [1.2, 3.4, 5.6], 'feature2': [2.3, 4.5, 6.7], 'feature3': [3.4, 5.6, 7.8]})\n    >>> df2 = pd.DataFrame({'id': [1, 2, 3], 'target': [4.5, 6.7, 8.9]})\n    >>> feature_selection(df1, df2)\n    \"\"\"\n    df = pd.merge(df1, df2, on='id')\n\n    X = df[FEATURES]\n    y = df[TARGET]\n\n    selector = SelectKBest(f_classif, k=2)\n    X_new = selector.fit_transform(X, y)\n\n    selected_features = [x for x, y in zip(FEATURES, selector.get_support()) if y]\n\n    sns.heatmap(X_new.corr(), annot=True)\n    plt.show()\n\n    return selected_features\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \npandas, numpy, sklearn, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\n# Constants\nFEATURES = ['feature1', 'feature2']\n\ndef kmeans_clustering(df1, df2):\n    \"\"\"\n    Perform KMeans clustering on the merged dataframe and plot the clusters.\n\n    Parameters:\n    df1 (DataFrame): The first dataframe.\n    df2 (DataFrame): The second dataframe.\n\n    Returns:\n    ndarray: The labels of the clusters.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.cluster.KMeans\n    - matplotlib.pyplot\n\n    Example:\n    >>> df1 = pd.DataFrame({'id': [1, 2, 3], 'feature1': [1.2, 3.4, 5.6]})\n    >>> df2 = pd.DataFrame({'id': [1, 2, 3], 'feature2': [2.3, 4.5, 6.7]})\n    >>> kmeans_clustering(df1, df2)\n    \"\"\"\n    df = pd.merge(df1, df2, on='id')\n\n    X = df[FEATURES]\n\n    kmeans = KMeans(n_clusters=2)\n    kmeans.fit(X)\n\n    plt.scatter(X[FEATURES[0]], X[FEATURES[1]], c=kmeans.labels_)\n    plt.show()\n\n    return kmeans.labels_\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \npandas, seaborn, matplotlib, scipy\n```python\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import chi2_contingency\n\n# Constants\nFEATURES = ['feature1', 'feature2']\n\ndef chi_squared_test(df1, df2):\n    \"\"\"\n    Perform a Chi-Squared test of independence on the merged dataframe and\n    plot a heatmap of the feature counts.\n\n    Parameters:\n    df1 (DataFrame): The first dataframe.\n    df2 (DataFrame): The second dataframe.\n\n    Returns:\n    float: The p-value of the Chi-Squared test.\n    \n    Requirements:\n    - pandas\n    - seaborn\n    - matplotlib.pyplot\n    - scipy.stats.chi2_contingency\n\n    Example:\n    >>> df1 = pd.DataFrame({'id': [1, 2, 3], 'feature1': ['A', 'B', 'A']})\n    >>> df2 = pd.DataFrame({'id': [1, 2, 3], 'feature2': ['X', 'Y', 'X']})\n    >>> chi_squared_test(df1, df2)\n    \"\"\"\n    df = pd.merge(df1, df2, on='id')\n\n    contingency_table = pd.crosstab(df[FEATURES[0]], df[FEATURES[1]])\n\n    sns.heatmap(contingency_table, annot=True)\n    plt.show()\n\n    chi2, p, dof, expected = chi2_contingency(contingency_table)\n\n    return p\n```"}
{"seed": "{\"task_id\": 10697757, \"prompt\": \"def f_10697757(s):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"s.split(' ', 4)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('1 0 A10B 100 Description: This is a description with spaces') ==         ['1', '0', 'A10B', '100', 'Description: This is a description with spaces']\\n\", \"\\n    assert candidate('this-is-a-continuous-sequence') == ['this-is-a-continuous-sequence']\\n\", \"\\n    assert candidate('') == ['']\\n\", \"\\n    assert candidate('\\t') == ['\\t']\\n\"], \"entry_point\": \"f_10697757\", \"intent\": \"Split a string `s` by space with `4` splits\", \"library\": []}", "generation": "Scenario 1: \npandas, re, matplotlib, numpy, random\n```python\nimport pandas as pd\nimport re\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom random import choice\n\n# Constants\nPRODUCTS = ['Apple', 'Banana', 'Orange', 'Pear', 'Grape']\nDESCRIPTIONS = ['This is a fresh and organic product',\n                'Taste sweet and delicious',\n                'High in vitamins and nutrients',\n                'Perfect for a healthy diet',\n                'Buy and enjoy the natural taste']\n\ndef generate_product_df(s):\n    \"\"\"\n    Generate a pandas DataFrame of products with their id, quantity, price and description \n    based on the provided string of product data. The string is split into 4 parts: id, quantity, \n    price and description.\n\n    Parameters:\n    s (str): The string of product data.\n\n    Returns:\n    DataFrame: A pandas DataFrame with product data.\n\n    Requirements:\n    - pandas\n    - re\n    - matplotlib.pyplot\n    - numpy\n    - random\n\n    Example:\n    >>> s = '1 10 A10B 100 Description: This is a description with spaces'\n    >>> df = generate_product_df(s)\n    >>> print(df)\n    >>> df['Price'].plot(kind='bar')\n    \"\"\"\n    data_list = re.split(r'\\s+', s, 4)\n    product = choice(PRODUCTS)\n    description = choice(DESCRIPTIONS)\n    data_list.append(product)\n    data_list.append(description)\n    data_df = pd.DataFrame([data_list], columns=['ID', 'Quantity', 'Code', 'Price', 'Raw Description', 'Product', 'Description'])\n    return data_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \npandas, re, numpy, random\n```python\nimport pandas as pd\nimport re\nimport numpy as np\nfrom random import randint\n\n# Constants\nEMPLOYEES = ['John', 'Alice', 'Bob', 'Charlie', 'David']\nJOBS = ['Engineer', 'Manager', 'Analyst', 'Developer', 'Tester']\n\ndef generate_employee_df(s):\n    \"\"\"\n    Generate a pandas DataFrame of employees with their id, age, salary, job title and bio \n    based on the provided string of employee data. The string is split into 4 parts: id, age, \n    salary and bio.\n\n    Parameters:\n    s (str): The string of employee data.\n\n    Returns:\n    DataFrame: A pandas DataFrame with employee data.\n\n    Requirements:\n    - pandas\n    - re\n    - numpy\n    - random\n\n    Example:\n    >>> s = '1 30 A10B 5000 Bio: This is a bio with spaces'\n    >>> df = generate_employee_df(s)\n    >>> print(df)\n    >>> df['Age'].plot(kind='hist')\n    \"\"\"\n    data_list = re.split(r'\\s+', s, 4)\n    employee = EMPLOYEES[randint(0, len(EMPLOYEES)-1)]\n    job = JOBS[randint(0, len(JOBS)-1)]\n    data_list[0] = employee\n    data_list.append(job)\n    data_df = pd.DataFrame([data_list], columns=['Name', 'Age', 'Code', 'Salary', 'Raw Bio', 'Job Title'])\n    return data_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nre, nltk, collections, random\n```python\nimport re\nfrom nltk.corpus import words\nfrom collections import Counter\nfrom random import sample\n\n# Constants\nENGLISH_WORDS = set(words.words())\n\ndef extract_english_words(s, n):\n    \"\"\"\n    Extract n different English words from a string. The string is split into words and only\n    those that are English are kept. If there are less than n English words, all are returned.\n\n    Parameters:\n    s (str): The string to extract words from.\n    n (int): The number of different English words to extract.\n\n    Returns:\n    list: A list with n different English words.\n\n    Requirements:\n    - re\n    - nltk.corpus\n    - collections\n    - random\n\n    Example:\n    >>> s = 'This is a test string with some random words: apple, banana, test, hello, world'\n    >>> extract_english_words(s, 5)\n    \"\"\"\n    word_list = re.findall(r'\\b\\w+\\b', s)\n    english_words = [word for word in word_list if word in ENGLISH_WORDS]\n    if len(english_words) < n:\n        return english_words\n    else:\n        return sample(english_words, n)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nre, matplotlib, random, string\n```python\nimport re\nimport matplotlib.pyplot as plt\nimport random\nimport string\n\n# Constants\nLETTERS = string.ascii_lowercase\n\ndef letter_frequency(s):\n    \"\"\"\n    Calculate the frequency of each letter in a string and plot a bar chart of the frequencies.\n\n    Parameters:\n    s (str): The string to calculate letter frequencies.\n\n    Returns:\n    dict: A dictionary with the frequency of each letter.\n\n    Requirements:\n    - re\n    - matplotlib.pyplot\n    - random\n    - string\n\n    Example:\n    >>> s = 'This is a test string.'\n    >>> freqs = letter_frequency(s)\n    >>> print(freqs)\n    >>> plt.bar(freqs.keys(), freqs.values())\n    \"\"\"\n    s = s.lower()\n    letter_counts = {letter: s.count(letter) for letter in LETTERS}\n    return letter_counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nre, sklearn, numpy, random\n```python\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\nfrom random import choice\n\n# Constants\nSENTENCES = ['This is a sentence', 'Another sentence here', 'More sentences']\n\ndef text_vectorization(s):\n    \"\"\"\n    Vectorize a string using the bag-of-words model. The string is split into words and each \n    word is considered a feature. The value of each feature is the count of the word in the string.\n\n    Parameters:\n    s (str): The string to vectorize.\n\n    Returns:\n    array: A numpy array with the vectorized string.\n\n    Requirements:\n    - re\n    - sklearn.feature_extraction.text\n    - numpy\n    - random\n\n    Example:\n    >>> s = 'This is a test string.'\n    >>> vec = text_vectorization(s)\n    >>> print(vec)\n    \"\"\"\n    s = re.sub(r'\\W+', ' ', s)\n    vectorizer = CountVectorizer()\n    X = vectorizer.fit_transform([s] + SENTENCES)\n    return X.toarray()[0]\n```\n`GPT_ODEX_BREAK`"}
{"seed": "{\"task_id\": 16344756, \"prompt\": \"def f_16344756(app):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"app.run(debug=True)\", \"test_start\": \"\\nfrom flask import Flask\\nfrom unittest.mock import Mock\\n\\ndef check(candidate):\", \"test\": [\"\\n    Flask = Mock()\\n    app = Flask('mai')\\n    try:\\n        candidate(app)\\n    except:\\n        return False\\n\"], \"entry_point\": \"f_16344756\", \"intent\": \"enable debug mode on Flask application `app`\", \"library\": [\"flask\"]}", "generation": "Scenario 1: \nflask, os, logging, werkzeug\n```python\nfrom flask import Flask, request\nimport os\nimport logging\nfrom werkzeug.exceptions import HTTPException\n\nLOGGING_FORMAT = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\nLOGGING_LEVEL = logging.DEBUG\nLOGGING_FILE = 'app.log'\n\ndef create_app(debug_mode):\n    \"\"\"\n    Create a Flask application with error handlers, logging and debug mode.\n\n    Parameters:\n    debug_mode (bool): If True, the Flask application will run in debug mode.\n\n    Returns:\n    Flask: The created Flask application.\n\n    Requirements:\n    - flask\n    - os\n    - logging\n    - werkzeug.exceptions\n\n    Example:\n    >>> app = create_app(True)\n    >>> app.run()\n    \"\"\"\n    app = Flask(__name__)\n    app.debug = debug_mode\n\n    # Setup logging\n    if not app.debug:\n        logging.basicConfig(filename=LOGGING_FILE, level=LOGGING_LEVEL, format=LOGGING_FORMAT)\n\n    # Error handlers\n    @app.errorhandler(HTTPException)\n    def handle_exception(e):\n        response = e.get_response()\n        logging.error(f\"{request.path} - {e.description}\")\n        return response\n\n    return app\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nflask, flask_sqlalchemy, flask_migrate, os\n```python\nfrom flask import Flask\nfrom flask_sqlalchemy import SQLAlchemy\nfrom flask_migrate import Migrate\nimport os\n\nBASEDIR = os.path.abspath(os.path.dirname(__file__))\nSQLALCHEMY_DATABASE_URI = 'sqlite:///' + os.path.join(BASEDIR, 'app.db')\n\ndef create_db_app(debug_mode):\n    \"\"\"\n    Create a Flask application with SQLAlchemy database and database migration support.\n\n    Parameters:\n    debug_mode (bool): If True, the Flask application will run in debug mode.\n\n    Returns:\n    Flask: The created Flask application.\n\n    Requirements:\n    - flask\n    - flask_sqlalchemy\n    - flask_migrate\n    - os\n\n    Example:\n    >>> app = create_db_app(True)\n    >>> app.run()\n    \"\"\"\n    app = Flask(__name__)\n    app.config['SQLALCHEMY_DATABASE_URI'] = SQLALCHEMY_DATABASE_URI\n    app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False\n    app.debug = debug_mode\n\n    db = SQLAlchemy(app)\n    migrate = Migrate(app, db)\n\n    return app\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nflask, flask_restful, flask_jwt_extended, flask_sqlalchemy\n```python\nfrom flask import Flask\nfrom flask_restful import Api\nfrom flask_jwt_extended import JWTManager\nfrom flask_sqlalchemy import SQLAlchemy\n\nSQLALCHEMY_DATABASE_URI = 'sqlite:///app.db'\n\ndef create_api_app(debug_mode):\n    \"\"\"\n    Create a Flask application with RESTful API, JWT authentication and SQLAlchemy database support.\n\n    Parameters:\n    debug_mode (bool): If True, the Flask application will run in debug mode.\n\n    Returns:\n    Flask: The created Flask application.\n\n    Requirements:\n    - flask\n    - flask_restful\n    - flask_jwt_extended\n    - flask_sqlalchemy\n\n    Example:\n    >>> app = create_api_app(True)\n    >>> app.run()\n    \"\"\"\n    app = Flask(__name__)\n    app.config['SQLALCHEMY_DATABASE_URI'] = SQLALCHEMY_DATABASE_URI\n    app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False\n    app.debug = debug_mode\n\n    api = Api(app)\n    jwt = JWTManager(app)\n    db = SQLAlchemy(app)\n\n    return app\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nflask, flask_socketio, eventlet, os\n```python\nfrom flask import Flask\nfrom flask_socketio import SocketIO\nimport eventlet\nimport os\n\neventlet.monkey_patch()\n\ndef create_socketio_app(debug_mode):\n    \"\"\"\n    Create a Flask application with SocketIO support for real-time communication.\n\n    Parameters:\n    debug_mode (bool): If True, the Flask application will run in debug mode.\n\n    Returns:\n    Flask: The created Flask application.\n\n    Requirements:\n    - flask\n    - flask_socketio\n    - eventlet\n    - os\n\n    Example:\n    >>> app = create_socketio_app(True)\n    >>> app.run()\n    \"\"\"\n    app = Flask(__name__)\n    app.debug = debug_mode\n    socketio = SocketIO(app)\n\n    @socketio.on('message')\n    def handle_message(message):\n        print('received message: ' + message)\n\n    return app\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nflask, flask_login, flask_wtf, flask_sqlalchemy\n```python\nfrom flask import Flask\nfrom flask_login import LoginManager\nfrom flask_wtf import CSRFProtect\nfrom flask_sqlalchemy import SQLAlchemy\n\nSQLALCHEMY_DATABASE_URI = 'sqlite:///app.db'\n\ndef create_login_app(debug_mode):\n    \"\"\"\n    Create a Flask application with user login and CSRF protection support.\n\n    Parameters:\n    debug_mode (bool): If True, the Flask application will run in debug mode.\n\n    Returns:\n    Flask: The created Flask application.\n\n    Requirements:\n    - flask\n    - flask_login\n    - flask_wtf\n    - flask_sqlalchemy\n\n    Example:\n    >>> app = create_login_app(True)\n    >>> app.run()\n    \"\"\"\n    app = Flask(__name__)\n    app.config['SQLALCHEMY_DATABASE_URI'] = SQLALCHEMY_DATABASE_URI\n    app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False\n    app.debug = debug_mode\n\n    login_manager = LoginManager(app)\n    csrf = CSRFProtect(app)\n    db = SQLAlchemy(app)\n\n    return app\n```\n"}
{"seed": "{\"task_id\": 40133826, \"prompt\": \"def f_40133826(mylist):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"pickle.dump(mylist, open('save.txt', 'wb'))\", \"test_start\": \"\\nimport pickle\\n\\ndef check(candidate):\", \"test\": [\"\\n    candidate([i for i in range(10)])\\n    data = pickle.load(open('save.txt', 'rb'))\\n    assert data == [i for i in range(10)]\\n\", \"\\n    candidate([\\\"hello\\\", \\\"world\\\", \\\"!\\\"])\\n    data = pickle.load(open('save.txt', 'rb'))\\n    assert data == [\\\"hello\\\", \\\"world\\\", \\\"!\\\"]\\n\"], \"entry_point\": \"f_40133826\", \"intent\": \"python save list `mylist` to file object 'save.txt'\", \"library\": [\"pickle\"]}", "generation": "Scenario 1: \npickle, os, pandas, numpy\n```python\nimport pickle\nimport os\nimport pandas as pd\nimport numpy as np\n\n# Constants\nFILE_NAME = 'save.pkl'\nDATA_FRAME = pd.DataFrame(np.random.randint(0,100,size=(100, 4)), columns=list('ABCD'))\n\ndef save_dataframe_to_pickle(df):\n    \"\"\"\n    Save pandas DataFrame `df` to pickle file 'save.pkl' and then reads it back for validation.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame to be saved.\n\n    Returns:\n    DataFrame: The loaded DataFrame from 'save.pkl'.\n\n    Requirements:\n    - pickle\n    - os\n    - pandas\n    - numpy\n\n    Example:\n    >>> df = pd.DataFrame(np.random.randint(0,100,size=(100, 4)), columns=list('ABCD'))\n    >>> loaded_df = save_dataframe_to_pickle(df)\n    >>> assert df.equals(loaded_df)\n    \"\"\"\n    with open(FILE_NAME, 'wb') as file:\n        pickle.dump(df, file)\n    \n    with open(FILE_NAME, 'rb') as file:\n        loaded_df = pickle.load(file)\n\n    os.remove(FILE_NAME)\n\n    return loaded_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \npickle, os, random, string\n```python\nimport pickle\nimport os\nimport random\nimport string\n\n# Constants\nFILE_NAME = 'save.pkl'\nRANDOM_STRINGS = [''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(10)) for _ in range(10)]\n\ndef save_random_strings_to_pickle(strings):\n    \"\"\"\n    Save list of random strings `strings` to pickle file 'save.pkl' and then reads it back for validation.\n\n    Parameters:\n    strings (list): The list of random strings to be saved.\n\n    Returns:\n    list: The loaded list of strings from 'save.pkl'.\n\n    Requirements:\n    - pickle\n    - os\n    - random\n    - string\n\n    Example:\n    >>> strings = [''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(10)) for _ in range(10)]\n    >>> loaded_strings = save_random_strings_to_pickle(strings)\n    >>> assert strings == loaded_strings\n    \"\"\"\n    with open(FILE_NAME, 'wb') as file:\n        pickle.dump(strings, file)\n    \n    with open(FILE_NAME, 'rb') as file:\n        loaded_strings = pickle.load(file)\n\n    os.remove(FILE_NAME)\n\n    return loaded_strings\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \npickle, os, datetime, pytz\n```python\nimport pickle\nimport os\nfrom datetime import datetime\nimport pytz\n\n# Constants\nFILE_NAME = 'save.pkl'\nUTC_NOW = datetime.now(pytz.UTC)\n\ndef save_datetime_to_pickle(dt):\n    \"\"\"\n    Save datetime object `dt` to pickle file 'save.pkl' and then reads it back for validation.\n\n    Parameters:\n    dt (datetime): The datetime object to be saved.\n\n    Returns:\n    datetime: The loaded datetime object from 'save.pkl'.\n\n    Requirements:\n    - pickle\n    - os\n    - datetime\n    - pytz\n\n    Example:\n    >>> dt = datetime.now(pytz.UTC)\n    >>> loaded_dt = save_datetime_to_pickle(dt)\n    >>> assert dt == loaded_dt\n    \"\"\"\n    with open(FILE_NAME, 'wb') as file:\n        pickle.dump(dt, file)\n    \n    with open(FILE_NAME, 'rb') as file:\n        loaded_dt = pickle.load(file)\n\n    os.remove(FILE_NAME)\n\n    return loaded_dt\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \npickle, os, numpy, sklearn.datasets\n```python\nimport pickle\nimport os\nimport numpy as np\nfrom sklearn.datasets import make_classification\n\n# Constants\nFILE_NAME = 'save.pkl'\nDATA, TARGET = make_classification(n_samples=100, n_features=20, n_informative=2, n_redundant=10, n_classes=2, random_state=1)\n\ndef save_sklearn_dataset_to_pickle(data, target):\n    \"\"\"\n    Save sklearn dataset (`data` and `target`) to pickle file 'save.pkl' and then reads it back for validation.\n\n    Parameters:\n    data (numpy array): The data part of the sklearn dataset.\n    target (numpy array): The target part of the sklearn dataset.\n\n    Returns:\n    tuple: The loaded tuple (data, target) from 'save.pkl'.\n\n    Requirements:\n    - pickle\n    - os\n    - numpy\n    - sklearn.datasets\n\n    Example:\n    >>> data, target = make_classification(n_samples=100, n_features=20, n_informative=2, n_redundant=10, n_classes=2, random_state=1)\n    >>> loaded_data, loaded_target = save_sklearn_dataset_to_pickle(data, target)\n    >>> assert np.array_equal(data, loaded_data) and np.array_equal(target, loaded_target)\n    \"\"\"\n    with open(FILE_NAME, 'wb') as file:\n        pickle.dump((data, target), file)\n    \n    with open(FILE_NAME, 'rb') as file:\n        loaded_data, loaded_target = pickle.load(file)\n\n    os.remove(FILE_NAME)\n\n    return loaded_data, loaded_target\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \npickle, os, random, matplotlib.pyplot\n```python\nimport pickle\nimport os\nimport random\nimport matplotlib.pyplot as plt\n\n# Constants\nFILE_NAME = 'save.pkl'\nRANDOM_NUMBERS = [random.random() for _ in range(100)]\n\ndef save_matplotlib_figure_to_pickle(numbers):\n    \"\"\"\n    Save matplotlib figure generated from `numbers` to pickle file 'save.pkl' and then reads it back for validation.\n\n    Parameters:\n    numbers (list): The list of numbers used to generate the matplotlib figure.\n\n    Returns:\n    Figure: The loaded matplotlib figure from 'save.pkl'.\n\n    Requirements:\n    - pickle\n    - os\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> numbers = [random.random() for _ in range(100)]\n    >>> fig = plt.figure()\n    >>> plt.plot(numbers)\n    >>> loaded_fig = save_matplotlib_figure_to_pickle(numbers)\n    >>> assert fig == loaded_fig\n    \"\"\"\n    fig = plt.figure()\n    plt.plot(numbers)\n\n    with open(FILE_NAME, 'wb') as file:\n        pickle.dump(fig, file)\n    \n    with open(FILE_NAME, 'rb') as file:\n        loaded_fig = pickle.load(file)\n\n    os.remove(FILE_NAME)\n\n    return loaded_fig\n```\n"}
{"seed": "{\"task_id\": 4490961, \"prompt\": \"def f_4490961(P, T):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"scipy.tensordot(P, T, axes=[1, 1]).swapaxes(0, 1)\", \"test_start\": \"\\nimport scipy\\nimport numpy as np\\n\\ndef check(candidate):\", \"test\": [\"\\n    P = np.array([[6, 2, 7], [1, 1, 8], [8, 7, 1], [9, 6, 4], [2, 1, 1]])\\n    T = np.array([[[9, 7, 2, 3], [9, 6, 8, 2], [6, 6, 2, 8]],\\n                  [[4, 5, 5, 3], [1, 8, 3, 5], [2, 8, 1, 6]]])\\n    result = np.array([[[114,  96,  42,  78], [ 66,  61,  26,  69], [141, 104,  74,  46], [159, 123,  74,  71],  [ 33,  26,  14,  16]], \\n                      [[ 40, 102,  43,  70], [ 21,  77,  16,  56], [ 41, 104,  62,  65], [ 50, 125,  67,  81], [ 11,  26,  14,  17]]])\\n    assert np.array_equal(candidate(P, T), result)\\n\"], \"entry_point\": \"f_4490961\", \"intent\": \"Multiply a matrix `P` with a 3d tensor `T` in scipy\", \"library\": [\"numpy\", \"scipy\"]}", "generation": "Scenario 1: \nnumpy, scipy, matplotlib, seaborn\n```python\nimport numpy as np\nimport scipy\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nTENSOR_SHAPE = (3, 3, 3)\n\ndef matrix_tensor_product_and_visualization(P, T):\n    \"\"\"\n    Calculate the product of a matrix `P` and a 3D tensor `T` using scipy, \n    and then visualize the result as a heat map using seaborn.\n\n    Parameters:\n    P (numpy.ndarray): The input matrix.\n    T (numpy.ndarray): The input tensor.\n\n    Returns:\n    numpy.ndarray: The result of the product.\n\n    Requirements:\n    - numpy\n    - scipy\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> P = np.array([[6, 2, 7], [1, 1, 8], [8, 7, 1], [9, 6, 4], [2, 1, 1]])\n    >>> T = np.random.rand(*TENSOR_SHAPE)\n    >>> result = matrix_tensor_product_and_visualization(P, T)\n    >>> sns.heatmap(result)\n    \"\"\"\n    result = scipy.tensordot(P, T, axes=[1, 1]).swapaxes(0, 1)\n    sns.heatmap(result)\n    return result\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, scipy, pandas, sklearn.preprocessing\n```python\nimport numpy as np\nimport scipy\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\n# Constants\nFEATURE_NAMES = ['feature_1', 'feature_2', 'feature_3']\n\ndef matrix_tensor_product_and_normalize(P, T):\n    \"\"\"\n    Calculate the product of a matrix `P` and a 3D tensor `T` using scipy, \n    and then normalize the result using StandardScaler from sklearn.\n\n    Parameters:\n    P (numpy.ndarray): The input matrix.\n    T (numpy.ndarray): The input tensor.\n\n    Returns:\n    pandas.DataFrame: A DataFrame with the normalized result.\n\n    Requirements:\n    - numpy\n    - scipy\n    - pandas\n    - sklearn.preprocessing\n\n    Example:\n    >>> P = np.array([[6, 2, 7], [1, 1, 8], [8, 7, 1], [9, 6, 4], [2, 1, 1]])\n    >>> T = np.random.rand(*TENSOR_SHAPE)\n    >>> result = matrix_tensor_product_and_normalize(P, T)\n    >>> print(result)\n    \"\"\"\n    result = scipy.tensordot(P, T, axes=[1, 1]).swapaxes(0, 1)\n    scaler = StandardScaler()\n    result = scaler.fit_transform(result)\n    result = pd.DataFrame(result, columns=FEATURE_NAMES)\n    return result\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, scipy, matplotlib, mpl_toolkits.mplot3d\n```python\nimport numpy as np\nimport scipy\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Constants\nTENSOR_SHAPE = (3, 3, 3)\n\ndef matrix_tensor_product_and_3d_visualization(P, T):\n    \"\"\"\n    Calculate the product of a matrix `P` and a 3D tensor `T` using scipy, \n    and then visualize the result in 3D using matplotlib.\n\n    Parameters:\n    P (numpy.ndarray): The input matrix.\n    T (numpy.ndarray): The input tensor.\n\n    Returns:\n    numpy.ndarray: The result of the product.\n\n    Requirements:\n    - numpy\n    - scipy\n    - matplotlib.pyplot\n    - mpl_toolkits.mplot3d\n\n    Example:\n    >>> P = np.array([[6, 2, 7], [1, 1, 8], [8, 7, 1], [9, 6, 4], [2, 1, 1]])\n    >>> T = np.random.rand(*TENSOR_SHAPE)\n    >>> result = matrix_tensor_product_and_3d_visualization(P, T)\n    >>> fig = plt.figure()\n    >>> ax = fig.add_subplot(111, projection='3d')\n    >>> ax.scatter(result[:, 0], result[:, 1], result[:, 2])\n    \"\"\"\n    result = scipy.tensordot(P, T, axes=[1, 1]).swapaxes(0, 1)\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    ax.scatter(result[:, 0], result[:, 1], result[:, 2])\n    return result\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, scipy, sklearn.decomposition, matplotlib\n```python\nimport numpy as np\nimport scipy\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\n# Constants\nTENSOR_SHAPE = (3, 3, 3)\n\ndef matrix_tensor_product_and_pca(P, T):\n    \"\"\"\n    Calculate the product of a matrix `P` and a 3D tensor `T` using scipy, \n    then apply PCA to reduce the dimensionality of the result and visualize it.\n\n    Parameters:\n    P (numpy.ndarray): The input matrix.\n    T (numpy.ndarray): The input tensor.\n\n    Returns:\n    numpy.ndarray: The result of PCA.\n\n    Requirements:\n    - numpy\n    - scipy\n    - sklearn.decomposition\n    - matplotlib.pyplot\n\n    Example:\n    >>> P = np.array([[6, 2, 7], [1, 1, 8], [8, 7, 1], [9, 6, 4], [2, 1, 1]])\n    >>> T = np.random.rand(*TENSOR_SHAPE)\n    >>> result = matrix_tensor_product_and_pca(P, T)\n    >>> plt.scatter(result[:, 0], result[:, 1])\n    \"\"\"\n    result = scipy.tensordot(P, T, axes=[1, 1]).swapaxes(0, 1)\n    pca = PCA(n_components=2)\n    result = pca.fit_transform(result)\n    plt.scatter(result[:, 0], result[:, 1])\n    return result\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, scipy, sklearn.cluster, matplotlib\n```python\nimport numpy as np\nimport scipy\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\n# Constants\nTENSOR_SHAPE = (3, 3, 3)\n\ndef matrix_tensor_product_and_clustering(P, T):\n    \"\"\"\n    Calculate the product of a matrix `P` and a 3D tensor `T` using scipy, \n    then apply KMeans clustering to the result and visualize it.\n\n    Parameters:\n    P (numpy.ndarray): The input matrix.\n    T (numpy.ndarray): The input tensor.\n\n    Returns:\n    numpy.ndarray: The result of KMeans clustering.\n\n    Requirements:\n    - numpy\n    - scipy\n    - sklearn.cluster\n    - matplotlib.pyplot\n\n    Example:\n    >>> P = np.array([[6, 2, 7], [1, 1, 8], [8, 7, 1], [9, 6, 4], [2, 1, 1]])\n    >>> T = np.random.rand(*TENSOR_SHAPE)\n    >>> result = matrix_tensor_product_and_clustering(P, T)\n    >>> plt.scatter(result[:, 0], result[:, 1], c=result[:, 2])\n    \"\"\"\n    result = scipy.tensordot(P, T, axes=[1, 1]).swapaxes(0, 1)\n    kmeans = KMeans(n_clusters=3, random_state=0)\n    result = kmeans.fit_predict(result)\n    plt.scatter(result[:, 0], result[:, 1], c=result[:, 2])\n    return result\n```\n"}
{"seed": "{\"task_id\": 2173087, \"prompt\": \"def f_2173087():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"numpy.zeros((3, 3, 3))\", \"test_start\": \"\\nimport numpy \\nimport numpy as np\\n\\ndef check(candidate):\", \"test\": [\"\\n    result = np.array([[[0., 0., 0.], [0., 0., 0.], [0., 0., 0.]],\\n                          [[0., 0., 0.], [0., 0., 0.], [0., 0., 0.]],\\n                          [[0., 0., 0.], [0., 0., 0.], [0., 0., 0.]]])\\n    assert np.array_equal(candidate(), result)\\n\"], \"entry_point\": \"f_2173087\", \"intent\": \"Create 3d array of zeroes of size `(3,3,3)`\", \"library\": [\"numpy\"]}", "generation": "Scenario 1: \nnumpy, matplotlib, mpl_toolkits.mplot3d\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\ndef plot_3d_scatter_random_points(n_points=100):\n    \"\"\"\n    Generate an array of random 3D points and plot them in a 3D scatter plot.\n    \n    Parameters:\n    n_points (int): The number of points to generate and plot. Default is 100.\n    \n    Returns:\n    ndarray: A numpy ndarray of shape (n_points, 3) with the coordinates of the points.\n    \n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - mpl_toolkits.mplot3d.Axes3D\n    \n    Example:\n    >>> points = plot_3d_scatter_random_points(200)\n    \"\"\"\n    points = np.random.random((n_points, 3))\n    \n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    ax.scatter(points[:, 0], points[:, 1], points[:, 2])\n    plt.show()\n    \n    return points\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, scipy.spatial, matplotlib\n```python\nimport numpy as np\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\nimport matplotlib.pyplot as plt\n\ndef calculate_and_plot_voronoi(points):\n    \"\"\"\n    Calculate the Voronoi diagram for a set of points in 2D and plot it.\n    \n    Parameters:\n    points (ndarray): A numpy ndarray of shape (n_points, 2) with the coordinates of the points.\n    \n    Returns:\n    Voronoi: A Voronoi object representing the Voronoi diagram of the points.\n    \n    Requirements:\n    - numpy\n    - scipy.spatial.Voronoi\n    - scipy.spatial.voronoi_plot_2d\n    - matplotlib.pyplot\n    \n    Example:\n    >>> points = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n    >>> vor = calculate_and_plot_voronoi(points)\n    \"\"\"\n    vor = Voronoi(points)\n    voronoi_plot_2d(vor)\n    plt.show()\n    \n    return vor\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnumpy, matplotlib, sklearn.datasets\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\n\ndef generate_and_plot_blobs(n_samples=100, centers=3, n_features=2):\n    \"\"\"\n    Generate isotropic Gaussian blobs for clustering and plot them.\n    \n    Parameters:\n    n_samples (int): The total number of points divided among clusters.\n    centers (int): The number of centers to generate.\n    n_features (int): The number of features for each sample.\n    \n    Returns:\n    tuple: A tuple (X, y) where X is the matrix of blob points and y is the vector of blob labels.\n    \n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.datasets.make_blobs\n    \n    Example:\n    >>> X, y = generate_and_plot_blobs(n_samples=500, centers=5)\n    \"\"\"\n    X, y = make_blobs(n_samples=n_samples, centers=centers, n_features=n_features)\n    \n    plt.scatter(X[:, 0], X[:, 1], c=y)\n    plt.show()\n    \n    return X, y\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, matplotlib, sklearn.decomposition\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\ndef perform_and_plot_pca(data, n_components=2):\n    \"\"\"\n    Perform Principal Component Analysis (PCA) on the given data and plot the results.\n    \n    Parameters:\n    data (ndarray): A numpy ndarray of shape (n_samples, n_features) representing the data.\n    n_components (int): Number of components to keep. If n_components is not set, all components are kept.\n    \n    Returns:\n    ndarray: The transformed data.\n    \n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.decomposition.PCA\n    \n    Example:\n    >>> data = np.random.random((100, 5))\n    >>> transformed_data = perform_and_plot_pca(data)\n    \"\"\"\n    pca = PCA(n_components=n_components)\n    transformed_data = pca.fit_transform(data)\n    \n    plt.scatter(transformed_data[:, 0], transformed_data[:, 1])\n    plt.show()\n    \n    return transformed_data\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnumpy, matplotlib, scipy.stats\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\ndef plot_normal_distribution(mu=0, sigma=1):\n    \"\"\"\n    Plot a normal distribution with the given mean and standard deviation.\n    \n    Parameters:\n    mu (float): The mean of the distribution. Default is 0.\n    sigma (float): The standard deviation of the distribution. Default is 1.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - scipy.stats.norm\n    \n    Example:\n    >>> plot_normal_distribution(mu=5, sigma=2)\n    \"\"\"\n    x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n    y = norm.pdf(x, mu, sigma)\n    \n    plt.plot(x, y)\n    plt.show()\n```\n"}
{"seed": "{\"task_id\": 6266727, \"prompt\": \"def f_6266727(content):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"\\\"\\\"\\\" \\\"\\\"\\\".join(content.split(' ')[:-1])\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('test') == ''\\n\", \"\\n    assert candidate('this is an example content') == 'this is an example'\\n\", \"\\n    assert candidate('  ') == ' '\\n\", \"\\n    assert candidate('') == ''\\n\", \"\\n    assert candidate('blank and tab\\t') == 'blank and'\\n\"], \"entry_point\": \"f_6266727\", \"intent\": \"cut off the last word of a sentence `content`\", \"library\": []}", "generation": "Scenario 1: \ncollections, re, nltk, string\n```python\nimport collections\nimport re\nfrom nltk.corpus import stopwords\nimport string\n\n# Constants\nSTOPWORDS = set(stopwords.words('english'))\n\ndef frequent_words_except_last(content):\n    \"\"\"\n    Frequency of each word in a sentence except the last word and also excluding stop words.\n    \n    Parameters:\n    content (str): The input sentence.\n    \n    Returns:\n    dict: A dictionary with words as keys and their frequency as values.\n\n    Requirements:\n    - collections\n    - re\n    - nltk.corpus\n    - string\n    \n    Example:\n    >>> frequent_words_except_last('this is an example content')\n    {'example': 1}\n    \"\"\"\n    content = content.split(' ')[:-1]\n    words = [\n        word.strip(string.punctuation).lower() \n        for sentence in content \n        for word in re.split('\\W+', sentence) \n        if word not in STOPWORDS\n    ]\n    word_counts = collections.Counter(words)\n\n    return dict(word_counts)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nre, string, nltk, collections\n```python\nimport re\nimport string\nfrom nltk.stem import PorterStemmer\nfrom collections import Counter\n\nSTEMMER = PorterStemmer()\n\ndef stem_and_count(content):\n    \"\"\"\n    Stem each word in a sentence except the last one and count the frequency of each stemmed word.\n\n    Parameters:\n    content (str): The sentence to stem and count.\n\n    Returns:\n    dict: A dictionary with stemmed words as keys and their frequency as values.\n\n    Requirements:\n    - re\n    - string\n    - nltk.stem\n    - collections.Counter\n\n    Example:\n    >>> stem_and_count('running runner run')\n    {'run': 3}\n    \"\"\"\n    content = content.split(' ')[:-1]\n    words = [word.strip(string.punctuation).lower() for word in re.split('\\W+', ' '.join(content))]\n    stemmed_words = [STEMMER.stem(word) for word in words]\n    word_counts = Counter(stemmed_words)\n\n    return dict(word_counts)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nre, nltk, string, collections\n```python\nimport re\nimport string\nfrom nltk.corpus import stopwords\nfrom collections import Counter\n\nSTOPWORDS = set(stopwords.words('english'))\n\ndef count_non_stopwords(content):\n    \"\"\"\n    Count the non-stopwords in a sentence excluding the last word.\n\n    Parameters:\n    content (str): The sentence to count non-stopwords from.\n\n    Returns:\n    int: The count of non-stopwords.\n\n    Requirements:\n    - re\n    - string\n    - nltk.corpus\n    - collections.Counter\n\n    Example:\n    >>> count_non_stopwords('this is an example content')\n    1\n    \"\"\"\n    content = content.split(' ')[:-1]\n    words = [word.strip(string.punctuation).lower() for word in re.split('\\W+', ' '.join(content))]\n    non_stopwords = [word for word in words if word not in STOPWORDS]\n    count = len(non_stopwords)\n\n    return count\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnltk, string, re, collections\n```python\nimport nltk\nimport string\nimport re\nfrom collections import Counter\n\ndef count_pos_tags(content):\n    \"\"\"\n    Count the Part-of-speech (POS) tags in a sentence excluding the last word.\n\n    Parameters:\n    content (str): The sentence to count POS tags from.\n\n    Returns:\n    dict: A dictionary with POS tags as keys and their count as values.\n\n    Requirements:\n    - nltk\n    - string\n    - re\n    - collections.Counter\n\n    Example:\n    >>> count_pos_tags('this is an example content')\n    {'DT': 1, 'VBZ': 1, 'DT': 1, 'NN': 1}\n    \"\"\"\n    content = content.split(' ')[:-1]\n    words = nltk.word_tokenize(' '.join(content))\n    pos_tags = nltk.pos_tag(words)\n    pos_counts = Counter(tag for word, tag in pos_tags)\n\n    return dict(pos_counts)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnltk, string, re, collections\n```python\nimport nltk\nimport string\nimport re\nfrom collections import Counter\n\ndef count_noun_phrases(content):\n    \"\"\"\n    Count the noun phrases in a sentence excluding the last word.\n\n    Parameters:\n    content (str): The sentence to count noun phrases from.\n\n    Returns:\n    dict: A dictionary with noun phrases as keys and their count as values.\n\n    Requirements:\n    - nltk\n    - string\n    - re\n    - collections.Counter\n\n    Example:\n    >>> count_noun_phrases('the big black cat sat on the big green mat')\n    {'the big black cat': 1, 'the big green mat': 1}\n    \"\"\"\n    content = content.split(' ')[:-1]\n    words = nltk.word_tokenize(' '.join(content))\n    pos_tags = nltk.pos_tag(words)\n    grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n    cp = nltk.RegexpParser(grammar)\n    tree = cp.parse(pos_tags)\n    noun_phrases = [' '.join(leaf[0] for leaf in tree.leaves()) for tree in tree.subtrees(filter=lambda t: t.label() == 'NP')]\n    np_counts = Counter(noun_phrases)\n\n    return dict(np_counts)\n```\n"}
{"seed": "{\"task_id\": 30385151, \"prompt\": \"def f_30385151(x):\\n\\t\", \"suffix\": \"\\n\\treturn x\", \"canonical_solution\": \"x = np.asarray(x).reshape(1, -1)[(0), :]\", \"test_start\": \"\\nimport numpy as np\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert all(candidate(1.) == np.asarray(1.))\\n\", \"\\n    assert all(candidate(123) == np.asarray(123))\\n\", \"\\n    assert all(candidate('a') == np.asarray('a'))\\n\", \"\\n    assert all(candidate(False) == np.asarray(False))\\n\"], \"entry_point\": \"f_30385151\", \"intent\": \"convert scalar `x` to array\", \"library\": [\"numpy\"]}", "generation": "Scenario 1:\nnumpy, pandas, matplotlib, sklearn\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\n# Constants\nFEATURES = ['Feature1', 'Feature2', 'Feature3', 'Feature4', 'Feature5']\n\ndef standardize_and_plot(data):\n    \"\"\"\n    Standardize the given data and then plot a histogram of each feature.\n\n    Parameters:\n    data (DataFrame): The data to be standardized and plotted.\n\n    Returns:\n    DataFrame: The standardized data.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - sklearn.preprocessing.StandardScaler\n\n    Example:\n    >>> data = pd.DataFrame(np.random.rand(100, 5), columns=FEATURES)\n    >>> standardized_data = standardize_and_plot(data)\n    >>> print(standardized_data)\n    \"\"\"\n    scaler = StandardScaler()\n    data_standardized = pd.DataFrame(scaler.fit_transform(data), columns=FEATURES)\n\n    for feature in FEATURES:\n        plt.hist(data_standardized[feature], bins=20, alpha=0.5)\n        plt.title('Histogram of {}'.format(feature))\n        plt.show()\n\n    return data_standardized\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, scipy, sklearn, matplotlib\n```python\nimport numpy as np\nfrom scipy.spatial.distance import cdist\nfrom sklearn.datasets import make_blobs\nimport matplotlib.pyplot as plt\n\n# Constants\nN_SAMPLES = 200\nN_FEATURES = 2\nCENTERS = 4\n\ndef create_clusters_and_compute_distance():\n    \"\"\"\n    Create a dataset with make_blobs, plot the dataset, and then compute the \n    distance between each sample in the dataset using the Euclidean distance.\n\n    Returns:\n    ndarray: A 2D array with the distances between each sample.\n\n    Requirements:\n    - numpy\n    - scipy.spatial.distance.cdist\n    - sklearn.datasets.make_blobs\n    - matplotlib.pyplot\n\n    Example:\n    >>> distances = create_clusters_and_compute_distance()\n    >>> print(distances)\n    \"\"\"\n    X, y = make_blobs(n_samples=N_SAMPLES, n_features=N_FEATURES, centers=CENTERS)\n\n    plt.scatter(X[:, 0], X[:, 1], c=y)\n    plt.show()\n\n    distances = cdist(X, X)\n\n    return distances\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, sklearn, matplotlib, seaborn\n```python\nimport numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nN_SAMPLES = 500\nN_FEATURES = 50\n\ndef perform_pca_and_plot():\n    \"\"\"\n    Generate a high-dimensional dataset, perform PCA to reduce its dimensionality, \n    and then plot a heatmap of the covariance matrix of the transformed data.\n\n    Returns:\n    ndarray: The transformed data.\n\n    Requirements:\n    - numpy\n    - sklearn.decomposition.PCA\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> transformed_data = perform_pca_and_plot()\n    >>> print(transformed_data)\n    \"\"\"\n    X = np.random.rand(N_SAMPLES, N_FEATURES)\n\n    pca = PCA(n_components=2)\n    X_transformed = pca.fit_transform(X)\n\n    plt.figure(figsize=(10, 7))\n    sns.heatmap(np.cov(X_transformed.T), annot=True, fmt=\".2f\")\n    plt.show()\n\n    return X_transformed\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, pandas, sklearn, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\n\n# Constants\nN_SAMPLES = 1000\nN_FEATURES = 10\n\ndef generate_data_train_model_and_plot():\n    \"\"\"\n    Generate a dataset, split it into training and test sets, train a linear regression \n    model on the training set, and then plot the model's predictions on the test set.\n\n    Returns:\n    ndarray: The model's predictions on the test set.\n\n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.model_selection.train_test_split\n    - sklearn.linear_model.LinearRegression\n    - matplotlib.pyplot\n\n    Example:\n    >>> predictions = generate_data_train_model_and_plot()\n    >>> print(predictions)\n    \"\"\"\n    X = np.random.rand(N_SAMPLES, N_FEATURES)\n    y = np.sum(X, axis=1) + np.random.normal(size=N_SAMPLES)\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    predictions = model.predict(X_test)\n\n    plt.scatter(y_test, predictions)\n    plt.xlabel('True Values')\n    plt.ylabel('Predictions')\n    plt.show()\n\n    return predictions\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, pandas, sklearn, seaborn\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport seaborn as sns\n\n# Constants\nFEATURES = ['Feature1', 'Feature2', 'Feature3', 'Feature4', 'Feature5']\n\ndef normalize_data_and_plot_correlation(data):\n    \"\"\"\n    Normalize the given data using MinMaxScaler and then plot a correlation heatmap of the features.\n\n    Parameters:\n    data (DataFrame): The data to be normalized and plotted.\n\n    Returns:\n    DataFrame: The normalized data.\n\n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.preprocessing.MinMaxScaler\n    - seaborn\n\n    Example:\n    >>> data = pd.DataFrame(np.random.rand(100, 5), columns=FEATURES)\n    >>> normalized_data = normalize_data_and_plot_correlation(data)\n    >>> print(normalized_data)\n    \"\"\"\n    scaler = MinMaxScaler()\n    data_normalized = pd.DataFrame(scaler.fit_transform(data), columns=FEATURES)\n\n    sns.heatmap(data_normalized.corr(), annot=True, cmap='coolwarm')\n    plt.show()\n\n    return data_normalized\n```"}
{"seed": "{\"task_id\": 15856127, \"prompt\": \"def f_15856127(L):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"sum(sum(i) if isinstance(i, list) else i for i in L)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([1,2,3,4]) == 10\\n\", \"\\n    assert candidate([[1],[2],[3],[4]]) == 10\\n\", \"\\n    assert candidate([1,1,1,1]) == 4\\n\", \"\\n    assert candidate([1,[2,3],[4]]) == 10\\n\", \"\\n    assert candidate([]) == 0\\n\", \"\\n    assert candidate([[], []]) == 0\\n\"], \"entry_point\": \"f_15856127\", \"intent\": \"sum all elements of nested list `L`\", \"library\": []}", "generation": "Scenario 1: \nnumpy, math, itertools\n```python\nimport numpy as np\nimport math\nfrom itertools import chain\n\ndef calculate_mean_and_variance(L):\n    \"\"\"\n    Calculate the mean and variance of all elements in a nested list `L`.\n    \n    Parameters:\n    L (list): The nested list.\n    \n    Returns:\n    dict: A dictionary containing the mean and variance.\n    \n    Requirements:\n    - numpy\n    - math\n    - itertools.chain\n\n    Example:\n    >>> calculate_mean_and_variance([[1,2,3],[4,5,6]])\n    {'mean': 3.5, 'variance': 2.9166666666666665}\n    \"\"\"\n    flattened = list(chain.from_iterable(L))\n    mean = np.mean(flattened)\n    variance = np.var(flattened)\n    \n    return {'mean': mean, 'variance': variance}\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, pandas, matplotlib.pyplot\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef plot_histogram(L):\n    \"\"\"\n    Plot a histogram of all elements in a nested list `L`.\n    \n    Parameters:\n    L (list): The nested list.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_histogram([[1,2,3],[4,5,6]])\n    \"\"\"\n    flattened = np.array(L).flatten()\n    series = pd.Series(flattened)\n    series.plot(kind='hist', rwidth=0.8)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, scipy.stats\n```python\nimport numpy as np\nfrom scipy import stats\n\ndef calculate_mode(L):\n    \"\"\"\n    Calculate the mode of all elements in a nested list `L`.\n    \n    Parameters:\n    L (list): The nested list.\n    \n    Returns:\n    int: The mode.\n    \n    Requirements:\n    - numpy\n    - scipy.stats\n\n    Example:\n    >>> calculate_mode([[1,2,3],[4,5,6]])\n    1\n    \"\"\"\n    flattened = np.array(L).flatten()\n    mode = stats.mode(flattened)[0][0]\n    \n    return mode\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, scipy.stats\n```python\nimport numpy as np\nfrom scipy import stats\n\ndef calculate_median(L):\n    \"\"\"\n    Calculate the median of all elements in a nested list `L`.\n    \n    Parameters:\n    L (list): The nested list.\n    \n    Returns:\n    float: The median.\n    \n    Requirements:\n    - numpy\n    - scipy.stats\n\n    Example:\n    >>> calculate_median([[1,2,3],[4,5,6]])\n    3.5\n    \"\"\"\n    flattened = np.array(L).flatten()\n    median = np.median(flattened)\n    \n    return median\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, scipy.stats\n```python\nimport numpy as np\nfrom scipy.stats import iqr\n\ndef calculate_iqr(L):\n    \"\"\"\n    Calculate the interquartile range of all elements in a nested list `L`.\n    \n    Parameters:\n    L (list): The nested list.\n    \n    Returns:\n    float: The interquartile range.\n    \n    Requirements:\n    - numpy\n    - scipy.stats\n\n    Example:\n    >>> calculate_iqr([[1,2,3],[4,5,6]])\n    2.5\n    \"\"\"\n    flattened = np.array(L).flatten()\n    iqr_value = iqr(flattened)\n    \n    return iqr_value\n```"}
{"seed": "{\"task_id\": 1592158, \"prompt\": \"def f_1592158():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"struct.unpack('!f', bytes.fromhex('470FC614'))[0]\", \"test_start\": \"\\nimport struct \\n\\ndef check(candidate):\", \"test\": [\"\\n    assert (candidate() - 36806.078125) < 1e-6\\n\", \"\\n    assert (candidate() - 32806.079125) > 1e-6\\n\"], \"entry_point\": \"f_1592158\", \"intent\": \"convert hex string '470FC614' to a float number\", \"library\": [\"struct\"]}", "generation": "Scenario 1:\nstruct, binascii, hashlib, random\n```python\nimport struct\nimport binascii\nimport hashlib\nimport random\n\n# Constants\nKEYS = ['470FC614', '4A0FC614', '4B9FC614', '4C8FC614', '4D7FC614']\n\ndef generate_hashed_float():\n    \"\"\"\n    Generate a random float number from a list of hex strings, and then hash the float number to md5.\n\n    Returns:\n    str: The md5 hash string.\n\n    Requirements:\n    - struct\n    - binascii\n    - hashlib\n    - random\n\n    Example:\n    >>> generate_hashed_float()\n    \"\"\"\n    hex_key = random.choice(KEYS)\n    float_num = struct.unpack('!f', bytes.fromhex(hex_key))[0]\n    hashed_float = hashlib.md5(str(float_num).encode()).hexdigest()\n\n    return hashed_float\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nstruct, os, base64, binascii\n```python\nimport struct\nimport os\nimport base64\nimport binascii\n\ndef generate_base64_encoded_float():\n    \"\"\"\n    Generate a random float number, convert it to hex string and encode it to base64.\n\n    Returns:\n    str: The base64 encoded string.\n\n    Requirements:\n    - struct\n    - os\n    - base64\n    - binascii\n\n    Example:\n    >>> generate_base64_encoded_float()\n    \"\"\"\n    float_num = struct.unpack('!f', os.urandom(4))[0]\n    hex_str = hex(int(float_num))\n    encoded_str = base64.b64encode(binascii.unhexlify(hex_str[2:]))\n\n    return encoded_str.decode()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nstruct, sys, binascii, zlib\n```python\nimport struct\nimport sys\nimport binascii\nimport zlib\n\n# Constants\nKEY = '470FC614'\n\ndef generate_compressed_float():\n    \"\"\"\n    Convert a hex string to float number, and then compress the float number.\n\n    Returns:\n    bytes: The compressed float number.\n\n    Requirements:\n    - struct\n    - sys\n    - binascii\n    - zlib\n\n    Example:\n    >>> generate_compressed_float()\n    \"\"\"\n    float_num = struct.unpack('!f', bytes.fromhex(KEY))[0]\n    compressed_float = zlib.compress(sys.getsizeof(float_num))\n\n    return compressed_float\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nstruct, codecs, binascii, random\n```python\nimport struct\nimport codecs\nimport binascii\nimport random\n\n# Constants\nKEYS = ['470FC614', '4A0FC614', '4B9FC614', '4C8FC614', '4D7FC614']\n\ndef generate_encoded_float():\n    \"\"\"\n    Generate a random float number from a list of hex strings, and then encode the float number to utf-8.\n\n    Returns:\n    bytes: The utf-8 encoded float number.\n\n    Requirements:\n    - struct\n    - codecs\n    - binascii\n    - random\n\n    Example:\n    >>> generate_encoded_float()\n    \"\"\"\n    hex_key = random.choice(KEYS)\n    float_num = struct.unpack('!f', bytes.fromhex(hex_key))[0]\n    encoded_float = codecs.encode(str(float_num), 'utf-8')\n\n    return encoded_float\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nstruct, math, binascii, random\n```python\nimport struct\nimport math\nimport binascii\nimport random\n\n# Constants\nKEYS = ['470FC614', '4A0FC614', '4B9FC614', '4C8FC614', '4D7FC614']\n\ndef generate_rounded_float():\n    \"\"\"\n    Generate a random float number from a list of hex strings, and then round the float number to 2 decimal places.\n\n    Returns:\n    float: The rounded float number.\n\n    Requirements:\n    - struct\n    - math\n    - binascii\n    - random\n\n    Example:\n    >>> generate_rounded_float()\n    \"\"\"\n    hex_key = random.choice(KEYS)\n    float_num = struct.unpack('!f', bytes.fromhex(hex_key))[0]\n    rounded_float = round(float_num, 2)\n\n    return rounded_float\n```\n"}
{"seed": "{\"task_id\": 5010536, \"prompt\": \"def f_5010536(my_dict):\\n\\t\", \"suffix\": \"\\n\\treturn my_dict\", \"canonical_solution\": \"my_dict.update((x, y * 2) for x, y in list(my_dict.items()))\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate({'a': [1], 'b': 4.9}) == {'a': [1, 1], 'b': 9.8}\\n\", \"\\n    assert candidate({1:1}) == {1:2}\\n\", \"\\n    assert candidate({(1,2):[1]}) == {(1,2):[1,1]}\\n\", \"\\n    assert candidate({'asd':0}) == {'asd':0}\\n\", \"\\n    assert candidate({}) == {}\\n\"], \"entry_point\": \"f_5010536\", \"intent\": \"Multiple each value by `2` for all keys in a dictionary `my_dict`\", \"library\": []}", "generation": "Scenario 3:\npandas, numpy, json, re\n```python\nimport pandas as pd\nimport numpy as np\nimport json\nimport re\n\n# Constants\nNUMBERS = re.compile(r'\\d+')\n\ndef normalize_dict_and_create_dataframe(json_str):\n    \"\"\"\n    Load a JSON string into a dictionary, normalize the dictionary by doubling \n    the numeric values, and then create a pandas DataFrame from the dictionary.\n\n    Parameters:\n    json_str (str): The JSON string.\n\n    Returns:\n    DataFrame: A pandas DataFrame created from the dictionary.\n\n    Requirements:\n    - pandas\n    - numpy\n    - json\n    - re\n\n    Example:\n    >>> json_str = '{\"a\": [1, 2, 3], \"b\": 4.9, \"c\": \"5\"}'\n    >>> df = normalize_dict_and_create_dataframe(json_str)\n    >>> print(df)\n    \"\"\"\n    my_dict = json.loads(json_str)\n\n    for key, value in my_dict.items():\n        if isinstance(value, list):\n            my_dict[key] = [v * 2 if isinstance(v, (int, float)) else v for v in value]\n        elif isinstance(value, (int, float)):\n            my_dict[key] = value * 2\n        elif isinstance(value, str) and NUMBERS.match(value):\n            my_dict[key] = int(value) * 2\n\n    df = pd.DataFrame(my_dict)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ncollections, heapq, random\n```python\nfrom collections import Counter\nimport heapq\nimport random\n\n# Constants\nLETTERS = list('abcdefghijklmnopqrstuvwxyz')\n\ndef most_common_letters_in_dict(my_dict):\n    \"\"\"\n    Create a dictionary where the keys are letters and the values are random integers.\n    Find the 3 most common letters in the dictionary.\n\n    Parameters:\n    my_dict (dict): The dictionary to process.\n\n    Returns:\n    list: The 3 most common letters.\n\n    Requirements:\n    - collections\n    - heapq\n    - random\n\n    Example:\n    >>> my_dict = {letter: random.randint(1, 100) for letter in LETTERS}\n    >>> most_common_letters = most_common_letters_in_dict(my_dict)\n    >>> print(most_common_letters)\n    \"\"\"\n    letter_counter = Counter(my_dict)\n    most_common_letters = heapq.nlargest(3, letter_counter, key=letter_counter.get)\n\n    return most_common_letters\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nitertools, operator, functools\n```python\nfrom itertools import groupby\nfrom operator import itemgetter\nfrom functools import reduce\n\n# Constants\nKEY_FUNC = itemgetter(0)\n\ndef aggregate_dict_values(my_dict):\n    \"\"\"\n    Group the dictionary items by the first character of the key and sum the \n    values for each group.\n\n    Parameters:\n    my_dict (dict): The dictionary to process.\n\n    Returns:\n    dict: The aggregated dictionary.\n\n    Requirements:\n    - itertools\n    - operator\n    - functools\n\n    Example:\n    >>> my_dict = {'apple': 1, 'banana': 2, 'avocado': 3, 'blueberry': 4, 'blackberry': 5}\n    >>> aggregated_dict = aggregate_dict_values(my_dict)\n    >>> print(aggregated_dict)\n    \"\"\"\n    sorted_items = sorted(my_dict.items(), key=KEY_FUNC)\n    aggregated_dict = {key: sum(item[1] for item in group) for key, group in groupby(sorted_items, KEY_FUNC)}\n\n    return aggregated_dict\n```\n`GPT_ODEX_BREAK`\n\nScenario 6:\nast, math, collections\n```python\nimport ast\nimport math\nfrom collections import defaultdict\n\n# Constants\nOPERATORS = {ast.Add: operator.add, ast.Sub: operator.sub, ast.Mult: operator.mul,\n             ast.Div: operator.truediv, ast.USub: operator.neg}\n\ndef evaluate_dict_and_calculate_sum(my_dict):\n    \"\"\"\n    Evaluate the values in the dictionary as mathematical expressions and calculate\n    the sum of the results.\n\n    Parameters:\n    my_dict (dict): The dictionary to process.\n\n    Returns:\n    float: The sum of the results.\n\n    Requirements:\n    - ast\n    - math\n    - collections\n\n    Example:\n    >>> my_dict = {'a': '1+2*3', 'b': '4/2', 'c': '5-2'}\n    >>> total = evaluate_dict_and_calculate_sum(my_dict)\n    >>> print(total)\n    \"\"\"\n    def eval_expr(node):\n        if isinstance(node, ast.Num): \n            return node.n\n        elif isinstance(node, ast.BinOp):\n            return OPERATORS[type(node.op)](eval_expr(node.left), eval_expr(node.right))\n        elif isinstance(node, ast.UnaryOp):\n            return OPERATORS[type(node.op)](eval_expr(node.operand))\n        else:\n            raise TypeError(node)\n\n    total = 0\n    for value in my_dict.values():\n        node = ast.parse(value, mode='eval')\n        total += eval_expr(node.body)\n\n    return total\n```\n"}
{"seed": "{\"task_id\": 13745648, \"prompt\": \"def f_13745648():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"subprocess.call('sleep.sh', shell=True)\", \"test_start\": \"\\nimport subprocess \\nfrom unittest.mock import Mock\\n\\ndef check(candidate):\", \"test\": [\"\\n    subprocess.call = Mock()\\n    try:\\n        candidate()\\n    except:\\n        assert False\\n\"], \"entry_point\": \"f_13745648\", \"intent\": \"running bash script 'sleep.sh'\", \"library\": [\"subprocess\"]}", "generation": "Scenario 1: \nsubprocess, os, time, datetime\n```python\nimport subprocess\nimport os\nimport time\nfrom datetime import datetime\n\n# Constants\nSCRIPT_DIR = '/path/to/scripts/'\nSCRIPTS = ['script1.sh', 'script2.sh', 'script3.sh']\n\ndef execute_scripts_with_delay(delay):\n    \"\"\"\n    Execute a list of bash scripts with a given delay between each script.\n\n    Parameters:\n    delay (int): The delay in seconds between each script execution.\n\n    Returns:\n    None\n\n    Requirements:\n    - subprocess\n    - os\n    - time\n    - datetime\n\n    Example:\n    >>> execute_scripts_with_delay(5)\n    \"\"\"\n    for script in SCRIPTS:\n        script_path = os.path.join(SCRIPT_DIR, script)\n        print(f\"Starting {script} at {datetime.now()}\")\n        subprocess.call(script_path, shell=True)\n        time.sleep(delay)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nsubprocess, csv, pandas, matplotlib.pyplot\n```python\nimport subprocess\nimport csv\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nCSV_FILE_PATH = '/path/to/csv/file.csv'\n\ndef execute_script_and_analyze_output(script):\n    \"\"\"\n    Execute a bash script that generates a CSV file. Then load the CSV file \n    into a pandas DataFrame and create a bar plot of the data.\n\n    Parameters:\n    script (str): The path to the bash script.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the CSV data.\n\n    Requirements:\n    - subprocess\n    - csv\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = execute_script_and_analyze_output('script.sh')\n    >>> print(df)\n    >>> df.plot(kind='bar')\n    \"\"\"\n    subprocess.call(script, shell=True)\n    \n    df = pd.read_csv(CSV_FILE_PATH)\n    \n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nsubprocess, os, time, threading\n```python\nimport subprocess\nimport os\nimport time\nimport threading\n\n# Constants\nSCRIPTS = ['script1.sh', 'script2.sh', 'script3.sh']\n\ndef parallel_script_execution():\n    \"\"\"\n    Execute multiple bash scripts in parallel.\n\n    Returns:\n    None\n\n    Requirements:\n    - subprocess\n    - os\n    - time\n    - threading\n\n    Example:\n    >>> parallel_script_execution()\n    \"\"\"\n    threads = []\n    for script in SCRIPTS:\n        thread = threading.Thread(target=subprocess.call, args=(script, shell=True))\n        threads.append(thread)\n        thread.start()\n\n    for thread in threads:\n        thread.join()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nsubprocess, os, time, random\n```python\nimport subprocess\nimport os\nimport time\nimport random\n\n# Constants\nSCRIPTS = ['script1.sh', 'script2.sh', 'script3.sh']\n\ndef random_script_execution():\n    \"\"\"\n    Execute a random bash script from a list of scripts.\n\n    Returns:\n    str: The name of the script that was executed.\n\n    Requirements:\n    - subprocess\n    - os\n    - time\n    - random\n\n    Example:\n    >>> script = random_script_execution()\n    >>> print(f\"Executed script: {script}\")\n    \"\"\"\n    script = random.choice(SCRIPTS)\n    subprocess.call(script, shell=True)\n\n    return script\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nsubprocess, os, psutil, time\n```python\nimport subprocess\nimport os\nimport psutil\nimport time\n\n# Constants\nSCRIPT = 'script.sh'\n\ndef execute_script_and_monitor_resources():\n    \"\"\"\n    Execute a bash script and monitor the CPU and memory usage of the process.\n\n    Returns:\n    dict: A dictionary with the CPU and memory usage.\n\n    Requirements:\n    - subprocess\n    - os\n    - psutil\n    - time\n\n    Example:\n    >>> resources = execute_script_and_monitor_resources()\n    >>> print(resources)\n    \"\"\"\n    p = subprocess.Popen(SCRIPT, shell=True)\n    \n    cpu_usage = 0\n    memory_usage = 0\n\n    while p.poll() is None:\n        process = psutil.Process(p.pid)\n        cpu_usage += process.cpu_percent(interval=1)\n        memory_usage += process.memory_info().rss\n        time.sleep(1)\n\n    return {'CPU Usage': cpu_usage, 'Memory Usage': memory_usage}\n```"}
{"seed": "{\"task_id\": 44778, \"prompt\": \"def f_44778(l):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"\\\"\\\"\\\",\\\"\\\"\\\".join(l)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(['a','b','c']) == 'a,b,c'\\n\", \"\\n    assert candidate(['a','b']) == 'a,b'\\n\", \"\\n    assert candidate([',',',',',']) == ',,,,,'\\n\", \"\\n    assert candidate([' ','  ','c']) == ' ,  ,c'\\n\", \"\\n    assert candidate([]) == ''\\n\"], \"entry_point\": \"f_44778\", \"intent\": \"Join elements of list `l` with a comma `,`\", \"library\": []}", "generation": "Scenario 1:\npandas, numpy, random, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nCATEGORIES = ['a', 'b', 'c', 'd', 'e']\nCOUNT = 100\n\ndef generate_category_data():\n    \"\"\"\n    Generate a pandas DataFrame with COUNT rows. Each row contains a randomly \n    selected category from CATEGORIES and a random integer between 1 and 100.\n\n    The function also plots a bar chart showing the counts of each category in the DataFrame.\n\n    Returns:\n    DataFrame: A pandas DataFrame with randomly generated category data.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = generate_category_data()\n    >>> print(df)\n    >>> df['Category'].value_counts().plot(kind='bar')\n    \"\"\"\n    data = []\n\n    for _ in range(COUNT):\n        category = CATEGORIES[randint(0, len(CATEGORIES)-1)]\n        value = randint(1, 100)\n        data.append([category, value])\n\n    df = pd.DataFrame(data, columns=['Category', 'Value'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, pandas, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nSEPARATOR = ','\n\ndef plot_histogram(data_str):\n    \"\"\"\n    Convert a string of comma-separated numbers into a pandas Series, and then\n    plot a histogram of the data.\n\n    Parameters:\n    data_str (str): The string of comma-separated numbers.\n\n    Returns:\n    Series: A pandas Series of the numbers.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> plot_histogram('1,2,3,4,5,5,5,4,3,2,1')\n    \"\"\"\n    data = pd.Series(np.fromstring(data_str, sep=SEPARATOR))\n    data.plot.hist(grid=True, bins=20, rwidth=0.9, color='#607c8e')\n\n    return data\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, random, matplotlib\n```python\nimport numpy as np\nfrom random import uniform\nimport matplotlib.pyplot as plt\n\n# Constants\nPOINTS = 100\n\ndef plot_scatter():\n    \"\"\"\n    Generate POINTS number of random (x, y) pairs, where x and y are floats \n    between 0 and 1, and then plot a scatter plot of the pairs.\n\n    Returns:\n    ndarray: A numpy array of shape (POINTS, 2) containing the (x, y) pairs.\n    \n    Requirements:\n    - numpy\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> plot_scatter()\n    \"\"\"\n    points = np.array([[uniform(0, 1), uniform(0, 1)] for _ in range(POINTS)])\n    plt.scatter(points[:, 0], points[:, 1])\n    \n    return points\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, re\n```python\nimport numpy as np\nimport re\n\n# Constants\nSEPARATOR = ','\n\ndef count_numbers_in_string(s):\n    \"\"\"\n    Count the number of integers and floats in a comma-separated string.\n\n    Parameters:\n    s (str): The comma-separated string.\n\n    Returns:\n    int: The number of integers and floats in the string.\n    \n    Requirements:\n    - numpy\n    - re\n    \n    Example:\n    >>> count_numbers_in_string('1,2,3.5,abc,4,5.6')\n    \"\"\"\n    numbers = re.findall(r'\\b\\d+\\b', s)\n    return len(numbers)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\ncsv, pandas, matplotlib\n```python\nimport csv\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nFILENAME = 'data.csv'\nCOLUMNS = ['A', 'B', 'C']\n\ndef read_and_plot_csv():\n    \"\"\"\n    Read a CSV file into a pandas DataFrame and then plot a line chart of the data.\n\n    Returns:\n    DataFrame: A pandas DataFrame of the data in the CSV file.\n    \n    Requirements:\n    - csv\n    - pandas\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = read_and_plot_csv()\n    >>> print(df)\n    >>> df.plot()\n    \"\"\"\n    df = pd.read_csv(FILENAME, names=COLUMNS)\n    df.plot()\n\n    return df\n```"}
{"seed": "{\"task_id\": 44778, \"prompt\": \"def f_44778(myList):\\n\\t\", \"suffix\": \"\\n\\treturn myList\", \"canonical_solution\": \"myList = ','.join(map(str, myList))\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([1,2,3]) == '1,2,3'\\n\", \"\\n    assert candidate([1,2,'a']) == '1,2,a'\\n\", \"\\n    assert candidate([]) == ''\\n\", \"\\n    assert candidate(['frg',3253]) == 'frg,3253'\\n\"], \"entry_point\": \"f_44778\", \"intent\": \"make a comma-separated string from a list `myList`\", \"library\": []}", "generation": "Scenario 1:\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n\n# Constants\nGRADES = ['A', 'B', 'C', 'D', 'F']\n\ndef student_grades_report(student_grades):\n    \"\"\"\n    Generate a report of student grades in a class, including a count of each grade and a bar plot.\n\n    Parameters:\n    student_grades (list): List of student grades.\n\n    Returns:\n    DataFrame: A pandas DataFrame with counts of each grade.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - collections.Counter\n\n    Example:\n    >>> student_grades = ['A', 'B', 'B', 'C', 'A', 'D', 'F', 'B', 'A', 'C']\n    >>> report = student_grades_report(student_grades)\n    >>> print(report)\n    >>> report['Count'].plot(kind='bar')\n    \"\"\"\n    grade_counts = dict(Counter(student_grades))\n    for grade in GRADES:\n        if grade not in grade_counts:\n            grade_counts[grade] = 0\n\n    report_df = pd.DataFrame.from_dict(grade_counts, orient='index', columns=['Count'])\n\n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\n```python\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\n\ndef normalize_data(myList):\n    \"\"\"\n    Normalize a list of numerical values to the range [0, 1] using Min-Max scaling.\n\n    Parameters:\n    myList (list): List of numerical values to normalize.\n\n    Returns:\n    ndarray: An array of normalized values.\n\n    Requirements:\n    - sklearn.preprocessing.MinMaxScaler\n    - numpy\n\n    Example:\n    >>> myList = [10, 20, 30, 40, 50]\n    >>> normalize_data(myList)\n    \"\"\"\n    myList = np.array(myList).reshape(-1, 1)\n    scaler = MinMaxScaler()\n    normalized_list = scaler.fit_transform(myList)\n\n    return normalized_list.flatten()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_histogram(myList):\n    \"\"\"\n    Plot a histogram of values in a list.\n\n    Parameters:\n    myList (list): List of numerical values to plot.\n\n    Requirements:\n    - matplotlib.pyplot\n    - numpy\n\n    Example:\n    >>> myList = [1, 2, 2, 3, 3, 3, 4, 4, 4, 4]\n    >>> plot_histogram(myList)\n    \"\"\"\n    plt.hist(myList, bins=np.arange(min(myList), max(myList) + 2) - 0.5, edgecolor='black')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of Values')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\n```python\nfrom collections import Counter\nimport pandas as pd\n\ndef word_frequency(myList):\n    \"\"\"\n    Count the frequency of each word in a list and return a DataFrame of words and their counts.\n\n    Parameters:\n    myList (list): List of words.\n\n    Returns:\n    DataFrame: A pandas DataFrame with words and their counts.\n\n    Requirements:\n    - collections.Counter\n    - pandas\n\n    Example:\n    >>> myList = ['apple', 'banana', 'apple', 'cherry', 'banana', 'banana']\n    >>> word_frequency(myList)\n    \"\"\"\n    word_counts = dict(Counter(myList))\n    report_df = pd.DataFrame.from_dict(word_counts, orient='index', columns=['Count'])\n\n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\n```python\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nimport numpy as np\n\ndef cluster_data(myList, n_clusters):\n    \"\"\"\n    Cluster a list of 2D points using KMeans and plot the clusters.\n\n    Parameters:\n    myList (list): List of 2D points.\n    n_clusters (int): Number of clusters.\n\n    Requirements:\n    - matplotlib.pyplot\n    - sklearn.cluster.KMeans\n    - numpy\n\n    Example:\n    >>> myList = [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]\n    >>> cluster_data(myList, 2)\n    \"\"\"\n    kmeans = KMeans(n_clusters=n_clusters)\n    kmeans.fit(myList)\n\n    plt.scatter(*zip(*myList), c=kmeans.labels_)\n    plt.scatter(*zip(*kmeans.cluster_centers_), marker='x', color='red')\n    plt.show()\n```"}
{"seed": "{\"task_id\": 7286365, \"prompt\": \"def f_7286365():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"list(reversed(list(range(10))))\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate() == [9,8,7,6,5,4,3,2,1,0]\\n\", \"\\n    assert len(candidate()) == 10\\n\", \"\\n    assert min(candidate()) == 0\\n\", \"\\n    assert type(candidate()) == list\\n\", \"\\n    assert type(candidate()[-2]) == int\\n\"], \"entry_point\": \"f_7286365\", \"intent\": \"reverse the list that contains 1 to 10\", \"library\": []}", "generation": "Scenario 1:\nnumpy, matplotlib, random, itertools\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nimport itertools\n\n# Constants\nCOLORS = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n\ndef plot_random_walks(n_walks, n_steps):\n    \"\"\"\n    Generate n_walks random walks of n_steps and plot them in different colors.\n    \n    Parameters:\n    n_walks (int): The number of random walks.\n    n_steps (int): The number of steps in each walk.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - matplotlib\n    - random\n    - itertools\n    \n    Example:\n    >>> plot_random_walks(5, 100)\n    \"\"\"\n    color_cycle = itertools.cycle(COLORS)\n    \n    for _ in range(n_walks):\n        walk = np.random.choice([-1, 1], size=n_steps)\n        walk = np.cumsum(walk)\n        plt.plot(walk, next(color_cycle))\n\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, matplotlib, scipy.stats\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n# Constants\nN_SAMPLES = 1000\nMU = 0\nSIGMA = 1\n\ndef plot_normal_distribution():\n    \"\"\"\n    Generate a histogram of N_SAMPLES random numbers from a normal distribution \n    with mean MU and standard deviation SIGMA, and overlay the PDF.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - matplotlib\n    - scipy.stats.norm\n\n    Example:\n    >>> plot_normal_distribution()\n    \"\"\"\n    samples = np.random.normal(MU, SIGMA, N_SAMPLES)\n    count, bins, ignored = plt.hist(samples, 30, density=True)\n\n    plt.plot(bins, norm.pdf(bins, MU, SIGMA), linewidth=2, color='r')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, collections, random\n```python\nimport numpy as np\nfrom collections import Counter\nimport random\n\n# Constants\nVALUES = list(range(1, 11))\nWEIGHTS = list(reversed(VALUES))\nN_SAMPLES = 1000\n\ndef sample_weighted_distribution():\n    \"\"\"\n    Sample N_SAMPLES random numbers from VALUES according to the WEIGHTS distribution,\n    and return a histogram of the samples.\n\n    Returns:\n    dict: A dictionary with the values as keys and counts as values.\n\n    Requirements:\n    - numpy\n    - collections.Counter\n    - random\n\n    Example:\n    >>> sample_weighted_distribution()\n    \"\"\"\n    samples = random.choices(VALUES, weights=WEIGHTS, k=N_SAMPLES)\n    histogram = dict(Counter(samples))\n\n    return histogram\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, matplotlib, scipy.optimize\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import curve_fit\n\n# Constants\nX = np.linspace(-10, 10, 100)\nY = 3*X**2 + 2*X + 1 + np.random.normal(0, 20, len(X))\n\ndef fit_quadratic():\n    \"\"\"\n    Fit a quadratic function to the data (X, Y) and plot the data and the fit.\n\n    Returns:\n    list: The optimized parameters of the quadratic function.\n\n    Requirements:\n    - numpy\n    - matplotlib\n    - scipy.optimize.curve_fit\n\n    Example:\n    >>> fit_quadratic()\n    \"\"\"\n    def func(x, a, b, c):\n        return a*x**2 + b*x + c\n\n    popt, pcov = curve_fit(func, X, Y)\n\n    plt.scatter(X, Y)\n    plt.plot(X, func(X, *popt), 'r-')\n\n    plt.show()\n\n    return list(popt)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, pandas, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nN = 100\nCATEGORIES = ['A', 'B', 'C', 'D', 'E']\n\ndef create_category_scatterplot():\n    \"\"\"\n    Create a DataFrame with N rows and 3 columns: 'x' and 'y' with random values, \n    and 'category' with random categories. Plot a scatterplot of 'x' vs 'y' colored by 'category'.\n\n    Returns:\n    DataFrame: The generated DataFrame.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib\n\n    Example:\n    >>> df = create_category_scatterplot()\n    \"\"\"\n    df = pd.DataFrame({\n        'x': np.random.rand(N),\n        'y': np.random.rand(N),\n        'category': np.random.choice(CATEGORIES, N)\n    })\n\n    for category in CATEGORIES:\n        plt.scatter(df[df['category'] == category]['x'], \n                    df[df['category'] == category]['y'], \n                    label=category)\n\n    plt.legend()\n    plt.show()\n\n    return df\n```\n"}
{"seed": "{\"task_id\": 18454570, \"prompt\": \"def f_18454570():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"'lamp, bag, mirror'.replace('bag,', '')\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate() == 'lamp,  mirror'\\n    assert type(candidate()) == str\\n    assert len(candidate()) == 13\\n    assert candidate().startswith('lamp')\\n\"], \"entry_point\": \"f_18454570\", \"intent\": \"remove substring 'bag,' from a string 'lamp, bag, mirror'\", \"library\": []}", "generation": "Scenario 1:\npandas, regex, string, random\n```python\nimport pandas as pd\nimport re\nimport string\nimport random\n\n# Constants\nDATA = ['lamp, bag, mirror', 'table, chair, bag, lamp', 'mirror, bag, table',\n        'lamp, bag, chair', 'mirror, bag, lamp', 'table, bag, chair']\n\ndef remove_random_substring(data_list):\n    \"\"\"\n    Remove a random substring from a list of strings.\n\n    Parameters:\n    data_list (list): The list of strings.\n\n    Returns:\n    DataFrame: A pandas DataFrame with original and modified strings.\n\n    Requirements:\n    - pandas\n    - regex\n    - string\n    - random\n\n    Example:\n    >>> remove_random_substring(DATA)\n    \"\"\"\n    df = pd.DataFrame(data_list, columns=['Original String'])\n\n    modified_strings = []\n    for s in data_list:\n        substrings = re.split(', ', s)\n        random_substring = random.choice(substrings)\n        modified_s = s.replace(random_substring + ', ', '')\n        modified_strings.append(modified_s)\n\n    df['Modified String'] = modified_strings\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nrandom, string, re, pandas\n```python\nimport random\nimport string\nimport re\nimport pandas as pd\n\n# Constants\nDATA = ['lamp, bag, mirror', 'table, chair, bag, lamp', 'mirror, bag, table',\n        'lamp, bag, chair', 'mirror, bag, lamp', 'table, bag, chair']\n\ndef replace_random_substring(data_list):\n    \"\"\"\n    Replace a random substring in a list of strings with a random string.\n\n    Parameters:\n    data_list (list): The list of strings.\n\n    Returns:\n    DataFrame: A pandas DataFrame with original and modified strings.\n\n    Requirements:\n    - pandas\n    - random\n    - string\n    - regex\n\n    Example:\n    >>> replace_random_substring(DATA)\n    \"\"\"\n    df = pd.DataFrame(data_list, columns=['Original String'])\n\n    modified_strings = []\n    for s in data_list:\n        substrings = re.split(', ', s)\n        random_substring = random.choice(substrings)\n        random_string = ''.join(random.choices(string.ascii_lowercase, k=5))\n        modified_s = s.replace(random_substring, random_string)\n        modified_strings.append(modified_s)\n\n    df['Modified String'] = modified_strings\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nre, string, random, pandas\n```python\nimport re\nimport string\nimport random\nimport pandas as pd\n\n# Constants\nDATA = ['lamp, bag, mirror', 'table, chair, bag, lamp', 'mirror, bag, table',\n        'lamp, bag, chair', 'mirror, bag, lamp', 'table, bag, chair']\n\ndef shuffle_substrings(data_list):\n    \"\"\"\n    Shuffle the substrings in a list of strings.\n\n    Parameters:\n    data_list (list): The list of strings.\n\n    Returns:\n    DataFrame: A pandas DataFrame with original and shuffled strings.\n\n    Requirements:\n    - pandas\n    - random\n    - string\n    - regex\n\n    Example:\n    >>> shuffle_substrings(DATA)\n    \"\"\"\n    df = pd.DataFrame(data_list, columns=['Original String'])\n\n    shuffled_strings = []\n    for s in data_list:\n        substrings = re.split(', ', s)\n        random.shuffle(substrings)\n        shuffled_s = ', '.join(substrings)\n        shuffled_strings.append(shuffled_s)\n\n    df['Shuffled String'] = shuffled_strings\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, random, string, re\n```python\nimport pandas as pd\nimport random\nimport string\nimport re\n\n# Constants\nDATA = ['lamp, bag, mirror', 'table, chair, bag, lamp', 'mirror, bag, table',\n        'lamp, bag, chair', 'mirror, bag, lamp', 'table, bag, chair']\n\ndef randomize_substrings(data_list):\n    \"\"\"\n    Randomize the position of substrings in a list of strings.\n\n    Parameters:\n    data_list (list): The list of strings.\n\n    Returns:\n    DataFrame: A pandas DataFrame with original and randomized strings.\n\n    Requirements:\n    - pandas\n    - random\n    - string\n    - regex\n\n    Example:\n    >>> randomize_substrings(DATA)\n    \"\"\"\n    df = pd.DataFrame(data_list, columns=['Original String'])\n\n    randomized_strings = []\n    for s in data_list:\n        substrings = re.split(', ', s)\n        random_positions = random.sample(range(len(substrings)), len(substrings))\n        randomized_s = ', '.join([substrings[i] for i in random_positions])\n        randomized_strings.append(randomized_s)\n\n    df['Randomized String'] = randomized_strings\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, re, string, random\n```python\nimport pandas as pd\nimport re\nimport string\nimport random\n\n# Constants\nDATA = ['lamp, bag, mirror', 'table, chair, bag, lamp', 'mirror, bag, table',\n        'lamp, bag, chair', 'mirror, bag, lamp', 'table, bag, chair']\n\ndef random_substring_operations(data_list):\n    \"\"\"\n    Apply a random operation (remove, replace, shuffle, or randomize) to substrings in a list of strings.\n\n    Parameters:\n    data_list (list): The list of strings.\n\n    Returns:\n    DataFrame: A pandas DataFrame with original and modified strings.\n\n    Requirements:\n    - pandas\n    - random\n    - string\n    - regex\n\n    Example:\n    >>> random_substring_operations(DATA)\n    \"\"\"\n    df = pd.DataFrame(data_list, columns=['Original String'])\n\n    modified_strings = []\n    for s in data_list:\n        substrings = re.split(', ', s)\n        operation = random.choice(['remove', 'replace', 'shuffle', 'randomize'])\n        if operation == 'remove':\n            random_substring = random.choice(substrings)\n            modified_s = s.replace(random_substring + ', ', '')\n        elif operation == 'replace':\n            random_substring = random.choice(substrings)\n            random_string = ''.join(random.choices(string.ascii_lowercase, k=5))\n            modified_s = s.replace(random_substring, random_string)\n        elif operation == 'shuffle':\n            random.shuffle(substrings)\n            modified_s = ', '.join(substrings)\n        elif operation == 'randomize':\n            random_positions = random.sample(range(len(substrings)), len(substrings))\n            modified_s = ', '.join([substrings[i] for i in random_positions])\n        modified_strings.append(modified_s)\n\n    df['Modified String'] = modified_strings\n\n    return df\n```"}
{"seed": "{\"task_id\": 4357787, \"prompt\": \"def f_4357787(s):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"\\\"\\\"\\\".\\\"\\\"\\\".join(s.split('.')[::-1])\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('apple.orange.red.green.yellow') == 'yellow.green.red.orange.apple'\\n\", \"\\n    assert candidate('apple') == 'apple'\\n\", \"\\n    assert candidate('apple.orange') == 'orange.apple'\\n\", \"\\n    assert candidate('123.456') == '456.123'\\n\", \"\\n    assert candidate('.') == '.'\\n\"], \"entry_point\": \"f_4357787\", \"intent\": \"Reverse the order of words, delimited by `.`, in string `s`\", \"library\": []}", "generation": "Scenario 1: \nos, sys, re, fileinput\n```python\nimport os\nimport sys\nimport re\nimport fileinput\n\ndef reverse_file_content(file_path):\n    \"\"\"\n    Reverse the order of lines in a file and reverse the order of words, delimited by `.`, in each line.\n    \n    Parameters:\n    file_path (str): The path to the file.\n    \n    Returns:\n    str: The contents of the file with lines and words in reversed order.\n    \n    Requirements:\n    - os\n    - sys\n    - re\n    - fileinput\n    \n    Example:\n    >>> reverse_file_content('path_to_file.txt')\n    \"\"\"\n    if not os.path.isfile(file_path):\n        sys.exit(\"File path {} does not exist. Exiting...\".format(file_path))\n\n    content = []\n    for line in fileinput.input(file_path):\n        content.append('.'.join(line.strip().split('.')[::-1]))\n    return '\\n'.join(content[::-1])\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nurllib, bs4, re\n```python\nfrom urllib.request import urlopen\nfrom bs4 import BeautifulSoup\nimport re\n\ndef scrape_and_reverse_webpage(url):\n    \"\"\"\n    Scrape a webpage and reverse the order of sentences, delimited by `.`, in each paragraph.\n\n    Parameters:\n    url (str): The URL of the webpage.\n\n    Returns:\n    str: The contents of the webpage with sentences in reversed order.\n\n    Requirements:\n    - urllib\n    - bs4 (BeautifulSoup)\n    - re\n\n    Example:\n    >>> scrape_and_reverse_webpage('http://example.com')\n    \"\"\"\n    page = urlopen(url)\n    soup = BeautifulSoup(page, 'html.parser')\n    \n    reversed_content = []\n    for paragraph in soup.find_all('p'):\n        sentences = re.split('\\. ', paragraph.text)\n        reversed_sentences = '. '.join(sentences[::-1])\n        reversed_content.append(reversed_sentences)\n        \n    return ' '.join(reversed_content)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \npandas, re\n```python\nimport pandas as pd\nimport re\n\ndef reverse_dataframe_column(df, column_name):\n    \"\"\"\n    Reverse the order of words, delimited by `.`, in a specific column of a pandas DataFrame.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n    column_name (str): The name of the column.\n\n    Returns:\n    DataFrame: The pandas DataFrame with the specified column's content reversed.\n\n    Requirements:\n    - pandas\n    - re\n\n    Example:\n    >>> df = pd.DataFrame({'A': ['apple.orange', 'red.green.yellow'], 'B': [1, 2]})\n    >>> reversed_df = reverse_dataframe_column(df, 'A')\n    >>> print(reversed_df)\n    \"\"\"\n    df[column_name] = df[column_name].apply(lambda s: '.'.join(s.split('.')[::-1]))\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nos, re, glob\n```python\nimport os\nimport re\nimport glob\n\ndef reverse_directory_filenames(directory_path):\n    \"\"\"\n    Reverse the order of words, delimited by `.`, in all filenames in a directory.\n\n    Parameters:\n    directory_path (str): The path to the directory.\n\n    Returns:\n    None\n\n    Requirements:\n    - os\n    - re\n    - glob\n\n    Example:\n    >>> reverse_directory_filenames('/path/to/directory')\n    \"\"\"\n    for filename in glob.glob(os.path.join(directory_path, '*')):\n        base_name = os.path.basename(filename)\n        new_base_name = '.'.join(base_name.split('.')[::-1])\n        os.rename(filename, os.path.join(directory_path, new_base_name))\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnumpy, re\n```python\nimport numpy as np\nimport re\n\ndef reverse_numpy_array_strings(arr):\n    \"\"\"\n    Reverse the order of words, delimited by `.`, in all strings in a numpy array.\n\n    Parameters:\n    arr (numpy array): The numpy array.\n\n    Returns:\n    numpy array: The numpy array with the strings reversed.\n\n    Requirements:\n    - numpy\n    - re\n\n    Example:\n    >>> arr = np.array(['apple.orange', 'red.green.yellow'])\n    >>> reversed_arr = reverse_numpy_array_strings(arr)\n    >>> print(reversed_arr)\n    \"\"\"\n    vectorized_reverse = np.vectorize(lambda s: '.'.join(s.split('.')[::-1]))\n    return vectorized_reverse(arr)\n```"}
{"seed": "{\"task_id\": 21787496, \"prompt\": \"def f_21787496(s):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"datetime.datetime.fromtimestamp(s).strftime('%Y-%m-%d %H:%M:%S.%f')\", \"test_start\": \"\\nimport time\\nimport datetime\\n\\ndef check(candidate): \", \"test\": [\"\\n    assert candidate(1236472) == '1970-01-15 07:27:52.000000'\\n\", \"\\n    assert candidate(0) == '1970-01-01 00:00:00.000000'\\n\", \"\\n    assert candidate(5.3) == '1970-01-01 00:00:05.300000'\\n\"], \"entry_point\": \"f_21787496\", \"intent\": \"convert epoch time represented as milliseconds `s` to string using format '%Y-%m-%d %H:%M:%S.%f'\", \"library\": [\"datetime\", \"time\"]}", "generation": "Scenario 1: \npandas, datetime, math, numpy, random\n```python\nimport pandas as pd\nfrom datetime import datetime\nimport math\nimport numpy as np\nfrom random import randint\n\n# Constants\nCOLUMNS = ['Timestamp', 'Sensor1', 'Sensor2', 'Sensor3', 'SensorStatus']\nSENSOR_STATUSES = ['OK', 'MAINTENANCE_REQUIRED', 'ERROR']\n\ndef generate_sensor_data(start_time, end_time, step):\n    \"\"\"\n    Generate a time series data for multiple sensors from start_time to end_time with a given step.\n    The sensor data is generated using a sin function with some random noise.\n    The sensor status is randomly selected from SENSOR_STATUSES.\n\n    Parameters:\n    start_time (int): The start epoch time in milliseconds.\n    end_time (int): The end epoch time in milliseconds.\n    step (int): The step in milliseconds between each data point.\n\n    Returns:\n    DataFrame: A pandas DataFrame with generated sensor data.\n    \n    Requirements:\n    - pandas\n    - datetime\n    - math\n    - numpy\n    - random\n\n    Example:\n    >>> df = generate_sensor_data(0, 10000, 100)\n    >>> print(df)\n    \"\"\"\n    timestamps = np.arange(start_time, end_time, step)\n    df = pd.DataFrame(columns=COLUMNS)\n\n    for ts in timestamps:\n        dt = datetime.fromtimestamp(ts/1000).strftime('%Y-%m-%d %H:%M:%S.%f')\n        sensor1 = math.sin(ts/1000) + np.random.normal(0, 0.1)\n        sensor2 = math.cos(ts/1000) + np.random.normal(0, 0.1)\n        sensor3 = math.tan(ts/1000) + np.random.normal(0, 0.1)\n        status = SENSOR_STATUSES[randint(0, len(SENSOR_STATUSES)-1)]\n        df = df.append(dict(zip(COLUMNS, [dt, sensor1, sensor2, sensor3, status])), ignore_index=True)\n    \n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ndatetime, pytz, numpy, matplotlib\n```python\nfrom datetime import datetime, timedelta\nimport pytz\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nTIMEZONES = ['UTC', 'America/Los_Angeles', 'Europe/Paris', 'Asia/Kolkata', 'Australia/Sydney']\nCOLORS = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n\ndef plot_timezone_difference(start_time, end_time):\n    \"\"\"\n    Plot the difference in hours between the current timezone and various other timezones \n    for each day from start_time to end_time.\n\n    Parameters:\n    start_time (str): The start time in \"yyyy-mm-dd\" format.\n    end_time (str): The end time in \"yyyy-mm-dd\" format.\n\n    Returns:\n    None: Shows the plot.\n    \n    Requirements:\n    - datetime\n    - pytz\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_timezone_difference('2021-01-01', '2021-12-31')\n    \"\"\"\n    start_date = datetime.strptime(start_time, '%Y-%m-%d')\n    end_date = datetime.strptime(end_time, '%Y-%m-%d')\n\n    current_tz = pytz.timezone('UTC')\n\n    dates = np.arange(start_date, end_date, timedelta(days=1)).astype(datetime)\n    differences = []\n\n    for tz in TIMEZONES:\n        other_tz = pytz.timezone(tz)\n        difference = [(other_tz.localize(dt) - current_tz.localize(dt)).total_seconds() / 3600 for dt in dates]\n        differences.append(difference)\n\n    for i, difference in enumerate(differences):\n        plt.plot(dates, difference, color=COLORS[i % len(COLORS)], label=TIMEZONES[i])\n    \n    plt.xlabel('Date')\n    plt.ylabel('Time difference (hours)')\n    plt.legend()\n    plt.show()\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 3:\ndatetime, pandas, numpy, matplotlib\n```python\nfrom datetime import datetime\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['Time', 'Value']\n\ndef plot_time_series_with_trend(start_time, end_time, step, trend):\n    \"\"\"\n    Generate a time series with a given trend from start_time to end_time with a given step,\n    and plot the time series with the trend.\n\n    Parameters:\n    start_time (int): The start epoch time in milliseconds.\n    end_time (int): The end epoch time in milliseconds.\n    step (int): The step in milliseconds between each data point.\n    trend (float): The trend to be added to the time series.\n\n    Returns:\n    None: Shows the plot.\n    \n    Requirements:\n    - datetime\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_time_series_with_trend(0, 10000, 100, 0.001)\n    \"\"\"\n    timestamps = np.arange(start_time, end_time, step)\n    df = pd.DataFrame(columns=COLUMNS)\n    values = np.random.normal(size=len(timestamps))\n\n    for i, ts in enumerate(timestamps):\n        dt = datetime.fromtimestamp(ts/1000).strftime('%Y-%m-%d %H:%M:%S.%f')\n        value = values[i] + trend * i\n        df = df.append(dict(zip(COLUMNS, [dt, value])), ignore_index=True)\n    \n    df.plot(x='Time', y='Value')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ndatetime, re, pandas, csv\n```python\nfrom datetime import datetime\nimport re\nimport pandas as pd\nimport csv\n\n# Constants\nLOG_REGEX = r'^(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}.\\d{6}) - ([\\w\\s]+) - (.*)$'\nCOLUMNS = ['Timestamp', 'Level', 'Message']\n\ndef parse_log_file(file_path):\n    \"\"\"\n    Parse a log file with a specific format and return a pandas DataFrame.\n\n    Parameters:\n    file_path (str): The path to the log file.\n\n    Returns:\n    DataFrame: A pandas DataFrame with parsed log data.\n    \n    Requirements:\n    - datetime\n    - re\n    - pandas\n    - csv\n\n    Example:\n    >>> df = parse_log_file('log.txt')\n    >>> print(df)\n    \"\"\"\n    df = pd.DataFrame(columns=COLUMNS)\n\n    with open(file_path, 'r') as f:\n        reader = csv.reader(f)\n        for line in reader:\n            match = re.match(LOG_REGEX, line[0])\n            if match:\n                timestamp = datetime.strptime(match.group(1), '%Y-%m-%d %H:%M:%S.%f')\n                level = match.group(2)\n                message = match.group(3)\n                df = df.append(dict(zip(COLUMNS, [timestamp, level, message])), ignore_index=True)\n    \n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\ndatetime, pandas, numpy, matplotlib\n```python\nfrom datetime import datetime, timedelta\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['Timestamp', 'Value']\n\ndef plot_time_series_with_seasonality(start_time, end_time, step, amplitude, period):\n    \"\"\"\n    Generate a time series with a given seasonality from start_time to end_time with a given step,\n    and plot the time series with the seasonality.\n\n    Parameters:\n    start_time (int): The start epoch time in milliseconds.\n    end_time (int): The end epoch time in milliseconds.\n    step (int): The step in milliseconds between each data point.\n    amplitude (float): The amplitude of the seasonality.\n    period (int): The period of the seasonality in milliseconds.\n\n    Returns:\n    None: Shows the plot.\n    \n    Requirements:\n    - datetime\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_time_series_with_seasonality(0, 10000, 100, 1, 1000)\n    \"\"\"\n    timestamps = np.arange(start_time, end_time, step)\n    df = pd.DataFrame(columns=COLUMNS)\n    values = np.random.normal(size=len(timestamps))\n\n    for i, ts in enumerate(timestamps):\n        dt = datetime.fromtimestamp(ts/1000).strftime('%Y-%m-%d %H:%M:%S.%f')\n        value = values[i] + amplitude * np.sin(2 * np.pi * ts / period)\n        df = df.append(dict(zip(COLUMNS, [dt, value])), ignore_index=True)\n    \n    df.plot(x='Timestamp', y='Value')\n    plt.show()\n```\n"}
{"seed": "{\"task_id\": 21787496, \"prompt\": \"def f_21787496():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"time.strftime('%Y-%m-%d %H:%M:%S', time.gmtime(1236472051807 / 1000.0))\", \"test_start\": \"\\nimport time\\n\\ndef check(candidate): \", \"test\": [\"\\n    assert candidate() == '2009-03-08 00:27:31'\\n\"], \"entry_point\": \"f_21787496\", \"intent\": \"parse milliseconds epoch time '1236472051807' to format '%Y-%m-%d %H:%M:%S'\", \"library\": [\"time\"]}", "generation": "Scenario 1:\npandas, datetime, random, time\n```python\nimport pandas as pd\nfrom datetime import datetime, timedelta\nfrom random import randint\nimport time\n\n# Constants\nUSERS = ['user1', 'user2', 'user3', 'user4', 'user5']\nACTIVITIES = ['login', 'logout', 'purchase', 'browse']\n\ndef generate_user_activity_log(epoch_milliseconds):\n    \"\"\"\n    Generate a log of user activities for a list of users from a given epoch \n    milliseconds to current time. \n\n    Parameters:\n    epoch_milliseconds (int): The epoch milliseconds from where to start.\n\n    Returns:\n    DataFrame: A pandas DataFrame with user activities.\n\n    Requirements:\n    - pandas\n    - datetime\n    - random\n    - time\n\n    Example:\n    >>> log = generate_user_activity_log(1236472051807)\n    >>> print(log)\n    \"\"\"\n    log_data = []\n\n    start_time = datetime.fromtimestamp(epoch_milliseconds / 1000.0)\n    current_time = datetime.now()\n    time_diff = current_time - start_time\n    time_diff_seconds = int(time_diff.total_seconds())\n\n    for i in range(time_diff_seconds):\n        user = USERS[randint(0, len(USERS)-1)]\n        activity = ACTIVITIES[randint(0, len(ACTIVITIES)-1)]\n        activity_time = start_time + timedelta(seconds=i)\n        log_data.append([user, activity, activity_time])\n\n    log_df = pd.DataFrame(log_data, columns=['User', 'Activity', 'Time'])\n\n    return log_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ndatetime, time, random, matplotlib\n```python\nfrom datetime import datetime, timedelta\nimport time\nimport random\nimport matplotlib.pyplot as plt\n\n# Constants\nCATEGORIES = ['Electronics', 'Clothing', 'Home', 'Books', 'Sports']\n\ndef sales_trend(epoch_milliseconds):\n    \"\"\"\n    Generate and plot a sales trend for different categories from a given epoch \n    milliseconds to current time.\n\n    Parameters:\n    epoch_milliseconds (int): The epoch milliseconds from where to start.\n\n    Requirements:\n    - datetime\n    - time\n    - random\n    - matplotlib\n\n    Example:\n    >>> sales_trend(1236472051807)\n    \"\"\"\n    start_time = datetime.fromtimestamp(epoch_milliseconds / 1000.0)\n    current_time = datetime.now()\n    days_diff = (current_time - start_time).days\n\n    sales_data = {category: [0]*days_diff for category in CATEGORIES}\n\n    for i in range(days_diff):\n        for category in CATEGORIES:\n            sales = random.randint(10, 50)\n            sales_data[category][i] += sales\n\n    for category, sales in sales_data.items():\n        plt.plot(range(days_diff), sales, label=category)\n\n    plt.xlabel('Days since ' + start_time.strftime('%Y-%m-%d %H:%M:%S'))\n    plt.ylabel('Sales')\n    plt.legend()\n\n    return sales_data\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, datetime, random, time\n```python\nimport pandas as pd\nfrom datetime import datetime, timedelta\nimport random\nimport time\n\n# Constants\nPRODUCTS = ['Product1', 'Product2', 'Product3', 'Product4', 'Product5']\n\ndef generate_product_sales_data(epoch_milliseconds):\n    \"\"\"\n    Generate a sales data for a list of products from a given epoch milliseconds \n    to current time.\n\n    Parameters:\n    epoch_milliseconds (int): The epoch milliseconds from where to start.\n\n    Returns:\n    DataFrame: A pandas DataFrame with product sales data.\n\n    Requirements:\n    - pandas\n    - datetime\n    - random\n    - time\n\n    Example:\n    >>> sales_data = generate_product_sales_data(1236472051807)\n    >>> print(sales_data)\n    \"\"\"\n    sales_data = []\n\n    start_time = datetime.fromtimestamp(epoch_milliseconds / 1000.0)\n    current_time = datetime.now()\n    days_diff = (current_time - start_time).days\n\n    for i in range(days_diff):\n        for product in PRODUCTS:\n            sales = random.randint(10, 50)\n            sales_date = start_time + timedelta(days=i)\n            sales_data.append([product, sales_date, sales])\n\n    sales_df = pd.DataFrame(sales_data, columns=['Product', 'Date', 'Sales'])\n\n    return sales_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ndatetime, time, random, matplotlib\n```python\nfrom datetime import datetime, timedelta\nimport random\nimport time\nimport matplotlib.pyplot as plt\n\n# Constants\nTEAMS = ['Team1', 'Team2', 'Team3', 'Team4', 'Team5']\n\ndef team_performance_trend(epoch_milliseconds):\n    \"\"\"\n    Generate and plot a performance trend for different teams from a given epoch \n    milliseconds to current time.\n\n    Parameters:\n    epoch_milliseconds (int): The epoch milliseconds from where to start.\n\n    Requirements:\n    - datetime\n    - time\n    - random\n    - matplotlib\n\n    Example:\n    >>> team_performance_trend(1236472051807)\n    \"\"\"\n    start_time = datetime.fromtimestamp(epoch_milliseconds / 1000.0)\n    current_time = datetime.now()\n    days_diff = (current_time - start_time).days\n\n    performance_data = {team: [0]*days_diff for team in TEAMS}\n\n    for i in range(days_diff):\n        for team in TEAMS:\n            performance = random.uniform(0.1, 1)\n            performance_data[team][i] += performance\n\n    for team, performance in performance_data.items():\n        plt.plot(range(days_diff), performance, label=team)\n\n    plt.xlabel('Days since ' + start_time.strftime('%Y-%m-%d %H:%M:%S'))\n    plt.ylabel('Performance')\n    plt.legend()\n\n    return performance_data\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\ndatetime, time, random, pytz\n```python\nfrom datetime import datetime, timedelta\nimport random\nimport time\nimport pytz\n\n# Constants\nEVENTS = ['Event1', 'Event2', 'Event3', 'Event4', 'Event5']\nTIMEZONES = ['UTC', 'US/Eastern', 'Europe/London', 'Asia/Kolkata', 'Australia/Sydney']\n\ndef generate_event_schedule(epoch_milliseconds):\n    \"\"\"\n    Generate a schedule of events for different timezones from a given epoch \n    milliseconds to current time.\n\n    Parameters:\n    epoch_milliseconds (int): The epoch milliseconds from where to start.\n\n    Returns:\n    dict: A dictionary with event schedule.\n\n    Requirements:\n    - datetime\n    - time\n    - random\n    - pytz\n\n    Example:\n    >>> schedule = generate_event_schedule(1236472051807)\n    >>> print(schedule)\n    \"\"\"\n    schedule = {}\n\n    start_time = datetime.fromtimestamp(epoch_milliseconds / 1000.0)\n    current_time = datetime.now()\n    days_diff = (current_time - start_time).days\n\n    for i in range(days_diff):\n        for event in EVENTS:\n            timezone = random.choice(TIMEZONES)\n            event_time = (start_time + timedelta(days=i)).astimezone(pytz.timezone(timezone))\n            schedule[event] = {'date': event_time.date(), 'time': event_time.time(), 'timezone': timezone}\n\n    return schedule\n```\n"}
{"seed": "{\"task_id\": 20573459, \"prompt\": \"def f_20573459():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"(datetime.datetime.now() - datetime.timedelta(days=7)).date()\", \"test_start\": \"\\nimport datetime\\n\\ndef check(candidate): \", \"test\": [\"\\n    assert datetime.datetime.now().date() - candidate() < datetime.timedelta(days = 7, seconds = 1)\\n\", \"\\n    assert datetime.datetime.now().date() - candidate() >= datetime.timedelta(days = 7)\\n\"], \"entry_point\": \"f_20573459\", \"intent\": \"get the date 7 days before the current date\", \"library\": [\"datetime\"]}", "generation": "Scenario 1:\npandas, datetime, random, matplotlib\n```python\nimport pandas as pd\nfrom datetime import datetime, timedelta\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nCATEGORIES = ['Groceries', 'Entertainment', 'Rent', 'Utilities', 'Miscellaneous']\nDAYS_IN_PAST = 7\n\ndef generate_expenditure_report(days_in_past):\n    \"\"\"\n    Generate a report of daily expenditures for a list of categories over the past week.\n    \n    Parameters:\n    days_in_past (int): The number of days in the past for which to generate the report.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with daily expenditures for each category.\n    \n    Requirements:\n    - pandas\n    - datetime\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> report = generate_expenditure_report(DAYS_IN_PAST)\n    >>> print(report)\n    >>> report.plot(kind='bar')\n    \"\"\"\n    report_data = []\n    current_date = datetime.now().date()\n\n    for i in range(days_in_past):\n        date = current_date - timedelta(days=i)\n        for category in CATEGORIES:\n            expenditure = randint(0, 100)\n            report_data.append([date, category, expenditure])\n\n    report_df = pd.DataFrame(report_data, columns=['Date', 'Category', 'Expenditure'])\n    report_df = report_df.pivot(index='Date', columns='Category', values='Expenditure')\n\n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ndatetime, numpy, matplotlib\n```python\nfrom datetime import datetime, timedelta\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nDAYS_IN_PAST = 7\n\ndef plot_temperature_trend(days_in_past):\n    \"\"\"\n    Plot a graph of temperature trends for the past week using randomly generated data.\n    \n    Parameters:\n    days_in_past (int): The number of days in the past for which to generate the graph.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - datetime\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> plot_temperature_trend(DAYS_IN_PAST)\n    \"\"\"\n    dates = [datetime.now().date() - timedelta(days=i) for i in range(days_in_past)]\n    temperatures = np.random.randint(low=15, high=35, size=days_in_past)\n    \n    plt.plot(dates, temperatures)\n    plt.xlabel('Date')\n    plt.ylabel('Temperature (\u00b0C)')\n    plt.title('Temperature Trend')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ndatetime, pytz, calendar\n```python\nfrom datetime import datetime, timedelta\nimport pytz\nimport calendar\n\n# Constants\nDAYS_IN_PAST = 7\n\ndef get_weekday_of_date_days_ago(days_in_past):\n    \"\"\"\n    Get the weekday of the date 'days_in_past' ago.\n    \n    Parameters:\n    days_in_past (int): The number of days in the past.\n    \n    Returns:\n    str: The weekday of the date.\n    \n    Requirements:\n    - datetime\n    - pytz\n    - calendar\n    \n    Example:\n    >>> get_weekday_of_date_days_ago(DAYS_IN_PAST)\n    \"\"\"\n    date = datetime.now(pytz.UTC) - timedelta(days=days_in_past)\n    weekday = calendar.day_name[date.weekday()]\n    \n    return weekday\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ndatetime, pandas, random, seaborn\n```python\nfrom datetime import datetime, timedelta\nimport pandas as pd\nimport random\nimport seaborn as sns\n\n# Constants\nDAYS_IN_PAST = 7\nACTIVITIES = ['Running', 'Swimming', 'Cycling', 'Yoga', 'Weight Training']\n\ndef plot_activity_duration(days_in_past):\n    \"\"\"\n    Plot a graph of activity durations for the past week using randomly generated data.\n    \n    Parameters:\n    days_in_past (int): The number of days in the past for which to generate the graph.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - datetime\n    - pandas\n    - random\n    - seaborn\n    \n    Example:\n    >>> plot_activity_duration(DAYS_IN_PAST)\n    \"\"\"\n    data = []\n    for i in range(days_in_past):\n        date = datetime.now().date() - timedelta(days=i)\n        for activity in ACTIVITIES:\n            duration = random.randint(0, 120)\n            data.append([date, activity, duration])\n    \n    df = pd.DataFrame(data, columns=['Date', 'Activity', 'Duration'])\n    sns.lineplot(data=df, x='Date', y='Duration', hue='Activity')\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\ndatetime, pandas, numpy\n```python\nfrom datetime import datetime, timedelta\nimport pandas as pd\nimport numpy as np\n\n# Constants\nDAYS_IN_PAST = 7\nSTOCK_NAMES = ['AAPL', 'GOOGL', 'MSFT', 'AMZN', 'FB']\n\ndef generate_stock_prices(days_in_past):\n    \"\"\"\n    Generate a DataFrame of stock prices for the past week using randomly generated data.\n    \n    Parameters:\n    days_in_past (int): The number of days in the past.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with stock prices.\n    \n    Requirements:\n    - datetime\n    - pandas\n    - numpy\n    \n    Example:\n    >>> generate_stock_prices(DAYS_IN_PAST)\n    \"\"\"\n    dates = pd.date_range(end=datetime.now().date(), periods=days_in_past)\n    prices = np.random.rand(days_in_past, len(STOCK_NAMES)) * 100\n    df = pd.DataFrame(prices, columns=STOCK_NAMES, index=dates)\n    \n    return df\n```\n"}
{"seed": "{\"task_id\": 15352457, \"prompt\": \"def f_15352457(column, data):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"sum(row[column] for row in data)\", \"test_start\": \"\\ndef check(candidate): \", \"test\": [\"\\n    assert candidate(1, [[1,2,3], [4,5,6]]) == 7\\n\", \"\\n    assert candidate(0, [[1,1,1], [0,1,1]]) == 1\\n\", \"\\n    assert candidate(5, [[1,1,1,1,1,2], [0,1,1,1,1,1,1,1,1,1,1]]) == 3\\n\", \"\\n    assert candidate(0, [[1],[2],[3],[4]]) == 10\\n\"], \"entry_point\": \"f_15352457\", \"intent\": \"sum elements at index `column` of each list in list `data`\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, datetime, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['Date', 'Temperature', 'Humidity', 'Wind Speed', 'Precipitation']\n\ndef analyze_weather_data(column, data):\n    \"\"\"\n    Analyze a list of weather data, calculate the sum, mean, min, max of a specific column, \n    and plot its histogram.\n\n    Parameters:\n    column (str): The column to analyze.\n    data (list): The weather data.\n\n    Returns:\n    dict: A dictionary with the sum, mean, min, max of the column.\n\n    Requirements:\n    - pandas\n    - numpy\n    - datetime\n    - matplotlib.pyplot\n\n    Example:\n    >>> data = [[datetime(2022, 1, 1), -5, 80, 10, 0], [datetime(2022, 1, 2), -3, 85, 12, 0.5], [datetime(2022, 1, 3), -2, 83, 15, 0]]\n    >>> analyze_weather_data('Temperature', data)\n    \"\"\"\n    df = pd.DataFrame(data, columns=COLUMNS)\n    column_data = df[column]\n\n    result = {\n        'sum': np.sum(column_data),\n        'mean': np.mean(column_data),\n        'min': np.min(column_data),\n        'max': np.max(column_data)\n    }\n\n    plt.hist(column_data)\n    plt.title(f'Histogram of {column}')\n    plt.show()\n\n    return result\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, datetime, json\n```python\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nimport json\n\n# Constants\nCOLUMNS = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n\ndef analyze_stock_data(column, data):\n    \"\"\"\n    Analyze a list of stock data, calculate the sum, mean, min, max of a specific column, \n    and save the result as a JSON file.\n\n    Parameters:\n    column (str): The column to analyze.\n    data (list): The stock data.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - numpy\n    - datetime\n    - json\n\n    Example:\n    >>> data = [[datetime(2022, 1, 1), 100, 105, 95, 102, 10000], [datetime(2022, 1, 2), 102, 108, 100, 105, 15000], [datetime(2022, 1, 3), 105, 110, 103, 108, 20000]]\n    >>> analyze_stock_data('Open', data)\n    \"\"\"\n    df = pd.DataFrame(data, columns=COLUMNS)\n    column_data = df[column]\n\n    result = {\n        'sum': np.sum(column_data),\n        'mean': np.mean(column_data),\n        'min': np.min(column_data),\n        'max': np.max(column_data)\n    }\n\n    with open(f'{column}_analysis.json', 'w') as f:\n        json.dump(result, f)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, random, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['Age', 'Salary', 'Experience']\n\ndef analyze_employee_data(column, data):\n    \"\"\"\n    Analyze a list of employee data, calculate the sum, mean, min, max of a specific column, \n    and plot its pie chart.\n\n    Parameters:\n    column (str): The column to analyze.\n    data (list): The employee data.\n\n    Returns:\n    dict: A dictionary with the sum, mean, min, max of the column.\n\n    Requirements:\n    - pandas\n    - numpy\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> data = [[25, 50000, 2], [30, 75000, 5], [35, 100000, 7], [40, 125000, 10], [45, 150000, 12]]\n    >>> analyze_employee_data('Salary', data)\n    \"\"\"\n    df = pd.DataFrame(data, columns=COLUMNS)\n    column_data = df[column]\n\n    result = {\n        'sum': np.sum(column_data),\n        'mean': np.mean(column_data),\n        'min': np.min(column_data),\n        'max': np.max(column_data)\n    }\n\n    plt.pie(column_data, labels=df['Age'], autopct='%1.1f%%')\n    plt.title(f'Pie Chart of {column}')\n    plt.show()\n\n    return result\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, random, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['Product', 'Quantity Sold', 'Total Sales']\n\ndef analyze_sales_data(column, data):\n    \"\"\"\n    Analyze a list of sales data, calculate the sum, mean, min, max of a specific column, \n    and plot its bar chart.\n\n    Parameters:\n    column (str): The column to analyze.\n    data (list): The sales data.\n\n    Returns:\n    dict: A dictionary with the sum, mean, min, max of the column.\n\n    Requirements:\n    - pandas\n    - numpy\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> data = [['Product A', 100, 10000], ['Product B', 150, 15000], ['Product C', 200, 20000], ['Product D', 250, 25000], ['Product E', 300, 30000]]\n    >>> analyze_sales_data('Total Sales', data)\n    \"\"\"\n    df = pd.DataFrame(data, columns=COLUMNS)\n    column_data = df[column]\n\n    result = {\n        'sum': np.sum(column_data),\n        'mean': np.mean(column_data),\n        'min': np.min(column_data),\n        'max': np.max(column_data)\n    }\n\n    df.plot.bar(x='Product', y=column)\n    plt.title(f'Bar Chart of {column}')\n    plt.show()\n\n    return result\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, datetime, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['Date', 'Steps', 'Calories Burned', 'Distance Walked']\n\ndef analyze_fitness_data(column, data):\n    \"\"\"\n    Analyze a list of fitness data, calculate the sum, mean, min, max of a specific column, \n    and plot its line chart.\n\n    Parameters:\n    column (str): The column to analyze.\n    data (list): The fitness data.\n\n    Returns:\n    dict: A dictionary with the sum, mean, min, max of the column.\n\n    Requirements:\n    - pandas\n    - numpy\n    - datetime\n    - matplotlib.pyplot\n\n    Example:\n    >>> data = [[datetime(2022, 1, 1), 5000, 200, 3.5], [datetime(2022, 1, 2), 5500, 220, 4.0], [datetime(2022, 1, 3), 6000, 240, 4.5]]\n    >>> analyze_fitness_data('Steps', data)\n    \"\"\"\n    df = pd.DataFrame(data, columns=COLUMNS)\n    column_data = df[column]\n\n    result = {\n        'sum': np.sum(column_data),\n        'mean': np.mean(column_data),\n        'min': np.min(column_data),\n        'max': np.max(column_data)\n    }\n\n    df.plot.line(x='Date', y=column)\n    plt.title(f'Line Chart of {column}')\n    plt.show()\n\n    return result\n```\n"}
{"seed": "{\"task_id\": 15352457, \"prompt\": \"def f_15352457(array):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"[sum(row[i] for row in array) for i in range(len(array[0]))]\", \"test_start\": \"\\ndef check(candidate): \", \"test\": [\"\\n    assert candidate([[1,2,3], [4,5,6]]) == [5, 7, 9]\\n\", \"\\n    assert candidate([[1,1,1], [0,1,1]]) == [1, 2, 2]\\n\", \"\\n    assert candidate([[1,1,1,1,1,2], [0,1,1,1,1,1,1,1,1,1,1]]) == [1, 2, 2, 2, 2, 3]\\n\", \"\\n    assert candidate([[1],[2],[3],[4]]) == [10]\\n\"], \"entry_point\": \"f_15352457\", \"intent\": \"sum columns of a list `array`\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef generate_dataframe_and_plot(array):\n    \"\"\"\n    Generate a Pandas DataFrame from a 2D list and plot the sum of each column.\n\n    Parameters:\n    array (list of list of int): The 2D list representing the data.\n\n    Returns:\n    DataFrame, Figure: A pandas DataFrame with the data and a matplotlib Figure.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> df, fig = generate_dataframe_and_plot([[1,2,3,4,5], [6,7,8,9,10]])\n    >>> print(df)\n    >>> print(fig)\n    \"\"\"\n    df = pd.DataFrame(array, columns=COLUMNS)\n    sums = df.sum()\n\n    fig, ax = plt.subplots()\n    sums.plot(kind='bar', ax=ax)\n\n    return df, fig\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef generate_dataframe_and_heatmap(array):\n    \"\"\"\n    Generate a Pandas DataFrame from a 2D list and plot a heatmap of the correlation matrix.\n\n    Parameters:\n    array (list of list of int): The 2D list representing the data.\n\n    Returns:\n    DataFrame, Figure: A pandas DataFrame with the data and a seaborn heatmap.\n\n    Requirements:\n    - pandas\n    - numpy\n    - seaborn\n\n    Example:\n    >>> df, heatmap = generate_dataframe_and_heatmap([[1,2,3,4,5], [6,7,8,9,10]])\n    >>> print(df)\n    >>> print(heatmap)\n    \"\"\"\n    df = pd.DataFrame(array, columns=COLUMNS)\n    correlation_matrix = df.corr()\n\n    heatmap = sns.heatmap(correlation_matrix, annot=True)\n\n    return df, heatmap\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, statsmodels\n```python\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'Response']\n\ndef generate_dataframe_and_regression(array):\n    \"\"\"\n    Generate a Pandas DataFrame from a 2D list and run a multiple linear regression.\n\n    Parameters:\n    array (list of list of int): The 2D list representing the data.\n\n    Returns:\n    DataFrame, RegressionResults: A pandas DataFrame with the data and a statsmodels RegressionResults object.\n\n    Requirements:\n    - pandas\n    - numpy\n    - statsmodels.api\n\n    Example:\n    >>> df, results = generate_dataframe_and_regression([[1,2,3,4,5], [6,7,8,9,10]])\n    >>> print(df)\n    >>> print(results.summary())\n    \"\"\"\n    df = pd.DataFrame(array, columns=COLUMNS)\n    X = df[COLUMNS[:-1]]\n    y = df['Response']\n    X = sm.add_constant(X) # Adding a constant\n\n    model = sm.OLS(y, X)\n    results = model.fit()\n\n    return df, results\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, sklearn\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.decomposition import PCA\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef generate_dataframe_and_pca(array):\n    \"\"\"\n    Generate a Pandas DataFrame from a 2D list and perform PCA.\n\n    Parameters:\n    array (list of list of int): The 2D list representing the data.\n\n    Returns:\n    DataFrame, ndarray: A pandas DataFrame with the data and a numpy ndarray with the transformed data.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.decomposition\n\n    Example:\n    >>> df, transformed_data = generate_dataframe_and_pca([[1,2,3,4,5], [6,7,8,9,10]])\n    >>> print(df)\n    >>> print(transformed_data)\n    \"\"\"\n    df = pd.DataFrame(array, columns=COLUMNS)\n\n    pca = PCA(n_components=2)\n    transformed_data = pca.fit_transform(df)\n\n    return df, transformed_data\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, scipy\n```python\nimport pandas as pd\nimport numpy as np\nfrom scipy.spatial.distance import pdist, squareform\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef generate_dataframe_and_distance_matrix(array):\n    \"\"\"\n    Generate a Pandas DataFrame from a 2D list and compute a distance matrix.\n\n    Parameters:\n    array (list of list of int): The 2D list representing the data.\n\n    Returns:\n    DataFrame, DataFrame: A pandas DataFrame with the data and a pandas DataFrame with the distance matrix.\n\n    Requirements:\n    - pandas\n    - numpy\n    - scipy.spatial.distance\n\n    Example:\n    >>> df, distance_matrix = generate_dataframe_and_distance_matrix([[1,2,3,4,5], [6,7,8,9,10]])\n    >>> print(df)\n    >>> print(distance_matrix)\n    \"\"\"\n    df = pd.DataFrame(array, columns=COLUMNS)\n\n    distances = pdist(df.values, metric='euclidean')\n    distance_matrix = pd.DataFrame(squareform(distances), index=df.index, columns=df.index)\n\n    return df, distance_matrix\n```\n"}
{"seed": "{\"task_id\": 23164058, \"prompt\": \"def f_23164058():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"base64.b64encode(bytes('your string', 'utf-8'))\", \"test_start\": \"\\nimport base64\\n\\ndef check(candidate): \", \"test\": [\"\\n    assert candidate() == b'eW91ciBzdHJpbmc='\\n\"], \"entry_point\": \"f_23164058\", \"intent\": \"encode binary string 'your string' to base64 code\", \"library\": [\"base64\"]}", "generation": "Scenario 1:\nbase64, hashlib, binascii, os\n```python\nimport base64\nimport hashlib\nimport binascii\nimport os\n\n# Constants\nSALT_LENGTH = 8\n\ndef encrypt_password(password):\n    \"\"\"\n    Encrypt a password using a salt and SHA-256, then encode the result in base64.\n\n    Parameters:\n    password (str): The password to be encrypted.\n\n    Returns:\n    str: The encrypted password in base64.\n\n    Requirements:\n    - base64\n    - hashlib\n    - binascii\n    - os\n\n    Example:\n    >>> encrypt_password('my_password')\n    \"\"\"\n    # Generate a random salt\n    salt = os.urandom(SALT_LENGTH)\n    # Use the salt and the password to create a SHA-256 hash\n    hash = hashlib.pbkdf2_hmac('sha256', password.encode('utf-8'), salt, 100000)\n    # Combine the salt and the hash\n    salted_hash = salt + hash\n    # Encode the salted hash in base64\n    encrypted_password = base64.b64encode(salted_hash)\n\n    return encrypted_password.decode('utf-8')\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nbase64, zlib, random, string\n```python\nimport base64\nimport zlib\nimport random\nimport string\n\n# Constants\nSTRING_LENGTH = 100\n\ndef generate_compressed_string():\n    \"\"\"\n    Generate a random string of a certain length, compress it using zlib and \n    then encode the compressed string in base64.\n\n    Returns:\n    str: The compressed string in base64.\n\n    Requirements:\n    - base64\n    - zlib\n    - random\n    - string\n\n    Example:\n    >>> compressed_string = generate_compressed_string()\n    >>> print(compressed_string)\n    \"\"\"\n    # Generate a random string\n    random_string = ''.join(random.choices(string.ascii_uppercase + string.digits, k=STRING_LENGTH))\n    # Compress the string\n    compressed_string = zlib.compress(random_string.encode('utf-8'))\n    # Encode the compressed string in base64\n    encoded_compressed_string = base64.b64encode(compressed_string)\n\n    return encoded_compressed_string.decode('utf-8')\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nbase64, pandas, io\n```python\nimport base64\nimport pandas as pd\nimport io\n\ndef encode_dataframe(df):\n    \"\"\"\n    Encode a pandas DataFrame as a base64 string.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame to be encoded.\n\n    Returns:\n    str: The base64 string.\n\n    Requirements:\n    - base64\n    - pandas\n    - io\n\n    Example:\n    >>> df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n    >>> encoded_df = encode_dataframe(df)\n    >>> print(encoded_df)\n    \"\"\"\n    # Convert DataFrame to CSV\n    csv = df.to_csv(index=False)\n    # Convert CSV to bytes\n    csv_bytes = csv.encode('utf-8')\n    # Encode bytes to base64\n    base64_bytes = base64.b64encode(csv_bytes)\n    # Convert base64 bytes to string\n    base64_string = base64_bytes.decode('utf-8')\n\n    return base64_string\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nbase64, json, requests, random\n```python\nimport base64\nimport json\nimport requests\nimport random\n\n# Constants\nURL = 'https://api.url.com/endpoint'\n\ndef send_encoded_data(data):\n    \"\"\"\n    Encode a python dictionary in base64 and send it as a POST request to a specified URL.\n\n    Parameters:\n    data (dict): The data to be sent.\n\n    Returns:\n    dict: The response from the server.\n\n    Requirements:\n    - base64\n    - json\n    - requests\n    - random\n\n    Example:\n    >>> data = {\"key1\": \"value1\", \"key2\": \"value2\"}\n    >>> response = send_encoded_data(data)\n    >>> print(response)\n    \"\"\"\n    # Convert dict to JSON\n    json_data = json.dumps(data)\n    # Convert JSON to bytes\n    bytes_data = json_data.encode('utf-8')\n    # Encode bytes in base64\n    base64_data = base64.b64encode(bytes_data)\n    # Send POST request\n    headers = {'Content-Type': 'application/json'}\n    response = requests.post(URL, data=base64_data, headers=headers)\n\n    return response.json()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nbase64, PIL, io\n```python\nfrom PIL import Image\nimport base64\nimport io\n\ndef encode_image(image_path):\n    \"\"\"\n    Encode an image in base64.\n\n    Parameters:\n    image_path (str): The path to the image.\n\n    Returns:\n    str: The base64 string.\n\n    Requirements:\n    - base64\n    - PIL (Pillow)\n    - io\n\n    Example:\n    >>> encoded_image = encode_image('path/to/image.jpg')\n    >>> print(encoded_image)\n    \"\"\"\n    # Open the image and convert it to RGB\n    image = Image.open(image_path).convert('RGB')\n    # Save the image to a BytesIO object\n    bytes_io = io.BytesIO()\n    image.save(bytes_io, format='JPEG')\n    # Get the bytes value of the BytesIO object\n    bytes_value = bytes_io.getvalue()\n    # Encode the bytes in base64\n    base64_image = base64.b64encode(bytes_value)\n\n    return base64_image.decode('utf-8')\n```"}
{"seed": "{\"task_id\": 11533274, \"prompt\": \"def f_11533274(dicts):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"dict((k, [d[k] for d in dicts]) for k in dicts[0])\", \"test_start\": \"\\ndef check(candidate): \", \"test\": [\"\\n    assert candidate([{'cat': 1, 'dog': 3}, {'cat' : 2, 'dog': ['happy']}]) ==         {'cat': [1, 2], 'dog': [3, ['happy']]}\\n\", \"\\n    assert candidate([{'cat': 1}, {'cat' : 2}]) != {'cat': 3}\\n\"], \"entry_point\": \"f_11533274\", \"intent\": \"combine list of dictionaries `dicts` with the same keys in each list to a single dictionary\", \"library\": []}", "generation": "Scenario 1:\npandas, matplotlib, seaborn, numpy\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Sample data\n# data = [{'apple': 10, 'banana': 15, 'cherry': 12}, {'apple': 12, 'banana': 20, 'cherry': 14}, \n#         {'apple': 15, 'banana': 18, 'cherry': 15}, {'apple': 11, 'banana': 17, 'cherry': 13}]\n\ndef visualize_fruit_sales(data):\n    \"\"\"\n    Combine a list of dictionaries with the same keys (fruit names) into a single dictionary, \n    transform it into a pandas DataFrame, and generate a line plot of the sales.\n    \n    Parameters:\n    data (list): A list of dictionaries. The keys are fruit names and the values are sales quantities.\n    \n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - seaborn\n    - numpy\n\n    Example:\n    >>> visualize_fruit_sales([{'apple': 10, 'banana': 15, 'cherry': 12}, \n                               {'apple': 12, 'banana': 20, 'cherry': 14}, \n                               {'apple': 15, 'banana': 18, 'cherry': 15}, \n                               {'apple': 11, 'banana': 17, 'cherry': 13}])\n    \"\"\"\n    df = pd.DataFrame(data)\n    sns.set_style('darkgrid')\n    plt.figure(figsize=(10, 6))\n    for fruit in df.columns:\n        plt.plot(df[fruit], label=fruit)\n    plt.xlabel('Time')\n    plt.ylabel('Sales Quantity')\n    plt.title('Fruit Sales over Time')\n    plt.legend()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncollections, itertools, matplotlib, pandas\n```python\nimport collections\nimport itertools\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Sample data\n# data = [{'apple': 10, 'banana': 15, 'cherry': 12}, {'apple': 12, 'banana': 20, 'cherry': 14}, \n#         {'apple': 15, 'banana': 18, 'cherry': 15}, {'apple': 11, 'banana': 17, 'cherry': 13}]\n\ndef analyze_fruit_sales(data):\n    \"\"\"\n    Combine a list of dictionaries with the same keys (fruit names) into a single dictionary, \n    calculate the total sales for each fruit, and plot a bar chart.\n    \n    Parameters:\n    data (list): A list of dictionaries. The keys are fruit names and the values are sales quantities.\n    \n    Returns:\n    None\n\n    Requirements:\n    - collections\n    - itertools\n    - matplotlib.pyplot\n    - pandas\n\n    Example:\n    >>> analyze_fruit_sales([{'apple': 10, 'banana': 15, 'cherry': 12}, \n                             {'apple': 12, 'banana': 20, 'cherry': 14}, \n                             {'apple': 15, 'banana': 18, 'cherry': 15}, \n                             {'apple': 11, 'banana': 17, 'cherry': 13}])\n    \"\"\"\n    combined_dict = dict((k, [d[k] for d in data]) for k in data[0])\n    total_sales = {k: sum(v) for k, v in combined_dict.items()}\n    total_sales = collections.OrderedDict(sorted(total_sales.items()))\n    labels, values = zip(*total_sales.items())\n    \n    plt.bar(labels, values, color=['red', 'yellow', 'green', 'blue', 'purple'])\n    plt.xlabel('Fruit')\n    plt.ylabel('Total Sales')\n    plt.title('Total Fruit Sales')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, matplotlib, seaborn, collections\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport collections\n\n# Sample data\n# data = [{'John': 5, 'Jane': 10, 'Joe': 7}, {'John': 6, 'Jane': 8, 'Joe': 10}, \n#         {'John': 5, 'Jane': 9, 'Joe': 8}, {'John': 7, 'Jane': 10, 'Joe': 9}]\n\ndef visualize_student_scores(data):\n    \"\"\"\n    Combine a list of dictionaries with the same keys (student names) into a single dictionary, \n    transform it into a pandas DataFrame, and generate a line plot of the scores.\n    \n    Parameters:\n    data (list): A list of dictionaries. The keys are student names and the values are scores.\n    \n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - seaborn\n    - collections\n\n    Example:\n    >>> visualize_student_scores([{'John': 5, 'Jane': 10, 'Joe': 7}, \n                                  {'John': 6, 'Jane': 8, 'Joe': 10}, \n                                  {'John': 5, 'Jane': 9, 'Joe': 8}, \n                                  {'John': 7, 'Jane': 10, 'Joe': 9}])\n    \"\"\"\n    df = pd.DataFrame(data)\n    sns.set_style('darkgrid')\n    plt.figure(figsize=(10, 6))\n    for student in df.columns:\n        plt.plot(df[student], label=student)\n    plt.xlabel('Test Number')\n    plt.ylabel('Score')\n    plt.title('Student Scores over Tests')\n    plt.legend()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ncollections, itertools, matplotlib, pandas\n```python\nimport collections\nimport itertools\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Sample data\n# data = [{'John': 5, 'Jane': 10, 'Joe': 7}, {'John': 6, 'Jane': 8, 'Joe': 10}, \n#         {'John': 5, 'Jane': 9, 'Joe': 8}, {'John': 7, 'Jane': 10, 'Joe': 9}]\n\ndef analyze_student_scores(data):\n    \"\"\"\n    Combine a list of dictionaries with the same keys (student names) into a single dictionary, \n    calculate the average score for each student, and plot a bar chart.\n    \n    Parameters:\n    data (list): A list of dictionaries. The keys are student names and the values are scores.\n    \n    Returns:\n    None\n\n    Requirements:\n    - collections\n    - itertools\n    - matplotlib.pyplot\n    - pandas\n\n    Example:\n    >>> analyze_student_scores([{'John': 5, 'Jane': 10, 'Joe': 7}, \n                                {'John': 6, 'Jane': 8, 'Joe': 10}, \n                                {'John': 5, 'Jane': 9, 'Joe': 8}, \n                                {'John': 7, 'Jane': 10, 'Joe': 9}])\n    \"\"\"\n    combined_dict = dict((k, [d[k] for d in data]) for k in data[0])\n    avg_scores = {k: sum(v)/len(v) for k, v in combined_dict.items()}\n    avg_scores = collections.OrderedDict(sorted(avg_scores.items()))\n    labels, values = zip(*avg_scores.items())\n    \n    plt.bar(labels, values, color=['red', 'yellow', 'green', 'blue', 'purple'])\n    plt.xlabel('Student')\n    plt.ylabel('Average Score')\n    plt.title('Average Student Scores')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, matplotlib, seaborn, numpy\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Sample data\n# data = [{'A': 10, 'B': 15, 'C': 12}, {'A': 12, 'B': 20, 'C': 14}, \n#         {'A': 15, 'B': 18, 'C': 15}, {'A': 11, 'B': 17, 'C': 13}]\n\ndef visualize_data(data):\n    \"\"\"\n    Combine a list of dictionaries with the same keys into a single dictionary, \n    transform it into a pandas DataFrame, and generate a line plot of the data.\n    \n    Parameters:\n    data (list): A list of dictionaries. The keys are labels and the values are data points.\n    \n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - seaborn\n    - numpy\n\n    Example:\n    >>> visualize_data([{'A': 10, 'B': 15, 'C': 12}, \n                        {'A': 12, 'B': 20, 'C': 14}, \n                        {'A': 15, 'B': 18, 'C': 15}, \n                        {'A': 11, 'B': 17, 'C': 13}])\n    \"\"\"\n    df = pd.DataFrame(data)\n    sns.set_style('darkgrid')\n    plt.figure(figsize=(10, 6))\n    for label in df.columns:\n        plt.plot(df[label], label=label)\n    plt.xlabel('Time')\n    plt.ylabel('Data Points')\n    plt.title('Data over Time')\n    plt.legend()\n    plt.show()\n```"}
{"seed": "{\"task_id\": 11533274, \"prompt\": \"def f_11533274(dicts):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"{k: [d[k] for d in dicts] for k in dicts[0]}\", \"test_start\": \"\\ndef check(candidate): \", \"test\": [\"\\n    assert candidate([{'cat': 1, 'dog': 3}, {'cat' : 2, 'dog': ['happy']}]) ==         {'cat': [1, 2], 'dog': [3, ['happy']]}\\n\", \"\\n    assert candidate([{'cat': 1}, {'cat' : 2}]) != {'cat': 3}\\n\"], \"entry_point\": \"f_11533274\", \"intent\": \"Merge a nested dictionary `dicts` into a flat dictionary by concatenating nested values with the same key `k`\", \"library\": []}", "generation": "Scenario 1:\npandas, numpy, collections, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\n\n# Sample data\nDATA = [{'cat': 1, 'dog': 3}, {'cat' : 2, 'dog': 5}, {'cat' : 3, 'dog': 7}]\n\ndef stats_visualization(data):\n    \"\"\"\n    Calculate statistical measures (mean and standard deviation) of the values \n    associated with each key in a list of dictionaries and visualize them with bar plots.\n\n    Parameters:\n    data (list): The list of dictionaries.\n\n    Returns:\n    dict: A dictionary with keys and their corresponding mean and standard deviation.\n\n    Requirements:\n    - pandas\n    - numpy\n    - collections\n    - matplotlib.pyplot\n\n    Example:\n    >>> stats = stats_visualization(DATA)\n    >>> print(stats)\n    >>> for key in stats:\n    >>>     plt.bar(x=['mean', 'std'], height=stats[key])\n    >>>     plt.title(f'Statistics of {key}')\n    >>>     plt.show()\n    \"\"\"\n    stats = defaultdict(list)\n    for d in data:\n        for key, value in d.items():\n            stats[key].append(value)\n    \n    result = {k: {'mean': np.mean(v), 'std': np.std(v)} for k, v in stats.items()}\n\n    return result\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, collections, json, csv\n```python\nimport numpy as np\nfrom collections import defaultdict\nimport json\nimport csv\n\n# Constants\nINPUT_FILE = 'data.json'\nOUTPUT_FILE = 'stats.csv'\n\ndef calculate_stats(input_file, output_file):\n    \"\"\"\n    Read a list of dictionaries from a JSON file, calculate the mean and median \n    for each key, and write the results to a CSV file.\n\n    Parameters:\n    input_file (str): The input JSON file name.\n    output_file (str): The output CSV file name.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - collections\n    - json\n    - csv\n\n    Example:\n    >>> calculate_stats(INPUT_FILE, OUTPUT_FILE)\n    \"\"\"\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n    \n    stats = defaultdict(list)\n    for d in data:\n        for key, value in d.items():\n            stats[key].append(value)\n    \n    result = {k: {'mean': np.mean(v), 'median': np.median(v)} for k, v in stats.items()}\n\n    with open(output_file, 'w', newline='') as f:\n        writer = csv.DictWriter(f, fieldnames=['key', 'mean', 'median'])\n        writer.writeheader()\n        for key, values in result.items():\n            writer.writerow({'key': key, 'mean': values['mean'], 'median': values['median']})\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, collections, json, matplotlib\n```python\nimport numpy as np\nfrom collections import defaultdict\nimport json\nimport matplotlib.pyplot as plt\n\n# Constants\nINPUT_FILE = 'data.json'\n\ndef visualize_stats(input_file):\n    \"\"\"\n    Read a list of dictionaries from a JSON file, calculate the mean and median \n    for each key, and visualize the results with bar plots.\n\n    Parameters:\n    input_file (str): The input JSON file name.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - collections\n    - json\n    - matplotlib.pyplot\n\n    Example:\n    >>> visualize_stats(INPUT_FILE)\n    \"\"\"\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n    \n    stats = defaultdict(list)\n    for d in data:\n        for key, value in d.items():\n            stats[key].append(value)\n    \n    result = {k: {'mean': np.mean(v), 'median': np.median(v)} for k, v in stats.items()}\n\n    for key, values in result.items():\n        plt.bar(['mean', 'median'], [values['mean'], values['median']])\n        plt.title(f'Statistics of {key}')\n        plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, collections, json, pandas\n```python\nimport numpy as np\nfrom collections import defaultdict\nimport json\nimport pandas as pd\n\n# Constants\nINPUT_FILE = 'data.json'\n\ndef stats_to_df(input_file):\n    \"\"\"\n    Read a list of dictionaries from a JSON file, calculate the mean and median \n    for each key, and convert the results to a pandas DataFrame.\n\n    Parameters:\n    input_file (str): The input JSON file name.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the statistics.\n\n    Requirements:\n    - numpy\n    - collections\n    - json\n    - pandas\n\n    Example:\n    >>> df = stats_to_df(INPUT_FILE)\n    >>> print(df)\n    \"\"\"\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n    \n    stats = defaultdict(list)\n    for d in data:\n        for key, value in d.items():\n            stats[key].append(value)\n    \n    result = {k: {'mean': np.mean(v), 'median': np.median(v)} for k, v in stats.items()}\n\n    df = pd.DataFrame(result).transpose()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, collections, json, seaborn\n```python\nimport numpy as np\nfrom collections import defaultdict\nimport json\nimport seaborn as sns\n\n# Constants\nINPUT_FILE = 'data.json'\n\ndef stats_visualization(input_file):\n    \"\"\"\n    Read a list of dictionaries from a JSON file, calculate the mean and median \n    for each key, and visualize the results with a seaborn boxplot.\n\n    Parameters:\n    input_file (str): The input JSON file name.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - collections\n    - json\n    - seaborn\n\n    Example:\n    >>> stats_visualization(INPUT_FILE)\n    \"\"\"\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n    \n    stats = defaultdict(list)\n    for d in data:\n        for key, value in d.items():\n            stats[key].append(value)\n    \n    result = {k: {'mean': np.mean(v), 'median': np.median(v)} for k, v in stats.items()}\n\n    sns.boxplot(x=list(result.keys()), y=[list(values.values()) for values in result.values()])\n    plt.show()\n```\n"}
{"seed": "{\"task_id\": 14026704, \"prompt\": \"def f_14026704(request):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"request.args['myParam']\", \"test_start\": \"\\nimport multidict\\n\\nclass Request:\\n        def __init__(self, args):\\n            self.args = args\\n\\ndef check(candidate): \", \"test\": [\"\\n    args = multidict.MultiDict([('myParam' , 'popeye')])\\n    request = Request(args)\\n    assert candidate(request) == 'popeye'\\n\"], \"entry_point\": \"f_14026704\", \"intent\": \"get the url parameter 'myParam' in a Flask view\", \"library\": [\"multidict\"]}", "generation": "Scenario 1: \nmultidict, flask, json, requests\n```python\nimport multidict\nfrom flask import Flask, request, jsonify\nimport json\nimport requests\n\n# Constants\nHEADERS = {'Content-type': 'application/json'}\n\ndef get_and_forward_args():\n    \"\"\"\n    Get the url parameters and forward them to another API endpoint using POST method.\n    \n    Requirements:\n    - multidict\n    - flask\n    - json\n    - requests\n    \n    Example:\n    >>> app = Flask(__name__)\n    >>> @app.route('/api/v1/resource', methods=['GET'])\n    >>> def api_resource():\n    ...     return get_and_forward_args()\n    \"\"\"\n    args = request.args\n    response = requests.post('http://another.api/endpoint', headers=HEADERS, data=json.dumps(dict(args)))\n    return jsonify(response.json())\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nmultidict, flask, sqlite3, pandas\n```python\nimport multidict\nfrom flask import Flask, request\nimport sqlite3\nimport pandas as pd\n\ndef get_and_db_store_args():\n    \"\"\"\n    Get the URL parameters and store them in an SQLite database.\n    \n    Requirements:\n    - multidict\n    - flask\n    - sqlite3\n    - pandas\n    \n    Example:\n    >>> app = Flask(__name__)\n    >>> @app.route('/api/v1/resource', methods=['GET'])\n    >>> def api_resource():\n    ...     return get_and_db_store_args()\n    \"\"\"\n    args = request.args\n    conn = sqlite3.connect('data.db')\n    df = pd.DataFrame([dict(args)], columns=args.keys())\n    df.to_sql('tbl_args', conn, if_exists='append', index=False)\n    return \"URL parameters are stored successfully.\", 200\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nmultidict, flask, os, json\n```python\nimport multidict\nfrom flask import Flask, request, jsonify\nimport os\nimport json\n\ndef get_and_file_store_args():\n    \"\"\"\n    Get the URL parameters and store them in a JSON file.\n    \n    Requirements:\n    - multidict\n    - flask\n    - os\n    - json\n    \n    Example:\n    >>> app = Flask(__name__)\n    >>> @app.route('/api/v1/resource', methods=['GET'])\n    >>> def api_resource():\n    ...     return get_and_file_store_args()\n    \"\"\"\n    args = request.args\n    file_name = os.path.join('data', 'args.json')\n    with open(file_name, 'w') as f:\n        json.dump(dict(args), f)\n    return jsonify({\"message\": \"URL parameters are stored successfully.\"}), 200\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nmultidict, flask, smtplib, email\n```python\nimport multidict\nfrom flask import Flask, request\nimport smtplib\nfrom email.mime.multipart import MIMEMultipart\nfrom email.mime.text import MIMEText\n\n# Constants\nSMTP_SERVER = 'smtp.gmail.com'\nSMTP_PORT = 587\nEMAIL = 'your_email@gmail.com'\nPASSWORD = 'your_password'\n\ndef get_args_and_send_email():\n    \"\"\"\n    Get the URL parameters and send them in an email.\n    \n    Requirements:\n    - multidict\n    - flask\n    - smtplib\n    - email\n    \n    Example:\n    >>> app = Flask(__name__)\n    >>> @app.route('/api/v1/resource', methods=['GET'])\n    >>> def api_resource():\n    ...     return get_args_and_send_email()\n    \"\"\"\n    args = request.args\n\n    msg = MIMEMultipart()\n    msg['From'] = EMAIL\n    msg['To'] = 'receiver_email@gmail.com'\n    msg['Subject'] = 'URL Parameters'\n\n    body = \"URL parameters: \" + str(dict(args))\n    msg.attach(MIMEText(body, 'plain'))\n\n    server = smtplib.SMTP(SMTP_SERVER, SMTP_PORT)\n    server.starttls()\n    server.login(EMAIL, PASSWORD)\n    text = msg.as_string()\n    server.sendmail(EMAIL, 'receiver_email@gmail.com', text)\n    server.quit()\n\n    return \"URL parameters are sent successfully.\", 200\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nmultidict, flask, hashlib, binascii\n```python\nimport multidict\nfrom flask import Flask, request\nimport hashlib\nimport binascii\n\ndef get_args_and_hash():\n    \"\"\"\n    Get the URL parameters and return their hash value.\n    \n    Requirements:\n    - multidict\n    - flask\n    - hashlib\n    - binascii\n    \n    Example:\n    >>> app = Flask(__name__)\n    >>> @app.route('/api/v1/resource', methods=['GET'])\n    >>> def api_resource():\n    ...     return get_args_and_hash()\n    \"\"\"\n    args = request.args\n    args_str = str(dict(args))\n    dk = hashlib.pbkdf2_hmac('sha256', args_str.encode(), b'salt', 100000)\n    hash_value = binascii.hexlify(dk)\n    return hash_value\n```"}
{"seed": "{\"task_id\": 11236006, \"prompt\": \"def f_11236006(mylist):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"[k for k, v in list(Counter(mylist).items()) if v > 1]\", \"test_start\": \"\\nfrom collections import Counter\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([1,3,2,2,1,4]) == [1, 2]\\n\", \"\\n    assert candidate([1,3,2,2,1,4]) != [3,4]\\n\", \"\\n    assert candidate([]) == []\\n\", \"\\n    assert candidate([1,1,1,1,1]) == [1]\\n\", \"\\n    assert candidate([1.,1.,1.]) == [1.]\\n\"], \"entry_point\": \"f_11236006\", \"intent\": \"identify duplicate values in list `mylist`\", \"library\": [\"collections\"]}", "generation": "Scenario 1: \ncsv, collections, numpy, pandas, matplotlib\n```python\nimport csv\nfrom collections import Counter\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Sample data\n# data.csv\n# name,age,city\n# Alice,25,New York\n# Bob,30,London\n# Alice,25,New York\n# Charlie,35,Tokyo\n# Bob,30,London\n# Alice,25,New York\n\ndef plot_duplicate_entries(filename):\n    \"\"\"\n    Read a CSV file and identify duplicate entries (rows). Display a bar plot \n    showing the count of duplicate entries.\n    \n    Parameters:\n    filename (str): The name of the CSV file.\n    \n    Returns:\n    Counter: A Counter object with the count of duplicate entries.\n    \n    Requirements:\n    - csv\n    - collections\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    \n    Example:\n    >>> plot_duplicate_entries('data.csv')\n    \"\"\"\n    with open(filename, 'r') as f:\n        reader = csv.reader(f)\n        data_list = list(reader)\n    \n    duplicates = [item for item, count in Counter(map(tuple, data_list)).items() if count > 1]\n    duplicates_counter = Counter(map(tuple, duplicates))\n    \n    df = pd.DataFrame.from_records(duplicates, columns=data_list[0])\n    df['count'] = df.apply(lambda row: duplicates_counter[tuple(row)], axis=1)\n    \n    df.plot(x='name', y='count', kind='bar', legend=False)\n    plt.xlabel('')\n    plt.ylabel('Count')\n    plt.title('Duplicate Entries')\n    plt.show()\n\n    return duplicates_counter\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncollections, itertools, random, numpy, matplotlib\n```python\nfrom collections import Counter\nimport itertools\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nPOSSIBLE_VALUES = list(range(1, 7))  # Possible values from a dice roll\n\ndef plot_dice_roll_distribution(num_rolls, num_dice):\n    \"\"\"\n    Simulate the rolling of a certain number of dice a certain number of times. \n    Identify and display the distribution of the sums of the dice rolls.\n    \n    Parameters:\n    num_rolls (int): The number of times to roll the dice.\n    num_dice (int): The number of dice to roll each time.\n    \n    Returns:\n    Counter: A Counter object with the count of each possible sum.\n    \n    Requirements:\n    - collections\n    - itertools\n    - random\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> plot_dice_roll_distribution(10000, 2)\n    \"\"\"\n    sums = []\n    for _ in range(num_rolls):\n        roll = [random.choice(POSSIBLE_VALUES) for _ in range(num_dice)]\n        sums.append(sum(roll))\n    \n    sums_counter = Counter(sums)\n\n    labels, values = zip(*sums_counter.items())\n    \n    plt.bar(labels, values)\n    plt.xlabel('Sum of Dice Roll')\n    plt.ylabel('Count')\n    plt.title('Distribution of Dice Roll Sums')\n    plt.show()\n\n    return sums_counter\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ncollections, numpy, pandas, seaborn\n```python\nfrom collections import Counter\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n# Sample data\n# df = pd.DataFrame({\n#     'name': ['Alice', 'Bob', 'Charlie', 'Alice', 'Bob', 'Alice'],\n#     'age': [25, 30, 35, 25, 30, 25],\n#     'city': ['New York', 'London', 'Tokyo', 'New York', 'London', 'New York']\n# })\n\ndef plot_age_distribution(df):\n    \"\"\"\n    Identify duplicate entries in a DataFrame and plot the distribution of ages \n    for the duplicate names.\n    \n    Parameters:\n    df (DataFrame): The DataFrame.\n    \n    Returns:\n    Counter: A Counter object with the count of each age.\n    \n    Requirements:\n    - collections\n    - numpy\n    - pandas\n    - seaborn\n    \n    Example:\n    >>> df = pd.DataFrame({\n    >>>     'name': ['Alice', 'Bob', 'Charlie', 'Alice', 'Bob', 'Alice'],\n    >>>     'age': [25, 30, 35, 25, 30, 25],\n    >>>     'city': ['New York', 'London', 'Tokyo', 'New York', 'London', 'New York']\n    >>> })\n    >>> plot_age_distribution(df)\n    \"\"\"\n    duplicates = df[df.duplicated()]\n    duplicates_counter = Counter(duplicates['age'])\n    \n    sns.distplot(duplicates['age'], kde=False, bins=np.arange(20, 40) - 0.5)\n    plt.xlabel('Age')\n    plt.ylabel('Count')\n    plt.title('Distribution of Ages for Duplicate Names')\n    plt.show()\n\n    return duplicates_counter\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ncollections, numpy, pandas, sklearn.cluster, matplotlib\n```python\nfrom collections import Counter\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\n# Constants\nN_CLUSTERS = 3  # Number of clusters for KMeans\n\n# Sample data\n# df = pd.DataFrame({\n#     'x': [1, 2, 2, 3, 3, 4],\n#     'y': [1, 1, 2, 2, 3, 3]\n# })\n\ndef cluster_and_plot(df):\n    \"\"\"\n    Identify duplicate points in a DataFrame, perform KMeans clustering on the \n    unique points, and plot the clusters.\n    \n    Parameters:\n    df (DataFrame): The DataFrame.\n    \n    Returns:\n    Counter: A Counter object with the count of each cluster.\n    \n    Requirements:\n    - collections\n    - numpy\n    - pandas\n    - sklearn.cluster\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = pd.DataFrame({\n    >>>     'x': [1, 2, 2, 3, 3, 4],\n    >>>     'y': [1, 1, 2, 2, 3, 3]\n    >>> })\n    >>> cluster_and_plot(df)\n    \"\"\"\n    duplicates = df[df.duplicated()]\n    duplicates_counter = Counter(map(tuple, duplicates.values))\n\n    kmeans = KMeans(n_clusters=N_CLUSTERS)\n    df['cluster'] = kmeans.fit_predict(df.values)\n    \n    plt.scatter(df['x'], df['y'], c=df['cluster'])\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.title('KMeans Clusters')\n    plt.show()\n\n    return duplicates_counter\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\ncollections, numpy, pandas, scipy.stats, matplotlib\n```python\nfrom collections import Counter\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\n# Sample data\n# df = pd.DataFrame({\n#     'value': [1, 2, 2, 3, 3, 4, 3, 2, 1, 4, 4, 4, 2, 2, 3, 1, 1, 1, 3, 2]\n# })\n\ndef plot_value_distribution(df):\n    \"\"\"\n    Identify duplicate values in a DataFrame and plot the distribution of the \n    values along with a normal distribution fitted to the data.\n    \n    Parameters:\n    df (DataFrame): The DataFrame.\n    \n    Returns:\n    Counter: A Counter object with the count of each value.\n    \n    Requirements:\n    - collections\n    - numpy\n    - pandas\n    - scipy.stats\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = pd.DataFrame({\n    >>>     'value': [1, 2, 2, 3, 3, 4, 3, 2, 1, 4, 4, 4, 2, 2, 3, 1, 1, 1, 3, 2]\n    >>> })\n    >>> plot_value_distribution(df)\n    \"\"\"\n    duplicates = df[df.duplicated()]\n    duplicates_counter = Counter(duplicates['value'])\n    \n    mu, std = norm.fit(df['value'])\n    \n    plt.hist(df['value'], bins=4, density=True, alpha=0.6, color='g')\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, mu, std)\n    plt.plot(x, p, 'k', linewidth=2)\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.title('Distribution of Values with Normal Fit')\n    plt.show()\n\n    return duplicates_counter\n```"}
{"seed": "{\"task_id\": 20211942, \"prompt\": \"def f_20211942(db):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"db.execute(\\\"INSERT INTO present VALUES('test2', ?, 10)\\\", (None,))\", \"test_start\": \"\\nimport sqlite3\\n\\ndef check(candidate):\", \"test\": [\"\\n    sqliteConnection = sqlite3.connect('dev.db')\\n    db = sqliteConnection.cursor()\\n    print(\\\"Database created and Successfully Connected to SQLite\\\")\\n    db.execute(\\\"CREATE TABLE present (name VARCHAR(5), age INTEGER, height INTEGER)\\\")\\n    try:\\n        candidate(db)\\n    except:\\n        assert False\\n\"], \"entry_point\": \"f_20211942\", \"intent\": \"Insert a 'None' value into a SQLite3 table.\", \"library\": [\"sqlite3\"]}", "generation": "Scenario 1:\npandas, sqlite3, numpy, random\n```python\nimport pandas as pd\nimport sqlite3\nimport numpy as np\nfrom random import choice\n\n# Constants\nNAMES = ['John', 'Jane', 'Steve', 'Emma', 'Liam', 'Olivia']\nAGES = np.arange(18, 65)\nHEIGHTS = np.arange(150, 200)  # in cm\n\ndef insert_random_data(db_name, table_name, num_entries):\n    \"\"\"\n    Insert random data into a SQLite3 table. The data includes random names, ages, and heights.\n\n    Parameters:\n    db_name (str): The name of the SQLite3 database.\n    table_name (str): The name of the table to insert data into.\n    num_entries (int): The number of entries to insert.\n\n    Returns:\n    int: The number of rows inserted.\n\n    Requirements:\n    - pandas\n    - sqlite3\n    - numpy\n    - random\n\n    Example:\n    >>> insert_random_data('test.db', 'People', 100)\n    \"\"\"\n    conn = sqlite3.connect(db_name)\n    cur = conn.cursor()\n\n    for _ in range(num_entries):\n        name = choice(NAMES)\n        age = np.random.choice(AGES)\n        height = np.random.choice(HEIGHTS)\n        cur.execute(f\"INSERT INTO {table_name} VALUES (?, ?, ?)\", (name, age, height))\n\n    conn.commit()\n\n    return cur.rowcount\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nsqlite3, pandas, os, csv\n```python\nimport sqlite3\nimport pandas as pd\nimport os\nimport csv\n\n# Constants\nCSV_PATH = 'data.csv'\n\ndef export_table_to_csv(db_name, table_name):\n    \"\"\"\n    Export a SQLite3 table to a CSV file.\n\n    Parameters:\n    db_name (str): The name of the SQLite3 database.\n    table_name (str): The name of the table to export.\n\n    Returns:\n    str: The path of the exported CSV file.\n\n    Requirements:\n    - sqlite3\n    - pandas\n    - os\n    - csv\n\n    Example:\n    >>> export_table_to_csv('test.db', 'People')\n    \"\"\"\n    conn = sqlite3.connect(db_name)\n    df = pd.read_sql_query(f\"SELECT * from {table_name}\", conn)\n    df.to_csv(CSV_PATH, index=False)\n\n    return os.path.abspath(CSV_PATH)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nsqlite3, pandas, matplotlib.pyplot, seaborn\n```python\nimport sqlite3\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nDB_NAME = 'test.db'\nTABLE_NAME = 'People'\n\ndef plot_age_distribution(db_name, table_name):\n    \"\"\"\n    Plot the age distribution of people in a SQLite3 table.\n\n    Parameters:\n    db_name (str): The name of the SQLite3 database.\n    table_name (str): The name of the table to plot from.\n\n    Returns:\n    None\n\n    Requirements:\n    - sqlite3\n    - pandas\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> plot_age_distribution('test.db', 'People')\n    \"\"\"\n    conn = sqlite3.connect(db_name)\n    df = pd.read_sql_query(f\"SELECT age from {table_name}\", conn)\n    sns.histplot(data=df, x=\"age\", bins=30, kde=True)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nsqlite3, pandas, numpy, matplotlib.pyplot\n```python\nimport sqlite3\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nDB_NAME = 'test.db'\nTABLE_NAME = 'People'\n\ndef plot_height_vs_age(db_name, table_name):\n    \"\"\"\n    Plot the height versus age of people in a SQLite3 table.\n\n    Parameters:\n    db_name (str): The name of the SQLite3 database.\n    table_name (str): The name of the table to plot from.\n\n    Returns:\n    None\n\n    Requirements:\n    - sqlite3\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_height_vs_age('test.db', 'People')\n    \"\"\"\n    conn = sqlite3.connect(db_name)\n    df = pd.read_sql_query(f\"SELECT age, height from {table_name}\", conn)\n    plt.scatter(df['age'], df['height'])\n    plt.xlabel('Age')\n    plt.ylabel('Height')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nsqlite3, pandas, random, os\n```python\nimport sqlite3\nimport pandas as pd\nfrom random import choice\nimport os\n\n# Constants\nNAMES = ['John', 'Jane', 'Steve', 'Emma', 'Liam', 'Olivia']\nAGES = range(18, 65)\nHEIGHTS = range(150, 200)  # in cm\n\ndef create_and_populate_table(db_name, table_name, num_entries):\n    \"\"\"\n    Create a SQLite3 table and populate it with random data.\n\n    Parameters:\n    db_name (str): The name of the SQLite3 database.\n    table_name (str): The name of the table to create and populate.\n    num_entries (int): The number of entries to insert.\n\n    Returns:\n    str: The path of the SQLite3 database file.\n\n    Requirements:\n    - sqlite3\n    - pandas\n    - random\n    - os\n\n    Example:\n    >>> create_and_populate_table('test.db', 'People', 100)\n    \"\"\"\n    conn = sqlite3.connect(db_name)\n    cur = conn.cursor()\n\n    cur.execute(f\"CREATE TABLE {table_name} (name TEXT, age INTEGER, height INTEGER)\")\n\n    for _ in range(num_entries):\n        name = choice(NAMES)\n        age = choice(AGES)\n        height = choice(HEIGHTS)\n        cur.execute(f\"INSERT INTO {table_name} VALUES (?, ?, ?)\", (name, age, height))\n\n    conn.commit()\n\n    return os.path.abspath(db_name)\n```"}
{"seed": "{\"task_id\": 406121, \"prompt\": \"def f_406121(list_of_menuitems):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"[image for menuitem in list_of_menuitems for image in menuitem]\", \"test_start\": \"\\nfrom collections import Counter\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([[1,2],[3,4,5]]) == [1,2,3,4,5]\\n\", \"\\n    assert candidate([[],[]]) == []\\n\", \"\\n    assert candidate([[1,1,1], []]) == [1,1,1]\\n\", \"\\n    assert candidate([['1'],['2']]) == ['1','2']\\n\"], \"entry_point\": \"f_406121\", \"intent\": \"flatten list `list_of_menuitems`\", \"library\": [\"collections\"]}", "generation": "Scenario 1:\ncollections, numpy, matplotlib, itertools\n```python\nfrom collections import Counter\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport itertools\n\ndef plot_menuitem_distribution(list_of_menuitems):\n    \"\"\"\n    Given a nested list of menu items, flatten the list and plot a histogram of \n    the distribution of menu items.\n    \n    Parameters:\n    list_of_menuitems (list): A nested list of menu items.\n    \n    Returns:\n    None\n\n    Requirements:\n    - collections\n    - numpy\n    - matplotlib.pyplot\n    - itertools\n\n    Example:\n    >>> plot_menuitem_distribution([['Pizza', 'Burger'], ['Pizza', 'Coke'], ['Pasta', 'Coke']])\n    \"\"\"\n    flat_list = list(itertools.chain(*list_of_menuitems))\n\n    counter = Counter(flat_list)\n    labels, values = zip(*counter.items())\n    indexes = np.arange(len(labels))\n    width = 1\n\n    plt.bar(indexes, values, width)\n    plt.xticks(indexes + width * 0.5, labels)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncollections, random, string\n```python\nfrom collections import Counter\nimport random\nimport string\n\n# Constants\nLETTERS = string.ascii_letters\n\ndef count_random_letters(list_of_lists):\n    \"\"\"\n    Given a nested list, replace each sub-list with a random letter and \n    return a count of each letter in the final list.\n\n    Parameters:\n    list_of_lists (list): A nested list.\n\n    Returns:\n    dict: A dictionary containing count of each letter in the list.\n\n    Requirements:\n    - collections\n    - random\n    - string\n\n    Example:\n    >>> count_random_letters([['Pizza', 'Burger'], ['Pizza', 'Coke'], ['Pasta', 'Coke']])\n    \"\"\"\n    flat_list = [random.choice(LETTERS) for _ in list_of_lists]\n\n    return dict(Counter(flat_list))\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ncollections, numpy, pandas\n```python\nfrom collections import Counter\nimport numpy as np\nimport pandas as pd\n\ndef create_menuitem_dataframe(list_of_menuitems):\n    \"\"\"\n    Given a nested list of menu items, flatten the list and create a pandas \n    DataFrame with the count of each menu item.\n\n    Parameters:\n    list_of_menuitems (list): A nested list of menu items.\n\n    Returns:\n    DataFrame: A pandas DataFrame with count of each menu item.\n\n    Requirements:\n    - collections\n    - numpy\n    - pandas\n\n    Example:\n    >>> create_menuitem_dataframe([['Pizza', 'Burger'], ['Pizza', 'Coke'], ['Pasta', 'Coke']])\n    \"\"\"\n    flat_list = np.array(list_of_menuitems).flatten()\n    counter = Counter(flat_list)\n    \n    df = pd.DataFrame.from_dict(counter, orient='index', columns=['Count'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ncollections, seaborn, pandas, matplotlib\n```python\nfrom collections import Counter\nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef visualize_menuitem_distribution(list_of_menuitems):\n    \"\"\"\n    Given a nested list of menu items, flatten the list and visualize the \n    frequency of each menu item using a seaborn countplot.\n\n    Parameters:\n    list_of_menuitems (list): A nested list of menu items.\n\n    Returns:\n    None\n\n    Requirements:\n    - collections\n    - seaborn\n    - pandas\n    - matplotlib\n\n    Example:\n    >>> visualize_menuitem_distribution([['Pizza', 'Burger'], ['Pizza', 'Coke'], ['Pasta', 'Coke']])\n    \"\"\"\n    flat_list = [item for sublist in list_of_menuitems for item in sublist]\n    counter = Counter(flat_list)\n    \n    df = pd.DataFrame.from_dict(counter, orient='index', columns=['Count']).reset_index()\n    df.columns = ['Item', 'Count']\n\n    sns.set(style=\"whitegrid\")\n    ax = sns.barplot(x=\"Count\", y=\"Item\", data=df)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\ncollections, itertools, operator\n```python\nfrom collections import Counter\nimport itertools\nimport operator\n\ndef most_common_menuitem(list_of_menuitems):\n    \"\"\"\n    Given a nested list of menu items, flatten the list and return the most common \n    menu item.\n\n    Parameters:\n    list_of_menuitems (list): A nested list of menu items.\n\n    Returns:\n    str: The most common menu item.\n\n    Requirements:\n    - collections\n    - itertools\n    - operator\n\n    Example:\n    >>> most_common_menuitem([['Pizza', 'Burger'], ['Pizza', 'Coke'], ['Pasta', 'Coke']])\n    \"\"\"\n    flat_list = list(itertools.chain(*list_of_menuitems))\n\n    counter = Counter(flat_list)\n\n    return max(counter.items(), key=operator.itemgetter(1))[0]\n```"}
{"seed": "{\"task_id\": 4741537, \"prompt\": \"def f_4741537(a, b):\\n\\t\", \"suffix\": \"\\n\\treturn a\", \"canonical_solution\": \"a.extend(b)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([1, 2, 2, 3], {4, 5, 2}) == [1, 2, 2, 3, 2, 4, 5]\\n\", \"\\n    assert candidate([], {4,5,2}) == [2,4,5]\\n\", \"\\n    assert candidate([1,2,3,4],{2}) == [1,2,3,4,2]\\n\", \"\\n    assert candidate([1], {'a'}) == [1, 'a']\\n\"], \"entry_point\": \"f_4741537\", \"intent\": \"append elements of a set `b` to a list `a`\", \"library\": []}", "generation": "Scenario 1: \ncollections, itertools, numpy, matplotlib\n```python\nimport collections\nimport itertools\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nITEMS = ['apple', 'banana', 'cherry', 'date', 'elderberry']\n\ndef plot_frequency_of_items(a, b):\n    \"\"\"\n    Combine two lists and plot the frequency of pre-defined items in the combined list.\n\n    Parameters:\n    a (list): A list of items.\n    b (list): Another list of items.\n\n    Requirements:\n    - collections\n    - itertools\n    - numpy\n    - matplotlib\n\n    Example:\n    >>> plot_frequency_of_items(['apple', 'banana', 'cherry'], ['date', 'elderberry', 'apple', 'banana', 'cherry'])\n    \"\"\"\n    combined = list(itertools.chain(a, b))\n    counter = collections.Counter(combined)\n    item_counts = [counter.get(item, 0) for item in ITEMS]\n\n    plt.bar(ITEMS, item_counts)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, pandas, matplotlib, random\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef generate_random_dataframe_and_plot(a, b):\n    \"\"\"\n    Generate a pandas DataFrame with random values based on two lists, then plot the DataFrame.\n\n    Parameters:\n    a (list): A list used to define the number of rows in the DataFrame.\n    b (list): Another list used to define the number of columns in the DataFrame.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib\n    - random\n\n    Example:\n    >>> generate_random_dataframe_and_plot([1, 2, 3], ['a', 'b', 'c', 'd', 'e'])\n    \"\"\"\n    np.random.seed(0)\n    df = pd.DataFrame(np.random.randn(len(a), len(b)), index=a, columns=COLUMNS[:len(b)])\n\n    df.plot(kind='bar')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, pandas, scipy.stats, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef calculate_and_plot_correlation(a, b):\n    \"\"\"\n    Calculate the Pearson correlation coefficient of two lists, generate a pandas DataFrame using these lists, \n    and then plot a scatterplot with a regression line.\n\n    Parameters:\n    a (list): A list of numbers.\n    b (list): Another list of numbers.\n\n    Requirements:\n    - numpy\n    - pandas\n    - scipy.stats\n    - matplotlib\n\n    Example:\n    >>> calculate_and_plot_correlation([1, 2, 3, 4, 5], [2, 4, 6, 8, 10])\n    \"\"\"\n    correlation, _ = stats.pearsonr(a, b)\n    print(f'Pearson correlation: {correlation}')\n\n    df = pd.DataFrame({'A': a, 'B': b})\n\n    plt.scatter(df['A'], df['B'])\n    plt.plot(np.unique(df['A']), np.poly1d(np.polyfit(df['A'], df['B'], 1))(np.unique(df['A'])), color='red')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, pandas, sklearn.preprocessing, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['A', 'B']\n\ndef standardize_and_plot(a, b):\n    \"\"\"\n    Standardize two lists using sklearn's StandardScaler, generate a pandas DataFrame using these lists,\n    and then plot the standardized values.\n\n    Parameters:\n    a (list): A list of numbers.\n    b (list): Another list of numbers.\n\n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.preprocessing\n    - matplotlib\n\n    Example:\n    >>> standardize_and_plot([1, 2, 3, 4, 5], [2, 4, 6, 8, 10])\n    \"\"\"\n    scaler = StandardScaler()\n    standardized_values = scaler.fit_transform(np.array([a, b]).T)\n    df = pd.DataFrame(standardized_values, columns=COLUMNS)\n\n    df.plot(kind='bar')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, pandas, scipy.spatial.distance, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nfrom scipy.spatial import distance\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['A', 'B']\n\ndef calculate_euclidean_distance_and_plot(a, b):\n    \"\"\"\n    Calculate the Euclidean distance between two lists, generate a pandas DataFrame using these lists,\n    and then plot the values with a line showing the Euclidean distance.\n\n    Parameters:\n    a (list): A list of numbers.\n    b (list): Another list of numbers.\n\n    Requirements:\n    - numpy\n    - pandas\n    - scipy.spatial.distance\n    - matplotlib\n\n    Example:\n    >>> calculate_euclidean_distance_and_plot([1, 2, 3], [2, 3, 4])\n    \"\"\"\n    euclidean_distance = distance.euclidean(a, b)\n    print(f'Euclidean distance: {euclidean_distance}')\n\n    df = pd.DataFrame({'A': a, 'B': b})\n\n    plt.plot(df['A'], df['B'])\n    plt.plot([df['A'][0], df['B'][0]], [df['A'][-1], df['B'][-1]], 'ro-')\n    plt.show()\n```"}
{"seed": "{\"task_id\": 15851568, \"prompt\": \"def f_15851568(x):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"x.rpartition('-')[0]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('djhajhdjk-dadwqd-dahdjkahsk') == 'djhajhdjk-dadwqd'\\n\", \"\\n    assert candidate('/-/') == '/'\\n\", \"\\n    assert candidate('---') == '--'\\n\", \"\\n    assert candidate('') == ''\\n\"], \"entry_point\": \"f_15851568\", \"intent\": \"Split a string `x` by last occurrence of character `-`\", \"library\": []}", "generation": "Scenario 1: \nre, os, pathlib, csv\n```python\nimport re\nimport os\nfrom pathlib import Path\nimport csv\n\n# Constants\nFILE_DIR = Path('data/')\nFILE_PATTERN = re.compile(r'^(.*?)-\\d+\\.csv$')\n\ndef process_files():\n    \"\"\"\n    Process all csv files in a directory whose name matches a specific pattern \n    by splitting the filename at the last occurrence of '-' and writing the \n    contents to a new file with the prefix part of the filename.\n    \n    Requirements:\n    - pathlib.Path\n    - re\n    - os\n    - csv\n\n    Example:\n    >>> process_files()\n    \"\"\"\n    for filename in os.listdir(FILE_DIR):\n        match = FILE_PATTERN.match(filename)\n        if match is not None:\n            prefix = match.group(1)\n            with open(FILE_DIR / filename, 'r') as infile, open(FILE_DIR / f'{prefix}.csv', 'w') as outfile:\n                reader = csv.reader(infile)\n                writer = csv.writer(outfile)\n                writer.writerows(reader)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, json, re, shutil\n```python\nimport os\nimport json\nimport re\nimport shutil\n\n# Constants\nSOURCE_DIR = '/source/dir'\nTARGET_DIR = '/target/dir'\nFILE_PATTERN = re.compile(r'^(.*?)-\\d+\\.json$')\n\ndef move_json_files():\n    \"\"\"\n    Move all json files in a source directory to a target directory, renaming \n    them by splitting the filename at the last occurrence of '-' and keeping \n    the prefix part of the filename.\n    \n    Requirements:\n    - os\n    - json\n    - re\n    - shutil\n\n    Example:\n    >>> move_json_files()\n    \"\"\"\n    for filename in os.listdir(SOURCE_DIR):\n        match = FILE_PATTERN.match(filename)\n        if match is not None:\n            prefix = match.group(1)\n            new_filename = f'{prefix}.json'\n            shutil.move(os.path.join(SOURCE_DIR, filename), os.path.join(TARGET_DIR, new_filename))\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nstring, re, collections\n```python\nfrom string import ascii_lowercase\nimport re\nfrom collections import Counter\n\n# Constants\nLETTERS_PATTERN = re.compile(r'^(.*?)-[a-z]$')\nLETTERS = ascii_lowercase\n\ndef count_letters(string):\n    \"\"\"\n    Given a string, split it at the last occurrence of '-' and count the frequency \n    of each lowercase letter in the prefix part of the string.\n    \n    Parameters:\n    string (str): The input string.\n\n    Returns:\n    dict: A dictionary with the frequency of each lowercase letter.\n\n    Requirements:\n    - string.ascii_lowercase\n    - re\n    - collections.Counter\n\n    Example:\n    >>> count_letters('abc-def-ghij')\n    \"\"\"\n    match = LETTERS_PATTERN.match(string)\n    if match is not None:\n        prefix = match.group(1)\n        letter_counts = Counter(prefix)\n        return {letter: letter_counts.get(letter, 0) for letter in LETTERS}\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nre, os, zipfile\n```python\nimport re\nimport os\nimport zipfile\n\n# Constants\nZIP_DIR = '/zip/dir'\nZIP_PATTERN = re.compile(r'^(.*?)-\\d+\\.zip$')\n\ndef unzip_files():\n    \"\"\"\n    Unzip all zip files in a directory whose name matches a specific pattern, \n    by splitting the filename at the last occurrence of '-' and using the \n    prefix part of the filename as the directory to extract to.\n    \n    Requirements:\n    - os\n    - re\n    - zipfile\n\n    Example:\n    >>> unzip_files()\n    \"\"\"\n    for filename in os.listdir(ZIP_DIR):\n        match = ZIP_PATTERN.match(filename)\n        if match is not None:\n            prefix = match.group(1)\n            with zipfile.ZipFile(os.path.join(ZIP_DIR, filename), 'r') as zip_ref:\n                zip_ref.extractall(os.path.join(ZIP_DIR, prefix))\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nre, os, io, PIL.Image\n```python\nimport re\nimport os\nimport io\nfrom PIL import Image\n\n# Constants\nIMAGE_DIR = '/image/dir'\nIMAGE_PATTERN = re.compile(r'^(.*?)-\\d+\\.png$')\n\ndef resize_images():\n    \"\"\"\n    Resize all png images in a directory whose name matches a specific pattern, \n    by splitting the filename at the last occurrence of '-' and saving the \n    resized image with the prefix part of the filename. The images are resized \n    to a fixed size of 500x500 pixels.\n    \n    Requirements:\n    - os\n    - re\n    - io\n    - PIL.Image\n\n    Example:\n    >>> resize_images()\n    \"\"\"\n    for filename in os.listdir(IMAGE_DIR):\n        match = IMAGE_PATTERN.match(filename)\n        if match is not None:\n            prefix = match.group(1)\n            with Image.open(os.path.join(IMAGE_DIR, filename)) as img:\n                resized_img = img.resize((500, 500))\n                resized_img.save(os.path.join(IMAGE_DIR, f'{prefix}.png'))\n```"}
{"seed": "{\"task_id\": 15851568, \"prompt\": \"def f_15851568(x):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"x.rsplit('-', 1)[0]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('2022-03-01') == '2022-03'\\n\", \"\\n    assert candidate('2020-2022') == '2020'\\n\"], \"entry_point\": \"f_15851568\", \"intent\": \"get the last part of a string before the character '-'\", \"library\": []}", "generation": "Scenario 1:\npandas, datetime, matplotlib, numpy\n```python\nimport pandas as pd\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nMONTHS = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n\ndef plot_monthly_data(data):\n    \"\"\"\n    Plot a bar chart of monthly data for a given year.\n\n    Parameters:\n    data (str): The data string in the format 'yyyy-mm-value'.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - datetime\n    - matplotlib.pyplot\n    - numpy\n\n    Example:\n    >>> data = '2022-01-100,2022-02-200,2022-03-150,2022-04-300,2022-05-250,2022-06-350,2022-07-400,2022-08-450,2022-09-500,2022-10-550,2022-11-600,2022-12-650'\n    >>> plot_monthly_data(data)\n    \"\"\"\n    data = data.split(',')\n    data = [d.rsplit('-', 1) for d in data]\n    data = [(datetime.strptime(d[0], '%Y-%m').strftime('%B'), int(d[1])) for d in data]\n    df = pd.DataFrame(data, columns=['Month', 'Value'])\n    df = df.set_index('Month')\n\n    plt.figure(figsize=(10, 6))\n    plt.bar(df.index, df['Value'])\n    plt.xlabel('Month')\n    plt.ylabel('Value')\n    plt.title('Monthly Data')\n    plt.xticks(rotation='vertical')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, sys, subprocess\n```python\nimport os\nimport sys\nimport subprocess\n\ndef get_file_info(filepath):\n    \"\"\"\n    Get the size and last modification date of a file.\n\n    Parameters:\n    filepath (str): The path to the file.\n\n    Returns:\n    dict: A dictionary containing the size and last modification date of the file.\n\n    Requirements:\n    - os\n    - sys\n    - subprocess\n\n    Example:\n    >>> get_file_info('/path/to/file.txt')\n    \"\"\"\n    try:\n        size = os.path.getsize(filepath)\n        mtime = os.path.getmtime(filepath)\n        mtime = datetime.fromtimestamp(mtime).strftime('%Y-%m-%d %H:%M:%S')\n    except OSError as e:\n        print(f\"Error: {e}\")\n        sys.exit(1)\n\n    file_info = {\n        'size': f\"{size} bytes\",\n        'last_modified': mtime\n    }\n\n    return file_info\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ndatetime, pytz, dateutil\n```python\nfrom datetime import datetime\nimport pytz\nfrom dateutil.parser import parse\n\ndef convert_timezone(date_str, from_tz, to_tz):\n    \"\"\"\n    Convert a datetime from one timezone to another.\n\n    Parameters:\n    date_str (str): The date string in \"yyyy-mm-dd hh:mm:ss\" format.\n    from_tz (str): The timezone of the given date string.\n    to_tz (str): The timezone to which the date should be converted.\n\n    Returns:\n    str: The converted datetime string.\n\n    Requirements:\n    - datetime\n    - pytz\n    - dateutil.parser\n\n    Example:\n    >>> convert_timezone('2022-03-01 12:00:00', 'UTC', 'America/New_York')\n    \"\"\"\n    from_tz = pytz.timezone(from_tz)\n    to_tz = pytz.timezone(to_tz)\n    date = parse(date_str).replace(tzinfo=from_tz)\n    date = date.astimezone(to_tz)\n\n    return date.strftime('%Y-%m-%d %H:%M:%S')\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_histogram(data):\n    \"\"\"\n    Plot a histogram of data.\n\n    Parameters:\n    data (str): The data string in the format 'value-value-value-...'.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> data = '1-2-3-4-5-6-7-8-9-10'\n    >>> plot_histogram(data)\n    \"\"\"\n    data = data.split('-')\n    data = [int(d) for d in data]\n    df = pd.DataFrame(data, columns=['Values'])\n    \n    plt.figure(figsize=(10, 6))\n    plt.hist(df['Values'], bins=np.arange(df['Values'].min(), df['Values'].max()+2) - 0.5, edgecolor='black')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of Values')\n    plt.xticks(np.arange(df['Values'].min(), df['Values'].max()+1))\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, shutil\n```python\nimport os\nimport shutil\n\ndef move_files_with_extension(src_dir, dest_dir, extension):\n    \"\"\"\n    Move all files with a given extension from one directory to another.\n\n    Parameters:\n    src_dir (str): The source directory.\n    dest_dir (str): The destination directory.\n    extension (str): The file extension.\n\n    Returns:\n    int: The number of files moved.\n\n    Requirements:\n    - os\n    - shutil\n\n    Example:\n    >>> move_files_with_extension('/path/to/src', '/path/to/dest', '.txt')\n    \"\"\"\n    files_moved = 0\n\n    for file_name in os.listdir(src_dir):\n        if file_name.endswith(extension):\n            shutil.move(os.path.join(src_dir, file_name), os.path.join(dest_dir, file_name))\n            files_moved += 1\n\n    return files_moved\n```"}
{"seed": "{\"task_id\": 17438096, \"prompt\": \"def f_17438096(filename, ftp):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"ftp.storlines('STOR ' + filename, open(filename, 'r'))\", \"test_start\": \"\\nimport ftplib\\nfrom unittest.mock import Mock\\n\\ndef check(candidate):\", \"test\": [\"\\n    ftplib.FTP = Mock()\\n    ftp = ftplib.FTP(\\\"10.10.10.10\\\")\\n    ftp.storlines = Mock()\\n    file_name = 'readme.txt'\\n    with open (file_name, 'a') as f:\\n        f.write('apple')\\n    candidate(file_name, ftp)\\n\"], \"entry_point\": \"f_17438096\", \"intent\": \"upload file using FTP\", \"library\": [\"ftplib\"]}", "generation": "Scenario 1:\nos, ftplib, socket, sys\n```python\nimport os\nimport ftplib\nimport socket\nimport sys\n\n# Constants\nFTP_SERVER = 'ftp.dlptest.com'\nFTP_USER = 'dlpuser'\nFTP_PASS = 'rNrKYTX9g7z3RgJRmxWuGHbeu'\n\ndef upload_files_to_ftp(directory):\n    \"\"\"\n    Upload all files from a local directory to an FTP server.\n\n    Parameters:\n    directory (str): The local directory containing the files to be uploaded.\n\n    Returns:\n    bool: True if all files uploaded successfully, False otherwise.\n\n    Requirements:\n    - os\n    - ftplib\n    - socket\n    - sys\n\n    Example:\n    >>> upload_files_to_ftp('/path/to/local/directory')\n    \"\"\"\n    try:\n        ftp = ftplib.FTP(FTP_SERVER)\n        ftp.login(FTP_USER, FTP_PASS)\n\n        for file in os.listdir(directory):\n            file_path = os.path.join(directory, file)\n            with open(file_path, 'rb') as f:\n                ftp.storbinary('STOR ' + file, f)\n\n        ftp.quit()\n        return True\n\n    except (socket.error, socket.gaierror) as e:\n        print('Error: cannot reach \"%s\"' % FTP_SERVER)\n        return False\n\n    except ftplib.error_perm:\n        print('Error: cannot login to \"%s\"' % FTP_SERVER)\n        ftp.quit()\n        return False\n\n    except Exception as e:\n        print('Error: cannot upload \"%s\"' % file)\n        ftp.quit()\n        return False\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nftplib, os, gzip, shutil\n```python\nimport ftplib\nimport os\nimport gzip\nimport shutil\n\n# Constants\nFTP_SERVER = 'ftp.dlptest.com'\nFTP_USER = 'dlpuser'\nFTP_PASS = 'rNrKYTX9g7z3RgJRmxWuGHbeu'\n\ndef download_and_unzip(ftp_directory, local_directory):\n    \"\"\"\n    Download all gzip files from an FTP server directory, unzip them and save to a local directory.\n\n    Parameters:\n    ftp_directory (str): The directory on the FTP server containing the files to be downloaded.\n    local_directory (str): The local directory where the files should be saved.\n\n    Returns:\n    bool: True if all files downloaded and unzipped successfully, False otherwise.\n\n    Requirements:\n    - ftplib\n    - os\n    - gzip\n    - shutil\n\n    Example:\n    >>> download_and_unzip('/path/to/ftp/directory', '/path/to/local/directory')\n    \"\"\"\n    try:\n        ftp = ftplib.FTP(FTP_SERVER)\n        ftp.login(FTP_USER, FTP_PASS)\n\n        if not os.path.exists(local_directory):\n            os.makedirs(local_directory)\n\n        files = ftp.nlst(ftp_directory)\n        for file in files:\n            if file.endswith('.gz'):\n                local_file = os.path.join(local_directory, os.path.basename(file))\n                with open(local_file, 'wb') as f:\n                    ftp.retrbinary('RETR ' + file, f.write)\n\n                with gzip.open(local_file, 'rb') as f_in:\n                    with open(local_file[:-3], 'wb') as f_out:\n                        shutil.copyfileobj(f_in, f_out)\n\n                os.remove(local_file)\n\n        ftp.quit()\n        return True\n\n    except ftplib.error_perm:\n        print('Error: cannot login to \"%s\"' % FTP_SERVER)\n        ftp.quit()\n        return False\n\n    except Exception as e:\n        print('Error: cannot download or unzip \"%s\"' % file)\n        ftp.quit()\n        return False\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nftplib, os, zipfile\n```python\nimport ftplib\nimport os\nimport zipfile\n\n# Constants\nFTP_SERVER = 'ftp.dlptest.com'\nFTP_USER = 'dlpuser'\nFTP_PASS = 'rNrKYTX9g7z3RgJRmxWuGHbeu'\n\ndef download_and_unzip_ftp_files(local_directory):\n    \"\"\"\n    Download all zip files from an FTP server, unzip them and save to a local directory.\n\n    Parameters:\n    local_directory (str): The local directory where the files should be saved.\n\n    Returns:\n    bool: True if all files downloaded and unzipped successfully, False otherwise.\n\n    Requirements:\n    - ftplib\n    - os\n    - zipfile\n\n    Example:\n    >>> download_and_unzip_ftp_files('/path/to/local/directory')\n    \"\"\"\n    try:\n        ftp = ftplib.FTP(FTP_SERVER)\n        ftp.login(FTP_USER, FTP_PASS)\n\n        files = ftp.nlst()\n        for file in files:\n            if file.endswith('.zip'):\n                local_file = os.path.join(local_directory, file)\n                with open(local_file, 'wb') as f:\n                    ftp.retrbinary('RETR ' + file, f.write)\n\n                with zipfile.ZipFile(local_file, 'r') as zip_ref:\n                    zip_ref.extractall(local_directory)\n\n                os.remove(local_file)\n\n        ftp.quit()\n        return True\n\n    except ftplib.error_perm:\n        print('Error: cannot login to \"%s\"' % FTP_SERVER)\n        ftp.quit()\n        return False\n\n    except Exception as e:\n        print('Error: cannot download or unzip \"%s\"' % file)\n        ftp.quit()\n        return False\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, ftplib, glob, csv\n```python\nimport os\nimport ftplib\nimport glob\nimport csv\n\n# Constants\nFTP_SERVER = 'ftp.dlptest.com'\nFTP_USER = 'dlpuser'\nFTP_PASS = 'rNrKYTX9g7z3RgJRmxWuGHbeu'\n\ndef upload_csv_files(directory):\n    \"\"\"\n    Upload all CSV files from a local directory to an FTP server.\n\n    Parameters:\n    directory (str): The local directory containing the files to be uploaded.\n\n    Returns:\n    bool: True if all files uploaded successfully, False otherwise.\n\n    Requirements:\n    - os\n    - ftplib\n    - glob\n    - csv\n\n    Example:\n    >>> upload_csv_files('/path/to/local/directory')\n    \"\"\"\n    try:\n        ftp = ftplib.FTP(FTP_SERVER)\n        ftp.login(FTP_USER, FTP_PASS)\n\n        for file in glob.glob(os.path.join(directory, '*.csv')):\n            with open(file, 'r') as f:\n                reader = csv.reader(f)\n                for row in reader:\n                    ftp.storlines('STOR ' + file, f)\n\n        ftp.quit()\n        return True\n\n    except ftplib.error_perm:\n        print('Error: cannot login to \"%s\"' % FTP_SERVER)\n        ftp.quit()\n        return False\n\n    except Exception as e:\n        print('Error: cannot upload \"%s\"' % file)\n        ftp.quit()\n        return False\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, ftplib, datetime\n```python\nimport os\nimport ftplib\nfrom datetime import datetime\n\n# Constants\nFTP_SERVER = 'ftp.dlptest.com'\nFTP_USER = 'dlpuser'\nFTP_PASS = 'rNrKYTX9g7z3RgJRmxWuGHbeu'\n\ndef upload_files_modified_today(directory):\n    \"\"\"\n    Upload all files from a local directory that were modified today to an FTP server.\n\n    Parameters:\n    directory (str): The local directory containing the files to be uploaded.\n\n    Returns:\n    bool: True if all files uploaded successfully, False otherwise.\n\n    Requirements:\n    - os\n    - ftplib\n    - datetime\n\n    Example:\n    >>> upload_files_modified_today('/path/to/local/directory')\n    \"\"\"\n    try:\n        ftp = ftplib.FTP(FTP_SERVER)\n        ftp.login(FTP_USER, FTP_PASS)\n\n        today = datetime.now().date()\n        for file in os.listdir(directory):\n            file_path = os.path.join(directory, file)\n            file_modified_date = datetime.fromtimestamp(os.path.getmtime(file_path)).date()\n            if file_modified_date == today:\n                with open(file_path, 'rb') as f:\n                    ftp.storbinary('STOR ' + file, f)\n\n        ftp.quit()\n        return True\n\n    except ftplib.error_perm:\n        print('Error: cannot login to \"%s\"' % FTP_SERVER)\n        ftp.quit()\n        return False\n\n    except Exception as e:\n        print('Error: cannot upload \"%s\"' % file)\n        ftp.quit()\n        return False\n```"}
{"seed": "{\"task_id\": 28742436, \"prompt\": \"def f_28742436():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"np.maximum([2, 3, 4], [1, 5, 2])\", \"test_start\": \"\\nimport numpy as np \\n\\ndef check(candidate):\", \"test\": [\"\\n    assert all(candidate() == np.array([2, 5, 4]))\\n\"], \"entry_point\": \"f_28742436\", \"intent\": \"create array containing the maximum value of respective elements of array `[2, 3, 4]` and array `[1, 5, 2]`\", \"library\": [\"numpy\"]}", "generation": "Scenario 3: \nnumpy, matplotlib, random\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nARRAY_LENGTH = 100\n\ndef plot_maximum_values():\n    \"\"\"\n    Generate two arrays of random integers and plot a line graph \n    of the maximum values of respective elements of the two arrays.\n\n    Returns:\n    None: This function doesn't return a value.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> plot_maximum_values()\n    \"\"\"\n    array1 = np.array([randint(1, 100) for _ in range(ARRAY_LENGTH)])\n    array2 = np.array([randint(1, 100) for _ in range(ARRAY_LENGTH)])\n\n    max_values = np.maximum(array1, array2)\n\n    plt.plot(max_values)\n    plt.ylabel('Maximum Values')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, itertools, math\n```python\nimport numpy as np\nfrom itertools import combinations\nimport math\n\n# Constants\nARRAY_LENGTH = 10\n\ndef calculate_combinations_distance(array1, array2):\n    \"\"\"\n    Generate all combinations of two elements from two arrays, \n    calculate the Euclidean distance for each combination, \n    and return the maximum distance.\n\n    Parameters:\n    array1 (numpy.array): The first array.\n    array2 (numpy.array): The second array.\n\n    Returns:\n    float: The maximum Euclidean distance.\n\n    Requirements:\n    - numpy\n    - itertools.combinations\n    - math\n\n    Example:\n    >>> array1 = np.array([2, 3, 4])\n    >>> array2 = np.array([1, 5, 2])\n    >>> calculate_combinations_distance(array1, array2)\n    \"\"\"\n    max_distance = 0\n    for comb in combinations(zip(array1, array2), 2):\n        distance = math.sqrt((comb[0][0] - comb[1][0])**2 + (comb[0][1] - comb[1][1])**2)\n        if distance > max_distance:\n            max_distance = distance\n\n    return max_distance\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnumpy, pandas, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nARRAY_LENGTH = 100\n\ndef compare_array_statistics():\n    \"\"\"\n    Generate two arrays of random numbers, calculate their mean, median, \n    and standard deviation, and plot a bar graph to compare these statistics.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the statistics of the arrays.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> compare_array_statistics()\n    \"\"\"\n    array1 = np.random.rand(ARRAY_LENGTH)\n    array2 = np.random.rand(ARRAY_LENGTH)\n\n    statistics = {\n        'Array1': [np.mean(array1), np.median(array1), np.std(array1)],\n        'Array2': [np.mean(array2), np.median(array2), np.std(array2)]\n    }\n\n    df = pd.DataFrame(statistics, index=['Mean', 'Median', 'Standard Deviation'])\n    df.plot(kind='bar')\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 6: \nnumpy, scipy.optimize, matplotlib\n```python\nimport numpy as np\nfrom scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\n\n# Constants\nARRAY_LENGTH = 100\nNOISE_LEVEL = 0.2\n\ndef fit_curve_to_data():\n    \"\"\"\n    Generate a noisy sine wave and fit a curve to the data using curve_fit from scipy.optimize.\n\n    Returns:\n    None: This function doesn't return a value.\n\n    Requirements:\n    - numpy\n    - scipy.optimize\n    - matplotlib.pyplot\n\n    Example:\n    >>> fit_curve_to_data()\n    \"\"\"\n    x = np.linspace(0, 4*np.pi, ARRAY_LENGTH)\n    y = np.sin(x) + NOISE_LEVEL * np.random.rand(ARRAY_LENGTH)\n\n    def func(x, a, b):\n        return a * np.sin(b * x)\n\n    popt, pcov = curve_fit(func, x, y, p0=[1, 1])\n\n    plt.plot(x, y, 'b-', label='data')\n    plt.plot(x, func(x, *popt), 'r-', label='fit: a=%5.3f, b=%5.3f' % tuple(popt))\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.legend()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 7: \nnumpy, sklearn.preprocessing, pandas\n```python\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\n\n# Constants\nARRAY_LENGTH = 100\n\ndef min_max_scaling():\n    \"\"\"\n    Generate a random array and apply Min-Max normalization (scaling) \n    to transform the array values to range between 0 and 1.\n\n    Returns:\n    numpy.array: The normalized array.\n\n    Requirements:\n    - numpy\n    - sklearn.preprocessing\n    - pandas\n\n    Example:\n    >>> min_max_scaling()\n    \"\"\"\n    array = np.random.randint(0, 100, ARRAY_LENGTH).reshape(-1, 1)\n    scaler = MinMaxScaler()\n    scaled_array = scaler.fit_transform(array)\n\n    return scaled_array\n```\n`GPT_ODEX_BREAK`"}
{"seed": "{\"task_id\": 34280147, \"prompt\": \"def f_34280147(l):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"l[3:] + l[:3]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(\\\"my-string\\\") == \\\"stringmy-\\\"\\n\", \"\\n    assert candidate(\\\"my \\\") == \\\"my \\\"\\n\", \"\\n    assert candidate(\\\"n;ho0-4w606[q\\\") == \\\"o0-4w606[qn;h\\\"\\n\"], \"entry_point\": \"f_34280147\", \"intent\": \"print a list `l` and move first 3 elements to the end of the list\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, itertools, random\n```python\nimport pandas as pd\nimport numpy as np\nfrom itertools import cycle\nfrom random import shuffle\n\n# Constants\nELEMENTS = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']\nN_GROUPS = 5\n\ndef generate_random_dataframe(l):\n    \"\"\"\n    Generate a dataframe from a list `l` and move first N_GROUPS elements to the end of each row.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - itertools.cycle\n    - random.shuffle\n\n    Example:\n    >>> generate_random_dataframe(ELEMENTS)\n    \"\"\"\n    shuffle(l)\n    l_cycled = cycle(l)\n    df = pd.DataFrame(np.array([next(l_cycled) for _ in range(len(l) * N_GROUPS)]).reshape(N_GROUPS, len(l)))\n    df = df.apply(lambda x: list(x[N_GROUPS:]) + list(x[:N_GROUPS]), axis=1)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, itertools, random\n```python\nimport pandas as pd\nimport numpy as np\nfrom itertools import cycle\nfrom random import shuffle, randint\n\n# Constants\nELEMENTS = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']\nN_GROUPS = 5\n\ndef generate_random_series(l):\n    \"\"\"\n    Generate a series from a list `l` and move first n randomly chosen elements to the end of the series.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - itertools.cycle\n    - random.shuffle\n    - random.randint\n\n    Example:\n    >>> generate_random_series(ELEMENTS)\n    \"\"\"\n    n = randint(1, len(l))\n    shuffle(l)\n    l_cycled = list(cycle(l))\n    series = pd.Series(l_cycled[:len(l) * N_GROUPS])\n    series = series.apply(lambda x: x[n:] + x[:n])\n\n    return series\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ncollections, random, itertools\n```python\nfrom collections import Counter\nfrom random import shuffle\nfrom itertools import cycle\n\n# Constants\nELEMENTS = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']\n\ndef generate_random_counter(l):\n    \"\"\"\n    Generate a Counter from a list `l` and move first 3 elements to the end of the list.\n    \n    Requirements:\n    - collections.Counter\n    - random.shuffle\n    - itertools.cycle\n\n    Example:\n    >>> generate_random_counter(ELEMENTS)\n    \"\"\"\n    shuffle(l)\n    l_cycled = cycle(l)\n    counter = Counter(next(l_cycled) for _ in range(30))\n    counter = Counter({k: counter[k] for k in list(counter.keys())[3:] + list(counter.keys())[:3]})\n\n    return counter\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, random\n```python\nimport numpy as np\nfrom random import shuffle\n\n# Constants\nELEMENTS = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']\n\ndef generate_random_array(l):\n    \"\"\"\n    Generate a numpy array from a list `l` and move first 3 elements to the end of the array.\n    \n    Requirements:\n    - numpy\n    - random.shuffle\n\n    Example:\n    >>> generate_random_array(ELEMENTS)\n    \"\"\"\n    shuffle(l)\n    arr = np.array(l)\n    arr = np.concatenate((arr[3:], arr[:3]))\n\n    return arr\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\ncollections, random, itertools\n```python\nfrom collections import deque\nfrom random import shuffle\nfrom itertools import cycle\n\n# Constants\nELEMENTS = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']\n\ndef generate_random_deque(l):\n    \"\"\"\n    Generate a deque from a list `l` and move first 3 elements to the end of the deque.\n    \n    Requirements:\n    - collections.deque\n    - random.shuffle\n    - itertools.cycle\n\n    Example:\n    >>> generate_random_deque(ELEMENTS)\n    \"\"\"\n    shuffle(l)\n    l_cycled = cycle(l)\n    dq = deque(next(l_cycled) for _ in range(15))\n    dq.rotate(-3)\n\n    return dq\n```\n"}
{"seed": "{\"task_id\": 4172131, \"prompt\": \"def f_4172131():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"[int(1000 * random.random()) for i in range(10000)]\", \"test_start\": \"\\nimport random\\n\\ndef check(candidate):\", \"test\": [\"\\n    result = candidate()\\n    assert isinstance(result, list)\\n    assert all([isinstance(item, int) for item in result])\\n\"], \"entry_point\": \"f_4172131\", \"intent\": \"create a random list of integers\", \"library\": [\"random\"]}", "generation": "Scenario 1: \npandas, random, statistics, matplotlib, numpy\n```python\nimport pandas as pd\nimport random\nimport statistics\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nRANGE = 10000\nSIZE = 1000\nBIN_WIDTH = 100\n\ndef create_random_dataframe():\n    \"\"\"\n    Create a pandas DataFrame with two columns, \"Random Numbers\" and \"Moving Average\", \n    containing a random list of integers and their moving average, respectively. \n    Then, plot a histogram of the \"Random Numbers\" column with a specified bin width.\n    \n    Requirements:\n    - pandas\n    - random\n    - statistics\n    - matplotlib.pyplot\n    - numpy\n    \n    Example:\n    >>> df = create_random_dataframe()\n    >>> print(df)\n    >>> df['Random Numbers'].plot(kind='hist', bins=np.arange(min(df['Random Numbers']), max(df['Random Numbers']) + BIN_WIDTH, BIN_WIDTH))\n    \"\"\"\n    numbers = [int(RANGE * random.random()) for _ in range(SIZE)]\n    moving_avg = [statistics.mean(numbers[max(0, i-5):i+1]) for i in range(SIZE)]\n    \n    df = pd.DataFrame({\n        'Random Numbers': numbers,\n        'Moving Average': moving_avg\n    })\n    \n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nrandom, math, matplotlib, numpy\n```python\nimport random\nimport math\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nRANGE = 10000\nSIZE = 1000\nPI = np.pi\n\ndef plot_sinusoidal_random():\n    \"\"\"\n    Generate a list of random sinusoidal values and plot them on a graph.\n    \n    Requirements:\n    - random\n    - math\n    - matplotlib.pyplot\n    - numpy\n    \n    Example:\n    >>> plot_sinusoidal_random()\n    \"\"\"\n    x_values = np.arange(0, SIZE)\n    y_values = [math.sin((2 * PI / RANGE) * (x + int(RANGE * random.random()))) for x in range(SIZE)]\n    \n    plt.plot(x_values, y_values)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nrandom, numpy, scipy, matplotlib\n```python\nimport random\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\n# Constants\nSIZE = 1000\nBIN_WIDTH = 100\n\ndef plot_normal_distribution():\n    \"\"\"\n    Generate a list of normally distributed random numbers and plot their histogram \n    and probability density function (PDF).\n    \n    Requirements:\n    - random\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n    \n    Example:\n    >>> plot_normal_distribution()\n    \"\"\"\n    data = np.random.randn(SIZE)\n    mu, std = stats.norm.fit(data)\n    \n    plt.hist(data, bins=np.arange(min(data), max(data) + BIN_WIDTH, BIN_WIDTH), density=True, alpha=0.6, color='g')\n\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, SIZE)\n    p = stats.norm.pdf(x, mu, std)\n    plt.plot(x, p, 'k', linewidth=2)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nrandom, numpy, pandas, seaborn, matplotlib\n```python\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nRANGE = 100\nSIZE = 1000\n\ndef plot_random_dataframe():\n    \"\"\"\n    Create a pandas DataFrame with two columns of random numbers and plot a scatterplot.\n    \n    Requirements:\n    - random\n    - numpy\n    - pandas\n    - seaborn\n    - matplotlib.pyplot\n    \n    Example:\n    >>> plot_random_dataframe()\n    \"\"\"\n    df = pd.DataFrame({\n        'X': np.random.randint(0, RANGE, SIZE),\n        'Y': np.random.randint(0, RANGE, SIZE)\n    })\n\n    sns.scatterplot(data=df, x='X', y='Y')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nrandom, numpy, sklearn, matplotlib\n```python\nimport random\nimport numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\n# Constants\nRANGE = 100\nSIZE = 1000\nCLUSTERS = 5\n\ndef random_kmeans():\n    \"\"\"\n    Generate a set of 2D random points and apply KMeans clustering, then plot the clusters.\n    \n    Requirements:\n    - random\n    - numpy\n    - sklearn.cluster\n    - matplotlib.pyplot\n    \n    Example:\n    >>> random_kmeans()\n    \"\"\"\n    data = np.array([(random.randint(0, RANGE), random.randint(0, RANGE)) for _ in range(SIZE)])\n    kmeans = KMeans(n_clusters=CLUSTERS)\n    kmeans.fit(data)\n    \n    plt.scatter(data[:, 0], data[:, 1], c=kmeans.labels_)\n    plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red')\n    plt.show()\n```"}
{"seed": "{\"task_id\": 6677332, \"prompt\": \"def f_6677332():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"datetime.datetime.now().strftime('%H:%M:%S.%f')\", \"test_start\": \"\\nimport datetime\\n\\ndef check(candidate):\", \"test\": [\"\\n    time_now = datetime.datetime.now().strftime('%H:%M:%S.%f')\\n    assert candidate().split('.')[0] == time_now.split('.')[0]\\n\"], \"entry_point\": \"f_6677332\", \"intent\": \"Using %f with strftime() in Python to get microseconds\", \"library\": [\"datetime\"]}", "generation": "Scenario 1: \npandas, datetime, os, random, matplotlib.pyplot\n```python\nimport pandas as pd\nfrom datetime import datetime\nimport os\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nFILE_PATH = 'temp_data.csv'\nTEMP_CATEGORIES = ['Cold', 'Normal', 'Hot']\n\ndef generate_and_plot_temperature_data(hours):\n    \"\"\"\n    Generate temperature data for the given number of hours and save it to a CSV file. \n    Then plots the data using matplotlib.\n    \n    Parameters:\n    hours (int): The number of hours for which temperature data is to be generated.\n    \n    Returns:\n    str: The path of the generated CSV file.\n    \n    Requirements:\n    - pandas\n    - datetime\n    - os\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> generate_and_plot_temperature_data(24)\n    \"\"\"\n    data = {'Time': [], 'Temperature': [], 'Category': []}\n    for i in range(hours):\n        temp = randint(-10, 40)  # random temperature between -10 and 40\n        data['Time'].append(datetime.now().strftime('%H:%M:%S.%f'))\n        data['Temperature'].append(temp)\n        if temp < 0:\n            data['Category'].append(TEMP_CATEGORIES[0])\n        elif temp > 25:\n            data['Category'].append(TEMP_CATEGORIES[2])\n        else:\n            data['Category'].append(TEMP_CATEGORIES[1])\n\n    df = pd.DataFrame(data)\n    df.to_csv(FILE_PATH, index=False)\n    \n    df.plot(x = 'Time', y = 'Temperature', kind = 'line')\n    plt.show()\n\n    return FILE_PATH\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \ndatetime, os, random, csv\n```python\nfrom datetime import datetime\nimport os\nfrom random import randint\nimport csv\n\n# Constants\nFILE_PATH = 'sensor_data.csv'\nSENSORS = ['Temperature', 'Humidity', 'Pressure']\n\ndef generate_sensor_data(hours):\n    \"\"\"\n    Generate sensor data for the given number of hours and save it to a CSV file.\n    \n    Parameters:\n    hours (int): The number of hours for which sensor data is to be generated.\n    \n    Returns:\n    str: The path of the generated CSV file.\n    \n    Requirements:\n    - datetime\n    - os\n    - random\n    - csv\n    \n    Example:\n    >>> generate_sensor_data(24)\n    \"\"\"\n    data = [['Time'] + SENSORS]\n    for i in range(hours):\n        row = [datetime.now().strftime('%H:%M:%S.%f')] + [randint(0, 100) for _ in SENSORS]\n        data.append(row)\n\n    with open(FILE_PATH, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerows(data)\n\n    return FILE_PATH\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \ndatetime, os, random, csv, matplotlib.pyplot\n```python\nfrom datetime import datetime\nimport os\nfrom random import randint\nimport csv\nimport matplotlib.pyplot as plt\n\n# Constants\nFILE_PATH = 'traffic_data.csv'\nVEHICLE_TYPES = ['Car', 'Bus', 'Truck', 'Bike']\n\ndef generate_and_plot_traffic_data(hours):\n    \"\"\"\n    Generate traffic data for the given number of hours, save it to a CSV file, \n    and plot the data.\n    \n    Parameters:\n    hours (int): The number of hours for which traffic data is to be generated.\n    \n    Returns:\n    str: The path of the generated CSV file.\n    \n    Requirements:\n    - datetime\n    - os\n    - random\n    - csv\n    - matplotlib.pyplot\n    \n    Example:\n    >>> generate_and_plot_traffic_data(24)\n    \"\"\"\n    data = [['Time'] + VEHICLE_TYPES]\n    for i in range(hours):\n        row = [datetime.now().strftime('%H:%M:%S.%f')] + [randint(0, 50) for _ in VEHICLE_TYPES]\n        data.append(row)\n\n    with open(FILE_PATH, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerows(data)\n\n    df = pd.read_csv(FILE_PATH)\n    df.plot(x = 'Time', y = VEHICLE_TYPES, kind = 'line')\n    plt.show()\n\n    return FILE_PATH\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \ndatetime, os, random, csv, shutil\n```python\nfrom datetime import datetime\nimport os\nfrom random import randint\nimport csv\nimport shutil\n\n# Constants\nFILE_PATH = 'weather_data.csv'\nWEATHER_CONDITIONS = ['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy']\nBACKUP_PATH = 'backup/'\n\ndef generate_weather_data_and_backup(hours):\n    \"\"\"\n    Generate weather data for the given number of hours, save it to a CSV file, \n    and back up the file to a backup directory.\n    \n    Parameters:\n    hours (int): The number of hours for which weather data is to be generated.\n    \n    Returns:\n    str: The path of the generated CSV file.\n    \n    Requirements:\n    - datetime\n    - os\n    - random\n    - csv\n    - shutil\n    \n    Example:\n    >>> generate_weather_data_and_backup(24)\n    \"\"\"\n    data = [['Time', 'Condition']]\n    for i in range(hours):\n        row = [datetime.now().strftime('%H:%M:%S.%f'), WEATHER_CONDITIONS[randint(0, len(WEATHER_CONDITIONS)-1)]]\n        data.append(row)\n\n    with open(FILE_PATH, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerows(data)\n\n    if not os.path.exists(BACKUP_PATH):\n        os.makedirs(BACKUP_PATH)\n    shutil.copy(FILE_PATH, BACKUP_PATH)\n\n    return FILE_PATH\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \ndatetime, time, random, matplotlib.pyplot\n```python\nfrom datetime import datetime\nimport time\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nVALUES_RANGE = (0, 100)\nPLOT_INTERVAL = 0.1\n\ndef generate_and_plot_real_time_data(duration):\n    \"\"\"\n    Generate and plot random data in real time for the given duration.\n    \n    Parameters:\n    duration (int): The duration in seconds for which data is to be generated and plotted.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - datetime\n    - time\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> generate_and_plot_real_time_data(60)\n    \"\"\"\n    plt.ion()\n    x_data = []\n    y_data = []\n\n    end_time = time.time() + duration\n    while time.time() < end_time:\n        x_data.append(datetime.now().strftime('%H:%M:%S.%f'))\n        y_data.append(randint(*VALUES_RANGE))\n\n        plt.clf()\n        plt.plot(x_data, y_data)\n        plt.draw()\n\n        time.sleep(PLOT_INTERVAL)\n```\n"}
{"seed": "{\"task_id\": 15325182, \"prompt\": \"def f_15325182(df):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"df.b.str.contains('^f')\", \"test_start\": \"\\nimport pandas as pd \\n\\ndef check(candidate):\", \"test\": [\"\\n    df = pd.DataFrame([[1, 'fat'], [2, 'hip'], [3, 'foo']], columns = ['a', 'b'])\\n    expected = [True, False, True]\\n    actual = candidate(df)\\n    for i in range (0, len(expected)):\\n        assert expected[i] == actual[i]\\n\"], \"entry_point\": \"f_15325182\", \"intent\": \"filter rows in pandas starting with alphabet 'f' using regular expression.\", \"library\": [\"pandas\"]}", "generation": "Scenario 1: \npandas, re, matplotlib.pyplot, random\n```python\nimport pandas as pd\nimport re\nimport matplotlib.pyplot as plt\nfrom random import choice, randint\n\n# Constants\nLETTERS = list('abcdefghijklmnopqrstuvwxyz')\n\ndef filter_and_plot(df, letter):\n    \"\"\"\n    The function filters rows in a DataFrame where a specific column's values start with a certain letter \n    using regular expression. Then it generates a bar plot of the count of each unique value in the filtered column.\n\n    Parameters:\n    df (DataFrame): The input DataFrame. It should have a 'Name' column.\n    letter (str): The letter to filter the 'Name' column.\n\n    Returns:\n    Series: A pandas Series of filtered 'Name' column.\n    \n    Requirements:\n    - pandas\n    - re\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> df = pd.DataFrame({'Name': [choice(LETTERS) + 'name' + str(randint(1, 100)) for _ in range(1000)]})\n    >>> filter_and_plot(df, 'a')\n    \"\"\"\n    regex = '^' + letter\n    filtered_df = df[df['Name'].str.contains(regex, regex=True)]\n    filtered_df['Name'].value_counts().plot(kind='bar')\n\n    return filtered_df['Name']\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \npandas, re, random, nltk\n```python\nimport pandas as pd\nimport re\nimport random\nfrom nltk.corpus import words\n\n# Constants\nLETTERS = list('abcdefghijklmnopqrstuvwxyz')\n\ndef filter_and_count_word_lengths(df, letter):\n    \"\"\"\n    The function filters rows in a DataFrame where a specific column's values start with a certain letter \n    using regular expression. Then it calculates the lengths of the words in the filtered column and returns \n    a dictionary of word lengths and their counts.\n\n    Parameters:\n    df (DataFrame): The input DataFrame. It should have a 'Word' column.\n    letter (str): The letter to filter the 'Word' column.\n\n    Returns:\n    dict: A dictionary of word lengths and their counts.\n    \n    Requirements:\n    - pandas\n    - re\n    - random\n    - nltk\n\n    Example:\n    >>> df = pd.DataFrame({'Word': random.sample(words.words(), 1000)})\n    >>> filter_and_count_word_lengths(df, 'a')\n    \"\"\"\n    regex = '^' + letter\n    filtered_df = df[df['Word'].str.contains(regex, regex=True)]\n    word_lengths = filtered_df['Word'].str.len()\n    count_dict = word_lengths.value_counts().to_dict()\n\n    return count_dict\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \npandas, re, random, nltk, matplotlib.pyplot\n```python\nimport pandas as pd\nimport re\nimport random\nfrom nltk.corpus import words\nimport matplotlib.pyplot as plt\n\n# Constants\nLETTERS = list('abcdefghijklmnopqrstuvwxyz')\n\ndef filter_and_plot_word_lengths(df, letter):\n    \"\"\"\n    The function filters rows in a DataFrame where a specific column's values start with a certain letter \n    using regular expression. Then it calculates the lengths of the words in the filtered column and \n    generates a histogram of the word lengths.\n\n    Parameters:\n    df (DataFrame): The input DataFrame. It should have a 'Word' column.\n    letter (str): The letter to filter the 'Word' column.\n\n    Returns:\n    None.\n    \n    Requirements:\n    - pandas\n    - re\n    - random\n    - nltk\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame({'Word': random.sample(words.words(), 1000)})\n    >>> filter_and_plot_word_lengths(df, 'a')\n    \"\"\"\n    regex = '^' + letter\n    filtered_df = df[df['Word'].str.contains(regex, regex=True)]\n    word_lengths = filtered_df['Word'].str.len()\n    plt.hist(word_lengths, bins=range(min(word_lengths), max(word_lengths) + 1), alpha=0.7, edgecolor='black')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \npandas, re, random, nltk, numpy\n```python\nimport pandas as pd\nimport re\nimport random\nfrom nltk.corpus import words\nimport numpy as np\n\n# Constants\nLETTERS = list('abcdefghijklmnopqrstuvwxyz')\n\ndef filter_and_analyze_word_lengths(df, letter):\n    \"\"\"\n    The function filters rows in a DataFrame where a specific column's values start with a certain letter \n    using regular expression. Then it calculates the lengths of the words in the filtered column and \n    returns basic statistics (mean, median, mode) of the word lengths.\n\n    Parameters:\n    df (DataFrame): The input DataFrame. It should have a 'Word' column.\n    letter (str): The letter to filter the 'Word' column.\n\n    Returns:\n    dict: A dictionary of mean, median, and mode of word lengths.\n    \n    Requirements:\n    - pandas\n    - re\n    - random\n    - nltk\n    - numpy\n\n    Example:\n    >>> df = pd.DataFrame({'Word': random.sample(words.words(), 1000)})\n    >>> filter_and_analyze_word_lengths(df, 'a')\n    \"\"\"\n    regex = '^' + letter\n    filtered_df = df[df['Word'].str.contains(regex, regex=True)]\n    word_lengths = filtered_df['Word'].str.len()\n    statistics = {'mean': np.mean(word_lengths), 'median': np.median(word_lengths), 'mode': word_lengths.mode().values[0]}\n\n    return statistics\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \npandas, re, random, nltk, seaborn\n```python\nimport pandas as pd\nimport re\nimport random\nfrom nltk.corpus import words\nimport seaborn as sns\n\n# Constants\nLETTERS = list('abcdefghijklmnopqrstuvwxyz')\n\ndef filter_and_boxplot_word_lengths(df, letter):\n    \"\"\"\n    The function filters rows in a DataFrame where a specific column's values start with a certain letter \n    using regular expression. Then it calculates the lengths of the words in the filtered column and \n    generates a box plot of the word lengths.\n\n    Parameters:\n    df (DataFrame): The input DataFrame. It should have a 'Word' column.\n    letter (str): The letter to filter the 'Word' column.\n\n    Returns:\n    None.\n    \n    Requirements:\n    - pandas\n    - re\n    - random\n    - nltk\n    - seaborn\n\n    Example:\n    >>> df = pd.DataFrame({'Word': random.sample(words.words(), 1000)})\n    >>> filter_and_boxplot_word_lengths(df, 'a')\n    \"\"\"\n    regex = '^' + letter\n    filtered_df = df[df['Word'].str.contains(regex, regex=True)]\n    word_lengths = filtered_df['Word'].str.len()\n    sns.boxplot(x=word_lengths)\n    plt.show()\n```"}
{"seed": "{\"task_id\": 583557, \"prompt\": \"def f_583557(tab):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"'\\\\n'.join('\\\\t'.join(str(col) for col in row) for row in tab)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([[1,2,3],[4,5,6]]) == \\\"1\\\\t2\\\\t3\\\\n4\\\\t5\\\\t6\\\"\\n\", \"\\n    assert candidate([[1, 'x' ,3],[4.4,5,\\\"six\\\"]]) == \\\"1\\\\tx\\\\t3\\\\n4.4\\\\t5\\\\tsix\\\"\\n\", \"\\n    assert candidate([]) == \\\"\\\"\\n\", \"\\n    assert candidate([[],[],[]]) == \\\"\\\\n\\\\n\\\"\\n\"], \"entry_point\": \"f_583557\", \"intent\": \"print a 2 dimensional list `tab` as a table with delimiters\", \"library\": []}", "generation": "Scenario 3: \npandas, numpy, random, csv\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import choice\nimport csv\n\n# Constants\nLETTERS = list('abcdefghijklmnopqrstuvwxyz')\n\ndef create_random_csv_matrix(file_path):\n    \"\"\"\n    Create a CSV file with a 2D matrix populated with random alphanumeric characters.\n    \n    Parameters:\n    file_path (str): The path of the CSV file to be created.\n    \n    Returns:\n    None: Writes a CSV file to the specified path.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - random\n    - csv\n\n    Example:\n    >>> create_random_csv_matrix('random_matrix.csv')\n    \"\"\"\n    matrix = pd.DataFrame(np.random.choice(LETTERS, (10, 10)))\n    matrix.to_csv(file_path, sep='\\t', header=False, index=False)\n\n    return None\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, itertools\n```python\nimport pandas as pd\nimport numpy as np\nimport itertools\n\ndef tabulate_matrices(matrix1, matrix2):\n    \"\"\"\n    Join two 2D numpy arrays (matrices) along the second axis (columns), convert \n    it to a pandas DataFrame, and return a string representation of the DataFrame.\n    \n    Parameters:\n    matrix1 (array): The first 2D numpy array.\n    matrix2 (array): The second 2D numpy array.\n    \n    Returns:\n    str: The string representation of the DataFrame.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - itertools\n\n    Example:\n    >>> matrix1 = np.array([[1, 2, 3], [4, 5, 6]])\n    >>> matrix2 = np.array([[7, 8, 9], [10, 11, 12]])\n    >>> print(tabulate_matrices(matrix1, matrix2))\n    \"\"\"\n    combined_matrix = np.concatenate((matrix1, matrix2), axis=1)\n    df = pd.DataFrame(combined_matrix)\n\n    return df.to_string(index=False, header=False)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef visualize_matrix(matrix):\n    \"\"\"\n    Visualize a 2D numpy array (matrix) as a heatmap using matplotlib.\n    \n    Parameters:\n    matrix (array): The 2D numpy array.\n    \n    Returns:\n    None: Displays a heatmap.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    >>> visualize_matrix(matrix)\n    \"\"\"\n    df = pd.DataFrame(matrix)\n\n    plt.imshow(df, cmap='hot', interpolation='nearest')\n    plt.show()\n\n    return None\n```\n`GPT_ODEX_BREAK`\n\nScenario 6:\npandas, numpy, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n\ndef cluster_matrix(matrix):\n    \"\"\"\n    Perform hierarchical clustering on a 2D numpy array (matrix) and visualize the results.\n    \n    Parameters:\n    matrix (array): The 2D numpy array.\n    \n    Returns:\n    None: Displays a dendrogram and a clustered heatmap.\n\n    Requirements:\n    - pandas\n    - numpy\n    - seaborn\n\n    Example:\n    >>> matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    >>> cluster_matrix(matrix)\n    \"\"\"\n    df = pd.DataFrame(matrix)\n\n    sns.clustermap(df)\n\n    return None\n```\n`GPT_ODEX_BREAK`\n\nScenario 7:\npandas, numpy, scipy\n```python\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\n\ndef normalize_matrix(matrix):\n    \"\"\"\n    Normalize a 2D numpy array (matrix) using the Z-score.\n    \n    Parameters:\n    matrix (array): The 2D numpy array.\n    \n    Returns:\n    DataFrame: The normalized DataFrame.\n\n    Requirements:\n    - pandas\n    - numpy\n    - scipy\n\n    Example:\n    >>> matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    >>> normalize_matrix(matrix)\n    \"\"\"\n    df = pd.DataFrame(matrix)\n\n    normalized_df = df.apply(stats.zscore)\n\n    return normalized_df\n```"}
{"seed": "{\"task_id\": 38535931, \"prompt\": \"def f_38535931(df, tuples):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"df.set_index(list('BC')).drop(tuples, errors='ignore').reset_index()\", \"test_start\": \"\\nimport pandas as pd\\n\\ndef check(candidate):\", \"test\": [\"\\n    df = pd.DataFrame([[3, 4], [4, 5], [-1, -2]], columns = ['B', 'C'])\\n    tuples = [(3, 4), (-1, -2)]\\n    expected = pd.DataFrame([[4, 5]], columns = ['B', 'C'])\\n    actual = candidate(df, tuples)\\n    assert pd.DataFrame.equals(actual, expected)\\n\"], \"entry_point\": \"f_38535931\", \"intent\": \"pandas: delete rows in dataframe `df` based on multiple columns values\", \"library\": [\"pandas\"]}", "generation": "Scenario 1:\npandas, numpy, matplotlib, random\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import sample\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef modify_dataframe(df, tuples, n_plots):\n    \"\"\"\n    Remove rows from a dataframe based on multiple columns values, then generate n \n    random plots of two columns against each other.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n    tuples (list): A list of tuples, where each tuple represents a row to be removed.\n    n_plots (int): The number of plots to be generated.\n\n    Returns:\n    DataFrame: The modified DataFrame.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> df = pd.DataFrame(np.random.randint(0,100,size=(100, 5)), columns=list('ABCDE'))\n    >>> tuples = [(10, 20, 30, 40, 50), (60, 70, 80, 90, 100)]\n    >>> modify_dataframe(df, tuples, 3)\n    \"\"\"\n    df.set_index(list('ABCDE')).drop(tuples, errors='ignore').reset_index()\n\n    for _ in range(n_plots):\n        selected_columns = sample(COLUMNS, 2)\n        df.plot(x=selected_columns[0], y=selected_columns[1])\n\n    plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, seaborn, random\n```python\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom random import sample\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef analyze_dataframe(df, tuples, n_plots):\n    \"\"\"\n    Remove rows from a dataframe based on multiple columns values, then generate n \n    random pairplots of two columns against each other.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n    tuples (list): A list of tuples, where each tuple represents a row to be removed.\n    n_plots (int): The number of pairplots to be generated.\n\n    Returns:\n    DataFrame: The modified DataFrame.\n\n    Requirements:\n    - pandas\n    - numpy\n    - seaborn\n    - random\n\n    Example:\n    >>> df = pd.DataFrame(np.random.randint(0,100,size=(100, 5)), columns=list('ABCDE'))\n    >>> tuples = [(10, 20, 30, 40, 50), (60, 70, 80, 90, 100)]\n    >>> analyze_dataframe(df, tuples, 3)\n    \"\"\"\n    df.set_index(list('ABCDE')).drop(tuples, errors='ignore').reset_index()\n\n    for _ in range(n_plots):\n        selected_columns = sample(COLUMNS, 2)\n        sns.pairplot(df, vars=selected_columns)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, matplotlib, random\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import sample\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef visualize_dataframe(df, tuples, n_plots):\n    \"\"\"\n    Remove rows from a dataframe based on multiple columns values, then generate n \n    random scatter plots of two columns against each other.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n    tuples (list): A list of tuples, where each tuple represents a row to be removed.\n    n_plots (int): The number of scatter plots to be generated.\n\n    Returns:\n    DataFrame: The modified DataFrame.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> df = pd.DataFrame(np.random.randint(0,100,size=(100, 5)), columns=list('ABCDE'))\n    >>> tuples = [(10, 20, 30, 40, 50), (60, 70, 80, 90, 100)]\n    >>> visualize_dataframe(df, tuples, 3)\n    \"\"\"\n    df.set_index(list('ABCDE')).drop(tuples, errors='ignore').reset_index()\n\n    for _ in range(n_plots):\n        selected_columns = sample(COLUMNS, 2)\n        plt.scatter(df[selected_columns[0]], df[selected_columns[1]])\n\n    plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, seaborn, random\n```python\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom random import sample\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef explore_dataframe(df, tuples, n_plots):\n    \"\"\"\n    Remove rows from a dataframe based on multiple columns values, then generate n \n    random jointplots of two columns against each other.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n    tuples (list): A list of tuples, where each tuple represents a row to be removed.\n    n_plots (int): The number of jointplots to be generated.\n\n    Returns:\n    DataFrame: The modified DataFrame.\n\n    Requirements:\n    - pandas\n    - numpy\n    - seaborn\n    - random\n\n    Example:\n    >>> df = pd.DataFrame(np.random.randint(0,100,size=(100, 5)), columns=list('ABCDE'))\n    >>> tuples = [(10, 20, 30, 40, 50), (60, 70, 80, 90, 100)]\n    >>> explore_dataframe(df, tuples, 3)\n    \"\"\"\n    df.set_index(list('ABCDE')).drop(tuples, errors='ignore').reset_index()\n\n    for _ in range(n_plots):\n        selected_columns = sample(COLUMNS, 2)\n        sns.jointplot(data=df, x=selected_columns[0], y=selected_columns[1])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, matplotlib, random\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import sample\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef plot_dataframe(df, tuples, n_plots):\n    \"\"\"\n    Remove rows from a dataframe based on multiple columns values, then generate n \n    random line plots of two columns against each other.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n    tuples (list): A list of tuples, where each tuple represents a row to be removed.\n    n_plots (int): The number of line plots to be generated.\n\n    Returns:\n    DataFrame: The modified DataFrame.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> df = pd.DataFrame(np.random.randint(0,100,size=(100, 5)), columns=list('ABCDE'))\n    >>> tuples = [(10, 20, 30, 40, 50), (60, 70, 80, 90, 100)]\n    >>> plot_dataframe(df, tuples, 3)\n    \"\"\"\n    df.set_index(list('ABCDE')).drop(tuples, errors='ignore').reset_index()\n\n    for _ in range(n_plots):\n        selected_columns = sample(COLUMNS, 2)\n        plt.plot(df[selected_columns[0]], df[selected_columns[1]])\n\n    plt.show()\n\n    return df\n```\nAbove are the illustrations."}
{"seed": "{\"task_id\": 13945749, \"prompt\": \"def f_13945749(goals, penalties):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"\\\"\\\"\\\"({:d} goals, ${:d})\\\"\\\"\\\".format(goals, penalties)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(0, 0) == \\\"(0 goals, $0)\\\"\\n\", \"\\n    assert candidate(123, 2) == \\\"(123 goals, $2)\\\"\\n\"], \"entry_point\": \"f_13945749\", \"intent\": \"format the variables `goals` and `penalties` using string formatting\", \"library\": []}", "generation": "Scenario 1:\npandas, matplotlib, random\n```python\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom random import choice\n\n# Constants\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTIES_COST = [100, 200, 300, 400, 500]\n\ndef generate_match_report(goals, penalties):\n    \"\"\"\n    Generate a match report for a set of teams showing goals scored and penalties incurred.\n    \n    Parameters:\n    goals (dict): A dictionary where keys are team names and values are numbers of goals scored.\n    penalties (dict): A dictionary where keys are team names and values are numbers of penalties incurred.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the match report for the teams.\n    \n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - random.choice\n    \n    Example:\n    >>> goals = {'Team A': 3, 'Team B': 2, 'Team C': 1, 'Team D': 0, 'Team E': 2}\n    >>> penalties = {'Team A': 1, 'Team B': 0, 'Team C': 2, 'Team D': 3, 'Team E': 1}\n    >>> report = generate_match_report(goals, penalties)\n    >>> print(report)\n    >>> report[['Goals', 'Penalties Cost']].plot(kind='bar')\n    \"\"\"\n    report_data = []\n\n    for team in TEAMS:\n        team_goals = goals.get(team, 0)\n        team_penalties = penalties.get(team, 0)\n        penalties_cost = team_penalties * choice(PENALTIES_COST)\n        report_data.append([team, team_goals, team_penalties, penalties_cost])\n\n    report_df = pd.DataFrame(report_data, columns=['Team', 'Goals', 'Penalties', 'Penalties Cost'])\n\n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ndatetime, json, requests\n```python\nfrom datetime import datetime\nimport json\nimport requests\n\n# Constants\nAPI_URL = 'http://api.football-data.org/v2/competitions/PL/standings'\n\ndef get_league_standings(goals, penalties):\n    \"\"\"\n    Retrieve the current standings for a football league and update with provided goals and penalties.\n    \n    Parameters:\n    goals (dict): A dictionary where keys are team names and values are numbers of goals scored.\n    penalties (dict): A dictionary where keys are team names and values are numbers of penalties incurred.\n    \n    Returns:\n    dict: A dictionary with updated league standings.\n    \n    Requirements:\n    - datetime\n    - json\n    - requests\n    \n    Example:\n    >>> goals = {'Liverpool': 3, 'Manchester City': 2}\n    >>> penalties = {'Liverpool': 1, 'Manchester City': 0}\n    >>> standings = get_league_standings(goals, penalties)\n    >>> print(json.dumps(standings, indent=2))\n    \"\"\"\n    response = requests.get(API_URL)\n    standings = response.json()\n\n    for team_standings in standings['standings']:\n        team_name = team_standings['team']['name']\n        team_standings['goalsFor'] += goals.get(team_name, 0)\n        team_standings['goalsAgainst'] += penalties.get(team_name, 0)\n\n    return standings\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, matplotlib, numpy\n```python\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n# Constants\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nGOALS_RANGE = (0, 10)\n\ndef generate_scores_distribution(goals, penalties):\n    \"\"\"\n    Generate a distribution of scores for a set of teams.\n\n    Parameters:\n    goals (dict): A dictionary where keys are team names and values are numbers of goals scored.\n    penalties (dict): A dictionary where keys are team names and values are numbers of penalties incurred.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the scores distribution for the teams.\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - numpy\n\n    Example:\n    >>> goals = {'Team A': 3, 'Team B': 2, 'Team C': 1, 'Team D': 0, 'Team E': 2}\n    >>> penalties = {'Team A': 1, 'Team B': 0, 'Team C': 2, 'Team D': 3, 'Team E': 1}\n    >>> distribution = generate_scores_distribution(goals, penalties)\n    >>> print(distribution)\n    >>> distribution.plot(kind='hist')\n    \"\"\"\n    scores_data = []\n\n    for team in TEAMS:\n        team_goals = goals.get(team, 0)\n        team_penalties = penalties.get(team, 0)\n        score = team_goals - team_penalties\n        scores_data.append([team, score])\n\n    scores_df = pd.DataFrame(scores_data, columns=['Team', 'Score'])\n    scores_df['Score'] = scores_df['Score'].clip(*GOALS_RANGE)\n\n    return scores_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ncsv, os, collections\n```python\nimport csv\nimport os\nfrom collections import Counter\n\n# Constants\nCSV_FILE_PATH = 'match_data.csv'\n\ndef count_goals_penalties(goals, penalties):\n    \"\"\"\n    Count the total number of goals and penalties from a CSV file and update with given goals and penalties.\n\n    Parameters:\n    goals (dict): A dictionary where keys are team names and values are numbers of goals scored.\n    penalties (dict): A dictionary where keys are team names and values are numbers of penalties incurred.\n\n    Returns:\n    Counter: A Counter object with total counts of goals and penalties.\n\n    Requirements:\n    - csv\n    - os\n    - collections.Counter\n\n    Example:\n    >>> goals = {'Team A': 3, 'Team B': 2, 'Team C': 1, 'Team D': 0, 'Team E': 2}\n    >>> penalties = {'Team A': 1, 'Team B': 0, 'Team C': 2, 'Team D': 3, 'Team E': 1}\n    >>> counts = count_goals_penalties(goals, penalties)\n    >>> print(counts)\n    \"\"\"\n    counts = Counter()\n\n    if os.path.exists(CSV_FILE_PATH):\n        with open(CSV_FILE_PATH, 'r') as file:\n            reader = csv.DictReader(file)\n            for row in reader:\n                counts['goals'] += int(row['goals'])\n                counts['penalties'] += int(row['penalties'])\n\n    for team, team_goals in goals.items():\n        counts['goals'] += team_goals\n\n    for team, team_penalties in penalties.items():\n        counts['penalties'] += team_penalties\n\n    return counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, matplotlib, seaborn\n```python\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n# Constants\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\n\ndef visualize_goals_penalties(goals, penalties):\n    \"\"\"\n    Visualize the distribution of goals and penalties for a set of teams.\n\n    Parameters:\n    goals (dict): A dictionary where keys are team names and values are numbers of goals scored.\n    penalties (dict): A dictionary where keys are team names and values are numbers of penalties incurred.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the goals and penalties for the teams.\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> goals = {'Team A': 3, 'Team B': 2, 'Team C': 1, 'Team D': 0, 'Team E': 2}\n    >>> penalties = {'Team A': 1, 'Team B': 0, 'Team C': 2, 'Team D': 3, 'Team E': 1}\n    >>> df = visualize_goals_penalties(goals, penalties)\n    >>> print(df)\n    >>> sns.pairplot(df, hue='Team')\n    \"\"\"\n    data = []\n\n    for team in TEAMS:\n        team_goals = goals.get(team, 0)\n        team_penalties = penalties.get(team, 0)\n        data.append([team, team_goals, team_penalties])\n\n    df = pd.DataFrame(data, columns=['Team', 'Goals', 'Penalties'])\n\n    return df\n```\n"}
{"seed": "{\"task_id\": 13945749, \"prompt\": \"def f_13945749(goals, penalties):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"\\\"\\\"\\\"({} goals, ${})\\\"\\\"\\\".format(goals, penalties)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(0, 0) == \\\"(0 goals, $0)\\\"\\n\", \"\\n    assert candidate(123, \\\"???\\\") == \\\"(123 goals, $???)\\\"\\n\", \"\\n    assert candidate(\\\"x\\\", 0.0) == \\\"(x goals, $0.0)\\\"\\n\"], \"entry_point\": \"f_13945749\", \"intent\": \"format string \\\"({} goals, ${})\\\" with variables `goals` and `penalties`\", \"library\": []}", "generation": "Scenario 1:\nnumpy, pandas, re, random\n```python\nimport numpy as np\nimport pandas as pd\nimport re\nfrom random import randint\n\n# Constants\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000  # in dollars\n\ndef generate_match_results(goals, penalties):\n    \"\"\"\n    Generate a pandas DataFrame of football match results for multiple teams \n    with random goals and penalties. The penalties are converted to monetary \n    penalties according to a penalty cost.\n\n    Parameters:\n    goals (int): The maximum number of goals a team can score in a match.\n    penalties (int): The maximum number of penalties a team can receive in a match.\n\n    Returns:\n    DataFrame: A pandas DataFrame with match results for the teams.\n\n    Requirements:\n    - pandas\n    - numpy\n    - random\n\n    Example:\n    >>> results = generate_match_results(5, 3)\n    >>> print(results)\n    \"\"\"\n\n    match_results = []\n\n    for team in TEAMS:\n        team_goals = randint(0, goals)\n        team_penalties = randint(0, penalties)\n        penalty_cost = PENALTY_COST * team_penalties\n        result_string = \"({} goals, ${})\".format(team_goals, penalty_cost)\n        match_results.append([team, result_string])\n\n    results_df = pd.DataFrame(match_results, columns=['Team', 'Match Result'])\n\n    return results_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, pandas, matplotlib, random\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000  # in dollars\n\ndef plot_match_results(goals, penalties):\n    \"\"\"\n    Generate and plot a pandas DataFrame of football match results for multiple \n    teams with random goals and penalties. The penalties are converted to \n    monetary penalties according to a penalty cost.\n\n    Parameters:\n    goals (int): The maximum number of goals a team can score in a match.\n    penalties (int): The maximum number of penalties a team can receive in a match.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> plot_match_results(5, 3)\n    \"\"\"\n\n    match_results = []\n\n    for team in TEAMS:\n        team_goals = randint(0, goals)\n        team_penalties = randint(0, penalties)\n        penalty_cost = PENALTY_COST * team_penalties\n        match_results.append([team, team_goals, penalty_cost])\n\n    results_df = pd.DataFrame(match_results, columns=['Team', 'Goals', 'Penalty Cost'])\n\n    results_df.set_index('Team').plot(kind='bar', stacked=True)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, pandas, matplotlib, random, re\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import randint\nimport re\n\n# Constants\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000  # in dollars\n\ndef analyze_match_results(goals, penalties):\n    \"\"\"\n    Generate a pandas DataFrame of football match results for multiple teams \n    with random goals and penalties. The penalties are converted to monetary \n    penalties according to a penalty cost. Then, analyze and visualize the data.\n\n    Parameters:\n    goals (int): The maximum number of goals a team can score in a match.\n    penalties (int): The maximum number of penalties a team can receive in a match.\n\n    Returns:\n    DataFrame: A pandas DataFrame with analyzed data.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - random\n    - re\n\n    Example:\n    >>> analyzed_data = analyze_match_results(5, 3)\n    >>> print(analyzed_data)\n    \"\"\"\n\n    match_results = []\n\n    for team in TEAMS:\n        team_goals = randint(0, goals)\n        team_penalties = randint(0, penalties)\n        penalty_cost = PENALTY_COST * team_penalties\n        result_string = \"({} goals, ${})\".format(team_goals, penalty_cost)\n        match_results.append([team, result_string])\n\n    results_df = pd.DataFrame(match_results, columns=['Team', 'Match Result'])\n\n    # Extract goals and penalty cost from the result string\n    results_df['Goals'] = results_df['Match Result'].apply(lambda x: int(re.search(r'\\((\\d+) goals', x).group(1)))\n    results_df['Penalty Cost'] = results_df['Match Result'].apply(lambda x: int(re.search(r'\\$(\\d+)', x).group(1)))\n\n    # Plot the results\n    results_df.set_index('Team')[['Goals', 'Penalty Cost']].plot(kind='bar', stacked=True)\n\n    return results_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, pandas, seaborn, random\n```python\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom random import randint\n\n# Constants\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000  # in dollars\n\ndef visualize_match_results(goals, penalties):\n    \"\"\"\n    Generate and visualize a pandas DataFrame of football match results for \n    multiple teams with random goals and penalties. The penalties are converted \n    to monetary penalties according to a penalty cost.\n\n    Parameters:\n    goals (int): The maximum number of goals a team can score in a match.\n    penalties (int): The maximum number of penalties a team can receive in a match.\n\n    Requirements:\n    - pandas\n    - numpy\n    - seaborn\n    - random\n\n    Example:\n    >>> visualize_match_results(5, 3)\n    \"\"\"\n\n    match_results = []\n\n    for team in TEAMS:\n        team_goals = randint(0, goals)\n        team_penalties = randint(0, penalties)\n        penalty_cost = PENALTY_COST * team_penalties\n        match_results.append([team, team_goals, penalty_cost])\n\n    results_df = pd.DataFrame(match_results, columns=['Team', 'Goals', 'Penalty Cost'])\n\n    sns.barplot(x='Team', y='Goals', data=results_df, palette='viridis')\n    plt.show()\n\n    sns.barplot(x='Team', y='Penalty Cost', data=results_df, palette='viridis')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, pandas, sklearn, random\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom random import randint\n\n# Constants\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000  # in dollars\n\ndef predict_penalty_cost(goals, penalties):\n    \"\"\"\n    Generate a pandas DataFrame of football match results for multiple teams \n    with random goals and penalties. The penalties are converted to monetary \n    penalties according to a penalty cost. Then, create a linear regression \n    model to predict the penalty cost based on the number of goals.\n\n    Parameters:\n    goals (int): The maximum number of goals a team can score in a match.\n    penalties (int): The maximum number of penalties a team can receive in a match.\n\n    Returns:\n    LinearRegression: A LinearRegression model.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.linear_model.LinearRegression\n    - random\n\n    Example:\n    >>> model = predict_penalty_cost(5, 3)\n    \"\"\"\n\n    match_results = []\n\n    for team in TEAMS:\n        team_goals = randint(0, goals)\n        team_penalties = randint(0, penalties)\n        penalty_cost = PENALTY_COST * team_penalties\n        match_results.append([team, team_goals, penalty_cost])\n\n    results_df = pd.DataFrame(match_results, columns=['Team', 'Goals', 'Penalty Cost'])\n\n    # Create a Linear Regression model\n    X = results_df['Goals'].values.reshape(-1, 1)\n    y = results_df['Penalty Cost']\n\n    model = LinearRegression()\n    model.fit(X, y)\n\n    return model\n```"}
{"seed": "{\"task_id\": 18524642, \"prompt\": \"def f_18524642(L):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"[int(''.join(str(d) for d in x)) for x in L]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([[1,2], [2,3,4], [1,0,0]]) == [12,234,100]\\n\", \"\\n    assert candidate([[1], [2], [3]]) == [1,2,3]\\n\"], \"entry_point\": \"f_18524642\", \"intent\": \"convert list of lists `L` to list of integers\", \"library\": []}", "generation": "Scenario 1:\nnumpy, itertools, pandas, random\n```python\nimport numpy as np\nfrom itertools import product\nimport pandas as pd\nfrom random import randint\n\n# Constants\nRANGE = (1, 100)\nSIZE = (10, 10)\n\ndef create_random_df_from_nested_list(L):\n    \"\"\"\n    Convert a list of lists `L` to a pandas DataFrame filled with random integers \n    where the number of rows and columns correspond to the integers in the nested lists.\n    \n    Requirements:\n    - numpy\n    - itertools\n    - pandas\n    - random\n\n    Parameters:\n    L (list of lists): A list of lists where each sublist contains two integers.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with random integers.\n    \n    Example:\n    >>> create_random_df_from_nested_list([[2, 3], [5, 6]])\n    \"\"\"\n    dimensions = list(product(*L))\n    random_array = np.random.randint(*RANGE, size=dimensions)\n    df = pd.DataFrame(random_array)\n    \n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, itertools, sklearn.preprocessing, matplotlib.pyplot\n```python\nimport numpy as np\nfrom itertools import chain\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\n# Constants\nRANGE = (-100, 100)\nSIZE = 100\n\ndef plot_standardized_list_of_lists(L):\n    \"\"\"\n    Convert a list of lists `L` to a list of integers, standardize it and plot the result.\n    \n    Requirements:\n    - numpy\n    - itertools\n    - sklearn.preprocessing\n    - matplotlib.pyplot\n\n    Parameters:\n    L (list of lists): A list of lists where each sublist contains integers.\n    \n    Returns:\n    None\n\n    Example:\n    >>> plot_standardized_list_of_lists([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    \"\"\"\n    data = list(chain(*L))\n    data = np.array(data).reshape(-1, 1)\n\n    scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(data)\n\n    plt.plot(standardized_data)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, itertools, scipy.stats, matplotlib.pyplot\n```python\nimport numpy as np\nfrom itertools import chain\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\n# Constants\nRANGE = (-100, 100)\nSIZE = 100\n\ndef plot_normal_distribution_of_list_of_lists(L):\n    \"\"\"\n    Convert a list of lists `L` to a list of integers, fit a normal distribution to it and plot the result.\n    \n    Requirements:\n    - numpy\n    - itertools\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Parameters:\n    L (list of lists): A list of lists where each sublist contains integers.\n    \n    Returns:\n    None\n\n    Example:\n    >>> plot_normal_distribution_of_list_of_lists([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    \"\"\"\n    data = list(chain(*L))\n    mu, std = norm.fit(data)\n\n    plt.hist(data, bins=30, density=True, alpha=0.6, color='g')\n\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, mu, std)\n    plt.plot(x, p, 'k', linewidth=2)\n    title = \"Fit results: mu = %.2f,  std = %.2f\" % (mu, std)\n    plt.title(title)\n\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, itertools, sklearn.cluster, matplotlib.pyplot\n```python\nimport numpy as np\nfrom itertools import chain\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\n# Constants\nN_CLUSTERS = 3\n\ndef apply_kmeans_to_list_of_lists(L):\n    \"\"\"\n    Convert a list of lists `L` to a list of integers, apply KMeans clustering to it and plot the result.\n    \n    Requirements:\n    - numpy\n    - itertools\n    - sklearn.cluster\n    - matplotlib.pyplot\n\n    Parameters:\n    L (list of lists): A list of lists where each sublist contains integers.\n    \n    Returns:\n    None\n\n    Example:\n    >>> apply_kmeans_to_list_of_lists([[1, 2, 3], [50, 60, 70], [100, 110, 120]])\n    \"\"\"\n    data = list(chain(*L))\n    data = np.array(data).reshape(-1, 1)\n\n    kmeans = KMeans(n_clusters=N_CLUSTERS).fit(data)\n\n    plt.scatter(data, [0]*len(data), c=kmeans.labels_.astype(float))\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, itertools, sklearn.decomposition, matplotlib.pyplot\n```python\nimport numpy as np\nfrom itertools import chain\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\n# Constants\nN_COMPONENTS = 2\n\ndef apply_pca_to_list_of_lists(L):\n    \"\"\"\n    Convert a list of lists `L` to a 2D numpy array, apply PCA to it and plot the result.\n    \n    Requirements:\n    - numpy\n    - itertools\n    - sklearn.decomposition\n    - matplotlib.pyplot\n\n    Parameters:\n    L (list of lists): A list of lists where each sublist contains integers.\n    \n    Returns:\n    None\n\n    Example:\n    >>> apply_pca_to_list_of_lists([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    \"\"\"\n    data = np.array(L)\n\n    pca = PCA(n_components=N_COMPONENTS)\n    pca_result = pca.fit_transform(data)\n\n    plt.scatter(pca_result[:,0], pca_result[:,1])\n    plt.show()\n```\n"}
{"seed": "{\"task_id\": 18524642, \"prompt\": \"def f_18524642(L):\\n\\t\", \"suffix\": \"\\n\\treturn L\", \"canonical_solution\": \"L = [int(''.join([str(y) for y in x])) for x in L]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([[1,2], [2,3,4], [1,0,0]]) == [12,234,100]\\n\", \"\\n    assert candidate([[1], [2], [3]]) == [1,2,3]\\n\", \"\\n    assert candidate([[1, 0], [0, 2], [3], [0, 0, 0, 0]]) == [10,2,3, 0]\\n\"], \"entry_point\": \"f_18524642\", \"intent\": \"convert a list of lists `L` to list of integers\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, math, random\n```python\nimport pandas as pd\nimport numpy as np\nimport math\nfrom random import randint\n\n# Constants\nCITIES = ['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']\n\ndef generate_population_data(cities_list):\n    \"\"\"\n    Generate a DataFrame of population data for a list of cities. The population is randomly\n    generated and rounded off to the nearest thousand.\n\n    Parameters:\n    cities_list (list): A list of city names.\n\n    Returns:\n    DataFrame: A pandas DataFrame with population data for the cities.\n\n    Requirements:\n    - pandas\n    - numpy\n    - math\n    - random\n\n    Example:\n    >>> cities = ['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']\n    >>> pop_data = generate_population_data(cities)\n    >>> print(pop_data)\n    \"\"\"\n    population_data = []\n\n    for city in cities_list:\n        population = math.ceil(randint(1000000, 20000000) / 1000.0) * 1000\n        population_data.append([city, population])\n\n    population_df = pd.DataFrame(population_data, columns=['City', 'Population'])\n\n    return population_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npytz, datetime, numpy, dateutil, random\n```python\nimport pytz\nfrom datetime import datetime\nimport numpy as np\nfrom dateutil.parser import parse\nfrom random import choice\n\n# Constants\nTIMEZONES = ['America/New_York', 'Europe/London', 'Asia/Shanghai', 'Asia/Tokyo', 'Australia/Sydney']\n\ndef random_timezone_converter(date_str, from_tz):\n    \"\"\"\n    Convert a given datetime from one timezone to a random timezone.\n\n    Parameters:\n    date_str (str): The date string in \"yyyy-mm-dd hh:mm:ss\" format.\n    from_tz (str): The timezone of the given date string.\n\n    Returns:\n    str: The converted datetime in string format and the new timezone.\n\n    Requirements:\n    - datetime\n    - pytz\n    - numpy\n    - dateutil.parser\n    - random\n\n    Example:\n    >>> random_timezone_converter('2023-06-15 12:00:00', 'UTC')\n    \"\"\"\n    from_tz = pytz.timezone(from_tz)\n    to_tz = pytz.timezone(choice(TIMEZONES))\n    given_date = parse(date_str).replace(tzinfo=from_tz)\n    converted_date = given_date.astimezone(to_tz)\n\n    return converted_date.strftime('%Y-%m-%d %H:%M:%S'), to_tz.zone\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, random, statistics\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import randint\nfrom statistics import mean\n\n# Constants\nPRODUCTS = ['Apples', 'Bananas', 'Grapes', 'Oranges', 'Pineapples']\n\ndef generate_sales_report(products_list):\n    \"\"\"\n    Generate a DataFrame of sales data for a list of products. The sales is randomly\n    generated and the average sales is calculated.\n\n    Parameters:\n    products_list (list): A list of product names.\n\n    Returns:\n    DataFrame: A pandas DataFrame with sales data for the products and the average sales.\n\n    Requirements:\n    - pandas\n    - numpy\n    - random\n    - statistics\n\n    Example:\n    >>> products = ['Apples', 'Bananas', 'Grapes', 'Oranges', 'Pineapples']\n    >>> sales_data = generate_sales_report(products)\n    >>> print(sales_data)\n    \"\"\"\n    sales_data = []\n\n    for product in products_list:\n        sales = [randint(100, 500) for _ in range(12)]\n        avg_sales = mean(sales)\n        sales.append(avg_sales)\n        sales_data.append([product] + sales)\n\n    sales_df = pd.DataFrame(sales_data, columns=['Product'] + [f'Month {i+1}' for i in range(12)] + ['Average Sales'])\n\n    return sales_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, random, itertools\n```python\nimport numpy as np\nfrom random import randint\nimport itertools\n\ndef generate_random_matrix(dimension):\n    \"\"\"\n    Generate a 2D numpy array (matrix) of given dimension with random integers. Also, create a flat list\n    of all the elements in the matrix.\n\n    Parameters:\n    dimension (int): The dimension of the square matrix to be created.\n\n    Returns:\n    array: A 2D numpy array of given dimension with random integers.\n    list: A flat list of all elements in the matrix.\n\n    Requirements:\n    - numpy\n    - random\n    - itertools\n\n    Example:\n    >>> matrix, flat_list = generate_random_matrix(3)\n    >>> print(matrix)\n    >>> print(flat_list)\n    \"\"\"\n    matrix = np.array([[randint(1, 100) for _ in range(dimension)] for _ in range(dimension)])\n\n    flat_list = list(itertools.chain(*matrix))\n\n    return matrix, flat_list\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nmath, random, matplotlib\n```python\nimport math\nfrom random import randint\nimport matplotlib.pyplot as plt\n\ndef plot_random_sin_wave():\n    \"\"\"\n    Generate and plot a sine wave with random frequency, amplitude, and phase shift.\n\n    Parameters:\n    None\n\n    Returns:\n    None\n\n    Requirements:\n    - math\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_random_sin_wave()\n    \"\"\"\n    x = [i/100 for i in range(1000)]\n    frequency = randint(1, 5)\n    amplitude = randint(1, 5)\n    phase_shift = randint(0, 360)\n\n    y = [amplitude * math.sin(2 * math.pi * frequency * (xi + phase_shift)) for xi in x]\n\n    plt.plot(x, y)\n    plt.title('Random Sine Wave')\n    plt.xlabel('Time')\n    plt.ylabel('Amplitude')\n    plt.grid(True)\n    plt.show()\n```\nAbove is the illustration."}
{"seed": "{\"task_id\": 7138686, \"prompt\": \"def f_7138686(lines, myfile):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"myfile.write('\\\\n'.join(lines))\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    with open('tmp.txt', 'w') as myfile:\\n        candidate([\\\"first\\\", \\\"second\\\", \\\"third\\\"], myfile)\\n    with open('tmp.txt', 'r') as fr: \\n        lines = fr.readlines()\\n    assert lines == [\\\"first\\\\n\\\", \\\"second\\\\n\\\", \\\"third\\\"]\\n\"], \"entry_point\": \"f_7138686\", \"intent\": \"write the elements of list `lines` concatenated by special character '\\\\n' to file `myfile`\", \"library\": []}", "generation": "Scenario 1:\npandas, numpy, os\n```python\nimport pandas as pd\nimport numpy as np\nimport os\n\n# Constants\nDATA_DIR = './data'\n\ndef write_dataset_to_file(dataset, filename):\n    \"\"\"\n    Writes the given dataset (a list of pandas DataFrame) to the file with the given \n    filename in CSV format. Each DataFrame is separated by a line of dashes ('------').\n    \n    Parameters:\n    dataset (list): A list of pandas DataFrame.\n    filename (str): The filename of the file to write to.\n    \n    Returns:\n    None: Writes to file.\n    \n    Requirements:\n    - pandas\n    - os\n    \n    Example:\n    >>> df1 = pd.DataFrame(np.random.rand(5, 3), columns=list('ABC'))\n    >>> df2 = pd.DataFrame(np.random.rand(5, 3), columns=list('DEF'))\n    >>> write_dataset_to_file([df1, df2], 'sample.csv')\n    \"\"\"\n    with open(os.path.join(DATA_DIR, filename), 'w') as f:\n        for i, df in enumerate(dataset):\n            if i > 0:\n                f.write('\\n------\\n')\n            df.to_csv(f, index=False)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, pandas, datetime\n```python\nimport os\nimport pandas as pd\nfrom datetime import datetime\n\n# Constants\nLOG_DIR = './logs'\n\ndef log_metrics(metrics, filename):\n    \"\"\"\n    Writes the given metrics (a dictionary) to the log file with the given filename. \n    Each entry is written with a timestamp.\n    \n    Parameters:\n    metrics (dict): A dictionary of metrics.\n    filename (str): The filename of the log file.\n    \n    Returns:\n    None: Writes to file.\n    \n    Requirements:\n    - os\n    - pandas\n    - datetime\n    \n    Example:\n    >>> metrics = {'accuracy': 0.98, 'loss': 0.05}\n    >>> log_metrics(metrics, 'metrics.log')\n    \"\"\"\n    with open(os.path.join(LOG_DIR, filename), 'a') as f:\n        f.write(f'{datetime.now()}\\n')\n        for key, value in metrics.items():\n            f.write(f'{key}: {value}\\n')\n        f.write('\\n')\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, pandas, json\n```python\nimport os\nimport pandas as pd\nimport json\n\n# Constants\nDATA_DIR = './data'\n\ndef save_dataframe_to_json(df, filename):\n    \"\"\"\n    Save a pandas DataFrame to a JSON file.\n    \n    Parameters:\n    df (DataFrame): A pandas DataFrame.\n    filename (str): The filename of the JSON file.\n    \n    Returns:\n    None: Writes to file.\n    \n    Requirements:\n    - os\n    - pandas\n    - json\n\n    Example:\n    >>> df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n    >>> save_dataframe_to_json(df, 'data.json')\n    \"\"\"\n    with open(os.path.join(DATA_DIR, filename), 'w') as f:\n        json.dump(df.to_dict(orient='records'), f)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, pandas, csv\n```python\nimport os\nimport pandas as pd\nimport csv\n\n# Constants\nDATA_DIR = './data'\n\ndef save_dataframe_to_csv(df, filename):\n    \"\"\"\n    Save a pandas DataFrame to a CSV file.\n    \n    Parameters:\n    df (DataFrame): A pandas DataFrame.\n    filename (str): The filename of the CSV file.\n    \n    Returns:\n    None: Writes to file.\n    \n    Requirements:\n    - os\n    - pandas\n    - csv\n\n    Example:\n    >>> df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n    >>> save_dataframe_to_csv(df, 'data.csv')\n    \"\"\"\n    df.to_csv(os.path.join(DATA_DIR, filename), index=False, quoting=csv.QUOTE_NONNUMERIC)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, pandas, json\n```python\nimport os\nimport pandas as pd\nimport json\n\n# Constants\nDATA_DIR = './data'\n\ndef write_dataframe_to_jsonlines(df, filename):\n    \"\"\"\n    Write a pandas DataFrame to a JSON Lines file.\n    \n    Parameters:\n    df (DataFrame): A pandas DataFrame.\n    filename (str): The filename of the JSON Lines file.\n    \n    Returns:\n    None: Writes to file.\n    \n    Requirements:\n    - os\n    - pandas\n    - json\n\n    Example:\n    >>> df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n    >>> write_dataframe_to_jsonlines(df, 'data.jsonl')\n    \"\"\"\n    with open(os.path.join(DATA_DIR, filename), 'w') as f:\n        for record in df.to_dict(orient='records'):\n            json.dump(record, f)\n            f.write('\\n')\n```"}
{"seed": "{\"task_id\": 17238587, \"prompt\": \"def f_17238587(text):\\n\\t\", \"suffix\": \"\\n\\treturn text\", \"canonical_solution\": \"text = re.sub('\\\\\\\\b(\\\\\\\\w+)( \\\\\\\\1\\\\\\\\b)+', '\\\\\\\\1', text)\", \"test_start\": \"\\nimport re \\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(\\\"text\\\") == \\\"text\\\"\\n\", \"\\n    assert candidate(\\\"text text\\\") == \\\"text\\\"\\n\", \"\\n    assert candidate(\\\"texttext\\\") == \\\"texttext\\\"\\n\", \"\\n    assert candidate(\\\"text and text\\\") == \\\"text and text\\\"\\n\"], \"entry_point\": \"f_17238587\", \"intent\": \"Remove duplicate words from a string `text` using regex\", \"library\": [\"re\"]}", "generation": "Scenario 1:\nre, nltk, pandas, matplotlib\n```python\nimport re\nfrom nltk.corpus import stopwords\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nSTOPWORDS = set(stopwords.words('english'))\n\ndef analyze_text(text):\n    \"\"\"\n    Remove duplicate and stopwords from a string `text`.\n    Then, generate a frequency distribution of the remaining words and plot it.\n\n    Parameters:\n    text (str): The text string to analyze.\n\n    Returns:\n    dict: The frequency distribution of the words in the text.\n\n    Requirements:\n    - re\n    - nltk.corpus.stopwords\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> text = \"The quick brown fox jumps over the lazy dog and the dog was not that quick to respond.\"\n    >>> freq_dist = analyze_text(text)\n    >>> print(freq_dist)\n    >>> pd.Series(freq_dist).sort_values(ascending=False).plot(kind='bar')\n    \"\"\"\n    text = re.sub(r'\\b(\\w+)( \\1\\b)+', r'\\1', text)\n    words = [word for word in re.findall(r'\\b\\w+\\b', text.lower()) if word not in STOPWORDS]\n\n    freq_dist = {}\n    for word in words:\n        freq_dist[word] = freq_dist.get(word, 0) + 1\n    \n    return freq_dist\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nre, string, nltk, collections\n```python\nimport re\nimport string\nfrom nltk.corpus import stopwords\nfrom collections import Counter\n\n# Constants\nSTOPWORDS = set(stopwords.words('english'))\n\ndef count_ngrams(text, n=2):\n    \"\"\"\n    Remove duplicate and stopwords from a string `text`.\n    Then, generate a count of n-grams (default is bigrams) in the text.\n\n    Parameters:\n    text (str): The text string to analyze.\n    n (int): The size of the n-grams.\n\n    Returns:\n    dict: The count of the n-grams in the text.\n\n    Requirements:\n    - re\n    - string\n    - nltk.corpus.stopwords\n    - collections.Counter\n\n    Example:\n    >>> text = \"The quick brown fox jumps over the lazy dog and the dog was not that quick to respond.\"\n    >>> ngrams = count_ngrams(text)\n    >>> print(ngrams)\n    \"\"\"\n    text = re.sub(r'\\b(\\w+)( \\1\\b)+', r'\\1', text)\n    words = [word for word in re.findall(r'\\b\\w+\\b', text.lower()) if word not in STOPWORDS]\n    ngrams = zip(*[words[i:] for i in range(n)])\n\n    return Counter(ngrams)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nre, string, pandas, matplotlib\n```python\nimport re\nimport string\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef plot_letter_frequency(text):\n    \"\"\"\n    Remove duplicate words from a string `text`.\n    Then, generate a frequency distribution of the letters in the text and plot it.\n\n    Parameters:\n    text (str): The text string to analyze.\n\n    Returns:\n    dict: The frequency distribution of the letters in the text.\n\n    Requirements:\n    - re\n    - string\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> text = \"The quick brown fox jumps over the lazy dog.\"\n    >>> freq_dist = plot_letter_frequency(text)\n    >>> print(freq_dist)\n    >>> pd.Series(freq_dist).sort_values(ascending=False).plot(kind='bar')\n    \"\"\"\n    text = re.sub(r'\\b(\\w+)( \\1\\b)+', r'\\1', text)\n    letters = [letter for letter in text.lower() if letter in string.ascii_lowercase]\n\n    freq_dist = {}\n    for letter in letters:\n        freq_dist[letter] = freq_dist.get(letter, 0) + 1\n    \n    return freq_dist\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nre, nltk, sklearn, matplotlib\n```python\nimport re\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\n\n# Constants\nSTOPWORDS = set(stopwords.words('english'))\n\ndef plot_word_co_occurrence(text, n=2):\n    \"\"\"\n    Remove duplicate and stopwords from a string `text`.\n    Then, generate a co-occurrence matrix of the words in the text and plot it.\n\n    Parameters:\n    text (str): The text string to analyze.\n    n (int): The size of the n-grams.\n\n    Returns:\n    DataFrame: The co-occurrence matrix of the words in the text.\n\n    Requirements:\n    - re\n    - nltk.corpus.stopwords\n    - sklearn.feature_extraction.text.CountVectorizer\n    - matplotlib.pyplot\n\n    Example:\n    >>> text = \"The quick brown fox jumps over the lazy dog and the dog was not that quick to respond.\"\n    >>> matrix = plot_word_co_occurrence(text)\n    >>> print(matrix)\n    >>> plt.imshow(matrix, cmap='hot', interpolation='nearest')\n    \"\"\"\n    text = re.sub(r'\\b(\\w+)( \\1\\b)+', r'\\1', text)\n    words = [word for word in re.findall(r'\\b\\w+\\b', text.lower()) if word not in STOPWORDS]\n    vectorizer = CountVectorizer(ngram_range=(n, n))\n    X = vectorizer.fit_transform(words)\n    matrix = X.T * X\n    matrix.setdiag(0)\n    \n    return pd.DataFrame(matrix.todense(), index=vectorizer.get_feature_names(), columns=vectorizer.get_feature_names())\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nre, nltk, textblob\n```python\nimport re\nfrom nltk.corpus import stopwords\nfrom textblob import TextBlob\n\n# Constants\nSTOPWORDS = set(stopwords.words('english'))\n\ndef analyze_sentiment(text):\n    \"\"\"\n    Remove duplicate and stopwords from a string `text`.\n    Then, analyze the sentiment of the text using TextBlob.\n\n    Parameters:\n    text (str): The text string to analyze.\n\n    Returns:\n    Sentiment: The sentiment of the text.\n\n    Requirements:\n    - re\n    - nltk.corpus.stopwords\n    - textblob.TextBlob\n\n    Example:\n    >>> text = \"The quick brown fox jumps over the lazy dog and the dog was not that quick to respond.\"\n    >>> sentiment = analyze_sentiment(text)\n    >>> print(sentiment)\n    \"\"\"\n    text = re.sub(r'\\b(\\w+)( \\1\\b)+', r'\\1', text)\n    words = [word for word in re.findall(r'\\b\\w+\\b', text.lower()) if word not in STOPWORDS]\n    text = ' '.join(words)\n    blob = TextBlob(text)\n    \n    return blob.sentiment\n```"}
{"seed": "{\"task_id\": 26053849, \"prompt\": \"def f_26053849(df):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"df.astype(bool).sum(axis=1)\", \"test_start\": \"\\nimport pandas as pd \\n\\ndef check(candidate):\", \"test\": [\"\\n    df1 = pd.DataFrame([[0,0,0], [0,1,0], [1,1,1]])\\n    assert candidate(df1).to_list() == [0, 1, 3]\\n\", \"\\n    df2 = pd.DataFrame([[0,0,0], [0,2,0], [1,10,8.9]])\\n    assert candidate(df1).to_list() == [0, 1, 3]\\n\", \"\\n    df2 = pd.DataFrame([[0,0.0,0], [0,2.0,0], [1,10,8.9]])\\n    assert candidate(df1).to_list() == [0, 1, 3]\\n\", \"\\n    df = df = pd.DataFrame([[4, 0, 0], [1, 0, 1]])\\n    expected = [1, 2]\\n    actual = candidate(df)\\n    for i in range(0, len(expected)):\\n        assert expected[i] == actual[i]\\n\"], \"entry_point\": \"f_26053849\", \"intent\": \"count non zero values in each column in pandas data frame `df`\", \"library\": [\"pandas\"]}", "generation": "Scenario 1:\nnumpy, pandas, random, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\n\ndef generate_and_visualize_dataframe(rows):\n    \"\"\"\n    Generate a Pandas DataFrame with random integer values between 0 and 9.\n    Count the non-zero values in each column, and visualize this information using a bar plot.\n\n    Parameters:\n    rows (int): The number of rows in the DataFrame.\n\n    Returns:\n    DataFrame: The generated DataFrame.\n\n    Requirements:\n    - numpy\n    - pandas\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = generate_and_visualize_dataframe(10)\n    >>> print(df)\n    >>> df.astype(bool).sum(axis=0).plot(kind='bar')\n    \"\"\"\n    data = np.random.randint(10, size=(rows, len(COLUMNS)))\n    df = pd.DataFrame(data, columns=COLUMNS)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, matplotlib, random\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import sample\n\n# Constants\nSTUDENTS = ['Student' + str(i) for i in range(1, 101)]\nCOURSES = ['Course' + str(i) for i in range(1, 6)]\n\ndef generate_and_analyze_grades(num_students):\n    \"\"\"\n    Generate a Pandas DataFrame representing the grades of a random sample of students in multiple courses.\n    Calculate the average grade in each course, the number of students with a passing grade (>= 60), \n    and visualize this information using a bar plot.\n\n    Parameters:\n    num_students (int): The number of students in the sample.\n\n    Returns:\n    DataFrame: The generated DataFrame.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> df = generate_and_analyze_grades(50)\n    >>> print(df)\n    >>> df.mean().plot(kind='bar', title='Average Grade')\n    >>> plt.show()\n    >>> df[df >= 60].count().plot(kind='bar', title='Number of Students with a Passing Grade')\n    >>> plt.show()\n    \"\"\"\n    students_sample = sample(STUDENTS, num_students)\n    grades = np.random.randint(40, 101, size=(num_students, len(COURSES)))\n    df = pd.DataFrame(grades, index=students_sample, columns=COURSES)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, matplotlib, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nTEAMS = ['Team' + str(i) for i in range(1, 6)]\nGAMES = ['Game' + str(i) for i in range(1, 101)]\n\ndef generate_and_visualize_scores():\n    \"\"\"\n    Generate a Pandas DataFrame representing the scores of different teams in multiple games.\n    Calculate the total score of each team, and visualize the distribution of scores using a box plot and a heatmap.\n\n    Returns:\n    DataFrame: The generated DataFrame.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> df = generate_and_visualize_scores()\n    >>> print(df)\n    >>> df.sum(axis=1).plot(kind='bar', title='Total Score')\n    >>> plt.show()\n    >>> sns.boxplot(data=df)\n    >>> plt.show()\n    >>> sns.heatmap(df)\n    >>> plt.show()\n    \"\"\"\n    scores = np.random.randint(0, 101, size=(len(TEAMS), len(GAMES)))\n    df = pd.DataFrame(scores, index=TEAMS, columns=GAMES)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, matplotlib, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nFEATURES = ['Feature' + str(i) for i in range(1, 6)]\nSAMPLES = ['Sample' + str(i) for i in range(1, 101)]\n\ndef generate_and_analyze_dataset():\n    \"\"\"\n    Generate a Pandas DataFrame representing a dataset with multiple features.\n    Calculate the correlation between the features, and visualize this information using a correlation matrix and a heatmap.\n\n    Returns:\n    DataFrame: The generated DataFrame.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> df = generate_and_analyze_dataset()\n    >>> print(df)\n    >>> corr_matrix = df.corr()\n    >>> print(corr_matrix)\n    >>> sns.heatmap(corr_matrix, annot=True)\n    >>> plt.show()\n    \"\"\"\n    data = np.random.rand(len(SAMPLES), len(FEATURES))\n    df = pd.DataFrame(data, index=SAMPLES, columns=FEATURES)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, matplotlib, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nPRODUCTS = ['Product' + str(i) for i in range(1, 6)]\nMONTHS = ['Month' + str(i) for i in range(1, 13)]\n\ndef generate_and_visualize_sales():\n    \"\"\"\n    Generate a Pandas DataFrame representing the monthly sales of different products.\n    Calculate the total sales of each product, and visualize this information using a line plot and a heatmap.\n\n    Returns:\n    DataFrame: The generated DataFrame.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> df = generate_and_visualize_sales()\n    >>> print(df)\n    >>> df.sum(axis=0).plot(kind='line', title='Total Sales')\n    >>> plt.show()\n    >>> sns.heatmap(df)\n    >>> plt.show()\n    \"\"\"\n    sales = np.random.randint(100, 1001, size=(len(MONTHS), len(PRODUCTS)))\n    df = pd.DataFrame(sales, index=MONTHS, columns=PRODUCTS)\n\n    return df\n```\n"}
{"seed": "{\"task_id\": 15534223, \"prompt\": \"def f_15534223():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"re.search('(?<!Distillr)\\\\\\\\\\\\\\\\AcroTray\\\\\\\\.exe', 'C:\\\\\\\\SomeDir\\\\\\\\AcroTray.exe')\", \"test_start\": \"\\nimport re \\n\\ndef check(candidate):\", \"test\": [\"\\n    result = candidate()\\n    assert result.span() == (10, 23)\\n    assert result.string == \\\"C:\\\\SomeDir\\\\AcroTray.exe\\\"\\n\"], \"entry_point\": \"f_15534223\", \"intent\": \"search for string that matches regular expression pattern '(?<!Distillr)\\\\\\\\\\\\\\\\AcroTray\\\\\\\\.exe' in string 'C:\\\\\\\\SomeDir\\\\\\\\AcroTray.exe'\", \"library\": [\"re\"]}", "generation": "Scenario 1:\nre, os, glob, subprocess\n```python\nimport re\nimport os\nimport glob\nimport subprocess\n\n# Constants\nEXE_PATTERN = r\"(?<!Distillr)\\\\\\\\AcroTray\\.exe\"\nDIR_PATTERN = r\"C:\\\\\\\\SomeDir\\\\\\\\*\"\n\ndef find_and_execute_file():\n    \"\"\"\n    Search for files that match regular expression pattern '(?<!Distillr)\\\\\\\\AcroTray\\\\.exe' \n    in directory 'C:\\\\SomeDir\\\\'. If found, execute the file.\n\n    Returns:\n    str: The standard output from the executed file.\n\n    Requirements:\n    - re\n    - os\n    - glob\n    - subprocess\n\n    Example:\n    >>> find_and_execute_file()\n    \"\"\"\n    for dirpath, dirnames, filenames in os.walk(os.path.normpath(DIR_PATTERN)):\n        for filename in filenames:\n            if re.search(EXE_PATTERN, filename):\n                exe_file = os.path.join(dirpath, filename)\n                result = subprocess.run([exe_file], stdout=subprocess.PIPE)\n                return result.stdout.decode('utf-8')\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nre, pathlib, shutil, tarfile\n```python\nimport re\nfrom pathlib import Path\nimport shutil\nimport tarfile\n\n# Constants\nPATTERN = r\"(?<!Distillr)\\\\\\\\AcroTray\\.exe\"\nDIRECTORY = r\"C:\\\\SomeDir\\\\\"\n\ndef archive_matching_files():\n    \"\"\"\n    Search for files that match regular expression pattern '(?<!Distillr)\\\\\\\\AcroTray\\\\.exe' \n    in directory 'C:\\\\SomeDir\\\\'. If found, archive these files into a tar file.\n\n    Returns:\n    str: Path to the created tar file.\n\n    Requirements:\n    - re\n    - pathlib\n    - shutil\n    - tarfile\n\n    Example:\n    >>> archive_matching_files()\n    \"\"\"\n    tar_path = Path(DIRECTORY) / 'archive.tar'\n    with tarfile.open(tar_path, 'w') as tar:\n        for path in Path(DIRECTORY).rglob('*'):\n            if re.search(PATTERN, str(path)):\n                tar.add(path)\n    return str(tar_path)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nre, pandas, csv, os\n```python\nimport re\nimport pandas as pd\nimport csv\nimport os\n\n# Constants\nPATTERN = r\"(?<!Distillr)\\\\\\\\AcroTray\\.exe\"\nDIRECTORY = r\"C:\\\\SomeDir\\\\\"\n\ndef directory_search_to_csv():\n    \"\"\"\n    Search for files that match regular expression pattern '(?<!Distillr)\\\\\\\\AcroTray\\\\.exe' \n    in directory 'C:\\\\SomeDir\\\\'. If found, write these file paths to a csv file.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the file paths.\n\n    Requirements:\n    - re\n    - pandas\n    - csv\n    - os\n\n    Example:\n    >>> directory_search_to_csv()\n    \"\"\"\n    file_paths = []\n    for root, dirs, files in os.walk(DIRECTORY):\n        for file in files:\n            if re.search(PATTERN, file):\n                file_paths.append(os.path.join(root, file))\n    df = pd.DataFrame(file_paths, columns=['File Path'])\n    df.to_csv('file_paths.csv', index=False)\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nre, os, hashlib, binascii\n```python\nimport re\nimport os\nimport hashlib\nimport binascii\n\n# Constants\nPATTERN = r\"(?<!Distillr)\\\\\\\\AcroTray\\.exe\"\nDIRECTORY = r\"C:\\\\SomeDir\\\\\"\n\ndef hash_matching_files():\n    \"\"\"\n    Search for files that match regular expression pattern '(?<!Distillr)\\\\\\\\AcroTray\\\\.exe' \n    in directory 'C:\\\\SomeDir\\\\'. If found, compute a SHA256 hash of these files.\n\n    Returns:\n    dict: A dictionary with file paths as keys and their corresponding SHA256 hash as values.\n\n    Requirements:\n    - re\n    - os\n    - hashlib\n    - binascii\n\n    Example:\n    >>> hash_matching_files()\n    \"\"\"\n    hashes = {}\n    for root, dirs, files in os.walk(DIRECTORY):\n        for file in files:\n            if re.search(PATTERN, file):\n                path = os.path.join(root, file)\n                with open(path, 'rb') as f:\n                    data = f.read()\n                    hash = hashlib.sha256(data).digest()\n                    hashes[path] = binascii.hexlify(hash).decode()\n    return hashes\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nre, os, configparser\n```python\nimport re\nimport os\nimport configparser\n\n# Constants\nPATTERN = r\"(?<!Distillr)\\\\\\\\AcroTray\\.exe\"\nDIRECTORY = r\"C:\\\\SomeDir\\\\\"\n\ndef write_matching_files_to_config():\n    \"\"\"\n    Search for files that match regular expression pattern '(?<!Distillr)\\\\\\\\AcroTray\\\\.exe' \n    in directory 'C:\\\\SomeDir\\\\'. If found, write these file paths to a configuration file.\n\n    Returns:\n    str: Path to the created configuration file.\n\n    Requirements:\n    - re\n    - os\n    - configparser\n\n    Example:\n    >>> write_matching_files_to_config()\n    \"\"\"\n    config = configparser.ConfigParser()\n    for root, dirs, files in os.walk(DIRECTORY):\n        for file in files:\n            if re.search(PATTERN, file):\n                path = os.path.join(root, file)\n                config[path] = {'file': path}\n    with open('config.ini', 'w') as configfile:\n        config.write(configfile)\n    return os.path.abspath('config.ini')\n```"}
{"seed": "{\"task_id\": 5453026, \"prompt\": \"def f_5453026():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"\\\"\\\"\\\"QH QD JC KD JS\\\"\\\"\\\".split()\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate() == [\\\"QH\\\", \\\"QD\\\", \\\"JC\\\", \\\"KD\\\", \\\"JS\\\"]\\n\"], \"entry_point\": \"f_5453026\", \"intent\": \"split string 'QH QD JC KD JS' into a list on white spaces\", \"library\": []}", "generation": "Scenario 1: \ncollections, random, string\n```python\nfrom collections import Counter\nimport random\nimport string\n\n# Constants\nHAND_RANKS = ['2', '3', '4', '5', '6', '7', '8', '9', '10', 'J', 'Q', 'K', 'A']\nSUITS = ['H', 'D', 'C', 'S']\n\ndef generate_poker_hand():\n    \"\"\"\n    Generate a random poker hand and count the frequency of each card rank.\n\n    Returns:\n    tuple: A tuple containing a list of 5 cards and a Counter dictionary of card ranks.\n\n    Requirements:\n    - collections\n    - random\n    - string\n\n    Example:\n    >>> hand, rank_counts = generate_poker_hand()\n    >>> print(hand)\n    >>> print(rank_counts)\n    \"\"\"\n    hand = []\n    for _ in range(5):\n        rank = random.choice(HAND_RANKS)\n        suit = random.choice(SUITS)\n        card = f'{rank}{suit}'\n        hand.append(card)\n\n    rank_counts = Counter([card[:-1] for card in hand])\n\n    return hand, rank_counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nre, string\n```python\nimport re\nimport string\n\n# Constants\nPUNCTUATION = string.punctuation\n\ndef split_and_clean_text(text):\n    \"\"\"\n    Split a string into words, remove punctuation and convert to lowercase.\n\n    Parameters:\n    text (str): The input string.\n\n    Returns:\n    list: A list of cleaned words.\n\n    Requirements:\n    - re\n    - string\n\n    Example:\n    >>> split_and_clean_text(\"Hello, world! This is a test.\")\n    \"\"\"\n    words = re.split(r'\\s+', text)\n    cleaned_words = [re.sub(f'[{PUNCTUATION}]', '', word).lower() for word in words]\n\n    return cleaned_words\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nrandom, string\n```python\nimport random\nimport string\n\n# Constants\nLETTERS = string.ascii_letters\n\ndef generate_random_words(num_words, word_length):\n    \"\"\"\n    Generate a list of random words with a specific length.\n\n    Parameters:\n    num_words (int): The number of words to generate.\n    word_length (int): The length of each word.\n\n    Returns:\n    list: A list of random words.\n\n    Requirements:\n    - random\n    - string\n\n    Example:\n    >>> generate_random_words(5, 3)\n    \"\"\"\n    words = [''.join(random.choice(LETTERS) for _ in range(word_length)) for _ in range(num_words)]\n\n    return words\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nrandom, string\n```python\nimport random\nimport string\n\n# Constants\nLETTERS = string.ascii_letters\nDIGITS = string.digits\n\ndef generate_random_password(length, num_digits):\n    \"\"\"\n    Generate a random password with a specific length and number of digits.\n\n    Parameters:\n    length (int): The length of the password.\n    num_digits (int): The number of digits in the password.\n\n    Returns:\n    str: The generated password.\n\n    Requirements:\n    - random\n    - string\n\n    Example:\n    >>> generate_random_password(10, 3)\n    \"\"\"\n    password = []\n    for _ in range(length - num_digits):\n        password.append(random.choice(LETTERS))\n    for _ in range(num_digits):\n        password.append(random.choice(DIGITS))\n\n    random.shuffle(password)\n\n    return ''.join(password)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nre, string\n```python\nimport re\nimport string\n\n# Constants\nPUNCTUATION = string.punctuation\n\ndef count_words_and_punctuation(text):\n    \"\"\"\n    Count the number of words and punctuation marks in a string.\n\n    Parameters:\n    text (str): The input string.\n\n    Returns:\n    tuple: A tuple containing the number of words and punctuation marks.\n\n    Requirements:\n    - re\n    - string\n\n    Example:\n    >>> count_words_and_punctuation(\"Hello, world! This is a test.\")\n    \"\"\"\n    words = re.split(r'\\s+', text)\n    punctuation_marks = [char for char in text if char in PUNCTUATION]\n\n    return len(words), len(punctuation_marks)\n```\n"}
{"seed": "{\"task_id\": 18168684, \"prompt\": \"def f_18168684(line):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"re.search('>.*<', line).group(0)\", \"test_start\": \"\\nimport re \\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(\\\"hahhdsf>0.0<;sgnd\\\") == \\\">0.0<\\\"\\n\", \"\\n    assert candidate(\\\"hahhdsf>2.34<;xbnfm\\\") == \\\">2.34<\\\"\\n\"], \"entry_point\": \"f_18168684\", \"intent\": \"search for occurrences of regex pattern '>.*<' in xml string `line`\", \"library\": [\"re\"]}", "generation": "Scenario 1:\nre, html, BeautifulSoup, urllib\n```python\nimport re\nfrom html import unescape\nfrom bs4 import BeautifulSoup\nimport urllib.request\n\n# Constants\nURL_PATTERN = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\nURL_PLACEHOLDER = '{URL}'\n\ndef extract_data_from_webpage(url):\n    \"\"\"\n    Extract data from the webpage at the provided url. It searches for occurrences of the \n    regex pattern '>.*<' in the webpage's HTML content and returns a list of all matched strings.\n\n    Parameters:\n    url (str): The url of the webpage.\n\n    Returns:\n    list: A list of all matched strings.\n\n    Requirements:\n    - re\n    - html\n    - BeautifulSoup\n    - urllib.request\n\n    Example:\n    >>> extract_data_from_webpage('https://example.com')\n    \"\"\"\n    with urllib.request.urlopen(url) as response:\n        html = response.read().decode()\n\n    soup = BeautifulSoup(html, 'html.parser')\n    html_text = soup.get_text()\n\n    matches = re.findall('>.*<', html_text)\n\n    # Unescape HTML entities in the matched strings\n    matches = [unescape(match) for match in matches]\n\n    return matches\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nre, os, shutil\n```python\nimport re\nimport os\nimport shutil\n\n# Constants\nFILE_PATTERN = r'\\b[A-Za-z0-9]+\\.(txt|doc|docx)\\b'\n\ndef move_matching_files(source_dir, target_dir):\n    \"\"\"\n    Move all files from the source directory to the target directory \n    that match the regex pattern '\\b[A-Za-z0-9]+\\.(txt|doc|docx)\\b'.\n\n    Parameters:\n    source_dir (str): The source directory.\n    target_dir (str): The target directory.\n\n    Returns:\n    int: The number of files moved.\n\n    Requirements:\n    - re\n    - os\n    - shutil\n\n    Example:\n    >>> move_matching_files('/path/to/source', '/path/to/target')\n    \"\"\"\n    moved_files_count = 0\n\n    for filename in os.listdir(source_dir):\n        if re.match(FILE_PATTERN, filename):\n            shutil.move(os.path.join(source_dir, filename), os.path.join(target_dir, filename))\n            moved_files_count += 1\n\n    return moved_files_count\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nre, pandas, numpy\n```python\nimport re\nimport pandas as pd\nimport numpy as np\n\n# Constants\nDATA_PATTERN = r'>\\d+\\.\\d+<'\n\ndef extract_numeric_data(dataframe):\n    \"\"\"\n    Extract numeric data from a pandas DataFrame. It searches each cell for occurrences of \n    the regex pattern '>\\d+\\.\\d+<' and replaces the cell with the numeric value if a match is found.\n\n    Parameters:\n    dataframe (DataFrame): The pandas DataFrame.\n\n    Returns:\n    DataFrame: The modified pandas DataFrame.\n\n    Requirements:\n    - re\n    - pandas\n    - numpy\n\n    Example:\n    >>> df = pd.DataFrame({'A': ['>1.23<', '>4.56<'], 'B': ['>7.89<', '>0.12<']})\n    >>> extract_numeric_data(df)\n    \"\"\"\n    for col in dataframe.columns:\n        dataframe[col] = dataframe[col].apply(lambda x: float(re.search(DATA_PATTERN, x).group(0)[1:-1]) \n                                              if pd.notnull(x) and re.search(DATA_PATTERN, x) else np.nan)\n\n    return dataframe\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nre, email, smtplib\n```python\nimport re\nimport email\nimport smtplib\n\n# Constants\nEMAIL_PATTERN = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n\ndef send_emails(email_text, recipient_list):\n    \"\"\"\n    Send an email to each recipient in the recipient list. The email addresses are \n    extracted from the recipient list using the regex pattern \n    '\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'.\n\n    Parameters:\n    email_text (str): The text of the email.\n    recipient_list (str): The list of recipients.\n\n    Returns:\n    int: The number of emails sent.\n\n    Requirements:\n    - re\n    - email\n    - smtplib\n\n    Example:\n    >>> send_emails('Hello, world!', 'john@example.com, jane@example.com')\n    \"\"\"\n    smtp = smtplib.SMTP('localhost')\n\n    email_addresses = re.findall(EMAIL_PATTERN, recipient_list)\n\n    for email_address in email_addresses:\n        msg = email.message_from_string(email_text)\n        msg['From'] = 'myemail@example.com'\n        msg['To'] = email_address\n        msg['Subject'] = 'Test Email'\n\n        smtp.send_message(msg)\n\n    smtp.quit()\n\n    return len(email_addresses)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nre, json, requests\n```python\nimport re\nimport json\nimport requests\n\n# Constants\nAPI_PATTERN = r'https://api\\.example\\.com/\\w+'\n\ndef query_api(api_list):\n    \"\"\"\n    Query each API in the API list and return the responses. \n    The API urls are extracted from the API list using the regex pattern \n    'https://api\\.example\\.com/\\w+'.\n\n    Parameters:\n    api_list (str): The list of APIs.\n\n    Returns:\n    dict: A dictionary with the API url as the key and the response as the value.\n\n    Requirements:\n    - re\n    - json\n    - requests\n\n    Example:\n    >>> query_api('https://api.example.com/users, https://api.example.com/posts')\n    \"\"\"\n    api_urls = re.findall(API_PATTERN, api_list)\n\n    responses = {}\n\n    for api_url in api_urls:\n        response = requests.get(api_url)\n        responses[api_url] = json.loads(response.text)\n\n    return responses\n```"}
{"seed": "{\"task_id\": 4914277, \"prompt\": \"def f_4914277(filename):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"open(filename, 'w').close()\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    filename = 'tmp.txt'\\n    with open(filename, 'w') as fw: fw.write(\\\"hello world!\\\")\\n    with open(filename, 'r') as fr: \\n        lines = fr.readlines()\\n        assert len(lines) == 1 and lines[0] == \\\"hello world!\\\"\\n    candidate(filename)\\n    with open(filename, 'r') as fr: \\n        lines = fr.readlines()\\n        assert len(lines) == 0\\n\"], \"entry_point\": \"f_4914277\", \"intent\": \"erase all the contents of a file `filename`\", \"library\": []}", "generation": "Scenario 1: \nos, errno, hashlib, base64\n```python\nimport os\nimport errno\nimport hashlib\nimport base64\n\ndef encrypt_and_write(filename, data, password):\n    \"\"\"\n    Encrypt a string using a password, then write the encrypted string to a file. If the file does not exist, create it.\n\n    Parameters:\n    filename (str): The name of the file to write to.\n    data (str): The string to encrypt and write to the file.\n    password (str): The password to use for encryption.\n\n    Returns:\n    str: The encrypted string.\n\n    Requirements:\n    - os\n    - errno\n    - hashlib\n    - base64\n\n    Example:\n    >>> encrypt_and_write('test.txt', 'Hello, World!', 'password')\n    \"\"\"\n    # Ensure the file exists\n    try:\n        open(filename, 'x').close()\n    except FileExistsError:\n        pass\n\n    # Encrypt the data\n    key = hashlib.sha256(password.encode()).digest()\n    encrypted = base64.b64encode(data.encode()).decode()\n\n    # Write to the file\n    with open(filename, 'w') as f:\n        f.write(encrypted)\n\n    return encrypted\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, errno, shutil\n```python\nimport os\nimport errno\nimport shutil\n\ndef copy_and_erase(filename, dest_dir):\n    \"\"\"\n    Copy a file to a destination directory, then erase all the contents of the original file.\n\n    Parameters:\n    filename (str): The name of the file to copy and erase.\n    dest_dir (str): The directory to copy the file to.\n\n    Returns:\n    str: The path of the copied file.\n\n    Requirements:\n    - os\n    - errno\n    - shutil\n\n    Example:\n    >>> copy_and_erase('test.txt', '/path/to/destination')\n    \"\"\"\n    # Ensure the destination directory exists\n    try:\n        os.makedirs(dest_dir)\n    except OSError as e:\n        if e.errno != errno.EEXIST:\n            raise\n\n    # Copy the file\n    dest = shutil.copy(filename, dest_dir)\n\n    # Erase the original file\n    open(filename, 'w').close()\n\n    return dest\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nos, errno, pandas\n```python\nimport os\nimport errno\nimport pandas as pd\n\ndef read_csv_and_clear(filename):\n    \"\"\"\n    Read a CSV file into a pandas DataFrame, then erase all the contents of the original file.\n\n    Parameters:\n    filename (str): The name of the CSV file to read and erase.\n\n    Returns:\n    DataFrame: The contents of the CSV file as a pandas DataFrame.\n\n    Requirements:\n    - os\n    - errno\n    - pandas\n\n    Example:\n    >>> read_csv_and_clear('data.csv')\n    \"\"\"\n    # Read the CSV file\n    df = pd.read_csv(filename)\n\n    # Erase the original file\n    open(filename, 'w').close()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, errno, json\n```python\nimport os\nimport errno\nimport json\n\ndef write_json_and_clear(filename, data):\n    \"\"\"\n    Write a dictionary to a file as JSON, then erase all the contents of the original file.\n\n    Parameters:\n    filename (str): The name of the file to write to and erase.\n    data (dict): The dictionary to write to the file.\n\n    Returns:\n    None\n\n    Requirements:\n    - os\n    - errno\n    - json\n\n    Example:\n    >>> write_json_and_clear('data.json', {'key': 'value'})\n    \"\"\"\n    # Write the dictionary to the file as JSON\n    with open(filename, 'w') as f:\n        json.dump(data, f)\n\n    # Erase the original file\n    open(filename, 'w').close()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nos, errno, pickle\n```python\nimport os\nimport errno\nimport pickle\n\ndef pickle_and_clear(filename, data):\n    \"\"\"\n    Serialize an object using pickle, write it to a file, then erase all the contents of the original file.\n\n    Parameters:\n    filename (str): The name of the file to write to and erase.\n    data (object): The object to serialize and write to the file.\n\n    Returns:\n    None\n\n    Requirements:\n    - os\n    - errno\n    - pickle\n\n    Example:\n    >>> pickle_and_clear('data.pkl', {'key': 'value'})\n    \"\"\"\n    # Serialize the object and write it to the file\n    with open(filename, 'wb') as f:\n        pickle.dump(data, f)\n\n    # Erase the original file\n    open(filename, 'w').close()\n```"}
{"seed": "{\"task_id\": 19068269, \"prompt\": \"def f_19068269(string_date):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"datetime.datetime.strptime(string_date, '%Y-%m-%d %H:%M:%S.%f')\", \"test_start\": \"\\nimport datetime \\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('2022-10-22 11:59:59.20') == datetime.datetime(2022, 10, 22, 11, 59, 59, 200000)\\n\", \"\\n    assert candidate('2000-01-01 11:59:59.20') == datetime.datetime(2000, 1, 1, 11, 59, 59, 200000)\\n\", \"\\n    assert candidate('1990-09-09 09:59:59.24') == datetime.datetime(1990, 9, 9, 9, 59, 59, 240000)\\n\", \"\\n    d = candidate('2022-12-14 07:06:00.25')\\n    assert d == datetime.datetime(2022, 12, 14, 7, 6, 0, 250000)\\n\"], \"entry_point\": \"f_19068269\", \"intent\": \"convert a string `string_date` into datetime using the format '%Y-%m-%d %H:%M:%S.%f'\", \"library\": [\"datetime\"]}", "generation": "Scenario 1:\npandas, datetime, dateutil, os, matplotlib\n```python\nimport os\nimport pandas as pd\nfrom datetime import datetime\nfrom dateutil.parser import parse\nimport matplotlib.pyplot as plt\n\n# Constants\nCSV_PATH = 'data.csv' # CSV file containing date strings\nDATE_COLUMN = 'date' # Column in the CSV file with the date strings\n\ndef read_and_plot(csv_path=CSV_PATH, date_column=DATE_COLUMN):\n    \"\"\"\n    Read a CSV file, convert a column of date strings into datetime objects, \n    and plot a histogram of the year distribution. \n\n    Parameters:\n    csv_path (str): The path to the CSV file.\n    date_column (str): The column in the CSV file with the date strings.\n\n    Requirements:\n    - pandas\n    - datetime\n    - dateutil.parser\n    - os\n    - matplotlib.pyplot\n\n    Example:\n    >>> read_and_plot()\n    \"\"\"\n    if not os.path.isfile(csv_path):\n        raise FileNotFoundError(f\"{csv_path} does not exist\")\n\n    df = pd.read_csv(csv_path)\n    df[date_column] = df[date_column].apply(lambda x: parse(x))\n\n    df[date_column].dt.year.hist()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npytz, datetime, dateutil, time\n```python\nimport time\nimport pytz\nfrom datetime import datetime\nfrom dateutil.parser import parse\n\ndef get_time_difference(date_str, from_tz, to_tz):\n    \"\"\"\n    Convert a date string from one timezone to another and return the time difference \n    in seconds from the current time in the target timezone.\n\n    Parameters:\n    date_str (str): The date string in \"yyyy-mm-dd hh:mm:ss\" format.\n    from_tz (str): The timezone of the given date string.\n    to_tz (str): The timezone to which the date string should be converted.\n\n    Returns:\n    int: The time difference in seconds.\n\n    Requirements:\n    - datetime\n    - pytz\n    - dateutil.parser\n    - time\n\n    Example:\n    >>> get_time_difference('2022-10-22 11:59:59', 'UTC', 'America/Chicago')\n    \"\"\"\n    from_tz = pytz.timezone(from_tz)\n    to_tz = pytz.timezone(to_tz)\n    given_date = parse(date_str).replace(tzinfo=from_tz).astimezone(to_tz)\n    current_date = datetime.now().astimezone(to_tz)\n\n    time_difference = current_date - given_date\n\n    return int(time_difference.total_seconds())\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ndatetime, dateutil, calendar\n```python\nfrom datetime import datetime\nfrom dateutil.parser import parse\nimport calendar\n\ndef next_business_day(date_str):\n    \"\"\"\n    Get the next business day (Mon-Fri) after a given date string.\n\n    Parameters:\n    date_str (str): The date string in \"yyyy-mm-dd\" format.\n\n    Returns:\n    datetime: The datetime object of the next business day.\n\n    Requirements:\n    - datetime\n    - dateutil.parser\n    - calendar\n\n    Example:\n    >>> next_business_day('2022-10-22')\n    \"\"\"\n    given_date = parse(date_str)\n    next_day = given_date\n\n    while True:\n        next_day = next_day + datetime.timedelta(days=1)\n\n        if calendar.isweekday(next_day.weekday()) and next_day.weekday() < 5: # Monday to Friday are business days\n            break\n\n    return next_day\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ndatetime, dateutil, numpy, pandas\n```python\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\nfrom dateutil.parser import parse\n\n# Constants\nDAYS_OF_WEEK = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n\ndef analyze_weekday_distribution(dates_str_list):\n    \"\"\"\n    Analyze the weekday distribution in a list of date strings.\n\n    Parameters:\n    dates_str_list (list): The list of date strings in \"yyyy-mm-dd\" format.\n\n    Returns:\n    Series: A pandas Series of the weekday distribution.\n\n    Requirements:\n    - datetime\n    - dateutil.parser\n    - numpy\n    - pandas\n\n    Example:\n    >>> analyze_weekday_distribution(['2022-10-22', '2022-10-23', '2022-10-24', '2022-10-25'])\n    \"\"\"\n    weekdays = [parse(date_str).weekday() for date_str in dates_str_list]\n    weekday_counts = np.bincount(weekdays)\n    \n    distribution = pd.Series(weekday_counts, index=DAYS_OF_WEEK)\n\n    return distribution\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\ndatetime, dateutil, time, pytz\n```python\nimport time\nimport pytz\nfrom datetime import datetime\nfrom dateutil.parser import parse\n\ndef time_until_new_year(date_str, tz_str):\n    \"\"\"\n    Get the time in seconds until the next New Year in a given timezone from a given date string.\n\n    Parameters:\n    date_str (str): The date string in \"yyyy-mm-dd hh:mm:ss\" format.\n    tz_str (str): The timezone string.\n\n    Returns:\n    int: The time in seconds until the next New Year.\n\n    Requirements:\n    - datetime\n    - dateutil.parser\n    - time\n    - pytz\n\n    Example:\n    >>> time_until_new_year('2022-10-22 11:59:59', 'America/Chicago')\n    \"\"\"\n    tz = pytz.timezone(tz_str)\n    given_date = parse(date_str).replace(tzinfo=pytz.UTC).astimezone(tz)\n\n    next_year = given_date.year + 1\n    new_year = datetime(next_year, 1, 1, tzinfo=tz)\n\n    time_until_new_year = new_year - given_date\n\n    return int(time_until_new_year.total_seconds())\n```"}
{"seed": "{\"task_id\": 20683167, \"prompt\": \"def f_20683167(thelist):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"[index for index, item in enumerate(thelist) if item[0] == '332']\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([[0,1,2], ['a','bb','ccc'], ['332',33,2], [33,22,332]]) == [2]\\n\", \"\\n    assert candidate([[0,1,2], ['332'], ['332'], ['332']]) == [1,2,3]\\n\", \"\\n    assert candidate([[0,1,2], [332], [332], [332]]) == []\\n\"], \"entry_point\": \"f_20683167\", \"intent\": \"find the index of a list with the first element equal to '332' within the list of lists `thelist`\", \"library\": []}", "generation": "Scenario 1:\npandas, os, shutil, csv\n```python\nimport pandas as pd\nimport os\nimport shutil\nimport csv\n\n# Constants\nTARGET_VALUE = '332'\nCSV_DIR = './csv_files/'\nPROCESSED_DIR = './processed_files/'\n\ndef process_csv_files():\n    \"\"\"\n    Scans a directory for CSV files, for each file, it finds the index of the row with the first \n    cell equal to '332'. It then moves the processed files to another directory.\n\n    Returns:\n    dict: A dictionary with file names as keys and the row indices as values.\n\n    Requirements:\n    - pandas\n    - os\n    - shutil\n    - csv\n\n    Example:\n    >>> process_csv_files()\n    \"\"\"\n    result = {}\n\n    # Scan the CSV files in the directory\n    for filename in os.listdir(CSV_DIR):\n        if filename.endswith('.csv'):\n            with open(os.path.join(CSV_DIR, filename), 'r') as f:\n                reader = csv.reader(f)\n                for i, row in enumerate(reader):\n                    if row[0] == TARGET_VALUE:\n                        result[filename] = i\n                        break\n\n            # Move the file to the processed directory\n            shutil.move(os.path.join(CSV_DIR, filename), PROCESSED_DIR)\n    \n    return result\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, matplotlib.pyplot\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nTARGET_VALUE = '332'\nDATAFRAME = pd.DataFrame({\n    'Column1': ['0', 'a', '332', '33'],\n    'Column2': ['1', 'bb', '33', '22'],\n    'Column3': ['2', 'ccc', '2', '332']\n})\n\ndef plot_target_occurrence():\n    \"\"\"\n    Searches a DataFrame for the rows with the first cell equal to '332'.\n    It then plots the count of such rows per column.\n\n    Returns:\n    Series: A pandas Series with counts of the target value per column.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_target_occurrence()\n    \"\"\"\n    counts = DATAFRAME.apply(lambda x: (x == TARGET_VALUE).sum())\n\n    # Plot the counts\n    counts.plot(kind='bar')\n\n    return counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, scipy.stats, matplotlib.pyplot\n```python\nimport numpy as np\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\n\n# Constants\nTARGET_VALUE = '332'\nARRAY = np.array([['0', '1', '2'], ['a', 'bb', 'ccc'], ['332', '33', '2'], ['33', '22', '332']])\n\ndef analyze_target_distribution():\n    \"\"\"\n    Finds the index of the row in a numpy array with the first cell equal to '332'.\n    It then performs a statistical analysis on these indices and plots the distribution.\n\n    Returns:\n    tuple: A tuple with mean, variance, skewness, and kurtosis of the indices.\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Example:\n    >>> analyze_target_distribution()\n    \"\"\"\n    indices = np.where(ARRAY[:,0] == TARGET_VALUE)[0]\n\n    # Perform statistical analysis\n    mean = np.mean(indices)\n    variance = np.var(indices)\n    skewness = stats.skew(indices)\n    kurtosis = stats.kurtosis(indices)\n\n    # Plot the distribution\n    plt.hist(indices, bins='auto')\n    plt.show()\n\n    return mean, variance, skewness, kurtosis\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, matplotlib.pyplot, seaborn\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nTARGET_VALUE = '332'\nDATAFRAME = pd.DataFrame({\n    'Column1': ['0', 'a', '332', '33'],\n    'Column2': ['1', 'bb', '33', '22'],\n    'Column3': ['2', 'ccc', '2', '332']\n})\n\ndef visualize_target_occurrence():\n    \"\"\"\n    Searches a DataFrame for the rows with the first cell equal to '332'.\n    It then visualizes these occurrences using a heatmap.\n\n    Returns:\n    DataFrame: A DataFrame with Boolean values indicating the presence of the target value.\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> visualize_target_occurrence()\n    \"\"\"\n    mask = DATAFRAME.applymap(lambda x: x == TARGET_VALUE)\n\n    # Plot the heatmap\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(mask, cmap='Blues')\n    plt.show()\n\n    return mask\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, scipy.optimize, matplotlib.pyplot\n```python\nimport numpy as np\nimport scipy.optimize as optimize\nimport matplotlib.pyplot as plt\n\n# Constants\nTARGET_VALUE = '332'\nARRAY = np.array([['0', '1', '2'], ['a', 'bb', 'ccc'], ['332', '33', '2'], ['33', '22', '332']])\n\ndef optimize_target_occurrence():\n    \"\"\"\n    Finds the index of the row in a numpy array with the first cell equal to '332'.\n    It then finds the function that best fits these indices and plots it.\n\n    Returns:\n    ndarray: The optimized coefficients for the fitting function.\n\n    Requirements:\n    - numpy\n    - scipy.optimize\n    - matplotlib.pyplot\n\n    Example:\n    >>> optimize_target_occurrence()\n    \"\"\"\n    indices = np.where(ARRAY[:,0] == TARGET_VALUE)[0]\n\n    # Define a fitting function\n    def func(x, a, b, c):\n        return a * np.exp(-b * x) + c\n\n    # Find the optimized coefficients\n    popt, _ = optimize.curve_fit(func, np.arange(len(indices)), indices)\n\n    # Plot the fitting function\n    plt.plot(np.arange(len(indices)), func(np.arange(len(indices)), *popt), 'r-')\n    plt.show()\n\n    return popt\n```"}
{"seed": "{\"task_id\": 30693804, \"prompt\": \"def f_30693804(text):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"re.sub('[^\\\\\\\\sa-zA-Z0-9]', '', text).lower().strip()\", \"test_start\": \"\\nimport re\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('ABjfK329r0&&*#5t') == 'abjfk329r05t'\\n\", \"\\n    assert candidate('jseguwphegoi339yup h') == 'jseguwphegoi339yup h'\\n\", \"\\n    assert candidate('   ') == ''\\n\"], \"entry_point\": \"f_30693804\", \"intent\": \"lower a string `text` and remove non-alphanumeric characters aside from space\", \"library\": [\"re\"]}", "generation": "Scenario 1:\nnltk, re, string, collections\n```python\nimport re\nimport string\nfrom nltk.corpus import stopwords\nfrom collections import Counter\n\n# Constants\nSTOPWORDS = set(stopwords.words('english'))\n\ndef count_most_common_words(text):\n    \"\"\"\n    Lower a string `text`, remove non-alphanumeric characters aside from space, remove stopwords \n    and then count the most common words.\n\n    Parameters:\n    text (str): The input text.\n\n    Returns:\n    list: A list of tuples, where the first element of each tuple is a word, and the second element \n    is the count of that word.\n\n    Requirements:\n    - nltk.corpus\n    - re\n    - string\n    - collections.Counter\n\n    Example:\n    >>> count_most_common_words(\"This is a sample text. It contains several words, including 'sample' and 'text'.\")\n    \"\"\"\n    text = re.sub('[^\\\\sa-zA-Z0-9]', '', text).lower().strip()\n    words = [word for word in text.split() if word not in STOPWORDS]\n    word_counts = Counter(words)\n\n    return word_counts.most_common()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, re, string, nltk\n```python\nimport pandas as pd\nimport re\nimport string\nfrom nltk.stem import PorterStemmer\n\n# Constants\nPUNCTUATION_TABLE = str.maketrans('', '', string.punctuation)\n\ndef process_text_series(text_series):\n    \"\"\"\n    Lower a string series `text_series`, remove non-alphanumeric characters aside from space, \n    remove punctuation, and stem words.\n\n    Parameters:\n    text_series (Series): The input text series.\n\n    Returns:\n    Series: A processed text series.\n\n    Requirements:\n    - pandas\n    - re\n    - string\n    - nltk.stem\n\n    Example:\n    >>> process_text_series(pd.Series([\"This is a sample text.\", \"It contains several words, including 'sample' and 'text'.\"]))\n    \"\"\"\n    stemmer = PorterStemmer()\n\n    def process_text(text):\n        text = re.sub('[^\\\\sa-zA-Z0-9]', '', text).lower().strip()\n        text = text.translate(PUNCTUATION_TABLE)\n        text = \" \".join([stemmer.stem(word) for word in text.split()])\n\n        return text\n\n    return text_series.apply(process_text)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, re, string, nltk\n```python\nimport os\nimport re\nimport string\nfrom nltk.tokenize import word_tokenize\n\n# Constants\nPUNCTUATION_TABLE = str.maketrans('', '', string.punctuation)\n\ndef tokenize_files_in_dir(directory):\n    \"\"\"\n    Lower a string `text` from each file in a directory, remove non-alphanumeric characters aside from space, \n    remove punctuation, and tokenize words.\n\n    Parameters:\n    directory (str): The directory path.\n\n    Returns:\n    dict: A dictionary where keys are file names and values are lists of tokenized words.\n\n    Requirements:\n    - os\n    - re\n    - string\n    - nltk.tokenize\n\n    Example:\n    >>> tokenize_files_in_dir('/path/to/directory')\n    \"\"\"\n    tokens = {}\n\n    for file in os.listdir(directory):\n        with open(os.path.join(directory, file), 'r') as f:\n            text = f.read()\n            text = re.sub('[^\\\\sa-zA-Z0-9]', '', text).lower().strip()\n            text = text.translate(PUNCTUATION_TABLE)\n            tokens[file] = word_tokenize(text)\n\n    return tokens\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nre, string, json, collections\n```python\nimport re\nimport string\nimport json\nfrom collections import defaultdict\n\n# Constants\nPUNCTUATION_TABLE = str.maketrans('', '', string.punctuation)\n\ndef count_words_from_json(json_string):\n    \"\"\"\n    Lower a string `text` from JSON, remove non-alphanumeric characters aside from space, \n    remove punctuation, and count words.\n\n    Parameters:\n    json_string (str): The JSON string.\n\n    Returns:\n    dict: A dictionary where keys are words and values are their counts.\n\n    Requirements:\n    - re\n    - string\n    - json\n    - collections.defaultdict\n\n    Example:\n    >>> count_words_from_json('{\"text\": \"This is a sample text. It contains several words, including sample and text.\"}')\n    \"\"\"\n    data = json.loads(json_string)\n    text = data.get('text', '')\n    text = re.sub('[^\\\\sa-zA-Z0-9]', '', text).lower().strip()\n    text = text.translate(PUNCTUATION_TABLE)\n\n    word_counts = defaultdict(int)\n    for word in text.split():\n        word_counts[word] += 1\n\n    return dict(word_counts)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nrandom, re, string, itertools\n```python\nimport random\nimport re\nimport string\nfrom itertools import chain\n\n# Constants\nWORD_LIST = [\"sample\", \"text\", \"contains\", \"several\", \"words\", \"including\"]\n\ndef generate_random_text(n_sentences):\n    \"\"\"\n    Generate random sentences using a word list, lowercase the text, \n    remove non-alphanumeric characters aside from space.\n\n    Parameters:\n    n_sentences (int): The number of sentences to generate.\n\n    Returns:\n    str: The generated text.\n\n    Requirements:\n    - random\n    - re\n    - string\n    - itertools\n\n    Example:\n    >>> generate_random_text(5)\n    \"\"\"\n    sentences = []\n    for _ in range(n_sentences):\n        sentence_len = random.randint(5, 10)\n        sentence = \" \".join(random.choice(WORD_LIST) for _ in range(sentence_len)) + \".\"\n        sentences.append(sentence)\n\n    text = \" \".join(sentences)\n    text = re.sub('[^\\\\sa-zA-Z0-9]', '', text).lower().strip()\n\n    return text\n```"}
{"seed": "{\"task_id\": 30693804, \"prompt\": \"def f_30693804(text):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"re.sub('(?!\\\\\\\\s)[\\\\\\\\W_]', '', text).lower().strip()\", \"test_start\": \"\\nimport re\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('ABjfK329r0&&*#5t') == 'abjfk329r05t'\\n\", \"\\n    assert candidate('jseguwphegoi339yup h') == 'jseguwphegoi339yup h'\\n\", \"\\n    assert candidate('   ') == ''\\n\"], \"entry_point\": \"f_30693804\", \"intent\": \"remove all non-alphanumeric characters except space from a string `text` and lower it\", \"library\": [\"re\"]}", "generation": "Scenario 1:\npython-Levenshtein, re, collections, scipy.spatial\n```python\nimport re\nfrom collections import Counter\nfrom scipy.spatial import distance\nfrom Levenshtein import ratio\n\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\n\ndef calculate_similarity_scores(text1, text2):\n    \"\"\"\n    Calculate the similarity scores between two texts using cosine similarity and \n    Levenshtein ratio. The texts are first cleaned by removing all non-alphanumeric \n    characters except space and lowercased.\n\n    Parameters:\n    text1 (str): The first string.\n    text2 (str): The second string.\n\n    Returns:\n    tuple: A tuple containing the cosine similarity and Levenshtein ratio.\n\n    Requirements:\n    - re\n    - collections\n    - scipy.spatial\n    - python-Levenshtein\n\n    Example:\n    >>> calculate_similarity_scores(\"Hello, World!\", \"Hello World\")\n    \"\"\"\n    text1 = ALPHANUMERIC.sub(' ', text1).lower()\n    text2 = ALPHANUMERIC.sub(' ', text2).lower()\n\n    vec1 = Counter(text1.split())\n    vec2 = Counter(text2.split())\n\n    intersection = set(vec1.keys()) & set(vec2.keys())\n    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n\n    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n    denominator = np.sqrt(sum1) * np.sqrt(sum2)\n\n    if not denominator:\n        cosine_similarity = 0.0\n    else:\n        cosine_similarity = float(numerator) / denominator\n\n    levenshtein_ratio = ratio(text1, text2)\n\n    return cosine_similarity, levenshtein_ratio\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nre, nltk, sklearn.feature_extraction.text, sklearn.decomposition\n```python\nimport re\nimport nltk\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import NMF\n\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\nSTOPWORDS = nltk.corpus.stopwords.words('english')\n\ndef extract_topics(texts, num_topics):\n    \"\"\"\n    Extract topics from a list of texts using Non-negative Matrix Factorization (NMF). \n    The texts are first cleaned by removing all non-alphanumeric characters except space, \n    lowercased, and stop words are removed.\n\n    Parameters:\n    texts (list): A list of strings.\n    num_topics (int): The number of topics to extract.\n\n    Returns:\n    list: A list of extracted topics.\n\n    Requirements:\n    - re\n    - nltk\n    - sklearn.feature_extraction.text\n    - sklearn.decomposition\n\n    Example:\n    >>> texts = [\"Hello, World!\", \"Machine Learning is great\", \"Python is my favorite programming language\"]\n    >>> extract_topics(texts, 2)\n    \"\"\"\n    cleaned_texts = [ALPHANUMERIC.sub(' ', text).lower() for text in texts]\n    tokenized_texts = [[word for word in text.split() if word not in STOPWORDS] for text in cleaned_texts]\n\n    vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n    tfidf = vectorizer.fit_transform([' '.join(text) for text in tokenized_texts])\n\n    nmf = NMF(n_components=num_topics, random_state=1).fit(tfidf)\n    feature_names = vectorizer.get_feature_names_out()\n\n    topics = []\n    for topic_idx, topic in enumerate(nmf.components_):\n        topics.append([feature_names[i] for i in topic.argsort()[:-num_topics - 1:-1]])\n\n    return topics\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nre, string, nltk.sentiment.vader\n```python\nimport re\nimport string\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\nPUNCTUATIONS = string.punctuation\n\ndef analyze_sentiment(text):\n    \"\"\"\n    Analyze the sentiment of a text using NLTK's VADER SentimentIntensityAnalyzer. \n    The text is first cleaned by removing all non-alphanumeric characters except space, \n    lowercased, and punctuation is removed.\n\n    Parameters:\n    text (str): The string to analyze.\n\n    Returns:\n    dict: A dictionary with sentiment scores.\n\n    Requirements:\n    - re\n    - string\n    - nltk.sentiment.vader\n\n    Example:\n    >>> analyze_sentiment(\"I love Python!\")\n    \"\"\"\n    text = ALPHANUMERIC.sub(' ', text).lower()\n    text = text.translate(str.maketrans('', '', PUNCTUATIONS))\n\n    sia = SentimentIntensityAnalyzer()\n    sentiment_scores = sia.polarity_scores(text)\n\n    return sentiment_scores\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nre, nltk, gensim\n```python\nimport re\nimport nltk\nfrom gensim.models import Word2Vec\n\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\nSTOPWORDS = nltk.corpus.stopwords.words('english')\n\ndef generate_word_vectors(texts):\n    \"\"\"\n    Generate word vectors from a list of texts using gensim's Word2Vec model. \n    The texts are first cleaned by removing all non-alphanumeric characters except space, \n    lowercased, and stop words are removed.\n\n    Parameters:\n    texts (list): A list of strings.\n\n    Returns:\n    Word2Vec: A trained Word2Vec model.\n\n    Requirements:\n    - re\n    - nltk\n    - gensim\n\n    Example:\n    >>> texts = [\"Hello, World!\", \"Machine Learning is great\", \"Python is my favorite programming language\"]\n    >>> model = generate_word_vectors(texts)\n    >>> vector = model.wv['python']\n    \"\"\"\n    cleaned_texts = [ALPHANUMERIC.sub(' ', text).lower() for text in texts]\n    tokenized_texts = [[word for word in text.split() if word not in STOPWORDS] for text in cleaned_texts]\n\n    model = Word2Vec(sentences=tokenized_texts, vector_size=100, window=5, min_count=1, workers=4)\n\n    return model\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nre, nltk, sklearn.feature_extraction.text\n```python\nimport re\nimport nltk\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\nSTOPWORDS = nltk.corpus.stopwords.words('english')\n\ndef generate_document_term_matrix(texts):\n    \"\"\"\n    Generate a document-term matrix from a list of texts using CountVectorizer from sklearn. \n    The texts are first cleaned by removing all non-alphanumeric characters except space, \n    lowercased, and stop words are removed.\n\n    Parameters:\n    texts (list): A list of strings.\n\n    Returns:\n    DataFrame: A pandas DataFrame representing the document-term matrix.\n\n    Requirements:\n    - re\n    - nltk\n    - sklearn.feature_extraction.text\n\n    Example:\n    >>> texts = [\"Hello, World!\", \"Machine Learning is great\", \"Python is my favorite programming language\"]\n    >>> dtm = generate_document_term_matrix(texts)\n    \"\"\"\n    cleaned_texts = [ALPHANUMERIC.sub(' ', text).lower() for text in texts]\n    tokenized_texts = [' '.join([word for word in text.split() if word not in STOPWORDS]) for text in cleaned_texts]\n\n    vectorizer = CountVectorizer()\n    dtm = vectorizer.fit_transform(tokenized_texts)\n    dtm_df = pd.DataFrame(dtm.toarray(), columns=vectorizer.get_feature_names_out())\n\n    return dtm_df\n```"}
{"seed": "{\"task_id\": 17138464, \"prompt\": \"def f_17138464(x, y):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"plt.plot(x, y, label='H\\\\u2082O')\", \"test_start\": \"\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\n\\ndef check(candidate):\", \"test\": [\"\\n    pic = candidate(np.array([1,2,3]),np.array([4,5,6]))[0]\\n    assert pic.get_label() == 'H\\u2082O'\\n    x, y = pic.get_data()\\n    assert all(x == np.array([1,2,3]))\\n    assert all(y == np.array([4,5,6]))\\n\", \"\\n    pic = candidate(np.array([6, 7, 899]),np.array([0, 1, 245]))[0]\\n    assert pic.get_label() == 'H\\u2082O'\\n    x, y = pic.get_data()\\n    assert all(x == np.array([6, 7, 899]))\\n    assert all(y == np.array([0, 1, 245]))\\n\"], \"entry_point\": \"f_17138464\", \"intent\": \"subscript text 'H20' with '2' as subscripted in matplotlib labels for arrays 'x' and 'y'.\", \"library\": [\"matplotlib\", \"numpy\"]}", "generation": "Scenario 1: \nnumpy, matplotlib.pyplot, scipy.stats, pandas\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nimport pandas as pd\n\n# Constants\nLABELS = ['H\\u2082O', 'O\\u2082', 'CO\\u2082', 'N\\u2082', 'Ar']\n\ndef plot_normal_distribution(x, y, labels):\n    \"\"\"\n    Plot normal distributions for multiple arrays 'x' and 'y' with labels.\n    Each pair (x, y) represents a different chemical compound in the 'labels' list.\n\n    Parameters:\n    x (list): List of numpy arrays representing the x-values of the data points.\n    y (list): List of numpy arrays representing the y-values of the data points.\n    labels (list): List of strings representing the labels for the chemical compounds.\n\n    Returns:\n    fig: Matplotlib figure object.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - scipy.stats\n    - pandas\n\n    Example:\n    >>> x = [np.array([1,2,3]), np.array([4,5,6]), np.array([7,8,9])]\n    >>> y = [np.array([4,5,6]), np.array([7,8,9]), np.array([10,11,12])]\n    >>> labels = ['H\\u2082O', 'O\\u2082', 'CO\\u2082']\n    >>> fig = plot_normal_distribution(x, y, labels)\n    >>> plt.show(fig)\n    \"\"\"\n    fig, ax = plt.subplots()\n\n    for i in range(len(x)):\n        mu = np.mean(y[i])\n        sigma = np.std(y[i])\n        pdf = stats.norm.pdf(x[i], mu, sigma)\n        ax.plot(x[i], pdf, label=labels[i])\n    \n    ax.legend()\n    \n    return fig\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, matplotlib.pyplot, pandas, sklearn.preprocessing\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\n# Constants\nLABELS = ['H\\u2082O', 'O\\u2082', 'CO\\u2082', 'N\\u2082', 'Ar']\n\ndef plot_scaled_data(x, y, labels):\n    \"\"\"\n    Scale the 'x' and 'y' arrays using sklearn's StandardScaler and plot them \n    with labels.\n\n    Parameters:\n    x (list): List of numpy arrays representing the x-values of the data points.\n    y (list): List of numpy arrays representing the y-values of the data points.\n    labels (list): List of strings representing the labels for the chemical compounds.\n\n    Returns:\n    fig: Matplotlib figure object.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - pandas\n    - sklearn.preprocessing\n\n    Example:\n    >>> x = [np.array([1,2,3]), np.array([4,5,6]), np.array([7,8,9])]\n    >>> y = [np.array([4,5,6]), np.array([7,8,9]), np.array([10,11,12])]\n    >>> labels = ['H\\u2082O', 'O\\u2082', 'CO\\u2082']\n    >>> fig = plot_scaled_data(x, y, labels)\n    >>> plt.show(fig)\n    \"\"\"\n    scaler = StandardScaler()\n\n    fig, ax = plt.subplots()\n\n    for i in range(len(x)):\n        xy = np.vstack((x[i], y[i])).T\n        xy_scaled = scaler.fit_transform(xy)\n        ax.plot(xy_scaled[:, 0], xy_scaled[:, 1], label=labels[i])\n    \n    ax.legend()\n    \n    return fig\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, matplotlib.pyplot, pandas, seaborn\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n# Constants\nLABELS = ['H\\u2082O', 'O\\u2082', 'CO\\u2082', 'N\\u2082', 'Ar']\n\ndef plot_heatmap(x, y, labels):\n    \"\"\"\n    Create a heatmap using seaborn library for 'x' and 'y' numpy arrays with labels.\n\n    Parameters:\n    x (list): List of numpy arrays representing the x-values of the data points.\n    y (list): List of numpy arrays representing the y-values of the data points.\n    labels (list): List of strings representing the labels for the chemical compounds.\n\n    Returns:\n    ax: A seaborn heatmap object.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - pandas\n    - seaborn\n\n    Example:\n    >>> x = [np.array([1,2,3]), np.array([4,5,6]), np.array([7,8,9])]\n    >>> y = [np.array([4,5,6]), np.array([7,8,9]), np.array([10,11,12])]\n    >>> labels = ['H\\u2082O', 'O\\u2082', 'CO\\u2082']\n    >>> ax = plot_heatmap(x, y, labels)\n    >>> plt.show()\n    \"\"\"\n    data = []\n\n    for i in range(len(x)):\n        data.append(np.concatenate((x[i], y[i])))\n\n    df = pd.DataFrame(data, index=labels)\n    ax = sns.heatmap(df, cmap='coolwarm')\n    \n    return ax\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, matplotlib.pyplot, pandas, sklearn.decomposition\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.decomposition import PCA\n\n# Constants\nLABELS = ['H\\u2082O', 'O\\u2082', 'CO\\u2082', 'N\\u2082', 'Ar']\n\ndef plot_pca(x, y, labels):\n    \"\"\"\n    Perform Principal Component Analysis (PCA) on 'x' and 'y' numpy arrays and plot \n    the results with labels.\n\n    Parameters:\n    x (list): List of numpy arrays representing the x-values of the data points.\n    y (list): List of numpy arrays representing the y-values of the data points.\n    labels (list): List of strings representing the labels for the chemical compounds.\n\n    Returns:\n    fig: Matplotlib figure object.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - pandas\n    - sklearn.decomposition\n\n    Example:\n    >>> x = [np.array([1,2,3]), np.array([4,5,6]), np.array([7,8,9])]\n    >>> y = [np.array([4,5,6]), np.array([7,8,9]), np.array([10,11,12])]\n    >>> labels = ['H\\u2082O', 'O\\u2082', 'CO\\u2082']\n    >>> fig = plot_pca(x, y, labels)\n    >>> plt.show(fig)\n    \"\"\"\n    pca = PCA(n_components=2)\n\n    fig, ax = plt.subplots()\n\n    for i in range(len(x)):\n        xy = np.vstack((x[i], y[i])).T\n        xy_transformed = pca.fit_transform(xy)\n        ax.plot(xy_transformed[:, 0], xy_transformed[:, 1], label=labels[i])\n    \n    ax.legend()\n    \n    return fig\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, matplotlib.pyplot, pandas, scipy.optimize\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom scipy.optimize import curve_fit\n\n# Constants\nLABELS = ['H\\u2082O', 'O\\u2082', 'CO\\u2082', 'N\\u2082', 'Ar']\n\ndef plot_curve_fit(x, y, labels):\n    \"\"\"\n    Fit a curve to the 'x' and 'y' numpy arrays using scipy's curve_fit function and plot \n    with labels.\n\n    Parameters:\n    x (list): List of numpy arrays representing the x-values of the data points.\n    y (list): List of numpy arrays representing the y-values of the data points.\n    labels (list): List of strings representing the labels for the chemical compounds.\n\n    Returns:\n    fig: Matplotlib figure object.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - pandas\n    - scipy.optimize\n\n    Example:\n    >>> x = [np.array([1,2,3]), np.array([4,5,6]), np.array([7,8,9])]\n    >>> y = [np.array([4,5,6]), np.array([7,8,9]), np.array([10,11,12])]\n    >>> labels = ['H\\u2082O', 'O\\u2082', 'CO\\u2082']\n    >>> fig = plot_curve_fit(x, y, labels)\n    >>> plt.show(fig)\n    \"\"\"\n    def func(x, a, b, c):\n        return a * np.exp(-b * x) + c\n\n    fig, ax = plt.subplots()\n\n    for i in range(len(x)):\n        popt, pcov = curve_fit(func, x[i], y[i])\n        ax.plot(x[i], func(x[i], *popt), label=labels[i])\n    \n    ax.legend()\n    \n    return fig\n```\n`GPT_ODEX_BREAK`"}
{"seed": "{\"task_id\": 17138464, \"prompt\": \"def f_17138464(x, y):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"plt.plot(x, y, label='$H_2O$')\", \"test_start\": \"\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\n\\ndef check(candidate):\", \"test\": [\"\\n    pic = candidate(np.array([1,2,3]),np.array([4,5,6]))[0]\\n    assert pic.get_label() == '$H_2O$'\\n    x, y = pic.get_data()\\n    assert all(x == np.array([1,2,3]))\\n    assert all(y == np.array([4,5,6]))\\n\", \"\\n    pic = candidate(np.array([6, 7, 899]),np.array([0, 1, 245]))[0]\\n    assert pic.get_label() == '$H_2O$'\\n    x, y = pic.get_data()\\n    assert all(x == np.array([6, 7, 899]))\\n    assert all(y == np.array([0, 1, 245]))\\n\"], \"entry_point\": \"f_17138464\", \"intent\": \"subscript text 'H20' with '2' as subscripted in matplotlib labels for arrays 'x' and 'y'.\", \"library\": [\"matplotlib\", \"numpy\"]}", "generation": "Scenario 1: \npandas, matplotlib, numpy, statistics\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport statistics\n\n# Constants\nLABELS = ['Product A', 'Product B', 'Product C', 'Product D', 'Product E']\n\ndef plot_sales_trends(sales_data):\n    \"\"\"\n    Plot the sales trends of five different products over a year with standard deviation lines.\n    \n    Parameters:\n    sales_data (pandas.DataFrame): The DataFrame containing the sales data.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - numpy\n    - statistics\n\n    Example:\n    >>> sales_data = pd.DataFrame({\n    ...     'Month': range(1, 13),\n    ...     'Product A': np.random.randint(100, 200, 12),\n    ...     'Product B': np.random.randint(150, 250, 12),\n    ...     'Product C': np.random.randint(120, 220, 12),\n    ...     'Product D': np.random.randint(130, 230, 12),\n    ...     'Product E': np.random.randint(140, 240, 12)\n    ... })\n    >>> plot_sales_trends(sales_data)\n    \"\"\"\n    for label in LABELS:\n        monthly_sales = sales_data[label]\n        plt.plot(sales_data['Month'], monthly_sales, label=label)\n        plt.fill_between(sales_data['Month'], \n                         monthly_sales - statistics.stdev(monthly_sales), \n                         monthly_sales + statistics.stdev(monthly_sales), \n                         alpha=0.2)\n        \n    plt.xlabel('Month')\n    plt.ylabel('Sales')\n    plt.legend()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nmatplotlib, numpy, scipy\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.optimize import curve_fit\n\ndef plot_exponential_curve_fit(x, y):\n    \"\"\"\n    Fit an exponential curve to the given x and y data and plot the curve along with the data.\n    \n    Parameters:\n    x (numpy.array): The x data.\n    y (numpy.array): The y data.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - matplotlib.pyplot\n    - numpy\n    - scipy.optimize.curve_fit\n\n    Example:\n    >>> x = np.array([1, 2, 3, 4, 5])\n    >>> y = np.array([2, 4, 8, 16, 32])\n    >>> plot_exponential_curve_fit(x, y)\n    \"\"\"\n    def exponential_func(x, a, b):\n        return a * np.exp(b * x)\n\n    params, params_covariance = curve_fit(exponential_func, x, y)\n\n    plt.scatter(x, y, label='Data')\n    plt.plot(x, exponential_func(x, params[0], params[1]), \n             label='Fit: a=%5.3f, b=%5.3f' % tuple(params))\n\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.legend()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nmatplotlib, numpy, sklearn\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\ndef plot_linear_regression(x, y):\n    \"\"\"\n    Perform linear regression on the given x and y data and plot the regression line along with the data.\n    \n    Parameters:\n    x (numpy.array): The x data.\n    y (numpy.array): The y data.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - matplotlib.pyplot\n    - numpy\n    - sklearn.linear_model.LinearRegression\n\n    Example:\n    >>> x = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n    >>> y = np.array([2, 3, 5, 7, 11]).reshape(-1, 1)\n    >>> plot_linear_regression(x, y)\n    \"\"\"\n    model = LinearRegression()\n    model.fit(x, y)\n\n    plt.scatter(x, y, color='blue', label='Data')\n    plt.plot(x, model.predict(x), color='red', label='Fit: y=%5.3fx + %5.3f' % (model.coef_, model.intercept_))\n\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.legend()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, matplotlib, numpy, seaborn\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\ndef plot_correlation_matrix(df):\n    \"\"\"\n    Plot a correlation matrix for a given data frame using seaborn.\n    \n    Parameters:\n    df (pandas.DataFrame): The DataFrame to analyze.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - numpy\n    - seaborn\n\n    Example:\n    >>> df = pd.DataFrame({\n    ...     'A': np.random.rand(10),\n    ...     'B': np.random.rand(10),\n    ...     'C': np.random.rand(10),\n    ...     'D': np.random.rand(10)\n    ... })\n    >>> plot_correlation_matrix(df)\n    \"\"\"\n    corr = df.corr()\n    sns.heatmap(corr, annot=True, cmap='coolwarm', center=0)\n\n    plt.title('Correlation Matrix')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nmatplotlib, numpy, pandas, seaborn\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\ndef plot_boxplot(df, column, group_by):\n    \"\"\"\n    Plot a boxplot of a column grouped by another column in a data frame.\n    \n    Parameters:\n    df (pandas.DataFrame): The DataFrame to analyze.\n    column (str): The column to plot.\n    group_by (str): The column to group by.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - matplotlib.pyplot\n    - numpy\n    - pandas\n    - seaborn\n\n    Example:\n    >>> df = pd.DataFrame({\n    ...     'Category': ['A', 'A', 'A', 'B', 'B', 'B', 'C', 'C', 'C'],\n    ...     'Value': np.random.rand(9)\n    ... })\n    >>> plot_boxplot(df, 'Value', 'Category')\n    \"\"\"\n    sns.boxplot(x=group_by, y=column, data=df)\n\n    plt.title('Boxplot of ' + column + ' grouped by ' + group_by)\n    plt.show()\n```"}
{"seed": "{\"task_id\": 9138112, \"prompt\": \"def f_9138112(mylist):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"[x for x in mylist if len(x) == 3]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([[1,2,3], 'abc', [345,53], 'avsvasf']) == [[1,2,3], 'abc']\\n\", \"\\n    assert candidate([[435,654.4,45,2],[34,34,757,65,32423]]) == []\\n\"], \"entry_point\": \"f_9138112\", \"intent\": \"loop over a list `mylist` if sublists length equals 3\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, matplotlib, random\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import choice\n\n# Constants\nCATEGORIES = ['Electronics', 'Books', 'Clothing', 'Home & Kitchen', 'Toys & Games']\n\ndef generate_sales_report(mylist):\n    \"\"\"\n    Generate a sales report for a list of products across various categories. Only include \n    products with exactly three attributes in the report. Then, plot a bar graph showing \n    the number of products in each category.\n\n    Parameters:\n    mylist (list): The list of products. Each product is represented as a list with at most \n    three attributes: [category, price, units sold].\n\n    Returns:\n    DataFrame: A pandas DataFrame with the sales report.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib\n    - random\n\n    Example:\n    >>> products = [['Electronics', 200, 100], ['Books', 15, 200], ['Home & Kitchen', 50, 150], \n    ...             ['Electronics', 150, 90], ['Books', 20, 300], ['Electronics', 100], \n    ...             ['Toys & Games', 30, 200, 50]]\n    >>> report = generate_sales_report(products)\n    >>> print(report)\n    >>> report['Category'].value_counts().plot(kind='bar')\n    \"\"\"\n    report_data = [product for product in mylist if len(product) == 3]\n    \n    report_df = pd.DataFrame(report_data, columns=['Category', 'Price', 'Units Sold'])\n\n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nnumpy, itertools, statistics, math\n```python\nimport numpy as np\nfrom itertools import combinations\nimport statistics as stats\nimport math\n\ndef find_optimal_combinations(mylist):\n    \"\"\"\n    Find all combinations of three elements in a given list, calculate their mean, and return \n    the combinations with the highest and lowest mean. If there are multiple combinations with \n    the same mean, return the combination with the smallest standard deviation.\n\n    Parameters:\n    mylist (list): The list of numbers.\n\n    Returns:\n    tuple: A tuple with two elements. The first element is the combination with the highest mean, \n    and the second element is the combination with the lowest mean.\n\n    Requirements:\n    - numpy\n    - itertools\n    - statistics\n    - math\n\n    Example:\n    >>> numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n    >>> find_optimal_combinations(numbers)\n    \"\"\"\n    comb_with_means = [(comb, stats.mean(comb), stats.stdev(comb)) \n                       for comb in combinations(mylist, 3)]\n    \n    max_mean = max(comb_with_means, key=lambda x: (x[1], -x[2]))\n    min_mean = min(comb_with_means, key=lambda x: (x[1], x[2]))\n    \n    return max_mean[0], min_mean[0]\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \ncollections, itertools, random\n```python\nfrom collections import Counter\nfrom itertools import chain\nfrom random import choice\n\n# Constants\nWORDS = ['apple', 'banana', 'cherry', 'date', 'elderberry']\n\ndef most_common_words(mylist):\n    \"\"\"\n    Find the most common words in a list of sublists. Only include sublists with exactly \n    three words in the analysis. Return the five most common words.\n\n    Parameters:\n    mylist (list): The list of sublists.\n\n    Returns:\n    list: A list with the five most common words.\n\n    Requirements:\n    - collections\n    - itertools\n    - random\n\n    Example:\n    >>> text = [['apple', 'banana', 'cherry'], ['date', 'elderberry', 'apple'], \n    ...         ['banana', 'cherry', 'date'], ['elderberry', 'apple', 'banana'], \n    ...         ['cherry', 'date', 'elderberry'], ['apple', 'banana'], ['cherry', 'date', 'elderberry', 'apple']]\n    >>> most_common_words(text)\n    \"\"\"\n    sublists = [sublist for sublist in mylist if len(sublist) == 3]\n    \n    words = list(chain(*sublists))\n    \n    counter = Counter(words)\n\n    return [word for word, _ in counter.most_common(5)]\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, scipy.stats, matplotlib\n```python\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef analyze_data(mylist):\n    \"\"\"\n    Analyze a list of data points. Only include sublists with exactly three data points in the \n    analysis. Calculate the mean and standard deviation of the data points, perform a t-test to \n    determine if the mean is significantly different from zero, and plot a histogram of the data \n    points.\n\n    Parameters:\n    mylist (list): The list of data points.\n\n    Returns:\n    tuple: A tuple with three elements. The first element is the mean, the second element is \n    the standard deviation, and the third element is the p-value from the t-test.\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib\n\n    Example:\n    >>> data = [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12], [13, 14, 15], [16, 17, 18], [19, 20]]\n    >>> analyze_data(data)\n    \"\"\"\n    sublists = [sublist for sublist in mylist if len(sublist) == 3]\n    \n    data_points = np.array(sublists).flatten()\n    \n    mean = np.mean(data_points)\n    std_dev = np.std(data_points)\n    t_stat, p_value = stats.ttest_1samp(data_points, 0)\n    \n    plt.hist(data_points, bins='auto')\n    plt.show()\n\n    return mean, std_dev, p_value\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nrandom, math, numpy, matplotlib\n```python\nfrom random import choices\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nNUMBERS = list(range(1, 101))\nWEIGHTS = [math.exp(-x/100) for x in NUMBERS]\n\ndef generate_histogram(mylist):\n    \"\"\"\n    Generate a histogram based on a list of numbers. Only include sublists with exactly \n    three numbers in the histogram. The numbers are randomly selected from a predefined list \n    of numbers (1-100) with weights determined by an exponential decay function.\n\n    Parameters:\n    mylist (list): The list of numbers.\n\n    Returns:\n    None\n\n    Requirements:\n    - random\n    - math\n    - numpy\n    - matplotlib\n\n    Example:\n    >>> numbers = [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12], [13, 14, 15], \n    ...            [16, 17, 18], [19, 20, 21, 22], [23, 24]]\n    >>> generate_histogram(numbers)\n    \"\"\"\n    sublists = [sublist for sublist in mylist if len(sublist) == 3]\n    \n    data = []\n    \n    for sublist in sublists:\n        data.extend(choices(NUMBERS, weights=WEIGHTS, k=len(sublist)))\n    \n    plt.hist(data, bins='auto')\n    plt.show()\n\n    return None\n```"}
{"seed": "{\"task_id\": 1807026, \"prompt\": \"def f_1807026():\\n\\t\", \"suffix\": \"\\n\\treturn lst\", \"canonical_solution\": \"lst = [Object() for _ in range(100)]\", \"test_start\": \"\\nclass Object(): \\n    def __init__(self): \\n        self.name = \\\"object\\\"\\n\\ndef check(candidate):\", \"test\": [\"\\n    lst = candidate()\\n    assert all([x.name == \\\"object\\\" for x in lst])\\n\"], \"entry_point\": \"f_1807026\", \"intent\": \"initialize a list `lst` of 100 objects Object()\", \"library\": []}", "generation": "Scenario 1: \ncollections, random, matplotlib, pandas\n```python\nimport collections\nimport random\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants\nNAMES = ['Alice', 'Bob', 'Charlie', 'David', 'Eve']\nAGES = list(range(20, 61))\n\nclass Person:\n    def __init__(self, name, age):\n        self.name = name\n        self.age = age\n\ndef generate_people_data(n):\n    \"\"\"\n    Generate a list of n Person objects with random names and ages.\n    Then, plot a histogram of the ages and return a pandas DataFrame of the data.\n\n    Parameters:\n    n (int): The number of Person objects to generate.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the names and ages of the generated people.\n\n    Requirements:\n    - collections\n    - random\n    - matplotlib.pyplot\n    - pandas\n\n    Example:\n    >>> people_data = generate_people_data(100)\n    >>> print(people_data)\n    >>> people_data['Age'].hist(bins=range(20, 61))\n    \"\"\"\n    people = [Person(random.choice(NAMES), random.choice(AGES)) for _ in range(n)]\n    people_data = pd.DataFrame([(p.name, p.age) for p in people], columns=['Name', 'Age'])\n\n    return people_data\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nnumpy, random, matplotlib, functools\n```python\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nimport functools\n\n# Constants\nNUMBER_RANGE = (1, 101)\nARRAY_SIZE = 100\n\nclass NumberObject:\n    def __init__(self, number):\n        self.number = number\n\ndef generate_number_objects(n):\n    \"\"\"\n    Generate a list of n NumberObject objects with random numbers in a range.\n    Then, plot a histogram of the numbers and return a numpy array of the numbers.\n\n    Parameters:\n    n (int): The number of NumberObject objects to generate.\n\n    Returns:\n    ndarray: A numpy array with the numbers of the generated objects.\n\n    Requirements:\n    - numpy\n    - random\n    - matplotlib.pyplot\n    - functools\n\n    Example:\n    >>> number_array = generate_number_objects(100)\n    >>> print(number_array)\n    >>> plt.hist(number_array, bins=np.arange(*NUMBER_RANGE) - 0.5, edgecolor='black')\n    \"\"\"\n    number_objects = [NumberObject(random.randint(*NUMBER_RANGE)) for _ in range(n)]\n    number_array = np.array([no.number for no in number_objects])\n\n    return number_array\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nitertools, random, matplotlib, pandas\n```python\nimport itertools\nimport random\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants\nCOLORS = ['red', 'blue', 'green', 'yellow', 'black']\nSHAPES = ['circle', 'square', 'triangle', 'star', 'hexagon']\n\nclass ColoredShape:\n    def __init__(self, color, shape):\n        self.color = color\n        self.shape = shape\n\ndef generate_colored_shapes(n):\n    \"\"\"\n    Generate a list of n ColoredShape objects with random colors and shapes.\n    Then, plot a bar chart of the color distribution and return a pandas DataFrame of the data.\n\n    Parameters:\n    n (int): The number of ColoredShape objects to generate.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the colors and shapes of the generated objects.\n\n    Requirements:\n    - itertools\n    - random\n    - matplotlib.pyplot\n    - pandas\n\n    Example:\n    >>> shape_data = generate_colored_shapes(100)\n    >>> print(shape_data)\n    >>> shape_data['Color'].value_counts().plot(kind='bar')\n    \"\"\"\n    colored_shapes = [ColoredShape(random.choice(COLORS), random.choice(SHAPES)) for _ in range(n)]\n    shape_data = pd.DataFrame([(cs.color, cs.shape) for cs in colored_shapes], columns=['Color', 'Shape'])\n\n    return shape_data\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, random, matplotlib, functools\n```python\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nimport functools\n\n# Constants\nLETTERS = list('abcdefghijklmnopqrstuvwxyz')\nARRAY_SIZE = 100\n\nclass LetterObject:\n    def __init__(self, letter):\n        self.letter = letter\n\ndef generate_letter_objects(n):\n    \"\"\"\n    Generate a list of n LetterObject objects with random letters.\n    Then, plot a histogram of the letters and return a numpy array of the letters.\n\n    Parameters:\n    n (int): The number of LetterObject objects to generate.\n\n    Returns:\n    ndarray: A numpy array with the letters of the generated objects.\n\n    Requirements:\n    - numpy\n    - random\n    - matplotlib.pyplot\n    - functools\n\n    Example:\n    >>> letter_array = generate_letter_objects(100)\n    >>> print(letter_array)\n    >>> letter_counts = np.bincount(letter_array)\n    >>> plt.bar(np.arange(len(letter_counts)), letter_counts)\n    \"\"\"\n    letter_objects = [LetterObject(random.choice(LETTERS)) for _ in range(n)]\n    letter_array = np.array([lo.letter for lo in letter_objects])\n\n    return letter_array\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnumpy, random, matplotlib, pandas\n```python\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants\nFRUITS = ['apple', 'banana', 'cherry', 'date', 'elderberry']\nWEIGHTS = np.linspace(0.1, 1.0, 100)\n\nclass Fruit:\n    def __init__(self, name, weight):\n        self.name = name\n        self.weight = weight\n\ndef generate_fruit_basket(n):\n    \"\"\"\n    Generate a list of n Fruit objects with random names and weights.\n    Then, plot a histogram of the weights and return a pandas DataFrame of the data.\n\n    Parameters:\n    n (int): The number of Fruit objects to generate.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the names and weights of the generated fruits.\n\n    Requirements:\n    - numpy\n    - random\n    - matplotlib.pyplot\n    - pandas\n\n    Example:\n    >>> fruit_data = generate_fruit_basket(100)\n    >>> print(fruit_data)\n    >>> fruit_data['Weight'].hist(bins=range(0, 1.1, 0.1))\n    \"\"\"\n    fruits = [Fruit(random.choice(FRUITS), random.choice(WEIGHTS)) for _ in range(n)]\n    fruit_data = pd.DataFrame([(f.name, f.weight) for f in fruits], columns=['Name', 'Weight'])\n\n    return fruit_data\n```\n"}
{"seed": "{\"task_id\": 13793321, \"prompt\": \"def f_13793321(df1, df2):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"df1.merge(df2, on='Date_Time')\", \"test_start\": \"\\nimport pandas as pd\\n\\ndef check(candidate):\", \"test\": [\"\\n    df1 = pd.DataFrame([[1, 2, 3]], columns=[\\\"Date\\\", \\\"Time\\\", \\\"Date_Time\\\"])\\n    df2 = pd.DataFrame([[1, 3],[4, 9]], columns=[\\\"Name\\\", \\\"Date_Time\\\"])\\n    assert candidate(df1, df2).to_dict() == {'Date': {0: 1}, 'Time': {0: 2}, 'Date_Time': {0: 3}, 'Name': {0: 1}}\\n\"], \"entry_point\": \"f_13793321\", \"intent\": \"joining data from dataframe `df1` with data from dataframe `df2` based on matching values of column 'Date_Time' in both dataframes\", \"library\": [\"pandas\"]}", "generation": "Scenario 1:\npandas, datetime, numpy, matplotlib\n```python\nimport pandas as pd\nfrom datetime import datetime\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Sample data\n# Assume df1 represents sales data and df2 represents stock data\n\ndef merge_and_plot(df1, df2, common_column, value_column):\n    \"\"\"\n    Merge two dataframes on a common column, calculate the mean of a value column, \n    and plot the results.\n\n    Parameters:\n    df1 (DataFrame): The first dataframe.\n    df2 (DataFrame): The second dataframe.\n    common_column (str): The common column to merge on.\n    value_column (str): The column to calculate the mean of.\n\n    Returns:\n    DataFrame: A dataframe that is the result of merging df1 and df2 on common_column.\n\n    Requirements:\n    - pandas\n    - datetime\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> df1 = pd.DataFrame({'Date': pd.date_range(start='1/1/2020', end='31/12/2020'), 'Sales': np.random.randint(50, 100, size=366)})\n    >>> df2 = pd.DataFrame({'Date': pd.date_range(start='1/1/2020', end='31/12/2020'), 'Stock': np.random.randint(200, 300, size=366)})\n    >>> merged_df = merge_and_plot(df1, df2, 'Date', 'Sales')\n    >>> print(merged_df)\n    >>> merged_df['Sales'].plot()\n    \"\"\"\n    merged_df = df1.merge(df2, on=common_column)\n    merged_df[value_column] = merged_df[value_column].mean()\n\n    return merged_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, datetime, numpy, seaborn\n```python\nimport pandas as pd\nfrom datetime import datetime\nimport numpy as np\nimport seaborn as sns\n\n# Sample data\n# Assume df1 represents customer data and df2 represents transaction data\n\ndef merge_and_visualize(df1, df2, common_column, value_column):\n    \"\"\"\n    Merge two dataframes on a common column, calculate the sum of a value column, and \n    visualize the distribution of the resulting values.\n\n    Parameters:\n    df1 (DataFrame): The first dataframe.\n    df2 (DataFrame): The second dataframe.\n    common_column (str): The common column to merge on.\n    value_column (str): The column to calculate the sum of.\n\n    Returns:\n    DataFrame: A dataframe that is the result of merging df1 and df2 on common_column.\n\n    Requirements:\n    - pandas\n    - datetime\n    - numpy\n    - seaborn\n\n    Example:\n    >>> df1 = pd.DataFrame({'Customer ID': range(1, 101), 'Name': ['Customer ' + str(i) for i in range(1, 101)]})\n    >>> df2 = pd.DataFrame({'Customer ID': np.random.randint(1, 101, size=1000), 'Transaction Amount': np.random.randint(1, 100, size=1000)})\n    >>> merged_df = merge_and_visualize(df1, df2, 'Customer ID', 'Transaction Amount')\n    >>> print(merged_df)\n    >>> sns.histplot(data=merged_df, x='Transaction Amount')\n    \"\"\"\n    merged_df = df1.merge(df2, on=common_column)\n    merged_df[value_column] = merged_df.groupby(common_column)[value_column].transform('sum')\n\n    return merged_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, datetime, numpy, matplotlib\n```python\nimport pandas as pd\nfrom datetime import datetime\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Sample data\n# Assume df1 represents employee data and df2 represents salary data\n\ndef merge_and_analyze(df1, df2, common_column, value_column):\n    \"\"\"\n    Merge two dataframes on a common column, calculate the median of a value column, and \n    plot the data.\n\n    Parameters:\n    df1 (DataFrame): The first dataframe.\n    df2 (DataFrame): The second dataframe.\n    common_column (str): The common column to merge on.\n    value_column (str): The column to calculate the median of.\n\n    Returns:\n    DataFrame: A dataframe that is the result of merging df1 and df2 on common_column.\n\n    Requirements:\n    - pandas\n    - datetime\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> df1 = pd.DataFrame({'Employee ID': range(1, 101), 'Name': ['Employee ' + str(i) for i in range(1, 101)]})\n    >>> df2 = pd.DataFrame({'Employee ID': np.random.randint(1, 101, size=1000), 'Salary': np.random.randint(30000, 60000, size=1000)})\n    >>> merged_df = merge_and_analyze(df1, df2, 'Employee ID', 'Salary')\n    >>> print(merged_df)\n    >>> merged_df['Salary'].plot(kind='box')\n    \"\"\"\n    merged_df = df1.merge(df2, on=common_column)\n    merged_df[value_column] = merged_df[value_column].median()\n\n    return merged_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, datetime, numpy, plotly.express\n```python\nimport pandas as pd\nfrom datetime import datetime\nimport numpy as np\nimport plotly.express as px\n\n# Sample data\n# Assume df1 represents product data and df2 represents sales data\n\ndef merge_and_interactive_plot(df1, df2, common_column, value_column):\n    \"\"\"\n    Merge two dataframes on a common column, calculate the max of a value column, \n    and create an interactive plot.\n\n    Parameters:\n    df1 (DataFrame): The first dataframe.\n    df2 (DataFrame): The second dataframe.\n    common_column (str): The common column to merge on.\n    value_column (str): The column to calculate the max of.\n\n    Returns:\n    DataFrame: A dataframe that is the result of merging df1 and df2 on common_column.\n\n    Requirements:\n    - pandas\n    - datetime\n    - numpy\n    - plotly.express\n\n    Example:\n    >>> df1 = pd.DataFrame({'Product ID': range(1, 101), 'Product Name': ['Product ' + str(i) for i in range(1, 101)]})\n    >>> df2 = pd.DataFrame({'Product ID': np.random.randint(1, 101, size=1000), 'Sales': np.random.randint(1, 100, size=1000)})\n    >>> merged_df = merge_and_interactive_plot(df1, df2, 'Product ID', 'Sales')\n    >>> print(merged_df)\n    >>> px.histogram(merged_df, x='Sales')\n    \"\"\"\n    merged_df = df1.merge(df2, on=common_column)\n    merged_df[value_column] = merged_df[value_column].max()\n\n    return merged_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, datetime, numpy, seaborn\n```python\nimport pandas as pd\nfrom datetime import datetime\nimport numpy as np\nimport seaborn as sns\n\n# Sample data\n# Assume df1 represents student data and df2 represents marks data\n\ndef merge_and_correlation(df1, df2, common_column, value_column):\n    \"\"\"\n    Merge two dataframes on a common column, calculate the correlation of a value column, \n    and plot a heatmap.\n\n    Parameters:\n    df1 (DataFrame): The first dataframe.\n    df2 (DataFrame): The second dataframe.\n    common_column (str): The common column to merge on.\n    value_column (str): The column to calculate the correlation of.\n\n    Returns:\n    DataFrame: A dataframe that is the result of merging df1 and df2 on common_column.\n\n    Requirements:\n    - pandas\n    - datetime\n    - numpy\n    - seaborn\n\n    Example:\n    >>> df1 = pd.DataFrame({'Student ID': range(1, 101), 'Name': ['Student ' + str(i) for i in range(1, 101)]})\n    >>> df2 = pd.DataFrame({'Student ID': np.random.randint(1, 101, size=1000), 'Marks': np.random.randint(1, 100, size=1000)})\n    >>> merged_df = merge_and_correlation(df1, df2, 'Student ID', 'Marks')\n    >>> print(merged_df)\n    >>> sns.heatmap(merged_df.corr(), annot=True)\n    \"\"\"\n    merged_df = df1.merge(df2, on=common_column)\n    correlation = merged_df.corr()[value_column]\n\n    return correlation\n```\n"}
{"seed": "{\"task_id\": 3367288, \"prompt\": \"def f_3367288(str1):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"'first string is: %s, second one is: %s' % (str1, 'geo.tif')\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(\\\"s001\\\") == \\\"first string is: s001, second one is: geo.tif\\\"\\n\", \"\\n    assert candidate(\\\"\\\") == \\\"first string is: , second one is: geo.tif\\\"\\n\", \"\\n    assert candidate(\\\"  \\\") == \\\"first string is:   , second one is: geo.tif\\\"\\n\"], \"entry_point\": \"f_3367288\", \"intent\": \"use `%s` operator to print variable values `str1` inside a string\", \"library\": []}", "generation": "Scenario 1: \nos, pathlib, zipfile\n\n```python\nimport os\nfrom pathlib import Path\nimport zipfile\n\n# Constants\nSECOND_FILE = 'geo.tif'\n\ndef extract_and_rename_zip_file(zip_file_path, str1):\n    \"\"\"\n    Function to extract a zip file and rename the second file in the archive.\n\n    Parameters:\n    zip_file_path (str): The path to the zip file.\n    str1 (str): The new name for the second file in the archive.\n\n    Returns:\n    bool: True if the process was successful, False otherwise.\n\n    Requirements:\n    - os\n    - pathlib\n    - zipfile\n\n    Example:\n    >>> extract_and_rename_zip_file('/path/to/archive.zip', 'new_file_name.tif')\n    \"\"\"\n    try:\n        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n            zip_ref.extractall()\n\n        files = os.listdir('.')\n        if SECOND_FILE in files:\n            os.rename(SECOND_FILE, str1)\n\n        return True\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return False\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \npandas, os, matplotlib.pyplot, seaborn\n\n```python\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nSECOND_FILE = 'geo.tif'\n\ndef plot_data_from_csv(csv_file_path, str1):\n    \"\"\"\n    Function to read a CSV file and plot the data in a column specified by str1.\n\n    Parameters:\n    csv_file_path (str): The path to the CSV file.\n    str1 (str): The name of the column to plot.\n\n    Returns:\n    None. Displays a plot.\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> plot_data_from_csv('/path/to/data.csv', 'column_name')\n    \"\"\"\n    try:\n        df = pd.read_csv(csv_file_path)\n        sns.displot(df, x=str1)\n        plt.show()\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nos, shutil, fnmatch\n\n```python\nimport os\nimport shutil\nfrom fnmatch import fnmatch\n\n# Constants\nPATTERN = '*.tif'\n\ndef copy_matching_files(source_directory, destination_directory, str1):\n    \"\"\"\n    Function to copy all files matching a pattern from a source directory to a destination directory, \n    and rename them by adding str1 as a prefix.\n\n    Parameters:\n    source_directory (str): The path to the source directory.\n    destination_directory (str): The path to the destination directory.\n    str1 (str): The prefix to add to the copied files.\n\n    Returns:\n    None. Copies and renames files.\n\n    Requirements:\n    - os\n    - shutil\n    - fnmatch\n\n    Example:\n    >>> copy_matching_files('/path/to/source', '/path/to/destination', 'prefix_')\n    \"\"\"\n    try:\n        for path, subdirs, files in os.walk(source_directory):\n            for name in files:\n                if fnmatch(name, PATTERN):\n                    shutil.copy(os.path.join(path, name), os.path.join(destination_directory, f\"{str1}{name}\"))\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \npandas, numpy, matplotlib.pyplot\n\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nSECOND_FILE = 'geo.tif'\n\ndef create_and_plot_random_data(str1):\n    \"\"\"\n    Function to create a DataFrame of random data and plot it using a line plot.\n    The DataFrame has a single column named str1.\n\n    Parameters:\n    str1 (str): The column name for the DataFrame.\n\n    Returns:\n    None. Displays a plot.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> create_and_plot_random_data('random_data')\n    \"\"\"\n    try:\n        df = pd.DataFrame(np.random.rand(50, 1), columns=[str1])\n        df.plot(kind='line')\n        plt.show()\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nos, subprocess, shlex\n\n```python\nimport os\nimport subprocess\nimport shlex\n\n# Constants\nSECOND_FILE = 'geo.tif'\n\ndef run_command_in_directory(directory_path, command_str):\n    \"\"\"\n    Function to run a command in a specific directory.\n\n    Parameters:\n    directory_path (str): The path to the directory in which to run the command.\n    command_str (str): The command to run.\n\n    Returns:\n    str: The output of the command.\n\n    Requirements:\n    - os\n    - subprocess\n    - shlex\n\n    Example:\n    >>> run_command_in_directory('/path/to/directory', 'ls -l')\n    \"\"\"\n    try:\n        original_directory = os.getcwd()\n        os.chdir(directory_path)\n        process = subprocess.Popen(shlex.split(command_str), stdout=subprocess.PIPE)\n        output, _ = process.communicate()\n        os.chdir(original_directory)\n\n        return output.decode('utf-8')\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n```"}
{"seed": "{\"task_id\": 3475251, \"prompt\": \"def f_3475251():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"[x.strip() for x in '2.MATCHES $$TEXT$$ STRING'.split('$$TEXT$$')]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate() == ['2.MATCHES', 'STRING']\\n\"], \"entry_point\": \"f_3475251\", \"intent\": \"Split a string '2.MATCHES $$TEXT$$ STRING' by a delimiter '$$TEXT$$'\", \"library\": []}", "generation": "Scenario 1: \npandas, re, random, matplotlib\n```python\nimport pandas as pd\nimport re\nfrom random import choice\nimport matplotlib.pyplot as plt\n\n# Constants\nANIMALS = ['cat', 'dog', 'bird', 'fish', 'snake']\nACTIONS = ['eats', 'sleeps', 'runs', 'jumps', 'swims']\n\ndef generate_animal_actions():\n    \"\"\"\n    Generate a series of sentences of animals performing actions, \n    then split these sentences by a randomly chosen action and count \n    the frequency of each first part before the split.\n\n    Returns:\n    Series: A pandas Series with the counts of each first part before the split.\n    \n    Requirements:\n    - pandas\n    - re\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> generate_animal_actions()\n    >>> plt.show()\n    \"\"\"\n    # Generate sentences\n    sentences = [f'The {choice(ANIMALS)} {choice(ACTIONS)} in the garden.' for _ in range(1000)]\n    # Choose a random action to split by\n    split_action = choice(ACTIONS)\n    # Split and count\n    split_counts = pd.Series([re.split(f' {split_action} ', sentence)[0] for sentence in sentences]).value_counts()\n    \n    # Plot\n    split_counts.plot(kind='bar')\n    plt.title(f'Split by \"{split_action}\"')\n    plt.ylabel('Count')\n    plt.show()\n\n    return split_counts\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, re, itertools\n```python\nimport numpy as np\nimport re\nimport itertools\n\n# Constants\nNUMBERS = np.arange(1, 11)\nOPERATORS = ['+', '-', '*', '/']\n\ndef generate_math_expressions():\n    \"\"\"\n    Generate all possible math expressions using numbers from 1 to 10 \n    and operators '+', '-', '*', '/', then split these expressions by \n    a randomly chosen operator and return the unique first parts before the split.\n\n    Returns:\n    numpy.ndarray: A numpy array with the unique first parts before the split.\n    \n    Requirements:\n    - numpy\n    - re\n    - itertools\n    \n    Example:\n    >>> generate_math_expressions()\n    \"\"\"\n    # Generate expressions\n    expressions = [' '.join(map(str, exp)) for exp in itertools.product(NUMBERS, OPERATORS, NUMBERS)]\n    # Choose a random operator to split by\n    split_operator = np.random.choice(OPERATORS)\n    # Split and find unique\n    unique_splits = np.unique([re.split(f' \\\\{split_operator} ', expression)[0] for expression in expressions])\n\n    return unique_splits\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, nltk, re, matplotlib\n```python\nimport pandas as pd\nimport nltk\nfrom nltk.corpus import brown\nimport re\nimport matplotlib.pyplot as plt\n\nnltk.download('brown')\n\n# Constants\nTARGET_WORDS = ['the', 'a', 'an']\n\ndef analyze_brown_corpus():\n    \"\"\"\n    Analyze the Brown Corpus to find all sentences containing \n    the words 'the', 'a', 'an', split these sentences by these words \n    and count the frequency of each first part before the split.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the counts of each first part before the split.\n    \n    Requirements:\n    - pandas\n    - nltk\n    - re\n    - matplotlib.pyplot\n    \n    Example:\n    >>> analyze_brown_corpus()\n    >>> plt.show()\n    \"\"\"\n    # Get sentences from Brown Corpus\n    sentences = [' '.join(sentence) for sentence in brown.sents()]\n    # Filter sentences containing target words\n    target_sentences = [sentence for sentence in sentences if any(word in sentence for word in TARGET_WORDS)]\n    # Split and count\n    split_counts = pd.Series([re.split('|'.join(TARGET_WORDS), sentence)[0] for sentence in target_sentences]).value_counts()\n    \n    # Plot\n    split_counts[:10].plot(kind='bar')\n    plt.title('Top 10 First Parts Before the Split')\n    plt.ylabel('Count')\n    plt.show()\n\n    return split_counts\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, re, itertools\n```python\nimport numpy as np\nimport re\nimport itertools\n\n# Constants\nLETTERS = ['a', 'b', 'c', 'd', 'e']\nSPECIAL_SYMBOLS = ['@', '#', '$', '%', '&']\n\ndef generate_string_patterns():\n    \"\"\"\n    Generate all possible string patterns using letters from 'a' to 'e' \n    and special symbols '@', '#', '$', '%', '&', then split these patterns by \n    a randomly chosen symbol and return the unique first parts before the split.\n\n    Returns:\n    numpy.ndarray: A numpy array with the unique first parts before the split.\n    \n    Requirements:\n    - numpy\n    - re\n    - itertools\n    \n    Example:\n    >>> generate_string_patterns()\n    \"\"\"\n    # Generate patterns\n    patterns = [''.join(p) for p in itertools.product(LETTERS, SPECIAL_SYMBOLS, LETTERS)]\n    # Choose a random symbol to split by\n    split_symbol = np.random.choice(SPECIAL_SYMBOLS)\n    # Split and find unique\n    unique_splits = np.unique([re.split(f'\\\\{split_symbol}', pattern)[0] for pattern in patterns])\n\n    return unique_splits\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, nltk, sklearn, re, matplotlib\n```python\nimport pandas as pd\nimport nltk\nfrom nltk.corpus import abc\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport re\nimport matplotlib.pyplot as plt\n\nnltk.download('abc')\n\n# Constants\nTARGET_WORDS = ['the', 'a', 'an']\n\ndef analyze_abc_corpus():\n    \"\"\"\n    Analyze the ABC Corpus to find all sentences containing \n    the words 'the', 'a', 'an', split these sentences by these words \n    and count the frequency of each first part before the split.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the counts of each first part before the split.\n    \n    Requirements:\n    - pandas\n    - nltk\n    - sklearn\n    - re\n    - matplotlib.pyplot\n    \n    Example:\n    >>> analyze_abc_corpus()\n    >>> plt.show()\n    \"\"\"\n    # Get sentences from ABC Corpus\n    sentences = [' '.join(sentence) for sentence in abc.sents()]\n    # Filter sentences containing target words\n    target_sentences = [sentence for sentence in sentences if any(word in sentence for word in TARGET_WORDS)]\n    # Split and count\n    split_counts = pd.Series([re.split('|'.join(TARGET_WORDS), sentence)[0] for sentence in target_sentences]).value_counts()\n    \n    # Plot\n    split_counts[:10].plot(kind='bar')\n    plt.title('Top 10 First Parts Before the Split')\n    plt.ylabel('Count')\n    plt.show()\n\n    return split_counts\n```"}
{"seed": "{\"task_id\": 273192, \"prompt\": \"def f_273192(directory):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"if (not os.path.exists(directory)):\\n\\t    os.makedirs(directory)\", \"test_start\": \"\\nimport os \\n\\ndef check(candidate):\", \"test\": [\"\\n    candidate(\\\"hello\\\")\\n    assert os.path.exists(\\\"hello\\\")\\n\", \"\\n    candidate(\\\"_some_dir\\\")\\n    assert os.path.exists(\\\"_some_dir\\\")\\n\"], \"entry_point\": \"f_273192\", \"intent\": \"check if directory `directory ` exists and create it if necessary\", \"library\": [\"os\"]}", "generation": "Scenario 3:\nos, shutil, random, string\n```python\nimport os\nimport shutil\nimport random\nimport string\n\n# Constants\nDIRECTORY_NAME_LENGTH = 10\n\ndef create_and_populate_directory(parent_dir):\n    \"\"\"\n    Generate a random directory inside a parent directory, create it if doesn't exist, \n    then create a series of text files in it.\n\n    Parameters:\n    parent_dir (str): The path to the parent directory.\n\n    Returns:\n    str: The directory path of the created directory.\n\n    Requirements:\n    - os\n    - shutil\n    - random\n    - string\n\n    Example:\n    >>> create_and_populate_directory('/tmp')\n    \"\"\"\n    # Generate a random directory name\n    dir_name = ''.join(random.choices(string.ascii_letters + string.digits, k=DIRECTORY_NAME_LENGTH))\n    directory = os.path.join(parent_dir, dir_name)\n\n    # Create directory if it doesn't exist\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    # Create a series of text files\n    for i in range(10):\n        with open(os.path.join(directory, f'file{i}.txt'), 'w') as f:\n            f.write(f'This is file {i} in the directory {dir_name}.')\n\n    return directory\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, glob, shutil, re\n```python\nimport os\nimport glob\nimport shutil\nimport re\n\n# Pattern for matching Python files\nPYTHON_FILES_PATTERN = r'*.py'\n\ndef move_python_files(src_dir, dst_dir):\n    \"\"\"\n    Move all Python files from one directory to another.\n\n    Parameters:\n    src_dir (str): The source directory.\n    dst_dir (str): The destination directory.\n\n    Returns:\n    list: List of moved files.\n\n    Requirements:\n    - os\n    - glob\n    - shutil\n    - re\n\n    Example:\n    >>> move_python_files('/path/to/src', '/path/to/dst')\n    \"\"\"\n    moved_files = []\n\n    # Check if destination directory exists, create if not\n    if not os.path.exists(dst_dir):\n        os.makedirs(dst_dir)\n\n    # Loop through all files in source directory\n    for file_path in glob.glob(os.path.join(src_dir, PYTHON_FILES_PATTERN)):\n\n        # Move file to destination directory\n        shutil.move(file_path, dst_dir)\n        moved_files.append(os.path.join(dst_dir, os.path.basename(file_path)))\n\n    return moved_files\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, csv, json, datetime\n```python\nimport os\nimport csv\nimport json\nfrom datetime import datetime\n\n# Constants\nCSV_FILE_NAME = 'data.csv'\nJSON_FILE_NAME = 'data.json'\n\ndef csv_to_json(directory):\n    \"\"\"\n    Convert a CSV file to a JSON file in a given directory.\n\n    Parameters:\n    directory (str): The directory containing the CSV file.\n\n    Returns:\n    str: The path to the created JSON file.\n\n    Requirements:\n    - os\n    - csv\n    - json\n    - datetime\n\n    Example:\n    >>> csv_to_json('/path/to/directory')\n    \"\"\"\n    csv_file = os.path.join(directory, CSV_FILE_NAME)\n    json_file = os.path.join(directory, JSON_FILE_NAME)\n\n    # Read the CSV and add the data to a dictionary\n    data = {}\n    with open(csv_file, 'r') as csvf:\n        csv_reader = csv.DictReader(csvf)\n        for row in csv_reader:\n            key = datetime.now().strftime(\"%Y%m%d%H%M%S%f\")\n            data[key] = row\n\n    # Write the data to a JSON file\n    with open(json_file, 'w') as jsonf:\n        jsonf.write(json.dumps(data, indent=4))\n\n    return json_file\n```\n`GPT_ODEX_BREAK`\n\nScenario 6:\nos, time, threading, logging\n```python\nimport os\nimport time\nimport threading\nimport logging\n\n# Constants\nDIRECTORY_NAME = 'log_directory'\nLOG_FILE_NAME = 'thread.log'\n\ndef monitor_directory(directory):\n    \"\"\"\n    Monitor a directory and log each time a new file is created in it.\n\n    Parameters:\n    directory (str): The directory to monitor.\n\n    Requirements:\n    - os\n    - time\n    - threading\n    - logging\n\n    Example:\n    >>> monitor_directory('/path/to/directory')\n    \"\"\"\n    # Configure logging\n    log_dir = os.path.join(directory, DIRECTORY_NAME)\n    if not os.path.exists(log_dir):\n        os.makedirs(log_dir)\n    logging.basicConfig(filename=os.path.join(log_dir, LOG_FILE_NAME), level=logging.INFO)\n\n    def monitor():\n        files = set()\n        while True:\n            current_files = set(os.listdir(directory))\n            new_files = current_files - files\n            if new_files:\n                for new_file in new_files:\n                    logging.info(f'File created: {new_file}')\n            files = current_files\n            time.sleep(1)\n\n    # Create thread\n    monitor_thread = threading.Thread(target=monitor)\n    monitor_thread.start()\n```\n`GPT_ODEX_BREAK`\n\nScenario 7:\nos, zipfile, glob, hashlib\n```python\nimport os\nimport zipfile\nimport glob\nimport hashlib\n\n# Constants\nZIP_FILE_NAME = 'archive.zip'\n\ndef compress_and_hash_files(directory):\n    \"\"\"\n    Compress all files in a directory into a ZIP file and calculate the MD5 hash of the ZIP file.\n\n    Parameters:\n    directory (str): The directory containing the files.\n\n    Returns:\n    str: The MD5 hash of the created ZIP file.\n\n    Requirements:\n    - os\n    - zipfile\n    - glob\n    - hashlib\n\n    Example:\n    >>> compress_and_hash_files('/path/to/directory')\n    \"\"\"\n    zip_file_path = os.path.join(directory, ZIP_FILE_NAME)\n\n    # Compress all files in directory\n    with zipfile.ZipFile(zip_file_path, 'w') as zipf:\n        for file_path in glob.glob(os.path.join(directory, '*.*')):\n            zipf.write(file_path, os.path.basename(file_path))\n\n    # Calculate MD5 hash of the ZIP file\n    hash_md5 = hashlib.md5()\n    with open(zip_file_path, 'rb') as f:\n        for chunk in iter(lambda: f.read(4096), b''):\n            hash_md5.update(chunk)\n\n    return hash_md5.hexdigest()\n```"}
{"seed": "{\"task_id\": 273192, \"prompt\": \"def f_273192(path):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"try:\\n\\t    os.makedirs(path)\\n\\texcept OSError:\\n\\t    if (not os.path.isdir(path)):\\n\\t        raise\", \"test_start\": \"\\nimport os \\n\\ndef check(candidate):\", \"test\": [\"\\n    candidate(\\\"hello\\\")\\n    assert os.path.exists(\\\"hello\\\")\\n\", \"\\n    candidate(\\\"_some_dir\\\")\\n    assert os.path.exists(\\\"_some_dir\\\")\\n\"], \"entry_point\": \"f_273192\", \"intent\": \"check if a directory `path` exists and create it if necessary\", \"library\": [\"os\"]}", "generation": "Scenario 1:\nos, csv, urllib, shutil\n```python\nimport os\nimport csv\nimport urllib.request\nimport shutil\n\n# Constants\nURL = 'https://example.com/data.csv'\nLOCAL_DIR = 'data'\nLOCAL_FILE = 'data.csv'\nLOCAL_PATH = os.path.join(LOCAL_DIR, LOCAL_FILE)\n\ndef download_and_read_csv():\n    \"\"\"\n    Download a CSV file from a URL, save it to a local directory, \n    and read its contents into a list of dictionaries. \n\n    Returns:\n    list: A list of dictionaries representing the CSV data.\n\n    Requirements:\n    - os\n    - csv\n    - urllib.request\n    - shutil\n\n    Example:\n    >>> data = download_and_read_csv()\n    >>> print(data)\n    \"\"\"\n    if not os.path.exists(LOCAL_DIR):\n        os.makedirs(LOCAL_DIR)\n\n    with urllib.request.urlopen(URL) as response, open(LOCAL_PATH, 'wb') as out_file:\n        shutil.copyfileobj(response, out_file)\n\n    data = []\n    with open(LOCAL_PATH, 'r') as file:\n        reader = csv.DictReader(file)\n        for row in reader:\n            data.append(row)\n\n    return data\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, glob, PIL, numpy, matplotlib\n```python\nimport os\nimport glob\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nIMAGES_DIR = 'images'\nTHUMBNAIL_SIZE = (128, 128)\n\ndef generate_thumbnails_and_histograms():\n    \"\"\"\n    Generate thumbnails for all JPEG images in a directory. \n    Also, compute and plot a histogram of pixel intensities in the grayscale image.\n\n    Requirements:\n    - os\n    - glob\n    - PIL\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> generate_thumbnails_and_histograms()\n    \"\"\"\n    if not os.path.exists(IMAGES_DIR):\n        os.makedirs(IMAGES_DIR)\n\n    for file in glob.glob(os.path.join(IMAGES_DIR, '*.jpg')):\n        with Image.open(file) as img:\n            img.thumbnail(THUMBNAIL_SIZE)\n            img.save(file.replace('.jpg', '_thumbnail.jpg'))\n\n            grayscale_img = img.convert('L')\n            hist = np.histogram(np.array(grayscale_img).flatten(), bins=256, range=[0,256])[0]\n            plt.figure()\n            plt.title('Histogram of pixel intensities')\n            plt.bar(range(256), hist)\n            plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, shutil, zipfile\n```python\nimport os\nimport shutil\nfrom zipfile import ZipFile\n\n# Constants\nSOURCE_DIR = 'source'\nDEST_DIR = 'destination'\nZIP_FILE_NAME = 'archive.zip'\n\ndef move_files_and_create_zip():\n    \"\"\"\n    Move all files from one directory to another and create a ZIP archive of the \n    destination directory.\n\n    Requirements:\n    - os\n    - shutil\n    - zipfile\n\n    Example:\n    >>> move_files_and_create_zip()\n    \"\"\"\n    if not os.path.exists(DEST_DIR):\n        os.makedirs(DEST_DIR)\n\n    for file_name in os.listdir(SOURCE_DIR):\n        shutil.move(os.path.join(SOURCE_DIR, file_name), DEST_DIR)\n\n    with ZipFile(ZIP_FILE_NAME, 'w') as zipf:\n        for root, dirs, files in os.walk(DEST_DIR):\n            for file in files:\n                zipf.write(os.path.join(root, file))\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, pandas, sqlite3\n```python\nimport os\nimport pandas as pd\nimport sqlite3\n\n# Constants\nDB_NAME = 'database.db'\nTABLE_NAME = 'data'\nCSV_FILE = 'data.csv'\n\ndef read_csv_to_sqlite():\n    \"\"\"\n    Read a CSV file into a pandas DataFrame and write the DataFrame to a SQLite database.\n\n    Requirements:\n    - os\n    - pandas\n    - sqlite3\n\n    Example:\n    >>> read_csv_to_sqlite()\n    \"\"\"\n    df = pd.read_csv(CSV_FILE)\n    conn = sqlite3.connect(DB_NAME)\n    df.to_sql(TABLE_NAME, conn, if_exists='replace', index=False)\n    conn.close()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, subprocess, time\n```python\nimport os\nimport subprocess\nimport time\n\n# Constants\nCOMMAND = 'ls -l'\nLOG_FILE = 'log.txt'\n\ndef execute_command_and_log_output():\n    \"\"\"\n    Execute a command, wait for it to finish, and log its output to a file.\n\n    Requirements:\n    - os\n    - subprocess\n    - time\n\n    Example:\n    >>> execute_command_and_log_output()\n    \"\"\"\n    with open(LOG_FILE, 'w') as file:\n        process = subprocess.Popen(COMMAND, shell=True, stdout=file, stderr=subprocess.STDOUT)\n        while process.poll() is None:\n            time.sleep(0.5)\n```\n"}
{"seed": "{\"task_id\": 273192, \"prompt\": \"def f_273192(path):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"try:\\n\\t    os.makedirs(path)\\n\\texcept OSError as exception:\\n\\t    if (exception.errno != errno.EEXIST):\\n\\t        raise\", \"test_start\": \"\\nimport os \\n\\ndef check(candidate):\", \"test\": [\"\\n    candidate(\\\"hello\\\")\\n    assert os.path.exists(\\\"hello\\\")\\n\", \"\\n    candidate(\\\"_some_dir\\\")\\n    assert os.path.exists(\\\"_some_dir\\\")\\n\"], \"entry_point\": \"f_273192\", \"intent\": \"check if a directory `path` exists and create it if necessary\", \"library\": [\"os\"]}", "generation": "Scenario 1:\nos, shutil, glob, pathlib\n```python\nimport os\nimport shutil\nimport glob\nfrom pathlib import Path\n\nROOT_DIR = 'my_data'\n\ndef prepare_dataset_directory(dataset_name, overwrite=False):\n    \"\"\"\n    Prepare a directory for a dataset. If the directory exists and overwrite is True, \n    delete the existing directory first. Then create the new directory and a subdirectory \n    called 'raw' inside it.\n\n    Parameters:\n    dataset_name (str): The name of the dataset.\n    overwrite (bool): Whether to overwrite the existing directory if it exists.\n\n    Requirements:\n    - os\n    - shutil\n    - glob\n    - pathlib.Path\n\n    Example:\n    >>> prepare_dataset_directory('mnist', overwrite=True)\n    >>> assert Path('my_data/mnist/raw').exists()\n    \"\"\"\n    dataset_dir = Path(ROOT_DIR) / dataset_name\n    raw_dir = dataset_dir / 'raw'\n    \n    if dataset_dir.exists():\n        if overwrite:\n            shutil.rmtree(dataset_dir)\n        else:\n            raise FileExistsError(f\"Directory {dataset_dir} already exists.\")\n            \n    os.makedirs(raw_dir)\n    return str(raw_dir)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, zipfile, glob\n```python\nimport os\nimport zipfile\nfrom glob import glob\n\ndef extract_zip_files(dir_path):\n    \"\"\"\n    Extract all zip files in a given directory.\n\n    Parameters:\n    dir_path (str): The path to the directory.\n\n    Requirements:\n    - os\n    - zipfile\n    - glob\n\n    Example:\n    >>> extract_zip_files('data_dir')\n    \"\"\"\n    os.chdir(dir_path) \n    zip_files = glob('*.zip')\n    for zip_file in zip_files:\n        with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n            zip_ref.extractall()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, shutil, fnmatch\n```python\nimport os\nimport shutil\nfrom fnmatch import fnmatch\n\ndef move_files(source_dir, target_dir, file_pattern):\n    \"\"\"\n    Move all files matching a pattern from source directory to target directory.\n\n    Parameters:\n    source_dir (str): The source directory.\n    target_dir (str): The target directory.\n    file_pattern (str): The file pattern.\n\n    Requirements:\n    - os\n    - shutil\n    - fnmatch\n\n    Example:\n    >>> move_files('source_dir', 'target_dir', '*.txt')\n    \"\"\"\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n        \n    for file_name in os.listdir(source_dir):\n        if fnmatch(file_name, file_pattern):\n            shutil.move(os.path.join(source_dir, file_name), target_dir)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, shutil, hashlib\n```python\nimport os\nimport shutil\nfrom hashlib import sha256\n\ndef archive_and_hash_directory(dir_path, archive_path):\n    \"\"\"\n    Archive a directory and compute the SHA-256 hash of the archive file.\n\n    Parameters:\n    dir_path (str): The path to the directory.\n    archive_path (str): The path to the archive file.\n\n    Returns:\n    str: The SHA-256 hash of the archive file.\n\n    Requirements:\n    - os\n    - shutil\n    - hashlib.sha256\n\n    Example:\n    >>> hash_ = archive_and_hash_directory('source_dir', 'archive.zip')\n    >>> print(hash_)\n    \"\"\"\n    shutil.make_archive(archive_path, 'zip', dir_path)\n    hash_ = sha256()\n\n    with open(archive_path, 'rb') as f:\n        for block in iter(lambda: f.read(4096), b''):\n            hash_.update(block)\n\n    return hash_.hexdigest()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, pathlib, csv\n```python\nimport os\nfrom pathlib import Path\nimport csv\n\ndef generate_directory_csv(dir_path, csv_path):\n    \"\"\"\n    Generate a CSV file with information about all files in a directory \n    including file name, size, and last modification time.\n\n    Parameters:\n    dir_path (str): The path to the directory.\n    csv_path (str): The path to the CSV file.\n\n    Requirements:\n    - os\n    - pathlib.Path\n    - csv\n\n    Example:\n    >>> generate_directory_csv('source_dir', 'file_info.csv')\n    \"\"\"\n    with open(csv_path, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(['File Name', 'Size', 'Last Modification Time'])\n\n        for file in Path(dir_path).iterdir():\n            if file.is_file():\n                writer.writerow([file.name, file.stat().st_size, file.stat().st_mtime])\n```"}
{"seed": "{\"task_id\": 18785032, \"prompt\": \"def f_18785032(text):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"re.sub('\\\\\\\\bH3\\\\\\\\b', 'H1', text)\", \"test_start\": \"\\nimport re\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(\\\"hello world and H3\\\") == \\\"hello world and H1\\\"\\n\", \"\\n    assert candidate(\\\"hello world and H1\\\") == \\\"hello world and H1\\\"\\n\", \"\\n    assert candidate(\\\"hello world!\\\") == \\\"hello world!\\\"\\n\"], \"entry_point\": \"f_18785032\", \"intent\": \"Replace a separate word 'H3' by 'H1' in a string 'text'\", \"library\": [\"re\"]}", "generation": "Scenario 1:\nre, pandas, nltk, matplotlib.pyplot\n```python\nimport re\nimport pandas as pd\nfrom nltk.corpus import stopwords\nimport matplotlib.pyplot as plt\n\n# Constants\nSTOPWORDS = set(stopwords.words('english'))\n\ndef analyze_text(text):\n    \"\"\"\n    Analyze a given text: remove stopwords, replace a separate word 'H3' by 'H1', \n    and plot a bar chart of word frequency.\n    \n    Parameters:\n    text (str): The input text.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with word counts.\n    \n    Requirements:\n    - re\n    - pandas\n    - nltk.corpus.stopwords\n    - matplotlib.pyplot\n    \n    Example:\n    >>> text = \"H3 is H3 and H3 is not H1 but H1 is H1.\"\n    >>> result = analyze_text(text)\n    >>> print(result)\n    >>> result.plot(kind='bar')\n    \"\"\"\n    text = re.sub('\\\\bH3\\\\b', 'H1', text)\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    words = [w for w in words if w not in STOPWORDS]\n    \n    word_counts = pd.Series(words).value_counts()\n    word_counts_df = pd.DataFrame(word_counts).reset_index()\n    word_counts_df.columns = ['Word', 'Frequency']\n    \n    return word_counts_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nre, glob, os, hashlib\n```python\nimport re\nimport glob\nimport os\nimport hashlib\n\n# Constants\nPATTERN = '\\\\bH3\\\\b'\n\ndef replace_in_files(directory):\n    \"\"\"\n    Replace a separate word 'H3' by 'H1' in all text files in a given directory \n    and calculate the md5 hash of each file.\n    \n    Parameters:\n    directory (str): The directory with text files.\n    \n    Returns:\n    dict: A dictionary with file names as keys and their md5 hashes as values.\n    \n    Requirements:\n    - re\n    - glob\n    - os\n    - hashlib\n    \n    Example:\n    >>> replace_in_files('/path/to/directory')\n    \"\"\"\n    file_hashes = {}\n    \n    for filename in glob.glob(os.path.join(directory, '*.txt')):\n        with open(filename, 'r+') as file:\n            text = file.read()\n            text = re.sub(PATTERN, 'H1', text)\n            file.seek(0)\n            file.write(text)\n            file.truncate()\n        \n        with open(filename, 'rb') as file:\n            file_hash = hashlib.md5()\n            while chunk := file.read(8192):\n                file_hash.update(chunk)\n            file_hashes[filename] = file_hash.hexdigest()\n    \n    return file_hashes\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nre, json, numpy, matplotlib.pyplot\n```python\nimport re\nimport json\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nPATTERN = '\\\\bH3\\\\b'\n\ndef analyze_json_data(json_data):\n    \"\"\"\n    Parse JSON data, replace a separate word 'H3' by 'H1' in all string values, \n    count the occurrences of each word, and plot a histogram of word counts.\n    \n    Parameters:\n    json_data (str): The JSON data as a string.\n    \n    Returns:\n    dict: A dictionary with words as keys and their counts as values.\n    \n    Requirements:\n    - re\n    - json\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> json_data = '{\"text\": \"H3 is H3 and H3 is not H1 but H1 is H1.\"}'\n    >>> word_counts = analyze_json_data(json_data)\n    >>> print(word_counts)\n    >>> plt.hist(list(word_counts.values()), bins=np.arange(0.5, max(word_counts.values())+1.5))\n    \"\"\"\n    data = json.loads(json_data)\n    word_counts = {}\n    \n    for value in data.values():\n        if isinstance(value, str):\n            value = re.sub(PATTERN, 'H1', value)\n            words = re.findall(r'\\b\\w+\\b', value.lower())\n            for word in words:\n                word_counts[word] = word_counts.get(word, 0) + 1\n    \n    return word_counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nre, collections, matplotlib.pyplot, nltk\n```python\nimport re\nimport collections\nimport matplotlib.pyplot as plt\nfrom nltk.tokenize import word_tokenize\n\n# Constants\nSTOPWORDS = set(stopwords.words('english'))\n\ndef plot_word_frequencies(text):\n    \"\"\"\n    Tokenize a text, remove stopwords, replace 'H3' by 'H1', \n    count the word frequencies, and plot a bar chart of the top 10.\n    \n    Parameters:\n    text (str): The input text.\n    \n    Returns:\n    List[Tuple[str, int]]: A list of tuples with the top 10 words and their frequencies.\n    \n    Requirements:\n    - re\n    - collections\n    - matplotlib.pyplot\n    - nltk.tokenize.word_tokenize\n    \n    Example:\n    >>> text = \"H3 is H3 and H3 is not H1 but H1 is H1.\"\n    >>> top_words = plot_word_frequencies(text)\n    >>> print(top_words)\n    >>> plt.bar([w[0] for w in top_words], [w[1] for w in top_words])\n    \"\"\"\n    text = re.sub('\\\\bH3\\\\b', 'H1', text)\n    words = word_tokenize(text.lower())\n    words = [w for w in words if w.isalpha() and w not in STOPWORDS]\n    word_counts = collections.Counter(words)\n    top_words = word_counts.most_common(10)\n\n    return top_words\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nre, csv, pandas, matplotlib.pyplot\n```python\nimport re\nimport csv\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nPATTERN = '\\\\bH3\\\\b'\n\ndef process_csv_file(filename):\n    \"\"\"\n    Read a CSV file, replace 'H3' by 'H1' in the 'text' column, \n    count the occurrences of 'H1' in each row, and plot a bar chart of the counts.\n    \n    Parameters:\n    filename (str): The CSV file name.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the processed data.\n    \n    Requirements:\n    - re\n    - csv\n    - pandas\n    - matplotlib.pyplot\n    \n    Example:\n    >>> process_csv_file('file.csv')\n    \"\"\"\n    data = []\n    \n    with open(filename, 'r') as file:\n        reader = csv.reader(file)\n        headers = next(reader)\n        for row in reader:\n            row = [re.sub(PATTERN, 'H1', cell) if cell.startswith('H3') else cell for cell in row]\n            data.append(row)\n    \n    df = pd.DataFrame(data, columns=headers)\n    df['H1 Count'] = df['text'].apply(lambda x: x.count('H1'))\n    df['H1 Count'].value_counts().sort_index().plot(kind='bar')\n\n    return df\n```\n"}
{"seed": "{\"task_id\": 1450897, \"prompt\": \"def f_1450897():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"re.sub('\\\\\\\\D', '', 'aas30dsa20')\", \"test_start\": \"\\nimport re\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate() == \\\"3020\\\"\\n\"], \"entry_point\": \"f_1450897\", \"intent\": \"substitute ASCII letters in string 'aas30dsa20' with empty string ''\", \"library\": [\"re\"]}", "generation": "Scenario 1:\npandas, re, random, matplotlib\n```python\nimport pandas as pd\nimport re\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nSTRINGS = ['aas30dsa20', 'sda90saddsa70', '90sddsad', 'dssaad', '333sda']\n\ndef count_numbers_in_strings():\n    \"\"\"\n    Count the total number of numerical characters in a list of strings and plot the count.\n\n    Returns:\n    int: The total number of numerical characters.\n\n    Requirements:\n    - pandas\n    - re\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> total_numerics = count_numbers_in_strings()\n    >>> print(total_numerics)\n    \"\"\"\n    total_numerics = 0\n    numeric_counts = []\n\n    for string in STRINGS:\n        numerics = re.sub('\\D', '', string)\n        numeric_counts.append(len(numerics))\n        total_numerics += len(numerics)\n\n    numeric_counts_series = pd.Series(numeric_counts, index=STRINGS)\n    numeric_counts_series.plot(kind='bar')\n\n    return total_numerics\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nre, numpy, string, scipy.stats\n```python\nimport re\nimport numpy as np\nimport string\nfrom scipy import stats\n\n# Constants\nSTRINGS = ['aas30dsa20', 'sda90saddsa70', '90sddsad', 'dssaad', '333sda']\n\ndef calculate_mode_of_alphabets():\n    \"\"\"\n    Find the most frequently occurring alphabetical character in a list of strings.\n\n    Returns:\n    str: The most frequently occurring alphabetical character.\n\n    Requirements:\n    - re\n    - numpy\n    - string\n    - scipy.stats\n\n    Example:\n    >>> mode_alphabet = calculate_mode_of_alphabets()\n    >>> print(mode_alphabet)\n    \"\"\"\n    alphabets = []\n    for string in STRINGS:\n        alphabets.extend(re.findall('[a-zA-Z]', string))\n\n    mode_alphabet = stats.mode(alphabets).mode[0]\n\n    return mode_alphabet\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nre, matplotlib, collections, random\n```python\nimport re\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom random import choice\n\n# Constants\nSTRINGS = ['aas30dsa20', 'sda90saddsa70', '90sddsad', 'dssaad', '333sda']\n\ndef plot_frequency_of_random_number():\n    \"\"\"\n    Select a random numerical character from a list of strings and plot its frequency.\n\n    Returns:\n    str: The selected numerical character.\n\n    Requirements:\n    - re\n    - matplotlib.pyplot\n    - collections\n    - random\n\n    Example:\n    >>> number = plot_frequency_of_random_number()\n    >>> print(number)\n    \"\"\"\n    numbers = []\n    for string in STRINGS:\n        numbers.extend(re.findall('\\d', string))\n\n    selected_number = choice(numbers)\n    counter = Counter(numbers)\n\n    plt.bar(counter.keys(), counter.values())\n    plt.xlabel('Numbers')\n    plt.ylabel('Frequency')\n    plt.title(f'Frequency of number {selected_number}')\n\n    return selected_number\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nre, datetime, pandas, random\n```python\nimport re\nfrom datetime import datetime\nimport pandas as pd\nfrom random import choice\n\n# Constants\nDATE_STRINGS = ['Today is 2022-01-01', 'Yesterday was 2022-05-30', 'Tomorrow will be 2022-06-01', 'The day after tomorrow will be 2022-06-02']\n\ndef find_random_date():\n    \"\"\"\n    Find a random date in a list of strings and return the day of the week for that date.\n\n    Returns:\n    str: The day of the week for the random date.\n\n    Requirements:\n    - re\n    - datetime\n    - pandas\n    - random\n\n    Example:\n    >>> day_of_week = find_random_date()\n    >>> print(day_of_week)\n    \"\"\"\n    dates = []\n    for date_string in DATE_STRINGS:\n        dates.extend(re.findall('\\d{4}-\\d{2}-\\d{2}', date_string))\n\n    selected_date = choice(dates)\n    selected_date = datetime.strptime(selected_date, '%Y-%m-%d')\n    day_of_week = selected_date.strftime('%A')\n\n    return day_of_week\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nre, numpy, math, scipy.optimize\n```python\nimport re\nimport numpy as np\nimport math\nfrom scipy.optimize import minimize\n\n# Constants\nSTRINGS = ['aas30dsa20', 'sda90saddsa70', '90sddsad', 'dssaad', '333sda']\n\ndef find_minimum_square_root_sum():\n    \"\"\"\n    Find the minimum sum of square roots of all numerical characters in a list of strings.\n\n    Returns:\n    float: The minimum sum of square roots.\n\n    Requirements:\n    - re\n    - numpy\n    - math\n    - scipy.optimize\n\n    Example:\n    >>> min_sum = find_minimum_square_root_sum()\n    >>> print(min_sum)\n    \"\"\"\n    numbers = []\n    for string in STRINGS:\n        numbers.extend(re.findall('\\d', string))\n\n    numbers = np.array(numbers, dtype=float)\n\n    def objective(x):\n        return np.sum(np.sqrt(numbers + x))\n\n    result = minimize(objective, 0)\n\n    min_sum = result.fun\n\n    return min_sum\n```\n"}
{"seed": "{\"task_id\": 1450897, \"prompt\": \"def f_1450897():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"\\\"\\\"\\\"\\\"\\\"\\\".join([x for x in 'aas30dsa20' if x.isdigit()])\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate() == \\\"3020\\\"\\n\"], \"entry_point\": \"f_1450897\", \"intent\": \"get digits only from a string `aas30dsa20` using lambda function\", \"library\": []}", "generation": "Scenario 1:\nre, string, random, itertools\n```python\nimport re\nimport string\nimport random\nimport itertools\n\n# Constants\nSAMPLE_STRINGS = ['aas30dsa20', 'bb45rev32', 'csa67gfd89']\n\ndef generate_random_string_patterns():\n    \"\"\"\n    Generate random string patterns from a list of sample strings and \n    return the digits and alphabets separately from each string pattern.\n    \n    Returns:\n    dict: A dictionary with string patterns as keys and a tuple of alphabets and digits as values.\n\n    Requirements:\n    - re\n    - string\n    - random\n    - itertools\n\n    Example:\n    >>> patterns = generate_random_string_patterns()\n    >>> print(patterns)\n    \"\"\"\n    patterns = {}\n\n    for _ in range(10):\n        pattern = ''.join(random.choice(SAMPLE_STRINGS) for _ in range(3))\n        digits = \"\".join(re.findall(r'\\d', pattern))\n        alphabets = \"\".join(re.findall(r'[a-zA-Z]', pattern))\n        patterns[pattern] = (alphabets, digits)\n\n    return patterns\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nre, pandas, matplotlib\n```python\nimport re\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nTEXT = \"\"\"\n        I have 30 apples and 20 oranges. \n        My friend gave me another 45 apples and 60 oranges. \n        Now, I have a total of 75 apples and 80 oranges.\n        \"\"\"\n\ndef analyze_text(text):\n    \"\"\"\n    Analyze a text string to find the number of apples and oranges mentioned in it. \n    Visualize the counts using a bar graph.\n\n    Parameters:\n    text (str): The text string to analyze.\n   \n    Requirements:\n    - re\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> analyze_text(TEXT)\n    \"\"\"\n    apple_count = sum(int(num) for num in re.findall(r'(\\d+) apples', text))\n    orange_count = sum(int(num) for num in re.findall(r'(\\d+) oranges', text))\n\n    data = {'Apples': apple_count, 'Oranges': orange_count}\n    df = pd.DataFrame(list(data.items()), columns=['Fruit', 'Count'])\n    df.set_index('Fruit', inplace=True)\n\n    df.plot(kind='bar')\n    plt.show()\n\n    return apple_count, orange_count\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nre, string, random, collections\n```python\nimport re\nimport string\nimport random\nimport collections\n\n# Constants\nALPHABETS = string.ascii_lowercase\nNUMBERS = string.digits\n\ndef generate_random_str_and_count_digits():\n    \"\"\"\n    Generate a random string with digits and alphabets. \n    Return the count of each digit in the string.\n\n    Returns:\n    str, collections.Counter: The random string and a Counter object with digit counts.\n\n    Requirements:\n    - re\n    - string\n    - random\n    - collections\n\n    Example:\n    >>> random_str, digit_counts = generate_random_str_and_count_digits()\n    >>> print(random_str)\n    >>> print(digit_counts)\n    \"\"\"\n    random_str = ''.join(random.choice(ALPHABETS + NUMBERS) for _ in range(100))\n    digit_counts = collections.Counter(re.findall(r'\\d', random_str))\n\n    return random_str, digit_counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nre, pandas, matplotlib.pyplot, numpy\n```python\nimport re\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nTEXT_DATA = \"Temperature data: 30, 32, 33, 34, 35, 36, 37, 38, 39, 40\"\n\ndef analyze_temperature_data(text_data):\n    \"\"\"\n    Analyze a text string containing temperature data. \n    Extract the temperatures, calculate their mean and standard deviation, \n    and visualize them using a line graph.\n\n    Parameters:\n    text_data (str): The text string to analyze.\n    \n    Returns:\n    float, float: The mean and standard deviation of the temperatures.\n\n    Requirements:\n    - re\n    - pandas\n    - matplotlib.pyplot\n    - numpy\n\n    Example:\n    >>> analyze_temperature_data(TEXT_DATA)\n    \"\"\"\n    temperatures = np.array([int(num) for num in re.findall(r'\\d+', text_data)])\n\n    mean_temp = np.mean(temperatures)\n    std_dev_temp = np.std(temperatures)\n\n    df = pd.DataFrame(temperatures, columns=['Temperature'])\n    df.plot(kind='line', y='Temperature')\n    plt.show()\n\n    return mean_temp, std_dev_temp\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nre, itertools, string, random\n```python\nimport re\nimport itertools\nimport string\nimport random\n\n# Constants\nSAMPLE_STRINGS = ['aas30dsa20', 'bb45rev32', 'csa67gfd89']\nCOMBINATIONS = 5\n\ndef generate_and_analyze_string_combinations():\n    \"\"\"\n    Generate 5 combinations of sample strings. \n    For each combination, return the digits and alphabets separately.\n\n    Returns:\n    dict: A dictionary with string combinations as keys and a tuple of alphabets and digits as values.\n\n    Requirements:\n    - re\n    - itertools\n    - string\n    - random\n\n    Example:\n    >>> combinations = generate_and_analyze_string_combinations()\n    >>> print(combinations)\n    \"\"\"\n    combinations = {}\n\n    for _ in range(COMBINATIONS):\n        combination = ''.join(random.choice(SAMPLE_STRINGS) for _ in range(3))\n        digits = \"\".join(re.findall(r'\\d', combination))\n        alphabets = \"\".join(re.findall(r'[a-zA-Z]', combination))\n        combinations[combination] = (alphabets, digits)\n\n    return combinations\n```"}
{"seed": "{\"task_id\": 14435268, \"prompt\": \"def f_14435268(soup):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"soup.find('name').string\", \"test_start\": \"\\nfrom bs4 import BeautifulSoup\\n\\ndef check(candidate):\", \"test\": [\"\\n    content = \\\"<contact><name>LastName</name><lastName>FirstName</lastName><phone>+90 333 12345</phone></contact>\\\"\\n    soup = BeautifulSoup(content)\\n    assert candidate(soup) == \\\"LastName\\\"\\n\", \"\\n    content = \\\"<name>hello world!</name>\\\"\\n    soup = BeautifulSoup(content)\\n    assert candidate(soup) == \\\"hello world!\\\"\\n\"], \"entry_point\": \"f_14435268\", \"intent\": \"access a tag called \\\"name\\\" in beautifulsoup `soup`\", \"library\": [\"bs4\"]}", "generation": "Scenario 1:\n```python\nfrom bs4 import BeautifulSoup\nimport requests\nimport re\nimport json\n\n# Constants\nURL = 'https://www.python.org/'\n\ndef get_news(url=URL):\n    \"\"\"\n    Fetches the Python.org website, parses it, finds all the news items, and returns them as a list of dictionaries.\n    Each dictionary contains 'title' and 'date' of a news item.\n\n    Parameters:\n    url (str): The URL of the web page. Default is 'https://www.python.org/'\n\n    Returns:\n    list: List of dictionaries where each dictionary contains 'title' and 'date' of a news item.\n\n    Requirements:\n    - bs4\n    - requests\n    - re\n    - json\n\n    Example:\n    >>> news = get_news()\n    >>> print(json.dumps(news, indent=2))\n    \"\"\"\n    response = requests.get(url)\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    news_items = soup.find_all('li', {'class': 'shrubbery'})\n\n    news_list = []\n    for news in news_items:\n        title = news.find('a').text.strip()\n        date = news.find('time').text.strip()\n        news_list.append({'title': title, 'date': date})\n\n    return news_list\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\n```python\nfrom bs4 import BeautifulSoup\nimport requests\nimport pandas as pd\n\n# Constants\nWIKI_URL = 'https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)'\n\ndef get_gdp_data(url=WIKI_URL):\n    \"\"\"\n    Scrapes a Wikipedia page that lists countries by nominal GDP and returns the data as a DataFrame.\n\n    Parameters:\n    url (str): The URL of the Wikipedia page. Default is 'https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)'\n\n    Returns:\n    DataFrame: A pandas DataFrame with the scraped data.\n\n    Requirements:\n    - bs4\n    - requests\n    - pandas\n\n    Example:\n    >>> gdp_data = get_gdp_data()\n    >>> print(gdp_data.head())\n    \"\"\"\n    response = requests.get(url)\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    table = soup.find('table', {'class': 'wikitable'})\n\n    data = []\n    headers = []\n    for row in table.find_all('tr'):\n        columns = row.find_all('td')\n\n        if len(columns) > 0:\n            data.append([col.text.strip() for col in columns])\n        else:\n            headers = [col.text.strip() for col in row.find_all('th')]\n\n    gdp_data = pd.DataFrame(data, columns=headers)\n\n    return gdp_data\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\n```python\nfrom bs4 import BeautifulSoup\nimport requests\nimport os\nimport wget\n\n# Constants\nIMAGES_URL = 'https://www.python.org/community/logos/'\n\ndef download_images(url=IMAGES_URL, save_dir='./'):\n    \"\"\"\n    Downloads all images from a web page and saves them in a specified directory.\n\n    Parameters:\n    url (str): The URL of the web page. Default is 'https://www.python.org/community/logos/'\n    save_dir (str): The directory where the images will be saved. Default is the current directory.\n\n    Requirements:\n    - bs4\n    - requests\n    - os\n    - wget\n\n    Example:\n    >>> download_images(save_dir='./images/')\n    \"\"\"\n    response = requests.get(url)\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    img_tags = soup.find_all('img')\n\n    for img in img_tags:\n        img_url = os.path.join(url, img.get('src'))\n        wget.download(img_url, out=save_dir)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\n```python\nfrom bs4 import BeautifulSoup\nimport requests\nimport csv\n\n# Constants\nURL = 'https://www.python.org/jobs/'\n\ndef fetch_job_listings(url=URL):\n    \"\"\"\n    Fetches the Python job listings from Python.org, parses them, and writes them to a CSV file.\n\n    Parameters:\n    url (str): The URL of the web page. Default is 'https://www.python.org/jobs/'\n\n    Requirements:\n    - bs4\n    - requests\n    - csv\n\n    Example:\n    >>> fetch_job_listings()\n    \"\"\"\n    response = requests.get(url)\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    job_listings = soup.find_all('li', {'class': 'job'})\n\n    with open('job_listings.csv', 'w', newline='', encoding='utf-8') as f:\n        writer = csv.writer(f)\n        writer.writerow(['Job Title', 'Company', 'Location'])\n\n        for job in job_listings:\n            title = job.find('h2').text.strip()\n            company = job.find('h3').text.strip()\n            location = job.find('span', {'class': 'listing-location'}).text.strip()\n            writer.writerow([title, company, location])\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\n```python\nfrom bs4 import BeautifulSoup\nimport requests\nimport json\nimport re\n\n# Constants\nURL = 'https://www.python.org/'\n\ndef extract_internal_links(url=URL):\n    \"\"\"\n    Extracts all the internal links from a webpage and returns them as a list.\n\n    Parameters:\n    url (str): The URL of the web page. Default is 'https://www.python.org/'\n\n    Returns:\n    list: List of internal links.\n\n    Requirements:\n    - bs4\n    - requests\n    - json\n    - re\n\n    Example:\n    >>> links = extract_internal_links()\n    >>> print(json.dumps(links, indent=2))\n    \"\"\"\n    response = requests.get(url)\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    internal_links = []\n    for link in soup.find_all('a'):\n        href = link.get('href')\n\n        if href and not re.match(r'^http', href):\n            internal_links.append(href)\n\n    return internal_links\n```\n"}
{"seed": "{\"task_id\": 20180210, \"prompt\": \"def f_20180210(A, B):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"np.concatenate((A, B))\", \"test_start\": \"\\nimport numpy as np \\n\\ndef check(candidate):\", \"test\": [\"\\n    A = np.array([1,2])\\n    B = np.array([3,4])\\n    assert np.allclose(candidate(A, B), np.array([1,2,3,4]))\\n\", \"\\n    A = np.array([[1,2]])\\n    B = np.array([[3,4]])\\n    assert np.allclose(candidate(A, B), np.array([[1,2],[3,4]]))\\n\", \"\\n    A = np.array([[1],[2]])\\n    B = np.array([[3],[4]])\\n    assert np.allclose(candidate(A, B), np.array([[1],[2],[3],[4]]))\\n\", \"\\n    a = np.array([[1, 3, 4], [4, 5, 6], [6, 0, -1]])\\n    b = np.array([[5, 6, 1], [0, 2, -1], [9, 4, 1]])\\n    expected = np.array([[ 1, 3, 4], [ 4, 5, 6],\\n        [ 6, 0, -1], [ 5, 6, 1], [ 0, 2, -1], [ 9, 4, 1]])\\n    assert np.array_equal(candidate(a, b), expected)\\n\"], \"entry_point\": \"f_20180210\", \"intent\": \"Create new matrix object  by concatenating data from matrix A and matrix B\", \"library\": [\"numpy\"]}", "generation": "Scenario 1:\npandas, numpy, matplotlib, seaborn, sklearn.preprocessing\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Constants\nFEATURES = ['feature1', 'feature2', 'feature3', 'feature4', 'feature5']\n\ndef plot_normalized_data(matrix_A, matrix_B):\n    \"\"\"\n    Normalize and concatenate two matrices, convert the result to a DataFrame, and plot a pairplot of the data.\n\n    Parameters:\n    matrix_A, matrix_B (np.array): Two matrices to be concatenated.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - seaborn\n    - sklearn.preprocessing.MinMaxScaler\n\n    Example:\n    >>> A = np.array([[1,2],[3,4]])\n    >>> B = np.array([[5,6],[7,8]])\n    >>> plot_normalized_data(A, B)\n    \"\"\"\n    scaler = MinMaxScaler()\n    matrix_A_norm = scaler.fit_transform(matrix_A)\n    matrix_B_norm = scaler.transform(matrix_B)\n\n    concatenated_matrix = np.concatenate((matrix_A_norm, matrix_B_norm))\n\n    df = pd.DataFrame(concatenated_matrix, columns=FEATURES)\n\n    sns.pairplot(df)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, scipy.stats, matplotlib\n```python\nimport numpy as np\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\n\ndef plot_distribution(matrix_A, matrix_B):\n    \"\"\"\n    Concatenate two matrices, calculate the histogram and kernel density estimation (KDE), and plot the result.\n\n    Parameters:\n    matrix_A, matrix_B (np.array): Two matrices to be concatenated.\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Example:\n    >>> A = np.array([1,2,3])\n    >>> B = np.array([4,5,6])\n    >>> plot_distribution(A, B)\n    \"\"\"\n    concatenated_matrix = np.concatenate((matrix_A, matrix_B))\n\n    density = stats.kde.gaussian_kde(concatenated_matrix)\n    x = np.arange(min(concatenated_matrix), max(concatenated_matrix), .1)\n\n    plt.hist(concatenated_matrix, bins=20, density=True)\n    plt.plot(x, density(x))\n\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, tensorflow, keras\n```python\nimport numpy as np\nfrom tensorflow import keras \n\ndef create_model(matrix_A, matrix_B):\n    \"\"\"\n    Concatenate two matrices, reshape the result, and use it as input shape for a Keras Sequential model.\n\n    Parameters:\n    matrix_A, matrix_B (np.array): Two matrices to be concatenated.\n\n    Returns:\n    model: A Keras Sequential model.\n\n    Requirements:\n    - numpy\n    - tensorflow\n    - keras\n\n    Example:\n    >>> A = np.array([[1,2],[3,4]])\n    >>> B = np.array([[5,6],[7,8]])\n    >>> model = create_model(A, B)\n    \"\"\"\n    concatenated_matrix = np.concatenate((matrix_A, matrix_B))\n    input_shape = concatenated_matrix.shape\n\n    model = keras.Sequential()\n    model.add(keras.layers.Dense(64, activation='relu', input_shape=input_shape))\n    model.add(keras.layers.Dense(64, activation='relu'))\n    model.add(keras.layers.Dense(10, activation='softmax'))\n\n    return model\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, sklearn.cluster, matplotlib\n```python\nimport numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\ndef perform_kmeans(matrix_A, matrix_B, n_clusters):\n    \"\"\"\n    Concatenate two matrices, perform KMeans clustering, and plot the clusters.\n\n    Parameters:\n    matrix_A, matrix_B (np.array): Two matrices to be concatenated.\n    n_clusters (int): The number of clusters to form.\n\n    Requirements:\n    - numpy\n    - sklearn.cluster.KMeans\n    - matplotlib.pyplot\n\n    Example:\n    >>> A = np.array([[1,2],[3,4]])\n    >>> B = np.array([[5,6],[7,8]])\n    >>> perform_kmeans(A, B, 2)\n    \"\"\"\n    concatenated_matrix = np.concatenate((matrix_A, matrix_B))\n\n    kmeans = KMeans(n_clusters=n_clusters)\n    kmeans.fit(concatenated_matrix)\n\n    plt.scatter(concatenated_matrix[:,0], concatenated_matrix[:,1], c=kmeans.labels_)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, pandas, seaborn, sklearn.preprocessing\n```python\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\n\n# Constants\nFEATURES = ['feature1', 'feature2', 'feature3', 'feature4', 'feature5']\n\ndef standardize_and_visualize(matrix_A, matrix_B):\n    \"\"\"\n    Standardize and concatenate two matrices, convert the result to a DataFrame, and visualize the data with a boxplot.\n\n    Parameters:\n    matrix_A, matrix_B (np.array): Two matrices to be concatenated.\n\n    Requirements:\n    - numpy\n    - pandas\n    - seaborn\n    - sklearn.preprocessing.StandardScaler\n\n    Example:\n    >>> A = np.array([[1,2],[3,4]])\n    >>> B = np.array([[5,6],[7,8]])\n    >>> standardize_and_visualize(A, B)\n    \"\"\"\n    scaler = StandardScaler()\n    matrix_A_std = scaler.fit_transform(matrix_A)\n    matrix_B_std = scaler.transform(matrix_B)\n\n    concatenated_matrix = np.concatenate((matrix_A_std, matrix_B_std))\n\n    df = pd.DataFrame(concatenated_matrix, columns=FEATURES)\n\n    sns.boxplot(data=df)\n    plt.show()\n```"}
{"seed": "{\"task_id\": 20180210, \"prompt\": \"def f_20180210(A, B):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"np.vstack((A, B))\", \"test_start\": \"\\nimport numpy as np \\n\\ndef check(candidate):\", \"test\": [\"\\n    A = np.array([1,2])\\n    B = np.array([3,4])\\n    assert np.allclose(candidate(A, B), np.array([[1,2],[3,4]]))\\n\", \"\\n    A = np.array([[1,2]])\\n    B = np.array([[3,4]])\\n    assert np.allclose(candidate(A, B), np.array([[1,2],[3,4]]))\\n\", \"\\n    A = np.array([[1],[2]])\\n    B = np.array([[3],[4]])\\n    assert np.allclose(candidate(A, B), np.array([[1],[2],[3],[4]]))\\n\", \"\\n    a = np.array([[1, 3, 4], [4, 5, 6], [6, 0, -1]])\\n    b = np.array([[5, 6, 1], [0, 2, -1], [9, 4, 1]])\\n    expected = np.array([[ 1, 3, 4], [ 4, 5, 6],\\n        [ 6, 0, -1], [ 5, 6, 1], [ 0, 2, -1], [ 9, 4, 1]])\\n    assert np.array_equal(candidate(a, b), expected)\\n\"], \"entry_point\": \"f_20180210\", \"intent\": \"concat two matrices `A` and `B` in numpy\", \"library\": [\"numpy\"]}", "generation": "Scenario 1:\nnumpy, scipy.stats, matplotlib, pandas\n```python\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef generate_normal_distribution(A, B):\n    \"\"\"\n    Generate a normal distribution using two numpy arrays `A` and `B` as the mean and standard deviation \n    parameters respectively. Plot the distribution and return the parameters and the distribution.\n\n    Parameters:\n    A (numpy array): Mean of the normal distribution.\n    B (numpy array): Standard deviation of the normal distribution.\n\n    Returns:\n    tuple: Tuple containing numpy arrays of mean, standard deviation and the generated distribution.\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n    - pandas\n\n    Example:\n    >>> A = np.array([1,2])\n    >>> B = np.array([3,4])\n    >>> mean, std_dev, distribution = generate_normal_distribution(A, B)\n    >>> print(mean)\n    >>> print(std_dev)\n    >>> plt.plot(distribution)\n    \"\"\"\n    mean = np.mean(A)\n    std_dev = np.std(B)\n    distribution = norm.rvs(mean, std_dev, size=1000)\n\n    return mean, std_dev, distribution\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, matplotlib, statistics\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom statistics import mean\n\ndef calculate_and_plot_means(A, B):\n    \"\"\"\n    Calculate the arithmetic mean of two numpy arrays `A` and `B`, and plot them.\n\n    Parameters:\n    A (numpy array): First array.\n    B (numpy array): Second array.\n\n    Returns:\n    tuple: Tuple containing the means of `A` and `B`.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - statistics\n\n    Example:\n    >>> A = np.array([1,2,3,4,5])\n    >>> B = np.array([6,7,8,9,10])\n    >>> mean_A, mean_B = calculate_and_plot_means(A, B)\n    >>> print(mean_A)\n    >>> print(mean_B)\n    >>> plt.show()\n    \"\"\"\n    mean_A = mean(A)\n    mean_B = mean(B)\n    plt.plot(A, label='A')\n    plt.plot(B, label='B')\n    plt.legend()\n    plt.show()\n\n    return mean_A, mean_B\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, matplotlib, seaborn, pandas\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\ndef perform_and_plot_bivariate_analysis(A, B):\n    \"\"\"\n    Perform a bivariate analysis between two numpy arrays `A` and `B`, and plot a scatter plot and a jointplot.\n\n    Parameters:\n    A (numpy array): First array.\n    B (numpy array): Second array.\n\n    Returns:\n    DataFrame: A pandas DataFrame with columns `A` and `B`.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - seaborn\n    - pandas\n\n    Example:\n    >>> A = np.array([1,2,3,4,5])\n    >>> B = np.array([6,7,8,9,10])\n    >>> df = perform_and_plot_bivariate_analysis(A, B)\n    >>> print(df)\n    \"\"\"\n    df = pd.DataFrame({\n        'A': A,\n        'B': B\n    })\n\n    plt.scatter(A, B)\n    plt.show()\n\n    sns.jointplot(x='A', y='B', data=df)\n    plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, matplotlib, seaborn, pandas\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\ndef create_and_plot_dataframe(A, B):\n    \"\"\"\n    Create a pandas DataFrame from two numpy arrays `A` and `B`, and plot a line plot and a histogram.\n\n    Parameters:\n    A (numpy array): First array.\n    B (numpy array): Second array.\n\n    Returns:\n    DataFrame: A pandas DataFrame with columns `A` and `B`.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - seaborn\n    - pandas\n\n    Example:\n    >>> A = np.array([1,2,3,4,5])\n    >>> B = np.array([6,7,8,9,10])\n    >>> df = create_and_plot_dataframe(A, B)\n    >>> print(df)\n    \"\"\"\n    df = pd.DataFrame({\n        'A': A,\n        'B': B\n    })\n\n    df.plot()\n    plt.show()\n\n    df.hist()\n    plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, matplotlib, scipy.stats, pandas\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import pearsonr\nimport pandas as pd\n\ndef calculate_and_plot_pearson_correlation(A, B):\n    \"\"\"\n    Calculate the Pearson correlation between two numpy arrays `A` and `B`, and plot a scatter plot.\n\n    Parameters:\n    A (numpy array): First array.\n    B (numpy array): Second array.\n\n    Returns:\n    float: The Pearson correlation coefficient.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - scipy.stats\n    - pandas\n\n    Example:\n    >>> A = np.array([1,2,3,4,5])\n    >>> B = np.array([6,7,8,9,10])\n    >>> correlation = calculate_and_plot_pearson_correlation(A, B)\n    >>> print(correlation)\n    \"\"\"\n    correlation, _ = pearsonr(A, B)\n\n    plt.scatter(A, B)\n    plt.show()\n\n    return correlation\n```"}
{"seed": "{\"task_id\": 2011048, \"prompt\": \"def f_2011048(filepath):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"os.stat(filepath).st_size\", \"test_start\": \"\\nimport os\\n\\ndef check(candidate):\", \"test\": [\"\\n    with open(\\\"tmp.txt\\\", 'w') as fw: fw.write(\\\"hello world!\\\")\\n    assert candidate(\\\"tmp.txt\\\") == 12\\n\", \"\\n    with open(\\\"tmp.txt\\\", 'w') as fw: fw.write(\\\"\\\")\\n    assert candidate(\\\"tmp.txt\\\") == 0\\n\", \"\\n    with open(\\\"tmp.txt\\\", 'w') as fw: fw.write('\\\\n')\\n    assert candidate(\\\"tmp.txt\\\") == 1\\n\", \"\\n    filename = 'o.txt'\\n    with open (filename, 'w') as f:\\n        f.write('a')\\n    assert candidate(filename) == 1\\n\"], \"entry_point\": \"f_2011048\", \"intent\": \"Get the characters count in a file `filepath`\", \"library\": [\"os\"]}", "generation": "Scenario 1:\nos, pandas, matplotlib, glob\n```python\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport glob\n\ndef plot_file_sizes(directory):\n    \"\"\"\n    Generate a bar plot for file sizes in a given directory.\n\n    Parameters:\n    directory (str): The directory to scan for files.\n\n    Returns:\n    None\n\n    Requirements:\n    - os\n    - pandas\n    - matplotlib.pyplot\n    - glob\n\n    Example:\n    >>> plot_file_sizes('./')\n    \"\"\"\n    files = glob.glob(os.path.join(directory, '*'))\n    file_sizes = {os.path.basename(file): os.stat(file).st_size for file in files}\n\n    df = pd.DataFrame(list(file_sizes.items()), columns=['File', 'Size'])\n    df.plot(kind='bar', x='File', y='Size')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, shutil, hashlib, csv\n```python\nimport os\nimport shutil\nimport hashlib\nimport csv\n\ndef save_file_hashes_to_csv(directory, csv_file):\n    \"\"\"\n    Scan a directory and save file names and their md5 hashes to a CSV file.\n\n    Parameters:\n    directory (str): The directory to scan for files.\n    csv_file (str): The CSV file to save the results.\n\n    Returns:\n    None\n\n    Requirements:\n    - os\n    - shutil\n    - hashlib\n    - csv\n\n    Example:\n    >>> save_file_hashes_to_csv('./', 'file_hashes.csv')\n    \"\"\"\n    with open(csv_file, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(['File', 'MD5 Hash'])\n\n        for filename in os.listdir(directory):\n            filepath = os.path.join(directory, filename)\n\n            if os.path.isfile(filepath):\n                with open(filepath, 'rb') as f:\n                    data = f.read()\n                    md5_hash = hashlib.md5(data).hexdigest()\n\n                writer.writerow([filename, md5_hash])\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, pathlib, zipfile\n```python\nimport os\nimport pathlib\nimport zipfile\n\ndef zip_files_in_directory(directory, zip_file):\n    \"\"\"\n    Compress all files in a directory into a zip file.\n\n    Parameters:\n    directory (str): The directory to scan for files.\n    zip_file (str): The output zip file.\n\n    Returns:\n    None\n\n    Requirements:\n    - os\n    - pathlib\n    - zipfile\n\n    Example:\n    >>> zip_files_in_directory('./', 'files.zip')\n    \"\"\"\n    with zipfile.ZipFile(zip_file, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        for root, dirs, files in os.walk(directory):\n            for file in files:\n                file_path = pathlib.Path(root, file)\n                zipf.write(file_path)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, csv, random, string\n```python\nimport os\nimport csv\nimport random\nimport string\n\ndef create_sample_csv_file(filepath, row_count, column_count):\n    \"\"\"\n    Create a sample CSV file with a specified number of rows and columns.\n\n    Parameters:\n    filepath (str): The path to the output CSV file.\n    row_count (int): The number of rows in the CSV file.\n    column_count (int): The number of columns in the CSV file.\n\n    Returns:\n    None\n\n    Requirements:\n    - os\n    - csv\n    - random\n    - string\n\n    Example:\n    >>> create_sample_csv_file('sample.csv', 10, 5)\n    \"\"\"\n    with open(filepath, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n\n        headers = ['column_{}'.format(i+1) for i in range(column_count)]\n        writer.writerow(headers)\n\n        for _ in range(row_count):\n            row = [''.join(random.choices(string.ascii_letters + string.digits, k=10)) for _ in range(column_count)]\n            writer.writerow(row)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, pandas, numpy, matplotlib\n```python\nimport os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_file_line_count_histogram(directory):\n    \"\"\"\n    Generate a histogram for line counts in all files in a given directory.\n\n    Parameters:\n    directory (str): The directory to scan for files.\n\n    Returns:\n    None\n\n    Requirements:\n    - os\n    - pandas\n    - numpy\n    - matplotlib\n\n    Example:\n    >>> plot_file_line_count_histogram('./')\n    \"\"\"\n    line_counts = []\n    for filename in os.listdir(directory):\n        filepath = os.path.join(directory, filename)\n        if os.path.isfile(filepath):\n            with open(filepath, 'r') as f:\n                line_count = sum(1 for _ in f)\n                line_counts.append(line_count)\n\n    df = pd.DataFrame(line_counts, columns=['Line Count'])\n    df.hist(bins=np.arange(0, max(line_counts)+10, 10))\n    plt.show()\n```"}
{"seed": "{\"task_id\": 2600191, \"prompt\": \"def f_2600191(l):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"l.count('a')\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(\\\"123456asf\\\") == 1\\n\", \"\\n    assert candidate(\\\"123456gyjnccfgsf\\\") == 0\\n\", \"\\n    assert candidate(\\\"aA\\\"*10) == 10\\n\"], \"entry_point\": \"f_2600191\", \"intent\": \"count the occurrences of item \\\"a\\\" in list `l`\", \"library\": []}", "generation": "Scenario 1:\npandas, numpy, random, os\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import choice\nimport os\n\n# Constants\nFILE_PATH = os.path.join(os.getcwd(), 'data.csv')\nLETTERS = list('abcdefghijklmnopqrstuvwxyz')\n\ndef generate_and_save_data(num_rows):\n    \"\"\"\n    Generate a DataFrame with a specified number of rows. Each row contains a random \n    string of lowercase letters. Save the DataFrame to a CSV file and return the count \n    of the letter 'a' in all strings.\n\n    Parameters:\n    num_rows (int): The number of rows in the DataFrame.\n\n    Returns:\n    int: The count of the letter 'a' in all strings.\n\n    Requirements:\n    - pandas\n    - numpy\n    - random\n    - os\n\n    Example:\n    >>> generate_and_save_data(1000)\n    \"\"\"\n    data = [''.join(choice(LETTERS) for _ in range(10)) for _ in range(num_rows)]\n    df = pd.DataFrame(data, columns=['Random String'])\n    df.to_csv(FILE_PATH, index=False)\n    a_count = sum(string.count('a') for string in data)\n    return a_count\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncollections, re, glob\n```python\nfrom collections import Counter\nimport re\nimport glob\n\n# Constants\nFILE_PATTERN = '*.txt'\n\ndef search_and_count_letter_in_files(letter):\n    \"\"\"\n    Search for text files in the current directory and count the total occurrences \n    of a specified letter (case-insensitive) in all files.\n\n    Parameters:\n    letter (str): The letter to search for.\n\n    Returns:\n    int: The total count of the letter.\n\n    Requirements:\n    - collections\n    - re\n    - glob\n\n    Example:\n    >>> search_and_count_letter_in_files('a')\n    \"\"\"\n    total_count = Counter()\n    for file_name in glob.glob(FILE_PATTERN):\n        with open(file_name, 'r') as file:\n            text = file.read().lower()\n            total_count += Counter(re.findall(letter, text))\n    return total_count[letter]\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, matplotlib.pyplot\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nNUM_SAMPLES = 1000\n\ndef plot_histogram_of_letter_counts():\n    \"\"\"\n    Generate a histogram of counts of the letter 'a' in a list of random strings. \n    The strings are generated from a normal distribution.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_histogram_of_letter_counts()\n    \"\"\"\n    data = np.random.choice(list('abcdefghijklmnopqrstuvwxyz'), (NUM_SAMPLES, 100))\n    data = [''.join(row) for row in data]\n    counts = [string.count('a') for string in data]\n    plt.hist(counts, bins=20, alpha=0.5)\n    plt.title(\"Histogram of 'a' Counts\")\n    plt.xlabel(\"'a' Count\")\n    plt.ylabel(\"Frequency\")\n    plt.show()\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, json, os\n```python\nimport numpy as np\nimport json\nimport os\n\n# Constants\nFILE_PATH = os.path.join(os.getcwd(), 'data.json')\n\ndef generate_and_save_json_data(num_elements):\n    \"\"\"\n    Generate a list of random strings of lowercase letters with a specified number \n    of elements. Save the list to a JSON file and return the count of the letter 'a' \n    in all strings.\n\n    Parameters:\n    num_elements (int): The number of elements in the list.\n\n    Returns:\n    int: The count of the letter 'a' in all strings.\n\n    Requirements:\n    - numpy\n    - json\n    - os\n\n    Example:\n    >>> generate_and_save_json_data(1000)\n    \"\"\"\n    data = ''.join(np.random.choice(list('abcdefghijklmnopqrstuvwxyz'), num_elements)).tolist()\n    with open(FILE_PATH, 'w') as file:\n        json.dump(data, file)\n    a_count = sum(string.count('a') for string in data)\n    return a_count\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, nltk\n```python\nimport pandas as pd\nfrom nltk import FreqDist\n\n# Constants\nDATA = ['apple', 'banana', 'cherry', 'apple', 'banana', 'apple']\n\ndef count_occurrences_in_list():\n    \"\"\"\n    Count the occurrences of each item in a list using nltk's FreqDist, \n    convert the result to a pandas Series and return the count of 'apple'.\n\n    Returns:\n    int: The count of 'apple'.\n\n    Requirements:\n    - pandas\n    - nltk\n\n    Example:\n    >>> count_occurrences_in_list()\n    \"\"\"\n    freq_dist = FreqDist(DATA)\n    series = pd.Series(dict(freq_dist))\n    return series['apple']\n```"}
{"seed": "{\"task_id\": 2600191, \"prompt\": \"def f_2600191(l):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"Counter(l)\", \"test_start\": \"\\nfrom collections import Counter \\n\\ndef check(candidate):\", \"test\": [\"\\n    assert dict(candidate(\\\"123456asf\\\")) == {'1': 1, '2': 1, '3': 1, '4': 1, '5': 1, '6': 1, 'a': 1, 's': 1, 'f': 1}\\n\", \"\\n    assert candidate(\\\"123456gyjnccfgsf\\\") == {'1': 1,'2': 1,'3': 1,'4': 1,'5': 1,'6': 1,'g': 2,'y': 1,'j': 1,'n': 1,'c': 2,'f': 2,'s': 1}\\n\", \"\\n    assert candidate(\\\"aA\\\"*10) == {'a': 10, 'A': 10}\\n\", \"\\n    y = candidate([1, 6])\\n    assert y[1] == 1\\n    assert y[6] == 1\\n\"], \"entry_point\": \"f_2600191\", \"intent\": \"count the occurrences of items in list `l`\", \"library\": [\"collections\"]}", "generation": "Scenario 1:\ncollections, random, string, matplotlib\n```python\nimport collections\nimport random\nimport string\nimport matplotlib.pyplot as plt\n\n# Constants\nLETTERS = string.ascii_letters\n\ndef generate_random_letter_histogram(num_letters):\n    \"\"\"\n    Generate a histogram of the occurrences of randomly chosen letters.\n\n    Parameters:\n    num_letters (int): The number of letters to generate.\n\n    Returns:\n    collections.Counter: A Counter object of the occurrences of each letter.\n\n    Requirements:\n    - collections\n    - random\n    - string\n    - matplotlib.pyplot\n\n    Example:\n    >>> histogram = generate_random_letter_histogram(100)\n    >>> plt.bar(histogram.keys(), histogram.values())\n    \"\"\"\n    letters = [random.choice(LETTERS) for _ in range(num_letters)]\n    return collections.Counter(letters)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncollections, pandas, numpy, matplotlib\n```python\nimport collections\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCATEGORIES = ['A', 'B', 'C', 'D', 'E']\n\ndef generate_category_data_histogram(num_samples):\n    \"\"\"\n    Generate a histogram of the occurrences of categories in a pandas Series.\n\n    Parameters:\n    num_samples (int): The number of samples to generate.\n\n    Returns:\n    collections.Counter: A Counter object of the occurrences of each category.\n\n    Requirements:\n    - collections\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> histogram = generate_category_data_histogram(100)\n    >>> plt.bar(histogram.keys(), histogram.values())\n    \"\"\"\n    data = pd.Series(np.random.choice(CATEGORIES, size=num_samples))\n    return collections.Counter(data)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ncollections, random, matplotlib, seaborn\n```python\nimport collections\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nDIE_VALUES = list(range(1, 7))\n\ndef roll_die_histogram(num_rolls):\n    \"\"\"\n    Generate a histogram of the outcomes of rolling a six-sided die.\n\n    Parameters:\n    num_rolls (int): The number of times to roll the die.\n\n    Returns:\n    collections.Counter: A Counter object of the outcomes of each roll.\n\n    Requirements:\n    - collections\n    - random\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> histogram = roll_die_histogram(100)\n    >>> sns.barplot(x=list(histogram.keys()), y=list(histogram.values()))\n    \"\"\"\n    rolls = [random.choice(DIE_VALUES) for _ in range(num_rolls)]\n    return collections.Counter(rolls)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ncollections, itertools, string, matplotlib\n```python\nimport collections\nimport itertools\nimport string\nimport matplotlib.pyplot as plt\n\n# Constants\nLETTERS = string.ascii_letters\n\ndef count_letter_pairs_histogram(text):\n    \"\"\"\n    Generate a histogram of the occurrences of letter pairs in a text.\n\n    Parameters:\n    text (str): The text to analyze.\n\n    Returns:\n    collections.Counter: A Counter object of the occurrences of each letter pair.\n\n    Requirements:\n    - collections\n    - itertools\n    - string\n    - matplotlib.pyplot\n\n    Example:\n    >>> histogram = count_letter_pairs_histogram('Hello, world!')\n    >>> plt.bar(histogram.keys(), histogram.values())\n    \"\"\"\n    pairs = [''.join(pair) for pair in itertools.combinations(text, 2)]\n    return collections.Counter(pairs)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\ncollections, os, matplotlib\n```python\nimport collections\nimport os\nimport matplotlib.pyplot as plt\n\ndef count_file_types_histogram(directory):\n    \"\"\"\n    Generate a histogram of the occurrences of file types in a directory.\n\n    Parameters:\n    directory (str): The directory to analyze.\n\n    Returns:\n    collections.Counter: A Counter object of the occurrences of each file type.\n\n    Requirements:\n    - collections\n    - os\n    - matplotlib.pyplot\n\n    Example:\n    >>> histogram = count_file_types_histogram('/path/to/directory')\n    >>> plt.bar(histogram.keys(), histogram.values())\n    \"\"\"\n    files = os.listdir(directory)\n    file_types = [os.path.splitext(file)[1] for file in files]\n    return collections.Counter(file_types)\n```"}
{"seed": "{\"task_id\": 2600191, \"prompt\": \"def f_2600191(l):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"[[x, l.count(x)] for x in set(l)]\", \"test_start\": \"\\nfrom collections import Counter \\n\\ndef check(candidate):\", \"test\": [\"\\n    assert sorted(candidate(\\\"123456asf\\\")) == [['1', 1],['2', 1],['3', 1],['4', 1],['5', 1],['6', 1],['a', 1],['f', 1],['s', 1]]\\n\", \"\\n    assert sorted(candidate(\\\"aA\\\"*10)) == [['A', 10], ['a', 10]]\\n\"], \"entry_point\": \"f_2600191\", \"intent\": \"count the occurrences of items in list `l`\", \"library\": [\"collections\"]}", "generation": "Scenario 1: \npandas, collections, matplotlib\n```python\nimport pandas as pd\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\n# Sample Data\nDATA = ['Apple', 'Banana', 'Apple', 'Orange', 'Banana', 'Banana']\n\ndef fruit_frequency(data):\n    \"\"\"\n    Generate a bar plot of the frequency of each fruit in the given list.\n\n    Parameters:\n    data (list): The list of fruits.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - collections\n    - matplotlib\n\n    Example:\n    >>> fruit_frequency(DATA)\n    \"\"\"\n    count_data = Counter(data)\n    df = pd.DataFrame.from_dict(count_data, orient='index').reset_index()\n    df = df.rename(columns={'index':'fruit', 0:'count'})\n    df.plot.bar(x='fruit', y='count', rot=0)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \ncollections, itertools, string\n```python\nfrom collections import Counter\nimport itertools\nimport string\n\n# Constants\nALPHABETS = string.ascii_lowercase\n\ndef count_char_combinations(text):\n    \"\"\"\n    Count the occurrences of all two-character combinations in a given text \n    and return a dictionary where keys are the combinations and values are their counts.\n\n    Parameters:\n    text (str): The input text.\n\n    Returns:\n    dict: The dictionary of combinations and their counts.\n\n    Requirements:\n    - collections\n    - itertools\n    - string\n\n    Example:\n    >>> count_char_combinations('abcabc')\n    \"\"\"\n    combinations = list(itertools.product(ALPHABETS, repeat=2))\n    combinations = [''.join(comb) for comb in combinations]\n    count_dict = {comb: 0 for comb in combinations}\n\n    for i in range(len(text)-1):\n        comb = text[i:i+2]\n        if comb in count_dict:\n            count_dict[comb] += 1\n\n    return count_dict\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \ncollections, numpy, matplotlib\n```python\nfrom collections import Counter\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Sample Data\nDATA = np.random.randint(0, 10, 100)\n\ndef plot_histogram(data):\n    \"\"\"\n    Plot a histogram of the occurrences of numbers in a numpy array.\n\n    Parameters:\n    data (numpy array): The input array.\n\n    Returns:\n    None\n\n    Requirements:\n    - collections\n    - numpy\n    - matplotlib\n\n    Example:\n    >>> plot_histogram(DATA)\n    \"\"\"\n    data_counter = Counter(data)\n    num, count = zip(*data_counter.items())\n    plt.bar(num, count)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \ncollections, operator, string\n```python\nfrom collections import Counter\nimport operator\nimport string\n\n# Constants\nPUNCTUATIONS = string.punctuation\n\ndef most_common_word(text):\n    \"\"\"\n    Find the most common word in a given text excluding punctuations.\n\n    Parameters:\n    text (str): The input text.\n\n    Returns:\n    str: The most common word.\n\n    Requirements:\n    - collections\n    - operator\n    - string\n\n    Example:\n    >>> most_common_word('Hello, world! Hello, everyone!')\n    \"\"\"\n    text = ''.join(ch for ch in text if ch not in PUNCTUATIONS)\n    words = text.split()\n    word_count = Counter(words)\n    most_common_word = max(word_count.items(), key=operator.itemgetter(1))[0]\n\n    return most_common_word\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \ncollections, re, os\n```python\nfrom collections import Counter\nimport re\nimport os\n\n# Constants\nPATH = './'\n\ndef count_file_extensions(path):\n    \"\"\"\n    Count the occurrences of file extensions in a given directory.\n\n    Parameters:\n    path (str): The directory path.\n\n    Returns:\n    dict: The dictionary of extensions and their counts.\n\n    Requirements:\n    - collections\n    - re\n    - os\n\n    Example:\n    >>> count_file_extensions(PATH)\n    \"\"\"\n    files = os.listdir(path)\n    extensions = [re.findall(r'\\.([^.]+$)', file)[0] for file in files if '.' in file]\n    extension_count = Counter(extensions)\n\n    return dict(extension_count)\n```"}
{"seed": "{\"task_id\": 2600191, \"prompt\": \"def f_2600191(l):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"dict(((x, l.count(x)) for x in set(l)))\", \"test_start\": \"\\nfrom collections import Counter \\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(\\\"123456asf\\\") == {'4': 1, 'a': 1, '1': 1, 's': 1, '6': 1, 'f': 1, '3': 1, '5': 1, '2': 1}\\n\", \"\\n    assert candidate(\\\"aA\\\"*10) == {'A': 10, 'a': 10}\\n\", \"\\n    assert candidate([1, 6]) == {1: 1, 6: 1}\\n\"], \"entry_point\": \"f_2600191\", \"intent\": \"count the occurrences of items in list `l`\", \"library\": [\"collections\"]}", "generation": "Scenario 1: \ncollections, re, string\n```python\nfrom collections import Counter\nimport re\nimport string\n\n# Constants\nPUNCTUATION = string.punctuation\n\ndef count_words_and_punctuation(text):\n    \"\"\"\n    Count the occurrences of words and punctuation in a given text.\n\n    Parameters:\n    text (str): The input text.\n\n    Returns:\n    dict: A dictionary with words and punctuation as keys and their counts as values.\n\n    Requirements:\n    - collections.Counter\n    - re\n    - string\n\n    Example:\n    >>> count_words_and_punctuation('Hello, world! How are you today?')\n    \"\"\"\n    words = re.findall(r'\\b\\w+\\b', text)\n    punctuation = re.findall(f'[{PUNCTUATION}]', text)\n    count = Counter(words + punctuation)\n\n    return dict(count)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \ncollections, random, math\n```python\nfrom collections import Counter\nimport random\nimport math\n\ndef count_random_numbers(n, seed):\n    \"\"\"\n    Generate n random numbers between 1 and 10 (inclusive) using a given seed and \n    count the occurrences of each number.\n\n    Parameters:\n    n (int): The number of random numbers to generate.\n    seed (int): The seed for the random number generator.\n\n    Returns:\n    dict: A dictionary with numbers as keys and their counts as values.\n\n    Requirements:\n    - collections.Counter\n    - random\n    - math\n\n    Example:\n    >>> count_random_numbers(100, 123)\n    \"\"\"\n    random.seed(seed)\n    numbers = [math.ceil(random.random() * 10) for _ in range(n)]\n    count = Counter(numbers)\n\n    return dict(count)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \ncollections, re, string\n```python\nfrom collections import Counter\nimport re\nimport string\n\n# Constants\nLETTERS = string.ascii_letters\n\ndef count_letters_and_numbers(text):\n    \"\"\"\n    Count the occurrences of letters and numbers in a given text.\n\n    Parameters:\n    text (str): The input text.\n\n    Returns:\n    dict: A dictionary with letters and numbers as keys and their counts as values.\n\n    Requirements:\n    - collections.Counter\n    - re\n    - string\n\n    Example:\n    >>> count_letters_and_numbers('abc123')\n    \"\"\"\n    letters = re.findall(f'[{LETTERS}]', text)\n    numbers = re.findall(r'\\b\\d+\\b', text)\n    count = Counter(letters + numbers)\n\n    return dict(count)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \ncollections, itertools\n```python\nfrom collections import Counter\nimport itertools\n\ndef count_sublists_occurrences(l):\n    \"\"\"\n    Count the occurrences of each sublist in a list of lists.\n\n    Parameters:\n    l (list): The input list of lists.\n\n    Returns:\n    dict: A dictionary with sublists as keys and their counts as values.\n\n    Requirements:\n    - collections.Counter\n    - itertools\n\n    Example:\n    >>> count_sublists_occurrences([[1,2,3],[4,5,6],[1,2,3]])\n    \"\"\"\n    l = [tuple(sublist) for sublist in l]\n    count = Counter(l)\n\n    return dict(count)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \ncollections, pandas\n```python\nfrom collections import Counter\nimport pandas as pd\n\ndef count_dataframe_values(df, column):\n    \"\"\"\n    Count the occurrences of each value in a specific column of a pandas DataFrame.\n\n    Parameters:\n    df (DataFrame): The input pandas DataFrame.\n    column (str): The column of the DataFrame to count the values of.\n\n    Returns:\n    dict: A dictionary with values as keys and their counts as values.\n\n    Requirements:\n    - collections.Counter\n    - pandas\n\n    Example:\n    >>> df = pd.DataFrame({'col1': [1, 2, 2, 3, 3, 3], 'col2': ['a', 'b', 'b', 'c', 'c', 'c']})\n    >>> count_dataframe_values(df, 'col1')\n    \"\"\"\n    count = Counter(df[column])\n\n    return dict(count)\n```"}
{"seed": "{\"task_id\": 2600191, \"prompt\": \"def f_2600191(l):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"l.count('b')\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(\\\"123456abbbsf\\\") == 3\\n\", \"\\n    assert candidate(\\\"123456gyjnccfgsf\\\") == 0\\n\", \"\\n    assert candidate(\\\"Ab\\\"*10) == 10\\n\"], \"entry_point\": \"f_2600191\", \"intent\": \"count the occurrences of item \\\"b\\\" in list `l`\", \"library\": []}", "generation": "Scenario 1:\nnumpy, pandas, matplotlib, random\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import choices, seed\n\n# Constants\nLETTERS = list('abcdefghijklmnopqrstuvwxyz')\nseed(1)  # For reproducibility\n\ndef generate_and_plot_letter_distribution(n_samples):\n    \"\"\"\n    Generate a list of random lower case letters, calculate and plot the distribution \n    of 'b' and 'c' in the list.\n\n    Parameters:\n    n_samples (int): The number of samples to generate.\n\n    Returns:\n    DataFrame: A pandas DataFrame with counts of 'b' and 'c'.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib\n    - random\n\n    Example:\n    >>> dist = generate_and_plot_letter_distribution(10000)\n    >>> print(dist)\n    >>> dist.plot(kind='bar')\n    \"\"\"\n    random_letters = choices(LETTERS, k=n_samples)\n    letter_counts = pd.Series(random_letters).value_counts()\n\n    bc_distribution = letter_counts.loc[['b', 'c']]\n\n    return bc_distribution\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, datetime, pytz, pandas\n```python\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\nimport pytz\n\n# Constants\nHOURS = np.arange(0, 24, 1)\n\ndef count_hour_occurrences(timezone):\n    \"\"\"\n    Generate a list of random hours, convert them to a specified timezone,\n    and count the number of occurrences of each hour in the converted list.\n\n    Parameters:\n    timezone (str): The timezone to convert the hours to.\n\n    Returns:\n    DataFrame: A pandas DataFrame with counts of each hour.\n\n    Requirements:\n    - numpy\n    - datetime\n    - pytz\n    - pandas\n\n    Example:\n    >>> hour_counts = count_hour_occurrences(\"Asia/Tokyo\")\n    >>> print(hour_counts)\n    \"\"\"\n    random_hours = np.random.choice(HOURS, size=1000)\n\n    tz = pytz.timezone(timezone)\n    utc = pytz.UTC\n\n    converted_hours = [(datetime.now().replace(hour=hour, minute=0, second=0)\n                        .replace(tzinfo=utc)\n                        .astimezone(tz).hour) for hour in random_hours]\n\n    hour_counts = pd.Series(converted_hours).value_counts().sort_index()\n\n    return hour_counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nitertools, collections, math\n```python\nfrom itertools import permutations\nfrom collections import Counter\nimport math\n\n# Constants\nLETTERS = list('abc')\n\ndef count_permutations_and_check_distribution(n_letters):\n    \"\"\"\n    Generate all permutations of n_letters from ['a', 'b', 'c'] list,\n    count the occurrence of 'b' in these permutations,\n    and check whether the distribution follows the binomial distribution.\n\n    Parameters:\n    n_letters (int): The number of letters in each permutation.\n\n    Returns:\n    bool: True if the distribution follows the binomial distribution, False otherwise.\n\n    Requirements:\n    - itertools\n    - collections\n    - math\n\n    Example:\n    >>> count_permutations_and_check_distribution(5)\n    \"\"\"\n    all_perms = permutations(LETTERS * n_letters, n_letters)\n    b_counts = Counter(''.join(perm).count('b') for perm in all_perms)\n\n    total_perms = math.factorial(n_letters) ** len(LETTERS)\n    expected_distribution = {k: (math.comb(n_letters, k) * (1/len(LETTERS))**k * (1 - 1/len(LETTERS))**(n_letters-k)) * total_perms for k in range(n_letters+1)}\n\n    return all(math.isclose(b_counts[k], expected_distribution[k], rel_tol=1e-9) for k in b_counts)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nre, collections, string\n```python\nimport re\nfrom collections import Counter\nimport string\n\n# Constants\nTEXT = \"I love Python. Python is a very easy to learn language. It's powerful and versatile.\"\n\ndef count_words_starting_with_b(text):\n    \"\"\"\n    Count the occurrence of words starting with 'b' in a given text.\n\n    Parameters:\n    text (str): The text to search.\n\n    Returns:\n    dict: A dictionary with words as keys and occurrences as values.\n\n    Requirements:\n    - re\n    - collections\n\n    Example:\n    >>> count_words_starting_with_b(TEXT)\n    \"\"\"\n    words = re.findall(r'\\b[bB]\\w*\\b', text)\n    word_counts = dict(Counter(words))\n\n    return word_counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, pandas, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nPOPULATION_SIZE = 10000\nAGES = np.arange(18, 81, 1)\n\ndef simulate_and_plot_population_age_distribution():\n    \"\"\"\n    Simulate a population age distribution, count the number of people in the age of '30s' and '40s',\n    and plot the distribution.\n\n    Returns:\n    DataFrame: A pandas DataFrame with counts of each age.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib\n\n    Example:\n    >>> age_dist = simulate_and_plot_population_age_distribution()\n    >>> print(age_dist)\n    >>> age_dist.plot(kind='bar')\n    \"\"\"\n    population_ages = np.random.choice(AGES, size=POPULATION_SIZE)\n    age_counts = pd.Series(population_ages).value_counts().sort_index()\n\n    return age_counts\n```"}
{"seed": "{\"task_id\": 12842997, \"prompt\": \"def f_12842997(srcfile, dstdir):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"shutil.copy(srcfile, dstdir)\", \"test_start\": \"\\nimport shutil\\nfrom unittest.mock import Mock\\n\\ndef check(candidate):\", \"test\": [\"\\n    shutil.copy = Mock()\\n    try:\\n        candidate('opera.txt', '/')\\n    except:\\n        return False \\n\"], \"entry_point\": \"f_12842997\", \"intent\": \"copy file `srcfile` to directory `dstdir`\", \"library\": [\"shutil\"]}", "generation": "Scenario 1:\nshutil, os, csv, pandas\n```python\nimport shutil\nimport os\nimport csv\nimport pandas as pd\n\n# Constants\nCSV_EXTENSION = '.csv'\nFILE_HEADERS = ['ID', 'Name', 'Age', 'Country']\n\ndef move_and_aggregate_files(src_dir, dst_dir, aggregate_file_name):\n    \"\"\"\n    Move all CSV files from `src_dir` to `dst_dir` and aggregate all the data \n    from these files into a new CSV file in `dst_dir`.\n\n    Parameters:\n    src_dir (str): The source directory.\n    dst_dir (str): The destination directory.\n    aggregate_file_name (str): The name of the file to aggregate the data into.\n\n    Requirements:\n    - shutil\n    - os\n    - csv\n    - pandas\n\n    Example:\n    >>> move_and_aggregate_files('./source', './destination', 'aggregate.csv')\n    \"\"\"\n    data_frames = []\n\n    for filename in os.listdir(src_dir):\n        if filename.endswith(CSV_EXTENSION):\n            # Move the file to the destination directory\n            shutil.move(os.path.join(src_dir, filename), dst_dir)\n\n            # Read the data and append to the list of data frames\n            df = pd.read_csv(os.path.join(dst_dir, filename))\n            data_frames.append(df)\n\n    # Concatenate all data frames\n    aggregate_df = pd.concat(data_frames)\n\n    # Write the aggregated data to a new CSV file\n    aggregate_df.to_csv(os.path.join(dst_dir, aggregate_file_name), index=False)\n\n    return os.path.join(dst_dir, aggregate_file_name)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nshutil, pathlib, zipfile, time\n```python\nimport shutil\nfrom pathlib import Path\nimport zipfile\nimport time\n\n# Constants\nZIP_EXTENSION = '.zip'\n\ndef archive_and_copy_directory(src_dir, dst_dir):\n    \"\"\"\n    Archive a source directory, copy it to a destination directory, and then \n    delete the original archived directory.\n\n    Parameters:\n    src_dir (str): The source directory.\n    dst_dir (str): The destination directory.\n\n    Returns:\n    str: The path to the copied archive.\n\n    Requirements:\n    - shutil\n    - pathlib\n    - zipfile\n    - time\n\n    Example:\n    >>> archive_and_copy_directory('./source', './destination')\n    \"\"\"\n    src_path = Path(src_dir)\n    archive_name = f\"{src_path.name}_{int(time.time())}{ZIP_EXTENSION}\"\n    archive_path = src_path.parent / archive_name\n\n    # Create a zip archive of the source directory\n    with zipfile.ZipFile(archive_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        for root, _, files in os.walk(src_path):\n            for file in files:\n                zipf.write(os.path.join(root, file), \n                           os.path.relpath(os.path.join(root, file), \n                           os.path.join(src_path, '..')))\n\n    # Copy the archive to the destination directory\n    dst_archive_path = shutil.copy2(archive_path, dst_dir)\n\n    # Delete the original archive\n    archive_path.unlink()\n\n    return str(dst_archive_path)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nshutil, os, glob, PIL\n```python\nimport shutil\nimport os\nimport glob\nfrom PIL import Image\n\n# Constants\nIMAGE_EXTENSIONS = ['.png', '.jpg', '.jpeg', '.bmp']\n\ndef copy_resized_images(src_dir, dst_dir, max_size):\n    \"\"\"\n    Copy all images from `src_dir` to `dst_dir` and resize them so that both \n    their width and height are at most `max_size`.\n\n    Parameters:\n    src_dir (str): The source directory.\n    dst_dir (str): The destination directory.\n    max_size (int): The maximum size for the width and height of the images.\n\n    Requirements:\n    - shutil\n    - os\n    - glob\n    - PIL\n\n    Example:\n    >>> copy_resized_images('./source', './destination', 800)\n    \"\"\"\n    paths = [glob.glob(f\"{src_dir}/*{ext}\") for ext in IMAGE_EXTENSIONS]\n    image_paths = [path for sublist in paths for path in sublist]\n\n    for path in image_paths:\n        img = Image.open(path)\n        img.thumbnail((max_size, max_size))\n\n        dst_path = os.path.join(dst_dir, os.path.basename(path))\n        img.save(dst_path)\n\n    return dst_dir\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nshutil, os, tarfile, hashlib\n```python\nimport shutil\nimport os\nimport tarfile\nimport hashlib\n\ndef create_and_copy_archive_with_checksum(src_dir, dst_dir):\n    \"\"\"\n    Create a tar archive of a source directory, calculate its checksum, and \n    copy it to a destination directory.\n\n    Parameters:\n    src_dir (str): The source directory.\n    dst_dir (str): The destination directory.\n\n    Returns:\n    str: The checksum of the archive.\n\n    Requirements:\n    - shutil\n    - os\n    - tarfile\n    - hashlib\n\n    Example:\n    >>> create_and_copy_archive_with_checksum('./source', './destination')\n    \"\"\"\n    archive_path = shutil.make_archive(src_dir, 'tar', src_dir)\n\n    # Calculate the checksum\n    with open(archive_path, 'rb') as file:\n        bytes = file.read()\n        checksum = hashlib.md5(bytes).hexdigest()\n\n    # Copy the archive to the destination directory\n    shutil.copy2(archive_path, dst_dir)\n\n    return checksum\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nshutil, os, fnmatch, itertools\n```python\nimport shutil\nimport os\nimport fnmatch\nimport itertools\n\n# Constants\nFILE_PATTERNS = ['*.txt', '*.docx']\n\ndef copy_matching_files(src_dir, dst_dir):\n    \"\"\"\n    Copy all files from `src_dir` to `dst_dir` that match any pattern in \n    `FILE_PATTERNS`.\n\n    Parameters:\n    src_dir (str): The source directory.\n    dst_dir (str): The destination directory.\n\n    Requirements:\n    - shutil\n    - os\n    - fnmatch\n    - itertools\n\n    Example:\n    >>> copy_matching_files('./source', './destination')\n    \"\"\"\n    # Find all matching files\n    matching_files = list(itertools.chain.from_iterable(\n        fnmatch.filter(os.listdir(src_dir), pattern) for pattern in FILE_PATTERNS))\n\n    for filename in matching_files:\n        shutil.copy2(os.path.join(src_dir, filename), dst_dir)\n\n    return dst_dir\n```"}
{"seed": "{\"task_id\": 1555968, \"prompt\": \"def f_1555968(x):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"max(k for k, v in x.items() if v != 0)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate({'a': 1, 'b': 2, 'c': 2000}) == 'c'\\n\", \"\\n    assert candidate({'a': 0., 'b': 0, 'c': 200.02}) == 'c'\\n\", \"\\n    assert candidate({'key1': -100, 'key2': 0.}) == 'key1'\\n\", \"\\n    x = {1:\\\"g\\\", 2:\\\"a\\\", 5:\\\"er\\\", -4:\\\"dr\\\"}\\n    assert candidate(x) == 5\\n\"], \"entry_point\": \"f_1555968\", \"intent\": \"find the key associated with the largest value in dictionary `x` whilst key is non-zero value\", \"library\": []}", "generation": "Scenario 1: \ncollections, itertools, random, numpy\n```python\nimport collections\nfrom itertools import combinations\nimport random\nimport numpy as np\n\n# Constants\nLETTERS = 'abcdefghijklmnopqrstuvwxyz'\nLETTER_WEIGHTS = np.random.randint(1, 100, size=len(LETTERS))\nLETTER_WEIGHT_DICT = dict(zip(LETTERS, LETTER_WEIGHTS))\n\ndef find_max_weighted_subsequence(seq):\n    \"\"\"\n    Find the subsequence in a string that has the maximum total weight, based on the \n    weights given for each character. The weights are randomly assigned and a subsequence \n    is a sequence that can be derived from another sequence by deleting some elements \n    without changing the order of the remaining elements.\n\n    Parameters:\n    seq (str): The input string.\n\n    Returns:\n    str: The subsequence with the highest weight.\n\n    Requirements:\n    - collections\n    - itertools\n    - random\n    - numpy\n\n    Example:\n    >>> find_max_weighted_subsequence('abc')\n    \"\"\"\n    max_weight = -1\n    max_subseq = ''\n\n    for r in range(1, len(seq) + 1):\n        for subseq in combinations(seq, r):\n            weight = sum(LETTER_WEIGHT_DICT[c] for c in subseq)\n            if weight > max_weight:\n                max_weight = weight\n                max_subseq = ''.join(subseq)\n\n    return max_subseq\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nheapq, collections, random\n```python\nimport heapq\nimport collections\nimport random\n\n# Constants\nLETTERS = 'abcdefghijklmnopqrstuvwxyz'\nLETTER_FREQUENCY = dict(zip(LETTERS, [random.randint(1, 100) for _ in LETTERS]))\n\ndef find_most_frequent_letters(x):\n    \"\"\"\n    Find the n most frequent letters in a dictionary, x, where the keys are letters \n    and the values are their frequencies.\n\n    Parameters:\n    x (dict): The dictionary of letter frequencies.\n\n    Returns:\n    list: The n most frequent letters.\n\n    Requirements:\n    - heapq\n    - collections\n\n    Example:\n    >>> find_most_frequent_letters(LETTER_FREQUENCY)\n    \"\"\"\n    counter = collections.Counter(x)\n    most_frequent = heapq.nlargest(3, counter.keys(), key=counter.get)\n\n    return most_frequent\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nitertools, collections, random\n```python\nimport itertools\nimport collections\nimport random\n\n# Constants\nLETTERS = 'abcdefghijklmnopqrstuvwxyz'\nLETTER_FREQUENCY = dict(zip(LETTERS, [random.randint(1, 100) for _ in LETTERS]))\n\ndef find_min_length_subsequence(x):\n    \"\"\"\n    Find the subsequence of a dictionary, x, with the minimum total length where \n    the keys are letters and the values are their lengths.\n\n    Parameters:\n    x (dict): The dictionary of letter lengths.\n\n    Returns:\n    list: The subsequence with the minimum total length.\n\n    Requirements:\n    - itertools\n    - collections\n    - random\n\n    Example:\n    >>> find_min_length_subsequence(LETTER_FREQUENCY)\n    \"\"\"\n    min_length = float('inf')\n    min_subseq = []\n\n    for r in range(1, len(x) + 1):\n        for subseq in itertools.combinations(x.items(), r):\n            length = sum(length for letter, length in subseq)\n            if length < min_length:\n                min_length = length\n                min_subseq = [letter for letter, length in subseq]\n\n    return min_subseq\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nheapq, collections, random\n```python\nimport heapq\nimport collections\nimport random\n\n# Constants\nLETTERS = 'abcdefghijklmnopqrstuvwxyz'\nLETTER_VALUES = dict(zip(LETTERS, [random.randint(1, 100) for _ in LETTERS]))\n\ndef find_highest_value_pairs(x):\n    \"\"\"\n    Find the pair of keys in a dictionary, x, that have the highest sum of their \n    values.\n\n    Parameters:\n    x (dict): The dictionary of key-value pairs.\n\n    Returns:\n    tuple: The pair of keys with the highest sum of their values.\n\n    Requirements:\n    - heapq\n    - collections\n    - random\n\n    Example:\n    >>> find_highest_value_pairs(LETTER_VALUES)\n    \"\"\"\n    pairs = list(itertools.combinations(x.keys(), 2))\n    max_pair = max(pairs, key=lambda pair: x[pair[0]] + x[pair[1]])\n\n    return max_pair\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nitertools, collections, random\n```python\nimport itertools\nimport collections\nimport random\n\n# Constants\nLETTERS = 'abcdefghijklmnopqrstuvwxyz'\nLETTER_WEIGHTS = dict(zip(LETTERS, [random.randint(1, 100) for _ in LETTERS]))\n\ndef find_max_weighted_substring(x):\n    \"\"\"\n    Find the substring in a dictionary, x, that has the maximum total weight. The \n    keys are characters and the values are their weights.\n\n    Parameters:\n    x (dict): The dictionary of character weights.\n\n    Returns:\n    str: The substring with the highest weight.\n\n    Requirements:\n    - itertools\n    - collections\n    - random\n\n    Example:\n    >>> find_max_weighted_substring(LETTER_WEIGHTS)\n    \"\"\"\n    max_weight = -1\n    max_substr = ''\n\n    for start in range(len(x)):\n        for end in range(start + 1, len(x) + 1):\n            substr = x[start:end]\n            weight = sum(LETTER_WEIGHTS[c] for c in substr)\n            if weight > max_weight:\n                max_weight = weight\n                max_substr = substr\n\n    return max_substr\n```\n"}
{"seed": "{\"task_id\": 17021863, \"prompt\": \"def f_17021863(file):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"file.seek(0)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    with open ('a.txt', 'w') as f:\\n        f.write('kangaroo\\\\nkoala\\\\noxford\\\\n')\\n    f = open('a.txt', 'r')\\n    f.read()\\n    candidate(f)\\n    assert f.readline() == 'kangaroo\\\\n'\\n\"], \"entry_point\": \"f_17021863\", \"intent\": \"Put the curser at beginning of the file\", \"library\": []}", "generation": "Scenario 1:\nos, random, string, json\n```python\nimport os\nimport random\nimport string\nimport json\n\n# Constants\nLETTERS = string.ascii_letters\n\ndef random_directory_json_files(directory):\n    \"\"\"\n    Generate random files in a directory with json content, and then reset the\n    cursor to the beginning of each file.\n\n    Parameters:\n    directory (str): The directory in which to generate the files.\n\n    Returns:\n    None\n\n    Requirements:\n    - os\n    - random\n    - string\n    - json\n\n    Example:\n    >>> random_directory_json_files('/path/to/directory')\n    \"\"\"\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    for i in range(10):\n        filename = \"\".join(random.choice(LETTERS) for _ in range(10)) + \".json\"\n        filepath = os.path.join(directory, filename)\n\n        with open(filepath, 'w') as file:\n            json.dump({'number': random.randint(1, 100)}, file)\n            file.seek(0)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, sys, csv\n```python\nimport os\nimport sys\nimport csv\n\ndef rewrite_csv_file(filename):\n    \"\"\"\n    Read a CSV file, reverse the order of rows, and write the reversed rows back\n    to the file. Then reset the cursor to the beginning of the file.\n\n    Parameters:\n    filename (str): The name of the CSV file.\n\n    Returns:\n    None\n\n    Requirements:\n    - os\n    - sys\n    - csv\n\n    Example:\n    >>> rewrite_csv_file('file.csv')\n    \"\"\"\n    try:\n        with open(filename, 'r+') as file:\n            reader = csv.reader(file)\n            rows = list(reader)\n            file.seek(0)\n            file.truncate()\n\n            writer = csv.writer(file)\n            writer.writerows(reversed(rows))\n\n            file.seek(0)\n    except Exception as e:\n        print(f\"An error occurred: {e}\", file=sys.stderr)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, json, random\n```python\nimport os\nimport json\nimport random\n\ndef random_json_files(directory, n_files):\n    \"\"\"\n    Generate n random JSON files in a given directory, write a random integer to each\n    file, and then reset the cursor to the beginning of each file.\n\n    Parameters:\n    directory (str): The directory in which to generate the files.\n    n_files (int): The number of files to generate.\n\n    Returns:\n    None\n\n    Requirements:\n    - os\n    - json\n    - random\n\n    Example:\n    >>> random_json_files('/path/to/directory', 5)\n    \"\"\"\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    for i in range(n_files):\n        filename = os.path.join(directory, f\"file_{i+1}.json\")\n\n        with open(filename, 'w') as file:\n            json.dump({'random_number': random.randint(1, 100)}, file)\n            file.seek(0)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, os\n```python\nimport pandas as pd\nimport os\n\ndef rewrite_csv_by_pandas(filename):\n    \"\"\"\n    Read a CSV file by pandas, reverse the order of rows, and write the reversed rows back\n    to the file. Then reset the cursor to the beginning of the file.\n\n    Parameters:\n    filename (str): The name of the CSV file.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - os\n\n    Example:\n    >>> rewrite_csv_by_pandas('file.csv')\n    \"\"\"\n    if not os.path.exists(filename):\n        print(f\"No such file: {filename}\")\n        return\n\n    df = pd.read_csv(filename)\n    df = df.iloc[::-1]\n    df.to_csv(filename, index=False)\n\n    with open(filename, 'r+') as file:\n        file.seek(0)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, random, string\n```python\nimport os\nimport random\nimport string\n\n# Constants\nLETTERS = string.ascii_letters\n\ndef random_text_files(directory, n_files):\n    \"\"\"\n    Generate n random text files in a given directory, write a random string to each\n    file, and then reset the cursor to the beginning of each file.\n\n    Parameters:\n    directory (str): The directory in which to generate the files.\n    n_files (int): The number of files to generate.\n\n    Returns:\n    None\n\n    Requirements:\n    - os\n    - random\n    - string\n\n    Example:\n    >>> random_text_files('/path/to/directory', 5)\n    \"\"\"\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    for i in range(n_files):\n        filename = os.path.join(directory, f\"file_{i+1}.txt\")\n\n        with open(filename, 'w') as file:\n            file.write(\"\".join(random.choice(LETTERS) for _ in range(100)))\n            file.seek(0)\n```"}
{"seed": "{\"task_id\": 38152389, \"prompt\": \"def f_38152389(df):\\n\\t\", \"suffix\": \"\\n\\treturn df\", \"canonical_solution\": \"df['c'] = np.where(df['a'].isnull, df['b'], df['a'])\", \"test_start\": \"\\nimport numpy as np \\nimport pandas as pd \\n\\ndef check(candidate):\", \"test\": [\"\\n    df = pd.DataFrame({'a': [1,2,3], 'b': [0,0,0]})\\n    assert np.allclose(candidate(df), pd.DataFrame({'a': [1,2,3], 'b': [0,0,0], 'c': [0,0,0]}))\\n\", \"\\n    df = pd.DataFrame({'a': [0,2,3], 'b': [4,5,6]})\\n    assert np.allclose(candidate(df), pd.DataFrame({'a': [0,2,3], 'b': [4,5,6], 'c': [4,5,6]}))\\n\"], \"entry_point\": \"f_38152389\", \"intent\": \"combine values from column 'b' and column 'a' of dataframe `df`  into column 'c' of datafram `df`\", \"library\": [\"numpy\", \"pandas\"]}", "generation": "Scenario 1: \npandas, numpy, random, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import choice\nimport seaborn as sns\n\n# Constants\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nMATCHES = list(range(1, 21))\n\ndef generate_match_scores(df):\n    \"\"\"\n    Generate a DataFrame containing match scores for a series of matches between different teams.\n    Each row of the input DataFrame represents a match, and contains two teams and their respective scores.\n    The function adds a 'winner' column to the DataFrame, which is the team with the highest score in each match.\n    If the scores are equal, the winner is chosen randomly.\n    \n    Parameters:\n    df (pandas.DataFrame): The input DataFrame with columns 'team1', 'team2', 'score1', 'score2'.\n    \n    Returns:\n    pandas.DataFrame: The DataFrame with the added 'winner' column.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - random\n    - seaborn\n\n    Example:\n    >>> df = pd.DataFrame({'team1': np.random.choice(TEAMS, 20),\n    ...                    'team2': np.random.choice(TEAMS, 20),\n    ...                    'score1': np.random.randint(0, 10, 20),\n    ...                    'score2': np.random.randint(0, 10, 20)})\n    >>> df = generate_match_scores(df)\n    >>> print(df)\n    >>> sns.countplot(x='winner', data=df)\n    \"\"\"\n    def determine_winner(row):\n        if row['score1'] > row['score2']:\n            return row['team1']\n        elif row['score1'] < row['score2']:\n            return row['team2']\n        else:\n            return choice([row['team1'], row['team2']])\n\n    df['winner'] = df.apply(determine_winner, axis=1)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, pandas, scipy.stats, matplotlib.pyplot\n```python\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import linregress\nimport matplotlib.pyplot as plt\n\n# Constants\nNUM_SAMPLES = 100\n\ndef analyze_relationship(df):\n    \"\"\"\n    Analyze the relationship between two variables in a DataFrame. \n    The function performs a linear regression on the two variables and adds a 'predicted' column to the DataFrame.\n    It also generates a scatter plot of the two variables with the regression line.\n    \n    Parameters:\n    df (pandas.DataFrame): The input DataFrame with columns 'var1', 'var2'.\n    \n    Returns:\n    pandas.DataFrame: The DataFrame with the added 'predicted' column.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame({'var1': np.random.randn(NUM_SAMPLES),\n    ...                    'var2': np.random.randn(NUM_SAMPLES)})\n    >>> df = analyze_relationship(df)\n    >>> print(df)\n    >>> plt.scatter(df['var1'], df['var2'])\n    >>> plt.plot(df['var1'], df['predicted'], color='red')\n    \"\"\"\n    regression = linregress(df['var1'], df['var2'])\n    df['predicted'] = regression.slope * df['var1'] + regression.intercept\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, json, os, shutil\n```python\nimport pandas as pd\nimport json\nimport os\nimport shutil\n\n# Constants\nPATH = './data/'\n\ndef process_json_files(path):\n    \"\"\"\n    Process JSON files in a directory. The function reads each JSON file into a DataFrame and adds a 'source' column\n    indicating the file name. The processed files are then moved to a 'processed' subdirectory.\n    \n    Parameters:\n    path (str): The path of the directory containing the JSON files.\n    \n    Returns:\n    pandas.DataFrame: A DataFrame containing the data from all processed files.\n    \n    Requirements:\n    - pandas\n    - json\n    - os\n    - shutil\n\n    Example:\n    >>> df = process_json_files(PATH)\n    >>> print(df)\n    \"\"\"\n    df = pd.DataFrame()\n    processed_path = os.path.join(path, 'processed')\n\n    if not os.path.exists(processed_path):\n        os.makedirs(processed_path)\n\n    for filename in os.listdir(path):\n        if filename.endswith('.json'):\n            file_path = os.path.join(path, filename)\n            with open(file_path, 'r') as file:\n                data = json.load(file)\n                temp_df = pd.DataFrame(data)\n                temp_df['source'] = filename\n                df = pd.concat([df, temp_df])\n\n            shutil.move(file_path, processed_path)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, itertools, collections\n```python\nimport pandas as pd\nimport numpy as np\nfrom itertools import combinations\nfrom collections import Counter\n\n# Constants\nITEMS = ['item1', 'item2', 'item3', 'item4', 'item5']\nITEM_COMBINATIONS = list(combinations(ITEMS, 2))\n\ndef calculate_item_combinations(df):\n    \"\"\"\n    Calculate the frequency of combinations of items in a DataFrame. \n    The function adds a 'combination' column to the DataFrame, which is the combination of items in each row.\n    It then calculates the frequency of each combination.\n    \n    Parameters:\n    df (pandas.DataFrame): The input DataFrame with columns 'item1', 'item2', 'item3', 'item4', 'item5'.\n    \n    Returns:\n    dict: A dictionary containing the frequency of each combination.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - itertools\n    - collections\n\n    Example:\n    >>> df = pd.DataFrame(np.random.choice(ITEMS, (20, 5)), columns=ITEMS)\n    >>> combination_freq = calculate_item_combinations(df)\n    >>> print(combination_freq)\n    \"\"\"\n    df['combination'] = df.apply(lambda row: tuple(sorted(row)), axis=1)\n\n    combination_freq = Counter(df['combination'])\n\n    return dict(combination_freq)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, sklearn.preprocessing, matplotlib.pyplot\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\n# Constants\nFEATURES = ['feature1', 'feature2', 'feature3']\n\ndef standardize_features(df):\n    \"\"\"\n    Standardize the features in a DataFrame. \n    The function applies standard scaling to the features and generates a boxplot of the standardized features.\n    \n    Parameters:\n    df (pandas.DataFrame): The input DataFrame.\n    \n    Returns:\n    pandas.DataFrame: The DataFrame with the standardized features.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.preprocessing\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame(np.random.randn(20, 3), columns=FEATURES)\n    >>> df = standardize_features(df)\n    >>> print(df)\n    >>> df.boxplot()\n    \"\"\"\n    scaler = StandardScaler()\n    df[FEATURES] = scaler.fit_transform(df[FEATURES])\n\n    return df\n```\n"}
{"seed": "{\"task_id\": 4175686, \"prompt\": \"def f_4175686(d):\\n\\t\", \"suffix\": \"\\n\\treturn d\", \"canonical_solution\": \"del d['ele']\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate({\\\"ale\\\":1, \\\"ele\\\": 2}) == {\\\"ale\\\": 1}\\n\"], \"entry_point\": \"f_4175686\", \"intent\": \"remove key 'ele' from dictionary `d`\", \"library\": []}", "generation": "Scenario 1: \npandas, json, os\n```python\nimport pandas as pd\nimport json\nimport os\n\n# Constants\nFILE_PATH = '/data/sample.json'\n\ndef process_json_file(file_path=FILE_PATH):\n    \"\"\"\n    Load a JSON file into a pandas DataFrame, remove a specific key from each object, \n    and write the processed DataFrame back to a JSON file.\n    \n    Parameters:\n    file_path (str): The path to the JSON file. Default is FILE_PATH constant.\n    \n    Returns:\n    DataFrame: A pandas DataFrame representation of the processed JSON data.\n    \n    Requirements:\n    - pandas\n    - json\n    - os\n    \n    Example:\n    >>> df = process_json_file()\n    \"\"\"\n    with open(file_path, 'r') as file:\n        data = json.load(file)\n\n    df = pd.DataFrame(data)\n    df.drop('ele', axis=1, inplace=True)\n\n    with open(file_path, 'w') as file:\n        file.write(df.to_json(orient='records'))\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncollections, itertools, operator\n```python\nfrom collections import Counter\nfrom itertools import chain\nfrom operator import itemgetter\n\n# Constants\nNESTED_DICT = {\n    'dict1': {'ale': 1, 'ele': 2, 'ile': 3},\n    'dict2': {'ele': 4, 'ole': 5, 'ule': 6},\n    'dict3': {'ile': 7, 'ale': 8, 'ele': 9}\n}\n\ndef aggregate_values(nested_dict=NESTED_DICT):\n    \"\"\"\n    Aggregate the values of the same keys from a nested dictionary and remove the 'ele' key.\n    \n    Parameters:\n    nested_dict (dict): The nested dictionary. Default is NESTED_DICT constant.\n    \n    Returns:\n    dict: A dictionary with aggregated values.\n    \n    Requirements:\n    - collections.Counter\n    - itertools.chain\n    - operator.itemgetter\n    \n    Example:\n    >>> result = aggregate_values()\n    \"\"\"\n    counter = Counter()\n    for sub_dict in nested_dict.values():\n        counter.update(sub_dict)\n\n    counter.pop('ele', None)\n\n    return dict(counter)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nyaml, os\n```python\nimport yaml\nimport os\n\n# Constants\nYAML_PATH = 'config.yaml'\n\ndef remove_yaml_key(yaml_path=YAML_PATH):\n    \"\"\"\n    Read a YAML file, remove a specific key from the data, and then write the modified data back into the YAML file.\n    \n    Parameters:\n    yaml_path (str): The path to the YAML file. Default is YAML_PATH constant.\n    \n    Returns:\n    dict: A dictionary representation of the modified YAML data.\n    \n    Requirements:\n    - yaml\n    - os\n    \n    Example:\n    >>> yaml_data = remove_yaml_key()\n    \"\"\"\n    with open(yaml_path, 'r') as file:\n        data = yaml.safe_load(file)\n\n    data.pop('ele', None)\n\n    with open(yaml_path, 'w') as file:\n        yaml.safe_dump(data, file)\n\n    return data\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy\n```python\nimport pandas as pd\nimport numpy as np\n\n# Constants\nDF = pd.DataFrame({\n    'ale': np.random.rand(10),\n    'ele': np.random.rand(10),\n    'ile': np.random.rand(10)\n})\n\ndef process_dataframe(df=DF):\n    \"\"\"\n    Process a pandas DataFrame by removing a specific column and returning the updated DataFrame.\n    \n    Parameters:\n    df (DataFrame): The pandas DataFrame to process. Default is DF constant.\n    \n    Returns:\n    DataFrame: The processed pandas DataFrame.\n    \n    Requirements:\n    - pandas\n    - numpy\n    \n    Example:\n    >>> result = process_dataframe()\n    \"\"\"\n    df.drop('ele', axis=1, inplace=True)\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\njson, requests\n```python\nimport json\nimport requests\n\n# Constants\nURL = 'http://api.example.com/data'\n\ndef process_api_data(url=URL):\n    \"\"\"\n    Send a GET request to an API, remove a specific key from the returned JSON data, and return the processed data.\n    \n    Parameters:\n    url (str): The API URL. Default is URL constant.\n    \n    Returns:\n    dict: The processed JSON data.\n    \n    Requirements:\n    - json\n    - requests\n    \n    Example:\n    >>> data = process_api_data()\n    \"\"\"\n    response = requests.get(url)\n    data = response.json()\n\n    data.pop('ele', None)\n\n    return data\n```"}
{"seed": "{\"task_id\": 11574195, \"prompt\": \"def f_11574195():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"['it'] + ['was'] + ['annoying']\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate() == ['it', 'was', 'annoying']\\n\"], \"entry_point\": \"f_11574195\", \"intent\": \"merge list `['it']` and list `['was']` and list `['annoying']` into one list\", \"library\": []}", "generation": "Scenario 1: \ncollections, itertools, random\n```python\nfrom collections import Counter\nfrom itertools import chain\nfrom random import randint, choice\n\n# Constants\nLIST_OF_LISTS = [['it'], ['was'], ['annoying'], ['to'], ['code'], ['in'], ['python']]\n\ndef random_sublist_merge():\n    \"\"\"\n    Merge randomly selected sublists from a list of lists into one list and return a counter of the elements.\n    \n    Returns:\n    collections.Counter: Counter object with the counts of the elements in the merged list.\n    \n    Requirements:\n    - collections.Counter\n    - itertools.chain\n    - random.randint\n    - random.choice\n    \n    Example:\n    >>> random_sublist_merge()\n    Counter({'it': 1, 'was': 1, 'python': 1})\n    \"\"\"\n    num_sublists = randint(1, len(LIST_OF_LISTS))\n    selected_sublists = [choice(LIST_OF_LISTS) for _ in range(num_sublists)]\n    merged_list = list(chain.from_iterable(selected_sublists))\n    return Counter(merged_list)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nnumpy, matplotlib, random\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nLISTS_TO_MERGE = [['it'], ['was'], ['annoying'], ['to'], ['code'], ['in'], ['python']]\n\ndef plot_random_merge_count():\n    \"\"\"\n    Merges a random number of lists from a predefined set of lists and plots a histogram \n    of the counts of the different elements in the merged list.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - random.randint\n\n    Example:\n    >>> plot_random_merge_count()\n    \"\"\"\n    num_lists = randint(1, len(LISTS_TO_MERGE))\n    indices = np.random.choice(len(LISTS_TO_MERGE), num_lists, replace=False)\n    merged_list = np.concatenate(np.array(LISTS_TO_MERGE)[indices])\n    unique, counts = np.unique(merged_list, return_counts=True)\n\n    plt.bar(unique, counts)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnltk, random, string\n```python\nfrom nltk.corpus import words\nfrom random import shuffle\nimport string\n\n# Constants\nLISTS_TO_MERGE = [[c] for c in string.ascii_lowercase]\n\ndef scramble_and_merge():\n    \"\"\"\n    Scrambles the order of a predefined set of lists, merges them into one list, and \n    counts the number of English words that can be formed from the characters in the list.\n\n    Returns:\n    int: The number of English words that can be formed.\n    \n    Requirements:\n    - nltk.corpus.words\n    - random.shuffle\n    - string\n\n    Example:\n    >>> scramble_and_merge()\n    370104\n    \"\"\"\n    shuffle(LISTS_TO_MERGE)\n    merged_list = [item for sublist in LISTS_TO_MERGE for item in sublist]\n    count = sum(1 for word in words.words() if all(c in merged_list for c in word))\n    return count\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, sklearn.preprocessing\n```python\nimport numpy as np\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Constants\nLISTS_TO_MERGE = [['it'], ['was'], ['annoying'], ['to'], ['code'], ['in'], ['python']]\n\ndef one_hot_encode_merged():\n    \"\"\"\n    Merges a predefined set of lists into one list and one-hot encodes the elements of the list.\n\n    Returns:\n    numpy.array: The one-hot encoding of the merged list.\n    \n    Requirements:\n    - numpy\n    - sklearn.preprocessing.OneHotEncoder\n\n    Example:\n    >>> one_hot_encode_merged()\n    array([[1., 0., 0., 0., 0., 0., 0.],\n           [0., 1., 0., 0., 0., 0., 0.],\n           [0., 0., 1., 0., 0., 0., 0.],\n           [0., 0., 0., 1., 0., 0., 0.],\n           [0., 0., 0., 0., 1., 0., 0.],\n           [0., 0., 0., 0., 0., 1., 0.],\n           [0., 0., 0., 0., 0., 0., 1.]])\n    \"\"\"\n    merged_list = np.array([item for sublist in LISTS_TO_MERGE for item in sublist]).reshape(-1, 1)\n    encoder = OneHotEncoder(sparse=False)\n    one_hot = encoder.fit_transform(merged_list)\n    return one_hot\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnumpy, scipy.stats\n```python\nimport numpy as np\nfrom scipy.stats import mode\n\n# Constants\nLISTS_TO_MERGE = [['it'], ['was'], ['annoying'], ['to'], ['code'], ['in'], ['python']]\n\ndef find_mode_after_merge():\n    \"\"\"\n    Merges a predefined set of lists into one list and finds the mode of the elements in the list.\n\n    Returns:\n    tuple: The mode and count of the mode in the merged list.\n    \n    Requirements:\n    - numpy\n    - scipy.stats.mode\n\n    Example:\n    >>> find_mode_after_merge()\n    (array(['annoying'], dtype='<U8'), array([1]))\n    \"\"\"\n    merged_list = np.array([item for sublist in LISTS_TO_MERGE for item in sublist])\n    mode_value, mode_count = mode(merged_list)\n    return mode_value, mode_count\n```"}
{"seed": "{\"task_id\": 587647, \"prompt\": \"def f_587647(x):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"str(int(x) + 1).zfill(len(x))\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(\\\"001\\\") == \\\"002\\\"\\n\", \"\\n    assert candidate(\\\"100\\\") == \\\"101\\\"\\n\"], \"entry_point\": \"f_587647\", \"intent\": \"increment a value with leading zeroes in a number `x`\", \"library\": []}", "generation": "Scenario 1: \nnumpy, pandas, matplotlib, random\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nSAMPLE_SIZE = 1000\nCATEGORIES = ['A', 'B', 'C', 'D', 'E']\n\ndef simulate_and_plot_distribution():\n    \"\"\"\n    Simulate a distribution of numerical values with leading zeroes across different \n    categories, increment each value by one, and plot the original and incremented \n    distributions for comparison.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the original and incremented numerical values.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib\n    - random\n    \n    Example:\n    >>> df = simulate_and_plot_distribution()\n    >>> df.plot(kind='bar')\n    \"\"\"\n    data = []\n    for _ in range(SAMPLE_SIZE):\n        category = CATEGORIES[randint(0, len(CATEGORIES)-1)]\n        original_value = str(randint(1, 100)).zfill(3)\n        incremented_value = str(int(original_value) + 1).zfill(3)\n        data.append([category, original_value, incremented_value])\n    \n    df = pd.DataFrame(data, columns=['Category', 'Original Value', 'Incremented Value'])\n    df['Original Value'] = df['Original Value'].astype(int)\n    df['Incremented Value'] = df['Incremented Value'].astype(int)\n    \n    plt.figure(figsize=(10,5))\n    \n    plt.subplot(1, 2, 1)\n    df.groupby('Category')['Original Value'].mean().plot(kind='bar')\n    plt.title('Original Values')\n    \n    plt.subplot(1, 2, 2)\n    df.groupby('Category')['Incremented Value'].mean().plot(kind='bar')\n    plt.title('Incremented Values')\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \ncollections, itertools, random\n```python\nfrom collections import Counter\nimport itertools\nfrom random import randint\n\n# Constants\nCATEGORIES = ['A', 'B', 'C', 'D', 'E']\nSAMPLE_SIZE = 1000\n\ndef increment_values_and_count():\n    \"\"\"\n    Generate a list of numerical values with leading zeroes in different categories, \n    increment each value by one, and return the count of original and incremented values \n    for each category.\n    \n    Returns:\n    dict: A dictionary with counts of original and incremented values for each category.\n    \n    Requirements:\n    - collections.Counter\n    - itertools\n    - random\n    \n    Example:\n    >>> print(increment_values_and_count())\n    \"\"\"\n    data = []\n    for _ in range(SAMPLE_SIZE):\n        category = CATEGORIES[randint(0, len(CATEGORIES)-1)]\n        original_value = str(randint(1, 100)).zfill(3)\n        incremented_value = str(int(original_value) + 1).zfill(3)\n        data.append((category, original_value, incremented_value))\n    \n    category_counts = Counter(category for category, _, _ in data)\n    original_value_counts = Counter(original_value for _, original_value, _ in data)\n    incremented_value_counts = Counter(incremented_value for _, _, incremented_value in data)\n    \n    return {\n        'Category Counts': category_counts,\n        'Original Value Counts': original_value_counts,\n        'Incremented Value Counts': incremented_value_counts\n    }\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \npandas, numpy, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nROWS = 100\n\ndef generate_incremented_dataframe():\n    \"\"\"\n    Generate a DataFrame filled with random three-digit numbers, increment each number by one, \n    and plot the distribution of original and incremented numbers.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the original and incremented numbers.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib\n    \n    Example:\n    >>> df = generate_incremented_dataframe()\n    >>> df.plot(kind='hist', alpha=0.5)\n    \"\"\"\n    original_numbers = np.random.randint(100, 999, size=ROWS)\n    incremented_numbers = original_numbers + 1\n    \n    df = pd.DataFrame({\n        'Original Numbers': original_numbers,\n        'Incremented Numbers': incremented_numbers\n    })\n    \n    plt.hist([df['Original Numbers'], df['Incremented Numbers']], bins=20, alpha=0.5, \n             label=['Original Numbers', 'Incremented Numbers'])\n    plt.legend()\n    \n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nrandom, csv\n```python\nimport csv\nfrom random import randint\n\n# Constants\nSAMPLE_SIZE = 1000\nCATEGORIES = ['A', 'B', 'C', 'D', 'E']\nOUTPUT_FILE = 'output.csv'\n\ndef generate_and_write_csv():\n    \"\"\"\n    Generate a list of numerical values with leading zeroes in different categories, \n    increment each value by one, and write the results to a CSV file.\n    \n    Returns:\n    str: The name of the output CSV file.\n    \n    Requirements:\n    - csv\n    - random\n    \n    Example:\n    >>> print(generate_and_write_csv())\n    \"\"\"\n    data = []\n    for _ in range(SAMPLE_SIZE):\n        category = CATEGORIES[randint(0, len(CATEGORIES)-1)]\n        original_value = str(randint(1, 100)).zfill(3)\n        incremented_value = str(int(original_value) + 1).zfill(3)\n        data.append([category, original_value, incremented_value])\n    \n    with open(OUTPUT_FILE, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(['Category', 'Original Value', 'Incremented Value'])\n        writer.writerows(data)\n    \n    return OUTPUT_FILE\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \npandas, matplotlib\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nCATEGORIES = ['A', 'B', 'C', 'D', 'E']\nSAMPLE_SIZE = 1000\n\ndef generate_and_plot_dataframe():\n    \"\"\"\n    Generate a DataFrame with categories and numerical values with leading zeroes, \n    increment each value by one, and plot the counts of original and incremented values \n    for each category.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the original and incremented values.\n    \n    Requirements:\n    - pandas\n    - matplotlib\n    \n    Example:\n    >>> df = generate_and_plot_dataframe()\n    >>> df.groupby('Category')['Incremented Value'].nunique().plot(kind='bar')\n    \"\"\"\n    data = {\n        'Category': np.random.choice(CATEGORIES, SAMPLE_SIZE),\n        'Original Value': [str(i).zfill(3) for i in np.random.randint(100, 999, SAMPLE_SIZE)]\n    }\n    \n    df = pd.DataFrame(data)\n    df['Incremented Value'] = (df['Original Value'].astype(int) + 1).astype(str).str.zfill(3)\n    \n    df.groupby('Category')['Original Value'].nunique().plot(kind='bar', alpha=0.5, \n                                                            label='Original Values')\n    df.groupby('Category')['Incremented Value'].nunique().plot(kind='bar', alpha=0.5, \n                                                               color='red', label='Incremented Values')\n    plt.legend()\n    \n    return df\n```"}
{"seed": "{\"task_id\": 17315881, \"prompt\": \"def f_17315881(df):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"all(df.index[:-1] <= df.index[1:])\", \"test_start\": \"\\nimport pandas as pd \\n\\ndef check(candidate):\", \"test\": [\"\\n    df1 = pd.DataFrame({'a': [1,2], 'bb': [0,2]})\\n    assert candidate(df1) == True\\n\", \"\\n    df2 = pd.DataFrame({'a': [1,2,3,4,5], 'bb': [0,3,5,7,9]})\\n    shuffled = df2.sample(frac=3, replace=True)\\n    assert candidate(shuffled) == False\\n\", \"\\n    df = pd.DataFrame([[1, 2], [5, 4]], columns=['a', 'b'])\\n    assert candidate(df)\\n\"], \"entry_point\": \"f_17315881\", \"intent\": \"check if a pandas dataframe `df`'s index is sorted\", \"library\": [\"pandas\"]}", "generation": "Scenario 1:\npandas, numpy, matplotlib.pyplot, seaborn, random\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\nROWS = 100\n\ndef generate_and_analyze_dataframe():\n    \"\"\"\n    Generate a pandas DataFrame with random numeric values and perform some basic analysis:\n    - Check if the DataFrame's index is sorted\n    - Calculate basic statistics (mean, median, max, min, std deviation)\n    - Visualize the distribution of values in each column with histograms\n\n    Returns:\n    dict: A dictionary with basic statistics for each column.\n    DataFrame: The generated DataFrame.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - seaborn\n    - random\n\n    Example:\n    >>> stats, df = generate_and_analyze_dataframe()\n    >>> print(stats)\n    >>> df.hist(bins=50, figsize=(20,15))\n    \"\"\"\n    # Generate DataFrame\n    df = pd.DataFrame(np.random.rand(ROWS, len(COLUMNS)) * 100, columns=COLUMNS)\n    \n    # Check if index is sorted\n    is_sorted = all(df.index[:-1] <= df.index[1:])\n    print(f'Index is sorted: {is_sorted}')\n\n    # Basic statistics\n    stats = df.describe().to_dict()\n\n    return stats, df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, sklearn.preprocessing, matplotlib.pyplot\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\nROWS = 200\nCOLUMNS = ['X1', 'X2', 'X3', 'X4']\n\ndef generate_and_standardize_dataframe():\n    \"\"\"\n    Generate a pandas DataFrame with random numeric values and standardize it using\n    sklearn's StandardScaler. Then, check if the DataFrame's index is sorted and\n    plot the original and standardized data for comparison.\n\n    Returns:\n    DataFrame: The original DataFrame.\n    DataFrame: The standardized DataFrame.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.preprocessing\n    - matplotlib.pyplot\n\n    Example:\n    >>> df_original, df_standardized = generate_and_standardize_dataframe()\n    >>> print(all(df_standardized.index[:-1] <= df_standardized.index[1:]))  # Check if index is sorted\n    >>> fig, axs = plt.subplots(2)\n    >>> df_original.plot(ax=axs[0])\n    >>> df_standardized.plot(ax=axs[1])\n    \"\"\"\n    # Generate DataFrame\n    df = pd.DataFrame(np.random.rand(ROWS, len(COLUMNS)) * 100, columns=COLUMNS)\n    \n    # Standardize data\n    scaler = StandardScaler()\n    df_standardized = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n\n    return df, df_standardized\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, seaborn, scipy.stats\n```python\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom scipy import stats\n\nROWS = 1000\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef generate_dataframe_and_test_normality():\n    \"\"\"\n    Generate a pandas DataFrame with random numeric values, then check if the DataFrame's index is sorted \n    and test if the data in each column is normally distributed using the Shapiro-Wilk test.\n\n    Returns:\n    dict: A dictionary with p-values from the Shapiro-Wilk test for each column.\n    DataFrame: The generated DataFrame.\n\n    Requirements:\n    - pandas\n    - numpy\n    - seaborn\n    - scipy.stats\n\n    Example:\n    >>> p_values, df = generate_dataframe_and_test_normality()\n    >>> print(all(df.index[:-1] <= df.index[1:]))  # Check if index is sorted\n    >>> print(p_values)\n    \"\"\"\n    # Generate DataFrame\n    df = pd.DataFrame(np.random.rand(ROWS, len(COLUMNS)) * 100, columns=COLUMNS)\n    \n    # Test normality\n    p_values = {col: stats.shapiro(df[col])[1] for col in df.columns}\n\n    return p_values, df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, matplotlib.pyplot, sklearn.linear_model\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\nROWS = 100\nCOLUMNS = ['X', 'Y']\n\ndef generate_dataframe_and_fit_linear_model():\n    \"\"\"\n    Generate a pandas DataFrame with random numeric values, then check if the DataFrame's index is sorted \n    and fit a linear model to the data using sklearn's LinearRegression.\n\n    Returns:\n    LinearRegression: The fitted linear model.\n    DataFrame: The generated DataFrame.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.linear_model\n\n    Example:\n    >>> model, df = generate_dataframe_and_fit_linear_model()\n    >>> print(all(df.index[:-1] <= df.index[1:]))  # Check if index is sorted\n    >>> plt.scatter(df['X'], df['Y'])\n    >>> plt.plot(df['X'], model.predict(df[['X']]), color='red')\n    \"\"\"\n    # Generate DataFrame\n    df = pd.DataFrame(np.random.rand(ROWS, len(COLUMNS)) * 100, columns=COLUMNS)\n    \n    # Fit linear model\n    model = LinearRegression().fit(df[['X']], df['Y'])\n\n    return model, df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, seaborn, sklearn.cluster\n```python\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.cluster import KMeans\n\nROWS = 500\nCOLUMNS = ['A', 'B']\n\ndef generate_dataframe_and_perform_clustering():\n    \"\"\"\n    Generate a pandas DataFrame with random numeric values, then check if the DataFrame's index is sorted \n    and perform KMeans clustering on the data.\n\n    Returns:\n    np.array: The labels from the KMeans clustering.\n    DataFrame: The generated DataFrame.\n\n    Requirements:\n    - pandas\n    - numpy\n    - seaborn\n    - sklearn.cluster\n\n    Example:\n    >>> labels, df = generate_dataframe_and_perform_clustering()\n    >>> print(all(df.index[:-1] <= df.index[1:]))  # Check if index is sorted\n    >>> sns.scatterplot(data=df, x='A', y='B', hue=labels, palette='viridis')\n    \"\"\"\n    # Generate DataFrame\n    df = pd.DataFrame(np.random.rand(ROWS, len(COLUMNS)) * 100, columns=COLUMNS)\n    \n    # Perform clustering\n    kmeans = KMeans(n_clusters=3, random_state=0).fit(df)\n    labels = kmeans.labels_\n\n    return labels, df\n```"}
{"seed": "{\"task_id\": 16296643, \"prompt\": \"def f_16296643(t):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"list(t)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate((0, 1, 2)) == [0,1,2]\\n\", \"\\n    assert candidate(('a', [], 100)) == ['a', [], 100]\\n\"], \"entry_point\": \"f_16296643\", \"intent\": \"Convert tuple `t` to list\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D']\n\ndef analyze_data(tuples_list):\n    \"\"\"\n    Convert a list of tuples into a pandas DataFrame, perform statistical analysis \n    and plot the distribution of each column.\n    \n    Parameters:\n    tuples_list (list): The list of tuples.\n    \n    Returns:\n    DataFrame: A pandas DataFrame.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> analyze_data([(1, 2, 3, 4), (5, 6, 7, 8), (9, 10, 11, 12)])\n    \"\"\"\n    df = pd.DataFrame(tuples_list, columns=COLUMNS)\n    print(df.describe())\n    df.hist()\n    plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nrandom, numpy, matplotlib\n```python\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nN = 100\n\ndef generate_random_data(t):\n    \"\"\"\n    Generate N random data points from a tuple of ranges and plot the histogram of the distribution.\n    \n    Parameters:\n    t (tuple): The tuple of ranges.\n    \n    Returns:\n    list: A list of N random numbers.\n    \n    Requirements:\n    - random\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> generate_random_data((1, 10))\n    \"\"\"\n    min_val, max_val = t\n    data = [random.uniform(min_val, max_val) for _ in range(N)]\n    \n    plt.hist(data, bins=10)\n    plt.show()\n\n    return data\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, sklearn.preprocessing\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D']\n\ndef preprocess_data(tuples_list):\n    \"\"\"\n    Convert a list of tuples into a pandas DataFrame, perform standard scaling on each column \n    and return the transformed DataFrame.\n    \n    Parameters:\n    tuples_list (list): The list of tuples.\n    \n    Returns:\n    DataFrame: A pandas DataFrame.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.preprocessing\n    \n    Example:\n    >>> preprocess_data([(1, 2, 3, 4), (5, 6, 7, 8), (9, 10, 11, 12)])\n    \"\"\"\n    df = pd.DataFrame(tuples_list, columns=COLUMNS)\n    scaler = StandardScaler()\n    df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n\n    return df_scaled\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nitertools, random\n```python\nimport itertools\nimport random\n\n# Constants\nN = 10\n\ndef generate_combinations(t):\n    \"\"\"\n    Generate all combinations of a tuple of length N and randomly select one.\n    \n    Parameters:\n    t (tuple): The tuple.\n    \n    Returns:\n    tuple: A combination of the input tuple.\n    \n    Requirements:\n    - itertools\n    - random\n    \n    Example:\n    >>> generate_combinations(('a', 'b', 'c', 'd', 'e'))\n    \"\"\"\n    combinations = list(itertools.combinations(t, N))\n    selected_combination = random.choice(combinations)\n\n    return selected_combination\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, sklearn.decomposition, matplotlib\n```python\nimport numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\n# Constants\nN_COMPONENTS = 2\n\ndef perform_pca(tuples_list):\n    \"\"\"\n    Perform Principal Component Analysis (PCA) on a list of tuples \n    and plot the transformed data in 2D.\n    \n    Parameters:\n    tuples_list (list): The list of tuples.\n    \n    Returns:\n    ndarray: The transformed data.\n    \n    Requirements:\n    - numpy\n    - sklearn.decomposition\n    - matplotlib.pyplot\n    \n    Example:\n    >>> perform_pca([(1, 2, 3, 4), (5, 6, 7, 8), (9, 10, 11, 12)])\n    \"\"\"\n    data = np.array(tuples_list)\n    pca = PCA(n_components=N_COMPONENTS)\n    transformed_data = pca.fit_transform(data)\n    \n    plt.scatter(transformed_data[:, 0], transformed_data[:, 1])\n    plt.show()\n\n    return transformed_data\n```"}
{"seed": "{\"task_id\": 16296643, \"prompt\": \"def f_16296643(t):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"tuple(t)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([0,1,2]) == (0, 1, 2)\\n\", \"\\n    assert candidate(['a', [], 100]) == ('a', [], 100)\\n\"], \"entry_point\": \"f_16296643\", \"intent\": \"Convert list `t` to tuple\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, random, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nSTATES = ['California', 'New York', 'Texas', 'Florida', 'Pennsylvania']\nSTATES_POPULATION = [39538223, 20201249, 29145505, 21538187, 13002700]\n\ndef generate_state_population_report(states_list):\n    \"\"\"\n    Generate a report of population for a list of states in the USA with the \n    population distribution plotted.\n    \n    Parameters:\n    states_list (list): list of states\n    \n    Returns:\n    DataFrame: A pandas DataFrame with population for the states.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> states = ['California', 'New York']\n    >>> report = generate_state_population_report(states)\n    >>> print(report)\n    >>> report['Population'].plot(kind='bar')\n    \"\"\"\n    report_data = []\n\n    for state in states_list:\n        if state in STATES:\n            index = STATES.index(state)\n            population = STATES_POPULATION[index]\n        else:\n            population = np.nan\n        report_data.append([state, population])\n\n    report_df = pd.DataFrame(report_data, columns=['State', 'Population'])\n\n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, random, matplotlib\n```python\nimport numpy as np\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nNUMBERS = np.arange(1, 101)\n\ndef plot_random_numbers_histogram(numbers_tuple):\n    \"\"\"\n    Generate a histogram of randomly selected numbers from a tuple.\n    \n    Parameters:\n    numbers_tuple (tuple): Tuple of numbers.\n    \n    Returns:\n    None.\n    \n    Requirements:\n    - numpy\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> numbers = tuple(range(1, 101))\n    >>> plot_random_numbers_histogram(numbers)\n    \"\"\"\n    samples = np.random.choice(numbers_tuple, size=1000, replace=True)\n    plt.hist(samples, bins=20, alpha=0.5)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnumpy, random, matplotlib\n```python\nimport numpy as np\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nLETTERS = tuple('abcdefghijklmnopqrstuvwxyz')\n\ndef plot_random_letters_histogram(letters_tuple):\n    \"\"\"\n    Generate a histogram of randomly selected letters from a tuple.\n    \n    Parameters:\n    letters_tuple (tuple): Tuple of letters.\n    \n    Returns:\n    None.\n    \n    Requirements:\n    - numpy\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> letters = tuple('abcdefghijklmnopqrstuvwxyz')\n    >>> plot_random_letters_histogram(letters)\n    \"\"\"\n    samples = np.random.choice(letters_tuple, size=1000, replace=True)\n    plt.hist(samples, bins=26, alpha=0.5)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, random, matplotlib\n```python\nimport numpy as np\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nSHAPES = ('Circle', 'Square', 'Rectangle', 'Triangle', 'Pentagon')\n\ndef plot_random_shapes_histogram(shapes_tuple):\n    \"\"\"\n    Generate a histogram of randomly selected shapes from a tuple.\n    \n    Parameters:\n    shapes_tuple (tuple): Tuple of shapes.\n    \n    Returns:\n    None.\n    \n    Requirements:\n    - numpy\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> shapes = ('Circle', 'Square', 'Rectangle', 'Triangle', 'Pentagon')\n    >>> plot_random_shapes_histogram(shapes)\n    \"\"\"\n    samples = np.random.choice(shapes_tuple, size=1000, replace=True)\n    plt.hist(samples, bins=len(shapes_tuple), alpha=0.5)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \npandas, numpy, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCOUNTRIES = ['USA', 'India', 'China', 'Japan', 'Germany']\n\ndef generate_country_gdp_report(countries_tuple):\n    \"\"\"\n    Generate a report of GDP for a list of countries with the GDP distribution plotted.\n    \n    Parameters:\n    countries_tuple (tuple): Tuple of countries.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with GDP for the countries.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> countries = ('USA', 'India', 'China', 'Japan', 'Germany')\n    >>> report = generate_country_gdp_report(countries)\n    >>> print(report)\n    >>> report['GDP'].plot(kind='bar')\n    \"\"\"\n    gdp_data = [21427700, 2875142, 14342903, 5081770, 3845630]  # GDP in million USD\n    gdp_dict = dict(zip(COUNTRIES, gdp_data))\n\n    report_data = []\n    for country in countries_tuple:\n        gdp = gdp_dict.get(country, np.nan)\n        report_data.append([country, gdp])\n\n    report_df = pd.DataFrame(report_data, columns=['Country', 'GDP'])\n\n    return report_df\n```"}
{"seed": "{\"task_id\": 16296643, \"prompt\": \"def f_16296643(level1):\\n\\t\", \"suffix\": \"\\n\\treturn level1\", \"canonical_solution\": \"level1 = map(list, level1)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    t = ((1, 2), (3, 4))\\n    t = candidate(t)\\n    assert list(t) == [[1, 2], [3, 4]]\\n\"], \"entry_point\": \"f_16296643\", \"intent\": \"Convert tuple `level1` to list\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, matplotlib, random\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint, uniform\n\n# Constants\nCOLUMNS = ['Age', 'Height', 'Weight', 'BMI', 'Activity Level']\n\ndef generate_health_data_frame(level1):\n    \"\"\"\n    Convert the provided tuple `level1` into a pandas DataFrame, add more health-related columns and fill them with random data.\n    Also, plot the distribution of the 'BMI' column.\n    \n    Parameters:\n    level1 (tuple): The tuple to be converted into a DataFrame.\n    \n    Returns:\n    DataFrame: A pandas DataFrame based on `level1` with additional health-related columns.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> level1 = (('John', 'Male'), ('Emily', 'Female'))\n    >>> df = generate_health_data_frame(level1)\n    >>> print(df)\n    >>> df['BMI'].plot(kind='hist')\n    \"\"\"\n    df = pd.DataFrame(list(level1), columns=['Name', 'Gender'])\n    \n    for column in COLUMNS:\n        if column == 'Age':\n            df[column] = np.random.randint(18, 65, df.shape[0])\n        elif column == 'Activity Level':\n            df[column] = np.random.choice(['Low', 'Medium', 'High'], df.shape[0])\n        else:\n            df[column] = np.random.uniform(1, 100, df.shape[0])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nnumpy, matplotlib, math, random\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\nfrom random import uniform\n\n# Constants\nRADIUS = 10\n\ndef generate_random_points_in_circle(level1):\n    \"\"\"\n    Convert tuple `level1` to a list of random points within a circle of a given radius and plot these points.\n    \n    Parameters:\n    level1 (tuple): The tuple to be converted into a list of points.\n\n    Returns:\n    List: A list of points within a circle.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - math\n    - random\n\n    Example:\n    >>> level1 = ((1, 2), (3, 4))\n    >>> points = generate_random_points_in_circle(level1)\n    >>> print(points)\n    >>> plt.scatter(*zip(*points))\n    \"\"\"\n    points = list(level1)\n    \n    for _ in range(len(points)):\n        theta = uniform(0, 2*np.pi)\n        r = RADIUS * math.sqrt(uniform(0, 1))\n        x = r * math.cos(theta)\n        y = r * math.sin(theta)\n        points.append((x, y))\n        \n    return points\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnumpy, matplotlib, random, itertools\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import choice\nimport itertools\n\n# Constants\nCOLORS = ['red', 'green', 'blue', 'yellow', 'black']\n\ndef plot_random_colored_points(level1):\n    \"\"\"\n    Convert tuple `level1` to a list of points, assign a random color to each point and plot these points.\n    \n    Parameters:\n    level1 (tuple): The tuple to be converted into a list of points.\n\n    Returns:\n    List: A list of points with their assigned colors.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - random\n    - itertools\n\n    Example:\n    >>> level1 = ((1, 2), (3, 4))\n    >>> colored_points = plot_random_colored_points(level1)\n    >>> print(colored_points)\n    >>> for point, color in colored_points:\n    >>>     plt.scatter(*point, color=color)\n    \"\"\"\n    points = list(level1)\n    colored_points = list(zip(points, itertools.cycle(COLORS)))\n    \n    return colored_points\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, matplotlib, random, math\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\nimport math\n\n# Constants\nANGLES = np.linspace(0, 2*np.pi, 100)\n\ndef plot_random_polygon(level1):\n    \"\"\"\n    Convert tuple `level1` to a list of vertices of a random polygon and plot this polygon.\n\n    Parameters:\n    level1 (tuple): The tuple to be converted into a list of vertices.\n\n    Returns:\n    List: A list of vertices of a random polygon.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - random\n    - math\n\n    Example:\n    >>> level1 = ((1, 2), (3, 4))\n    >>> vertices = plot_random_polygon(level1)\n    >>> print(vertices)\n    >>> plt.fill(*zip(*vertices), fill=True, color='b')\n    \"\"\"\n    vertices = list(level1)\n\n    for _ in range(randint(3, 10)):\n        theta = choice(ANGLES)\n        x = math.cos(theta)\n        y = math.sin(theta)\n        vertices.append((x, y))\n    \n    return vertices\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnumpy, pandas, matplotlib, random\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import choice\n\n# Constants\nCATEGORIES = ['A', 'B', 'C', 'D', 'E']\n\ndef generate_random_categorical_data(level1):\n    \"\"\"\n    Convert tuple `level1` to a pandas Series of random categorical data and plot the frequency distribution of the categories.\n    \n    Parameters:\n    level1 (tuple): The tuple to be converted into a pandas Series.\n\n    Returns:\n    Series: A pandas Series of random categorical data.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> level1 = ('John', 'Emily', 'Michael', 'Sarah', 'Jessica')\n    >>> series = generate_random_categorical_data(level1)\n    >>> print(series)\n    >>> series.value_counts().plot(kind='bar')\n    \"\"\"\n    series = pd.Series(level1)\n    \n    for _ in range(len(series)):\n        series = series.replace(choice(series), choice(CATEGORIES))\n        \n    return series\n```"}
{"seed": "{\"task_id\": 3880399, \"prompt\": \"def f_3880399(dataobject, logFile):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"pprint.pprint(dataobject, logFile)\", \"test_start\": \"\\nimport pprint \\n\\ndef check(candidate):\", \"test\": [\"\\n    f = open('kkk.txt', 'w')\\n    candidate('hello', f)\\n    f.close()\\n    with open('kkk.txt', 'r') as f:\\n        assert 'hello' in f.readline()\\n\"], \"entry_point\": \"f_3880399\", \"intent\": \"send the output of pprint object `dataobject` to file `logFile`\", \"library\": [\"pprint\"]}", "generation": "Scenario 1: \npprint, os, json, time\n```python\nimport pprint\nimport os\nimport json\nimport time\n\n# Constant\nLOG_DIR = './logs'\n\ndef log_data_to_file(data_object, log_file_name, timestamp=True):\n    \"\"\"\n    Log the output of pprint object to a specific file in a directory. If a file with the same \n    name exists, append the data to it. If timestamp is True, append the current time to the log data.\n\n    Parameters:\n    data_object (object): The object to be logged.\n    log_file_name (str): The name of the log file.\n    timestamp (bool, optional): If True, append the current time to the log data. Default is True.\n\n    Returns:\n    string: The path of the log file.\n\n    Requirements:\n    - pprint\n    - os\n    - json\n    - time\n\n    Examples:\n    >>> data = {'key1': 'value1', 'key2': 'value2'}\n    >>> log_data_to_file(data, 'log.txt')\n    \"\"\"\n    log_file_path = os.path.join(LOG_DIR, log_file_name)\n\n    with open(log_file_path, 'a') as log_file:\n        if timestamp:\n            log_file.write(f\"\\n\\n{time.ctime()}\\n\")\n        pprint.pprint(data_object, log_file)\n\n    return log_file_path\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npprint, csv, random, datetime\n```python\nimport pprint\nimport csv\nimport random\nfrom datetime import datetime\n\n# Constants\nCSV_FILE_PATH = './data.csv'\nPPRINT_FILE_PATH = './pprint_output.txt'\nITEMS = [\"item1\", \"item2\", \"item3\", \"item4\", \"item5\"]\n\ndef convert_csv_to_pprint():\n    \"\"\"\n    Read a CSV file, convert each row to a dictionary, pprint the dictionary to a text file.\n\n    Returns:\n    string: The path of the pprint output file.\n\n    Requirements:\n    - csv\n    - pprint\n    - random\n    - datetime\n\n    Examples:\n    >>> convert_csv_to_pprint()\n    \"\"\"\n    with open(CSV_FILE_PATH, 'r') as csv_file:\n        reader = csv.DictReader(csv_file)\n        with open(PPRINT_FILE_PATH, 'w') as pprint_file:\n            for row in reader:\n                pprint.pprint(row, pprint_file)\n    return PPRINT_FILE_PATH\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npprint, requests, bs4, json\n```python\nimport pprint\nimport requests\nfrom bs4 import BeautifulSoup\nimport json\n\n# Constants\nURL = 'https://www.example.com'\nLOG_FILE_PATH = './log.txt'\n\ndef scrape_and_log():\n    \"\"\"\n    Scrape a webpage, extract the title, meta description, and the first paragraph, and log them into a file using pprint.\n\n    Returns:\n    string: The path of the log file.\n\n    Requirements:\n    - pprint\n    - requests\n    - bs4.BeautifulSoup\n    - json\n\n    Examples:\n    >>> scrape_and_log()\n    \"\"\"\n    response = requests.get(URL)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    data = {\n        'title': soup.title.string,\n        'meta_description': soup.find('meta', attrs={'name':'description'}).get(\"content\"),\n        'first_paragraph': soup.p.text\n    }\n\n    with open(LOG_FILE_PATH, 'w') as log_file:\n        pprint.pprint(data, log_file)\n\n    return LOG_FILE_PATH\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npprint, os, configparser, json\n```python\nimport pprint\nimport os\nimport configparser\nimport json\n\n# Constants\nCONFIG_FILE_PATH = './config.ini'\nLOG_FILE_PATH = './log.txt'\n\ndef log_config_file():\n    \"\"\"\n    Parse a config file and log its contents to a file using pprint.\n\n    Returns:\n    string: The path of the log file.\n\n    Requirements:\n    - pprint\n    - os\n    - configparser\n    - json\n\n    Examples:\n    >>> log_config_file()\n    \"\"\"\n    config = configparser.ConfigParser()\n    config.read(CONFIG_FILE_PATH)\n\n    data = {section: dict(config.items(section)) for section in config.sections()}\n\n    with open(LOG_FILE_PATH, 'w') as log_file:\n        pprint.pprint(data, log_file)\n\n    return LOG_FILE_PATH\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npprint, pandas, json, datetime\n```python\nimport pprint\nimport pandas as pd\nimport json\nfrom datetime import datetime\n\n# Constants\nCSV_FILE_PATH = './data.csv'\nLOG_FILE_PATH = './log.txt'\n\ndef log_dataframe():\n    \"\"\"\n    Load a CSV file as a pandas DataFrame, convert the DataFrame to a dictionary, and log it using pprint.\n\n    Returns:\n    string: The path of the log file.\n\n    Requirements:\n    - pprint\n    - pandas\n    - json\n    - datetime\n\n    Examples:\n    >>> log_dataframe()\n    \"\"\"\n    df = pd.read_csv(CSV_FILE_PATH)\n    data = df.to_dict()\n\n    with open(LOG_FILE_PATH, 'w') as log_file:\n        pprint.pprint(data, log_file)\n\n    return LOG_FILE_PATH\n```\n"}
{"seed": "{\"task_id\": 21800169, \"prompt\": \"def f_21800169(df):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"df.loc[df['BoolCol']]\", \"test_start\": \"\\nimport pandas as pd\\n\\ndef check(candidate):\", \"test\": [\"\\n    df = pd.DataFrame([[True, 2, 3], [False, 5, 6]], columns = ['BoolCol', 'a', 'b'])\\n    y = candidate(df)\\n    assert y['a'][0] == 2\\n    assert y['b'][0] == 3\\n\"], \"entry_point\": \"f_21800169\", \"intent\": \"get index of rows in column 'BoolCol'\", \"library\": [\"pandas\"]}", "generation": "Scenario 1:\npandas, matplotlib, seaborn, numpy\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Constants\nCATEGORIES = ['A', 'B', 'C', 'D', 'E']\n\ndef plot_category_distribution(df):\n    \"\"\"\n    Plot a bar chart of the distribution of categories in a dataframe column 'Category' \n    and also return a dataframe of the counts of each category.\n\n    Parameters:\n    df (DataFrame): pandas DataFrame that contains a column named 'Category'.\n\n    Returns:\n    DataFrame: A pandas DataFrame with counts of each category.\n    \n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - seaborn\n    - numpy\n\n    Example:\n    >>> df = pd.DataFrame({'Category': np.random.choice(CATEGORIES, 100)})\n    >>> category_counts = plot_category_distribution(df)\n    >>> print(category_counts)\n    \"\"\"\n    category_counts = df['Category'].value_counts()\n\n    # Plot bar chart\n    plt.figure(figsize=(10, 6))\n    sns.barplot(x=category_counts.index, y=category_counts.values, alpha=0.8)\n    plt.title('Category distribution')\n    plt.ylabel('Number of Occurrences', fontsize=12)\n    plt.xlabel('Category', fontsize=12)\n    plt.show()\n\n    return category_counts.to_frame().reset_index().rename(columns={'index': 'Category', 'Category': 'Count'})\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, sklearn, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\n\ndef predict_and_plot(df):\n    \"\"\"\n    Fit a linear regression model to predict 'value' from 'feature' in the given dataframe,\n    plot the original data and the regression line, and return the coefficients and the intercept.\n\n    Parameters:\n    df (DataFrame): pandas DataFrame that contains columns named 'feature' and 'value'.\n\n    Returns:\n    dict: A dictionary with the coefficients and the intercept of the fitted linear regression model.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.linear_model\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame({'feature': np.random.rand(100), 'value': np.random.rand(100)})\n    >>> coefficients = predict_and_plot(df)\n    >>> print(coefficients)\n    \"\"\"\n    X = df['feature'].values.reshape(-1,1)\n    y = df['value'].values.reshape(-1,1)\n\n    # Fit linear regression\n    model = LinearRegression().fit(X, y)\n\n    # Plot data and regression line\n    plt.scatter(X, y, color='blue')\n    plt.plot(X, model.predict(X), color='red', linewidth=2)\n    plt.show()\n\n    return {'coefficients': model.coef_.tolist(), 'intercept': model.intercept_.tolist()}\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, sklearn, numpy\n```python\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\ndef split_dataset(df):\n    \"\"\"\n    Split the given DataFrame into a training set and a test set (70%:30% split), \n    separate the 'target' column, and return the four resulting dataframes.\n\n    Parameters:\n    df (DataFrame): pandas DataFrame that contains a column named 'target'.\n\n    Returns:\n    tuple: A tuple containing four DataFrames: X_train, X_test, y_train, y_test.\n    \n    Requirements:\n    - pandas\n    - sklearn.model_selection\n    - numpy\n\n    Example:\n    >>> df = pd.DataFrame(np.random.randint(0, 100, size=(100, 5)), columns=list('ABCDE'))\n    >>> df['target'] = np.random.randint(0, 2, size=100)\n    >>> X_train, X_test, y_train, y_test = split_dataset(df)\n    \"\"\"\n    X = df.drop('target', axis=1)\n    y = df['target']\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n    return X_train, X_test, y_train, y_test\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, sklearn, matplotlib\n```python\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\ndef k_means_clustering(df):\n    \"\"\"\n    Perform K-Means clustering on a DataFrame with two columns 'x' and 'y',\n    plot the data points and centroids, and return the labels and centroids.\n\n    Parameters:\n    df (DataFrame): pandas DataFrame that contains columns named 'x' and 'y'.\n\n    Returns:\n    tuple: The labels and centroids as numpy arrays.\n    \n    Requirements:\n    - pandas\n    - sklearn.cluster\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame({'x': [1, 2, 3, 4, 5, 6], 'y': [2, 3, 4, 5, 6, 7]})\n    >>> labels, centroids = k_means_clustering(df)\n    \"\"\"\n    kmeans = KMeans(n_clusters=2, random_state=0).fit(df)\n    \n    # plot the clustered data\n    plt.scatter(df['x'], df['y'], c=kmeans.labels_)\n    plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red')\n    plt.show()\n\n    return kmeans.labels_, kmeans.cluster_centers_\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_histogram(df):\n    \"\"\"\n    Plot a histogram of a 'value' column in the given DataFrame,\n    calculate and plot the mean and median.\n\n    Parameters:\n    df (DataFrame): pandas DataFrame that contains a column named 'value'.\n\n    Returns:\n    tuple: The mean and median of the 'value' column.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame({'value': np.random.randn(100)})\n    >>> mean, median = plot_histogram(df)\n    >>> print(f'Mean: {mean}, Median: {median}')\n    \"\"\"\n    mean = df['value'].mean()\n    median = df['value'].median()\n\n    plt.hist(df['value'], bins=20, alpha=0.5, color='g')\n    plt.axvline(mean, color='red', linestyle='dashed', linewidth=1)\n    plt.axvline(median, color='blue', linestyle='dashed', linewidth=1)\n    plt.title('Histogram of values')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.grid(True)\n    plt.show()\n\n    return mean, median\n```"}
{"seed": "{\"task_id\": 21800169, \"prompt\": \"def f_21800169(df):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"df.iloc[np.flatnonzero(df['BoolCol'])]\", \"test_start\": \"\\nimport numpy as np\\nimport pandas as pd\\n\\ndef check(candidate):\", \"test\": [\"\\n    df = pd.DataFrame([[True, 2, 3], [False, 5, 6]], columns = ['BoolCol', 'a', 'b'])\\n    y = candidate(df)\\n    assert y['a'][0] == 2\\n    assert y['b'][0] == 3\\n\"], \"entry_point\": \"f_21800169\", \"intent\": \"Create a list containing the indexes of rows where the value of column 'BoolCol' in dataframe `df` are equal to True\", \"library\": [\"numpy\", \"pandas\"]}", "generation": "Scenario 1: \npandas, numpy, matplotlib, scikit-learn\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\n\n# Constants\nRANDOM_STATE = 42\n\ndef perform_kmeans_clustering(df):\n    \"\"\"\n    Standardize the data in the DataFrame and perform K-Means clustering, and \n    create a scatter plot of the clusters.\n    \n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n    \n    Returns:\n    DataFrame, dict: The DataFrame with a new 'Cluster' column, and a dictionary \n    with cluster centers.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib\n    - scikit-learn\n    \n    Example:\n    >>> df = pd.DataFrame([[5.1, 3.5], [4.9, 3.0], [4.7, 3.2]], columns = ['x', 'y'])\n    >>> df, centers = perform_kmeans_clustering(df)\n    >>> print(df)\n    >>> print(centers)\n    \"\"\"\n    scaler = StandardScaler()\n    df_scaled = scaler.fit_transform(df)\n\n    kmeans = KMeans(n_clusters=2, random_state=RANDOM_STATE)\n    df['Cluster'] = kmeans.fit_predict(df_scaled)\n    \n    centers = {i: scaler.inverse_transform(center) for i, center in enumerate(kmeans.cluster_centers_)}\n    \n    plt.scatter(df['x'], df['y'], c=df['Cluster'])\n    plt.show()\n    \n    return df, centers\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, matplotlib, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef plot_correlation_heatmap(df):\n    \"\"\"\n    Calculate the correlation matrix of numeric columns in the DataFrame and \n    plot a heatmap of the correlations.\n    \n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n    \n    Returns:\n    DataFrame: The correlation matrix.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib\n    - seaborn\n    \n    Example:\n    >>> df = pd.DataFrame([[5.1, 3.5, 1.4], [4.9, 3.0, 1.4], [4.7, 3.2, 1.3]], columns = ['x', 'y', 'z'])\n    >>> correlation_matrix = plot_correlation_heatmap(df)\n    >>> print(correlation_matrix)\n    \"\"\"\n    correlation_matrix = df.corr()\n    \n    sns.heatmap(correlation_matrix, annot=True)\n    plt.show()\n    \n    return correlation_matrix\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, sklearn, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\n\ndef linear_regression_analysis(df, target):\n    \"\"\"\n    Perform a linear regression analysis on the DataFrame.\n    \n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n    target (str): The target variable.\n    \n    Returns:\n    float: The R-squared score of the model.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - sklearn\n    - matplotlib\n    \n    Example:\n    >>> df = pd.DataFrame([[5.1, 1.4], [4.9, 1.4], [4.7, 1.3]], columns = ['x', 'y'])\n    >>> r_squared = linear_regression_analysis(df, 'y')\n    >>> print(r_squared)\n    \"\"\"\n    X = df.drop(target, axis=1)\n    y = df[target]\n    \n    model = LinearRegression()\n    model.fit(X, y)\n    \n    y_pred = model.predict(X)\n    \n    plt.scatter(X, y)\n    plt.plot(X, y_pred, color='red')\n    plt.show()\n    \n    return model.score(X, y)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, sklearn, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\ndef perform_pca(df):\n    \"\"\"\n    Perform Principal Component Analysis (PCA) on the DataFrame and plot the \n    first two principal components.\n    \n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n    \n    Returns:\n    DataFrame: The DataFrame with the first two principal components.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - sklearn\n    - matplotlib\n    \n    Example:\n    >>> df = pd.DataFrame([[5.1, 3.5, 1.4], [4.9, 3.0, 1.4], [4.7, 3.2, 1.3]], columns = ['x', 'y', 'z'])\n    >>> df_pca = perform_pca(df)\n    >>> print(df_pca)\n    \"\"\"\n    pca = PCA(n_components=2)\n    df_pca = pca.fit_transform(df)\n    \n    df_pca = pd.DataFrame(df_pca, columns=['PC1', 'PC2'])\n    \n    plt.scatter(df_pca['PC1'], df_pca['PC2'])\n    plt.show()\n    \n    return df_pca\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, sklearn, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cluster import DBSCAN\nimport matplotlib.pyplot as plt\n\ndef perform_dbscan_clustering(df):\n    \"\"\"\n    Perform DBSCAN clustering on the DataFrame and plot the clusters.\n    \n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n    \n    Returns:\n    DataFrame: The DataFrame with a new 'Cluster' column.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - sklearn\n    - matplotlib\n    \n    Example:\n    >>> df = pd.DataFrame([[5.1, 3.5], [4.9, 3.0], [4.7, 3.2]], columns = ['x', 'y'])\n    >>> df = perform_dbscan_clustering(df)\n    >>> print(df)\n    \"\"\"\n    dbscan = DBSCAN(eps=0.3)\n    df['Cluster'] = dbscan.fit_predict(df)\n    \n    plt.scatter(df['x'], df['y'], c=df['Cluster'])\n    plt.show()\n    \n    return df\n```\n"}
{"seed": "{\"task_id\": 21800169, \"prompt\": \"def f_21800169(df):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"df[df['BoolCol'] == True].index.tolist()\", \"test_start\": \"\\nimport pandas as pd\\n\\ndef check(candidate):\", \"test\": [\"\\n    df = pd.DataFrame([[True, 2, 3], [False, 5, 6]], columns = ['BoolCol', 'a', 'b'])\\n    y = candidate(df)\\n    assert y == [0]\\n\"], \"entry_point\": \"f_21800169\", \"intent\": \"from dataframe `df` get list of indexes of rows where column 'BoolCol' values match True\", \"library\": [\"pandas\"]}", "generation": "Scenario 1:\npandas, numpy, itertools, math\n```python\nimport pandas as pd\nimport numpy as np\nfrom itertools import combinations\nfrom math import ceil\n\n# Constants\nMIN_PERCENTAGE = 0.75\n\ndef get_combinations_above_threshold(df, percentage):\n    \"\"\"\n    From a given DataFrame, find all combinations of columns such that the absolute correlation \n    between them is greater than a specified threshold.\n\n    Parameters:\n    df (DataFrame): The input DataFrame.\n    percentage (float): The threshold for the absolute correlation.\n\n    Returns:\n    list: A list of tuples where each tuple contains two column names.\n\n    Requirements:\n    - pandas\n    - numpy\n    - itertools\n    - math\n\n    Example:\n    >>> df = pd.DataFrame(np.random.rand(10, 5), columns=list('ABCDE'))\n    >>> get_combinations_above_threshold(df, 0.8)\n    \"\"\"\n    if not 0 <= percentage <= 1:\n        raise ValueError('Percentage must be between 0 and 1')\n\n    corr_matrix = df.corr().abs()\n    columns = corr_matrix.columns\n    corr_combinations = []\n\n    for col1, col2 in combinations(columns, 2):\n        if corr_matrix.loc[col1, col2] > percentage:\n            corr_combinations.append((col1, col2))\n\n    return corr_combinations\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, matplotlib, seaborn, numpy\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\ndef plot_categorical_distribution(df, column):\n    \"\"\"\n    Plot the distribution of a specific categorical column from a DataFrame using a bar chart.\n\n    Parameters:\n    df (DataFrame): The input DataFrame.\n    column (str): The column name.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - seaborn\n    - numpy\n\n    Example:\n    >>> df = pd.DataFrame({'Category': np.random.choice(['A', 'B', 'C', 'D', 'E'], 500)})\n    >>> plot_categorical_distribution(df, 'Category')\n    \"\"\"\n    if column not in df.columns:\n        raise ValueError('Column does not exist in DataFrame')\n\n    plt.figure(figsize=(10, 6))\n    sns.countplot(data=df, x=column)\n    plt.title(f'Distribution of {column}')\n    plt.show()\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, sklearn.preprocessing, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\ndef standardize_and_plot(df, column):\n    \"\"\"\n    Standardize a specific numerical column from a DataFrame and plot its histogram before and after standardization.\n\n    Parameters:\n    df (DataFrame): The input DataFrame.\n    column (str): The column name.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.preprocessing\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame({'Value': np.random.normal(10, 2, 1000)})\n    >>> standardize_and_plot(df, 'Value')\n    \"\"\"\n    if column not in df.columns:\n        raise ValueError('Column does not exist in DataFrame')\n\n    scaler = StandardScaler()\n    df[column + '_standardized'] = scaler.fit_transform(df[[column]])\n\n    fig, axs = plt.subplots(1, 2, figsize=(14, 7))\n    axs[0].hist(df[column], bins=30)\n    axs[0].set_title('Before Standardization')\n\n    axs[1].hist(df[column + '_standardized'], bins=30)\n    axs[1].set_title('After Standardization')\n    plt.show()\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, scipy.stats, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\n# Constants\nALPHA = 0.05\n\ndef test_normality_and_plot(df, column):\n    \"\"\"\n    Test the normality of a specific numerical column from a DataFrame using Shapiro-Wilk test and \n    plot its histogram and Q-Q plot.\n\n    Parameters:\n    df (DataFrame): The input DataFrame.\n    column (str): The column name.\n\n    Returns:\n    bool: True if the column passes the normality test, False otherwise.\n\n    Requirements:\n    - pandas\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame({'Value': np.random.normal(0, 1, 1000)})\n    >>> test_normality_and_plot(df, 'Value')\n    \"\"\"\n    if column not in df.columns:\n        raise ValueError('Column does not exist in DataFrame')\n\n    fig, axs = plt.subplots(1, 2, figsize=(14, 7))\n    axs[0].hist(df[column], bins=30)\n    axs[0].set_title('Histogram')\n\n    stats.probplot(df[column], plot=axs[1])\n    axs[1].set_title('Q-Q Plot')\n    plt.show()\n\n    _, p = stats.shapiro(df[column])\n    if p > ALPHA:\n        print('Data is normally distributed.')\n        return True\n    else:\n        print('Data is not normally distributed.')\n        return False\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, sklearn.model_selection, sklearn.linear_model, sklearn.metrics\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\ndef logistic_regression(df, target_column):\n    \"\"\"\n    Conduct a logistic regression on a DataFrame to predict a specific target column.\n\n    Parameters:\n    df (DataFrame): The input DataFrame.\n    target_column (str): The target column name.\n\n    Returns:\n    float: The accuracy of the logistic regression model.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.model_selection\n    - sklearn.linear_model\n    - sklearn.metrics\n\n    Example:\n    >>> df = pd.DataFrame(np.random.randint(0, 2, (1000, 5)), columns=list('ABCDE'))\n    >>> logistic_regression(df, 'E')\n    \"\"\"\n    if target_column not in df.columns:\n        raise ValueError('Target column does not exist in DataFrame')\n\n    X = df.drop(columns=target_column)\n    y = df[target_column]\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    model = LogisticRegression()\n    model.fit(X_train, y_train)\n\n    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n\n    return accuracy\n```\n\nAbove is the illustration."}
{"seed": "{\"task_id\": 21800169, \"prompt\": \"def f_21800169(df):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"df[df['BoolCol']].index.tolist()\", \"test_start\": \"\\nimport pandas as pd\\n\\ndef check(candidate):\", \"test\": [\"\\n    df = pd.DataFrame([[True, 2, 3], [False, 5, 6]], columns = ['BoolCol', 'a', 'b'])\\n    y = candidate(df)\\n    assert y == [0]\\n\"], \"entry_point\": \"f_21800169\", \"intent\": \"get index of rows in dataframe `df` which column 'BoolCol' matches value True\", \"library\": [\"pandas\"]}", "generation": "Scenario 1: \npandas, numpy, datetime, random\n```python\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nfrom random import randint\n\n# Constants\nCOLUMNS = ['ID', 'BoolCol', 'IntCol', 'FloatCol', 'DateCol']\nID_RANGE = (1, 1000)\nINT_RANGE = (1, 100)\nFLOAT_RANGE = (0.0, 1.0)\nDATE_RANGE = (datetime(2021, 1, 1), datetime(2022, 12, 31))\n\ndef generate_and_filter_dataframe(n_rows, filter_value=True):\n    \"\"\"\n    Generate a DataFrame with random values and return the indices of the rows \n    where the 'BoolCol' column matches a given value.\n\n    Parameters:\n    n_rows (int): The number of rows in the DataFrame.\n    filter_value (bool): The value to filter the 'BoolCol' column.\n\n    Returns:\n    list: A list of indices.\n\n    Requirements:\n    - pandas\n    - numpy\n    - datetime\n    - random\n\n    Example:\n    >>> indices = generate_and_filter_dataframe(1000, True)\n    >>> print(indices)\n    \"\"\"\n    data = []\n    for _ in range(n_rows):\n        id = randint(*ID_RANGE)\n        bool_col = bool(randint(0, 1))\n        int_col = randint(*INT_RANGE)\n        float_col = np.random.uniform(*FLOAT_RANGE)\n        date_col = DATE_RANGE[0] + (DATE_RANGE[1] - DATE_RANGE[0]) * random.random()\n        data.append([id, bool_col, int_col, float_col, date_col])\n\n    df = pd.DataFrame(data, columns=COLUMNS)\n    return df[df['BoolCol'] == filter_value].index.tolist()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \npandas, numpy, datetime, random\n```python\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nfrom random import randint\n\n# Constants\nCOLUMNS = ['ID', 'BoolCol', 'IntCol', 'StrCol']\nID_RANGE = (1, 1000)\nINT_RANGE = (1, 100)\nSTRING_VALUES = ['A', 'B', 'C', 'D', 'E']\n\ndef generate_and_group_dataframe(n_rows):\n    \"\"\"\n    Generate a DataFrame with random values and return the DataFrame grouped by \n    the 'BoolCol' column and aggregated by the mean of the 'IntCol' column.\n\n    Parameters:\n    n_rows (int): The number of rows in the DataFrame.\n\n    Returns:\n    DataFrame: A pandas DataFrame.\n\n    Requirements:\n    - pandas\n    - numpy\n    - datetime\n    - random\n\n    Example:\n    >>> df_grouped = generate_and_group_dataframe(1000)\n    >>> print(df_grouped)\n    \"\"\"\n    data = []\n    for _ in range(n_rows):\n        id = randint(*ID_RANGE)\n        bool_col = bool(randint(0, 1))\n        int_col = randint(*INT_RANGE)\n        str_col = STRING_VALUES[randint(0, len(STRING_VALUES) - 1)]\n        data.append([id, bool_col, int_col, str_col])\n\n    df = pd.DataFrame(data, columns=COLUMNS)\n    return df.groupby('BoolCol')['IntCol'].mean()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \npandas, numpy, datetime, random\n```python\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nfrom random import randint\n\n# Constants\nCOLUMNS = ['ID', 'BoolCol', 'IntCol', 'DateCol']\nID_RANGE = (1, 1000)\nINT_RANGE = (1, 100)\nDATE_RANGE = (datetime(2021, 1, 1), datetime(2022, 12, 31))\n\ndef generate_and_sort_dataframe(n_rows):\n    \"\"\"\n    Generate a DataFrame with random values and return the DataFrame sorted by \n    the 'DateCol' column in descending order.\n\n    Parameters:\n    n_rows (int): The number of rows in the DataFrame.\n\n    Returns:\n    DataFrame: A pandas DataFrame.\n\n    Requirements:\n    - pandas\n    - numpy\n    - datetime\n    - random\n\n    Example:\n    >>> df_sorted = generate_and_sort_dataframe(1000)\n    >>> print(df_sorted)\n    \"\"\"\n    data = []\n    for _ in range(n_rows):\n        id = randint(*ID_RANGE)\n        bool_col = bool(randint(0, 1))\n        int_col = randint(*INT_RANGE)\n        date_col = DATE_RANGE[0] + (DATE_RANGE[1] - DATE_RANGE[0]) * random.random()\n        data.append([id, bool_col, int_col, date_col])\n\n    df = pd.DataFrame(data, columns=COLUMNS)\n    return df.sort_values('DateCol', ascending=False)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \npandas, numpy, datetime, random\n```python\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nfrom random import randint\n\n# Constants\nCOLUMNS = ['ID', 'BoolCol', 'IntCol', 'StrCol']\nID_RANGE = (1, 1000)\nINT_RANGE = (1, 100)\nSTRING_VALUES = ['A', 'B', 'C', 'D', 'E']\n\ndef generate_and_pivot_dataframe(n_rows):\n    \"\"\"\n    Generate a DataFrame with random values and return the DataFrame pivoted by \n    the 'BoolCol' and 'StrCol' columns and the 'IntCol' column as values.\n\n    Parameters:\n    n_rows (int): The number of rows in the DataFrame.\n\n    Returns:\n    DataFrame: A pandas DataFrame.\n\n    Requirements:\n    - pandas\n    - numpy\n    - datetime\n    - random\n\n    Example:\n    >>> df_pivoted = generate_and_pivot_dataframe(1000)\n    >>> print(df_pivoted)\n    \"\"\"\n    data = []\n    for _ in range(n_rows):\n        id = randint(*ID_RANGE)\n        bool_col = bool(randint(0, 1))\n        int_col = randint(*INT_RANGE)\n        str_col = STRING_VALUES[randint(0, len(STRING_VALUES) - 1)]\n        data.append([id, bool_col, int_col, str_col])\n\n    df = pd.DataFrame(data, columns=COLUMNS)\n    return df.pivot_table(values='IntCol', index='BoolCol', columns='StrCol')\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \npandas, numpy, datetime, random\n```python\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nfrom random import randint\n\n# Constants\nCOLUMNS = ['ID', 'BoolCol', 'IntCol', 'DateCol']\nID_RANGE = (1, 1000)\nINT_RANGE = (1, 100)\nDATE_RANGE = (datetime(2021, 1, 1), datetime(2022, 12, 31))\n\ndef generate_and_transform_dataframe(n_rows):\n    \"\"\"\n    Generate a DataFrame with random values and return the DataFrame with the \n    'IntCol' column transformed by a logarithm (base 10).\n\n    Parameters:\n    n_rows (int): The number of rows in the DataFrame.\n\n    Returns:\n    DataFrame: A pandas DataFrame.\n\n    Requirements:\n    - pandas\n    - numpy\n    - datetime\n    - random\n\n    Example:\n    >>> df_transformed = generate_and_transform_dataframe(1000)\n    >>> print(df_transformed)\n    \"\"\"\n    data = []\n    for _ in range(n_rows):\n        id = randint(*ID_RANGE)\n        bool_col = bool(randint(0, 1))\n        int_col = randint(*INT_RANGE)\n        date_col = DATE_RANGE[0] + (DATE_RANGE[1] - DATE_RANGE[0]) * random.random()\n        data.append([id, bool_col, int_col, date_col])\n\n    df = pd.DataFrame(data, columns=COLUMNS)\n    df['IntCol'] = np.log10(df['IntCol'])\n\n    return df\n```\n"}
{"seed": "{\"task_id\": 299446, \"prompt\": \"def f_299446(owd):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"os.chdir(owd)\", \"test_start\": \"\\nimport os\\nfrom unittest.mock import Mock\\n\\ndef check(candidate):\", \"test\": [\"\\n    os.chdir = Mock()\\n    try:\\n        candidate('/')\\n    except:\\n        assert False\\n\"], \"entry_point\": \"f_299446\", \"intent\": \"change working directory to the directory `owd`\", \"library\": [\"os\"]}", "generation": "Scenario 1:\nos, sys, shutil, time\n```python\nimport os\nimport sys\nimport shutil\nimport time\n\n# Constants\nBACKUP_DIR = '/backup/'\n\ndef backup_and_change_dir(owd, filename):\n    \"\"\"\n    Change the working directory to the directory `owd` and before changing, \n    backup a file to a backup directory.\n    \n    Parameters:\n    owd (str): The directory to which to change the working directory.\n    filename (str): The name of the file to backup.\n    \n    Returns:\n    str: The new working directory.\n    \n    Requirements:\n    - os\n    - sys\n    - shutil\n    - time\n    \n    Example:\n    >>> backup_and_change_dir('/new_directory', 'important_file.txt')\n    \"\"\"\n    try:\n        shutil.copy(filename, BACKUP_DIR + 'backup_' + filename + '_' + str(time.time()))\n    except IOError as e:\n        print(f'Unable to copy file. {e}')\n        sys.exit(1)\n    except:\n        print('Unexpected error:', sys.exc_info())\n        sys.exit(1)\n\n    os.chdir(owd)\n    return os.getcwd()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, glob, zipfile\n```python\nimport os\nimport glob\nimport zipfile\n\n# Constants\nARCHIVE_NAME = 'archive.zip'\n\ndef archive_and_change_dir(owd, ext):\n    \"\"\"\n    Change the working directory to the directory `owd` and before changing, \n    archive all files with a given extension in the current directory.\n    \n    Parameters:\n    owd (str): The directory to which to change the working directory.\n    ext (str): The extension of the files to archive.\n    \n    Returns:\n    str: The new working directory.\n    \n    Requirements:\n    - os\n    - glob\n    - zipfile\n    \n    Example:\n    >>> archive_and_change_dir('/new_directory', '*.txt')\n    \"\"\"\n    with zipfile.ZipFile(ARCHIVE_NAME, 'w') as zipf:\n        for file in glob.glob(ext):\n            zipf.write(file)\n    \n    os.chdir(owd)\n    return os.getcwd()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, shutil, glob\n```python\nimport os\nimport shutil\nimport glob\n\n# Constants\nCOPY_DIR = '/copy/'\n\ndef copy_and_change_dir(owd, pattern):\n    \"\"\"\n    Change the working directory to the directory `owd` and before changing, \n    copy all files matching a pattern to a copy directory.\n    \n    Parameters:\n    owd (str): The directory to which to change the working directory.\n    pattern (str): The pattern of the files to copy.\n    \n    Returns:\n    str: The new working directory.\n    \n    Requirements:\n    - os\n    - shutil\n    - glob\n    \n    Example:\n    >>> copy_and_change_dir('/new_directory', '*_backup.txt')\n    \"\"\"\n    for file in glob.glob(pattern):\n        shutil.copy(file, COPY_DIR)\n    \n    os.chdir(owd)\n    return os.getcwd()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, subprocess\n```python\nimport os\nimport subprocess\n\n# Constants\nCOMMAND = 'ls -l'\n\ndef list_and_change_dir(owd):\n    \"\"\"\n    Change the working directory to the directory `owd` and before changing, \n    list the contents of the current directory using a subprocess.\n    \n    Parameters:\n    owd (str): The directory to which to change the working directory.\n    \n    Returns:\n    str: The new working directory.\n    \n    Requirements:\n    - os\n    - subprocess\n    \n    Example:\n    >>> list_and_change_dir('/new_directory')\n    \"\"\"\n    subprocess.run(COMMAND, shell=True)\n    \n    os.chdir(owd)\n    return os.getcwd()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, json, pathlib\n```python\nimport os\nimport json\nfrom pathlib import Path\n\n# Constants\nJSON_FILE = 'dir_info.json'\n\ndef save_info_and_change_dir(owd):\n    \"\"\"\n    Change the working directory to the directory `owd` and before changing, \n    save the path information of all files in the current directory to a JSON file.\n    \n    Parameters:\n    owd (str): The directory to which to change the working directory.\n    \n    Returns:\n    str: The new working directory.\n    \n    Requirements:\n    - os\n    - json\n    - pathlib\n    \n    Example:\n    >>> save_info_and_change_dir('/new_directory')\n    \"\"\"\n    path_info = [str(path) for path in Path('.').rglob('*')]\n\n    with open(JSON_FILE, 'w') as f:\n        json.dump(path_info, f)\n    \n    os.chdir(owd)\n    return os.getcwd()\n```"}
{"seed": "{\"task_id\": 14695134, \"prompt\": \"def f_14695134(c, testfield):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"c.execute(\\\"INSERT INTO test VALUES (?, 'bar')\\\", (testfield,))\", \"test_start\": \"\\nimport sqlite3\\n\\ndef check(candidate):\", \"test\": [\"\\n    conn = sqlite3.connect('dev.db')\\n    cur = conn.cursor()\\n    cur.execute(\\\"CREATE TABLE test (x VARCHAR(10), y VARCHAR(10))\\\")\\n    candidate(cur, 'kang')\\n    cur.execute(\\\"SELECT * FROM test\\\")\\n    rows = cur.fetchall()\\n    assert len(rows) == 1\\n\"], \"entry_point\": \"f_14695134\", \"intent\": \"insert data from a string `testfield` to sqlite db `c`\", \"library\": [\"sqlite3\"]}", "generation": "Scenario 1: \nsqlite3, pandas, random, matplotlib\n```python\nimport sqlite3\nimport pandas as pd\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nEMPLOYEE_NAMES = ['John', 'Mike', 'Anna', 'Sarah', 'Harry']\nDEPARTMENTS = ['Sales', 'Marketing', 'HR', 'IT', 'Finance']\n\ndef insert_random_employee_data(db_conn):\n    \"\"\"\n    Insert random employee data into a SQLite database and then visualize the department distribution.\n\n    Parameters:\n    db_conn (sqlite3.Connection): The SQLite database connection.\n\n    Requirements:\n    - sqlite3\n    - pandas\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> conn = sqlite3.connect('test.db')\n    >>> insert_random_employee_data(conn)\n    \"\"\"\n    cur = db_conn.cursor()\n    cur.execute(\"CREATE TABLE IF NOT EXISTS Employee (id INTEGER PRIMARY KEY, name TEXT, department TEXT)\")\n    for i in range(100):\n        cur.execute(\"INSERT INTO Employee VALUES (?, ?, ?)\", (i, EMPLOYEE_NAMES[randint(0, len(EMPLOYEE_NAMES)-1)], DEPARTMENTS[randint(0, len(DEPARTMENTS)-1)]))\n\n    db_conn.commit()\n\n    df = pd.read_sql_query(\"SELECT * from Employee\", db_conn)\n    df['department'].value_counts().plot(kind='bar')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nsqlite3, csv, os, shutil\n```python\nimport sqlite3\nimport csv\nimport os\nimport shutil\n\n# Constants\nDATABASE_FILE = 'test.db'\nCSV_EXPORT_FILE = 'export.csv'\nTEMP_DIRECTORY = 'temp_dir'\n\ndef export_database_to_csv(db_file, csv_file):\n    \"\"\"\n    Export the contents of a SQLite database to a CSV file.\n\n    Parameters:\n    db_file (str): The SQLite database file name.\n    csv_file (str): The CSV file name to export data to.\n\n    Requirements:\n    - sqlite3\n    - csv\n    - os\n    - shutil\n\n    Example:\n    >>> export_database_to_csv('test.db', 'export.csv')\n    \"\"\"\n    conn = sqlite3.connect(db_file)\n    cur = conn.cursor()\n\n    cur.execute(\"SELECT * FROM Employee\")\n    rows = cur.fetchall()\n\n    if not os.path.exists(TEMP_DIRECTORY):\n        os.makedirs(TEMP_DIRECTORY)\n\n    with open(os.path.join(TEMP_DIRECTORY, csv_file), 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerows(rows)\n\n    shutil.move(os.path.join(TEMP_DIRECTORY, csv_file), csv_file)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nsqlite3, datetime, pytz, random\n```python\nimport sqlite3\nfrom datetime import datetime\nimport pytz\nfrom random import randint\n\n# Constants\nUSER_NAMES = ['John', 'Mike', 'Anna', 'Sarah', 'Harry']\nMESSAGE_CONTENTS = ['Hello!', 'How are you?', 'Good morning.', 'Good night.', 'Happy weekend.']\nTIMEZONES = ['America/New_York', 'Europe/London', 'Asia/Shanghai', 'Asia/Tokyo', 'Australia/Sydney']\n\ndef insert_random_chat_data(db_conn):\n    \"\"\"\n    Insert random chat data (user, message, timestamp) into a SQLite database.\n\n    Parameters:\n    db_conn (sqlite3.Connection): The SQLite database connection.\n\n    Requirements:\n    - sqlite3\n    - datetime\n    - pytz\n    - random\n\n    Example:\n    >>> conn = sqlite3.connect('test.db')\n    >>> insert_random_chat_data(conn)\n    \"\"\"\n    cur = db_conn.cursor()\n    cur.execute(\"CREATE TABLE IF NOT EXISTS Chat (id INTEGER PRIMARY KEY, user TEXT, message TEXT, timestamp TEXT)\")\n\n    for i in range(100):\n        user = USER_NAMES[randint(0, len(USER_NAMES)-1)]\n        message = MESSAGE_CONTENTS[randint(0, len(MESSAGE_CONTENTS)-1)]\n        timestamp = datetime.now(pytz.timezone(TIMEZONES[randint(0, len(TIMEZONES)-1)])).isoformat()\n        cur.execute(\"INSERT INTO Chat VALUES (?, ?, ?, ?)\", (i, user, message, timestamp))\n\n    db_conn.commit()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nsqlite3, os, hashlib\n```python\nimport sqlite3\nimport os\nimport hashlib\n\n# Constants\nFILE_DIRECTORY = 'files'\nDATABASE_FILE = 'test.db'\n\ndef compute_file_hashes_and_store_to_db(directory, db_file):\n    \"\"\"\n    Compute the SHA256 hash of all files in a directory and store them into a SQLite database.\n\n    Parameters:\n    directory (str): The directory containing the files.\n    db_file (str): The SQLite database file name.\n\n    Requirements:\n    - sqlite3\n    - os\n    - hashlib\n\n    Example:\n    >>> compute_file_hashes_and_store_to_db('files', 'test.db')\n    \"\"\"\n    conn = sqlite3.connect(db_file)\n    cur = conn.cursor()\n\n    cur.execute(\"CREATE TABLE IF NOT EXISTS FileHashes (id INTEGER PRIMARY KEY, filename TEXT, hash TEXT)\")\n\n    for i, filename in enumerate(os.listdir(directory)):\n        with open(os.path.join(directory, filename), 'rb') as file:\n            data = file.read()\n            hash = hashlib.sha256(data).hexdigest()\n            cur.execute(\"INSERT INTO FileHashes VALUES (?, ?, ?)\", (i, filename, hash))\n\n    conn.commit()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nsqlite3, requests, bs4\n```python\nimport sqlite3\nimport requests\nfrom bs4 import BeautifulSoup\n\n# Constants\nURL = 'https://www.python.org/downloads/'\nDATABASE_FILE = 'test.db'\n\ndef scrape_python_versions_and_store_to_db(url, db_file):\n    \"\"\"\n    Scrape the Python versions from python.org and store them into a SQLite database.\n\n    Parameters:\n    url (str): The URL to scrape the Python versions from.\n    db_file (str): The SQLite database file name.\n\n    Requirements:\n    - sqlite3\n    - requests\n    - bs4\n\n    Example:\n    >>> scrape_python_versions_and_store_to_db('https://www.python.org/downloads/', 'test.db')\n    \"\"\"\n    conn = sqlite3.connect(db_file)\n    cur = conn.cursor()\n\n    cur.execute(\"CREATE TABLE IF NOT EXISTS PythonVersions (id INTEGER PRIMARY KEY, version TEXT, release_date TEXT)\")\n\n    response = requests.get(url)\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    for i, release in enumerate(soup.find_all('ol', class_='list-row-container menu')[0].find_all('li')):\n        version = release.find('span', class_='release-number').find('a').text.split(' ')[1]\n        release_date = release.find('span', class_='release-date').text\n        cur.execute(\"INSERT INTO PythonVersions VALUES (?, ?, ?)\", (i, version, release_date))\n\n    conn.commit()\n```"}
{"seed": "{\"task_id\": 24242433, \"prompt\": \"def f_24242433():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"b'\\\\\\\\x89\\\\\\\\n'.decode('unicode_escape')\", \"test_start\": \"\\nimport sqlite3\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate() == '\\\\x89\\\\n'\\n\"], \"entry_point\": \"f_24242433\", \"intent\": \"decode string \\\"\\\\\\\\x89\\\\\\\\n\\\" into a normal string\", \"library\": [\"sqlite3\"]}", "generation": "Scenario 1:\nsqlite3, pandas, os, csv\n```python\nimport sqlite3\nimport pandas as pd\nimport os\nimport csv\n\n# Constants\nDB_NAME = 'test_db.sqlite'\nDATA_FILE = 'data.csv'\n\ndef import_csv_to_db():\n    \"\"\"\n    Import data from a CSV file to a SQLite database.\n    \n    Returns:\n    bool: True if the operation is successful, False otherwise.\n    \n    Requirements:\n    - sqlite3\n    - pandas\n    - os\n    - csv\n    \n    Example:\n    >>> import_csv_to_db()\n    True\n    \"\"\"\n    if not os.path.isfile(DATA_FILE):\n        return False\n\n    conn = sqlite3.connect(DB_NAME)\n    df = pd.read_csv(DATA_FILE)\n    df.to_sql('test_table', conn, if_exists='replace', index=False)\n\n    conn.close()\n\n    return True\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 2:\nsqlite3, datetime, random\n```python\nimport sqlite3\nfrom datetime import datetime, timedelta\nfrom random import randint\n\n# Constants\nDB_NAME = 'test_db.sqlite'\n\ndef generate_random_date_records(num_records):\n    \"\"\"\n    Generate a specified number of records with a random date and insert them into a SQLite database.\n    \n    Parameters:\n    num_records (int): The number of records to generate.\n    \n    Returns:\n    bool: True if the operation is successful, False otherwise.\n    \n    Requirements:\n    - sqlite3\n    - datetime\n    - random\n    \n    Example:\n    >>> generate_random_date_records(10)\n    True\n    \"\"\"\n    conn = sqlite3.connect(DB_NAME)\n    cursor = conn.cursor()\n\n    for _ in range(num_records):\n        date = datetime.today() - timedelta(days=randint(1, 365))\n        cursor.execute(\"INSERT INTO date_records (date) VALUES (?)\", (date,))\n\n    conn.commit()\n    conn.close()\n\n    return True\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 3:\nsqlite3, time, datetime\n```python\nimport sqlite3\nimport time\nfrom datetime import datetime\n\n# Constants\nDB_NAME = 'test_db.sqlite'\n\ndef add_timestamp_record():\n    \"\"\"\n    Add a record with the current timestamp to a SQLite database.\n    \n    Returns:\n    bool: True if the operation is successful, False otherwise.\n    \n    Requirements:\n    - sqlite3\n    - time\n    - datetime\n    \n    Example:\n    >>> add_timestamp_record()\n    True\n    \"\"\"\n    conn = sqlite3.connect(DB_NAME)\n    cursor = conn.cursor()\n\n    timestamp = time.time()\n    date = datetime.fromtimestamp(timestamp)\n    cursor.execute(\"INSERT INTO timestamps (timestamp, date) VALUES (?, ?)\", (timestamp, date))\n\n    conn.commit()\n    conn.close()\n\n    return True\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 4:\nsqlite3, hashlib, binascii\n```python\nimport sqlite3\nimport hashlib\nimport binascii\n\n# Constants\nDB_NAME = 'test_db.sqlite'\nSALT = b'salt'\n\ndef add_hashed_password(username, password):\n    \"\"\"\n    Add a user to a SQLite database with a hashed password.\n    \n    Parameters:\n    username (str): The username.\n    password (str): The password.\n    \n    Returns:\n    bool: True if the operation is successful, False otherwise.\n    \n    Requirements:\n    - sqlite3\n    - hashlib\n    - binascii\n    \n    Example:\n    >>> add_hashed_password('test_user', 'test_password')\n    True\n    \"\"\"\n    conn = sqlite3.connect(DB_NAME)\n    cursor = conn.cursor()\n\n    # Hash the password\n    dk = hashlib.pbkdf2_hmac('sha256', password.encode(), SALT, 100000)\n    hashed_password = binascii.hexlify(dk).decode()\n    \n    cursor.execute(\"INSERT INTO users (username, password) VALUES (?, ?)\", (username, hashed_password))\n\n    conn.commit()\n    conn.close()\n\n    return True\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 5:\nsqlite3, os, shutil\n```python\nimport sqlite3\nimport os\nimport shutil\n\n# Constants\nDB_NAME = 'test_db.sqlite'\nBACKUP_PATH = 'db_backup'\n\ndef backup_database():\n    \"\"\"\n    Create a backup of a SQLite database.\n    \n    Returns:\n    bool: True if the operation is successful, False otherwise.\n    \n    Requirements:\n    - sqlite3\n    - os\n    - shutil\n    \n    Example:\n    >>> backup_database()\n    True\n    \"\"\"\n    if not os.path.isfile(DB_NAME):\n        return False\n\n    if not os.path.exists(BACKUP_PATH):\n        os.mkdir(BACKUP_PATH)\n\n    backup_file = os.path.join(BACKUP_PATH, f'{DB_NAME}_backup')\n    shutil.copy(DB_NAME, backup_file)\n\n    return True\n```"}
{"seed": "{\"task_id\": 24242433, \"prompt\": \"def f_24242433(raw_string):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"raw_string.decode('unicode_escape')\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(b\\\"Hello\\\") == \\\"Hello\\\"\\n\", \"\\n    assert candidate(b\\\"hello world!\\\") == \\\"hello world!\\\"\\n\", \"\\n    assert candidate(b\\\". ?? !!x\\\") == \\\". ?? !!x\\\"\\n\"], \"entry_point\": \"f_24242433\", \"intent\": \"convert a raw string `raw_string` into a normal string\", \"library\": []}", "generation": "Scenario 3:\njson, csv, os, datetime, base64\n```python\nimport json\nimport csv\nimport os\nfrom datetime import datetime\nimport base64\n\nOUTPUT_DIR = 'output'\n\ndef process_and_save_data(raw_string, filename):\n    \"\"\"\n    Processes a base64 encoded JSON string, saves the data to a CSV file and returns the path of the file.\n\n    Parameters:\n    raw_string (str): The base64 encoded JSON string.\n    filename (str): The name of the file to which the data should be saved (without extension).\n\n    Returns:\n    str: The path of the file.\n\n    Requirements:\n    - json\n    - csv\n    - os\n    - datetime\n    - base64\n\n    Example:\n    >>> process_and_save_data('eyJrZXkiOiAiVmFsdWUifQ==', 'data')\n    \"\"\"\n    # Decode the string and load the data\n    decoded_string = base64.b64decode(raw_string).decode('utf-8')\n    data = json.loads(decoded_string)\n\n    # Prepare the output directory\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n\n    # Prepare the file path\n    timestamp = datetime.now().strftime('%Y%m%d%H%M%S')\n    file_path = os.path.join(OUTPUT_DIR, f'{filename}_{timestamp}.csv')\n\n    # Save the data to the file\n    with open(file_path, 'w', newline='') as f:\n        writer = csv.writer(f)\n        for key, value in data.items():\n            writer.writerow([key, value])\n\n    return file_path\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nbase64, hashlib, binascii, datetime\n```python\nimport base64\nimport hashlib\nfrom binascii import hexlify, unhexlify\nfrom datetime import datetime\n\ndef hash_and_encode(raw_string, salt):\n    \"\"\"\n    Hashes a raw string with salt, encodes the hash in base64 and returns the encoded hash along with the current time.\n\n    Parameters:\n    raw_string (str): The raw string to hash.\n    salt (str): The salt to use for hashing.\n\n    Returns:\n    tuple: A tuple with the base64 encoded hash and the current time.\n\n    Requirements:\n    - base64\n    - hashlib\n    - binascii\n    - datetime\n\n    Example:\n    >>> hash_and_encode('Hello, World!', 'Salt')\n    \"\"\"\n    # Hash the string with salt\n    salted_string = raw_string + salt\n    hash_obj = hashlib.sha256(salted_string.encode())\n    hash_bytes = hash_obj.digest()\n\n    # Encode the hash in base64\n    encoded_hash = base64.b64encode(hash_bytes).decode('utf-8')\n\n    # Get the current time\n    current_time = datetime.now()\n\n    return (encoded_hash, current_time)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\njson, base64, zlib, hashlib\n```python\nimport json\nimport base64\nimport zlib\nimport hashlib\n\ndef process_data(raw_string):\n    \"\"\"\n    Decodes the raw string from base64, decompresses the data, hashes the decompressed data and returns the hash.\n\n    Parameters:\n    raw_string (str): The base64 encoded and zlib compressed string.\n\n    Returns:\n    str: The SHA256 hash of the decompressed data.\n\n    Requirements:\n    - json\n    - base64\n    - zlib\n    - hashlib\n\n    Example:\n    >>> process_data('eJzLSM3JyQcABiwCFQ==')\n    \"\"\"\n    # Decode the string from base64\n    decoded_string = base64.b64decode(raw_string)\n\n    # Decompress the data\n    decompressed_data = zlib.decompress(decoded_string)\n\n    # Hash the decompressed data\n    hash_obj = hashlib.sha256(decompressed_data)\n    hash_hex = hash_obj.hexdigest()\n\n    return hash_hex\n```\n`GPT_ODEX_BREAK`\n\nScenario 6:\nbase64, re, html, textwrap\n```python\nimport base64\nimport re\nfrom html import unescape\nimport textwrap\n\ndef clean_and_format(raw_string, line_length):\n    \"\"\"\n    Decodes a raw string from base64, unescapes HTML entities, replaces multiple spaces with a single space,\n    strips leading and trailing spaces and wraps the text to a specified line length.\n\n    Parameters:\n    raw_string (str): The base64 encoded string.\n    line_length (int): The maximum length of a line.\n\n    Returns:\n    str: The cleaned and formatted string.\n\n    Requirements:\n    - base64\n    - re\n    - html\n    - textwrap\n\n    Example:\n    >>> clean_and_format('SGVsbG8sICBXb3JsZCEgICAg', 5)\n    \"\"\"\n    # Decode the string from base64\n    decoded_string = base64.b64decode(raw_string).decode('utf-8')\n\n    # Unescape HTML entities\n    unescaped_string = unescape(decoded_string)\n\n    # Replace multiple spaces with a single space and strip leading and trailing spaces\n    cleaned_string = re.sub(' +', ' ', unescaped_string).strip()\n\n    # Wrap the text\n    wrapped_text = textwrap.fill(cleaned_string, line_length)\n\n    return wrapped_text\n```\n`GPT_ODEX_BREAK`\n\nScenario 7:\nbase64, binascii, hashlib, secrets\n```python\nimport base64\nimport binascii\nimport hashlib\nfrom secrets import token_bytes\n\ndef verify_signature(raw_string, salt, signature):\n    \"\"\"\n    Verifies a signature of a raw string with salt.\n\n    Parameters:\n    raw_string (str): The raw string.\n    salt (str): The salt.\n    signature (str): The base64 encoded signature.\n\n    Returns:\n    bool: True if the signature is valid, False otherwise.\n\n    Requirements:\n    - base64\n    - binascii\n    - hashlib\n    - secrets\n\n    Example:\n    >>> verify_signature('Hello, World!', 'Salt', 'SGVsbG8sIFdvcmxkISBTYWx0')\n    \"\"\"\n    # Hash the string with salt\n    salted_string = raw_string + salt\n    hash_obj = hashlib.sha256(salted_string.encode())\n    hash_bytes = hash_obj.digest()\n\n    # Compare the hash with the signature\n    signature_bytes = base64.b64decode(signature)\n    return hash_bytes == signature_bytes\n```"}
{"seed": "{\"task_id\": 24242433, \"prompt\": \"def f_24242433(raw_byte_string):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"raw_byte_string.decode('unicode_escape')\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(b\\\"Hello\\\") == \\\"Hello\\\"\\n\", \"\\n    assert candidate(b\\\"hello world!\\\") == \\\"hello world!\\\"\\n\", \"\\n    assert candidate(b\\\". ?? !!x\\\") == \\\". ?? !!x\\\"\\n\"], \"entry_point\": \"f_24242433\", \"intent\": \"convert a raw string `raw_byte_string` into a normal string\", \"library\": []}", "generation": "Scenario 1: \nsys, os, hashlib, base64\n```python\nimport sys\nimport os\nimport hashlib\nimport base64\n\ndef encrypt_file(file_path, password):\n    \"\"\"\n    Encrypt a given file using a password and save the encrypted file\n    to the same directory with '.enc' extension.\n    \n    Parameters:\n    file_path (str): The path to the file.\n    password (str): The password used for encryption.\n    \n    Requirements:\n    - sys\n    - os\n    - hashlib\n    - base64\n    \n    Example:\n    >>> encrypt_file('/path/to/file.txt', 'password123')\n    \"\"\"\n    key = hashlib.sha256(password.encode()).digest()\n    \n    with open(file_path, 'rb') as f:\n        data = f.read()\n        \n    encrypted = base64.b64encode(data)\n    \n    with open(f'{file_path}.enc', 'wb') as f:\n        f.write(encrypted)\n    \n    return f'{file_path}.enc'\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \npandas, numpy, matplotlib.pyplot, sklearn.preprocessing\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Constants\nDATA_PATH = 'path_to_data_file.csv'\n\ndef normalize_and_plot_data(data_path=DATA_PATH):\n    \"\"\"\n    Normalize a dataset and plot a histogram of the data.\n    \n    Parameters:\n    data_path (str): The path to the data file. Defaults to DATA_PATH.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.preprocessing\n    \n    Example:\n    >>> normalize_and_plot_data('path_to_data_file.csv')\n    \"\"\"\n    df = pd.read_csv(data_path)\n    data = df.to_numpy()\n    \n    scaler = MinMaxScaler()\n    normalized_data = scaler.fit_transform(data)\n    \n    plt.hist(normalized_data, bins=50)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nre, json, csv\n```python\nimport re\nimport json\nimport csv\n\ndef convert_json_to_csv(json_file, csv_file):\n    \"\"\"\n    Convert a JSON file to CSV.\n    \n    Parameters:\n    json_file (str): The path to the JSON file.\n    csv_file (str): The path to the CSV file.\n    \n    Requirements:\n    - re\n    - json\n    - csv\n    \n    Example:\n    >>> convert_json_to_csv('path_to_json_file.json', 'path_to_csv_file.csv')\n    \"\"\"\n    with open(json_file, 'r') as f:\n        data = json.load(f)\n        \n    csv_data = [data.keys()]\n    \n    for item in data.values():\n        csv_data.append(item)\n        \n    with open(csv_file, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerows(csv_data)\n    \n    return csv_file\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nrandom, string, hashlib\n```python\nimport random\nimport string\nimport hashlib\n\ndef generate_random_password(length, hash_password=False):\n    \"\"\"\n    Generate a random password of a given length. If hash_password is True, return the hashed password.\n    \n    Parameters:\n    length (int): The length of the password.\n    hash_password (bool): Whether to hash the password. Defaults to False.\n    \n    Requirements:\n    - random\n    - string\n    - hashlib\n    \n    Example:\n    >>> generate_random_password(10)\n    >>> generate_random_password(10, hash_password=True)\n    \"\"\"\n    password = ''.join(random.choice(string.ascii_letters + string.digits) for _ in range(length))\n    \n    if hash_password:\n        password = hashlib.sha256(password.encode()).hexdigest()\n        \n    return password\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nos, shutil, glob\n```python\nimport os\nimport shutil\nimport glob\n\ndef move_files_with_extension(source_dir, dest_dir, extension):\n    \"\"\"\n    Move all files with a certain extension from one directory to another.\n    \n    Parameters:\n    source_dir (str): The source directory.\n    dest_dir (str): The destination directory.\n    extension (str): The file extension.\n    \n    Requirements:\n    - os\n    - shutil\n    - glob\n    \n    Example:\n    >>> move_files_with_extension('path_to_source_dir', 'path_to_dest_dir', '.txt')\n    \"\"\"\n    files = glob.glob(os.path.join(source_dir, f'*.{extension}'))\n    \n    for file in files:\n        shutil.move(file, dest_dir)\n        \n    return len(files)\n```"}
{"seed": "{\"task_id\": 22882922, \"prompt\": \"def f_22882922(s):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"[m.group(0) for m in re.finditer('(\\\\\\\\d)\\\\\\\\1*', s)]\", \"test_start\": \"\\nimport re \\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('111234') == ['111', '2', '3', '4']\\n\"], \"entry_point\": \"f_22882922\", \"intent\": \"split a string `s` with into all strings of repeated characters\", \"library\": [\"re\"]}", "generation": "Scenario 1: \nre, collections, string, random\n```python\nimport re\nfrom collections import Counter\nimport string\nimport random\n\n# Constants\nLETTERS = string.ascii_lowercase\nNUMBERS = string.digits\n\ndef generate_random_string(length):\n    \"\"\"\n    Generate a random alphanumeric string of a given length and return the \n    frequency of repeated characters in the string.\n\n    Parameters:\n    length (int): The length of the string to be generated.\n\n    Returns:\n    dict: A dictionary with characters as keys and their frequencies as values.\n\n    Requirements:\n    - re\n    - collections.Counter\n    - string\n    - random\n\n    Example:\n    >>> random_string = generate_random_string(100)\n    >>> print(random_string)\n    \"\"\"\n    random_string = ''.join(random.choice(NUMBERS + LETTERS) for _ in range(length))\n    repeated_chars = [m.group(0) for m in re.finditer(r'(\\w)\\1*', random_string)]\n    \n    frequency = dict(Counter(repeated_chars))\n\n    return frequency\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nre, random, string, os\n```python\nimport re\nimport random\nimport string\nimport os\n\n# Constants\nLETTERS = string.ascii_lowercase\nFILENAME = 'random_string.txt'\n\ndef create_file_with_random_string(length):\n    \"\"\"\n    Generate a random string of a given length, write it to a file, and return \n    the count of characters repeated more than once in the string.\n\n    Parameters:\n    length (int): The length of the string to be generated.\n\n    Returns:\n    int: The count of characters repeated more than once.\n\n    Requirements:\n    - re\n    - random\n    - string\n    - os\n\n    Example:\n    >>> repetition_count = create_file_with_random_string(100)\n    >>> print(repetition_count)\n    \"\"\"\n    random_string = ''.join(random.choice(LETTERS) for _ in range(length))\n\n    with open(FILENAME, 'w') as file:\n        file.write(random_string)\n\n    repeated_chars = [m.group(0)[0] for m in re.finditer(r'(\\w)\\1*', random_string) if len(m.group(0)) > 1]\n\n    return len(set(repeated_chars))\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nre, json, string, random\n```python\nimport re\nimport json\nimport string\nimport random\n\n# Constants\nLETTERS = string.ascii_lowercase\nFILENAME = 'random_string.json'\n\ndef create_json_with_random_string(length):\n    \"\"\"\n    Generate a random string of a given length, write it to a JSON file, and return \n    a list of all strings with repeated characters in the string.\n\n    Parameters:\n    length (int): The length of the string to be generated.\n\n    Returns:\n    list: A list of all strings with repeated characters.\n\n    Requirements:\n    - re\n    - json\n    - string\n    - random\n\n    Example:\n    >>> repeated_strings = create_json_with_random_string(100)\n    >>> print(repeated_strings)\n    \"\"\"\n    random_string = ''.join(random.choice(LETTERS) for _ in range(length))\n\n    with open(FILENAME, 'w') as file:\n        json.dump({'random_string': random_string}, file)\n\n    repeated_strings = [m.group(0) for m in re.finditer(r'(\\w)\\1+', random_string)]\n\n    return repeated_strings\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nre, itertools, string, random\n```python\nimport re\nimport itertools\nimport string\nimport random\n\n# Constants\nLETTERS = string.ascii_lowercase\n\ndef generate_string_combinations(length, n):\n    \"\"\"\n    Generate all combinations of a random string of a given length and return \n    the combinations with repeated characters.\n\n    Parameters:\n    length (int): The length of the string to be generated.\n    n (int): The length of the combinations to be generated.\n\n    Returns:\n    list: A list of combinations with repeated characters.\n\n    Requirements:\n    - re\n    - itertools\n    - string\n    - random\n\n    Example:\n    >>> repeated_combinations = generate_string_combinations(10, 3)\n    >>> print(repeated_combinations)\n    \"\"\"\n    random_string = ''.join(random.choice(LETTERS) for _ in range(length))\n\n    combinations = list(itertools.combinations(random_string, n))\n\n    repeated_combinations = [\"\".join(comb) for comb in combinations if re.search(r'(\\w)\\1+', \"\".join(comb))]\n\n    return repeated_combinations\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nre, numpy, string, random\n```python\nimport re\nimport numpy as np\nimport string\nimport random\n\n# Constants\nLETTERS = string.ascii_lowercase\n\ndef create_random_matrix(size):\n    \"\"\"\n    Generate a random matrix of a given size with elements being strings of \n    repeated characters, and return the matrix and the total number of repeated characters.\n\n    Parameters:\n    size (int): The size of the matrix to be generated.\n\n    Returns:\n    ndarray, int: The generated matrix and the total number of repeated characters.\n\n    Requirements:\n    - re\n    - numpy\n    - string\n    - random\n\n    Example:\n    >>> matrix, total_repeated_chars = create_random_matrix(3)\n    >>> print(matrix)\n    >>> print(total_repeated_chars)\n    \"\"\"\n    matrix = np.empty((size, size), dtype=object)\n\n    total_repeated_chars = 0\n    for i in range(size):\n        for j in range(size):\n            length = random.randint(1, 5)\n            random_string = ''.join(random.choice(LETTERS) for _ in range(length))\n            matrix[i, j] = random_string\n            repeated_chars = [m.group(0) for m in re.finditer(r'(\\w)\\1+', random_string)]\n            total_repeated_chars += len(repeated_chars)\n\n    return matrix, total_repeated_chars\n```"}
{"seed": "{\"task_id\": 4143502, \"prompt\": \"def f_4143502():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"plt.scatter(np.random.randn(100), np.random.randn(100), facecolors='none')\", \"test_start\": \"\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert 'matplotlib' in str(type(candidate()))\\n\"], \"entry_point\": \"f_4143502\", \"intent\": \"scatter a plot with x, y position of `np.random.randn(100)` and face color equal to none\", \"library\": [\"matplotlib\", \"numpy\"]}", "generation": "Scenario 1: \nnumpy, matplotlib, random\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import choice\n\n# Constants\nCOLORS = ['red', 'blue', 'green', 'yellow', 'pink', 'black', 'orange']\n\ndef scatter_plot_with_random_color():\n    \"\"\"\n    Generate a scatter plot with x, y position of `np.random.randn(100)` and \n    face color randomly chosen from a list of colors.\n\n    Returns:\n    Matplotlib figure: A matplotlib figure object.\n\n    Requirements:\n    - matplotlib\n    - numpy\n    - random\n\n    Example:\n    >>> scatter_plot_with_random_color()\n    \"\"\"\n    color = choice(COLORS)\n    fig = plt.figure()\n    plt.scatter(np.random.randn(100), np.random.randn(100), facecolors=color)\n\n    return fig\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, matplotlib, scipy\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\ndef scatter_plot_with_normal_distribution():\n    \"\"\"\n    Generate a scatter plot with x, y positions drawn from normal distribution \n    and face color equal to none.\n\n    Returns:\n    Matplotlib figure: A matplotlib figure object.\n\n    Requirements:\n    - matplotlib\n    - numpy\n    - scipy\n\n    Example:\n    >>> scatter_plot_with_normal_distribution()\n    \"\"\"\n    mu, sigma = 0, 0.1 \n    x = np.random.normal(mu, sigma, 100)\n    y = np.random.normal(mu, sigma, 100)\n    \n    fig = plt.figure()\n    plt.scatter(x, y, facecolors='none')\n\n    return fig\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnumpy, matplotlib, pandas\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef scatter_plot_from_dataframe():\n    \"\"\"\n    Generate a scatter plot with x, y positions drawn from a pandas DataFrame \n    and face color equal to none.\n\n    Returns:\n    Matplotlib figure: A matplotlib figure object.\n\n    Requirements:\n    - matplotlib\n    - numpy\n    - pandas\n\n    Example:\n    >>> scatter_plot_from_dataframe()\n    \"\"\"\n    df = pd.DataFrame({'x': np.random.randn(100), 'y': np.random.randn(100)})\n    \n    fig = plt.figure()\n    plt.scatter(df['x'], df['y'], facecolors='none')\n\n    return fig\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, matplotlib, seaborn\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef scatter_plot_with_seaborn_style():\n    \"\"\"\n    Generate a scatter plot with x, y positions of `np.random.randn(100)` \n    and face color equal to none, styled with seaborn.\n\n    Returns:\n    Matplotlib figure: A matplotlib figure object.\n\n    Requirements:\n    - matplotlib\n    - numpy\n    - seaborn\n\n    Example:\n    >>> scatter_plot_with_seaborn_style()\n    \"\"\"\n    sns.set_style(\"darkgrid\")\n    \n    fig = plt.figure()\n    plt.scatter(np.random.randn(100), np.random.randn(100), facecolors='none')\n\n    return fig\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnumpy, matplotlib, math\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\n\ndef scatter_plot_with_sine_wave():\n    \"\"\"\n    Generate a scatter plot with x positions of `np.random.randn(100)` and \n    y positions as sine of x, with face color equal to none.\n\n    Returns:\n    Matplotlib figure: A matplotlib figure object.\n\n    Requirements:\n    - matplotlib\n    - numpy\n    - math\n\n    Example:\n    >>> scatter_plot_with_sine_wave()\n    \"\"\"\n    x = np.random.randn(100)\n    y = np.sin(x)\n    \n    fig = plt.figure()\n    plt.scatter(x, y, facecolors='none')\n\n    return fig\n```\nAbove is the illustration."}
{"seed": "{\"task_id\": 4143502, \"prompt\": \"def f_4143502():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"plt.plot(np.random.randn(100), np.random.randn(100), 'o', mfc='none')\", \"test_start\": \"\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert 'matplotlib' in str(type(candidate()[0]))\\n\"], \"entry_point\": \"f_4143502\", \"intent\": \"do a scatter plot with empty circles\", \"library\": [\"matplotlib\", \"numpy\"]}", "generation": "Scenario 1:\nnumpy, matplotlib, scipy.stats, pandas, seaborn\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nimport pandas as pd\nimport seaborn as sns\n\n# Constants\nSAMPLE_SIZE = 100\n\ndef plot_normal_distribution_with_kde():\n    \"\"\"\n    Generate a normally distributed random data of a given sample size, plot its histogram, \n    overlay a standard normal probability density function (PDF), and kernel density estimate (KDE).\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - scipy.stats\n    - pandas\n    - seaborn\n\n    Example:\n    >>> plot_normal_distribution_with_kde()\n    \"\"\"\n    # Generate normally distributed data\n    data = np.random.randn(SAMPLE_SIZE)\n\n    # Plot histogram\n    plt.hist(data, density=True, alpha=0.6, color='g')\n\n    # Plot PDF\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, 0, 1)\n    plt.plot(x, p, 'k', linewidth=2)\n\n    # Plot KDE\n    sns.kdeplot(data, color='darkblue');\n\n    plt.title(\"Normal Distribution with Histogram, PDF, and KDE\")\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, matplotlib, sklearn.cluster, sklearn.datasets\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import KMeans\n\n# Constants\nN_SAMPLES = 500\nN_CLUSTERS = 5\nRANDOM_STATE = 42\n\ndef plot_kmeans_clustering():\n    \"\"\"\n    Generate a 2D dataset with a specified number of samples, perform KMeans clustering, \n    and visualize the clusters using matplotlib.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.datasets\n    - sklearn.cluster\n\n    Example:\n    >>> plot_kmeans_clustering()\n    \"\"\"\n    # Generate dataset\n    X, y = make_blobs(n_samples=N_SAMPLES, random_state=RANDOM_STATE)\n\n    # Perform KMeans clustering\n    kmeans = KMeans(n_clusters=N_CLUSTERS)\n    kmeans.fit(X)\n\n    # Plot clusters\n    plt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_, cmap='viridis')\n    plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, matplotlib, pandas, seaborn\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n# Constants\nDATA_SIZE = 100\nCATEGORIES = ['A', 'B', 'C', 'D', 'E']\n\ndef plot_categorical_scatter():\n    \"\"\"\n    Generate a random scatter plot with empty circles for categorical data.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - pandas\n    - seaborn\n\n    Example:\n    >>> plot_categorical_scatter()\n    \"\"\"\n    # Generate data\n    x = np.random.choice(CATEGORIES, DATA_SIZE)\n    y = np.random.randn(DATA_SIZE)\n\n    # Create DataFrame\n    df = pd.DataFrame({'Category': x, 'Value': y})\n\n    # Scatter plot\n    sns.stripplot(x='Category', y='Value', data=df, jitter=True, size=8, linewidth=1, palette='Set1')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, matplotlib, sklearn.linear_model, sklearn.model_selection\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\n# Constants\nSAMPLE_SIZE = 100\n\ndef plot_linear_regression():\n    \"\"\"\n    Generate a dataset and split it into training and test sets. Fit a linear regression model \n    on the training set and visualize the results on a plot.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.linear_model\n    - sklearn.model_selection\n\n    Example:\n    >>> plot_linear_regression()\n    \"\"\"\n    # Generate dataset\n    X = np.random.rand(SAMPLE_SIZE, 1)\n    y = 2 + 3 * X + np.random.rand(SAMPLE_SIZE, 1)\n\n    # Split into training and test sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\n    # Fit linear regression model\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    # Predict\n    y_pred = model.predict(X_test)\n\n    # Plot\n    plt.scatter(X_test, y_test, color='blue')\n    plt.plot(X_test, y_pred, color='red', linewidth=2)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, matplotlib, scipy.optimize\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import curve_fit\n\n# Constants\nSAMPLE_SIZE = 100\n\ndef plot_curve_fitting():\n    \"\"\"\n    Generate a set of data points based on a function with some noise, fit a curve to the data \n    using SciPy's curve_fit function, and plot the original data along with the fitted curve.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - scipy.optimize\n\n    Example:\n    >>> plot_curve_fitting()\n    \"\"\"\n    # Define the function to be fitted\n    def func(x, a, b, c):\n        return a * np.exp(-b * x) + c\n\n    # Generate noisy data\n    x = np.linspace(0, 4, SAMPLE_SIZE)\n    y = func(x, 2.5, 1.3, 0.5)\n    yn = y + 0.2 * np.random.normal(size=len(x))\n\n    # Fit the curve\n    popt, pcov = curve_fit(func, x, yn)\n\n    # Plot the noisy data\n    plt.plot(x, yn, 'b-', label='data')\n\n    # Plot the fitted curve\n    plt.plot(x, func(x, *popt), 'r-', label='fit: a=%5.3f, b=%5.3f, c=%5.3f' % tuple(popt))\n\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.legend()\n    plt.show()\n```"}
{"seed": "{\"task_id\": 32063985, \"prompt\": \"def f_32063985(soup):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"soup.find('div', id='main-content').decompose()\", \"test_start\": \"\\nfrom bs4 import BeautifulSoup\\n\\ndef check(candidate):\", \"test\": [\"\\n    markup = \\\"<a>This is not div <div>This is div 1</div><div id='main-content'>This is div 2</div></a>\\\"\\n    soup = BeautifulSoup(markup,\\\"html.parser\\\")\\n    candidate(soup)\\n    assert str(soup) == '<a>This is not div <div>This is div 1</div></a>'\\n\"], \"entry_point\": \"f_32063985\", \"intent\": \"remove a div from `soup` with a id `main-content` using beautifulsoup\", \"library\": [\"bs4\"]}", "generation": "Scenario 1:\nbeautifulsoup4, os, re, pandas\n```python\nfrom bs4 import BeautifulSoup\nimport os\nimport re\nimport pandas as pd\n\n# Constants\nDIRECTORY = './html_files'\n\ndef extract_table_data(directory=DIRECTORY):\n    \"\"\"\n    Extract data from all tables in every HTML file in a given directory and \n    return as a list of pandas DataFrames. Filenames are used as table names.\n\n    Parameters:\n    directory (str): The directory where HTML files are located.\n\n    Returns:\n    list: A list of pandas DataFrames.\n\n    Requirements:\n    - beautifulsoup4\n    - os\n    - re\n    - pandas\n\n    Example:\n    >>> tables = extract_table_data()\n    >>> for table in tables:\n    >>>     print(table.head())\n    \"\"\"\n    data_frames = []\n    for filename in os.listdir(directory):\n        if filename.endswith(\".html\"):\n            with open(os.path.join(directory, filename), 'r') as f:\n                soup = BeautifulSoup(f, 'html.parser')\n\n            for table in soup.find_all('table'):\n                headers = []\n                rows = []\n\n                for th in table.find_all('th'):\n                    headers.append(th.text.strip())\n\n                for row in table.find_all('tr'):\n                    cells = [cell.text.strip() for cell in row.find_all('td')]\n                    if cells:\n                        rows.append(cells)\n\n                df = pd.DataFrame(rows, columns=headers)\n                df.name = re.sub('.html$', '', filename)\n                data_frames.append(df)\n\n    return data_frames\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nbeautifulsoup4, requests, csv, re\n```python\nfrom bs4 import BeautifulSoup\nimport requests\nimport csv\nimport re\n\n# Constants\nURL = 'https://example.com'\nCSV_FILE = 'output.csv'\n\ndef scrape_and_save_to_csv(url=URL, csv_file=CSV_FILE):\n    \"\"\"\n    Scrape data from a webpage, extract text from specific HTML tags, \n    clean the text using regex, and save the data to a CSV file.\n\n    Parameters:\n    url (str): The URL of the webpage to scrape.\n    csv_file (str): The name of the CSV file to save the data.\n\n    Returns:\n    str: The name of the CSV file where the data is saved.\n\n    Requirements:\n    - beautifulsoup4\n    - requests\n    - csv\n    - re\n\n    Example:\n    >>> scrape_and_save_to_csv()\n    \"\"\"\n    response = requests.get(url)\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    data = []\n    for div in soup.find_all('div', id='main-content'):\n        text = div.text.strip()\n        cleaned_text = re.sub('\\s+', ' ', text)\n        data.append(cleaned_text)\n\n    with open(csv_file, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerows(data)\n\n    return csv_file\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nbeautifulsoup4, pandas, requests, json\n```python\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport requests\nimport json\n\n# Constants\nURL = 'https://example.com'\nJSON_FILE = 'output.json'\n\ndef parse_html_and_save_to_json(url=URL, json_file=JSON_FILE):\n    \"\"\"\n    Parse an HTML page, extract specific data, store it in a pandas DataFrame, \n    and save the DataFrame to a JSON file.\n\n    Parameters:\n    url (str): The URL of the webpage to parse.\n    json_file (str): The name of the JSON file to save the DataFrame.\n\n    Returns:\n    str: The name of the JSON file where the DataFrame is saved.\n\n    Requirements:\n    - beautifulsoup4\n    - pandas\n    - requests\n    - json\n\n    Example:\n    >>> parse_html_and_save_to_json()\n    \"\"\"\n    response = requests.get(url)\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    data = []\n    for div in soup.find_all('div', id='main-content'):\n        text = div.text.strip()\n        data.append({'content': text})\n\n    df = pd.DataFrame(data)\n    df.to_json(json_file, orient='records')\n\n    return json_file\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nbeautifulsoup4, os, re, csv\n```python\nfrom bs4 import BeautifulSoup\nimport os\nimport re\nimport csv\n\n# Constants\nDIRECTORY = './html_files'\nCSV_FILE = 'output.csv'\n\ndef extract_links_and_save_to_csv(directory=DIRECTORY, csv_file=CSV_FILE):\n    \"\"\"\n    Extract all links from every HTML file in a given directory and save them \n    to a CSV file.\n\n    Parameters:\n    directory (str): The directory where HTML files are located.\n    csv_file (str): The name of the CSV file to save the links.\n\n    Returns:\n    str: The name of the CSV file where the links are saved.\n\n    Requirements:\n    - beautifulsoup4\n    - os\n    - re\n    - csv\n\n    Example:\n    >>> extract_links_and_save_to_csv()\n    \"\"\"\n    links = []\n    for filename in os.listdir(directory):\n        if filename.endswith(\".html\"):\n            with open(os.path.join(directory, filename), 'r') as f:\n                soup = BeautifulSoup(f, 'html.parser')\n\n            for link in soup.find_all('a', href=re.compile('^http')):\n                href = link.get('href')\n                links.append([href])\n\n    with open(csv_file, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerows(links)\n\n    return csv_file\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nbeautifulsoup4, requests, re, json\n```python\nfrom bs4 import BeautifulSoup\nimport requests\nimport re\nimport json\n\n# Constants\nURL = 'https://example.com'\nJSON_FILE = 'output.json'\n\ndef scrape_html_and_extract_data(url=URL, json_file=JSON_FILE):\n    \"\"\"\n    Scrape a webpage, extract phone numbers and emails using regex, \n    and save the extracted data to a JSON file.\n\n    Parameters:\n    url (str): The URL of the webpage to scrape.\n    json_file (str): The name of the JSON file to save the extracted data.\n\n    Returns:\n    str: The name of the JSON file where the extracted data is saved.\n\n    Requirements:\n    - beautifulsoup4\n    - requests\n    - re\n    - json\n\n    Example:\n    >>> scrape_html_and_extract_data()\n    \"\"\"\n    response = requests.get(url)\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    data = []\n    for div in soup.find_all('div', id='main-content'):\n        text = div.text.strip()\n\n        phone_numbers = re.findall(r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}', text)\n        emails = re.findall(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', text)\n\n        data.append({\n            'phone_numbers': phone_numbers,\n            'emails': emails\n        })\n\n    with open(json_file, 'w') as f:\n        json.dump(data, f)\n\n    return json_file\n```"}
{"seed": "{\"task_id\": 27975069, \"prompt\": \"def f_27975069(df):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"df[df['ids'].str.contains('ball')]\", \"test_start\": \"\\nimport pandas as pd\\n\\ndef check(candidate):\", \"test\": [\"\\n    f = pd.DataFrame([[\\\"ball1\\\", 1, 2], [\\\"hall\\\", 5, 4]], columns = ['ids', 'x', 'y'])\\n    f1 = candidate(f)\\n    assert f1['x'][0] == 1\\n    assert f1['y'][0] == 2\\n\"], \"entry_point\": \"f_27975069\", \"intent\": \"filter rows of datafram `df` containing key word `ball` in column `ids`\", \"library\": [\"pandas\"]}", "generation": "Scenario 1: \npandas, numpy, matplotlib, random, re\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\nimport re\n\n# Constants\nPRODUCT_CATEGORIES = ['Electronics', 'Clothing', 'Groceries', 'Books', 'Furniture']\nSALES_RANGE = (50, 1000)\n\ndef generate_sales_report(df, keyword):\n    \"\"\"\n    Generate a sales report which filters rows of dataframe `df` containing keyword `keyword` \n    in column `Product`, then calculates the total sales for each category and \n    plots a bar chart of the sales.\n\n    Parameters:\n    df (DataFrame): The original sales data.\n    keyword (str): The keyword to filter the product names.\n\n    Returns:\n    DataFrame: A DataFrame with the total sales for each category.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - random\n    - re\n\n    Example:\n    >>> data = {'Product': ['Electronics1', 'Clothing1', 'Groceries1', 'Books1', 'Furniture1'],\n                'Category': ['Electronics', 'Clothing', 'Groceries', 'Books', 'Furniture'],\n                'Sales': [randint(*SALES_RANGE) for _ in range(5)]}\n    >>> df = pd.DataFrame(data)\n    >>> report = generate_sales_report(df, '1')\n    >>> print(report)\n    >>> report.plot(kind='bar', x='Category', y='Sales')\n    \"\"\"\n    df_filtered = df[df['Product'].apply(lambda x: bool(re.search(keyword, x)))]\n    df_grouped = df_filtered.groupby('Category').sum().reset_index()\n\n    return df_grouped\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \npandas, numpy, matplotlib, random, re\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\nimport re\n\n# Constants\nTEAMS = ['Team1', 'Team2', 'Team3', 'Team4', 'Team5']\nSCORE_RANGE = (0, 100)\n\ndef generate_score_report(df, keyword):\n    \"\"\"\n    Generate a score report which filters rows of dataframe `df` containing keyword `keyword` \n    in column `Team`, then calculates the average score for each team and \n    plots a line chart of the scores.\n\n    Parameters:\n    df (DataFrame): The original scores data.\n    keyword (str): The keyword to filter the team names.\n\n    Returns:\n    DataFrame: A DataFrame with the average scores for each team.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - random\n    - re\n\n    Example:\n    >>> data = {'Team': ['Team1', 'Team2', 'Team3', 'Team4', 'Team5'],\n                'Score': [randint(*SCORE_RANGE) for _ in range(5)]}\n    >>> df = pd.DataFrame(data)\n    >>> report = generate_score_report(df, '1')\n    >>> print(report)\n    >>> report.plot(kind='line', x='Team', y='Average Score')\n    \"\"\"\n    df_filtered = df[df['Team'].apply(lambda x: bool(re.search(keyword, x)))]\n    df_grouped = df_filtered.groupby('Team').mean().reset_index()\n    df_grouped.columns = ['Team', 'Average Score']\n\n    return df_grouped\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \npandas, numpy, matplotlib, random, re\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\nimport re\n\n# Constants\nSTUDENTS = ['Student1', 'Student2', 'Student3', 'Student4', 'Student5']\nGRADE_RANGE = (0, 100)\n\ndef generate_grade_report(df, keyword):\n    \"\"\"\n    Generate a grade report which filters rows of dataframe `df` containing keyword `keyword` \n    in column `Student`, then calculates the median grade for each student and \n    plots a histogram of the grades.\n\n    Parameters:\n    df (DataFrame): The original grades data.\n    keyword (str): The keyword to filter the student names.\n\n    Returns:\n    DataFrame: A DataFrame with the median grades for each student.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - random\n    - re\n\n    Example:\n    >>> data = {'Student': ['Student1', 'Student2', 'Student3', 'Student4', 'Student5'],\n                'Grade': [randint(*GRADE_RANGE) for _ in range(5)]}\n    >>> df = pd.DataFrame(data)\n    >>> report = generate_grade_report(df, '1')\n    >>> print(report)\n    >>> report['Median Grade'].plot(kind='hist', bins=10)\n    \"\"\"\n    df_filtered = df[df['Student'].apply(lambda x: bool(re.search(keyword, x)))]\n    df_grouped = df_filtered.groupby('Student').median().reset_index()\n    df_grouped.columns = ['Student', 'Median Grade']\n\n    return df_grouped\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \npandas, numpy, matplotlib, random, re\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\nimport re\n\n# Constants\nCARS = ['Car1', 'Car2', 'Car3', 'Car4', 'Car5']\nMILEAGE_RANGE = (0, 200000)\n\ndef generate_mileage_report(df, keyword):\n    \"\"\"\n    Generate a mileage report which filters rows of dataframe `df` containing keyword `keyword` \n    in column `Car`, then calculates the maximum mileage for each car and \n    plots a pie chart of the mileages.\n\n    Parameters:\n    df (DataFrame): The original mileage data.\n    keyword (str): The keyword to filter the car names.\n\n    Returns:\n    DataFrame: A DataFrame with the maximum mileages for each car.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - random\n    - re\n\n    Example:\n    >>> data = {'Car': ['Car1', 'Car2', 'Car3', 'Car4', 'Car5'],\n                'Mileage': [randint(*MILEAGE_RANGE) for _ in range(5)]}\n    >>> df = pd.DataFrame(data)\n    >>> report = generate_mileage_report(df, '1')\n    >>> print(report)\n    >>> report['Max Mileage'].plot(kind='pie', autopct='%1.1f%%')\n    \"\"\"\n    df_filtered = df[df['Car'].apply(lambda x: bool(re.search(keyword, x)))]\n    df_grouped = df_filtered.groupby('Car').max().reset_index()\n    df_grouped.columns = ['Car', 'Max Mileage']\n\n    return df_grouped\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \npandas, numpy, matplotlib, random, re\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\nimport re\n\n# Constants\nCOMPANIES = ['Company1', 'Company2', 'Company3', 'Company4', 'Company5']\nREVENUE_RANGE = (100000, 1000000)\n\ndef generate_revenue_report(df, keyword):\n    \"\"\"\n    Generate a revenue report which filters rows of dataframe `df` containing keyword `keyword` \n    in column `Company`, then calculates the minimum revenue for each company and \n    plots a scatter plot of the revenues.\n\n    Parameters:\n    df (DataFrame): The original revenue data.\n    keyword (str): The keyword to filter the company names.\n\n    Returns:\n    DataFrame: A DataFrame with the minimum revenues for each company.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - random\n    - re\n\n    Example:\n    >>> data = {'Company': ['Company1', 'Company2', 'Company3', 'Company4', 'Company5'],\n                'Revenue': [randint(*REVENUE_RANGE) for _ in range(5)]}\n    >>> df = pd.DataFrame(data)\n    >>> report = generate_revenue_report(df, '1')\n    >>> print(report)\n    >>> report.plot(kind='scatter', x='Company', y='Min Revenue')\n    \"\"\"\n    df_filtered = df[df['Company'].apply(lambda x: bool(re.search(keyword, x)))]\n    df_grouped = df_filtered.groupby('Company').min().reset_index()\n    df_grouped.columns = ['Company', 'Min Revenue']\n\n    return df_grouped\n```\n"}
{"seed": "{\"task_id\": 20461165, \"prompt\": \"def f_20461165(df):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"df.reset_index(level=0, inplace=True)\", \"test_start\": \"\\nimport pandas as pd\\n\\ndef check(candidate):\", \"test\": [\"\\n    df = pd.DataFrame([[384, 593], [781, 123]], columns = ['gi', 'ptt_loc'])\\n    candidate(df)\\n    assert df['index'][0] == 0\\n    assert df['index'][1] == 1\\n\"], \"entry_point\": \"f_20461165\", \"intent\": \"convert index at level 0 into a column in dataframe `df`\", \"library\": [\"pandas\"]}", "generation": "Scenario 1:\npandas, numpy, matplotlib, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nCOLUMN_NAMES = ['Age', 'Height', 'Weight']\n\ndef generate_and_visualize_data(df):\n    \"\"\"\n    Generate and visualize the distribution of Age, Height and Weight from the given DataFrame. \n    The function first resets the DataFrame index and generates the columns 'Age', 'Height' and 'Weight' \n    with random values. Then, it plots the distribution of these columns.\n    \n    Parameters:\n    df (DataFrame): The input DataFrame.\n    \n    Returns:\n    DataFrame: The DataFrame with the added columns.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - seaborn\n    \n    Example:\n    >>> df = pd.DataFrame([[384, 593], [781, 123]], columns = ['gi', 'ptt_loc'])\n    >>> df = generate_and_visualize_data(df)\n    >>> print(df)\n    \"\"\"\n    df.reset_index(level=0, inplace=True)\n    \n    for col in COLUMN_NAMES:\n        df[col] = np.random.normal(loc=50, scale=10, size=df.shape[0])\n        \n    plt.figure(figsize=(10, 6))\n    for col in COLUMN_NAMES:\n        sns.kdeplot(df[col], shade=True)\n    plt.title('Distribution of Age, Height, and Weight')\n    plt.show()\n    \n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, scipy.stats\n```python\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import chi2_contingency\n\n# Constants\nCOLUMN_NAMES = ['A', 'B', 'C', 'D']\n\ndef chi_squared_test(df):\n    \"\"\"\n    Conduct a Chi-Squared test on the categorical variables in the given DataFrame. \n    The function first resets the DataFrame index and generates the categorical columns 'A', 'B', 'C' and 'D' \n    with random values. Then, it conducts the Chi-Squared test on these columns.\n\n    Parameters:\n    df (DataFrame): The input DataFrame.\n    \n    Returns:\n    float: The p-value from the Chi-Squared test.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - scipy.stats\n    \n    Example:\n    >>> df = pd.DataFrame([[384, 593], [781, 123]], columns = ['gi', 'ptt_loc'])\n    >>> p_value = chi_squared_test(df)\n    >>> print(p_value)\n    \"\"\"\n    df.reset_index(level=0, inplace=True)\n    \n    for col in COLUMN_NAMES:\n        df[col] = np.random.choice(['X', 'Y', 'Z'], size=df.shape[0])\n    \n    chi2, p, dof, expected = chi2_contingency(df[COLUMN_NAMES].values)\n    \n    return p\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, sklearn.preprocessing\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\n\n# Constants\nCOLUMN_NAMES = ['Temperature', 'Pressure', 'Humidity']\n\ndef standardize_data(df):\n    \"\"\"\n    Standardize the continuous variables in the given DataFrame. \n    The function first resets the DataFrame index and generates the columns 'Temperature', 'Pressure', \n    and 'Humidity' with random values. Then, it standardizes these columns.\n\n    Parameters:\n    df (DataFrame): The input DataFrame.\n    \n    Returns:\n    DataFrame: The DataFrame with the standardized columns.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.preprocessing\n    \n    Example:\n    >>> df = pd.DataFrame([[384, 593], [781, 123]], columns = ['gi', 'ptt_loc'])\n    >>> df = standardize_data(df)\n    >>> print(df)\n    \"\"\"\n    df.reset_index(level=0, inplace=True)\n    \n    for col in COLUMN_NAMES:\n        df[col] = np.random.normal(loc=50, scale=10, size=df.shape[0])\n        \n    scaler = StandardScaler()\n    df[COLUMN_NAMES] = scaler.fit_transform(df[COLUMN_NAMES])\n    \n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, sklearn.decomposition\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.decomposition import PCA\n\n# Constants\nCOLUMN_NAMES = ['Feature1', 'Feature2', 'Feature3', 'Feature4', 'Feature5']\n\ndef apply_pca(df):\n    \"\"\"\n    Apply Principal Component Analysis (PCA) to the variables in the given DataFrame. \n    The function first resets the DataFrame index and generates the columns 'Feature1' to 'Feature5' \n    with random values. Then, it applies PCA to these columns.\n\n    Parameters:\n    df (DataFrame): The input DataFrame.\n    \n    Returns:\n    DataFrame: The DataFrame with the principal components.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.decomposition\n    \n    Example:\n    >>> df = pd.DataFrame([[384, 593], [781, 123]], columns = ['gi', 'ptt_loc'])\n    >>> df = apply_pca(df)\n    >>> print(df)\n    \"\"\"\n    df.reset_index(level=0, inplace=True)\n    \n    for col in COLUMN_NAMES:\n        df[col] = np.random.normal(loc=0, scale=1, size=df.shape[0])\n        \n    pca = PCA(n_components=2)\n    principal_components = pca.fit_transform(df[COLUMN_NAMES])\n    \n    df = pd.DataFrame(data = principal_components, columns = ['Principal Component 1', 'Principal Component 2'])\n    \n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, sklearn.cluster\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cluster import KMeans\n\n# Constants\nCOLUMN_NAMES = ['Feature1', 'Feature2']\n\ndef perform_clustering(df):\n    \"\"\"\n    Perform K-Means clustering on the variables in the given DataFrame. \n    The function first resets the DataFrame index and generates the columns 'Feature1' and 'Feature2' \n    with random values. Then, it applies K-Means clustering to these columns.\n\n    Parameters:\n    df (DataFrame): The input DataFrame.\n    \n    Returns:\n    DataFrame: The DataFrame with the cluster labels.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.cluster\n    \n    Example:\n    >>> df = pd.DataFrame([[384, 593], [781, 123]], columns = ['gi', 'ptt_loc'])\n    >>> df = perform_clustering(df)\n    >>> print(df)\n    \"\"\"\n    df.reset_index(level=0, inplace=True)\n    \n    for col in COLUMN_NAMES:\n        df[col] = np.random.normal(loc=0, scale=1, size=df.shape[0])\n        \n    kmeans = KMeans(n_clusters=2)\n    df['Cluster'] = kmeans.fit_predict(df[COLUMN_NAMES])\n    \n    return df\n```"}
{"seed": "{\"task_id\": 20461165, \"prompt\": \"def f_20461165(df):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"df['index1'] = df.index\", \"test_start\": \"\\nimport pandas as pd\\n\\ndef check(candidate):\", \"test\": [\"\\n    df = pd.DataFrame([[384, 593], [781, 123]], columns = ['gi', 'ptt_loc'])\\n    candidate(df)\\n    assert df['index1'][0] == 0\\n    assert df['index1'][1] == 1\\n\"], \"entry_point\": \"f_20461165\", \"intent\": \"Add indexes in a data frame `df` to a column `index1`\", \"library\": [\"pandas\"]}", "generation": "Scenario 1: \npandas, sklearn.preprocessing, numpy\n```python\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\n\n# Constants\nNUMERICAL_COLUMNS = ['age', 'salary', 'years_of_experience']\n\ndef standardize_dataframe(df):\n    \"\"\"\n    Standardize numerical columns in the dataframe, add the standardized columns to the dataframe,\n    and return a dataframe with original and standardized columns. \n    \n    Parameters:\n    df (pandas.DataFrame): A dataframe containing the numerical columns.\n    \n    Returns:\n    pandas.DataFrame: A dataframe with original and standardized columns.\n    \n    Requirements:\n    - pandas\n    - sklearn.preprocessing\n    - numpy\n    \n    Example:\n    >>> df = pd.DataFrame({'age': [25, 35, 45], 'salary': [50000, 70000, 90000], 'years_of_experience': [2, 4, 6]})\n    >>> standardize_dataframe(df)\n    \"\"\"\n    scaler = StandardScaler()\n\n    df_scaled = df.copy()\n\n    for column in NUMERICAL_COLUMNS:\n        df_scaled[column + '_std'] = scaler.fit_transform(df[[column]])\n\n    return df_scaled\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \npandas, datetime, numpy\n```python\nimport pandas as pd\nfrom datetime import datetime\nimport numpy as np\n\n# Constants\nDATE_FORMAT = \"%Y-%m-%d\"\n\ndef add_age_and_day_of_week(df):\n    \"\"\"\n    Calculate the age from 'birth_date' column and day of the week from 'registered_date' column \n    in the dataframe, and add these calculated columns to the dataframe.\n    \n    Parameters:\n    df (pandas.DataFrame): A dataframe containing the 'birth_date' and 'registered_date' columns.\n    \n    Returns:\n    pandas.DataFrame: A dataframe with the added 'age' and 'day_of_week' columns.\n    \n    Requirements:\n    - pandas\n    - datetime\n    - numpy\n    \n    Example:\n    >>> df = pd.DataFrame({'birth_date': ['1980-05-22', '1990-08-15'], 'registered_date': ['2022-03-01', '2022-03-02']})\n    >>> add_age_and_day_of_week(df)\n    \"\"\"\n    current_year = datetime.now().year\n\n    df['birth_date'] = pd.to_datetime(df['birth_date'], format=DATE_FORMAT)\n    df['registered_date'] = pd.to_datetime(df['registered_date'], format=DATE_FORMAT)\n\n    df['age'] = current_year - df['birth_date'].dt.year\n    df['day_of_week'] = df['registered_date'].dt.day_name()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \npandas, matplotlib, seaborn\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nCOLUMNS_TO_PLOT = ['age', 'salary', 'years_of_experience']\n\ndef plot_histograms(df):\n    \"\"\"\n    Plot histograms for specified columns in the dataframe.\n    \n    Parameters:\n    df (pandas.DataFrame): A dataframe containing the columns to plot.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - pandas\n    - matplotlib\n    - seaborn\n    \n    Example:\n    >>> df = pd.DataFrame({'age': [25, 35, 45], 'salary': [50000, 70000, 90000], 'years_of_experience': [2, 4, 6]})\n    >>> plot_histograms(df)\n    \"\"\"\n    for column in COLUMNS_TO_PLOT:\n        plt.figure(figsize=(6, 4))\n        sns.histplot(df[column])\n        plt.title(f'Distribution of {column}')\n        plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \npandas, numpy, sklearn.linear_model\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\n# Constants\nTARGET_COLUMN = 'salary'\nFEATURE_COLUMNS = ['age', 'years_of_experience']\n\ndef train_linear_regression(df):\n    \"\"\"\n    Train a linear regression model using specified feature columns to predict the target column.\n    \n    Parameters:\n    df (pandas.DataFrame): A dataframe containing the feature and target columns.\n    \n    Returns:\n    sklearn.linear_model.LinearRegression: The trained linear regression model.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.linear_model\n    \n    Example:\n    >>> df = pd.DataFrame({'age': [25, 35, 45], 'salary': [50000, 70000, 90000], 'years_of_experience': [2, 4, 6]})\n    >>> model = train_linear_regression(df)\n    \"\"\"\n    X = df[FEATURE_COLUMNS]\n    y = df[TARGET_COLUMN]\n\n    model = LinearRegression()\n\n    model.fit(X, y)\n\n    return model\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \npandas, sklearn.model_selection, sklearn.ensemble\n```python\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Constants\nTARGET_COLUMN = 'target'\nRANDOM_STATE = 42\n\ndef train_random_forest(df):\n    \"\"\"\n    Split the dataframe into training and testing sets, and train a random forest classifier using the training set.\n    \n    Parameters:\n    df (pandas.DataFrame): A dataframe containing the features and the target column.\n    \n    Returns:\n    sklearn.ensemble.RandomForestClassifier: The trained random forest classifier.\n    \n    Requirements:\n    - pandas\n    - sklearn.model_selection\n    - sklearn.ensemble\n    \n    Example:\n    >>> df = pd.DataFrame({'feature1': [1, 2, 3, 4, 5], 'feature2': [5, 4, 3, 2, 1], 'target': [0, 1, 0, 1, 0]})\n    >>> model = train_random_forest(df)\n    \"\"\"\n    X = df.drop(TARGET_COLUMN, axis=1)\n    y = df[TARGET_COLUMN]\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE)\n\n    model = RandomForestClassifier(random_state=RANDOM_STATE)\n\n    model.fit(X_train, y_train)\n\n    return model\n```"}
{"seed": "{\"task_id\": 20461165, \"prompt\": \"def f_20461165(df):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"df.reset_index(level=['tick', 'obs'])\", \"test_start\": \"\\nimport pandas as pd\\n\\ndef check(candidate):\", \"test\": [\"\\n    df = pd.DataFrame([['2016-09-13', 'C', 2, 0.0139], ['2016-07-17', 'A', 2, 0.5577]], columns = ['tick', 'tag', 'obs', 'val'])\\n    df = df.set_index(['tick', 'tag', 'obs'])\\n    df = candidate(df)\\n    assert df['tick']['C'] == '2016-09-13'\\n\"], \"entry_point\": \"f_20461165\", \"intent\": \"convert pandas index in a dataframe `df` to columns\", \"library\": [\"pandas\"]}", "generation": "Scenario 1: \npandas, random, matplotlib, seaborn, numpy\n```python\nimport pandas as pd\nfrom random import randint, choices\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Constants\nPRODUCTS = ['product1', 'product2', 'product3', 'product4', 'product5']\nREGIONS = ['region1', 'region2', 'region3', 'region4', 'region5']\n\ndef generate_sales_report():\n    \"\"\"\n    Generate a sales report for a list of products across various regions.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with sales data for the products.\n    \n    Requirements:\n    - pandas\n    - random\n    - matplotlib.pyplot\n    - seaborn\n    - numpy\n    \n    Example:\n    >>> report = generate_sales_report()\n    >>> print(report)\n    >>> sns.heatmap(report.pivot(index='Product', columns='Region', values='Sales'))\n    \"\"\"\n    report_data = []\n\n    for product in PRODUCTS:\n        for region in REGIONS:\n            sales = randint(50, 500)\n            report_data.append([product, region, sales])\n\n    report_df = pd.DataFrame(report_data, columns=['Product', 'Region', 'Sales'])\n\n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, matplotlib, sklearn, numpy\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn import linear_model\nimport numpy as np\n\n# Constants\nYEARS = [i for i in range(2000, 2023)]\n\ndef generate_sales_prediction(data):\n    \"\"\"\n    Generate a sales prediction for the next year based on the given sales data.\n    \n    Parameters:\n    data (DataFrame): The sales data for past years.\n    \n    Returns:\n    float: The predicted sales for the next year.\n    \n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - sklearn\n    - numpy\n    \n    Example:\n    >>> data = pd.DataFrame({'Year': YEARS, 'Sales': np.random.randint(1000, 5000, len(YEARS))})\n    >>> prediction = generate_sales_prediction(data)\n    >>> print(prediction)\n    \"\"\"\n    X = data['Year'].values.reshape(-1, 1)\n    Y = data['Sales'].values\n\n    regr = linear_model.LinearRegression()\n    regr.fit(X, Y)\n\n    next_year = X[-1][0] + 1\n    prediction = regr.predict([[next_year]])\n\n    return prediction[0]\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, seaborn, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nCATEGORIES = ['cat1', 'cat2', 'cat3', 'cat4', 'cat5']\n\ndef generate_and_visualize_data():\n    \"\"\"\n    Generate and visualize data for different categories using Boxplot.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with generated data for the categories.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - seaborn\n    - matplotlib.pyplot\n    \n    Example:\n    >>> data = generate_and_visualize_data()\n    >>> print(data)\n    >>> sns.boxplot(x='Category', y='Value', data=data)\n    \"\"\"\n    data = []\n\n    for category in CATEGORIES:\n        values = np.random.normal(loc=np.random.randint(50, 100), scale=10.0, size=1000)\n        for value in values:\n            data.append([category, value])\n\n    df = pd.DataFrame(data, columns=['Category', 'Value'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, sklearn, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\n# Constants\nN_CLUSTERS = 3\n\ndef generate_and_cluster_data():\n    \"\"\"\n    Generate data and cluster it into groups using KMeans algorithm.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with generated data and their corresponding cluster.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.cluster\n    - matplotlib.pyplot\n    \n    Example:\n    >>> data = generate_and_cluster_data()\n    >>> print(data)\n    >>> plt.scatter(data['X'], data['Y'], c=data['Cluster'])\n    \"\"\"\n    X = -2 * np.random.rand(100, 2)\n    X1 = 1 + 2 * np.random.rand(50, 2)\n    X[50:100, :] = X1\n\n    kmeans = KMeans(n_clusters=N_CLUSTERS)\n    kmeans.fit(X)\n\n    df = pd.DataFrame(X, columns=['X', 'Y'])\n    df['Cluster'] = kmeans.labels_\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, scipy, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\n# Constants\nDISTRIBUTIONS = [stats.norm(loc=0, scale=1), stats.expon(scale=1), stats.uniform(loc=-1, scale=2)]\n\ndef generate_and_visualize_distribution_data():\n    \"\"\"\n    Generate and visualize data for different statistical distributions.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with generated data for the distributions.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - scipy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> data = generate_and_visualize_distribution_data()\n    >>> print(data)\n    >>> plt.hist(data['Value'], bins=50, density=True)\n    \"\"\"\n    data = []\n\n    for distribution in DISTRIBUTIONS:\n        values = distribution.rvs(size=1000)\n        for value in values:\n            data.append([str(distribution), value])\n\n    df = pd.DataFrame(data, columns=['Distribution', 'Value'])\n\n    return df\n```\n"}
{"seed": "{\"task_id\": 4685571, \"prompt\": \"def f_4685571(b):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"[x[::-1] for x in b]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    b = [('spam',0), ('eggs',1)]\\n    b1 = candidate(b)\\n    assert b1 == [(0, 'spam'), (1, 'eggs')]\\n\"], \"entry_point\": \"f_4685571\", \"intent\": \"Get reverse of list items from list 'b' using extended slicing\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCATEGORIES = ['Fruits', 'Vegetables', 'Meat', 'Dairy', 'Bakery']\n\ndef generate_sales_report(sales_data):\n    \"\"\"\n    Generate a bar chart of sales for different categories. The sales data is a list \n    of tuples, each tuple contains a category and its corresponding sales.\n    \n    Parameters:\n    sales_data (list of tuple): The sales data.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> sales_data = [('Fruits', 500), ('Vegetables', 750), ('Meat', 300), ('Dairy', 400), ('Bakery', 600)]\n    >>> generate_sales_report(sales_data)\n    \"\"\"\n    sales_df = pd.DataFrame(sales_data, columns=['Category', 'Sales'])\n    sales_df = sales_df.set_index('Category').T\n    sales_df.plot(kind='bar')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncollections, pandas, numpy\n```python\nimport pandas as pd\nimport numpy as np\nfrom collections import Counter\n\n# Constants\nANIMALS = ['Dog', 'Cat', 'Bird', 'Fish', 'Rabbit']\n\ndef count_animal_types(animal_list):\n    \"\"\"\n    Count the occurrence of each type of animal in a list, return a pandas \n    DataFrame sorted by the count in descending order.\n    \n    Parameters:\n    animal_list (list): The list of animals.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with animal types and their counts.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - collections.Counter\n    \n    Example:\n    >>> animal_list = ['Dog', 'Cat', 'Bird', 'Dog', 'Dog', 'Fish', 'Rabbit', 'Cat', 'Fish']\n    >>> count_animal_types(animal_list)\n    \"\"\"\n    counter = Counter(animal_list)\n    df = pd.DataFrame.from_dict(counter, orient='index').reset_index()\n    df.columns = ['Animal Type', 'Count']\n    df = df.sort_values(by='Count', ascending=False)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nitertools, pandas, numpy\n```python\nimport pandas as pd\nimport numpy as np\nfrom itertools import permutations\n\n# Constants\nWORDS = ['dog', 'cat', 'fish']\n\ndef generate_word_permutations(word_list):\n    \"\"\"\n    Generate all permutations of a list of words, return a pandas DataFrame \n    with permutations and their lengths.\n    \n    Parameters:\n    word_list (list): The list of words.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with permutations and their lengths.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - itertools.permutations\n    \n    Example:\n    >>> word_list = ['dog', 'cat', 'fish']\n    >>> generate_word_permutations(word_list)\n    \"\"\"\n    perms = list(permutations(word_list))\n    data = [(perm, len(perm)) for perm in perms]\n    df = pd.DataFrame(data, columns=['Permutation', 'Length'])\n    \n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, pandas, sklearn.preprocessing\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\n# Constants\nFEATURES = ['height', 'weight', 'age']\n\ndef standardize_features(data):\n    \"\"\"\n    Standardize a list of features using sklearn's StandardScaler, return \n    a pandas DataFrame with standardized features.\n    \n    Parameters:\n    data (list of tuple): The data.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with standardized features.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.preprocessing.StandardScaler\n    \n    Example:\n    >>> data = [(180, 75, 30), (170, 80, 35), (160, 70, 40)]\n    >>> standardize_features(data)\n    \"\"\"\n    df = pd.DataFrame(data, columns=FEATURES)\n    scaler = StandardScaler()\n    df[FEATURES] = scaler.fit_transform(df[FEATURES])\n    \n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nSTUDENTS = ['John', 'Mary', 'Tom', 'Sue', 'Bob']\n\ndef plot_student_scores(scores):\n    \"\"\"\n    Plot a line chart of scores for different students. The scores is a list \n    of tuples, each tuple contains a student and his/her corresponding score.\n    \n    Parameters:\n    scores (list of tuple): The scores.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> scores = [('John', 85), ('Mary', 90), ('Tom', 80), ('Sue', 95), ('Bob', 70)]\n    >>> plot_student_scores(scores)\n    \"\"\"\n    scores_df = pd.DataFrame(scores, columns=['Student', 'Score'])\n    scores_df = scores_df.set_index('Student').T\n    scores_df.plot(kind='line')\n    plt.show()\n```"}
{"seed": "{\"task_id\": 17960441, \"prompt\": \"def f_17960441(a, b):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"np.array([zip(x, y) for x, y in zip(a, b)])\", \"test_start\": \"\\nimport numpy as np\\n\\ndef check(candidate):\", \"test\": [\"\\n    a = np.array([[9, 8], [7, 6]])\\n    b = np.array([[7, 1], [5, 2]])\\n    c = candidate(a, b)\\n    expected = [(9, 7), (8, 1)]\\n    ctr = 0\\n    for i in c[0]:\\n        assert i == expected[ctr]\\n        ctr += 1\\n\"], \"entry_point\": \"f_17960441\", \"intent\": \"join each element in array `a` with element at the same index in array `b` as a tuple\", \"library\": [\"numpy\"]}", "generation": "Scenario 1: \nnumpy, pandas, scipy.stats, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLORS = ['Red', 'Blue', 'Green', 'Yellow', 'Black']\n\ndef calculate_statistics_and_plot(a, b):\n    \"\"\"\n    Join each element in array `a` with element at the same index in array `b` \n    as a tuple, calculate statistics for each pair (mean, median, mode) and plot \n    a histogram of the frequency of each pair.\n\n    Parameters:\n    a (numpy array): The first array.\n    b (numpy array): The second array.\n\n    Returns:\n    DataFrame: A pandas DataFrame with statistics for each pair.\n    Plot: A matplotlib histogram plot.\n\n    Requirements:\n    - numpy\n    - pandas\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Example:\n    >>> a = np.array([1, 2, 3, 4, 5])\n    >>> b = np.array([6, 7, 8, 9, 10])\n    >>> df, plot = calculate_statistics_and_plot(a, b)\n    >>> print(df)\n    >>> plt.show(plot)\n    \"\"\"\n    pairs = np.array([zip(x, y) for x, y in zip(a, b)])\n\n    data = []\n    for pair in pairs:\n        mean = np.mean(pair)\n        median = np.median(pair)\n        mode = stats.mode(pair)\n        data.append([pair, mean, median, mode])\n\n    df = pd.DataFrame(data, columns=['Pair', 'Mean', 'Median', 'Mode'])\n\n    plot = df['Pair'].value_counts().plot(kind='bar', color=COLORS)\n\n    return df, plot\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nnumpy, seaborn, sklearn.preprocessing, matplotlib\n```python\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nimport matplotlib.pyplot as plt\n\ndef encode_and_visualize_correlation(a, b):\n    \"\"\"\n    Join each element in array `a` with element at the same index in array `b` \n    as a tuple, encode the pairs, and visualize the correlation between the \n    original arrays and the encoded pairs using a heatmap.\n\n    Parameters:\n    a (numpy array): The first array.\n    b (numpy array): The second array.\n\n    Returns:\n    numpy array: The encoded pairs.\n    Plot: A seaborn heatmap plot.\n\n    Requirements:\n    - numpy\n    - seaborn\n    - sklearn.preprocessing\n    - matplotlib.pyplot\n\n    Example:\n    >>> a = np.array(['Dog', 'Cat', 'Bird', 'Fish', 'Reptile'])\n    >>> b = np.array(['Mammal', 'Mammal', 'Bird', 'Fish', 'Reptile'])\n    >>> encoded_pairs, plot = encode_and_visualize_correlation(a, b)\n    >>> print(encoded_pairs)\n    >>> plt.show(plot)\n    \"\"\"\n    pairs = np.array([zip(x, y) for x, y in zip(a, b)])\n\n    encoder = LabelEncoder()\n    encoded_pairs = encoder.fit_transform(pairs)\n\n    correlation_matrix = np.corrcoef([a, b, encoded_pairs])\n\n    plt.figure(figsize=(10, 10))\n    plot = sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n\n    return encoded_pairs, plot\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnumpy, pandas, sklearn.linear_model\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\ndef perform_linear_regression(a, b):\n    \"\"\"\n    Join each element in array `a` with element at the same index in array `b` \n    as a tuple, construct a DataFrame, and predict the value of `b` based on `a` \n    using linear regression.\n\n    Parameters:\n    a (numpy array): The independent variables.\n    b (numpy array): The dependent variable.\n\n    Returns:\n    LinearRegression: The fitted linear regression model.\n    float: The model's score.\n\n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.linear_model\n\n    Example:\n    >>> a = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]])\n    >>> b = np.array([2, 3, 4, 5, 6])\n    >>> model, score = perform_linear_regression(a, b)\n    >>> print(score)\n    \"\"\"\n    pairs = np.array([zip(x, y) for x, y in zip(a, b)])\n\n    df = pd.DataFrame(pairs, columns=['a', 'b'])\n\n    X = df[['a']]\n    y = df['b']\n\n    model = LinearRegression().fit(X, y)\n\n    score = model.score(X, y)\n\n    return model, score\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, pandas, sklearn.cluster, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\ndef perform_clustering(a, b):\n    \"\"\"\n    Join each element in array `a` with element at the same index in array `b` \n    as a tuple, construct a DataFrame, and perform KMeans clustering.\n\n    Parameters:\n    a (numpy array): The first array.\n    b (numpy array): The second array.\n\n    Returns:\n    KMeans: The fitted KMeans model.\n    Plot: A matplotlib scatter plot showing the clusters.\n\n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.cluster\n    - matplotlib.pyplot\n\n    Example:\n    >>> a = np.array([1, 2, 3, 4, 5])\n    >>> b = np.array([6, 7, 8, 9, 10])\n    >>> model, plot = perform_clustering(a, b)\n    >>> plt.show(plot)\n    \"\"\"\n    pairs = np.array([zip(x, y) for x, y in zip(a, b)])\n\n    df = pd.DataFrame(pairs, columns=['a', 'b'])\n\n    kmeans = KMeans(n_clusters=2)\n    kmeans.fit(df)\n\n    plt.figure(figsize=(10, 10))\n    plot = plt.scatter(df['a'], df['b'], c=kmeans.labels_)\n\n    return kmeans, plot\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnumpy, pandas, sklearn.decomposition, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\ndef perform_pca(a, b):\n    \"\"\"\n    Join each element in array `a` with element at the same index in array `b` \n    as a tuple, construct a DataFrame, and perform Principal Component Analysis (PCA).\n\n    Parameters:\n    a (numpy array): The first array.\n    b (numpy array): The second array.\n\n    Returns:\n    PCA: The fitted PCA model.\n    DataFrame: The DataFrame with the principal components.\n\n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.decomposition\n    - matplotlib.pyplot\n\n    Example:\n    >>> a = np.array([1, 2, 3, 4, 5])\n    >>> b = np.array([6, 7, 8, 9, 10])\n    >>> model, df_pca = perform_pca(a, b)\n    >>> print(df_pca)\n    \"\"\"\n    pairs = np.array([zip(x, y) for x, y in zip(a, b)])\n\n    df = pd.DataFrame(pairs, columns=['a', 'b'])\n\n    pca = PCA(n_components=1)\n    principal_components = pca.fit_transform(df)\n\n    df_pca = pd.DataFrame(data=principal_components, columns=['PC1'])\n\n    return pca, df_pca\n```"}
{"seed": "{\"task_id\": 17960441, \"prompt\": \"def f_17960441(a, b):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"np.array(list(zip(a.ravel(),b.ravel())), dtype=('i4,i4')).reshape(a.shape)\", \"test_start\": \"\\nimport numpy as np\\n\\ndef check(candidate):\", \"test\": [\"\\n    a = np.array([[9, 8], [7, 6]])\\n    b = np.array([[7, 1], [5, 2]])\\n    c = candidate(a, b)\\n    e = np.array([[(9, 7), (8, 1)], [(7, 5), (6, 2)]], dtype=[('f0', '<i4'), ('f1', '<i4')])\\n    assert np.array_equal(c, e)\\n\"], \"entry_point\": \"f_17960441\", \"intent\": \"zip two 2-d arrays `a` and `b`\", \"library\": [\"numpy\"]}", "generation": "Scenario 1:\nnumpy, matplotlib, random, math\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nimport math\n\n# Constants\nCOLORS = ['blue', 'green', 'red', 'cyan', 'magenta', 'yellow', 'black']\n\ndef plot_arrays(a, b):\n    \"\"\"\n    Given two 2D arrays `a` and `b`, generate a scatter plot of their zipped values.\n\n    Parameters:\n    a (numpy.array): The first 2D array.\n    b (numpy.array): The second 2D array.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - random\n    - math\n\n    Example:\n    >>> a = np.array([[9, 8], [7, 6]])\n    >>> b = np.array([[7, 1], [5, 2]])\n    >>> plot_arrays(a, b)\n    \"\"\"\n    zipped = np.array(list(zip(a.ravel(), b.ravel())), dtype=('i4,i4')).reshape(a.shape)\n\n    for i in range(zipped.shape[0]):\n        for j in range(zipped.shape[1]):\n            plt.scatter(*zipped[i, j], color=random.choice(COLORS))\n\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, scipy.stats, pandas\n```python\nimport numpy as np\nfrom scipy import stats\nimport pandas as pd\n\ndef calculate_statistics(a, b):\n    \"\"\"\n    Given two 2D arrays `a` and `b`, calculate descriptive statistics and perform t-test.\n\n    Parameters:\n    a (numpy.array): The first 2D array.\n    b (numpy.array): The second 2D array.\n\n    Returns:\n    DataFrame: A pandas DataFrame with descriptive statistics and p-value from t-test.\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - pandas\n\n    Example:\n    >>> a = np.array([[9, 8], [7, 6]])\n    >>> b = np.array([[7, 1], [5, 2]])\n    >>> stats = calculate_statistics(a, b)\n    >>> print(stats)\n    \"\"\"\n    zipped = np.array(list(zip(a.ravel(), b.ravel())), dtype=('i4,i4')).reshape(a.shape)\n\n    a_values = zipped[:, :, 0].ravel()\n    b_values = zipped[:, :, 1].ravel()\n\n    stats_a = stats.describe(a_values)\n    stats_b = stats.describe(b_values)\n\n    stats_result = stats.ttest_ind(a_values, b_values)\n\n    result = pd.DataFrame({\n        'Array': ['A', 'B'],\n        'Mean': [stats_a.mean, stats_b.mean],\n        'Variance': [stats_a.variance, stats_b.variance],\n        'Skewness': [stats_a.skewness, stats_b.skewness],\n        'Kurtosis': [stats_a.kurtosis, stats_b.kurtosis],\n        'T-test p-value': [stats_result.pvalue, stats_result.pvalue]\n    })\n\n    return result\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, PIL, matplotlib\n```python\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\ndef create_image(a, b):\n    \"\"\"\n    Given two 2D arrays `a` and `b`, create an image where the pixel values are obtained \n    by zipping the arrays and then normalizing.\n\n    Parameters:\n    a (numpy.array): The first 2D array.\n    b (numpy.array): The second 2D array.\n\n    Returns:\n    Image: A PIL Image object.\n\n    Requirements:\n    - numpy\n    - PIL\n    - matplotlib\n\n    Example:\n    >>> a = np.array([[9, 8], [7, 6]])\n    >>> b = np.array([[7, 1], [5, 2]])\n    >>> img = create_image(a, b)\n    >>> plt.imshow(img)\n    \"\"\"\n    zipped = np.array(list(zip(a.ravel(), b.ravel())), dtype=('i4,i4')).reshape(a.shape)\n\n    min_val = np.min(zipped)\n    max_val = np.max(zipped)\n\n    normalized = ((zipped - min_val) / (max_val - min_val) * 255).astype(np.uint8)\n\n    img = Image.fromarray(normalized)\n\n    return img\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, sklearn.decomposition, matplotlib\n```python\nimport numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\ndef visualize_pca(a, b):\n    \"\"\"\n    Given two 2D arrays `a` and `b`, perform PCA on the zipped arrays and visualize.\n\n    Parameters:\n    a (numpy.array): The first 2D array.\n    b (numpy.array): The second 2D array.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - sklearn.decomposition\n    - matplotlib\n\n    Example:\n    >>> a = np.array([[9, 8], [7, 6]])\n    >>> b = np.array([[7, 1], [5, 2]])\n    >>> visualize_pca(a, b)\n    \"\"\"\n    zipped = np.array(list(zip(a.ravel(), b.ravel())), dtype=('i4,i4')).reshape(a.shape)\n\n    pca = PCA(n_components=2)\n    pca_result = pca.fit_transform(zipped.reshape(-1, 2))\n\n    plt.scatter(pca_result[:, 0], pca_result[:, 1])\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, scipy.spatial.distance, pandas\n```python\nimport numpy as np\nfrom scipy.spatial import distance\nimport pandas as pd\n\ndef calculate_distances(a, b):\n    \"\"\"\n    Given two 2D arrays `a` and `b`, calculate Euclidean distances between corresponding pairs.\n\n    Parameters:\n    a (numpy.array): The first 2D array.\n    b (numpy.array): The second 2D array.\n\n    Returns:\n    DataFrame: A pandas DataFrame with distances.\n\n    Requirements:\n    - numpy\n    - scipy.spatial.distance\n    - pandas\n\n    Example:\n    >>> a = np.array([[9, 8], [7, 6]])\n    >>> b = np.array([[7, 1], [5, 2]])\n    >>> distances = calculate_distances(a, b)\n    >>> print(distances)\n    \"\"\"\n    zipped = np.array(list(zip(a.ravel(), b.ravel())), dtype=('i4,i4')).reshape(a.shape)\n\n    distances = distance.cdist(zipped.reshape(-1, 2), zipped.reshape(-1, 2))\n\n    df = pd.DataFrame(distances)\n\n    return df\n```"}
{"seed": "{\"task_id\": 438684, \"prompt\": \"def f_438684(list_of_ints):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"\\\"\\\"\\\",\\\"\\\"\\\".join([str(i) for i in list_of_ints])\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    list_of_ints = [8, 7, 6]\\n    assert candidate(list_of_ints) == '8,7,6'\\n\", \"\\n    list_of_ints = [0, 1, 6]\\n    assert candidate(list_of_ints) == '0,1,6'\\n\"], \"entry_point\": \"f_438684\", \"intent\": \"convert list `list_of_ints` into a comma separated string\", \"library\": []}", "generation": "Scenario 1:\npandas, numpy, matplotlib, random\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import choice\n\n# Constants\nCATEGORIES = ['Fruits','Vegetables','Dairy','Meat','Seafood']\n\n# Sample data\nPRODUCTS = ['Apple', 'Carrot', 'Milk', 'Beef', 'Fish']\n\ndef generate_product_list(list_of_ints):\n    \"\"\"\n    Generate a DataFrame of products with their categories and quantities based on \n    a list of integers representing quantities. Also plot a bar chart of the quantities.\n\n    Parameters:\n    list_of_ints (list): The list of integers representing quantities.\n\n    Returns:\n    DataFrame: A pandas DataFrame with products, their categories and quantities.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> quantities = [8, 7, 6, 5, 4]\n    >>> df = generate_product_list(quantities)\n    >>> print(df)\n    >>> df['Quantity'].plot(kind='bar')\n    \"\"\"\n    data = []\n    for qty, product in zip(list_of_ints, PRODUCTS):\n        category = choice(CATEGORIES)\n        data.append([product, category, qty])\n\n    df = pd.DataFrame(data, columns=['Product', 'Category', 'Quantity'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, datetime, dateutil, pytz\n```python\nimport numpy as np\nfrom datetime import datetime\nfrom dateutil.parser import parse\nimport pytz\n\n# Constants\nTIMEZONES = ['UTC', 'America/New_York', 'Europe/London', 'Asia/Shanghai', 'Asia/Tokyo', 'Australia/Sydney']\n\ndef convert_list_to_datetimes(list_of_ints, from_tz, to_tz):\n    \"\"\"\n    Convert a list of integers representing Unix timestamps into datetime strings\n    in a specified timezone.\n\n    Parameters:\n    list_of_ints (list): The list of integers representing Unix timestamps.\n    from_tz (str): The source timezone of the timestamps.\n    to_tz (str): The target timezone to convert the timestamps to.\n\n    Returns:\n    list: A list of datetime strings.\n\n    Requirements:\n    - numpy\n    - datetime\n    - dateutil.parser\n    - pytz\n\n    Example:\n    >>> timestamps = [1609459200, 1612137600, 1614556800]\n    >>> datetimes = convert_list_to_datetimes(timestamps, 'UTC', 'Asia/Tokyo')\n    >>> print(datetimes)\n    \"\"\"\n    from_tz = pytz.timezone(from_tz)\n    to_tz = pytz.timezone(to_tz)\n\n    datetimes = []\n    for ts in list_of_ints:\n        dt = datetime.fromtimestamp(ts, tz=from_tz)\n        dt = dt.astimezone(to_tz)\n        datetimes.append(dt.strftime('%Y-%m-%d %H:%M:%S'))\n    \n    return datetimes\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, random\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import randint\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef generate_dataframe(list_of_ints):\n    \"\"\"\n    Generate a DataFrame with specified columns and rows with random integers based on \n    a list of integers representing the maximum values for each row.\n\n    Parameters:\n    list_of_ints (list): The list of integers representing the maximum values for each row.\n\n    Returns:\n    DataFrame: A pandas DataFrame with random integers.\n\n    Requirements:\n    - pandas\n    - numpy\n    - random\n\n    Example:\n    >>> max_values = [10, 20, 30, 40, 50]\n    >>> df = generate_dataframe(max_values)\n    >>> print(df)\n    \"\"\"\n    df = pd.DataFrame(np.random.randint(0, [max_value for max_value in list_of_ints], size=(len(list_of_ints), len(COLUMNS))), columns=COLUMNS)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, matplotlib, random\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import sample\n\n# Constants\nCOLORS = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n\ndef plot_histogram(list_of_ints):\n    \"\"\"\n    Generate a histogram with a list of integers and randomly selected colors.\n\n    Parameters:\n    list_of_ints (list): The list of integers.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> values = [8, 7, 6, 5, 4]\n    >>> plot_histogram(values)\n    \"\"\"\n    color = sample(COLORS, 1)[0]\n    plt.hist(list_of_ints, color=color, edgecolor='black')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, sklearn.preprocessing\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Constants\nCOLUMNS = ['Feature1', 'Feature2', 'Feature3', 'Feature4', 'Feature5']\n\ndef normalize_data(list_of_ints):\n    \"\"\"\n    Normalize a list of integers using MinMaxScaler and convert it to a DataFrame.\n\n    Parameters:\n    list_of_ints (list): The list of integers.\n\n    Returns:\n    DataFrame: A pandas DataFrame with normalized values.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.preprocessing\n\n    Example:\n    >>> values = [8, 7, 6, 5, 4]\n    >>> df = normalize_data(values)\n    >>> print(df)\n    \"\"\"\n    scaler = MinMaxScaler()\n    normalized_values = scaler.fit_transform(np.array(list_of_ints).reshape(-1, 1))\n    df = pd.DataFrame(normalized_values, columns=COLUMNS)\n\n    return df\n```"}
{"seed": "{\"task_id\": 8519922, \"prompt\": \"def f_8519922(url, DATA, HEADERS_DICT, username, password):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"requests.post(url, data=DATA, headers=HEADERS_DICT, auth=(username, password))\", \"test_start\": \"\\nimport requests\\nfrom unittest.mock import Mock\\n\\ndef check(candidate):\", \"test\": [\"\\n    url='https://www.google.com'\\n    HEADERS_DICT = {'Accept':'text/json'}\\n    requests.post = Mock()\\n    try:\\n        candidate(url, \\\"{'name': 'abc'}\\\", HEADERS_DICT, 'admin', 'admin123')\\n    except:\\n        assert False\\n\"], \"entry_point\": \"f_8519922\", \"intent\": \"Send a post request with raw data `DATA` and basic authentication with `username` and `password`\", \"library\": [\"requests\"]}", "generation": "Scenario 1: \nrequests, json, base64, time\n```python\nimport requests\nimport json\nimport base64\nimport time\n\n# Constants\nURL = 'https://api.github.com'\nHEADER = {'Content-Type': 'application/json'}\nUSERNAME = 'user'\nPASSWORD = 'password'\n\ndef create_github_repo(repo_name, description):\n    \"\"\"\n    Send a POST request to GitHub API to create a new repository with the given name \n    and description using basic authentication.\n\n    Parameters:\n    repo_name (str): The name of the new repository.\n    description (str): The description of the new repository.\n\n    Returns:\n    dict: The response from the GitHub API.\n\n    Requirements:\n    - requests\n    - json\n    - base64\n    - time\n\n    Example:\n    >>> create_github_repo('new-repo', 'This is a new repo')\n    \"\"\"\n    url = f'{URL}/user/repos'\n    auth = (USERNAME, PASSWORD)\n    data = json.dumps({'name': repo_name, 'description': description, 'private': False})\n    \n    response = requests.post(url, data=data, headers=HEADER, auth=auth)\n    time.sleep(2)  # ensure the repo has time to be created\n\n    return response.json()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nrequests, csv, io, pandas\n```python\nimport requests\nimport csv\nimport io\nimport pandas as pd\n\n# Constants\nURL = 'https://api.github.com'\nHEADER = {'Content-Type': 'application/json'}\nUSERNAME = 'user'\nPASSWORD = 'password'\n\ndef fetch_user_repos(username):\n    \"\"\"\n    Fetch the list of repositories for a given GitHub user and return it as a pandas DataFrame.\n\n    Parameters:\n    username (str): The GitHub username.\n\n    Returns:\n    DataFrame: The list of repositories.\n\n    Requirements:\n    - requests\n    - csv\n    - io\n    - pandas\n\n    Example:\n    >>> fetch_user_repos('octocat')\n    \"\"\"\n    url = f'{URL}/users/{username}/repos'\n    auth = (USERNAME, PASSWORD)\n    \n    response = requests.get(url, headers=HEADER, auth=auth)\n    repos = response.json()\n\n    # Convert to DataFrame\n    repos_df = pd.json_normalize(repos)\n\n    return repos_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nrequests, json, os, time\n```python\nimport requests\nimport json\nimport os\nimport time\n\n# Constants\nAPI_KEY = 'your_api_key'\nHEADER = {'Content-Type': 'application/json'}\n\ndef send_sms(phone_number, message):\n    \"\"\"\n    Send a SMS to a given phone number using Twilio API.\n\n    Parameters:\n    phone_number (str): The phone number to send the SMS to.\n    message (str): The message to send.\n\n    Returns:\n    dict: The response from the Twilio API.\n\n    Requirements:\n    - requests\n    - json\n    - os\n    - time\n\n    Example:\n    >>> send_sms('+1234567890', 'Hello, World!')\n    \"\"\"\n    url = 'https://api.twilio.com/2010-04-01/Accounts/{}/Messages.json'.format(API_KEY)\n    data = {'To': phone_number, 'From': os.getenv('TWILIO_PHONE_NUMBER'), 'Body': message}\n    \n    response = requests.post(url, data=data, headers=HEADER)\n    time.sleep(2)  # ensure the message has time to be sent\n\n    return response.json()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nrequests, xmltodict, pandas\n```python\nimport requests\nimport xmltodict\nimport pandas as pd\n\n# Constants\nURL = 'http://www.example.com/api'\nHEADER = {'Content-Type': 'application/xml'}\nDATA = '<request><param1>value1</param1><param2>value2</param2></request>'\n\ndef fetch_data_from_api():\n    \"\"\"\n    Send a POST request with XML data to a given API and convert the XML response to a pandas DataFrame.\n\n    Returns:\n    DataFrame: The response data.\n\n    Requirements:\n    - requests\n    - xmltodict\n    - pandas\n\n    Example:\n    >>> fetch_data_from_api()\n    \"\"\"\n    response = requests.post(URL, data=DATA, headers=HEADER)\n    response_dict = xmltodict.parse(response.text)\n    \n    # Convert to DataFrame\n    response_df = pd.json_normalize(response_dict)\n\n    return response_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nrequests, json, time, hashlib\n```python\nimport requests\nimport json\nimport time\nimport hashlib\n\n# Constants\nAPI_KEY = 'your_api_key'\nAPI_SECRET = 'your_api_secret'\nHEADER = {'Content-Type': 'application/json'}\nURL = 'https://api.example.com/v1'\n\ndef create_order(symbol, side, quantity, price):\n    \"\"\"\n    Send a POST request to a trading API to create a new order.\n\n    Parameters:\n    symbol (str): The symbol of the asset to trade.\n    side (str): The side of the trade ('buy' or 'sell').\n    quantity (float): The quantity of the asset to trade.\n    price (float): The price of the asset.\n\n    Returns:\n    dict: The response from the API.\n\n    Requirements:\n    - requests\n    - json\n    - time\n    - hashlib\n\n    Example:\n    >>> create_order('BTCUSD', 'buy', 0.01, 10000)\n    \"\"\"\n    timestamp = int(time.time())\n    data = {\n        'symbol': symbol,\n        'side': side,\n        'quantity': quantity,\n        'price': price\n    }\n    payload = f'{timestamp}{json.dumps(data)}'\n    signature = hashlib.sha256((API_SECRET + payload).encode('utf-8')).hexdigest()\n    \n    headers = {\n        'X-API-KEY': API_KEY,\n        'X-SIGNATURE': signature,\n        'Content-Type': 'application/json'\n    }\n    \n    response = requests.post(f'{URL}/orders', data=json.dumps(data), headers=headers)\n\n    return response.json()\n```\n"}
{"seed": "{\"task_id\": 26443308, \"prompt\": \"def f_26443308():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"'abcd}def}'.rfind('}')\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate() == 8\\n\"], \"entry_point\": \"f_26443308\", \"intent\": \"Find last occurrence of character '}' in string \\\"abcd}def}\\\"\", \"library\": []}", "generation": "Scenario 1:\nnumpy, re, pandas, matplotlib\n\n```python\nimport numpy as np\nimport re\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nSTRINGS = ['abcd}def}', 'pqrs}tuv}', 'wxyz}123}', '456}789}', '0ab}cde}']\n\ndef analyze_string_patterns():\n    \"\"\"\n    Analyze the given list of strings for pattern occurrence, pattern positions, and visualize the results.\n\n    Returns:\n    DataFrame: A pandas DataFrame with results of pattern analysis.\n    \n    Requirements:\n    - numpy\n    - re\n    - pandas\n    - matplotlib.pyplot\n    \n    Example:\n    >>> results = analyze_string_patterns()\n    >>> print(results)\n    >>> results['Pattern Count'].plot(kind='bar')\n    \"\"\"\n    pattern = '}'\n    analysis_data = []\n\n    for string in STRINGS:\n        matches = [m.start() for m in re.finditer(pattern, string)]\n        analysis_data.append([string, len(matches), matches])\n\n    analysis_df = pd.DataFrame(analysis_data, columns=['String', 'Pattern Count', 'Pattern Positions'])\n\n    return analysis_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, re, datetime, pytz\n\n```python\nimport numpy as np\nimport re\nfrom datetime import datetime\nimport pytz\n\n# Constants\nLOGS = ['2021-06-15 09:45:00 ERROR: Failed to connect to database}',\n        '2021-06-15 10:15:00 WARNING: Low disk space}',\n        '2021-06-15 10:35:00 INFO: Backup completed successfully}']\n\ndef analyze_logs():\n    \"\"\"\n    Analyze the given list of logs for error occurrence, error times, and calculate the time difference between errors.\n\n    Returns:\n    list: A list with results of log analysis.\n    \n    Requirements:\n    - numpy\n    - re\n    - datetime\n    - pytz\n    \n    Example:\n    >>> analyze_logs()\n    \"\"\"\n    pattern = 'ERROR'\n    error_logs = []\n\n    for log in LOGS:\n        if re.search(pattern, log):\n            log_time = datetime.strptime(log.split(' ')[0]+' '+log.split(' ')[1], '%Y-%m-%d %H:%M:%S')\n            error_logs.append(log_time)\n\n    time_diffs = [t1-t0 for t0, t1 in zip(error_logs, error_logs[1:])]\n\n    return error_logs, time_diffs\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, re, random, collections\n\n```python\nimport numpy as np\nimport re\nimport random\nfrom collections import Counter\n\n# Constants\nSTRINGS = ['abcd}def}', 'pqrs}tuv}', 'wxyz}123}', '456}789}', '0ab}cde}']\n\ndef random_string_analysis():\n    \"\"\"\n    Randomly analyze the given list of strings for pattern occurrence, and count the occurrences.\n\n    Returns:\n    dict: A dictionary with results of random string analysis.\n    \n    Requirements:\n    - numpy\n    - re\n    - random\n    - collections\n    \n    Example:\n    >>> random_string_analysis()\n    \"\"\"\n    pattern = '}'\n    random_choices = random.choices(STRINGS, k=10)\n    pattern_counts = Counter([string.count(pattern) for string in random_choices])\n\n    return pattern_counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, re, pandas, matplotlib\n\n```python\nimport numpy as np\nimport re\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nSTRINGS = ['abcd}def}', 'pqrs}tuv}', 'wxyz}123}', '456}789}', '0ab}cde}']\n\ndef string_length_analysis():\n    \"\"\"\n    Analyze the given list of strings for pattern occurrence, and calculate the average length of strings.\n\n    Returns:\n    tuple: A tuple with results of string length analysis.\n    \n    Requirements:\n    - numpy\n    - re\n    - pandas\n    - matplotlib\n    \n    Example:\n    >>> string_length_analysis()\n    \"\"\"\n    pattern = '}'\n    lengths = [len(string) for string in STRINGS if pattern in string]\n    avg_length = np.mean(lengths)\n\n    return lengths, avg_length\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, re, datetime, pytz\n\n```python\nimport numpy as np\nimport re\nfrom datetime import datetime\nimport pytz\n\n# Constants\nLOGS = ['2021-06-15 09:45:00 ERROR: Failed to connect to database}',\n        '2021-06-15 10:15:00 WARNING: Low disk space}',\n        '2021-06-15 10:35:00 INFO: Backup completed successfully}']\n\ndef error_time_analysis():\n    \"\"\"\n    Analyze the given list of logs for error occurrence, and calculate the average time of error occurrence.\n\n    Returns:\n    tuple: A tuple with results of error time analysis.\n    \n    Requirements:\n    - numpy\n    - re\n    - datetime\n    - pytz\n    \n    Example:\n    >>> error_time_analysis()\n    \"\"\"\n    pattern = 'ERROR'\n    error_times = []\n\n    for log in LOGS:\n        if re.search(pattern, log):\n            log_time = datetime.strptime(log.split(' ')[0]+' '+log.split(' ')[1], '%Y-%m-%d %H:%M:%S')\n            error_times.append(log_time.time())\n\n    avg_time = datetime.strptime(str(sum(error_times, datetime.min.time())), '%H:%M:%S').time()\n\n    return error_times, avg_time\n```"}
{"seed": "{\"task_id\": 22365172, \"prompt\": \"def f_22365172():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"[item for item in [1, 2, 3]]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate() == [1,2,3]\\n\"], \"entry_point\": \"f_22365172\", \"intent\": \"Iterate ove list `[1, 2, 3]` using list comprehension\", \"library\": []}", "generation": "Scenario 1: \nnumpy, itertools, pandas, matplotlib\n```python\nimport numpy as np\nimport itertools\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nARRAY_SIZE = 10000\n\ndef create_array_and_calculate_statistics():\n    \"\"\"\n    Create a numpy array with random integers, calculate the mean and standard deviation,\n    and plot a histogram of the distribution.\n\n    Returns:\n    Tuple: A tuple containing the array, mean, and standard deviation.\n\n    Requirements:\n    - numpy\n    - itertools\n    - pandas\n    - matplotlib.pyplot\n    \n    Example:\n    >>> array, mean, std = create_array_and_calculate_statistics()\n    >>> print(mean, std)\n    >>> plt.hist(array, bins='auto')\n    \"\"\"\n    array = np.random.randint(1, 100, size=ARRAY_SIZE)\n    mean = np.mean(array)\n    std = np.std(array)\n\n    return array, mean, std\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nrandom, itertools, collections\n```python\nimport random\nimport itertools\nfrom collections import Counter\n\n# Constants\nLETTERS = ['a', 'b', 'c', 'd', 'e']\n\ndef generate_and_analyze_strings(length, count):\n    \"\"\"\n    Generate a number of random strings with a given length from a list of letters, \n    and analyze the frequency of each letter in the generated strings.\n\n    Parameters:\n    length (int): The length of each string.\n    count (int): The number of strings to generate.\n\n    Returns:\n    Counter: A Counter object with the frequency of each letter.\n\n    Requirements:\n    - random\n    - itertools\n    - collections.Counter\n    \n    Example:\n    >>> letter_frequency = generate_and_analyze_strings(5, 1000)\n    >>> print(letter_frequency)\n    \"\"\"\n    strings = [''.join(random.choices(LETTERS, k=length)) for _ in range(count)]\n    letter_frequency = Counter(itertools.chain(*strings))\n\n    return letter_frequency\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, itertools, matplotlib, random\n```python\nimport numpy as np\nimport itertools\nimport matplotlib.pyplot as plt\nimport random\n\n# Constants\nNUMBERS = list(range(1, 101))\n\ndef simulate_and_plot_dice_rolls(rolls):\n    \"\"\"\n    Simulate a number of dice rolls, calculate the frequency of each outcome, \n    and plot a histogram of the results.\n\n    Parameters:\n    rolls (int): The number of dice rolls.\n\n    Returns:\n    np.array: A numpy array with the frequency of each outcome.\n\n    Requirements:\n    - numpy\n    - itertools\n    - matplotlib.pyplot\n    - random\n    \n    Example:\n    >>> outcomes = simulate_and_plot_dice_rolls(10000)\n    >>> print(outcomes)\n    >>> plt.hist(outcomes, bins=np.arange(1, 7+1.5)-0.5, edgecolor='black')\n    \"\"\"\n    outcomes = [random.choice(NUMBERS) for _ in range(rolls)]\n    frequencies = np.bincount(outcomes, minlength=7)\n\n    return frequencies\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nitertools, collections, random\n```python\nimport itertools\nfrom collections import Counter\nimport random\n\n# Constants\nLETTERS = ['a', 'b', 'c', 'd', 'e']\n\ndef generate_and_analyze_pairs(count):\n    \"\"\"\n    Generate a number of random pairs from a list of letters, \n    and analyze the frequency of each pair.\n\n    Parameters:\n    count (int): The number of pairs to generate.\n\n    Returns:\n    Counter: A Counter object with the frequency of each pair.\n\n    Requirements:\n    - itertools\n    - collections.Counter\n    - random\n    \n    Example:\n    >>> pair_frequency = generate_and_analyze_pairs(1000)\n    >>> print(pair_frequency)\n    \"\"\"\n    pairs = [tuple(random.choices(LETTERS, k=2)) for _ in range(count)]\n    pair_frequency = Counter(pairs)\n\n    return pair_frequency\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, matplotlib, random\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\n\n# Constants\nARRAY_SIZE = 10000\n\ndef generate_and_plot_random_walk():\n    \"\"\"\n    Generate a random walk and plot the progression of the walk over time.\n\n    Returns:\n    np.array: A numpy array with the positions of the walk at each step.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - random\n    \n    Example:\n    >>> walk = generate_and_plot_random_walk()\n    >>> print(walk)\n    >>> plt.plot(walk)\n    \"\"\"\n    steps = [1 if random.random() > 0.5 else -1 for _ in range(ARRAY_SIZE)]\n    walk = np.cumsum(steps)\n\n    return walk\n```"}
{"seed": "{\"task_id\": 12300912, \"prompt\": \"def f_12300912(d):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"[(x['x'], x['y']) for x in d]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    data = [{'x': 1, 'y': 10}, {'x': 3, 'y': 15}, {'x': 2, 'y': 1}]\\n    res = candidate(data)\\n    assert res == [(1, 10), (3, 15), (2, 1)]\\n\"], \"entry_point\": \"f_12300912\", \"intent\": \"extract all the values with keys 'x' and 'y' from a list of dictionaries `d` to list of tuples\", \"library\": []}", "generation": "Scenario 1:\n```python\nimport pandas as pd\nimport numpy as np\nfrom statistics import mean\n\n# Constants\nKEYS = ['x', 'y', 'z']\n\ndef calculate_statistics(d):\n    \"\"\"\n    Calculate mean, sum, max, min, and standard deviation for 'x', 'y', and 'z' keys from a list of dictionaries `d`.\n    \n    Parameters:\n    d (list): A list of dictionaries.\n\n    Returns:\n    dict: A dictionary with keys as 'x', 'y', and 'z' and values as dictionaries of statistics.\n\n    Requirements:\n    - pandas\n    - numpy\n    - statistics\n\n    Example:\n    >>> data = [{'x': 1, 'y': 10, 'z': 5}, {'x': 3, 'y': 15, 'z': 6}, {'x': 2, 'y': 1, 'z': 7}]\n    >>> calculate_statistics(data)\n    \"\"\"\n    df = pd.DataFrame(d)\n    stats = {}\n\n    for key in KEYS:\n        stats[key] = {\n            'mean': mean(df[key]),\n            'sum': np.sum(df[key]),\n            'max': np.max(df[key]),\n            'min': np.min(df[key]),\n            'std': np.std(df[key])\n        }\n\n    return stats\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\n```python\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Constants\nKEYS = ['x', 'y', 'z']\n\ndef scale_values(d):\n    \"\"\"\n    Scale all the values with keys 'x', 'y', and 'z' from a list of dictionaries `d` using MinMaxScaler.\n\n    Parameters:\n    d (list): A list of dictionaries.\n\n    Returns:\n    DataFrame: A pandas DataFrame with scaled values.\n\n    Requirements:\n    - pandas\n    - sklearn.preprocessing.MinMaxScaler\n\n    Example:\n    >>> data = [{'x': 1, 'y': 10, 'z': 5}, {'x': 3, 'y': 15, 'z': 6}, {'x': 2, 'y': 1, 'z': 7}]\n    >>> scale_values(data)\n    \"\"\"\n    df = pd.DataFrame(d)\n    scaler = MinMaxScaler()\n    scaled_df = pd.DataFrame(scaler.fit_transform(df[KEYS]), columns=KEYS)\n\n    return scaled_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\n```python\nimport pandas as pd\nfrom collections import Counter\n\n# Constants\nKEYS = ['x', 'y', 'z']\n\ndef count_occurrences(d):\n    \"\"\"\n    Count the occurrences of values with keys 'x', 'y', and 'z' from a list of dictionaries `d`.\n\n    Parameters:\n    d (list): A list of dictionaries.\n\n    Returns:\n    dict: A dictionary with keys as 'x', 'y', and 'z' and values as Counter objects.\n\n    Requirements:\n    - pandas\n    - collections.Counter\n\n    Example:\n    >>> data = [{'x': 1, 'y': 10, 'z': 5}, {'x': 3, 'y': 15, 'z': 5}, {'x': 2, 'y': 1, 'z': 7}]\n    >>> count_occurrences(data)\n    \"\"\"\n    df = pd.DataFrame(d)\n    counts = {}\n\n    for key in KEYS:\n        counts[key] = dict(Counter(df[key]))\n\n    return counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\n```python\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\n# Constants\nKEYS = ['x', 'y', 'z']\n\ndef linear_regression(d, target='z'):\n    \"\"\"\n    Perform linear regression on 'x', 'y', against 'z' from a list of dictionaries `d`.\n\n    Parameters:\n    d (list): A list of dictionaries.\n    target (str): The target variable for the regression.\n\n    Returns:\n    LinearRegression: A LinearRegression model.\n\n    Requirements:\n    - pandas\n    - sklearn.linear_model.LinearRegression\n\n    Example:\n    >>> data = [{'x': 1, 'y': 10, 'z': 5}, {'x': 3, 'y': 15, 'z': 6}, {'x': 2, 'y': 1, 'z': 7}]\n    >>> model = linear_regression(data)\n    \"\"\"\n    df = pd.DataFrame(d)\n    predictors = KEYS.copy()\n    predictors.remove(target)\n\n    X = df[predictors]\n    y = df[target]\n\n    model = LinearRegression().fit(X, y)\n\n    return model\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nKEYS = ['x', 'y', 'z']\n\ndef plot_values(d):\n    \"\"\"\n    Plot all the values with keys 'x', 'y', and 'z' from a list of dictionaries `d`.\n\n    Parameters:\n    d (list): A list of dictionaries.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> data = [{'x': 1, 'y': 10, 'z': 5}, {'x': 3, 'y': 15, 'z': 6}, {'x': 2, 'y': 1, 'z': 7}]\n    >>> plot_values(data)\n    \"\"\"\n    df = pd.DataFrame(d)\n\n    for key in KEYS:\n        plt.plot(df[key], label=key)\n\n    plt.legend()\n    plt.show()\n```\n"}
{"seed": "{\"task_id\": 678236, \"prompt\": \"def f_678236():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"os.path.splitext(os.path.basename('hemanth.txt'))[0]\", \"test_start\": \"\\nimport os \\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate() == \\\"hemanth\\\"\\n\"], \"entry_point\": \"f_678236\", \"intent\": \"get the filename without the extension from file 'hemanth.txt'\", \"library\": [\"os\"]}", "generation": "Scenario 1: \nos, glob, re, csv\n```python\nimport os\nimport glob\nimport re\nimport csv\n\n# Constants\nFILE_PATH = '/home/user/data/'\nFILE_EXTENSION = '.csv'\n\ndef extract_data_from_csv_files():\n    \"\"\"\n    Extracts and returns data from all CSV files in a specified directory. \n    The function gets the filename without the extension and uses it as a key in the output dictionary.\n\n    Requirements:\n    - os\n    - glob\n    - re\n    - csv\n\n    Example:\n    >>> data = extract_data_from_csv_files()\n    >>> print(data['file1'])\n    \"\"\"\n    data = {}\n\n    for file in glob.glob(os.path.join(FILE_PATH, '*' + FILE_EXTENSION)):\n        filename = os.path.splitext(os.path.basename(file))[0]\n        with open(file, 'r') as f:\n            reader = csv.reader(f)\n            data[filename] = list(reader)\n\n    return data\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, shutil, re, zipfile\n```python\nimport os\nimport shutil\nimport re\nimport zipfile\n\n# Constants\nSOURCE_DIR = '/home/user/data/'\nTARGET_DIR = '/home/user/data_processed/'\nARCHIVE_NAME = 'archive.zip'\n\ndef archive_processed_files():\n    \"\"\"\n    Archives all the processed files in a directory. \n    The function identifies processed files by the '_processed' suffix in the filename.\n\n    Requirements:\n    - os\n    - shutil\n    - re\n    - zipfile\n\n    Example:\n    >>> archive_processed_files()\n    \"\"\"\n    with zipfile.ZipFile(os.path.join(TARGET_DIR, ARCHIVE_NAME), 'w') as archive:\n        for file in os.listdir(SOURCE_DIR):\n            if re.search(r'_processed$', os.path.splitext(file)[0]):\n                archive.write(os.path.join(SOURCE_DIR, file), arcname=file)\n                shutil.move(os.path.join(SOURCE_DIR, file), TARGET_DIR)\n\n    return True\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, sys, re, argparse\n```python\nimport os\nimport sys\nimport re\nimport argparse\n\n# Constants\nFILE_EXTENSION = '.txt'\n\ndef rename_files_with_pattern(pattern, replacement, directory):\n    \"\"\"\n    Renames all the files in a directory that match a certain pattern.\n\n    Parameters:\n    pattern (str): The pattern to search for in the filenames.\n    replacement (str): The string to replace the pattern with.\n    directory (str): The directory in which to search for files.\n\n    Requirements:\n    - os\n    - sys\n    - re\n    - argparse\n\n    Example:\n    >>> rename_files_with_pattern('draft', 'final', '/home/user/documents')\n    \"\"\"\n    for file in os.listdir(directory):\n        if re.search(pattern, file):\n            new_filename = re.sub(pattern, replacement, file)\n            os.rename(os.path.join(directory, file), os.path.join(directory, new_filename))\n\n    return True\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, hashlib, re, json\n```python\nimport os\nimport hashlib\nimport re\nimport json\n\n# Constants\nFILE_EXTENSION = '.txt'\nDIRECTORY = '/home/user/data/'\n\ndef generate_checksums_for_files():\n    \"\"\"\n    Generates checksums for all files in a directory and saves them in a JSON file.\n\n    Requirements:\n    - os\n    - hashlib\n    - re\n    - json\n\n    Example:\n    >>> generate_checksums_for_files()\n    \"\"\"\n    checksums = {}\n\n    for file in os.listdir(DIRECTORY):\n        if re.search(FILE_EXTENSION + '$', file):\n            with open(os.path.join(DIRECTORY, file), 'rb') as f:\n                checksums[file] = hashlib.md5(f.read()).hexdigest()\n\n    with open(os.path.join(DIRECTORY, 'checksums.json'), 'w') as f:\n        json.dump(checksums, f)\n\n    return checksums\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, pandas, re, matplotlib.pyplot\n```python\nimport os\nimport pandas as pd\nimport re\nimport matplotlib.pyplot as plt\n\n# Constants\nDIRECTORY = '/home/user/data/'\nPATTERN = r'^sales_data_\\d{4}.csv$'\n\ndef plot_sales_data():\n    \"\"\"\n    Plots sales data from all CSV files in a directory that match a certain pattern.\n\n    Requirements:\n    - os\n    - pandas\n    - re\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_sales_data()\n    \"\"\"\n    for file in os.listdir(DIRECTORY):\n        if re.match(PATTERN, file):\n            df = pd.read_csv(os.path.join(DIRECTORY, file))\n            df.plot(x='Month', y='Sales', title=file)\n            plt.show()\n\n    return True\n```"}
{"seed": "{\"task_id\": 7895449, \"prompt\": \"def f_7895449():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"sum([['A', 'B', 'C'], ['D', 'E', 'F'], ['G', 'H', 'I']], [])\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate() == ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I']\\n\"], \"entry_point\": \"f_7895449\", \"intent\": \"create a list containing flattened list `[['A', 'B', 'C'], ['D', 'E', 'F'], ['G', 'H', 'I']]`\", \"library\": []}", "generation": "Scenario 1: \npandas, itertools, random\n```python\nimport pandas as pd\nimport itertools\nfrom random import randint\n\n# Constants\nLETTERS = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I']\nCATEGORIES = ['Category 1', 'Category 2', 'Category 3']\n\ndef create_dataframe_from_lists():\n    \"\"\"\n    Create a pandas DataFrame from multiple lists where each list represents a category. \n    Each letter in a list is randomly assigned to a category.\n\n    Returns:\n    DataFrame: A pandas DataFrame with each letter and its corresponding category.\n\n    Requirements:\n    - pandas\n    - itertools\n    - random\n\n    Example:\n    >>> df = create_dataframe_from_lists()\n    >>> print(df)\n    \"\"\"\n    flattened_list = list(itertools.chain(*[LETTERS for _ in range(len(CATEGORIES))]))\n    categories = list(itertools.chain(*[[category] * len(LETTERS) for category in CATEGORIES]))\n    random.shuffle(categories)\n\n    df = pd.DataFrame({'Letter': flattened_list, 'Category': categories})\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, itertools, matplotlib\n```python\nimport numpy as np\nimport itertools\nimport matplotlib.pyplot as plt\n\n# Constants\nLETTERS = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I']\nCOLORS = ['red', 'green', 'blue']\n\ndef plot_letters_frequency():\n    \"\"\"\n    Generate a bar plot of the frequency of each letter in a flattened list \n    with multiple repetitions of the original list. Each repetition of the list \n    is associated with a different color in the plot.\n\n    Requirements:\n    - numpy\n    - itertools\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_letters_frequency()\n    \"\"\"\n    flattened_list = list(itertools.chain(*[LETTERS for _ in range(len(COLORS))]))\n\n    unique, counts = np.unique(flattened_list, return_counts=True)\n\n    plt.bar(unique, counts, color=COLORS)\n    plt.xlabel('Letters')\n    plt.ylabel('Frequency')\n    plt.title('Frequency of Letters')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nfunctools, operator, string\n```python\nfrom functools import reduce\nimport operator\nimport string\n\n# Constants\nLETTERS = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I']\nLETTER_TO_NUMBER = {letter: i+1 for i, letter in enumerate(string.ascii_uppercase)}\n\ndef calculate_product_of_letters_numbers():\n    \"\"\"\n    Calculate the product of the corresponding numbers of a list of letters \n    where 'A' corresponds to 1, 'B' to 2, and so on.\n\n    Returns:\n    int: The product of the numbers.\n\n    Requirements:\n    - functools\n    - operator\n    - string\n\n    Example:\n    >>> product = calculate_product_of_letters_numbers()\n    >>> print(product)\n    \"\"\"\n    numbers = [LETTER_TO_NUMBER[letter] for letter in LETTERS]\n    product = reduce(operator.mul, numbers, 1)\n\n    return product\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ncollections, itertools, string\n```python\nfrom collections import Counter\nimport itertools\nimport string\n\n# Constants\nLETTERS = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I']\n\ndef count_letters_in_flattened_list():\n    \"\"\"\n    Count the frequency of each letter in a flattened list with multiple \n    repetitions of the original list.\n\n    Returns:\n    dict: A dictionary with each letter and its frequency.\n\n    Requirements:\n    - collections\n    - itertools\n    - string\n\n    Example:\n    >>> counts = count_letters_in_flattened_list()\n    >>> print(counts)\n    \"\"\"\n    flattened_list = list(itertools.chain(*[LETTERS for _ in range(len(string.ascii_uppercase))]))\n    counts = dict(Counter(flattened_list))\n\n    return counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, itertools, scipy.stats\n```python\nimport numpy as np\nimport itertools\nfrom scipy import stats\n\n# Constants\nLETTERS = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I']\n\ndef calculate_mode_of_letters():\n    \"\"\"\n    Calculate the mode of a list of letters in a flattened list with multiple \n    repetitions of the original list.\n\n    Returns:\n    ModeResult: A ModeResult object with the mode(s) and count(s).\n\n    Requirements:\n    - numpy\n    - itertools\n    - scipy.stats\n\n    Example:\n    >>> mode = calculate_mode_of_letters()\n    >>> print(mode)\n    \"\"\"\n    flattened_list = np.array(list(itertools.chain(*[LETTERS for _ in range(10)])))\n    mode = stats.mode(flattened_list)\n\n    return mode\n```"}
{"seed": "{\"task_id\": 31617845, \"prompt\": \"def f_31617845(df):\\n\\t\", \"suffix\": \"\\n\\treturn df\", \"canonical_solution\": \"df = df[(df['closing_price'] >= 99) & (df['closing_price'] <= 101)]\", \"test_start\": \"\\nimport pandas as pd\\n\\ndef check(candidate):\", \"test\": [\"\\n    df = pd.DataFrame([67, 68, 69, 70, 99, 100, 101, 102], columns = ['closing_price'])\\n    assert candidate(df).shape[0] == 3\\n\"], \"entry_point\": \"f_31617845\", \"intent\": \"select rows in a dataframe `df` column 'closing_price' between two values 99 and 101\", \"library\": [\"pandas\"]}", "generation": "Scenario 1: \npandas, numpy, matplotlib, statistics\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom statistics import mean\n\n# Constants\nSECTORS = ['Technology', 'Healthcare', 'Finance', 'Retail', 'Energy']\n\ndef sector_analysis(df):\n    \"\"\"\n    Analyze the given dataframe `df` and generate summary statistics and plots \n    for 'closing_price' of different sectors.\n\n    Parameters:\n    df (DataFrame): The input dataframe.\n\n    Returns:\n    DataFrame: A dataframe with summary statistics.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - statistics\n\n    Example:\n    >>> df = pd.DataFrame({\n    ...     'sector': ['Technology', 'Healthcare', 'Finance', 'Retail', 'Energy'],\n    ...     'closing_price': [120, 102, 98, 105, 112]\n    ... })\n    >>> summary = sector_analysis(df)\n    \"\"\"\n    summary_data = []\n\n    for sector in SECTORS:\n        sector_df = df[df['sector'] == sector]\n        if not sector_df.empty:\n            min_price = sector_df['closing_price'].min()\n            max_price = sector_df['closing_price'].max()\n            mean_price = mean(sector_df['closing_price'])\n            summary_data.append([sector, min_price, max_price, mean_price])\n\n            plt.hist(sector_df['closing_price'], bins='auto')\n            plt.title(f'Histogram of {sector} Closing Prices')\n            plt.show()\n\n    summary_df = pd.DataFrame(summary_data, columns=['Sector', 'Min', 'Max', 'Mean'])\n\n    return summary_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, matplotlib, sklearn\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\ndef predict_stock_prices(df):\n    \"\"\"\n    Predict the stock closing prices for the next 7 days using simple linear regression.\n\n    Parameters:\n    df (DataFrame): The input dataframe with 'date' and 'closing_price'.\n\n    Returns:\n    list: A list with predicted prices for the next 7 days.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.linear_model.LinearRegression\n\n    Example:\n    >>> df = pd.DataFrame({\n    ...     'date': pd.date_range(start='1/1/2021', end='1/7/2021'),\n    ...     'closing_price': [100, 101, 102, 103, 104, 105, 106]\n    ... })\n    >>> pred_prices = predict_stock_prices(df)\n    \"\"\"\n    df['date'] = pd.to_datetime(df['date'])\n    df['date'] = df['date'].map(pd.Timestamp.timestamp)\n    X = df['date'].values.reshape(-1, 1)\n    y = df['closing_price'].values\n\n    model = LinearRegression()\n    model.fit(X, y)\n\n    future_dates = np.array([df['date'].max() + i*24*60*60 for i in range(1, 8)]).reshape(-1, 1)\n    pred_prices = model.predict(future_dates)\n\n    plt.scatter(df['date'], df['closing_price'], color='black')\n    plt.plot(future_dates, pred_prices, color='blue', linewidth=3)\n    plt.xticks(())\n    plt.yticks(())\n    plt.show()\n\n    return pred_prices.tolist()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, matplotlib, scipy\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import zscore\n\ndef detect_outliers(df):\n    \"\"\"\n    Detect the outliers in the closing prices using the Z-score method.\n\n    Parameters:\n    df (DataFrame): The input dataframe.\n\n    Returns:\n    DataFrame: A dataframe with the outliers.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - scipy.stats.zscore\n\n    Example:\n    >>> df = pd.DataFrame({\n    ...     'closing_price': [100, 101, 102, 103, 104, 150]\n    ... })\n    >>> outliers = detect_outliers(df)\n    \"\"\"\n    df['Z_score'] = zscore(df['closing_price'])\n    outliers = df[np.abs(df['Z_score']) > 2]\n\n    plt.figure(figsize=(10, 5))\n    plt.plot(df['closing_price'], color='blue', label='Normal')\n    plt.plot(outliers['closing_price'], linestyle='none', marker='X', color='red', markersize=12, label='Outlier')\n    plt.xlabel('Index')\n    plt.ylabel('Closing Price')\n    plt.title('Outliers in Closing Prices')\n    plt.legend(loc='best')\n    plt.show()\n\n    return outliers\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, matplotlib, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef visualize_price_distribution(df):\n    \"\"\"\n    Visualize the distribution of stock closing prices with a box plot and a histogram.\n\n    Parameters:\n    df (DataFrame): The input dataframe.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> df = pd.DataFrame({\n    ...     'closing_price': [100, 101, 102, 103, 104, 150]\n    ... })\n    >>> visualize_price_distribution(df)\n    \"\"\"\n    plt.figure(figsize=(12, 6))\n\n    plt.subplot(121)\n    sns.boxplot(x=df['closing_price'])\n    plt.title('Box Plot of Closing Prices')\n\n    plt.subplot(122)\n    sns.histplot(df['closing_price'], kde=True)\n    plt.title('Histogram of Closing Prices')\n\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, matplotlib, statsmodels\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.arima_model import ARIMA\n\ndef forecast_prices(df):\n    \"\"\"\n    Forecast the stock closing prices for the next 7 days using ARIMA model.\n\n    Parameters:\n    df (DataFrame): The input dataframe with 'date' and 'closing_price'.\n\n    Returns:\n    list: A list with forecasted prices for the next 7 days.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - statsmodels.tsa.arima_model.ARIMA\n\n    Example:\n    >>> df = pd.DataFrame({\n    ...     'date': pd.date_range(start='1/1/2021', end='1/7/2021'),\n    ...     'closing_price': [100, 101, 102, 103, 104, 105, 106]\n    ... })\n    >>> forecast = forecast_prices(df)\n    \"\"\"\n    model = ARIMA(df['closing_price'], order=(5,1,0))\n    model_fit = model.fit(disp=0)\n    forecast = model_fit.forecast(steps=7)[0]\n\n    plt.plot(df['closing_price'])\n    plt.plot(range(len(df), len(df)+7), forecast)\n    plt.show()\n\n    return forecast.tolist()\n```"}
{"seed": "{\"task_id\": 25698710, \"prompt\": \"def f_25698710(df):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"df.replace({'\\\\n': '<br>'}, regex=True)\", \"test_start\": \"\\nimport pandas as pd\\n\\ndef check(candidate):\", \"test\": [\"\\n    df = pd.DataFrame(['klm\\\\npqr', 'wxy\\\\njkl'], columns = ['val'])\\n    expected = pd.DataFrame(['klm<br>pqr', 'wxy<br>jkl'], columns = ['val'])\\n    assert pd.DataFrame.equals(candidate(df), expected)\\n\"], \"entry_point\": \"f_25698710\", \"intent\": \"replace all occurences of newlines `\\\\n` with `<br>` in dataframe `df`\", \"library\": [\"pandas\"]}", "generation": "Scenario 1: \npandas, re, numpy\n```python\nimport pandas as pd\nimport re\nimport numpy as np\n\n# Constants\nPATTERN = re.compile(r'\\b[A-Z]+\\b')\n\ndef replace_acronyms_with_full_words(df, mapping):\n    \"\"\"\n    Replace all acronyms in a DataFrame with their full words according to a provided mapping.\n\n    Parameters:\n    df (DataFrame): The input DataFrame.\n    mapping (dict): A dictionary where keys are acronyms and values are the full words.\n\n    Returns:\n    DataFrame: The DataFrame with all acronyms replaced with their full words.\n\n    Requirements:\n    - pandas\n    - re\n    - numpy\n\n    Example:\n    >>> df = pd.DataFrame(['NASA is located in USA'], columns=['text'])\n    >>> mapping = {'NASA': 'National Aeronautics and Space Administration', 'USA': 'United States of America'}\n    >>> print(replace_acronyms_with_full_words(df, mapping))\n    \"\"\"\n    def replace_match(match):\n        return mapping.get(match.group(0), match.group(0))\n\n    df = df.applymap(lambda x: PATTERN.sub(replace_match, x) if isinstance(x, str) else x)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \npandas, numpy, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCATEGORIES = ['A', 'B', 'C', 'D', 'E']\n\ndef plot_category_distribution(df, column):\n    \"\"\"\n    Plot a bar chart showing the distribution of categories in a specific column of a DataFrame.\n\n    Parameters:\n    df (DataFrame): The DataFrame.\n    column (str): The column name.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame({'Category': ['A', 'B', 'B', 'C', 'A', 'D', 'E', 'E', 'D']})\n    >>> plot_category_distribution(df, 'Category')\n    \"\"\"\n    counts = df[column].value_counts()\n    missing_categories = list(set(CATEGORIES) - set(counts.index))\n    for category in missing_categories:\n        counts[category] = 0\n\n    counts = counts.reindex(CATEGORIES)\n\n    plt.bar(counts.index, counts.values)\n    plt.xlabel('Category')\n    plt.ylabel('Count')\n    plt.title(f'Distribution of {column}')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \npandas, seaborn, matplotlib\n```python\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef plot_correlation_matrix(df):\n    \"\"\"\n    Plot a correlation matrix heatmap for a DataFrame.\n\n    Parameters:\n    df (DataFrame): The DataFrame.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - seaborn\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6], 'c': [7, 8, 9]})\n    >>> plot_correlation_matrix(df)\n    \"\"\"\n    correlation_matrix = df.corr()\n    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n    plt.title('Correlation Matrix')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \npandas, sklearn.preprocessing, numpy\n```python\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\n\n# Constants\nSCALER = MinMaxScaler()\n\ndef normalize_columns(df, columns):\n    \"\"\"\n    Normalize specified columns of a DataFrame using Min-Max scaling.\n\n    Parameters:\n    df (DataFrame): The DataFrame.\n    columns (list): A list of column names to be normalized.\n\n    Returns:\n    DataFrame: A DataFrame with the specified columns normalized.\n\n    Requirements:\n    - pandas\n    - sklearn.preprocessing\n    - numpy\n\n    Example:\n    >>> df = pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6]})\n    >>> print(normalize_columns(df, ['a', 'b']))\n    \"\"\"\n    df[columns] = SCALER.fit_transform(df[columns])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \npandas, nltk.corpus, re\n```python\nimport pandas as pd\nfrom nltk.corpus import stopwords\nimport re\n\n# Constants\nSTOPWORDS = set(stopwords.words('english'))\n\ndef remove_stopwords_from_text(df, column):\n    \"\"\"\n    Remove English stopwords from a text column in a DataFrame.\n\n    Parameters:\n    df (DataFrame): The DataFrame.\n    column (str): The name of the text column from which stopwords should be removed.\n\n    Returns:\n    DataFrame: A DataFrame with the stopwords removed from the specified column.\n\n    Requirements:\n    - pandas\n    - nltk.corpus\n    - re\n\n    Example:\n    >>> df = pd.DataFrame({'text': ['This is a sample sentence.']})\n    >>> print(remove_stopwords_from_text(df, 'text'))\n    \"\"\"\n    df[column] = df[column].apply(lambda x: ' '.join([word for word in re.findall(r'\\b\\w+\\b', x) if word not in STOPWORDS]))\n\n    return df\n```"}
{"seed": "{\"task_id\": 25698710, \"prompt\": \"def f_25698710(df):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"df.replace({'\\\\n': '<br>'}, regex=True)\", \"test_start\": \"\\nimport pandas as pd\\n\\ndef check(candidate):\", \"test\": [\"\\n    df = pd.DataFrame(['klm\\\\npqr', 'wxy\\\\njkl'], columns = ['val'])\\n    expected = pd.DataFrame(['klm<br>pqr', 'wxy<br>jkl'], columns = ['val'])\\n    assert pd.DataFrame.equals(candidate(df), expected)\\n\"], \"entry_point\": \"f_25698710\", \"intent\": \"replace all occurrences of a string `\\\\n` by string `<br>` in a pandas data frame `df`\", \"library\": [\"pandas\"]}", "generation": "Scenario 1: \npandas, numpy, random, re\n```python\nimport pandas as pd\nimport numpy as np\nimport random\nimport re\n\n# Constants\nPERSON_NAMES = ['John Doe', 'Jane Smith', 'Bob Johnson', 'Alice Williams', 'Charlie Brown']\nEMAIL_DOMAINS = ['gmail.com', 'yahoo.com', 'outlook.com', 'hotmail.com', 'icloud.com']\n\ndef generate_and_clean_email_dataset():\n    \"\"\"\n    Generate a dataset of person names and emails, then clean the emails by \n    replacing all occurrences of a string '@' by string ' [at] ' in the dataset.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with person names and cleaned emails.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - random\n    - re\n    \n    Example:\n    >>> df = generate_and_clean_email_dataset()\n    >>> print(df)\n    \"\"\"\n    data = []\n\n    for name in PERSON_NAMES:\n        email = re.sub('@', ' [at] ', '{}@{}'.format(name.split()[0].lower(), random.choice(EMAIL_DOMAINS)))\n        data.append([name, email])\n\n    df = pd.DataFrame(data, columns=['Name', 'Email'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \npandas, os, sys, matplotlib\n```python\nimport pandas as pd\nimport os\nimport sys\nimport matplotlib.pyplot as plt\n\n# Constants\nFILE_PATH = './data.csv'\nCOLUMN_NAME = 'Value'\n\ndef load_and_visualize_data():\n    \"\"\"\n    Load a CSV file into a pandas DataFrame, replace all occurrences of a \n    string '\\n' by string '<br>' in the specified column, and visualize the data.\n    \n    Returns:\n    DataFrame: The cleaned pandas DataFrame.\n    \n    Requirements:\n    - pandas\n    - os\n    - sys\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = load_and_visualize_data()\n    >>> print(df)\n    >>> df[COLUMN_NAME].value_counts().plot(kind='bar')\n    \"\"\"\n    if not os.path.exists(FILE_PATH):\n        print(f'File does not exist: {FILE_PATH}')\n        sys.exit(1)\n\n    df = pd.read_csv(FILE_PATH)\n    df[COLUMN_NAME] = df[COLUMN_NAME].replace({'\\n': '<br>'}, regex=True)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \npandas, numpy, seaborn, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nDATA_SIZE = 1000\nCOLUMN_NAMES = ['A', 'B', 'C', 'D', 'E']\n\ndef generate_and_analyze_data():\n    \"\"\"\n    Generate a pandas DataFrame with random numeric values, replace all \n    occurrences of a specific number by another number, and perform correlation analysis.\n    \n    Returns:\n    DataFrame: The modified pandas DataFrame.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - seaborn\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = generate_and_analyze_data()\n    >>> print(df)\n    >>> correlation_matrix = df.corr()\n    >>> sns.heatmap(correlation_matrix, annot=True)\n    \"\"\"\n    df = pd.DataFrame(np.random.randint(0,100,size=(DATA_SIZE, len(COLUMN_NAMES))), columns=COLUMN_NAMES)\n    df = df.replace({0: -1})\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \npandas, sqlite3, re\n```python\nimport pandas as pd\nimport sqlite3\nimport re\n\n# Constants\nDB_PATH = './data.db'\nTABLE_NAME = 'MyTable'\nCOLUMN_NAME = 'TextData'\n\ndef load_and_clean_database_data():\n    \"\"\"\n    Load data from a SQLite database into a pandas DataFrame, replace all occurrences \n    of a string '\\n' by string '<br>' in the specified column.\n    \n    Returns:\n    DataFrame: The cleaned pandas DataFrame.\n    \n    Requirements:\n    - pandas\n    - sqlite3\n    - re\n    \n    Example:\n    >>> df = load_and_clean_database_data()\n    >>> print(df)\n    \"\"\"\n    conn = sqlite3.connect(DB_PATH)\n    df = pd.read_sql_query(f\"SELECT * FROM {TABLE_NAME}\", conn)\n    df[COLUMN_NAME] = df[COLUMN_NAME].replace({'\\n': '<br>'}, regex=True)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \npandas, csv, sklearn.preprocessing\n```python\nimport pandas as pd\nimport csv\nfrom sklearn.preprocessing import LabelEncoder\n\n# Constants\nFILE_PATH = './data.csv'\nCOLUMN_NAME = 'Category'\n\ndef load_and_encode_data():\n    \"\"\"\n    Load a CSV file into a pandas DataFrame, replace all occurrences of a string '\\n' \n    by string '<br>' in the specified column, and encode a categorical variable.\n    \n    Returns:\n    DataFrame: The encoded pandas DataFrame.\n    \n    Requirements:\n    - pandas\n    - csv\n    - sklearn.preprocessing.LabelEncoder\n    \n    Example:\n    >>> df = load_and_encode_data()\n    >>> print(df)\n    \"\"\"\n    df = pd.read_csv(FILE_PATH)\n    df[COLUMN_NAME] = df[COLUMN_NAME].replace({'\\n': '<br>'}, regex=True)\n\n    le = LabelEncoder()\n    df[COLUMN_NAME] = le.fit_transform(df[COLUMN_NAME])\n\n    return df\n```\n"}
{"seed": "{\"task_id\": 41923858, \"prompt\": \"def f_41923858(word):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"[(x + y) for x, y in zip(word, word[1:])]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('abcdef') == ['ab', 'bc', 'cd', 'de', 'ef']\\n\", \"\\n    assert candidate([\\\"hello\\\", \\\"world\\\", \\\"!\\\"]) == [\\\"helloworld\\\", \\\"world!\\\"]\\n\"], \"entry_point\": \"f_41923858\", \"intent\": \"create a list containing each two adjacent letters in string `word` as its elements\", \"library\": []}", "generation": "Scenario 1: \ncollections, itertools, string\n```python\nfrom collections import Counter\nimport itertools\nimport string\n\n# Constants\nALPHABETS = string.ascii_lowercase\n\ndef generate_char_combinations(word):\n    \"\"\"\n    Generate a dictionary with keys as all possible two-letter combinations of alphabets and values as \n    their counts in the given word. If a combination is not present in the word, its count should be 0.\n\n    Parameters:\n    word (str): The input string.\n\n    Returns:\n    dict: A dictionary with keys as two-letter combinations and values as their counts in the word.\n\n    Requirements:\n    - collections.Counter\n    - itertools.permutations\n    - string\n\n    Example:\n    >>> generate_char_combinations('abcdef')\n    \"\"\"\n    combinations = [''.join(x) for x in itertools.permutations(ALPHABETS, 2)]\n    word_combinations = [''.join(x) for x in zip(word, word[1:])]\n    word_counter = Counter(word_combinations)\n\n    return {key: word_counter.get(key, 0) for key in combinations}\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nnumpy, itertools\n```python\nimport numpy as np\nimport itertools\n\ndef calculate_adjacent_difference(word):\n    \"\"\"\n    Calculate the difference between the ASCII value of each pair of adjacent letters in the word.\n\n    Parameters:\n    word (str): The input string.\n\n    Returns:\n    numpy array: A numpy array containing the difference between the ASCII value of each pair of adjacent letters.\n\n    Requirements:\n    - numpy\n    - itertools\n\n    Example:\n    >>> calculate_adjacent_difference('abcdef')\n    \"\"\"\n    word_ascii_values = np.array([ord(x) for x in word])\n    difference = np.diff(word_ascii_values)\n\n    return difference\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nitertools, random\n```python\nimport itertools\nimport random\n\n# Constants\nPOSSIBLE_LETTERS = ['a', 'b', 'c']\n\ndef generate_random_adjacent_pairs(word):\n    \"\"\"\n    Generate a list of random pairs of adjacent letters from the given word.\n\n    Parameters:\n    word (str): The input string.\n\n    Returns:\n    list: A list of random pairs of adjacent letters from the word.\n\n    Requirements:\n    - itertools\n    - random\n\n    Example:\n    >>> generate_random_adjacent_pairs('abcdef')\n    \"\"\"\n    pairs = [''.join(x) for x in zip(word, word[1:])]\n    random_pairs = [random.choice(pairs) for _ in range(len(POSSIBLE_LETTERS))]\n\n    return random_pairs\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \ncollections, string\n```python\nfrom collections import defaultdict\nimport string\n\n# Constants\nALPHABETS = string.ascii_lowercase\n\ndef find_occurrences(word):\n    \"\"\"\n    Find the occurrences of each two-letter combination in the word.\n\n    Parameters:\n    word (str): The input string.\n\n    Returns:\n    dict: A dictionary with keys as two-letter combinations and values as their counts in the word.\n\n    Requirements:\n    - collections.defaultdict\n    - string\n\n    Example:\n    >>> find_occurrences('abcdef')\n    \"\"\"\n    occurrences = defaultdict(int)\n    pairs = [''.join(x) for x in zip(word, word[1:])]\n\n    for pair in pairs:\n        occurrences[pair] += 1\n\n    return occurrences\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nitertools, collections\n```python\nimport itertools\nfrom collections import Counter\n\ndef find_most_common_pairs(word):\n    \"\"\"\n    Find the most common two-letter combinations in the word.\n\n    Parameters:\n    word (str): The input string.\n\n    Returns:\n    list: A list of the most common two-letter combinations in the word.\n\n    Requirements:\n    - itertools\n    - collections.Counter\n\n    Example:\n    >>> find_most_common_pairs('abcdef')\n    \"\"\"\n    pairs = [''.join(x) for x in zip(word, word[1:])]\n    pair_counter = Counter(pairs)\n    most_common_pairs = pair_counter.most_common(1)\n\n    return most_common_pairs\n```"}
{"seed": "{\"task_id\": 41923858, \"prompt\": \"def f_41923858(word):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"list(map(lambda x, y: x + y, word[:-1], word[1:]))\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('abcdef') == ['ab', 'bc', 'cd', 'de', 'ef']\\n\"], \"entry_point\": \"f_41923858\", \"intent\": \"Get a list of pairs from a string `word` using lambda function\", \"library\": []}", "generation": "Scenario 1: \nnumpy, itertools, string\n```python\nimport numpy as np\nimport itertools\nimport string\n\n# Constants\nALPHABET = list(string.ascii_lowercase)\nNUMBERS = list(range(1, 27))\n\ndef generate_code_pairs(word):\n    \"\"\"\n    Generate a list of tuples, where each tuple contains the pair of letters from a \n    word and their corresponding numbers from an alphabet using numpy and itertools.\n\n    Parameters:\n    word (str): The word to generate the pairs.\n\n    Returns:\n    list: The list of tuples.\n\n    Requirements:\n    - numpy\n    - itertools\n    - string\n\n    Example:\n    >>> generate_code_pairs('abc')\n    [('a', 1, 'b', 2), ('b', 2, 'c', 3)]\n    \"\"\"\n    word_numbers = list(map(lambda x: ALPHABET.index(x) + 1, word))\n    pairs = list(itertools.chain(*np.array(list(zip(word, word_numbers)))[:-1].reshape(-1, 2).tolist()))\n\n    return pairs\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncollections, functools, operator\n```python\nfrom collections import Counter\nfrom functools import reduce\nimport operator\n\ndef count_letter_pairs(word):\n    \"\"\"\n    Count the occurrence of each letter pair in a word using collections and functools.\n\n    Parameters:\n    word (str): The word to count the letter pairs.\n\n    Returns:\n    dict: The dictionary of letter pairs and their counts.\n\n    Requirements:\n    - collections.Counter\n    - functools.reduce\n    - operator.add\n\n    Example:\n    >>> count_letter_pairs('abracadabra')\n    {'ab': 2, 'br': 1, 'ra': 2, 'ac': 1, 'ca': 1, 'ad': 1, 'da': 1}\n    \"\"\"\n    pairs = list(map(''.join, zip(word[:-1], word[1:])))\n    pairs_count = dict(Counter(pairs))\n\n    return pairs_count\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, string\n```python\nimport pandas as pd\nimport string\n\n# Constants\nALPHABET = list(string.ascii_lowercase)\n\ndef create_dataframe(word):\n    \"\"\"\n    Create a pandas DataFrame from a word, where each row contains the letter and \n    its position in an alphabet.\n\n    Parameters:\n    word (str): The word to create the DataFrame.\n\n    Returns:\n    DataFrame: The pandas DataFrame.\n\n    Requirements:\n    - pandas\n    - string\n\n    Example:\n    >>> create_dataframe('abc')\n        Letter  Position\n    0      a         1\n    1      b         2\n    2      c         3\n    \"\"\"\n    letters = list(word)\n    positions = list(map(lambda x: ALPHABET.index(x) + 1, letters))\n    df = pd.DataFrame({'Letter': letters, 'Position': positions})\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, matplotlib.pyplot\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nALPHABET = list(string.ascii_lowercase)\n\ndef plot_letter_positions(word):\n    \"\"\"\n    Plot a bar graph of the positions of the letters in a word using numpy and matplotlib.pyplot.\n\n    Parameters:\n    word (str): The word to plot the letter positions.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_letter_positions('abc')\n    \"\"\"\n    letter_positions = np.array(list(map(lambda x: ALPHABET.index(x) + 1, word)))\n    plt.bar(np.arange(len(letter_positions)), letter_positions)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\ncollections, string\n```python\nfrom collections import OrderedDict\nimport string\n\n# Constants\nALPHABET = list(string.ascii_lowercase)\n\ndef sort_by_alphabet(word):\n    \"\"\"\n    Sort the letters in a word by their position in an alphabet using collections and string.\n\n    Parameters:\n    word (str): The word to sort the letters.\n\n    Returns:\n    str: The sorted word.\n\n    Requirements:\n    - collections.OrderedDict\n    - string\n\n    Example:\n    >>> sort_by_alphabet('cba')\n    'abc'\n    \"\"\"\n    letter_positions = OrderedDict(zip(word, map(lambda x: ALPHABET.index(x) + 1, word)))\n    sorted_word = ''.join(sorted(letter_positions, key=letter_positions.get))\n\n    return sorted_word\n```"}
{"seed": "{\"task_id\": 9760588, \"prompt\": \"def f_9760588(myString):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"re.findall('(https?://[^\\\\\\\\s]+)', myString)\", \"test_start\": \"\\nimport re\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(\\\"This is a link http://www.google.com\\\") == [\\\"http://www.google.com\\\"]\\n\", \"\\n    assert candidate(\\\"Please refer to the website: http://www.google.com\\\") == [\\\"http://www.google.com\\\"]\\n\"], \"entry_point\": \"f_9760588\", \"intent\": \"extract a url from a string `myString`\", \"library\": [\"re\"]}", "generation": "Scenario 1: \nre, urllib.request, bs4, requests\n```python\nimport re\nimport urllib.request\nfrom bs4 import BeautifulSoup\nimport requests\n\n# Constants\nUSER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n\ndef get_webpage_titles(myString):\n    \"\"\"\n    Extract all URLs from a string `myString`, make a GET request to each URL and return the title of each webpage.\n    \n    Parameters:\n    myString (str): The string to extract URLs from.\n    \n    Returns:\n    dict: A dictionary with URLs as keys and webpage titles as values.\n\n    Requirements:\n    - re\n    - urllib.request\n    - bs4.BeautifulSoup\n    - requests\n    \n    Example:\n    >>> get_webpage_titles(\"Check these links: http://www.google.com, https://www.python.org\")\n    {'http://www.google.com': 'Google', 'https://www.python.org': 'Welcome to Python.org'}\n    \"\"\"\n    urls = re.findall('(https?://[^\\\\s]+)', myString)\n    titles = {}\n\n    for url in urls:\n        req = urllib.request.Request(\n            url, \n            data=None, \n            headers={\n                'User-Agent': USER_AGENT\n            }\n        )\n\n        with urllib.request.urlopen(req) as response:\n            page_content = response.read()\n\n        soup = BeautifulSoup(page_content, 'html.parser')\n        titles[url] = soup.title.string\n\n    return titles\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nre, urllib.parse, requests, json\n```python\nimport re\nimport urllib.parse\nimport requests\nimport json\n\n# Constants\nAPI_KEY = 'YOUR_API_KEY'\n\ndef get_geo_data(myString):\n    \"\"\"\n    Extract all URLs from a string `myString`, parse each URL to extract the domain, \n    and use the IP-API to get the geolocation data of each domain.\n    \n    Parameters:\n    myString (str): The string to extract URLs from.\n    \n    Returns:\n    dict: A dictionary with domains as keys and geolocation data as values.\n\n    Requirements:\n    - re\n    - urllib.parse\n    - requests\n    - json\n    \n    Example:\n    >>> get_geo_data(\"Check these links: http://www.google.com, https://www.python.org\")\n    {'www.google.com': {'status': 'success', 'country': 'United States', 'countryCode': 'US', 'region': 'CA', 'regionName': 'California', 'city': 'Mountain View', 'zip': '94043', 'lat': '37.4192', 'lon': '-122.0574', 'timezone': 'America/Los_Angeles', 'isp': 'Google LLC', 'org': 'Google LLC', 'as': 'AS15169 Google LLC', 'query': '172.217.12.142'}, 'www.python.org': {'status': 'success', 'country': 'United States', 'countryCode': 'US', 'region': 'OR', 'regionName': 'Oregon', 'city': 'Boardman', 'zip': '97818', 'lat': '45.8696', 'lon': '-119.688', 'timezone': 'America/Los_Angeles', 'isp': 'Amazon.com, Inc.', 'org': 'Amazon Data Services NoVa', 'as': 'AS16509 Amazon.com, Inc.', 'query': '151.101.193.223'}}\n    \"\"\"\n    urls = re.findall('(https?://[^\\\\s]+)', myString)\n    geo_data = {}\n\n    for url in urls:\n        domain = urllib.parse.urlparse(url).netloc\n        response = requests.get(f\"http://ip-api.com/json/{domain}?access_key={API_KEY}\")\n        geo_data[domain] = json.loads(response.text)\n\n    return geo_data\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nre, urllib.parse, requests, xml.etree.ElementTree\n```python\nimport re\nimport urllib.parse\nimport requests\nfrom xml.etree import ElementTree as ET\n\n# Constants\nAPI_KEY = 'YOUR_API_KEY'\n\ndef get_sitemap_urls(myString):\n    \"\"\"\n    Extract all URLs from a string `myString`, parse each URL to extract the domain, \n    and make a GET request to each domain's sitemap.xml to extract all sitemap URLs.\n    \n    Parameters:\n    myString (str): The string to extract URLs from.\n    \n    Returns:\n    dict: A dictionary with domains as keys and sitemap URLs as values.\n\n    Requirements:\n    - re\n    - urllib.parse\n    - requests\n    - xml.etree.ElementTree\n    \n    Example:\n    >>> get_sitemap_urls(\"Check these links: http://www.google.com, https://www.python.org\")\n    {'www.google.com': ['https://www.google.com/sitemap_1.xml', 'https://www.google.com/sitemap_2.xml'], 'www.python.org': ['https://www.python.org/sitemap_1.xml', 'https://www.python.org/sitemap_2.xml']}\n    \"\"\"\n    urls = re.findall('(https?://[^\\\\s]+)', myString)\n    sitemap_urls = {}\n\n    for url in urls:\n        domain = urllib.parse.urlparse(url).netloc\n        response = requests.get(f\"https://{domain}/sitemap.xml\")\n        root = ET.fromstring(response.text)\n\n        sitemap_urls[domain] = [sitemap.text for sitemap in root.findall('{http://www.sitemaps.org/schemas/sitemap/0.9}sitemaploc')]\n\n    return sitemap_urls\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nre, urllib.parse, requests, socket\n```python\nimport re\nimport urllib.parse\nimport requests\nimport socket\n\ndef get_ip_addresses(myString):\n    \"\"\"\n    Extract all URLs from a string `myString`, parse each URL to extract the domain, \n    and get the IP address of each domain.\n    \n    Parameters:\n    myString (str): The string to extract URLs from.\n    \n    Returns:\n    dict: A dictionary with domains as keys and IP addresses as values.\n\n    Requirements:\n    - re\n    - urllib.parse\n    - requests\n    - socket\n    \n    Example:\n    >>> get_ip_addresses(\"Check these links: http://www.google.com, https://www.python.org\")\n    {'www.google.com': '172.217.12.142', 'www.python.org': '151.101.193.223'}\n    \"\"\"\n    urls = re.findall('(https?://[^\\\\s]+)', myString)\n    ip_addresses = {}\n\n    for url in urls:\n        domain = urllib.parse.urlparse(url).netloc\n        ip_addresses[domain] = socket.gethostbyname(domain)\n\n    return ip_addresses\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nre, urllib.parse, requests, ssl, socket\n```python\nimport re\nimport urllib.parse\nimport requests\nimport ssl\nimport socket\n\ndef get_ssl_cert_expiry(myString):\n    \"\"\"\n    Extract all URLs from a string `myString`, parse each URL to extract the domain, \n    and get the SSL certificate expiry date of each domain.\n    \n    Parameters:\n    myString (str): The string to extract URLs from.\n    \n    Returns:\n    dict: A dictionary with domains as keys and SSL certificate expiry dates as values.\n\n    Requirements:\n    - re\n    - urllib.parse\n    - requests\n    - ssl\n    - socket\n    \n    Example:\n    >>> get_ssl_cert_expiry(\"Check these links: http://www.google.com, https://www.python.org\")\n    {'www.google.com': '2023-06-15 12:00:00', 'www.python.org': '2023-07-20 12:00:00'}\n    \"\"\"\n    urls = re.findall('(https?://[^\\\\s]+)', myString)\n    ssl_expiry_dates = {}\n\n    for url in urls:\n        domain = urllib.parse.urlparse(url).netloc\n\n        context = ssl.create_default_context()\n        with socket.create_connection((domain, 443)) as sock:\n            with context.wrap_socket(sock, server_hostname=domain) as ssock:\n                ssl_expiry_dates[domain] = ssock.getpeercert()['notAfter']\n\n    return ssl_expiry_dates\n```\n"}
{"seed": "{\"task_id\": 9760588, \"prompt\": \"def f_9760588(myString):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"re.search('(?P<url>https?://[^\\\\\\\\s]+)', myString).group('url')\", \"test_start\": \"\\nimport re\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(\\\"This is a link http://www.google.com\\\") == \\\"http://www.google.com\\\"\\n\", \"\\n    assert candidate(\\\"Please refer to the website: http://www.google.com\\\") == \\\"http://www.google.com\\\"\\n\"], \"entry_point\": \"f_9760588\", \"intent\": \"extract a url from a string `myString`\", \"library\": [\"re\"]}", "generation": "Scenario 1: \nre, urllib, BeautifulSoup, requests\n```python\nimport re\nfrom urllib.parse import urlparse\nfrom bs4 import BeautifulSoup\nimport requests\n\n# Constants\nHEADERS = {'User-Agent': 'Mozilla/5.0'}\n\ndef extract_and_crawl_url(myString):\n    \"\"\"\n    Extract a URL from a string and fetch the title of the webpage at that URL.\n\n    Parameters:\n    myString (str): The string from which to extract the URL.\n\n    Returns:\n    str: The title of the webpage at the extracted URL.\n\n    Requirements:\n    - re\n    - urllib.parse\n    - bs4.BeautifulSoup\n    - requests\n\n    Example:\n    >>> extract_and_crawl_url('Check this out: https://www.google.com')\n    'Google'\n    \"\"\"\n    url = re.search(r'(https?://\\S+)', myString).group()\n    domain = urlparse(url).netloc\n    response = requests.get(url, headers=HEADERS)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    title = soup.title.string\n\n    return title\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nre, json, requests\n```python\nimport re\nimport json\nimport requests\n\n# Constants\nTOKEN = 'your_token_here'\n\ndef extract_and_post_url(myString):\n    \"\"\"\n    Extract a URL from a string and post it to a REST API.\n\n    Parameters:\n    myString (str): The string from which to extract the URL.\n\n    Returns:\n    dict: The response from the API.\n\n    Requirements:\n    - re\n    - json\n    - requests\n\n    Example:\n    >>> extract_and_post_url('Please check: https://www.google.com')\n    {'message': 'URL received'}\n    \"\"\"\n    url = re.search(r'(https?://\\S+)', myString).group()\n    headers = {'Authorization': 'Bearer ' + TOKEN}\n    data = {'url': url}\n    response = requests.post('https://api.example.com/urls', headers=headers, data=json.dumps(data))\n\n    return response.json()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nre, urllib, selenium, webdriver_manager\n```python\nimport re\nfrom urllib.parse import urlparse\nfrom selenium import webdriver\nfrom webdriver_manager.chrome import ChromeDriverManager\n\ndef extract_and_screenshot_url(myString):\n    \"\"\"\n    Extract a URL from a string and take a screenshot of the webpage at that URL.\n\n    Parameters:\n    myString (str): The string from which to extract the URL.\n\n    Returns:\n    str: The filename of the screenshot.\n\n    Requirements:\n    - re\n    - urllib.parse\n    - selenium.webdriver\n    - webdriver_manager.chrome\n\n    Example:\n    >>> extract_and_screenshot_url('See this: https://www.google.com')\n    'www.google.com.png'\n    \"\"\"\n    url = re.search(r'(https?://\\S+)', myString).group()\n    domain = urlparse(url).netloc\n    driver = webdriver.Chrome(ChromeDriverManager().install())\n    driver.get(url)\n    screenshot_filename = domain + '.png'\n    driver.save_screenshot(screenshot_filename)\n    driver.quit()\n\n    return screenshot_filename\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nre, bs4, requests, pandas\n```python\nimport re\nfrom bs4 import BeautifulSoup\nimport requests\nimport pandas as pd\n\n# Constants\nHEADERS = {'User-Agent': 'Mozilla/5.0'}\n\ndef extract_and_scrape_url(myString):\n    \"\"\"\n    Extract a URL from a string and scrape the webpage at that URL for links.\n\n    Parameters:\n    myString (str): The string from which to extract the URL.\n\n    Returns:\n    DataFrame: A pandas DataFrame of the links found on the webpage.\n\n    Requirements:\n    - re\n    - bs4.BeautifulSoup\n    - requests\n    - pandas\n\n    Example:\n    >>> extract_and_scrape_url('Visit: https://www.google.com')\n    \"\"\"\n    url = re.search(r'(https?://\\S+)', myString).group()\n    response = requests.get(url, headers=HEADERS)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    links = [a['href'] for a in soup.find_all('a', href=True)]\n    links_df = pd.DataFrame(links, columns=['Link'])\n\n    return links_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nre, urllib, requests, PIL\n```python\nimport re\nfrom urllib.parse import urlparse\nimport requests\nfrom PIL import Image\n\ndef extract_and_download_image(myString):\n    \"\"\"\n    Extract a URL from a string and download the image at that URL.\n\n    Parameters:\n    myString (str): The string from which to extract the URL.\n\n    Returns:\n    str: The filename of the downloaded image.\n\n    Requirements:\n    - re\n    - urllib.parse\n    - requests\n    - PIL.Image\n\n    Example:\n    >>> extract_and_download_image('Image: https://www.google.com/logo.png')\n    'logo.png'\n    \"\"\"\n    url = re.search(r'(https?://\\S+)', myString).group()\n    filename = urlparse(url).path.split('/')[-1]\n    response = requests.get(url)\n    image = Image.open(BytesIO(response.content))\n    image.save(filename)\n\n    return filename\n```"}
{"seed": "{\"task_id\": 5843518, \"prompt\": \"def f_5843518(mystring):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"re.sub('[^A-Za-z0-9]+', '', mystring)\", \"test_start\": \"\\nimport re\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('Special $#! characters   spaces 888323') == 'Specialcharactersspaces888323'\\n\"], \"entry_point\": \"f_5843518\", \"intent\": \"remove all special characters, punctuation and spaces from a string `mystring` using regex\", \"library\": [\"re\"]}", "generation": "Scenario 1:\nre, string, collections\n```python\nimport re\nimport string\nfrom collections import Counter\n\ndef count_alphanumeric_freq(input_str):\n    \"\"\"\n    Remove all special characters, punctuation, and spaces from a string `input_str` using regex, \n    and then count the frequency of each alphanumeric character.\n\n    Parameters:\n    input_str (str): The input string.\n\n    Returns:\n    dict: A dictionary with the frequency of each alphanumeric character.\n\n    Requirements:\n    - re\n    - string\n    - collections.Counter\n\n    Example:\n    >>> count_alphanumeric_freq('Special $#! characters   spaces 888323')\n    {'S': 1, 'p': 1, 'e': 2, 'c': 2, 'i': 1, 'a': 2, 'l': 1, 'h': 1, 'r': 2, 't': 1, 's': 2, '8': 3, '3': 2, '2': 1}\n    \"\"\"\n    cleaned_str = re.sub('[^A-Za-z0-9]+', '', input_str)\n    freq_dict = Counter(cleaned_str)\n\n    return freq_dict\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nre, pandas, numpy\n```python\nimport re\nimport pandas as pd\nimport numpy as np\n\ndef process_data_frame(df):\n    \"\"\"\n    Remove all special characters, punctuation, and spaces from a string column `text` in the DataFrame `df` using regex, \n    and then calculate the length of each cleaned string.\n\n    Parameters:\n    df (DataFrame): The input pandas DataFrame with a column named 'text'.\n\n    Returns:\n    DataFrame: A DataFrame with the cleaned text and their lengths.\n\n    Requirements:\n    - re\n    - pandas\n    - numpy\n\n    Example:\n    >>> df = pd.DataFrame({'text': ['Special $#! characters   spaces 888323', 'Hello, World!']})\n    >>> process_data_frame(df)\n    \"\"\"\n    df['clean_text'] = df['text'].apply(lambda x: re.sub('[^A-Za-z0-9]+', '', x))\n    df['text_length'] = df['clean_text'].apply(len)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nre, os, glob\n```python\nimport re\nimport os\nimport glob\n\ndef clean_file_names(dir_path):\n    \"\"\"\n    Remove all special characters, punctuation, and spaces from file names in the directory `dir_path` using regex.\n\n    Parameters:\n    dir_path (str): The path to the directory.\n\n    Returns:\n    list: A list of new file names.\n\n    Requirements:\n    - re\n    - os\n    - glob\n\n    Example:\n    >>> clean_file_names('/path/to/your/directory')\n    \"\"\"\n    new_names = []\n    for file_path in glob.glob(os.path.join(dir_path, '*')):\n        base_name = os.path.basename(file_path)\n        new_name = re.sub('[^A-Za-z0-9]+', '', base_name)\n        new_path = os.path.join(dir_path, new_name)\n        os.rename(file_path, new_path)\n        new_names.append(new_name)\n\n    return new_names\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nre, nltk, collections\n```python\nimport re\nfrom nltk import word_tokenize\nfrom collections import Counter\n\ndef count_word_freq(input_str):\n    \"\"\"\n    Remove all special characters, punctuation, and spaces from a string `input_str` using regex, \n    and then count the frequency of each word.\n\n    Parameters:\n    input_str (str): The input string.\n\n    Returns:\n    dict: A dictionary with the frequency of each word.\n\n    Requirements:\n    - re\n    - nltk.word_tokenize\n    - collections.Counter\n\n    Example:\n    >>> count_word_freq('Special $#! characters   spaces 888323')\n    {'Special': 1, 'characters': 1, 'spaces': 1, '888323': 1}\n    \"\"\"\n    cleaned_str = re.sub('[^A-Za-z0-9 ]+', '', input_str)\n    words = word_tokenize(cleaned_str)\n    freq_dict = Counter(words)\n\n    return freq_dict\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nre, hashlib\n```python\nimport re\nimport hashlib\n\ndef hash_string(input_str):\n    \"\"\"\n    Remove all special characters, punctuation, and spaces from a string `input_str` using regex, \n    and then hash the cleaned string using SHA256.\n\n    Parameters:\n    input_str (str): The input string.\n\n    Returns:\n    str: The hashed string.\n\n    Requirements:\n    - re\n    - hashlib\n\n    Example:\n    >>> hash_string('Special $#! characters   spaces 888323')\n    \"\"\"\n    cleaned_str = re.sub('[^A-Za-z0-9]+', '', input_str)\n    hashed_str = hashlib.sha256(cleaned_str.encode()).hexdigest()\n\n    return hashed_str\n```"}
{"seed": "{\"task_id\": 36674519, \"prompt\": \"def f_36674519():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"pd.date_range('2016-01-01', freq='WOM-2FRI', periods=13)\", \"test_start\": \"\\nimport pandas as pd\\nimport datetime\\n\\ndef check(candidate):\", \"test\": [\"\\n    actual = candidate() \\n    expected = [[2016, 1, 8], [2016, 2, 12],\\n                [2016, 3, 11], [2016, 4, 8],\\n                [2016, 5, 13], [2016, 6, 10],\\n                [2016, 7, 8], [2016, 8, 12],\\n                [2016, 9, 9], [2016, 10, 14],\\n                [2016, 11, 11], [2016, 12, 9],\\n                [2017, 1, 13]]\\n    for i in range(0, len(expected)):\\n        d = datetime.date(expected[i][0], expected[i][1], expected[i][2])\\n        assert d == actual[i].date()\\n\"], \"entry_point\": \"f_36674519\", \"intent\": \"create a DatetimeIndex containing 13 periods of the second friday of each month starting from date '2016-01-01'\", \"library\": [\"datetime\", \"pandas\"]}", "generation": "Scenario 1:\npandas, numpy, matplotlib, datetime\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom datetime import datetime, timedelta\n\n# Constants\nDATE_FORMAT = '%Y-%m-%d'\nSTART_DATE = '2016-01-01'\nPERIODS = 13\nFREQ = 'WOM-2FRI'\n\ndef plot_sales_forecast():\n    \"\"\"\n    Generate a sales forecast for 13 periods starting from a given date and plot the results.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - datetime\n\n    Example:\n    >>> plot_sales_forecast()\n    \"\"\"\n    date_range = pd.date_range(START_DATE, freq=FREQ, periods=PERIODS)\n    sales_forecast = np.random.randint(low=100, high=500, size=PERIODS)\n\n    forecast_df = pd.DataFrame({'Date': date_range, 'Sales': sales_forecast})\n    forecast_df.set_index('Date', inplace=True)\n\n    plt.figure(figsize=(10, 6))\n    plt.plot(forecast_df.index, forecast_df['Sales'], marker='o')\n    plt.title('Sales Forecast')\n    plt.xlabel('Date')\n    plt.ylabel('Sales')\n    plt.grid(True)\n    plt.show()\n\n    return forecast_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, datetime, matplotlib, seaborn\n```python\nimport pandas as pd\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nSTART_DATE = '2016-01-01'\nPERIODS = 13\nFREQ = 'WOM-2FRI'\nCATEGORIES = ['Electronics', 'Fashion', 'Home & Kitchen', 'Automotive', 'Sports']\n\ndef analyze_category_sales():\n    \"\"\"\n    Generate a sales report for different categories over a period and visualize it.\n\n    Requirements:\n    - pandas\n    - datetime\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> analyze_category_sales()\n    \"\"\"\n    date_range = pd.date_range(START_DATE, freq=FREQ, periods=PERIODS)\n    report_data = []\n\n    for date in date_range:\n        for category in CATEGORIES:\n            sales = np.random.randint(low=100, high=500)\n            report_data.append([date, category, sales])\n\n    sales_df = pd.DataFrame(report_data, columns=['Date', 'Category', 'Sales'])\n    sales_df.set_index('Date', inplace=True)\n\n    plt.figure(figsize=(12, 8))\n    sns.lineplot(data=sales_df, x='Date', y='Sales', hue='Category')\n    plt.title('Category-wise Sales')\n    plt.grid(True)\n    plt.show()\n\n    return sales_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, datetime, numpy, statsmodels\n```python\nimport pandas as pd\nfrom datetime import datetime\nimport numpy as np\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\n# Constants\nSTART_DATE = '2016-01-01'\nPERIODS = 13\nFREQ = 'WOM-2FRI'\n\ndef decompose_sales_trend():\n    \"\"\"\n    Generate a sales series and decompose it into trend, seasonality and residuals.\n\n    Requirements:\n    - pandas\n    - datetime\n    - numpy\n    - statsmodels.tsa.seasonal\n\n    Example:\n    >>> decompose_sales_trend()\n    \"\"\"\n    date_range = pd.date_range(START_DATE, freq=FREQ, periods=PERIODS)\n    sales_data = np.random.randint(low=100, high=500, size=PERIODS)\n\n    sales_series = pd.Series(sales_data, index=date_range)\n\n    decomposition = seasonal_decompose(sales_series)\n\n    return decomposition\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, datetime, numpy, matplotlib\n```python\nimport pandas as pd\nfrom datetime import datetime\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nSTART_DATE = '2016-01-01'\nPERIODS = 13\nFREQ = 'WOM-2FRI'\n\ndef plot_stock_prices():\n    \"\"\"\n    Generate a stock price series for a period and plot the results.\n\n    Requirements:\n    - pandas\n    - datetime\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_stock_prices()\n    \"\"\"\n    date_range = pd.date_range(START_DATE, freq=FREQ, periods=PERIODS)\n    stock_prices = np.random.uniform(low=100, high=500, size=PERIODS)\n\n    prices_df = pd.DataFrame({'Date': date_range, 'Price': stock_prices})\n    prices_df.set_index('Date', inplace=True)\n\n    plt.figure(figsize=(10, 6))\n    plt.plot(prices_df.index, prices_df['Price'], marker='o')\n    plt.title('Stock Prices')\n    plt.xlabel('Date')\n    plt.ylabel('Price')\n    plt.grid(True)\n    plt.show()\n\n    return prices_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, datetime, numpy, sklearn\n```python\nimport pandas as pd\nfrom datetime import datetime\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\n# Constants\nSTART_DATE = '2016-01-01'\nPERIODS = 13\nFREQ = 'WOM-2FRI'\n\ndef forecast_sales_with_linear_regression():\n    \"\"\"\n    Generate a sales series and forecast future sales using linear regression.\n\n    Requirements:\n    - pandas\n    - datetime\n    - numpy\n    - sklearn.linear_model\n\n    Example:\n    >>> forecast_sales_with_linear_regression()\n    \"\"\"\n    date_range = pd.date_range(START_DATE, freq=FREQ, periods=PERIODS)\n    sales_data = np.random.randint(low=100, high=500, size=PERIODS)\n\n    sales_df = pd.DataFrame({'Date': date_range, 'Sales': sales_data})\n    sales_df['Date'] = pd.to_datetime(sales_df['Date'])\n\n    X = np.array(range(len(sales_df))).reshape(-1, 1)\n    y = sales_df['Sales'].values\n\n    model = LinearRegression()\n    model.fit(X, y)\n\n    future_dates = pd.date_range(start=date_range[-1], freq=FREQ, periods=PERIODS)\n    future_dates = np.array(range(len(sales_df), len(sales_df) + PERIODS)).reshape(-1, 1)\n\n    future_sales = model.predict(future_dates)\n\n    return future_sales\n```"}
{"seed": "{\"task_id\": 508657, \"prompt\": \"def f_508657():\\n\\t\", \"suffix\": \"\\n\\treturn matrix\", \"canonical_solution\": \"matrix = [['a', 'b'], ['c', 'd'], ['e', 'f']]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    matrix = candidate()\\n    assert len(matrix) == 3\\n    assert all([len(row)==2 for row in matrix])\\n\"], \"entry_point\": \"f_508657\", \"intent\": \"Create multidimensional array `matrix` with 3 rows and 2 columns in python\", \"library\": []}", "generation": "Scenario 1:\nnumpy, pandas, random, matplotlib.pyplot\n```python\nimport numpy as np\nimport pandas as pd\nfrom random import randrange\nimport matplotlib.pyplot as plt\n\n# Constants\nROWS = 3\nCOLUMNS = 2\nMIN_VAL = 0\nMAX_VAL = 100\n\ndef generate_random_matrix():\n    \"\"\"\n    Generates a 3x2 matrix with random integers between 0 and 100, and then converts it into a DataFrame.\n    The DataFrame is then plotted as a bar graph.\n\n    Returns:\n    DataFrame: A pandas DataFrame with random integers.\n\n    Requirements:\n    - numpy\n    - pandas\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = generate_random_matrix()\n    >>> print(df)\n    >>> df.plot(kind='bar')\n    \"\"\"\n    matrix = np.array([[randrange(MIN_VAL, MAX_VAL) for j in range(COLUMNS)] for i in range(ROWS)])\n    df = pd.DataFrame(matrix)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, itertools, datetime\n```python\nimport numpy as np\nimport itertools\nfrom datetime import datetime\n\n# Constants\nROWS = 3\nCOLUMNS = 2\nSTART_DATE = datetime(2021, 1, 1)\nEND_DATE = datetime(2021, 12, 31)\n\ndef generate_date_matrix():\n    \"\"\"\n    Generate a 3x2 matrix with unique dates between a start and end date.\n\n    Returns:\n    ndarray: A numpy ndarray with unique dates.\n\n    Requirements:\n    - numpy\n    - itertools\n    - datetime\n\n    Example:\n    >>> matrix = generate_date_matrix()\n    >>> print(matrix)\n    \"\"\"\n    date_range = np.array([START_DATE + np.timedelta64(i, 'D') for i in range((END_DATE - START_DATE).days + 1)])\n    np.random.shuffle(date_range)\n\n    matrix = np.array(list(itertools.islice(date_range, ROWS * COLUMNS))).reshape(ROWS, COLUMNS)\n\n    return matrix\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnumpy, matplotlib, sklearn.preprocessing\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Constants\nROWS = 3\nCOLUMNS = 2\n\ndef generate_and_scale_matrix():\n    \"\"\"\n    Generate a 3x2 matrix with random values and scale them between 0 and 1.\n\n    Returns:\n    ndarray: A numpy ndarray with scaled values.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.preprocessing\n\n    Example:\n    >>> matrix = generate_and_scale_matrix()\n    >>> print(matrix)\n    >>> plt.imshow(matrix, cmap='hot', interpolation='nearest')\n    \"\"\"\n    matrix = np.random.rand(ROWS, COLUMNS)\n    scaler = MinMaxScaler()\n    scaled_matrix = scaler.fit_transform(matrix)\n\n    return scaled_matrix\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, pandas, seaborn\n```python\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n# Constants\nROWS = 3\nCOLUMNS = 2\n\ndef generate_and_visualize_matrix():\n    \"\"\"\n    Generate a 3x2 matrix with random values and visualize it as a heatmap.\n\n    Returns:\n    DataFrame: A pandas DataFrame with random values.\n\n    Requirements:\n    - numpy\n    - pandas\n    - seaborn\n\n    Example:\n    >>> df = generate_and_visualize_matrix()\n    >>> print(df)\n    >>> sns.heatmap(df)\n    \"\"\"\n    matrix = np.random.rand(ROWS, COLUMNS)\n    df = pd.DataFrame(matrix)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnumpy, scipy.linalg, matplotlib\n```python\nimport numpy as np\nfrom scipy.linalg import svd\nimport matplotlib.pyplot as plt\n\n# Constants\nROWS = 3\nCOLUMNS = 2\n\ndef generate_and_decompose_matrix():\n    \"\"\"\n    Generate a 3x2 matrix with random values and perform singular value decomposition (SVD).\n\n    Returns:\n    tuple: A tuple with the decomposed matrices U, s, Vh.\n\n    Requirements:\n    - numpy\n    - scipy.linalg\n    - matplotlib\n\n    Example:\n    >>> U, s, Vh = generate_and_decompose_matrix()\n    >>> print('U:', U)\n    >>> print('s:', s)\n    >>> print('Vh:', Vh)\n    >>> plt.matshow(U, cmap='viridis')\n    >>> plt.matshow(np.diag(s), cmap='viridis')\n    >>> plt.matshow(Vh, cmap='viridis')\n    \"\"\"\n    matrix = np.random.rand(ROWS, COLUMNS)\n    U, s, Vh = svd(matrix)\n\n    return U, s, Vh\n```\n"}
{"seed": "{\"task_id\": 1007481, \"prompt\": \"def f_1007481(mystring):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"mystring.replace(' ', '_')\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(' ') == '_'\\n\", \"\\n    assert candidate(' _ ') == '___'\\n\", \"\\n    assert candidate('') == ''\\n\", \"\\n    assert candidate('123123') == '123123'\\n\", \"\\n    assert candidate('\\\\_ ') == '\\\\__'\\n\"], \"entry_point\": \"f_1007481\", \"intent\": \"replace spaces with underscore in string `mystring`\", \"library\": []}", "generation": "Scenario 1: \npandas, re, numpy, random\n```python\nimport pandas as pd\nimport re\nimport numpy as np\nfrom random import randint\n\n# Constants\nCATEGORIES = ['Electronics', 'Clothing', 'Home & Kitchen', 'Books', 'Toys & Games']\n\ndef create_product_catalogue(mystrings, n_products):\n    \"\"\"\n    Generate a product catalogue dataframe with replaced spaces by underscore in product names. \n    \n    Parameters:\n    mystrings (list): List of product names.\n    n_products (int): Number of products to generate.\n\n    Returns:\n    DataFrame: A pandas DataFrame with product catalogue information.\n\n    Requirements:\n    - pandas\n    - re\n    - numpy\n    - random\n\n    Example:\n    >>> create_product_catalogue(['Mobile Phone', 'T Shirt', 'Coffee Maker', 'Python Book', 'Toy Car'], 10)\n    \"\"\"\n    catalogue_data = []\n\n    for _ in range(n_products):\n        product_name = mystrings[randint(0, len(mystrings)-1)].replace(' ', '_')\n        category = CATEGORIES[randint(0, len(CATEGORIES)-1)]\n        price = round(np.random.normal(50, 10), 2)\n        catalogue_data.append([product_name, category, price])\n\n    catalogue_df = pd.DataFrame(catalogue_data, columns=['Product Name', 'Category', 'Price'])\n\n    return catalogue_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, random, re, datetime\n```python\nimport pandas as pd\nimport random \nimport re\nfrom datetime import datetime\n\n# Constants\nEMPLOYEES = ['John Doe', 'Jane Smith', 'James Brown', 'Mary Johnson', 'Robert Davis']\n\ndef assign_tasks(mystrings, n_tasks):\n    \"\"\"\n    Assign tasks to employees with replaced spaces by underscore in task names. \n    \n    Parameters:\n    mystrings (list): List of tasks.\n    n_tasks (int): Number of tasks to be assigned.\n\n    Returns:\n    DataFrame: A pandas DataFrame with task assignment information.\n\n    Requirements:\n    - pandas\n    - random\n    - re\n    - datetime\n\n    Example:\n    >>> assign_tasks(['Clean Office', 'Prepare Report', 'Client Meeting', 'Code Review', 'Product Launch'], 10)\n    \"\"\"\n    assignment_data = []\n\n    for _ in range(n_tasks):\n        task_name = mystrings[random.randint(0, len(mystrings)-1)].replace(' ', '_')\n        employee = EMPLOYEES[random.randint(0, len(EMPLOYEES)-1)]\n        due_date = datetime.today().strftime('%Y-%m-%d')\n        assignment_data.append([task_name, employee, due_date])\n\n    assignment_df = pd.DataFrame(assignment_data, columns=['Task Name', 'Assigned To', 'Due Date'])\n\n    return assignment_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, matplotlib, os, re\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport re\n\n# Constants\nFOLDER_PATH = './images/'\n\ndef save_plots(mystrings):\n    \"\"\"\n    Plot and save bar plots for given data with replaced spaces by underscore in file names. \n    \n    Parameters:\n    mystrings (list): List of names for the plots.\n\n    Returns:\n    list: A list of saved plot file names.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - os\n    - re\n\n    Example:\n    >>> save_plots(['Plot 1', 'Plot 2', 'Plot 3'])\n    \"\"\"\n    saved_plots = []\n\n    for name in mystrings:\n        data = np.random.rand(10)\n        plt.bar(range(len(data)), data)\n        plt.title(name)\n\n        file_name = re.sub(' ', '_', name) + '.png'\n        plt.savefig(os.path.join(FOLDER_PATH, file_name))\n\n        saved_plots.append(file_name)\n\n    return saved_plots\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, nltk, re, random\n```python\nimport pandas as pd\nimport nltk\nfrom nltk.corpus import words\nimport random\nimport re\n\n# Constants\nnltk.download('words')\nENGLISH_WORDS = words.words()\n\ndef generate_sentences(mystrings, n_sentences):\n    \"\"\"\n    Generate sentences with replaced spaces by underscore in certain words. \n    \n    Parameters:\n    mystrings (list): List of words to replace spaces with underscore.\n    n_sentences (int): Number of sentences to generate.\n\n    Returns:\n    list: A list of generated sentences.\n\n    Requirements:\n    - pandas\n    - nltk.corpus.words\n    - random\n    - re\n\n    Example:\n    >>> generate_sentences(['Mobile Phone', 'Coffee Maker', 'Python Book'], 5)\n    \"\"\"\n    sentences = []\n\n    for _ in range(n_sentences):\n        sentence = ' '.join(random.choices(ENGLISH_WORDS, k=10))\n        for word in mystrings:\n            sentence = re.sub(word, word.replace(' ', '_'), sentence, flags=re.IGNORECASE)\n        sentences.append(sentence)\n\n    return sentences\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, matplotlib, re, collections\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re\nfrom collections import Counter\n\n# Constants\nTEXT = 'Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed euismod eget nunc ac euismod. Donec auctor dolor auctor enim facilisis, sit amet porttitor nisl mollis. Proin ultrices, tortor a mollis pharetra, nisl lacus accumsan odio, at condimentum nunc risus non nulla. Nullam quis diam ac metus fermentum fermentum. Donec in euismod massa. Nullam semper id ante a cursus.'\n\ndef plot_word_frequencies(mystrings):\n    \"\"\"\n    Plot the frequency of certain words with replaced spaces by underscore in a given text. \n    \n    Parameters:\n    mystrings (list): List of words to replace spaces with underscore.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - re\n    - collections.Counter\n\n    Example:\n    >>> plot_word_frequencies(['Lorem ipsum', 'consectetur adipiscing', 'Donec auctor'])\n    \"\"\"\n    for word in mystrings:\n        TEXT = re.sub(word, word.replace(' ', '_'), TEXT, flags=re.IGNORECASE)\n\n    word_counts = Counter(TEXT.split())\n\n    words, frequencies = zip(*word_counts.items())\n    indices = np.arange(len(word_counts))\n\n    plt.figure(figsize=(12, 8))\n    plt.bar(indices, frequencies, color='b')\n    plt.xticks(indices, words, rotation=45)\n    plt.tight_layout()\n    plt.show()\n```\n"}
{"seed": "{\"task_id\": 1249786, \"prompt\": \"def f_1249786(my_string):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"\\\"\\\"\\\" \\\"\\\"\\\".join(my_string.split())\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('hello   world ') == 'hello world'\\n\", \"\\n    assert candidate('') == ''\\n\", \"\\n    assert candidate('    ') == ''\\n\", \"\\n    assert candidate('  hello') == 'hello'\\n\", \"\\n    assert candidate(' h  e  l  l  o   ') == 'h e l l o'\\n\"], \"entry_point\": \"f_1249786\", \"intent\": \"split string `my_string` on white spaces\", \"library\": []}", "generation": "Scenario 1: \nre, string, random\n```python\nimport re\nimport string\nimport random\n\n# Constants\nSPECIAL_CHARS = string.punctuation\nREPLACEMENTS = {' ': '_', '\\t': '__', '\\n': '___'}\n\ndef preprocess_text(text):\n    \"\"\"\n    Function to preprocess text by removing special characters, replacing whitespace \n    characters with underscores and randomizing the case of the characters.\n\n    Parameters:\n    text (str): The text string to be preprocessed.\n\n    Returns:\n    str: The preprocessed text.\n\n    Requirements:\n    - re\n    - string\n    - random\n\n    Example:\n    >>> preprocess_text('Hello   World!')\n    'hElLO_WoRlD'\n    \"\"\"\n    # Remove special characters\n    text = re.sub('[%s]' % re.escape(SPECIAL_CHARS), '', text)\n\n    # Replace whitespace characters\n    for k, v in REPLACEMENTS.items():\n        text = text.replace(k, v)\n\n    # Randomize case\n    text = ''.join(random.choice([k.upper(), k ]) for k in text)\n\n    return text\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncollections, string, re\n```python\nimport collections\nimport string\nimport re\n\n# Constants\nPUNCTUATION = string.punctuation\n\ndef count_words_and_chars(text):\n    \"\"\"\n    Function to count the number of words and characters in a text. \n    Punctuation is not considered as a character.\n\n    Parameters:\n    text (str): The text to be analyzed.\n\n    Returns:\n    tuple: A tuple containing the number of words and characters.\n\n    Requirements:\n    - collections\n    - string\n    - re\n\n    Example:\n    >>> count_words_and_chars('Hello, world!')\n    (2, 10)\n    \"\"\"\n    words = text.split()\n    chars = re.sub('['+string.punctuation+']', '', text).replace(' ', '')\n    \n    return len(words), len(chars)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nstring, re, random\n```python\nimport string\nimport re\nimport random\n\n# Constants\nLETTERS = string.ascii_letters\n\ndef scramble_words(text):\n    \"\"\"\n    Function to scramble the letters in each word of a text while keeping the \n    first and last letters of each word in place.\n\n    Parameters:\n    text (str): The text to be scrambled.\n\n    Returns:\n    str: The scrambled text.\n\n    Requirements:\n    - string\n    - re\n    - random\n\n    Example:\n    >>> scramble_words('Hello, world!')\n    'Hlelo, wlord!'\n    \"\"\"\n    words = text.split()\n    scrambled_words = []\n    \n    for word in words:\n        if len(word) > 3:\n            middle = list(word[1:-1])\n            random.shuffle(middle)\n            scrambled_word = word[0] + ''.join(middle) + word[-1]\n        else:\n            scrambled_word = word\n        scrambled_words.append(scrambled_word)\n    \n    return ' '.join(scrambled_words)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nstring, re, random\n```python\nimport string\nimport re\nimport random\n\n# Constants\nALPHABET = string.ascii_lowercase\n\ndef generate_string(text):\n    \"\"\"\n    Function to generate a string of random letters with the same length and \n    number of words as the input text.\n\n    Parameters:\n    text (str): The text to be mirrored.\n\n    Returns:\n    str: The generated string.\n\n    Requirements:\n    - string\n    - re\n    - random\n\n    Example:\n    >>> generate_string('Hello, world!')\n    'Bcqwi, vnkzy!'\n    \"\"\"\n    words = text.split()\n    new_words = []\n    \n    for word in words:\n        new_word = ''.join(random.choice(ALPHABET) for _ in range(len(word)))\n        new_words.append(new_word)\n    \n    return ' '.join(new_words)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nstring, re, random\n```python\nimport string\nimport re\nimport random\n\n# Constants\nALPHABET = string.ascii_lowercase\nNUMBERS = string.digits\n\ndef generate_password(text):\n    \"\"\"\n    Function to generate a password using the same number of words and characters \n    as the input text. The password will contain a mix of lower case letters and numbers.\n\n    Parameters:\n    text (str): The text to be mirrored.\n\n    Returns:\n    str: The generated password.\n\n    Requirements:\n    - string\n    - re\n    - random\n\n    Example:\n    >>> generate_password('Hello, world!')\n    'b3q1i, 7n6z0!'\n    \"\"\"\n    password = ''\n    for char in text:\n        if char.isalpha():\n            password += random.choice(ALPHABET)\n        elif char.isdigit():\n            password += random.choice(NUMBERS)\n        else:\n            password += char\n    \n    return password\n```"}
{"seed": "{\"task_id\": 4444923, \"prompt\": \"def f_4444923(filename):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"os.path.splitext(filename)[0]\", \"test_start\": \"\\nimport os\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('/Users/test/hello.txt') == '/Users/test/hello'\\n\", \"\\n    assert candidate('hello.txt') == 'hello'\\n\", \"\\n    assert candidate('hello') == 'hello'\\n\", \"\\n    assert candidate('.gitignore') == '.gitignore'\\n\"], \"entry_point\": \"f_4444923\", \"intent\": \"get filename without extension from file `filename`\", \"library\": [\"os\"]}", "generation": "Scenario 1: \nos, re, pathlib, collections\n```python\nimport os\nimport re\nfrom pathlib import Path\nfrom collections import Counter\n\n# Constants\nEXTENSIONS = ['.txt', '.docx', '.xlsx', '.csv']\n\ndef count_file_extensions(directory):\n    \"\"\"\n    Count the number of files for each file extension in a directory.\n\n    Parameters:\n    directory (str): The directory to count the files in.\n\n    Returns:\n    Counter: A collections.Counter object with the count of each file extension.\n\n    Requirements:\n    - os\n    - re\n    - pathlib\n    - collections\n\n    Example:\n    >>> count_file_extensions('/Users/test/Documents')\n    \"\"\"\n    counter = Counter()\n\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            extension = Path(file).suffix\n            if extension in EXTENSIONS:\n                counter[extension] += 1\n\n    return counter\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, pathlib, shutil\n```python\nimport os\nfrom pathlib import Path\nimport shutil\n\n# Constants\nEXTENSIONS = ['.txt', '.docx', '.xlsx', '.csv']\n\ndef move_files_with_extensions(source_directory, target_directory):\n    \"\"\"\n    Move files with certain extensions from a source directory to a target directory.\n\n    Parameters:\n    source_directory (str): The source directory.\n    target_directory (str): The target directory.\n\n    Returns:\n    int: The number of files moved.\n\n    Requirements:\n    - os\n    - pathlib\n    - shutil\n\n    Example:\n    >>> move_files_with_extensions('/Users/test/Documents', '/Users/test/Documents/moved_files')\n    \"\"\"\n    moved_files = 0\n\n    for root, dirs, files in os.walk(source_directory):\n        for file in files:\n            extension = Path(file).suffix\n            if extension in EXTENSIONS:\n                shutil.move(os.path.join(root, file), target_directory)\n                moved_files += 1\n\n    return moved_files\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, glob, pathlib, zipfile\n```python\nimport os\nimport glob\nfrom pathlib import Path\nimport zipfile\n\n# Constants\nEXTENSIONS = ['.txt', '.docx', '.xlsx', '.csv']\n\ndef zip_files_with_extensions(source_directory, zip_name):\n    \"\"\"\n    Zip files with certain extensions from a source directory.\n\n    Parameters:\n    source_directory (str): The source directory.\n    zip_name (str): The name of the zip file.\n\n    Returns:\n    str: The path to the zip file.\n\n    Requirements:\n    - os\n    - glob\n    - pathlib\n    - zipfile\n\n    Example:\n    >>> zip_files_with_extensions('/Users/test/Documents', 'zipped_files.zip')\n    \"\"\"\n    with zipfile.ZipFile(zip_name, 'w') as zipf:\n        for extension in EXTENSIONS:\n            for file in glob.glob(f\"{source_directory}/**/*{extension}\", recursive=True):\n                zipf.write(file, arcname=Path(file).name)\n\n    return os.path.realpath(zipf.filename)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, pathlib, pandas, csv\n```python\nimport os\nfrom pathlib import Path\nimport pandas as pd\nimport csv\n\n# Constants\nEXTENSIONS = ['.txt', '.docx', '.xlsx', '.csv']\n\ndef convert_files_to_csv(source_directory, target_directory):\n    \"\"\"\n    Convert files with certain extensions from a source directory to csv files.\n\n    Parameters:\n    source_directory (str): The source directory.\n    target_directory (str): The target directory.\n\n    Returns:\n    int: The number of files converted.\n\n    Requirements:\n    - os\n    - pathlib\n    - pandas\n    - csv\n\n    Example:\n    >>> convert_files_to_csv('/Users/test/Documents', '/Users/test/Documents/csv_files')\n    \"\"\"\n    converted_files = 0\n\n    for root, dirs, files in os.walk(source_directory):\n        for file in files:\n            extension = Path(file).suffix\n            if extension in EXTENSIONS:\n                df = pd.read_csv(os.path.join(root, file))\n                df.to_csv(os.path.join(target_directory, Path(file).stem + '.csv'), index=False)\n                converted_files += 1\n\n    return converted_files\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, pathlib, re, shutil\n```python\nimport os\nfrom pathlib import Path\nimport re\nimport shutil\n\n# Constants\nPATTERN = r'\\d{4}'\n\ndef move_files_matching_pattern(source_directory, target_directory):\n    \"\"\"\n    Move files matching a pattern from a source directory to a target directory.\n\n    Parameters:\n    source_directory (str): The source directory.\n    target_directory (str): The target directory.\n\n    Returns:\n    int: The number of files moved.\n\n    Requirements:\n    - os\n    - pathlib\n    - re\n    - shutil\n\n    Example:\n    >>> move_files_matching_pattern('/Users/test/Documents', '/Users/test/Documents/moved_files')\n    \"\"\"\n    moved_files = 0\n\n    for root, dirs, files in os.walk(source_directory):\n        for file in files:\n            if re.search(PATTERN, file):\n                shutil.move(os.path.join(root, file), target_directory)\n                moved_files += 1\n\n    return moved_files\n```"}
{"seed": "{\"task_id\": 13728486, \"prompt\": \"def f_13728486(l):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"[sum(l[:i]) for i, _ in enumerate(l)]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([1,2,3]) == [0,1,3]\\n\", \"\\n    assert candidate([]) == []\\n\", \"\\n    assert candidate([1]) == [0]\\n\"], \"entry_point\": \"f_13728486\", \"intent\": \"get a list containing the sum of each element `i` in list `l` plus the previous elements\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, itertools, collections\n```python\nimport pandas as pd\nimport numpy as np\nfrom itertools import accumulate\nfrom collections import OrderedDict\n\n# Constants\nDATA = pd.DataFrame({'A': [1, 2, 3, 4, 5], 'B': [5, 4, 3, 2, 1]})\n\ndef cumulative_sum_df(df):\n    \"\"\"\n    Get a DataFrame containing the cumulative sum of each element `i` in each column of the DataFrame `df`.\n\n    Parameters:\n    df (pandas.DataFrame): The input DataFrame.\n\n    Returns:\n    pandas.DataFrame: The DataFrame with cumulative sums.\n\n    Requirements:\n    - pandas\n    - numpy\n    - itertools\n    - collections\n\n    Example:\n    >>> print(cumulative_sum_df(DATA))\n    \"\"\"\n    df_cumsum = pd.DataFrame(OrderedDict((col, list(accumulate(df[col]))) for col in df.columns))\n\n    return df_cumsum\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nnumpy, scipy, matplotlib\n```python\nimport numpy as np\nfrom scipy import integrate\nimport matplotlib.pyplot as plt\n\n# Constants\nX = np.linspace(-2, 2, 1000)\n\ndef plot_cumulative_integral(func):\n    \"\"\"\n    Plot the cumulative integral of a function `func` over a range of x values.\n\n    Parameters:\n    func (function): The function to integrate.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - scipy\n    - matplotlib\n\n    Example:\n    >>> plot_cumulative_integral(np.sin)\n    \"\"\"\n    y = func(X)\n    y_int = integrate.cumulative_trapezoid(y, X, initial=0)\n\n    plt.plot(X, y_int)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnumpy, pandas, seaborn\n```python\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n# Constants\nDATA = pd.DataFrame({'A': [1, 2, 3, 4, 5], 'B': [5, 4, 3, 2, 1]})\n\ndef cumulative_sum_heatmap(df):\n    \"\"\"\n    Create a heatmap of the cumulative sum of each column in a DataFrame.\n\n    Parameters:\n    df (pandas.DataFrame): The input DataFrame.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - numpy\n    - seaborn\n\n    Example:\n    >>> cumulative_sum_heatmap(DATA)\n    \"\"\"\n    df_cumsum = df.cumsum()\n\n    sns.heatmap(df_cumsum, annot=True, fmt=\".1f\")\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, sklearn.preprocessing, pandas\n```python\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\n\n# Constants\nDATA = pd.DataFrame({'A': [1, 2, 3, 4, 5], 'B': [5, 4, 3, 2, 1]})\n\ndef normalize_cumulative_sum(df):\n    \"\"\"\n    Normalize the cumulative sum of each column in a DataFrame.\n\n    Parameters:\n    df (pandas.DataFrame): The input DataFrame.\n\n    Returns:\n    pandas.DataFrame: The DataFrame with normalized cumulative sums.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.preprocessing\n\n    Example:\n    >>> print(normalize_cumulative_sum(DATA))\n    \"\"\"\n    df_cumsum = df.cumsum()\n\n    scaler = MinMaxScaler()\n    df_norm_cumsum = pd.DataFrame(scaler.fit_transform(df_cumsum), columns=df.columns)\n\n    return df_norm_cumsum\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnumpy, matplotlib, scipy\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n# Constants\nDATA = np.array([1, 2, 3, 4, 5])\n\ndef plot_cumulative_probability(data):\n    \"\"\"\n    Plot the cumulative probability of a list of numbers.\n\n    Parameters:\n    data (numpy.array): The input array of numbers.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - matplotlib\n    - scipy\n\n    Example:\n    >>> plot_cumulative_probability(DATA)\n    \"\"\"\n    cumulative_prob = np.cumsum(data) / np.sum(data)\n\n    plt.plot(cumulative_prob)\n    plt.xlabel('Index')\n    plt.ylabel('Cumulative Probability')\n    plt.show()\n```\nAbove is the illustration."}
{"seed": "{\"task_id\": 9743134, \"prompt\": \"def f_9743134():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"\\\"\\\"\\\"Docs/src/Scripts/temp\\\"\\\"\\\".replace('/', '/\\\\x00/').split('\\\\x00')\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate() == ['Docs/', '/src/', '/Scripts/', '/temp']\\n\", \"\\n    assert candidate() != ['Docs', 'src', 'Scripts', 'temp']\\n\"], \"entry_point\": \"f_9743134\", \"intent\": \"split a string `Docs/src/Scripts/temp` by `/` keeping `/` in the result\", \"library\": []}", "generation": "Scenario 1: \nos, re, pathlib\n```python\nimport os\nimport re\nfrom pathlib import Path\n\n# Constants\nFILE_PATH = 'Docs/src/Scripts/temp'\nSPLIT_PATTERN = '/'\n\ndef split_path_keep_delimiter(path=FILE_PATH, delimiter=SPLIT_PATTERN):\n    \"\"\"\n    Split an absolute file path by a specified delimiter, keeping the delimiter in the result.\n    The function also checks whether the file or the directories in the path exist.\n    \n    Parameters:\n    path (str): The file path to split.\n    delimiter (str): The delimiter to use for splitting the path.\n\n    Returns:\n    list: A list with the split path components.\n\n    Requirements:\n    - os\n    - re\n    - pathlib.Path\n\n    Example:\n    >>> split_path_keep_delimiter()\n    \"\"\"\n    path_components = re.split(f'({delimiter})', path)\n    path_exists = [Path(component).exists() if component != delimiter else True for component in path_components]\n\n    return list(zip(path_components, path_exists))\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, re, sys\n```python\nimport os\nimport re\nimport sys\n\n# Constants\nFILE_PATH = 'Docs/src/Scripts/temp'\nSPLIT_PATTERN = '/'\n\ndef split_path_validate_os(path=FILE_PATH, delimiter=SPLIT_PATTERN):\n    \"\"\"\n    Split a file path by a specified delimiter, keeping the delimiter in the result.\n    The function also checks whether the file path is valid for the current operating system.\n    \n    Parameters:\n    path (str): The file path to split.\n    delimiter (str): The delimiter to use for splitting the path.\n\n    Returns:\n    list: A list with the split path components.\n\n    Requirements:\n    - os\n    - re\n    - sys\n\n    Example:\n    >>> split_path_validate_os()\n    \"\"\"\n    path_components = re.split(f'({delimiter})', path)\n    valid_path = os.path.join(*path_components)\n\n    if sys.platform.startswith('win'):\n        valid_path = valid_path.replace('/', '\\\\')\n\n    return path_components if valid_path == path else []\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, re, shutil\n```python\nimport os\nimport re\nimport shutil\n\n# Constants\nFILE_PATH = 'Docs/src/Scripts/temp'\nSPLIT_PATTERN = '/'\n\ndef split_path_disk_usage(path=FILE_PATH, delimiter=SPLIT_PATTERN):\n    \"\"\"\n    Split a file path by a specified delimiter, keeping the delimiter in the result.\n    The function also computes the total, used and free disk space for each directory in the path.\n    \n    Parameters:\n    path (str): The file path to split.\n    delimiter (str): The delimiter to use for splitting the path.\n\n    Returns:\n    list: A list with the split path components and their disk usage.\n\n    Requirements:\n    - os\n    - re\n    - shutil\n\n    Example:\n    >>> split_path_disk_usage()\n    \"\"\"\n    path_components = re.split(f'({delimiter})', path)\n    disk_usage = [shutil.disk_usage(component) if component != delimiter else {} for component in path_components]\n\n    return list(zip(path_components, disk_usage))\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, re, glob\n```python\nimport os\nimport re\nimport glob\n\n# Constants\nFILE_PATH = 'Docs/src/Scripts/temp'\nSPLIT_PATTERN = '/'\n\ndef split_path_list_files(path=FILE_PATH, delimiter=SPLIT_PATTERN):\n    \"\"\"\n    Split a file path by a specified delimiter, keeping the delimiter in the result.\n    The function also lists all the files in each directory in the path.\n    \n    Parameters:\n    path (str): The file path to split.\n    delimiter (str): The delimiter to use for splitting the path.\n\n    Returns:\n    list: A list with the split path components and their files.\n\n    Requirements:\n    - os\n    - re\n    - glob\n\n    Example:\n    >>> split_path_list_files()\n    \"\"\"\n    path_components = re.split(f'({delimiter})', path)\n    files = [glob.glob(f'{component}/*') if component != delimiter else [] for component in path_components]\n\n    return list(zip(path_components, files))\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, re, hashlib\n```python\nimport os\nimport re\nimport hashlib\n\n# Constants\nFILE_PATH = 'Docs/src/Scripts/temp'\nSPLIT_PATTERN = '/'\n\ndef split_path_hash_files(path=FILE_PATH, delimiter=SPLIT_PATTERN):\n    \"\"\"\n    Split a file path by a specified delimiter, keeping the delimiter in the result.\n    The function also computes the hash of the file if the path is a file.\n    \n    Parameters:\n    path (str): The file path to split.\n    delimiter (str): The delimiter to use for splitting the path.\n\n    Returns:\n    list: A list with the split path components and their hash if they are files.\n\n    Requirements:\n    - os\n    - re\n    - hashlib\n\n    Example:\n    >>> split_path_hash_files()\n    \"\"\"\n    path_components = re.split(f'({delimiter})', path)\n    hashes = []\n\n    for component in path_components:\n        if component != delimiter and os.path.isfile(component):\n            with open(component, 'rb') as f:\n                hashes.append(hashlib.sha256(f.read()).hexdigest())\n        else:\n            hashes.append(None)\n\n    return list(zip(path_components, hashes))\n```"}
{"seed": "{\"task_id\": 20546419, \"prompt\": \"def f_20546419(r):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"np.random.shuffle(np.transpose(r))\", \"test_start\": \"\\nimport numpy as np\\n\\ndef check(candidate):\", \"test\": [\"\\n    a1 = np.array([[ 1, 20], [ 2, 30]])\\n    candidate(a1)\\n    assert np.array_equal(a1, np.array([[ 1, 20],[ 2, 30]])) or np.array_equal(a1, np.array([[ 20, 1], [ 30, 2]]))\\n\", \"\\n    a2 = np.array([[ 1], [ 2]])\\n    candidate(a2)                       \\n    assert np.array_equal(a2,np.array([[ 1], [ 2]]) )\\n\", \"\\n    a3 = np.array([[ 1,2,3]])\\n    candidate(a3)\\n    assert np.array_equal(a3,np.array([[ 1,2,3]])) or np.array_equal(a3,np.array([[ 2,1,3]]))           or np.array_equal(a3,np.array([[ 1,3,2]]))            or np.array_equal(a3,np.array([[3,2,1]])) or np.array_equal(a3,np.array([[3,1,2]]))            or np.array_equal(a3,np.array([[2,3,1]])) \\n\", \"\\n    a4 = np.zeros(shape=(5,2))\\n    candidate(a4)\\n    assert np.array_equal(a4, np.zeros(shape=(5,2)))\\n\"], \"entry_point\": \"f_20546419\", \"intent\": \"shuffle columns of an numpy array 'r'\", \"library\": [\"numpy\"]}", "generation": "Scenario 1: \nnumpy, pandas, random, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nfrom random import shuffle\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef create_and_display_shuffled_dataframe(rows):\n    \"\"\"\n    Generate a pandas DataFrame with a given number of rows and shuffled columns.\n    \n    Parameters:\n    rows (int): The number of rows for the DataFrame.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with shuffled columns.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = create_and_display_shuffled_dataframe(10)\n    >>> print(df)\n    >>> df.describe().plot(kind='bar')\n    \"\"\"\n    data = np.random.rand(rows, len(COLUMNS))\n    shuffle(COLUMNS)\n    df = pd.DataFrame(data, columns=COLUMNS)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, random, sklearn.preprocessing, pandas\n```python\nimport numpy as np\nfrom random import shuffle\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\n\n# Constants\nFEATURES = ['f1', 'f2', 'f3', 'f4', 'f5']\n\ndef preprocess_data(records):\n    \"\"\"\n    Preprocess the given data by shuffling the features, normalizing the values,\n    and converting it to a pandas DataFrame.\n\n    Parameters:\n    records (ndarray): The array of records to preprocess.\n\n    Returns:\n    DataFrame: The preprocessed data as a pandas DataFrame.\n\n    Requirements:\n    - numpy\n    - random\n    - sklearn.preprocessing\n    - pandas\n\n    Example:\n    >>> data = np.array([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n    >>> df = preprocess_data(data)\n    >>> print(df)\n    \"\"\"\n    np.random.shuffle(np.transpose(records))\n\n    scaler = StandardScaler()\n    normalized_records = scaler.fit_transform(records)\n\n    shuffle(FEATURES)\n    df = pd.DataFrame(normalized_records, columns=FEATURES)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnumpy, random, matplotlib, seaborn\n```python\nimport numpy as np\nfrom random import shuffle\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nFEATURES = ['f1', 'f2', 'f3', 'f4', 'f5']\n\ndef plot_shuffled_array(array):\n    \"\"\"\n    Shuffle the columns of a given numpy array and plot the array as a heatmap.\n\n    Parameters:\n    array (ndarray): The array to shuffle and plot.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - random\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> array = np.array([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n    >>> plot_shuffled_array(array)\n    \"\"\"\n    np.random.shuffle(np.transpose(array))\n\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(array, annot=True, cmap=\"YlGnBu\", xticklabels=FEATURES)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, random, pandas, sklearn.decomposition\n```python\nimport numpy as np\nfrom random import shuffle\nimport pandas as pd\nfrom sklearn.decomposition import PCA\n\n# Constants\nFEATURES = ['f1', 'f2', 'f3', 'f4', 'f5']\n\ndef pca_on_shuffled_array(array):\n    \"\"\"\n    Shuffle the columns of a given numpy array and perform PCA on the shuffled array.\n\n    Parameters:\n    array (ndarray): The array to shuffle and perform PCA on.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the first two principal components.\n\n    Requirements:\n    - numpy\n    - random\n    - pandas\n    - sklearn.decomposition\n\n    Example:\n    >>> array = np.array([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n    >>> df = pca_on_shuffled_array(array)\n    >>> print(df)\n    \"\"\"\n    np.random.shuffle(np.transpose(array))\n\n    pca = PCA(n_components=2)\n    principal_components = pca.fit_transform(array)\n\n    df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnumpy, random, pandas, sklearn.ensemble\n```python\nimport numpy as np\nfrom random import shuffle\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Constants\nFEATURES = ['f1', 'f2', 'f3', 'f4', 'f5']\nTARGET = 'target'\n\ndef train_classifier_on_shuffled_array(array, target):\n    \"\"\"\n    Shuffle the columns of a given numpy array, and train a random forest classifier on the shuffled array.\n\n    Parameters:\n    array (ndarray): The array to shuffle and use for training.\n    target (ndarray): The target variable for training.\n\n    Returns:\n    RandomForestClassifier: The trained random forest classifier.\n\n    Requirements:\n    - numpy\n    - random\n    - pandas\n    - sklearn.ensemble\n\n    Example:\n    >>> array = np.array([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n    >>> target = np.array([0, 1])\n    >>> clf = train_classifier_on_shuffled_array(array, target)\n    \"\"\"\n    np.random.shuffle(np.transpose(array))\n\n    df = pd.DataFrame(array, columns=FEATURES)\n    df[TARGET] = target\n\n    clf = RandomForestClassifier()\n    clf.fit(df[FEATURES], df[TARGET])\n\n    return clf\n```"}
{"seed": "{\"task_id\": 32675861, \"prompt\": \"def f_32675861(df):\\n\\t\", \"suffix\": \"\\n\\treturn df\", \"canonical_solution\": \"df['D'] = df['B']\", \"test_start\": \"\\nimport pandas as pd\\n\\ndef check(candidate):\", \"test\": [\"\\n    df_1 = pd.DataFrame({'A': [1,2,3], 'B': ['a', 'b', 'c']})\\n    candidate(df_1)\\n    assert (df_1['D'] == df_1['B']).all()\\n\", \"\\n    df_2 = pd.DataFrame({'A': [1,2,3], 'B': [1, 'A', 'B']})\\n    candidate(df_2)\\n    assert (df_2['D'] == df_2['B']).all()\\n\", \"\\n    df_3 = pd.DataFrame({'B': [1]})\\n    candidate(df_3)\\n    assert df_3['D'][0] == 1\\n\", \"\\n    df_4 = pd.DataFrame({'B': []})\\n    candidate(df_4)\\n    assert len(df_4['D']) == 0\\n\"], \"entry_point\": \"f_32675861\", \"intent\": \"copy all values in a column 'B' to a new column 'D' in a pandas data frame 'df'\", \"library\": [\"pandas\"]}", "generation": "Scenario 1: \npandas, numpy, seaborn, matplotlib, sklearn\n```python\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\ndef plot_correlation_and_normalize(df):\n    \"\"\"\n    Plot the correlation matrix of a given pandas DataFrame, and return a new DataFrame \n    where all numeric columns are standardized (zero mean and unit variance).\n    \n    Parameters:\n    df (DataFrame): The input pandas DataFrame.\n    \n    Returns:\n    DataFrame: A new pandas DataFrame where all numeric columns are standardized.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - seaborn\n    - matplotlib.pyplot\n    - sklearn.preprocessing.StandardScaler\n    \n    Example:\n    >>> df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6], 'C': [7, 8, 9]})\n    >>> normalized_df = plot_correlation_and_normalize(df)\n    >>> print(normalized_df)\n    \"\"\"\n    # Plot correlation matrix\n    correlation = df.corr()\n    sns.heatmap(correlation, annot=True, cmap='coolwarm')\n    plt.show()\n\n    # Normalize numeric columns\n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n    scaler = StandardScaler()\n    df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \npandas, datetime, random, matplotlib\n```python\nimport pandas as pd\nfrom datetime import datetime, timedelta\nfrom random import randint\nimport matplotlib.pyplot as plt\n\ndef generate_and_plot_random_timeseries(start_date, end_date, num_series):\n    \"\"\"\n    Generate a DataFrame with multiple random timeseries starting from a given start date \n    to an end date, and plot all the timeseries in one plot.\n    \n    Parameters:\n    start_date (str): The start date in \"yyyy-mm-dd\" format.\n    end_date (str): The end date in \"yyyy-mm-dd\" format.\n    num_series (int): The number of random timeseries to generate.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the generated random timeseries.\n    \n    Requirements:\n    - pandas\n    - datetime\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> start_date = '2020-01-01'\n    >>> end_date = '2020-12-31'\n    >>> num_series = 3\n    >>> df = generate_and_plot_random_timeseries(start_date, end_date, num_series)\n    >>> print(df)\n    \"\"\"\n    # Generate date range\n    start_date = datetime.strptime(start_date, '%Y-%m-%d')\n    end_date = datetime.strptime(end_date, '%Y-%m-%d')\n    date_range = pd.date_range(start_date, end_date)\n\n    # Generate random timeseries data\n    data = {}\n    for i in range(num_series):\n        series_name = f'series_{i+1}'\n        data[series_name] = [randint(0, 100) for _ in range(len(date_range))]\n\n    df = pd.DataFrame(data, index=date_range)\n\n    # Plot timeseries\n    df.plot()\n    plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \npandas, numpy, matplotlib, scipy\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\ndef plot_histogram_and_fit_normal(df, column):\n    \"\"\"\n    Plot the histogram of a specified column in a DataFrame, and fit a normal \n    distribution to this data. The plot includes the PDF of the fitted normal distribution.\n    \n    Parameters:\n    df (DataFrame): The input pandas DataFrame.\n    column (str): The column name.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - scipy.stats.norm\n    \n    Example:\n    >>> df = pd.DataFrame({'A': np.random.normal(0, 1, 1000)})\n    >>> plot_histogram_and_fit_normal(df, 'A')\n    \"\"\"\n    data = df[column]\n    mu, std = norm.fit(data)\n\n    plt.hist(data, bins=30, density=True, alpha=0.6, color='g')\n\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, mu, std)\n    plt.plot(x, p, 'k', linewidth=2)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \npandas, numpy, seaborn, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef plot_pairplot_and_calculate_covariance(df):\n    \"\"\"\n    Plot a pairplot of a DataFrame and calculate the covariance matrix.\n    \n    Parameters:\n    df (DataFrame): The input pandas DataFrame.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the covariance matrix.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - seaborn\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6], 'C': [7, 8, 9]})\n    >>> covariance_df = plot_pairplot_and_calculate_covariance(df)\n    >>> print(covariance_df)\n    \"\"\"\n    sns.pairplot(df)\n    plt.show()\n\n    covariance_df = df.cov()\n\n    return covariance_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \npandas, numpy, matplotlib, sklearn\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\ndef plot_scatter_and_fit_linear_regression(df, x_column, y_column):\n    \"\"\"\n    Plot a scatter plot of two columns of a DataFrame and fit a linear regression model to the data.\n    \n    Parameters:\n    df (DataFrame): The input pandas DataFrame.\n    x_column (str): The column name for the x-axis.\n    y_column (str): The column name for the y-axis.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.linear_model.LinearRegression\n    \n    Example:\n    >>> df = pd.DataFrame({'A': [1, 2, 3], 'B': [2, 3, 4]})\n    >>> plot_scatter_and_fit_linear_regression(df, 'A', 'B')\n    \"\"\"\n    X = df[x_column].values.reshape(-1, 1)\n    Y = df[y_column].values\n    reg = LinearRegression().fit(X, Y)\n    Y_pred = reg.predict(X)\n\n    plt.scatter(X, Y)\n    plt.plot(X, Y_pred, color='red')\n    plt.show()\n```\n"}
{"seed": "{\"task_id\": 14227561, \"prompt\": \"def f_14227561(data):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"list(data['A']['B'].values())[0]['maindata'][0]['Info']\", \"test_start\": \"\\nimport json\\n\\ndef check(candidate):\", \"test\": [\"\\n    s1 = '{\\\"A\\\":{\\\"B\\\":{\\\"unknown\\\":{\\\"1\\\":\\\"F\\\",\\\"maindata\\\":[{\\\"Info\\\":\\\"TEXT\\\"}]}}}}'\\n    data = json.loads(s1)\\n    assert candidate(data) == 'TEXT'\\n\", \"\\n    s2 = '{\\\"A\\\":{\\\"B\\\":{\\\"sample1\\\":{\\\"1\\\":\\\"F\\\",\\\"maindata\\\":[{\\\"Info\\\":\\\"TEXT!\\\"}]}}}}'\\n    data = json.loads(s2)\\n    assert candidate(data) == 'TEXT!'\\n\", \"\\n    s3 = '{\\\"A\\\":{\\\"B\\\":{\\\"sample_weird_un\\\":{\\\"1\\\":\\\"F\\\",\\\"maindata\\\":[{\\\"Info\\\":\\\"!\\\"}]}}}}'\\n    data = json.loads(s3)\\n    assert candidate(data) == '!'\\n\", \"\\n    s4 = '{\\\"A\\\":{\\\"B\\\":{\\\"sample_weird_un\\\":{\\\"1\\\":\\\"F\\\",\\\"maindata\\\":[{\\\"Info\\\":\\\"\\\"}]}}}}'\\n    data = json.loads(s4)\\n    assert candidate(data) == ''\\n\"], \"entry_point\": \"f_14227561\", \"intent\": \"find a value within nested json 'data' where the key inside another key 'B' is unknown.\", \"library\": [\"json\"]}", "generation": "Scenario 1: \npandas, json, os, numpy, random\n```python\nimport pandas as pd\nimport json\nimport os\nimport numpy as np\nfrom random import randint\n\n# Constants\nCOUNTRIES = ['USA', 'UK', 'China', 'Japan', 'Australia']\nPOPULATION = [331002651, 67886011, 1439323776, 126476461, 25499884]\n\ndef generate_country_report(json_data):\n    \"\"\"\n    Generate a report of population for a list of countries based on the provided \n    json data and save it as a csv file in the current directory. If a country's \n    population is not found in the json data, select a random population from \n    the list of constant POPULATION.\n    \n    Parameters:\n    json_data (str): The json data as a string.\n    \n    Returns:\n    str: The file path of the generated csv file.\n    \n    Requirements:\n    - pandas\n    - json\n    - os\n    - numpy\n    - random\n    \n    Example:\n    >>> json_str = '{\"Countries\": {\"USA\": 331002651, \"UK\": 67886011, \"China\": 1439323776}}'\n    >>> csv_file = generate_country_report(json_str)\n    >>> print(csv_file)\n    \"\"\"\n    data = json.loads(json_data)\n    country_data = []\n\n    for country in COUNTRIES:\n        population = data['Countries'].get(country, POPULATION[randint(0, len(POPULATION)-1)])\n        country_data.append([country, population])\n\n    df = pd.DataFrame(country_data, columns=['Country', 'Population'])\n\n    file_path = os.path.join(os.getcwd(), 'country_report.csv')\n    df.to_csv(file_path, index=False)\n\n    return file_path\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \njson, os, hashlib, base64, time\n```python\nimport json\nimport os\nimport hashlib\nimport base64\nimport time\n\ndef hash_file(file_path, unknown_key):\n    \"\"\"\n    Find a value within nested json 'data' where the key is unknown, hash the value using\n    SHA256, then write the hashed value to a new file with a timestamp in its name.\n    \n    Parameters:\n    file_path (str): The path of the json file.\n    unknown_key (str): The unknown key in the json.\n    \n    Returns:\n    str: The file path of the new file with the hashed value.\n    \n    Requirements:\n    - json\n    - os\n    - hashlib\n    - base64\n    - time\n    \n    Example:\n    >>> json_file = '/path/to/file.json'\n    >>> new_file = hash_file(json_file, 'B')\n    >>> print(new_file)\n    \"\"\"\n    with open(file_path, 'r') as f:\n        data = json.load(f)\n    \n    value = list(data['A'][unknown_key].values())[0]['maindata'][0]['Info']\n    hashed_value = hashlib.sha256(value.encode()).digest()\n    hashed_str = base64.b64encode(hashed_value).decode()\n\n    new_file_name = f\"{unknown_key}_hashed_{int(time.time())}.txt\"\n    new_file_path = os.path.join(os.getcwd(), new_file_name)\n\n    with open(new_file_path, 'w') as f:\n        f.write(hashed_str)\n\n    return new_file_path\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \njson, numpy, matplotlib, seaborn, pandas\n```python\nimport json\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\ndef visualize_data(json_data, unknown_key):\n    \"\"\"\n    Extract data from a nested json structure where the key is unknown,\n    convert the data into numpy array, and visualize the data using seaborn.\n    \n    Parameters:\n    json_data (str): The json data as a string.\n    unknown_key (str): The unknown key in the json.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - json\n    - numpy\n    - matplotlib\n    - seaborn\n    - pandas\n    \n    Example:\n    >>> json_str = '{\"A\": {\"B\": {\"unknown\": {\"1\": \"F\", \"maindata\": [{\"Info\": \"5,10,15,20,25\"}]}}}}'\n    >>> visualize_data(json_data, 'B')\n    \"\"\"\n    data = json.loads(json_data)\n    str_values = list(data['A'][unknown_key].values())[0]['maindata'][0]['Info']\n    values = np.fromstring(str_values, sep=',')\n    df = pd.DataFrame(values, columns=['Values'])\n    \n    plt.figure(figsize=(10, 6))\n    sns.boxplot(data=df)\n    plt.title('Data Distribution')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \njson, requests, os, time\n```python\nimport json\nimport requests\nimport os\nimport time\n\ndef download_file(json_data, unknown_key):\n    \"\"\"\n    Find a URL within nested json 'data' where the key is unknown, download the file from \n    the URL, and save it in the current directory with a timestamp in its name.\n    \n    Parameters:\n    json_data (str): The json data as a string.\n    unknown_key (str): The unknown key in the json.\n    \n    Returns:\n    str: The file path of the downloaded file.\n    \n    Requirements:\n    - json\n    - requests\n    - os\n    - time\n    \n    Example:\n    >>> json_str = '{\"A\": {\"B\": {\"unknown\": {\"1\": \"F\", \"maindata\": [{\"Info\": \"https://example.com/file.txt\"}]}}}}'\n    >>> file_path = download_file(json_data, 'B')\n    >>> print(file_path)\n    \"\"\"\n    data = json.loads(json_data)\n    url = list(data['A'][unknown_key].values())[0]['maindata'][0]['Info']\n    \n    response = requests.get(url)\n    \n    filename = f\"{unknown_key}_{int(time.time())}.txt\"\n    file_path = os.path.join(os.getcwd(), filename)\n    \n    with open(file_path, 'wb') as f:\n        f.write(response.content)\n\n    return file_path\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \njson, pandas, sklearn, matplotlib\n```python\nimport json\nimport pandas as pd\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\n\ndef normalize_data(json_data, unknown_key):\n    \"\"\"\n    Find a data series within nested json 'data' where the key is unknown and normalize \n    this data, then visualize the original data and the normalized data.\n    \n    Parameters:\n    json_data (str): The json data as a string.\n    unknown_key (str): The unknown key in the json.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - json\n    - pandas\n    - sklearn.preprocessing\n    - matplotlib\n    \n    Example:\n    >>> json_str = '{\"A\": {\"B\": {\"unknown\": {\"1\": \"F\", \"maindata\": [{\"Info\": \"5,10,15,20,25\"}]}}}}'\n    >>> normalize_data(json_data, 'B')\n    \"\"\"\n    data = json.loads(json_data)\n    str_values = list(data['A'][unknown_key].values())[0]['maindata'][0]['Info']\n    values = pd.Series(str_values.split(',')).astype(int)\n    \n    min_max_scaler = preprocessing.MinMaxScaler()\n    normalized_values = min_max_scaler.fit_transform(values.values.reshape(-1,1)).reshape(-1)\n    \n    plt.figure(figsize=(10, 6))\n    plt.plot(values, label='Original Data')\n    plt.plot(normalized_values, label='Normalized Data')\n    plt.legend()\n    plt.show()\n```\n"}
{"seed": "{\"task_id\": 14858916, \"prompt\": \"def f_14858916(string, predicate):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"all(predicate(x) for x in string)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    def predicate(x):\\n        if x == 'a':\\n            return True\\n        else:\\n            return False\\n    assert candidate('aab', predicate) == False\\n\", \"\\n    def predicate(x):\\n        if x == 'a':\\n            return True\\n        else:\\n            return False\\n    assert candidate('aa', predicate) == True\\n\", \"\\n    def predicate(x):\\n        if x == 'a':\\n            return True\\n        else:\\n            return False\\n    assert candidate('', predicate) == True\\n\", \"\\n    def predicate(x):\\n        if x.islower():\\n            return True\\n        else:\\n            return False\\n    assert candidate('abc', predicate) == True\\n\", \"\\n    def predicate(x):\\n        if x.islower():\\n            return True\\n        else:\\n            return False\\n    assert candidate('Ab', predicate) == False\\n\", \"\\n    def predicate(x):\\n        if x.islower():\\n            return True\\n        else:\\n            return False\\n    assert candidate('ABCD', predicate) == False\\n\"], \"entry_point\": \"f_14858916\", \"intent\": \"check characters of string `string` are true predication of function `predicate`\", \"library\": []}", "generation": "Scenario 1: \nos, json, pathlib, re\n```python\nimport os\nimport json\nfrom pathlib import Path\nimport re\n\n# Constants\nPREDICATES = {\n    'is_file': lambda x: Path(x).is_file(),\n    'is_dir': lambda x: Path(x).is_dir(),\n    'has_special_chars': lambda x: bool(re.search(r'\\W', x)),\n    'has_numbers': lambda x: bool(re.search(r'\\d', x))\n}\n\ndef check_directory_content(dir_path, predicates):\n    \"\"\"\n    Check the names of all files and directories in the given directory path against\n    a list of predicates.\n\n    Parameters:\n    dir_path (str): The path to the directory.\n    predicates (list): The list of predicates to check.\n\n    Returns:\n    dict: A dictionary with filenames/directory names as keys and results as values.\n\n    Requirements:\n    - os\n    - json\n    - pathlib.Path\n    - re\n\n    Example:\n    >>> check_directory_content('.', ['is_file', 'has_numbers'])\n    \"\"\"\n    results = {}\n    for item in os.listdir(dir_path):\n        results[item] = {predicate: PREDICATES[predicate](item) for predicate in predicates}\n\n    return results\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nre, string, random\n```python\nimport re\nimport string\nimport random\n\n# Constants\nPREDICATES = {\n    'has_uppercase': lambda x: bool(re.search(r'[A-Z]', x)),\n    'has_lowercase': lambda x: bool(re.search(r'[a-z]', x)),\n    'has_special_chars': lambda x: bool(re.search(r'\\W', x)),\n    'has_numbers': lambda x: bool(re.search(r'\\d', x))\n}\n\ndef generate_and_check_string(length, predicates):\n    \"\"\"\n    Generate a random string of a given length and check it against a list of predicates.\n\n    Parameters:\n    length (int): The length of the string to generate.\n    predicates (list): The list of predicates to check.\n\n    Returns:\n    tuple: A tuple where the first element is the generated string and the second element\n           is a dictionary with predicates as keys and results as values.\n\n    Requirements:\n    - re\n    - string\n    - random\n\n    Example:\n    >>> generate_and_check_string(10, ['has_uppercase', 'has_numbers'])\n    \"\"\"\n    s = ''.join(random.choices(string.ascii_letters + string.digits + string.punctuation, k=length))\n    results = {predicate: PREDICATES[predicate](s) for predicate in predicates}\n\n    return s, results\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \ncollections, string, re\n```python\nimport collections\nimport string\nimport re\n\n# Constants\nPREDICATES = {\n    'has_uppercase': lambda x: bool(re.search(r'[A-Z]', x)),\n    'has_lowercase': lambda x: bool(re.search(r'[a-z]', x)),\n    'has_special_chars': lambda x: bool(re.search(r'\\W', x)),\n    'has_numbers': lambda x: bool(re.search(r'\\d', x))\n}\n\ndef count_characters_and_check_predicates(text, predicates):\n    \"\"\"\n    Count the occurrences of each character in a string and check the string against a \n    list of predicates.\n\n    Parameters:\n    text (str): The text to check.\n    predicates (list): The list of predicates to check.\n\n    Returns:\n    dict: A dictionary where keys are characters and values are tuples. Each tuple contains \n          the count of the character and a dictionary with predicates as keys and results as values.\n\n    Requirements:\n    - collections\n    - string\n    - re\n\n    Example:\n    >>> count_characters_and_check_predicates('Hello, World!', ['has_uppercase', 'has_numbers'])\n    \"\"\"\n    counter = collections.Counter(text)\n    results = {char: (count, {predicate: PREDICATES[predicate](char) for predicate in predicates}) \n               for char, count in counter.items()}\n\n    return results\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \ncollections, string, re, itertools\n```python\nimport collections\nimport string\nimport re\nimport itertools\n\n# Constants\nPREDICATES = {\n    'has_uppercase': lambda x: bool(re.search(r'[A-Z]', x)),\n    'has_lowercase': lambda x: bool(re.search(r'[a-z]', x)),\n    'has_special_chars': lambda x: bool(re.search(r'\\W', x)),\n    'has_numbers': lambda x: bool(re.search(r'\\d', x))\n}\n\ndef check_strings_in_list(strings_list, predicates):\n    \"\"\"\n    Check each string in a list of strings against a list of predicates.\n\n    Parameters:\n    strings_list (list): The list of strings to check.\n    predicates (list): The list of predicates to check.\n\n    Returns:\n    dict: A dictionary where keys are strings and values are dictionaries with predicates \n          as keys and results as values.\n\n    Requirements:\n    - collections\n    - string\n    - re\n    - itertools\n\n    Example:\n    >>> check_strings_in_list(['Hello', 'World', '123'], ['has_uppercase', 'has_numbers'])\n    \"\"\"\n    results = {s: {predicate: PREDICATES[predicate](s) for predicate in predicates} for s in strings_list}\n\n    return results\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nos, string, re, glob\n```python\nimport os\nimport string\nimport re\nimport glob\n\n# Constants\nPREDICATES = {\n    'has_uppercase': lambda x: bool(re.search(r'[A-Z]', x)),\n    'has_lowercase': lambda x: bool(re.search(r'[a-z]', x)),\n    'has_special_chars': lambda x: bool(re.search(r'\\W', x)),\n    'has_numbers': lambda x: bool(re.search(r'\\d', x))\n}\n\ndef check_filenames_in_directory(dir_path, predicates):\n    \"\"\"\n    Check the filenames in a directory against a list of predicates.\n\n    Parameters:\n    dir_path (str): The path to the directory.\n    predicates (list): The list of predicates to check.\n\n    Returns:\n    dict: A dictionary where keys are filenames and values are dictionaries with predicates \n          as keys and results as values.\n\n    Requirements:\n    - os\n    - string\n    - re\n    - glob\n\n    Example:\n    >>> check_filenames_in_directory('.', ['has_uppercase', 'has_numbers'])\n    \"\"\"\n    results = {}\n    for filename in glob.glob(os.path.join(dir_path, '*')):\n        filename = os.path.basename(filename)\n        results[filename] = {predicate: PREDICATES[predicate](filename) for predicate in predicates}\n\n    return results\n```"}
{"seed": "{\"task_id\": 574236, \"prompt\": \"def f_574236():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"os.statvfs('/').f_files - os.statvfs('/').f_ffree\", \"test_start\": \"\\nimport os \\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate() == (os.statvfs('/').f_files - os.statvfs('/').f_ffree)\\n\"], \"entry_point\": \"f_574236\", \"intent\": \"determine number of files on a drive with python\", \"library\": [\"os\"]}", "generation": "Scenario 1:\nos, psutil, datetime, time\n```python\nimport os\nimport psutil\nfrom datetime import datetime, timedelta\nimport time\n\n# Constants\nCHECK_INTERVAL = 3  # in seconds\nMAX_DURATION = 60  # in seconds\n\ndef monitor_disk_usage():\n    \"\"\"\n    Monitor the disk usage and number of files on the disk for a given duration \n    with certain intervals.\n    \n    Parameters:\n    None.\n    \n    Returns:\n    dict: A dictionary with the timestamp as key and tuple of disk usage and \n    number of files as value.\n    \n    Requirements:\n    - os\n    - psutil\n    - datetime\n    - time\n    \n    Example:\n    >>> usage_report = monitor_disk_usage()\n    >>> for timestamp, data in usage_report.items():\n    ...     print(f\"At {timestamp}, Disk Usage: {data[0]*100}%, Number of Files: {data[1]}\")\n    \"\"\"\n    start_time = datetime.now()\n    usage_report = {}\n    \n    while (datetime.now() - start_time).total_seconds() <= MAX_DURATION:\n        disk_usage = psutil.disk_usage('/').percent\n        num_files = os.statvfs('/').f_files - os.statvfs('/').f_ffree\n        usage_report[datetime.now()] = (disk_usage, num_files)\n        time.sleep(CHECK_INTERVAL)\n        \n    return usage_report\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, pandas, matplotlib.pyplot, seaborn\n```python\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nDIRECTORIES = ['/home', '/var', '/etc', '/usr', '/bin']\n\ndef visualize_directory_sizes():\n    \"\"\"\n    Visualize the sizes of given directories using a bar plot.\n    \n    Parameters:\n    None.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the directory sizes.\n    \n    Requirements:\n    - os\n    - pandas\n    - matplotlib.pyplot\n    - seaborn\n    \n    Example:\n    >>> df = visualize_directory_sizes()\n    >>> sns.barplot(x='Directory', y='Size (MB)', data=df)\n    \"\"\"\n    data = []\n    \n    for directory in DIRECTORIES:\n        total_size = sum(os.path.getsize(os.path.join(dirpath, filename))\n                         for dirpath, dirnames, filenames in os.walk(directory)\n                         for filename in filenames)\n        data.append([directory, total_size / (1024 * 1024)])  # Convert to MB\n        \n    df = pd.DataFrame(data, columns=['Directory', 'Size (MB)'])\n    \n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, shutil, glob, zipfile\n```python\nimport os\nimport shutil\nimport glob\nfrom zipfile import ZipFile\n\n# Constants\nSOURCE_DIR = '/path/to/source/directory'\nDESTINATION_DIR = '/path/to/destination/directory'\nFILE_EXTENSION = '.txt'\n\ndef move_and_zip_files():\n    \"\"\"\n    Move all files with a certain file extension from a source directory to a \n    destination directory and then zip all the files in the destination directory.\n    \n    Parameters:\n    None.\n    \n    Returns:\n    str: The path of the zip file.\n    \n    Requirements:\n    - os\n    - shutil\n    - glob\n    - zipfile\n    \n    Example:\n    >>> zip_file = move_and_zip_files()\n    >>> print(f\"Zipped files at: {zip_file}\")\n    \"\"\"\n    # Move files\n    for file in glob.glob(os.path.join(SOURCE_DIR, f\"*{FILE_EXTENSION}\")):\n        shutil.move(file, DESTINATION_DIR)\n        \n    # Zip files\n    with ZipFile(os.path.join(DESTINATION_DIR, 'files.zip'), 'w') as zipf:\n        for file in glob.glob(os.path.join(DESTINATION_DIR, f\"*{FILE_EXTENSION}\")):\n            zipf.write(file, os.path.basename(file))\n            \n    return os.path.join(DESTINATION_DIR, 'files.zip')\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, stat, time, datetime\n```python\nimport os\nimport stat\nimport time\nfrom datetime import datetime, timedelta\n\n# Constants\nDIRECTORY = '/path/to/directory'\nDAYS = 30\n\ndef delete_old_files():\n    \"\"\"\n    Delete files in a certain directory that have not been modified for a certain \n    number of days.\n    \n    Parameters:\n    None.\n    \n    Returns:\n    int: The number of deleted files.\n    \n    Requirements:\n    - os\n    - stat\n    - time\n    - datetime\n    \n    Example:\n    >>> num_deleted = delete_old_files()\n    >>> print(f\"Deleted {num_deleted} files.\")\n    \"\"\"\n    num_deleted = 0\n    cutoff_time = time.time() - DAYS * 24 * 60 * 60\n    \n    for filename in os.listdir(DIRECTORY):\n        file_path = os.path.join(DIRECTORY, filename)\n        if os.path.isfile(file_path):\n            mtime = os.stat(file_path)[stat.ST_MTIME]\n            if mtime < cutoff_time:\n                os.remove(file_path)\n                num_deleted += 1\n                \n    return num_deleted\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, hashlib, json, pathlib\n```python\nimport os\nimport hashlib\nimport json\nfrom pathlib import Path\n\n# Constants\nDIRECTORY = '/path/to/directory'\n\ndef generate_file_hashes():\n    \"\"\"\n    Generate SHA256 hashes for all files in a certain directory and save the \n    hashes in a JSON file.\n    \n    Parameters:\n    None.\n    \n    Returns:\n    str: The path of the JSON file.\n    \n    Requirements:\n    - os\n    - hashlib\n    - json\n    - pathlib.Path\n    \n    Example:\n    >>> json_file = generate_file_hashes()\n    >>> print(f\"Hashes saved at: {json_file}\")\n    \"\"\"\n    hash_dict = {}\n    \n    for root, dirs, files in os.walk(DIRECTORY):\n        for file in files:\n            file_path = Path(root) / file\n            with open(file_path, 'rb') as f:\n                bytes = f.read()  # read entire file as bytes\n                readable_hash = hashlib.sha256(bytes).hexdigest();\n                hash_dict[str(file_path)] = readable_hash\n                \n    # Save to JSON file\n    json_file = Path(DIRECTORY) / 'hashes.json'\n    with open(json_file, 'w') as f:\n        json.dump(hash_dict, f)\n        \n    return str(json_file)\n```"}
{"seed": "{\"task_id\": 7011291, \"prompt\": \"def f_7011291(cursor):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"cursor.fetchone()[0]\", \"test_start\": \"\\nimport sqlite3\\n\\ndef check(candidate):\", \"test\": [\"\\n    conn = sqlite3.connect('main')\\n    cursor = conn.cursor()\\n    cursor.execute(\\\"CREATE TABLE student (name VARCHAR(10))\\\")\\n    cursor.execute(\\\"INSERT INTO student VALUES('abc')\\\")\\n    cursor.execute(\\\"SELECT * FROM student\\\")\\n    assert candidate(cursor) == 'abc'\\n\"], \"entry_point\": \"f_7011291\", \"intent\": \"how to get a single result from a SQLite query from `cursor`\", \"library\": [\"sqlite3\"]}", "generation": "Scenario 1:\nsqlite3, pandas, json, matplotlib.pyplot\n```python\nimport sqlite3\nimport pandas as pd\nimport json\nimport matplotlib.pyplot as plt\n\n# Constants\nDB_NAME = 'main.db'\n\ndef visualize_student_grades(cursor):\n    \"\"\"\n    Fetch all student grades from a SQLite database, convert them into a pandas DataFrame, \n    save it to a json file and display a histogram of the grades.\n\n    Parameters:\n    cursor (Cursor): SQLite cursor object.\n\n    Returns:\n    DataFrame: A pandas DataFrame with student grades.\n\n    Requirements:\n    - sqlite3\n    - pandas\n    - json\n    - matplotlib.pyplot\n\n    Example:\n    >>> conn = sqlite3.connect(DB_NAME)\n    >>> cursor = conn.cursor()\n    >>> visualize_student_grades(cursor)\n    \"\"\"\n    cursor.execute(\"SELECT grade FROM students\")\n    grades = cursor.fetchall()\n\n    grades_df = pd.DataFrame(grades, columns=['Grade'])\n    grades_df.to_json('student_grades.json')\n\n    grades_df.plot(kind='hist', rwidth=0.8)\n\n    return grades_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nsqlite3, csv, os, shutil\n```python\nimport sqlite3\nimport csv\nimport os\nimport shutil\n\n# Constants\nDB_NAME = 'main.db'\nCSV_DIR = 'csv_files'\n\ndef export_tables_to_csv(cursor):\n    \"\"\"\n    Export all tables from a SQLite database to CSV files.\n\n    Parameters:\n    cursor (Cursor): SQLite cursor object.\n\n    Returns:\n    None\n\n    Requirements:\n    - sqlite3\n    - csv\n    - os\n    - shutil\n\n    Example:\n    >>> conn = sqlite3.connect(DB_NAME)\n    >>> cursor = conn.cursor()\n    >>> export_tables_to_csv(cursor)\n    \"\"\"\n    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n    tables = cursor.fetchall()\n\n    if not os.path.exists(CSV_DIR):\n        os.mkdir(CSV_DIR)\n    else:\n        shutil.rmtree(CSV_DIR)\n        os.mkdir(CSV_DIR)\n\n    for table in tables:\n        cursor.execute(f\"SELECT * FROM {table[0]}\")\n        rows = cursor.fetchall()\n\n        with open(f'{CSV_DIR}/{table[0]}.csv', 'w', newline='') as file:\n            writer = csv.writer(file)\n            writer.writerows(rows)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nsqlite3, hashlib, binascii\n```python\nimport sqlite3\nimport hashlib\nimport binascii\n\n# Constants\nDB_NAME = 'main.db'\nSALT = \"somesalt\"\n\ndef hash_passwords(cursor):\n    \"\"\"\n    Hash all passwords in a user table of a SQLite database using SHA256.\n\n    Parameters:\n    cursor (Cursor): SQLite cursor object.\n\n    Returns:\n    None\n\n    Requirements:\n    - sqlite3\n    - hashlib\n    - binascii\n\n    Example:\n    >>> conn = sqlite3.connect(DB_NAME)\n    >>> cursor = conn.cursor()\n    >>> hash_passwords(cursor)\n    \"\"\"\n    cursor.execute(\"SELECT id, password FROM users\")\n    users = cursor.fetchall()\n\n    for user in users:\n        password = user[1].encode('utf-8')\n        salted_password = password + SALT.encode('utf-8')\n        hash_obj = hashlib.sha256(salted_password)\n        hashed_password = binascii.hexlify(hash_obj.digest()).decode('utf-8')\n\n        cursor.execute(f\"UPDATE users SET password = '{hashed_password}' WHERE id = {user[0]}\")\n\n    conn.commit()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nsqlite3, pandas, matplotlib.pyplot\n```python\nimport sqlite3\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nDB_NAME = 'main.db'\n\ndef analyze_sales(cursor):\n    \"\"\"\n    Fetch sales data from a SQLite database, convert it into a pandas DataFrame and \n    display a line plot of sales over time.\n\n    Parameters:\n    cursor (Cursor): SQLite cursor object.\n\n    Returns:\n    DataFrame: A pandas DataFrame with sales data.\n\n    Requirements:\n    - sqlite3\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> conn = sqlite3.connect(DB_NAME)\n    >>> cursor = conn.cursor()\n    >>> analyze_sales(cursor)\n    \"\"\"\n    cursor.execute(\"SELECT date, sales FROM sales_data\")\n    sales_data = cursor.fetchall()\n\n    sales_df = pd.DataFrame(sales_data, columns=['Date', 'Sales'])\n    sales_df['Date'] = pd.to_datetime(sales_df['Date'])\n    sales_df.set_index('Date', inplace=True)\n\n    sales_df.plot(kind='line')\n\n    return sales_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nsqlite3, numpy, matplotlib.pyplot\n```python\nimport sqlite3\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nDB_NAME = 'main.db'\n\ndef visualize_student_scores_distribution(cursor):\n    \"\"\"\n    Fetch student scores from a SQLite database, calculate the histogram and \n    display it using matplotlib.\n\n    Parameters:\n    cursor (Cursor): SQLite cursor object.\n\n    Returns:\n    None\n\n    Requirements:\n    - sqlite3\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> conn = sqlite3.connect(DB_NAME)\n    >>> cursor = conn.cursor()\n    >>> visualize_student_scores_distribution(cursor)\n    \"\"\"\n    cursor.execute(\"SELECT score FROM students\")\n    scores = cursor.fetchall()\n\n    scores = np.array(scores)\n    bins = np.arange(0, 101, 10)\n\n    plt.hist(scores, bins, alpha=0.5)\n    plt.show()\n```"}
{"seed": "{\"task_id\": 6378889, \"prompt\": \"def f_6378889(user_input):\\n\\t\", \"suffix\": \"\\n\\treturn user_list\", \"canonical_solution\": \"user_list = [int(number) for number in user_input.split(',')]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('0') == [0]\\n\", \"\\n    assert candidate('12') == [12]\\n\", \"\\n    assert candidate('12,33,223') == [12, 33, 223]\\n\"], \"entry_point\": \"f_6378889\", \"intent\": \"convert string `user_input` into a list of integers `user_list`\", \"library\": []}", "generation": "Scenario 1: \nnumpy, pandas, datetime, random\n```python\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\nfrom random import randint\n\n# Constants\nITEMS = ['Apple', 'Orange', 'Banana', 'Grapes', 'Watermelon']\n\ndef generate_sales_report(input_string):\n    \"\"\"\n    Convert string `input_string` into a list of integers `sales_list`.\n    Then, generate a sales report for a list of items across various days.\n    \n    Parameters:\n    input_string (str): The string of sales numbers separated by commas.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with sales data for the items.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - datetime\n    - random\n    \n    Example:\n    >>> input_string = '10,20,30,40,50'\n    >>> report = generate_sales_report(input_string)\n    >>> print(report)\n    \"\"\"\n    sales_list = [int(number) for number in input_string.split(',')]\n    report_data = []\n\n    for i, item in enumerate(ITEMS):\n        for j in range(sales_list[i]):\n            date = datetime.now().replace(day=randint(1, 28))\n            report_data.append([item, date])\n\n    report_df = pd.DataFrame(report_data, columns=['Item', 'Sale Date'])\n\n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, pandas, datetime, random\n```python\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\nfrom random import randint\n\n# Constants\nEMPLOYEES = ['John', 'Jane', 'Bill', 'Anna', 'Sam']\n\ndef generate_employee_hours(input_string):\n    \"\"\"\n    Convert string `input_string` into a list of integers `hours_list`.\n    Then, generate a report of hours worked for a list of employees across various days.\n    \n    Parameters:\n    input_string (str): The string of hours worked separated by commas.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with hours worked data for the employees.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - datetime\n    - random\n    \n    Example:\n    >>> input_string = '8,7,9,8,10'\n    >>> report = generate_employee_hours(input_string)\n    >>> print(report)\n    \"\"\"\n    hours_list = [int(number) for number in input_string.split(',')]\n    report_data = []\n\n    for i, employee in enumerate(EMPLOYEES):\n        for j in range(hours_list[i]):\n            date = datetime.now().replace(day=randint(1, 28))\n            report_data.append([employee, date, 1])\n\n    report_df = pd.DataFrame(report_data, columns=['Employee', 'Work Date', 'Hours'])\n\n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, pandas, matplotlib, random\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nPRODUCTS = ['TV', 'Laptop', 'Phone', 'Tablet', 'Headphones']\n\ndef generate_product_sales(input_string):\n    \"\"\"\n    Convert string `input_string` into a list of integers `sales_list`.\n    Then, generate a bar chart of sales for a list of products.\n    \n    Parameters:\n    input_string (str): The string of sales numbers separated by commas.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - random\n    \n    Example:\n    >>> input_string = '10,20,30,40,50'\n    >>> generate_product_sales(input_string)\n    \"\"\"\n    sales_list = [int(number) for number in input_string.split(',')]\n    sales_data = pd.DataFrame({'Product': PRODUCTS, 'Sales': sales_list})\n    sales_data.sort_values('Sales', inplace=True)\n\n    plt.barh(sales_data['Product'], sales_data['Sales'], color=[plt.cm.Paired(randint(0, 9)) for _ in range(5)])\n    plt.xlabel('Sales')\n    plt.title('Product Sales')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, pandas, datetime, random\n```python\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\nfrom random import randint\n\n# Constants\nCOUNTRIES = ['USA', 'UK', 'Germany', 'France', 'Italy']\n\ndef generate_country_population(input_string):\n    \"\"\"\n    Convert string `input_string` into a list of integers `population_list`.\n    Then, generate a report of population for a list of countries.\n    \n    Parameters:\n    input_string (str): The string of population numbers separated by commas.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with population data for the countries.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - datetime\n    - random\n    \n    Example:\n    >>> input_string = '330,66,83,67,60'\n    >>> report = generate_country_population(input_string)\n    >>> print(report)\n    \"\"\"\n    population_list = [int(number) for number in input_string.split(',')]\n    report_data = []\n\n    for i, country in enumerate(COUNTRIES):\n        date = datetime.now().replace(year=datetime.now().year - randint(1, 10))\n        report_data.append([country, date, population_list[i]])\n\n    report_df = pd.DataFrame(report_data, columns=['Country', 'Year', 'Population'])\n\n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, pandas, matplotlib, random\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nCURRENCIES = ['USD', 'EUR', 'GBP', 'JPY', 'AUD']\n\ndef generate_currency_values(input_string):\n    \"\"\"\n    Convert string `input_string` into a list of integers `value_list`.\n    Then, generate a pie chart of values for a list of currencies.\n    \n    Parameters:\n    input_string (str): The string of currency values separated by commas.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - random\n    \n    Example:\n    >>> input_string = '1.0,0.92,0.77,108.63,1.38'\n    >>> generate_currency_values(input_string)\n    \"\"\"\n    value_list = [float(number) for number in input_string.split(',')]\n    value_data = pd.DataFrame({'Currency': CURRENCIES, 'Value': value_list})\n\n    plt.pie(value_data['Value'], labels=value_data['Currency'], autopct='%1.1f%%',\n            colors=[plt.cm.Paired(randint(0, 9)) for _ in range(5)])\n    plt.title('Currency Values')\n    plt.show()\n```"}
{"seed": "{\"task_id\": 6378889, \"prompt\": \"def f_6378889(user):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"[int(s) for s in user.split(',')]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('0') == [0]\\n\", \"\\n    assert candidate('12') == [12]\\n\", \"\\n    assert candidate('12,33,223') == [12, 33, 223]\\n\"], \"entry_point\": \"f_6378889\", \"intent\": \"Get a list of integers by splitting  a string `user` with comma\", \"library\": []}", "generation": "Scenario 1:\nnumpy, pandas, matplotlib, seaborn, sklearn\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\n\ndef preprocess_and_visualize(data):\n    \"\"\"\n    Preprocess a DataFrame by converting string numbers into integers,\n    scaling numerical columns, and visualizing correlation matrix.\n    \n    Parameters:\n    data (DataFrame): The DataFrame to preprocess.\n    \n    Returns:\n    DataFrame: Preprocessed DataFrame.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - seaborn\n    - sklearn.preprocessing.StandardScaler\n\n    Example:\n\n    >>> data = pd.DataFrame({\n    ...     'A': ['1,2,3', '4,5,6', '7,8,9'],\n    ...     'B': ['10,11,12', '13,14,15', '16,17,18'],\n    ...     'C': ['19,20,21', '22,23,24', '25,26,27']\n    ... })\n    >>> preprocess_and_visualize(data)\n    \"\"\"\n    # Convert string numbers into integers\n    for col in data.columns:\n        data[col] = data[col].apply(lambda x: [int(s) for s in x.split(',')])\n\n    # Scaling numerical columns\n    scaler = StandardScaler()\n    data = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n\n    # Visualizing correlation matrix\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(data.corr(), annot=True, cmap='coolwarm')\n    plt.show()\n\n    return data\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, random, matplotlib, seaborn, numpy\n```python\nimport pandas as pd\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\ndef random_distribution_visualization(data, column):\n    \"\"\"\n    Convert a string column in a DataFrame into a list of integers,\n    generate a random distribution from these integers, and visualize this distribution.\n\n    Parameters:\n    data (DataFrame): The DataFrame.\n    column (str): The column containing the strings to convert.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - random\n    - matplotlib.pyplot\n    - seaborn\n    - numpy\n\n    Example:\n\n    >>> data = pd.DataFrame({\n    ...     'numbers': ['1,2,3,4,5', '6,7,8,9,10', '11,12,13,14,15']\n    ... })\n    >>> random_distribution_visualization(data, 'numbers')\n    \"\"\"\n    # Convert string numbers into integers\n    data[column] = data[column].apply(lambda x: [int(s) for s in x.split(',')])\n\n    # Generate random distribution\n    distribution = []\n    for row in data[column]:\n        distribution.extend(np.random.choice(row, size=1000))\n\n    # Visualize distribution\n    plt.figure(figsize=(10, 7))\n    sns.distplot(distribution, bins=30)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, pandas, matplotlib, seaborn, sklearn\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\n\ndef pca_visualization(data):\n    \"\"\"\n    Convert a string column in a DataFrame into a list of integers, \n    perform Principal Component Analysis (PCA) on the DataFrame, and visualize the results.\n\n    Parameters:\n    data (DataFrame): The DataFrame.\n    \n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - seaborn\n    - sklearn.decomposition.PCA\n\n    Example:\n\n    >>> data = pd.DataFrame({\n    ...     'numbers': ['1,2,3,4,5', '6,7,8,9,10', '11,12,13,14,15']\n    ... })\n    >>> pca_visualization(data)\n    \"\"\"\n    # Convert string numbers into integers\n    for col in data.columns:\n        data[col] = data[col].apply(lambda x: [int(s) for s in x.split(',')])\n\n    # Perform PCA\n    pca = PCA(n_components=2)\n    pca_result = pca.fit_transform(data)\n\n    # Visualize results\n    plt.figure(figsize=(10, 7))\n    plt.scatter(pca_result[:, 0], pca_result[:, 1])\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, matplotlib, seaborn, sklearn\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.cluster import KMeans\n\ndef cluster_visualization(data):\n    \"\"\"\n    Convert a string column in a DataFrame into a list of integers,\n    perform K-Means clustering on the DataFrame, and visualize the results.\n\n    Parameters:\n    data (DataFrame): The DataFrame.\n    \n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - seaborn\n    - sklearn.cluster.KMeans\n\n    Example:\n\n    >>> data = pd.DataFrame({\n    ...     'numbers': ['1,2,3,4,5', '6,7,8,9,10', '11,12,13,14,15']\n    ... })\n    >>> cluster_visualization(data)\n    \"\"\"\n    # Convert string numbers into integers\n    for col in data.columns:\n        data[col] = data[col].apply(lambda x: [int(s) for s in x.split(',')])\n\n    # Perform K-Means clustering\n    kmeans = KMeans(n_clusters=3)\n    kmeans.fit(data)\n\n    # Visualize results\n    data['cluster'] = kmeans.labels_\n    plt.figure(figsize=(10, 7))\n    sns.scatterplot(data=data, x='numbers', y='numbers', hue='cluster')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, matplotlib, seaborn, sklearn\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef scale_and_visualize(data):\n    \"\"\"\n    Convert a string column in a DataFrame into a list of integers,\n    scale the data using MinMaxScaler, and visualize the results.\n\n    Parameters:\n    data (DataFrame): The DataFrame.\n    \n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - seaborn\n    - sklearn.preprocessing.MinMaxScaler\n\n    Example:\n\n    >>> data = pd.DataFrame({\n    ...     'numbers': ['1,2,3,4,5', '6,7,8,9,10', '11,12,13,14,15']\n    ... })\n    >>> scale_and_visualize(data)\n    \"\"\"\n    # Convert string numbers into integers\n    for col in data.columns:\n        data[col] = data[col].apply(lambda x: [int(s) for s in x.split(',')])\n\n    # Scale data\n    scaler = MinMaxScaler()\n    data_scaled = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n\n    # Visualize results\n    plt.figure(figsize=(10, 7))\n    sns.boxplot(data=data_scaled)\n    plt.show()\n```\n"}
{"seed": "{\"task_id\": 5212870, \"prompt\": \"def f_5212870(list):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"sorted(list, key=lambda x: (x[0], -x[1]))\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    list = [(9, 0), (9, 1), (9, -1), (8, 5), (4, 5)]\\n    assert candidate(list) == [(4, 5), (8, 5), (9, 1), (9, 0), (9, -1)]\\n\"], \"entry_point\": \"f_5212870\", \"intent\": \"Sorting a Python list `list` by the first item ascending and last item descending\", \"library\": []}", "generation": "Scenario 1: \npandas, matplotlib, itertools, random\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom itertools import permutations\nfrom random import randint\n\n# Constants\nPRODUCTS = ['Product A', 'Product B', 'Product C', 'Product D', 'Product E']\nPRICES = [10, 15, 20, 25, 30]\n\ndef generate_sales_report():\n    \"\"\"\n    Generate a sales report of different products, sort the products by their \n    total sales in ascending order and individual sales in descending order.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with total and individual sales for the products.\n    \n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - itertools\n    - random\n    \n    Example:\n    >>> report = generate_sales_report()\n    >>> print(report)\n    >>> report.plot(kind='bar')\n    \"\"\"\n    sales_data = []\n\n    for product in PRODUCTS:\n        total_sales = 0\n        for _ in range(12):  # simulate sales for 12 months\n            monthly_sales = randint(1, 100)\n            total_sales += monthly_sales\n            sales_data.append([product, monthly_sales])\n\n        sales_data.append([product, total_sales])\n\n    sales_df = pd.DataFrame(sales_data, columns=['Product', 'Sales'])\n    sales_df = sales_df.sort_values(by=['Product', 'Sales'], key=lambda x: (x[0], -x[1]))\n\n    return sales_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, pandas, json, datetime\n```python\nimport os\nimport pandas as pd\nimport json\nfrom datetime import datetime\n\n# Constants\nFILE_PATH = 'data.json'\n\ndef sort_and_save_data():\n    \"\"\"\n    Load a list of dictionaries from a JSON file, sort the list by the value of \n    'name' in ascending order and 'date' in descending order, then save the sorted \n    list back to the file.\n    \n    Returns:\n    list: The sorted list of dictionaries.\n    \n    Requirements:\n    - os\n    - pandas\n    - json\n    - datetime\n    \n    Example:\n    >>> sorted_data = sort_and_save_data()\n    >>> print(sorted_data)\n    \"\"\"\n    if not os.path.exists(FILE_PATH):\n        return []\n\n    with open(FILE_PATH, 'r') as file:\n        data = json.load(file)\n\n    data_df = pd.DataFrame(data)\n    data_df['date'] = pd.to_datetime(data_df['date'])\n    data_df = data_df.sort_values(by=['name', 'date'], key=lambda x: (x[0], -x[1]))\n\n    sorted_data = data_df.to_dict('records')\n\n    with open(FILE_PATH, 'w') as file:\n        json.dump(sorted_data, file)\n\n    return sorted_data\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, matplotlib.pyplot, random\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nNUMBERS = np.arange(1, 101)\nCOLORS = ['Red', 'Green', 'Blue', 'Yellow', 'Black']\n\ndef plot_sorted_data():\n    \"\"\"\n    Create a list of tuples where the first element is a random number from 1 to \n    100 and the second element is a random color. Sort the list by the number in \n    ascending order and the color in descending order, then plot the sorted data.\n    \n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - random\n    \n    Example:\n    >>> plot_sorted_data()\n    \"\"\"\n    data = [(randint(1, 100), COLORS[randint(0, len(COLORS)-1)]) for _ in range(100)]\n    data.sort(key=lambda x: (x[0], -x[1]))\n\n    x, y = zip(*data)\n\n    plt.scatter(x, y)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ncollections, operator, random\n```python\nfrom collections import Counter\nimport operator\nfrom random import randint\n\n# Constants\nLETTERS = ['A', 'B', 'C', 'D', 'E']\n\ndef sort_counter():\n    \"\"\"\n    Generate a Counter of random letters, sort the Counter by the letter in \n    ascending order and the count in descending order, then return the sorted Counter.\n    \n    Returns:\n    Counter: The sorted Counter.\n    \n    Requirements:\n    - collections\n    - operator\n    - random\n    \n    Example:\n    >>> sorted_counter = sort_counter()\n    >>> print(sorted_counter)\n    \"\"\"\n    counter = Counter([LETTERS[randint(0, len(LETTERS)-1)] for _ in range(1000)])\n    sorted_counter = sorted(counter.items(), key=operator.itemgetter(0, 1), reverse=True)\n\n    return Counter(dict(sorted_counter))\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, shutil, glob\n```python\nimport os\nimport shutil\nfrom glob import glob\n\n# Constants\nSRC_DIR = './src'\nDST_DIR = './dst'\nEXTENSIONS = ['.txt', '.csv', '.xlsx', '.json']\n\ndef sort_and_move_files():\n    \"\"\"\n    Sort all files in a source directory by their extension in ascending order and \n    filename in descending order, then move the sorted files to a destination directory.\n    \n    Requirements:\n    - os\n    - shutil\n    - glob\n    \n    Example:\n    >>> sort_and_move_files()\n    \"\"\"\n    if not os.path.exists(DST_DIR):\n        os.mkdir(DST_DIR)\n\n    files = glob(os.path.join(SRC_DIR, '*'))\n    files.sort(key=lambda x: (os.path.splitext(x)[1], -os.path.basename(x)))\n\n    for file in files:\n        if os.path.splitext(file)[1] in EXTENSIONS:\n            shutil.move(file, DST_DIR)\n```\n"}
{"seed": "{\"task_id\": 403421, \"prompt\": \"def f_403421(ut, cmpfun):\\n\\t\", \"suffix\": \"\\n\\treturn ut\", \"canonical_solution\": \"ut.sort(key=cmpfun, reverse=True)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([], lambda x: x) == []\\n\", \"\\n    assert candidate(['a', 'b', 'c'], lambda x: x) == ['c', 'b', 'a']\\n\", \"\\n    assert candidate([2, 1, 3], lambda x: -x) == [1, 2, 3]\\n\"], \"entry_point\": \"f_403421\", \"intent\": \"sort a list of objects `ut`, based on a function `cmpfun` in descending order\", \"library\": []}", "generation": "Scenario 1: \npandas, pytz, datetime, random, matplotlib\n```python\nimport pandas as pd\nimport pytz\nfrom datetime import datetime\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nCITIES = ['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']\nWEATHER_CONDITIONS = ['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy']\n\n# Time zones for the cities\nTIMEZONES = {\n    'New York': 'America/New_York',\n    'London': 'Europe/London',\n    'Beijing': 'Asia/Shanghai',\n    'Tokyo': 'Asia/Tokyo',\n    'Sydney': 'Australia/Sydney'\n}\n\ndef sort_weather_report(utc_datetime, cmpfun):\n    \"\"\"\n    Generate a report of weather conditions for a list of cities across various \n    time zones at a given time (UTC) and sort it based on a custom function.\n    \n    Parameters:\n    utc_datetime (datetime): The datetime in UTC.\n    cmpfun (func): The custom function to sort the DataFrame.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with weather conditions for the cities sorted based on cmpfun.\n    \n    Requirements:\n    - pandas\n    - pytz\n    - datetime\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> utc_time = datetime(2023, 6, 15, 12, 0, 0, tzinfo=pytz.UTC)\n    >>> report = sort_weather_report(utc_time, lambda x: x['City'])\n    >>> print(report)\n    >>> report['Weather Condition'].value_counts().plot(kind='bar')\n    \"\"\"\n    report_data = []\n\n    for city in CITIES:\n        city_tz = pytz.timezone(TIMEZONES[city])\n        city_time = utc_datetime.astimezone(city_tz)\n        weather = WEATHER_CONDITIONS[randint(0, len(WEATHER_CONDITIONS)-1)]\n        report_data.append([city, city_time, weather])\n\n    report_df = pd.DataFrame(report_data, columns=['City', 'Local Time', 'Weather Condition'])\n    report_df.sort_values(by=cmpfun, ascending=False, inplace=True)\n\n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npytz, datetime, numpy, dateutil\n```python\nimport pytz\nfrom datetime import datetime\nimport numpy as np\nfrom dateutil.parser import parse\n\n# Constants\nLEAP_SECONDS = np.array([1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980,\n                         1981, 1982, 1983, 1985, 1988, 1990, 1993, 1994, 1997,\n                         1999, 2006, 2009, 2012, 2015, 2016, 2020])\n\ndef total_seconds_since_date(date_str, from_tz, to_tz, cmpfun):\n    \"\"\"\n    Calculate the total seconds that have passed since a given datetime from the current time \n    in different timezones considering the leap seconds. These dates are then sorted based on a custom function.\n\n    Parameters:\n    date_str (str): The date string in \"yyyy-mm-dd hh:mm:ss\" format.\n    from_tz (str): The timezone of the given date string.\n    to_tz (str): The timezone to which the current time should be converted.\n    cmpfun (func): The custom function to sort the dates.\n\n    Returns:\n    int: The total seconds.\n\n    Requirements:\n    - datetime\n    - pytz\n    - numpy\n    - dateutil.parser\n\n    Example:\n    >>> total_seconds_since_date('1970-01-01 00:00:00', 'UTC', 'America/New_York', lambda x: -x)\n    \"\"\"\n    from_tz = pytz.timezone(from_tz)\n    to_tz = pytz.timezone(to_tz)\n    given_date = parse(date_str).replace(tzinfo=from_tz)\n    current_date = datetime.now().astimezone(to_tz)\n\n    total_seconds = (current_date - given_date).total_seconds()\n\n    leap_years = LEAP_SECONDS[np.logical_and(LEAP_SECONDS >= given_date.year, LEAP_SECONDS <= current_date.year)]\n    leap_seconds = len(leap_years)\n\n    total_seconds += leap_seconds\n\n    dates = [given_date, current_date]\n    dates.sort(key=cmpfun, reverse=True)\n\n    return int(total_seconds), dates\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nmatplotlib, numpy, scipy.stats, random\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy import stats\nfrom random import randint\n\n# Constants\nPOPULATION = 1000\nSAMPLE_SIZE = 100\nRANGE = 100\n\ndef generate_and_sort_population(cmpfun):\n    \"\"\"\n    Generate a population of random integers, take a sample, calculate the sample mean and\n    standard deviation, and sort the sample based on a custom function.\n\n    Parameters:\n    cmpfun (func): The custom function to sort the sample.\n\n    Returns:\n    list: The sorted sample.\n    float: The sample mean.\n    float: The sample standard deviation.\n\n    Requirements:\n    - matplotlib.pyplot\n    - numpy\n    - scipy.stats\n    - random\n\n    Example:\n    >>> sample, mean, std_dev = generate_and_sort_population(lambda x: x)\n    >>> print(sample)\n    >>> print(mean)\n    >>> print(std_dev)\n    >>> plt.hist(sample, bins='auto')\n    \"\"\"\n    population = [randint(1, RANGE) for _ in range(POPULATION)]\n    sample = np.random.choice(population, SAMPLE_SIZE)\n    sample.sort()\n    mean = np.mean(sample)\n    std_dev = np.std(sample, ddof=1)\n\n    sorted_sample = sorted(sample, key=cmpfun, reverse=True)\n\n    return sorted_sample, mean, std_dev\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, sklearn.model_selection, sklearn.linear_model, sklearn.metrics\n```python\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n# Sample data\n# df = pd.read_csv('data.csv')\n\ndef train_and_evaluate_model(df, cmpfun):\n    \"\"\"\n    Train a linear regression model on a dataset, evaluate it, and sort the features based on a custom function.\n\n    Parameters:\n    df (DataFrame): The dataset.\n    cmpfun (func): The custom function to sort the features.\n\n    Returns:\n    LinearRegression: The trained model.\n    float: The mean squared error of the model.\n    list: The sorted features.\n\n    Requirements:\n    - pandas\n    - sklearn.model_selection\n    - sklearn.linear_model\n    - sklearn.metrics\n\n    Example:\n    >>> model, mse, features = train_and_evaluate_model(df, lambda x: x)\n    >>> print(model)\n    >>> print(mse)\n    >>> print(features)\n    \"\"\"\n    X = df.drop('target', axis=1)\n    y = df['target']\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    predictions = model.predict(X_test)\n    mse = mean_squared_error(y_test, predictions)\n\n    features = list(df.columns)\n    features.sort(key=cmpfun, reverse=True)\n\n    return model, mse, features\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, matplotlib, scipy.optimize\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import curve_fit\n\n# Constants\nX_RANGE = np.linspace(0, 10, 100)\n\ndef fit_and_sort_curve(cmpfun):\n    \"\"\"\n    Generate a curve based on a quadratic function with noise, fit a quadratic function to it, plot the original and fitted curves, \n    and sort the x values based on a custom function.\n\n    Parameters:\n    cmpfun (func): The custom function to sort the x values.\n\n    Returns:\n    list: The sorted x values.\n    ndarray: The y values of the original curve.\n    ndarray: The y values of the fitted curve.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - scipy.optimize\n\n    Example:\n    >>> x, y_original, y_fitted = fit_and_sort_curve(lambda x: x)\n    >>> plt.scatter(x, y_original, label='Original')\n    >>> plt.plot(x, y_fitted, color='red', label='Fitted')\n    >>> plt.legend()\n    \"\"\"\n    noise = np.random.normal(size=X_RANGE.size)\n    y = X_RANGE**2 + noise\n\n    def quad(x, a, b, c):\n        return a*x**2 + b*x + c\n\n    popt, _ = curve_fit(quad, X_RANGE, y)\n\n    y_fitted = quad(X_RANGE, *popt)\n\n    x_sorted = sorted(X_RANGE, key=cmpfun, reverse=True)\n\n    return x_sorted, y, y_fitted\n```\n"}
{"seed": "{\"task_id\": 403421, \"prompt\": \"def f_403421(ut):\\n\\t\", \"suffix\": \"\\n\\treturn ut\", \"canonical_solution\": \"ut.sort(key=lambda x: x.count, reverse=True)\", \"test_start\": \"\\nclass Tag: \\n    def __init__(self, name, count): \\n        self.name = name \\n        self.count = count \\n\\n    def __str__(self):\\n        return f\\\"[{self.name}]-[{self.count}]\\\"\\n\\ndef check(candidate):\", \"test\": [\"\\n    result = candidate([Tag(\\\"red\\\", 1), Tag(\\\"blue\\\", 22), Tag(\\\"black\\\", 0)])\\n    assert (result[0].name == \\\"blue\\\") and (result[0].count == 22)\\n    assert (result[1].name == \\\"red\\\") and (result[1].count == 1)\\n    assert (result[2].name == \\\"black\\\") and (result[2].count == 0)\\n\"], \"entry_point\": \"f_403421\", \"intent\": \"reverse list `ut` based on the `count` attribute of each object\", \"library\": []}", "generation": "Scenario 1:\npandas, matplotlib, random, numpy\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import choice\nimport numpy as np\n\n# Constants\nCATEGORIES = ['red', 'blue', 'black', 'green', 'yellow']\nSALES_RANGE = range(50, 500)\n\nclass Product:\n    def __init__(self, name, category, sales):\n        self.name = name\n        self.category = category\n        self.sales = sales\n\ndef analyze_sales(products):\n    \"\"\"\n    Analyze the sales of a list of products by category. The function generates a data frame\n    of sales data, calculates summary statistics, and plots a bar chart of the sales by category.\n\n    Parameters:\n    products (list): A list of Product objects.\n\n    Returns:\n    DataFrame: A pandas DataFrame with sales data by category.\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - random\n    - numpy\n\n    Example:\n    >>> products = [Product(f'Product_{i}', choice(CATEGORIES), choice(SALES_RANGE)) for i in range(1000)]\n    >>> df = analyze_sales(products)\n    >>> print(df.describe())\n    >>> df.plot(kind='bar')\n    \"\"\"\n    sales_data = []\n    for product in products:\n        sales_data.append([product.name, product.category, product.sales])\n\n    df = pd.DataFrame(sales_data, columns=['Product Name', 'Category', 'Sales'])\n    df = df.groupby('Category').sum().sort_values(by='Sales', ascending=False)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, matplotlib, numpy, random\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom random import choice\n\n# Constants\nGRADES = ['A', 'B', 'C', 'D', 'F']\n\nclass Student:\n    def __init__(self, name, grade, score):\n        self.name = name\n        self.grade = grade\n        self.score = score\n\ndef analyze_grades(students):\n    \"\"\"\n    Analyze the grades of a list of students. The function generates a data frame\n    of grades data, calculates summary statistics, and plots a pie chart of the grades distribution.\n\n    Parameters:\n    students (list): A list of Student objects.\n\n    Returns:\n    DataFrame: A pandas DataFrame with grades data.\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - numpy\n    - random\n\n    Example:\n    >>> students = [Student(f'Student_{i}', choice(GRADES), np.random.randint(50, 101)) for i in range(1000)]\n    >>> df = analyze_grades(students)\n    >>> print(df.describe())\n    >>> df['Grade'].value_counts().plot(kind='pie')\n    \"\"\"\n    grades_data = []\n    for student in students:\n        grades_data.append([student.name, student.grade, student.score])\n\n    df = pd.DataFrame(grades_data, columns=['Student Name', 'Grade', 'Score'])\n    df = df.groupby('Grade').count().sort_values(by='Score', ascending=False)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, matplotlib, random\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nPOPULATION_RANGE = range(1000, 10000)\nCITIES = ['City_A', 'City_B', 'City_C', 'City_D', 'City_E']\n\nclass City:\n    def __init__(self, name, population):\n        self.name = name\n        self.population = population\n\ndef analyze_population(cities):\n    \"\"\"\n    Analyze the population of a list of cities. The function generates a data frame\n    of population data, calculates summary statistics, and plots a histogram of the population distribution.\n\n    Parameters:\n    cities (list): A list of City objects.\n\n    Returns:\n    DataFrame: A pandas DataFrame with population data.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> cities = [City(name, randint(POPULATION_RANGE[0], POPULATION_RANGE[-1])) for name in CITIES]\n    >>> df = analyze_population(cities)\n    >>> print(df.describe())\n    >>> df['Population'].hist(bins=50)\n    \"\"\"\n    population_data = []\n    for city in cities:\n        population_data.append([city.name, city.population])\n\n    df = pd.DataFrame(population_data, columns=['City Name', 'Population'])\n    df = df.sort_values(by='Population', ascending=False)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, matplotlib, random\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nAGES_RANGE = range(18, 65)\nPROFESSIONS = ['Teacher', 'Engineer', 'Doctor', 'Artist', 'Writer']\n\nclass Person:\n    def __init__(self, name, age, profession):\n        self.name = name\n        self.age = age\n        self.profession = profession\n\ndef analyze_age_distribution(people):\n    \"\"\"\n    Analyze the age distribution of a list of people. The function generates a data frame\n    of age data, calculates summary statistics, and plots a boxplot of the age distribution.\n\n    Parameters:\n    people (list): A list of Person objects.\n\n    Returns:\n    DataFrame: A pandas DataFrame with age data.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> people = [Person(f'Person_{i}', randint(AGES_RANGE[0], AGES_RANGE[-1]), choice(PROFESSIONS)) for i in range(1000)]\n    >>> df = analyze_age_distribution(people)\n    >>> print(df.describe())\n    >>> df.boxplot(column='Age', by='Profession')\n    \"\"\"\n    age_data = []\n    for person in people:\n        age_data.append([person.name, person.age, person.profession])\n\n    df = pd.DataFrame(age_data, columns=['Person Name', 'Age', 'Profession'])\n    df = df.sort_values(by='Age', ascending=True)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, matplotlib, random\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nTEMP_RANGE = range(-10, 40)\nMONTHS = ['January', 'February', 'March', 'April', 'May', 'June', \n          'July', 'August', 'September', 'October', 'November', 'December']\n\nclass Month:\n    def __init__(self, name, temperature):\n        self.name = name\n        self.temperature = temperature\n\ndef analyze_temperature(months):\n    \"\"\"\n    Analyze the temperature of a list of months. The function generates a data frame\n    of temperature data, calculates summary statistics, and plots a line graph of the temperature distribution.\n\n    Parameters:\n    months (list): A list of Month objects.\n\n    Returns:\n    DataFrame: A pandas DataFrame with temperature data.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> months = [Month(name, randint(TEMP_RANGE[0], TEMP_RANGE[-1])) for name in MONTHS]\n    >>> df = analyze_temperature(months)\n    >>> print(df.describe())\n    >>> df.plot(kind='line')\n    \"\"\"\n    temp_data = []\n    for month in months:\n        temp_data.append([month.name, month.temperature])\n\n    df = pd.DataFrame(temp_data, columns=['Month', 'Temperature'])\n    df = df.sort_values(by='Month', ascending=True)\n\n    return df\n```\n"}
{"seed": "{\"task_id\": 3944876, \"prompt\": \"def f_3944876(i):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"'ME' + str(i)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(100) == \\\"ME100\\\"\\n\", \"\\n    assert candidate(0.22) == \\\"ME0.22\\\"\\n\", \"\\n    assert candidate(\\\"text\\\") == \\\"MEtext\\\"\\n\"], \"entry_point\": \"f_3944876\", \"intent\": \"cast an int `i` to a string and concat to string 'ME'\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, matplotlib, random\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nCATEGORIES = ['A', 'B', 'C', 'D', 'E']\nPREFIX = 'ME'\n\ndef generate_data_frame(n):\n    \"\"\"\n    Generate a pandas DataFrame with `n` rows. Each row contains a category from a predefined list \n    and an id that is a concatenation of a string 'ME' and a random integer.\n\n    Parameters:\n    n (int): The number of rows to generate.\n\n    Returns:\n    DataFrame: A pandas DataFrame with generated data.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> df = generate_data_frame(10)\n    >>> print(df)\n    >>> df['Category'].value_counts().plot(kind='bar')\n    \"\"\"\n    data = []\n\n    for _ in range(n):\n        category = CATEGORIES[randint(0, len(CATEGORIES)-1)]\n        id_ = PREFIX + str(randint(10000, 99999))\n        data.append([category, id_])\n\n    df = pd.DataFrame(data, columns=['Category', 'ID'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nos, hashlib, base64\n```python\nimport os\nimport hashlib\nimport base64\n\n# Constants\nPREFIX = 'ME'\nSALT_LENGTH = 16\n\ndef generate_hashed_password(password):\n    \"\"\"\n    Generate a hashed password from a given string by concatenating it with a prefix 'ME', \n    salting and hashing it using SHA256.\n\n    Parameters:\n    password (str): The password string.\n\n    Returns:\n    str: The hashed password.\n    \n    Requirements:\n    - os\n    - hashlib\n    - base64\n\n    Example:\n    >>> hashed_password = generate_hashed_password('password')\n    >>> print(hashed_password)\n    \"\"\"\n    salt = os.urandom(SALT_LENGTH)\n    salted_password = PREFIX + password + salt.hex()\n    \n    hashed_password = hashlib.sha256(salted_password.encode()).digest()\n\n    return base64.b64encode(hashed_password).decode()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \npandas, sklearn.preprocessing, sklearn.model_selection\n```python\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\n# Constants\nPREFIX = 'ME'\nTEST_SIZE = 0.2\nRANDOM_STATE = 42\n\ndef prepare_dataset(df):\n    \"\"\"\n    Prepare a pandas DataFrame for machine learning by encoding categorical features \n    and splitting it into a train and test sets.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n\n    Returns:\n    DataFrame, DataFrame: The train and test DataFrames.\n    \n    Requirements:\n    - pandas\n    - sklearn.preprocessing\n    - sklearn.model_selection\n\n    Example:\n    >>> df = pd.read_csv('some_file.csv')\n    >>> train_df, test_df = prepare_dataset(df)\n    >>> print(train_df)\n    >>> print(test_df)\n    \"\"\"\n    le = LabelEncoder()\n\n    for col in df.columns:\n        if df[col].dtype == 'object':\n            df[col] = le.fit_transform(df[col])\n            df[col] = PREFIX + df[col].astype(str)\n\n    train_df, test_df = train_test_split(df, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n\n    return train_df, test_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, scipy.stats, matplotlib\n```python\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\n# Constants\nPREFIX = 'ME'\nSIZE = 1000\nMU = 0\nSIGMA = 1\n\ndef generate_normal_distribution_graph():\n    \"\"\"\n    Generate a graph of normal distribution by creating a random normal distribution \n    with mean 0 and standard deviation 1 of size 1000, then plot it using matplotlib.\n\n    Parameters:\n    None\n\n    Returns:\n    None\n    \n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Example:\n    >>> generate_normal_distribution_graph()\n    \"\"\"\n    data = np.random.normal(MU, SIGMA, SIZE)\n\n    plt.hist(data, bins=30, density=True, alpha=0.6, color='g')\n\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, MU, SIGMA)\n    plt.plot(x, p, 'k', linewidth=2)\n    title = PREFIX + \" Normal Distribution Graph\"\n    plt.title(title)\n\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \njson, requests, os\n```python\nimport json\nimport requests\nimport os\n\n# Constants\nPREFIX = 'ME'\nAPI_URL = 'https://some-api-url.com/'\n\ndef fetch_and_write_json(endpoint):\n    \"\"\"\n    Fetch JSON data from an API endpoint and write it to a file with a filename that \n    is a concatenation of a prefix 'ME' and the endpoint.\n\n    Parameters:\n    endpoint (str): The API endpoint.\n\n    Returns:\n    str: The filename that the JSON data was written to.\n    \n    Requirements:\n    - json\n    - requests\n    - os\n\n    Example:\n    >>> filename = fetch_and_write_json('users')\n    >>> print(filename)\n    \"\"\"\n    response = requests.get(API_URL + endpoint)\n    data = response.json()\n\n    filename = PREFIX + endpoint + '.json'\n    with open(filename, 'w') as f:\n        json.dump(data, f)\n\n    return filename\n```\n"}
{"seed": "{\"task_id\": 40903174, \"prompt\": \"def f_40903174(df):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"df.sort_values(['System_num', 'Dis'])\", \"test_start\": \"\\nimport pandas as pd \\n\\ndef check(candidate):\", \"test\": [\"\\n    df1 = pd.DataFrame([[6, 1, 1], [5, 1, 1], [4, 1, 1], [3, 2, 1], [2, 2, 1], [1, 2, 1]], columns = ['Dis', 'System_num', 'Energy'])\\n    df_ans1 = pd.DataFrame([[4, 1, 1], [5, 1, 1], [6, 1, 1], [1, 2, 1], [2, 2, 1], [3, 2, 1]], columns = ['Dis', 'System_num', 'Energy'])\\n    assert (df_ans1.equals(candidate(df1).reset_index(drop = True))) == True\\n\", \"\\n    df2 = pd.DataFrame([[6, 3, 1], [5, 2, 1], [4, 1, 1]], columns = ['Dis', 'System_num', 'Energy'])\\n    df_ans2 = pd.DataFrame([[4, 1, 1], [5, 2, 1], [6, 3, 1]], columns = ['Dis', 'System_num', 'Energy'])\\n    assert (df_ans2.equals(candidate(df2).reset_index(drop = True))) == True\\n\", \"\\n    df3 = pd.DataFrame([[1, 3, 1], [3, 3, 1], [2, 3, 1], [6, 1, 1], [4, 1, 1], [5, 2, 1], [3, 2, 1]], columns = ['Dis', 'System_num', 'Energy'])\\n    df_ans3 = pd.DataFrame([[4, 1,1], [6, 1, 1], [3, 2, 1], [5, 2, 1], [1, 3, 1], [2, 3, 1], [3, 3, 1]], columns = ['Dis', 'System_num', 'Energy'])\\n    assert (df_ans3.equals(candidate(df3).reset_index(drop = True))) == True \\n\", \"\\n    df4 = pd.DataFrame([[1, 2, 3], [1, 2, 3], [4, 1, 3]], columns = ['Dis', 'System_num', 'Energy'])\\n    df_ans4 = pd.DataFrame([[1, 2, 3], [1, 2, 3], [4, 1, 3]])\\n    assert (df_ans4.equals(candidate(df4).reset_index(drop = True))) == False\\n\"], \"entry_point\": \"f_40903174\", \"intent\": \"Sorting data in Pandas DataFrame `df` with columns 'System_num' and 'Dis'\", \"library\": [\"pandas\"]}", "generation": "Scenario 1:\npandas, numpy, matplotlib, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nCOLUMNS = ['System_num', 'Dis', 'Energy']\n\ndef analyze_energy_distribution(df):\n    \"\"\"\n    Analyze the energy distribution for each system by sorting data in the DataFrame `df` \n    with columns 'System_num' and 'Dis', and then plot the energy distribution for each system.\n\n    Parameters:\n    df (DataFrame): The input DataFrame.\n\n    Returns:\n    Seaborn plot: The energy distribution plot.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> df = pd.DataFrame(np.random.randint(0,100,size=(100, 3)), columns=COLUMNS)\n    >>> analyze_energy_distribution(df)\n    \"\"\"\n    df_sorted = df.sort_values(COLUMNS)\n    plt.figure(figsize=(10,6))\n    sns.boxplot(x='System_num', y='Energy', data=df_sorted)\n    plt.title('Energy Distribution for Each System')\n    plt.xlabel('System Number')\n    plt.ylabel('Energy')\n    plt.grid(True)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, matplotlib, sklearn\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Constants\nCOLUMNS = ['System_num', 'Dis', 'Energy']\n\ndef normalize_and_plot(df):\n    \"\"\"\n    Normalize the energy for each system by sorting data in the DataFrame `df` \n    with columns 'System_num' and 'Dis', and then plot the normalized energy for each system.\n\n    Parameters:\n    df (DataFrame): The input DataFrame.\n\n    Returns:\n    DataFrame: The normalized DataFrame.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.preprocessing.MinMaxScaler\n\n    Example:\n    >>> df = pd.DataFrame(np.random.randint(0,100,size=(100, 3)), columns=COLUMNS)\n    >>> normalize_and_plot(df)\n    \"\"\"\n    df_sorted = df.sort_values(COLUMNS)\n    scaler = MinMaxScaler()\n    df_sorted['Energy'] = scaler.fit_transform(df_sorted[['Energy']])\n    \n    df_sorted.groupby('System_num')['Energy'].plot(legend=True)\n    plt.title('Normalized Energy for Each System')\n    plt.xlabel('Index')\n    plt.ylabel('Normalized Energy')\n    plt.grid(True)\n    plt.show()\n\n    return df_sorted\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, sklearn, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cluster import KMeans\nimport seaborn as sns\n\n# Constants\nCOLUMNS = ['System_num', 'Dis', 'Energy']\n\ndef cluster_and_visualize(df):\n    \"\"\"\n    Perform K-Means clustering on the DataFrame `df` with columns 'System_num', 'Dis', and 'Energy', \n    and then visualize the clusters using a scatter plot.\n\n    Parameters:\n    df (DataFrame): The input DataFrame.\n\n    Returns:\n    DataFrame: The DataFrame with cluster labels.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.cluster.KMeans\n    - seaborn\n\n    Example:\n    >>> df = pd.DataFrame(np.random.randint(0,100,size=(100, 3)), columns=COLUMNS)\n    >>> cluster_and_visualize(df)\n    \"\"\"\n    kmeans = KMeans(n_clusters=3)\n    df['Cluster'] = kmeans.fit_predict(df[COLUMNS])\n    \n    sns.scatterplot(x='Dis', y='Energy', hue='Cluster', data=df)\n    plt.title('KMeans Clustering of Systems')\n    plt.xlabel('Dis')\n    plt.ylabel('Energy')\n    plt.grid(True)\n    plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, matplotlib, scipy\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import linregress\n\n# Constants\nCOLUMNS = ['System_num', 'Dis', 'Energy']\n\ndef regression_and_plot(df):\n    \"\"\"\n    Perform linear regression on the DataFrame `df` with columns 'Dis' and 'Energy', \n    and then plot the regression line.\n\n    Parameters:\n    df (DataFrame): The input DataFrame.\n\n    Returns:\n    dict: The regression result.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - scipy.stats.linregress\n\n    Example:\n    >>> df = pd.DataFrame(np.random.randint(0,100,size=(100, 3)), columns=COLUMNS)\n    >>> regression_and_plot(df)\n    \"\"\"\n    regression = linregress(df['Dis'], df['Energy'])\n    df.plot(x='Dis', y='Energy', kind='scatter')\n    plt.plot(df['Dis'], regression.intercept + regression.slope * df['Dis'], 'r')\n    plt.title('Linear Regression of Dis and Energy')\n    plt.xlabel('Dis')\n    plt.ylabel('Energy')\n    plt.grid(True)\n    plt.show()\n\n    return {'slope': regression.slope, 'intercept': regression.intercept, 'rvalue': regression.rvalue}\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, matplotlib, sklearn\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n# Constants\nCOLUMNS = ['System_num', 'Dis', 'Energy']\n\ndef predict_energy(df, dis):\n    \"\"\"\n    Train a linear regression model on the DataFrame `df` with columns 'Dis' and 'Energy', \n    and then predict the energy for a given 'Dis'.\n\n    Parameters:\n    df (DataFrame): The input DataFrame.\n    dis (float): The 'Dis' for which to predict the 'Energy'.\n\n    Returns:\n    float: The predicted 'Energy'.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.linear_model.LinearRegression\n\n    Example:\n    >>> df = pd.DataFrame(np.random.randint(0,100,size=(100, 3)), columns=COLUMNS)\n    >>> predict_energy(df, 50)\n    \"\"\"\n    model = LinearRegression()\n    model.fit(df[['Dis']], df['Energy'])\n    \n    plt.scatter(df['Dis'], df['Energy'], color='blue')\n    plt.plot(df['Dis'], model.predict(df[['Dis']]), color='red')\n    plt.title('Energy Prediction')\n    plt.xlabel('Dis')\n    plt.ylabel('Energy')\n    plt.grid(True)\n    plt.show()\n    \n    return model.predict(np.array([[dis]]))[0]\n```\n"}
{"seed": "{\"task_id\": 4454298, \"prompt\": \"def f_4454298(infile, outfile):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"open(outfile, 'w').write('#test firstline\\\\n' + open(infile).read())\", \"test_start\": \"\\nimport filecmp\\n\\ndef check(candidate): \", \"test\": [\"\\n    open('test1.txt', 'w').write('test1')\\n    candidate('test1.txt', 'test1_out.txt')\\n    open('test1_ans.txt', 'w').write('#test firstline\\\\ntest1')\\n    assert filecmp.cmp('test1_out.txt', 'test1_ans.txt') == True\\n\", \"\\n    open('test2.txt', 'w').write('\\\\ntest2\\\\n')\\n    candidate('test2.txt', 'test2_out.txt')\\n    open('test2_ans.txt', 'w').write('#test firstline\\\\n\\\\ntest2\\\\n')\\n    assert filecmp.cmp('test2_out.txt', 'test2_ans.txt') == True\\n\", \"\\n    open('test3.txt', 'w').write(' \\\\n \\\\n')\\n    candidate('test3.txt', 'test3_out.txt')\\n    open('test3_ans.txt', 'w').write('#test firstline\\\\n \\\\n \\\\n')\\n    assert filecmp.cmp('test3_out.txt', 'test3_ans.txt') == True\\n\", \"\\n    open('test4.txt', 'w').write('hello')\\n    candidate('test4.txt', 'test4_out.txt')\\n    open('test4_ans.txt', 'w').write('hello')\\n    assert filecmp.cmp('test4_out.txt', 'test4_ans.txt') == False\\n\"], \"entry_point\": \"f_4454298\", \"intent\": \"prepend the line '#test firstline\\\\n' to the contents of file 'infile' and save as the file 'outfile'\", \"library\": [\"filecmp\"]}", "generation": "Scenario 1:\nos, shutil, glob, filecmp\n```python\nimport os\nimport shutil\nimport glob\nimport filecmp\n\n# Constants\nSOURCE_DIR = 'source'\nTARGET_DIR = 'target'\nPREFIX = '#test firstline\\\\n'\n\ndef prepend_and_copy_files(source_dir=SOURCE_DIR, target_dir=TARGET_DIR, prefix=PREFIX):\n    \"\"\"\n    Prepend a line to the contents of all files in the 'source_dir' directory \n    and save them to the 'target_dir' directory.\n    If a file with the same name already exists in the target directory, it should be overwritten.\n\n    Parameters:\n    source_dir (str): The source directory.\n    target_dir (str): The target directory.\n    prefix (str): The line to prepend.\n\n    Requirements:\n    - os\n    - shutil\n    - glob\n    - filecmp\n\n    Example:\n    >>> prepend_and_copy_files()\n    \"\"\"\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n    for file in glob.glob(source_dir + '/*'):\n        with open(file, 'r') as infile:\n            content = infile.read()\n        with open(os.path.join(target_dir, os.path.basename(file)), 'w') as outfile:\n            outfile.write(prefix + content)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, shutil, glob, filecmp, random\n```python\nimport os\nimport shutil\nimport glob\nimport filecmp\nimport random\n\n# Constants\nSOURCE_DIR = 'source'\nTARGET_DIR = 'target'\nPREFIXES = ['#test firstline\\\\n', '#second test line\\\\n', '#third test line\\\\n']\n\ndef prepend_random_and_copy_files(source_dir=SOURCE_DIR, target_dir=TARGET_DIR, prefixes=PREFIXES):\n    \"\"\"\n    Prepend a line from a list of prefixes randomly chosen to the contents of all files in the \n    'source_dir' directory and save them to the 'target_dir' directory.\n    If a file with the same name already exists in the target directory, it should be overwritten.\n\n    Parameters:\n    source_dir (str): The source directory.\n    target_dir (str): The target directory.\n    prefixes (list): The list of lines to prepend.\n\n    Requirements:\n    - os\n    - shutil\n    - glob\n    - filecmp\n    - random\n\n    Example:\n    >>> prepend_random_and_copy_files()\n    \"\"\"\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n    for file in glob.glob(source_dir + '/*'):\n        with open(file, 'r') as infile:\n            content = infile.read()\n        with open(os.path.join(target_dir, os.path.basename(file)), 'w') as outfile:\n            outfile.write(random.choice(prefixes) + content)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, shutil, glob, filecmp, random\n```python\nimport os\nimport shutil\nimport glob\nimport filecmp\nimport random\n\n# Constants\nSOURCE_DIR = 'source'\nTARGET_DIR = 'target'\nPOSTFIXES = ['\\\\n#test endline', '\\\\n#second test endline', '\\\\n#third test endline']\n\ndef append_random_and_copy_files(source_dir=SOURCE_DIR, target_dir=TARGET_DIR, postfixes=POSTFIXES):\n    \"\"\"\n    Append a line from a list of postfixes randomly chosen to the contents of all files in the \n    'source_dir' directory and save them to the 'target_dir' directory.\n    If a file with the same name already exists in the target directory, it should be overwritten.\n\n    Parameters:\n    source_dir (str): The source directory.\n    target_dir (str): The target directory.\n    postfixes (list): The list of lines to append.\n\n    Requirements:\n    - os\n    - shutil\n    - glob\n    - filecmp\n    - random\n\n    Example:\n    >>> append_random_and_copy_files()\n    \"\"\"\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n    for file in glob.glob(source_dir + '/*'):\n        with open(file, 'r') as infile:\n            content = infile.read()\n        with open(os.path.join(target_dir, os.path.basename(file)), 'w') as outfile:\n            outfile.write(content + random.choice(postfixes))\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, shutil, glob, filecmp, random, time\n```python\nimport os\nimport shutil\nimport glob\nimport filecmp\nimport random\nimport time\n\n# Constants\nSOURCE_DIR = 'source'\nTARGET_DIR = 'target'\nPREFIX = '#Timestamp: '\n\ndef prepend_timestamp_and_copy_files(source_dir=SOURCE_DIR, target_dir=TARGET_DIR, prefix=PREFIX):\n    \"\"\"\n    Prepend a line containing the current timestamp to the contents of all files in the \n    'source_dir' directory and save them to the 'target_dir' directory.\n    If a file with the same name already exists in the target directory, it should be overwritten.\n\n    Parameters:\n    source_dir (str): The source directory.\n    target_dir (str): The target directory.\n    prefix (str): The line to prepend.\n\n    Requirements:\n    - os\n    - shutil\n    - glob\n    - filecmp\n    - random\n    - time\n\n    Example:\n    >>> prepend_timestamp_and_copy_files()\n    \"\"\"\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n    for file in glob.glob(source_dir + '/*'):\n        with open(file, 'r') as infile:\n            content = infile.read()\n        with open(os.path.join(target_dir, os.path.basename(file)), 'w') as outfile:\n            outfile.write(prefix + time.ctime() + '\\\\n' + content)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, shutil, glob, filecmp, hashlib\n```python\nimport os\nimport shutil\nimport glob\nimport filecmp\nimport hashlib\n\n# Constants\nSOURCE_DIR = 'source'\nTARGET_DIR = 'target'\nPREFIX = '#Hash: '\n\ndef prepend_hash_and_copy_files(source_dir=SOURCE_DIR, target_dir=TARGET_DIR, prefix=PREFIX):\n    \"\"\"\n    Prepend a line containing the hash (MD5) of the contents of all files in the \n    'source_dir' directory and save them to the 'target_dir' directory.\n    If a file with the same name already exists in the target directory, it should be overwritten.\n\n    Parameters:\n    source_dir (str): The source directory.\n    target_dir (str): The target directory.\n    prefix (str): The line to prepend.\n\n    Requirements:\n    - os\n    - shutil\n    - glob\n    - filecmp\n    - hashlib\n\n    Example:\n    >>> prepend_hash_and_copy_files()\n    \"\"\"\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n    for file in glob.glob(source_dir + '/*'):\n        with open(file, 'r') as infile:\n            content = infile.read()\n        hash_object = hashlib.md5(content.encode())\n        with open(os.path.join(target_dir, os.path.basename(file)), 'w') as outfile:\n            outfile.write(prefix + hash_object.hexdigest() + '\\\\n' + content)\n```\n"}
{"seed": "{\"task_id\": 19729928, \"prompt\": \"def f_19729928(l):\\n\\t\", \"suffix\": \"\\n\\treturn l\", \"canonical_solution\": \"l.sort(key=lambda t: len(t[1]), reverse=True)\", \"test_start\": \"\\ndef check(candidate): \", \"test\": [\"\\n    assert candidate([(\\\"a\\\", [1]), (\\\"b\\\", [1,2]), (\\\"c\\\", [1,2,3])]) ==         [(\\\"c\\\", [1,2,3]), (\\\"b\\\", [1,2]), (\\\"a\\\", [1])]\\n\", \"\\n    assert candidate([(\\\"a\\\", [1]), (\\\"b\\\", [2]), (\\\"c\\\", [1,2,3])]) ==         [(\\\"c\\\", [1,2,3]), (\\\"a\\\", [1]), (\\\"b\\\", [2])]\\n\", \"\\n    assert candidate([(\\\"a\\\", [1]), (\\\"b\\\", [2]), (\\\"c\\\", [3])]) ==         [(\\\"a\\\", [1]), (\\\"b\\\", [2]), (\\\"c\\\", [3])]\\n\"], \"entry_point\": \"f_19729928\", \"intent\": \"sort a list `l` by length of value in tuple\", \"library\": []}", "generation": "Scenario 1: \npandas, random, os, json, datetime\n```python\nimport pandas as pd\nimport random\nimport os\nimport json\nfrom datetime import datetime\n\n# Constants\nEMPLOYEE_NAMES = ['John', 'Sara', 'Mike', 'Amanda', 'Alex', 'Sophia', 'Lucas', 'Emma']\nPROJECTS = ['Project A', 'Project B', 'Project C', 'Project D', 'Project E', 'Project F']\n\ndef assign_project_to_employee():\n    \"\"\"\n    Generate a json file with employee names and projects assigned to them. \n    The assignment should be random and sorted by the number of projects assigned in descending order.\n    \n    Requirements:\n    - pandas\n    - random\n    - os\n    - json\n    - datetime\n\n    Example:\n    >>> assign_project_to_employee()\n    ... # json file generated with employee project assignments\n    \"\"\"\n    assignments = []\n    for employee in EMPLOYEE_NAMES:\n        project_count = random.randint(1, len(PROJECTS))\n        assigned_projects = random.sample(PROJECTS, project_count)\n        assignments.append((employee, assigned_projects))\n\n    assignments.sort(key=lambda x: len(x[1]), reverse=True)\n\n    df = pd.DataFrame(assignments, columns=['Employee', 'Projects'])\n    json_file = df.to_json(orient='records')\n\n    with open(f'assignments_{datetime.now().strftime(\"%Y%m%d%H%M%S\")}.json', 'w') as file:\n        file.write(json_file)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, scipy.stats, matplotlib.pyplot, random\n```python\nimport numpy as np\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nimport random\n\n# Constants\nSAMPLE_SIZE = 1000\nHIST_BIN_COUNT = 50\nPOPULATION_MEAN = 100\nPOPULATION_STD_DEV = 15\n\ndef generate_and_analyze_data():\n    \"\"\"\n    Generate a normal distribution of data with a given mean and std deviation.\n    Calculate and return the skewness and kurtosis, and plot the histogram and QQ plot.\n    \n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> generate_and_analyze_data()\n    ... # skewness, kurtosis, histogram and QQ plot generated\n    \"\"\"\n    data = np.random.normal(POPULATION_MEAN, POPULATION_STD_DEV, SAMPLE_SIZE)\n    \n    skewness = stats.skew(data)\n    kurtosis = stats.kurtosis(data)\n    \n    plt.figure(figsize=(10,5))\n\n    plt.subplot(1, 2, 1)\n    plt.hist(data, bins=HIST_BIN_COUNT)\n    plt.title('Histogram')\n\n    plt.subplot(1, 2, 2)\n    stats.probplot(data, dist=\"norm\", plot=plt)\n    plt.title('QQ Plot')\n\n    plt.tight_layout()\n    plt.show()\n\n    return skewness, kurtosis\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, sklearn.preprocessing, matplotlib.pyplot\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\n\n# Constants\nDATA = {'Column1': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n        'Column2': [15, 25, 35, 45, 55, 65, 75, 85, 95, 105],\n        'Column3': [20, 30, 40, 50, 60, 70, 80, 90, 100, 110]}\n\ndef preprocess_and_visualize_data():\n    \"\"\"\n    Preprocess the data using MinMaxScaler and visualize the data before and after scaling using matplotlib.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.preprocessing\n    - matplotlib.pyplot\n\n    Example:\n    >>> preprocess_and_visualize_data()\n    ... # visualization of data before and after scaling\n    \"\"\"\n    df = pd.DataFrame(DATA)\n    \n    scaler = MinMaxScaler()\n    scaled_data = scaler.fit_transform(df)\n    scaled_df = pd.DataFrame(scaled_data, columns=df.columns)\n    \n    plt.figure(figsize=(10,5))\n\n    plt.subplot(1, 2, 1)\n    df.boxplot()\n    plt.title('Before Scaling')\n\n    plt.subplot(1, 2, 2)\n    scaled_df.boxplot()\n    plt.title('After Scaling')\n\n    plt.tight_layout()\n    plt.show()\n    \n    return scaled_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, os, glob, json\n```python\nimport pandas as pd\nimport os\nimport glob\nimport json\n\n# Constants\nDIRECTORY_PATH = './data_files/'\n\ndef aggregate_json_files():\n    \"\"\"\n    Aggregate all json files from a given directory into a single pandas DataFrame and sort by a specific column.\n    \n    Requirements:\n    - pandas\n    - os\n    - glob\n    - json\n\n    Example:\n    >>> aggregate_json_files()\n    ... # DataFrame with aggregated data from all json files\n    \"\"\"\n    all_files = glob.glob(os.path.join(DIRECTORY_PATH, '*.json'))\n\n    data_frames = []\n    for file in all_files:\n        with open(file) as json_file:\n            data = json.load(json_file)\n            data_frame = pd.DataFrame(data)\n            data_frames.append(data_frame)\n\n    aggregated_df = pd.concat(data_frames, ignore_index=True)\n    aggregated_df.sort_values(by='Column1', inplace=True)\n    \n    return aggregated_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, matplotlib.pyplot, random, math\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nimport math\n\n# Constants\nRADIUS = 100\nSAMPLE_SIZE = 1000\n\ndef estimate_pi_using_monte_carlo():\n    \"\"\"\n    Estimate the value of Pi using the Monte Carlo method and visualize the points inside and outside of the circle using matplotlib.\n    \n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - random\n    - math\n\n    Example:\n    >>> estimate_pi_using_monte_carlo()\n    ... # Estimated value of Pi and visualization of points\n    \"\"\"\n    inside_circle = 0\n\n    x_inside = []\n    y_inside = []\n    x_outside = []\n    y_outside = []\n\n    for _ in range(0, SAMPLE_SIZE):\n        x = random.uniform(-RADIUS, RADIUS)\n        y = random.uniform(-RADIUS, RADIUS)\n\n        if math.sqrt(x**2 + y**2) <= RADIUS:\n            inside_circle += 1\n            x_inside.append(x)\n            y_inside.append(y)\n        else:\n            x_outside.append(x)\n            y_outside.append(y)\n\n    pi_estimate = 4 * inside_circle / SAMPLE_SIZE\n\n    plt.figure(figsize=(5,5))\n    plt.scatter(x_inside, y_inside, color='b')\n    plt.scatter(x_outside, y_outside, color='r')\n    plt.show()\n\n    return pi_estimate\n```\n"}
{"seed": "{\"task_id\": 31371879, \"prompt\": \"def f_31371879(s):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"re.findall('\\\\\\\\b(\\\\\\\\w+)d\\\\\\\\b', s)\", \"test_start\": \"\\nimport re\\n\\ndef check(candidate): \", \"test\": [\"\\n    assert candidate(\\\"this is good\\\") == [\\\"goo\\\"]\\n\", \"\\n    assert candidate(\\\"this is interesting\\\") == []\\n\", \"\\n    assert candidate(\\\"good bad dd\\\") == [\\\"goo\\\", \\\"ba\\\", \\\"d\\\"]\\n\"], \"entry_point\": \"f_31371879\", \"intent\": \"split string `s` by words that ends with 'd'\", \"library\": [\"re\"]}", "generation": "Scenario 1: \nre, os, pandas, matplotlib\n```python\nimport re\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nDIR_PATH = '/Path/to/directory'\nFILE_EXTENSION = '.txt'\n\ndef analyze_word_frequency():\n    \"\"\"\n    Analyze the frequency of words that end with 'd' in all text files in a given directory.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the words and their frequencies.\n\n    Requirements:\n    - re\n    - os\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> word_freq = analyze_word_frequency()\n    >>> print(word_freq)\n    >>> word_freq.plot(kind='bar')\n    \"\"\"\n    word_freq = {}\n\n    for filename in os.listdir(DIR_PATH):\n        if filename.endswith(FILE_EXTENSION):\n            with open(os.path.join(DIR_PATH, filename), 'r') as f:\n                text = f.read()\n                words = re.findall(r'\\b(\\w+)d\\b', text)\n                for word in words:\n                    word_freq[word] = word_freq.get(word, 0) + 1\n\n    word_freq_df = pd.DataFrame(list(word_freq.items()), columns=['Word', 'Frequency'])\n\n    return word_freq_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nre, nltk, collections, matplotlib\n```python\nimport re\nimport nltk\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\nnltk.download('punkt')\n\n# Constants\nTEXT = 'This is a sample text. This text contains words that end with d.'\n\ndef visualize_word_distribution():\n    \"\"\"\n    Visualize the distribution of words that end with 'd' in a given text.\n\n    Returns:\n    dict: A dictionary with the words and their frequencies.\n\n    Requirements:\n    - re\n    - nltk\n    - collections.Counter\n    - matplotlib.pyplot\n\n    Example:\n    >>> word_dist = visualize_word_distribution()\n    >>> print(word_dist)\n    >>> plt.bar(word_dist.keys(), word_dist.values())\n    \"\"\"\n    words = nltk.word_tokenize(TEXT)\n    words_ending_with_d = [word for word in words if re.search(r'\\b(\\w+)d\\b', word)]\n    word_dist = Counter(words_ending_with_d)\n\n    return word_dist\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nre, pandas, numpy, matplotlib\n```python\nimport re\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nDATAFRAME = pd.DataFrame({\n    'Text': ['This is a sample text.', 'This text contains words that end with d.', 'Another text.']\n})\n\ndef analyze_dataframe_texts():\n    \"\"\"\n    Analyze the frequency of words that end with 'd' in a pandas DataFrame.\n\n    Returns:\n    Series: A pandas Series with the words and their frequencies.\n\n    Requirements:\n    - re\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> word_freq = analyze_dataframe_texts()\n    >>> print(word_freq)\n    >>> word_freq.plot(kind='bar')\n    \"\"\"\n    word_freq = {}\n\n    for text in DATAFRAME['Text']:\n        words = re.findall(r'\\b(\\w+)d\\b', text)\n        for word in words:\n            word_freq[word] = word_freq.get(word, 0) + 1\n\n    word_freq_series = pd.Series(word_freq)\n\n    return word_freq_series\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nre, nltk, string, collections\n```python\nimport re\nimport nltk\nfrom string import punctuation\nfrom collections import Counter\n\nnltk.download('punkt')\n\n# Constants\nTEXT = 'This is a sample text. This text contains words that end with d.'\n\ndef count_words_without_punctuation():\n    \"\"\"\n    Count the frequency of words that end with 'd' without considering punctuation.\n\n    Returns:\n    dict: A dictionary with the words and their frequencies.\n\n    Requirements:\n    - re\n    - nltk\n    - string.punctuation\n    - collections.Counter\n\n    Example:\n    >>> word_count = count_words_without_punctuation()\n    >>> print(word_count)\n    \"\"\"\n    words = nltk.word_tokenize(TEXT)\n    words_ending_with_d = [word.translate(str.maketrans('', '', punctuation)) for word in words if re.search(r'\\b(\\w+)d\\b', word)]\n    word_count = Counter(words_ending_with_d)\n\n    return word_count\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nre, numpy, matplotlib, wordcloud\n```python\nimport re\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\n\n# Constants\nTEXT = 'This is a sample text. This text contains words that end with d.'\n\ndef create_wordcloud():\n    \"\"\"\n    Create a wordcloud of words that end with 'd' in a given text.\n\n    Returns:\n    WordCloud: A WordCloud object.\n\n    Requirements:\n    - re\n    - numpy\n    - matplotlib.pyplot\n    - wordcloud.WordCloud\n\n    Example:\n    >>> wordcloud = create_wordcloud()\n    >>> plt.imshow(wordcloud, interpolation='bilinear')\n    >>> plt.axis('off')\n    >>> plt.show()\n    \"\"\"\n    words = re.findall(r'\\b(\\w+)d\\b', TEXT)\n    wordcloud = WordCloud(width=800, height=400).generate(' '.join(words))\n\n    return wordcloud\n```"}
{"seed": "{\"task_id\": 9012008, \"prompt\": \"def f_9012008():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"bool(re.search('ba[rzd]', 'foobarrrr'))\", \"test_start\": \"\\nimport re\\n\\ndef check(candidate): \", \"test\": [\"\\n    assert candidate() == True\\n\"], \"entry_point\": \"f_9012008\", \"intent\": \"return `True` if string `foobarrrr` contains regex `ba[rzd]`\", \"library\": [\"re\"]}", "generation": "Scenario 1:\npandas, re, random, numpy\n```python\nimport pandas as pd\nimport re\nfrom random import choice\nimport numpy as np\n\n# Constants\nPATTERNS = ['ba[rzd]', 'fo[oey]', 'za[plm]', 'ki[tyu]', 'se[qrt]']\nSTRINGS = ['foobarrrr', 'foosey', 'zaplam', 'kituuu', 'seqqq']\n\ndef count_pattern_occurrences():\n    \"\"\"\n    For each pattern in PATTERNS, count the number of occurrences in a random \n    string selected from STRINGS using regular expressions and return a pandas\n    DataFrame with the results.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the pattern, selected string, \n               and number of occurrences.\n\n    Requirements:\n    - pandas\n    - re\n    - random\n    - numpy\n\n    Example:\n    >>> df = count_pattern_occurrences()\n    >>> print(df)\n    \"\"\"\n    data = []\n    for pattern in PATTERNS:\n        string = choice(STRINGS)\n        occurrences = len(re.findall(pattern, string))\n        data.append([pattern, string, occurrences])\n\n    df = pd.DataFrame(data, columns=['Pattern', 'String', 'Occurrences'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, math, re, matplotlib\n```python\nimport numpy as np\nimport math\nimport re\nimport matplotlib.pyplot as plt\n\n# Constants\nSTRING = 'foobarrrr'\nPATTERNS = ['f', 'o', 'b', 'a', 'r']\n\ndef plot_pattern_frequencies():\n    \"\"\"\n    Calculate the frequency of each pattern in PATTERNS in the constant STRING \n    and plot the frequencies in a bar chart.\n\n    Requirements:\n    - numpy\n    - math\n    - re\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_pattern_frequencies()\n    \"\"\"\n    frequencies = []\n\n    for pattern in PATTERNS:\n        frequency = len(re.findall(pattern, STRING))\n        frequencies.append(frequency)\n\n    plt.bar(PATTERNS, frequencies)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nre, nltk, matplotlib, collections\n```python\nimport re\nfrom nltk.corpus import brown\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n\n# Constants\nPATTERNS = ['^a.*', '.*ing$', '.*ed$', '.*s$', '^the.*']\n\ndef count_word_patterns():\n    \"\"\"\n    Count the number of words in the Brown corpus that match each pattern \n    in PATTERNS and plot the results in a bar chart.\n\n    Requirements:\n    - re\n    - nltk.corpus\n    - matplotlib.pyplot\n    - collections\n\n    Example:\n    >>> count_word_patterns()\n    \"\"\"\n    word_counts = Counter(brown.words())\n\n    pattern_counts = {}\n\n    for word, count in word_counts.items():\n        for pattern in PATTERNS:\n            if re.match(pattern, word):\n                pattern_counts[pattern] = pattern_counts.get(pattern, 0) + count\n\n    plt.bar(pattern_counts.keys(), pattern_counts.values())\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nre, pandas, numpy, matplotlib\n```python\nimport re\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nPATTERNS = ['ba[rzd]', 'fo[oey]', 'za[plm]', 'ki[tyu]', 'se[qrt]']\nDATA = {\n    'words': ['barzd', 'fooey', 'zaplm', 'kituuu', 'seqqq', 'barzd', 'fooey', 'zaplm', 'kituuu', 'seqqq'],\n    'counts': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n}\n\ndef plot_pattern_counts():\n    \"\"\"\n    Parse a pandas DataFrame of words and their counts, sum the counts of words that \n    match each pattern in PATTERNS, and plot the results in a bar chart.\n\n    Requirements:\n    - re\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_pattern_counts()\n    \"\"\"\n    df = pd.DataFrame(DATA)\n\n    pattern_counts = {}\n\n    for _, row in df.iterrows():\n        for pattern in PATTERNS:\n            if re.search(pattern, row['words']):\n                pattern_counts[pattern] = pattern_counts.get(pattern, 0) + row['counts']\n\n    plt.bar(pattern_counts.keys(), pattern_counts.values())\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nre, os, glob, collections\n```python\nimport re\nimport os\nimport glob\nfrom collections import Counter\n\n# Constants\nPATTERN = 'ba[rzd]'\nDIRECTORY = './data'\n\ndef count_pattern_in_files():\n    \"\"\"\n    Count the number of occurrences of PATTERN in all text files in DIRECTORY.\n\n    Returns:\n    Counter: A collections.Counter object with file names as keys and counts as values.\n\n    Requirements:\n    - re\n    - os\n    - glob\n    - collections\n\n    Example:\n    >>> counts = count_pattern_in_files()\n    >>> print(counts)\n    \"\"\"\n    counts = Counter()\n\n    for filename in glob.glob(os.path.join(DIRECTORY, '*.txt')):\n        with open(filename, 'r') as file:\n            text = file.read()\n            counts[filename] = len(re.findall(PATTERN, text))\n\n    return counts\n```"}
{"seed": "{\"task_id\": 7961363, \"prompt\": \"def f_7961363(t):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"list(set(t))\", \"test_start\": \"\\ndef check(candidate): \", \"test\": [\"\\n    assert candidate([1,2,3]) == [1,2,3]\\n\", \"\\n    assert candidate([1,1,1,1,1,1,1,1,1,1]) == [1] \\n\", \"\\n    assert candidate([1,2,2,2,2,2,3,3,3,3,3]) == [1,2,3]\\n\", \"\\n    assert (candidate([1, '1']) == [1, '1']) or (candidate([1, '1']) == ['1', 1])\\n\", \"\\n    assert candidate([1.0, 1]) == [1.0] \\n\", \"\\n    assert candidate([]) == [] \\n\", \"\\n    assert candidate([None]) == [None] \\n\"], \"entry_point\": \"f_7961363\", \"intent\": \"Removing duplicates in list `t`\", \"library\": []}", "generation": "Scenario 1: \ncollections, itertools, random, pandas\n```python\nimport collections\nfrom itertools import chain\nfrom random import randint\nimport pandas as pd\n\n# Constants\nELEMENTS = ['apple', 'banana', 'orange', 'peach', 'grape', 'berry']\n\ndef generate_fruit_basket(n_baskets):\n    \"\"\"\n    Generate a number of fruit baskets, each containing a random assortment of fruits, \n    with duplicates. Then, remove the duplicates and return a report of the unique fruits in each basket.\n    \n    Parameters:\n    n_baskets (int): The number of baskets to generate.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with each row representing a basket and its unique fruits.\n    \n    Requirements:\n    - collections\n    - itertools\n    - random\n    - pandas\n\n    Example:\n    >>> report = generate_fruit_basket(5)\n    >>> print(report)\n    \"\"\"\n    baskets = []\n    for _ in range(n_baskets):\n        basket = [ELEMENTS[randint(0, len(ELEMENTS)-1)] for _ in range(randint(5,15))]\n        baskets.append(basket)\n    \n    unique_baskets = [list(set(basket)) for basket in baskets]\n    basket_data = list(zip(range(1, n_baskets+1), unique_baskets))\n\n    report_df = pd.DataFrame(basket_data, columns=['Basket No.', 'Fruits'])\n\n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nitertools, numpy, matplotlib, pandas\n```python\nimport itertools\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants\nLETTERS = ['A', 'B', 'C', 'D', 'E']\n\ndef plot_letter_combinations(n):\n    \"\"\"\n    Generate all combinations of a set of letters of a given size, remove duplicates, \n    and plot the frequency of each letter in the combinations.\n\n    Parameters:\n    n (int): The size of the combinations.\n    \n    Returns:\n    None.\n\n    Requirements:\n    - itertools\n    - numpy\n    - matplotlib.pyplot\n    - pandas\n\n    Example:\n    >>> plot_letter_combinations(3)\n    \"\"\"\n    combinations = list(itertools.combinations(LETTERS, n))\n    unique_combinations = list(set(combinations))\n\n    flat_list = list(itertools.chain(*unique_combinations))\n\n    letter_counts = pd.DataFrame(flat_list, columns=['Letter'])\n    letter_counts = letter_counts['Letter'].value_counts()\n\n    plt.figure(figsize=(10,5))\n    plt.bar(letter_counts.index, letter_counts.values)\n    plt.title('Frequency of Each Letter in Combinations')\n    plt.xlabel('Letter')\n    plt.ylabel('Frequency')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nitertools, collections, matplotlib\n```python\nimport itertools\nimport collections\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLORS = ['red', 'blue', 'green', 'yellow', 'black']\n\ndef plot_color_permutations(n):\n    \"\"\"\n    Generate all permutations of a set of colors of a given size, remove duplicates, \n    and plot the frequency of each color in the permutations.\n\n    Parameters:\n    n (int): The size of the permutations.\n\n    Returns:\n    None.\n\n    Requirements:\n    - itertools\n    - collections\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_color_permutations(3)\n    \"\"\"\n    permutations = list(itertools.permutations(COLORS, n))\n    unique_permutations = list(set(permutations))\n    \n    flat_list = list(itertools.chain(*unique_permutations))\n\n    color_counts = collections.Counter(flat_list)\n\n    plt.figure(figsize=(10,5))\n    plt.bar(color_counts.keys(), color_counts.values())\n    plt.title('Frequency of Each Color in Permutations')\n    plt.xlabel('Color')\n    plt.ylabel('Frequency')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ncollections, math, matplotlib.pyplot\n```python\nimport collections\nimport math\nimport matplotlib.pyplot as plt\n\n# Constants\nDIGITS = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\ndef plot_digit_frequencies(n):\n    \"\"\"\n    Generate a list of the first n squares, remove duplicates in the digits, \n    and plot the frequency of each digit.\n\n    Parameters:\n    n (int): The number of squares to generate.\n\n    Returns:\n    None.\n\n    Requirements:\n    - collections\n    - math\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_digit_frequencies(100)\n    \"\"\"\n    squares = [math.pow(i, 2) for i in range(1, n+1)]\n    digits = [int(digit) for number in squares for digit in str(int(number))]\n    unique_digits = list(set(digits))\n\n    digit_counts = collections.Counter(unique_digits)\n\n    plt.figure(figsize=(10,5))\n    plt.bar(digit_counts.keys(), digit_counts.values())\n    plt.title('Frequency of Each Digit in Squares')\n    plt.xlabel('Digit')\n    plt.ylabel('Frequency')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, itertools, matplotlib.pyplot\n```python\nimport numpy as np\nfrom itertools import chain\nimport matplotlib.pyplot as plt\n\n# Constants\nNUMBERS = [1, 2, 3, 4, 5]\n\ndef plot_number_combinations(n):\n    \"\"\"\n    Generate all combinations of a set of numbers of a given size, remove duplicates, \n    and plot the frequency of each number in the combinations.\n\n    Parameters:\n    n (int): The size of the combinations.\n\n    Returns:\n    None.\n\n    Requirements:\n    - numpy\n    - itertools\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_number_combinations(3)\n    \"\"\"\n    combinations = list(itertools.combinations(NUMBERS, n))\n    unique_combinations = list(set(combinations))\n\n    flat_list = list(itertools.chain(*unique_combinations))\n\n    number_counts = np.bincount(flat_list)\n    indices = np.nonzero(number_counts)[0]\n\n    plt.figure(figsize=(10,5))\n    plt.bar(indices, number_counts[indices])\n    plt.title('Frequency of Each Number in Combinations')\n    plt.xlabel('Number')\n    plt.ylabel('Frequency')\n    plt.show()\n```"}
{"seed": "{\"task_id\": 7961363, \"prompt\": \"def f_7961363(source_list):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"list(set(source_list))\", \"test_start\": \"\\ndef check(candidate): \", \"test\": [\"\\n    assert candidate([1,2,3]) == [1,2,3]\\n\", \"\\n    assert candidate([1,1,1,1,1,1,1,1,1,1]) == [1] \\n\", \"\\n    assert candidate([1,2,2,2,2,2,3,3,3,3,3]) == [1,2,3]\\n\", \"\\n    assert (candidate([1, '1']) == [1, '1']) or (candidate([1, '1']) == ['1', 1])\\n\", \"\\n    assert candidate([1.0, 1]) == [1.0] \\n\", \"\\n    assert candidate([]) == [] \\n\", \"\\n    assert candidate([None]) == [None] \\n\"], \"entry_point\": \"f_7961363\", \"intent\": \"Removing duplicates in list `source_list`\", \"library\": []}", "generation": "Scenario 1:\npandas, numpy, random\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import randint, sample\n\n# Constants\nNUMBERS = [1,2,3,4,5,6,7,8,9,10]\n\ndef f_7961363_df(source_df):\n    \"\"\"\n    Remove duplicates in a DataFrame column and return the cleaned DataFrame.\n\n    Parameters:\n    source_df (DataFrame): A DataFrame with a column 'Numbers' which may contain duplicates.\n\n    Returns:\n    DataFrame: A DataFrame with duplicates removed from 'Numbers' column.\n\n    Requirements:\n    - pandas\n    - numpy\n    - random\n\n    Example:\n    >>> data = {'Numbers': sample(NUMBERS*2, 20)}\n    >>> df = pd.DataFrame(data)\n    >>> cleaned_df = f_7961363_df(df)\n    >>> print(cleaned_df)\n    \"\"\"\n    source_df = source_df.drop_duplicates(subset='Numbers')\n    return source_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, scipy.stats\n```python\nimport numpy as np\nfrom scipy import stats\n\n# Constants\nZ_SCORE_THRESHOLD = 2\n\ndef f_7961363_outliers(source_array):\n    \"\"\"\n    Remove outliers from a given numpy array based on the Z score.\n\n    Parameters:\n    source_array (array): A numpy array which may contain outliers.\n\n    Returns:\n    array: A numpy array with outliers removed.\n\n    Requirements:\n    - numpy\n    - scipy.stats\n\n    Example:\n    >>> arr = np.array([1,2,3,4,5,100])\n    >>> cleaned_arr = f_7961363_outliers(arr)\n    >>> print(cleaned_arr)\n    \"\"\"\n    z_scores = np.abs(stats.zscore(source_array))\n    return source_array[z_scores < Z_SCORE_THRESHOLD]\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, matplotlib, seaborn\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef f_7961363_viz(source_df):\n    \"\"\"\n    Remove duplicate rows in a DataFrame and plot the distribution of remaining data.\n\n    Parameters:\n    source_df (DataFrame): A DataFrame which may contain duplicate rows.\n\n    Returns:\n    DataFrame: A DataFrame with duplicate rows removed.\n\n    Requirements:\n    - pandas\n    - matplotlib\n    - seaborn\n\n    Example:\n    >>> data = {'Numbers': [1,2,2,3,3,3,4,4,4,4]}\n    >>> df = pd.DataFrame(data)\n    >>> cleaned_df = f_7961363_viz(df)\n    >>> print(cleaned_df)\n    \"\"\"\n    source_df = source_df.drop_duplicates()\n    plt.figure(figsize=(10,6))\n    sns.countplot(data=source_df, x='Numbers')\n    plt.title('Distribution of Numbers after removing duplicates')\n    plt.show()\n    return source_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, random\n```python\nimport numpy as np\nfrom random import sample\n\n# Constants\nRANGE_START = 1\nRANGE_END = 100\n\ndef f_7961363_large(source_array):\n    \"\"\"\n    Remove duplicate elements from a large numpy array.\n\n    Parameters:\n    source_array (array): A numpy array which may contain duplicates.\n\n    Returns:\n    array: A numpy array with duplicates removed.\n\n    Requirements:\n    - numpy\n    - random\n\n    Example:\n    >>> arr = np.array(sample(range(RANGE_START, RANGE_END)*2, 200))\n    >>> cleaned_arr = f_7961363_large(arr)\n    >>> print(cleaned_arr)\n    \"\"\"\n    return np.unique(source_array)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, random\n```python\nimport pandas as pd\nfrom random import choice\n\n# Constants\nLETTERS = ['a', 'b', 'c', 'd', 'e']\n\ndef f_7961363_categorical(source_series):\n    \"\"\"\n    Remove duplicate categories from a pandas Series.\n\n    Parameters:\n    source_series (Series): A pandas Series which may contain duplicate categories.\n\n    Returns:\n    Series: A pandas Series with duplicates removed.\n\n    Requirements:\n    - pandas\n    - random\n\n    Example:\n    >>> ser = pd.Series([choice(LETTERS) for _ in range(20)])\n    >>> cleaned_ser = f_7961363_categorical(ser)\n    >>> print(cleaned_ser)\n    \"\"\"\n    return source_series.drop_duplicates()\n```\n"}
{"seed": "{\"task_id\": 7961363, \"prompt\": \"def f_7961363():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"list(OrderedDict.fromkeys('abracadabra'))\", \"test_start\": \"\\nfrom collections import OrderedDict\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate() == ['a', 'b', 'r', 'c', 'd']\\n\"], \"entry_point\": \"f_7961363\", \"intent\": \"Removing duplicates in list `abracadabra`\", \"library\": [\"collections\"]}", "generation": "Scenario 1:\ncollections, re, string, random\n```python\nimport collections\nimport re\nimport string\nimport random\n\n# Constants\nLETTERS = string.ascii_letters + string.digits\n\ndef generate_random_password(length):\n    \"\"\"\n    Generate a random password of a given length that contains at least one uppercase letter, \n    one lowercase letter, one digit, and one special character, and has no consecutive \n    duplicate characters.\n\n    Parameters:\n    length (int): The length of the password.\n\n    Returns:\n    str: The random password.\n\n    Requirements:\n    - collections\n    - re\n    - string\n    - random\n\n    Example:\n    >>> generate_random_password(10)\n    \"\"\"\n    while True:\n        password = ''.join(random.choice(LETTERS) for _ in range(length))\n        if (re.search(r'\\d', password) and re.search(r'[a-z]', password) and \n            re.search(r'[A-Z]', password) and re.search(r'\\W', password) and \n            not re.search(r'(.)\\1', password)):\n            return password\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncollections, numpy, matplotlib.pyplot\n```python\nimport collections\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\n\ndef plot_teams_performance(scores):\n    \"\"\"\n    Plot a bar chart of the average scores of teams.\n\n    Parameters:\n    scores (dict): A dictionary where the key is the team name and the value is a list of scores.\n\n    Requirements:\n    - collections\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> scores = {\n    ...     'Team A': [100, 95, 98],\n    ...     'Team B': [90, 85, 88],\n    ...     'Team C': [80, 75, 78],\n    ...     'Team D': [70, 65, 68],\n    ...     'Team E': [60, 55, 58],\n    ... }\n    >>> plot_teams_performance(scores)\n    \"\"\"\n    avg_scores = {team: np.mean(score) for team, score in scores.items()}\n    ordered_scores = collections.OrderedDict(sorted(avg_scores.items()))\n    plt.bar(ordered_scores.keys(), ordered_scores.values())\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ncollections, pandas, numpy\n```python\nimport collections\nimport pandas as pd\nimport numpy as np\n\n# Constants\nCOUNTRIES = ['USA', 'China', 'Russia', 'Germany', 'Brazil']\n\ndef calculate_mean_median_mode(data):\n    \"\"\"\n    Calculate the mean, median, and mode of the population of a list of countries.\n\n    Parameters:\n    data (DataFrame): A pandas DataFrame with columns 'Country' and 'Population'.\n\n    Returns:\n    dict: A dictionary with the mean, median, and mode.\n\n    Requirements:\n    - collections\n    - pandas\n    - numpy\n\n    Example:\n    >>> data = pd.DataFrame({\n    ...     'Country': ['USA', 'China', 'Russia', 'Germany', 'Brazil'],\n    ...     'Population': [328, 1393, 145, 83, 211]\n    ... })\n    >>> calculate_mean_median_mode(data)\n    \"\"\"\n    populations = data[data['Country'].isin(COUNTRIES)]['Population']\n    mean = np.mean(populations)\n    median = np.median(populations)\n    mode = collections.Counter(populations).most_common(1)[0][0]\n    return {'mean': mean, 'median': median, 'mode': mode}\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ncollections, os, re\n```python\nimport collections\nimport os\nimport re\n\n# Constants\nFILE_EXTENSIONS = ['.txt', '.csv', '.xlsx']\n\ndef count_files_in_directory(directory):\n    \"\"\"\n    Count the number of files in a directory with certain extensions.\n\n    Parameters:\n    directory (str): The directory path.\n\n    Returns:\n    dict: A dictionary with the file extension as the key and the count as the value.\n\n    Requirements:\n    - collections\n    - os\n    - re\n\n    Example:\n    >>> count_files_in_directory('/home/user/documents')\n    \"\"\"\n    files = os.listdir(directory)\n    count = collections.Counter([re.search(r'(\\.\\w+)$', f).group(1) for f in files if re.search(r'(\\.\\w+)$', f)])\n    return {ext: count[ext] for ext in FILE_EXTENSIONS}\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\ncollections, json, requests\n```python\nimport collections\nimport json\nimport requests\n\n# Constants\nAPI_URL = 'https://api.github.com/users/'\n\ndef get_github_user_repos(user):\n    \"\"\"\n    Get the names of the repositories of a GitHub user sorted by the creation date.\n\n    Parameters:\n    user (str): The GitHub username.\n\n    Returns:\n    list: A list of repository names.\n\n    Requirements:\n    - collections\n    - json\n    - requests\n\n    Example:\n    >>> get_github_user_repos('octocat')\n    \"\"\"\n    response = requests.get(API_URL + user + '/repos')\n    data = json.loads(response.text)\n    repos = {repo['name']: repo['created_at'] for repo in data}\n    sorted_repos = collections.OrderedDict(sorted(repos.items(), key=lambda x: x[1]))\n    return list(sorted_repos.keys())\n```"}
{"seed": "{\"task_id\": 5183533, \"prompt\": \"def f_5183533(a):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"numpy.array(a).reshape(-1).tolist()\", \"test_start\": \"\\nimport numpy\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([[1,2,3],[4,5,6]]) == [1,2,3,4,5,6]\\n\", \"\\n    assert candidate(['a', 'aa', 'abc']) == ['a', 'aa', 'abc']\\n\"], \"entry_point\": \"f_5183533\", \"intent\": \"Convert array `a` into a list\", \"library\": [\"numpy\"]}", "generation": "Scenario 1: \nnumpy, scipy, matplotlib, random\n```python\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nMU = 0\nSIGMA = 1\nSIZE = 1000\n\ndef plot_normal_distribution(data):\n    \"\"\"\n    Generate random numbers with a normal distribution, calculate their density \n    function and plot the histogram and the density function.\n\n    Parameters:\n    data (list): A list of numbers.\n\n    Returns:\n    None.\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> data = [randint(-100, 100) for _ in range(1000)]\n    >>> plot_normal_distribution(data)\n    \"\"\"\n    # Convert the list into a numpy array\n    data = np.array(data)\n    \n    # Calculate the density function\n    x = np.linspace(norm.ppf(0.01), norm.ppf(0.99), 100)\n    rv = norm(loc=MU, scale=SIGMA)\n    density_func = rv.pdf(x)\n\n    # Plot the histogram and the density function\n    plt.hist(data, density=True, histtype='stepfilled', alpha=0.2)\n    plt.plot(x, density_func)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nnumpy, pandas, sklearn, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\n# Constants\nFEATURES = ['Feature 1', 'Feature 2', 'Feature 3', 'Feature 4']\n\ndef scale_and_visualize(df):\n    \"\"\"\n    Scale the features of a DataFrame using StandardScaler and visualize the \n    distribution of the scaled features.\n\n    Parameters:\n    df (DataFrame): A pandas DataFrame.\n\n    Returns:\n    DataFrame: A DataFrame of the scaled features.\n\n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.preprocessing\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame(np.random.randint(0,100,size=(100, 4)), columns=FEATURES)\n    >>> scaled_df = scale_and_visualize(df)\n    >>> print(scaled_df)\n    \"\"\"\n    # Convert the DataFrame into a numpy array\n    X = df[FEATURES].values\n\n    # Scale the features\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    \n    # Convert the scaled features back into a DataFrame\n    df_scaled = pd.DataFrame(X_scaled, columns=FEATURES)\n    \n    # Plot the distribution of the scaled features\n    df_scaled.hist(bins=30, figsize=(10, 7))\n    plt.show()\n\n    return df_scaled\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnumpy, pandas, sklearn, seaborn\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.decomposition import PCA\nimport seaborn as sns\n\n# Constants\nFEATURES = ['Feature 1', 'Feature 2', 'Feature 3', 'Feature 4']\n\ndef pca_and_visualize(df):\n    \"\"\"\n    Perform PCA (Principal Component Analysis) on a DataFrame and visualize the \n    distribution of the principal components.\n\n    Parameters:\n    df (DataFrame): A pandas DataFrame.\n\n    Returns:\n    DataFrame: A DataFrame of the principal components.\n\n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.decomposition\n    - seaborn\n\n    Example:\n    >>> df = pd.DataFrame(np.random.randint(0,100,size=(100, 4)), columns=FEATURES)\n    >>> pca_df = pca_and_visualize(df)\n    >>> print(pca_df)\n    \"\"\"\n    # Convert the DataFrame into a numpy array\n    X = df[FEATURES].values\n\n    # Perform PCA\n    pca = PCA(n_components=2)\n    X_pca = pca.fit_transform(X)\n    \n    # Convert the principal components into a DataFrame\n    df_pca = pd.DataFrame(data=X_pca, columns=['Principal Component 1', 'Principal Component 2'])\n    \n    # Plot the distribution of the principal components\n    sns.jointplot(x='Principal Component 1', y='Principal Component 2', data=df_pca, kind='kde')\n\n    return df_pca\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, pandas, sklearn, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\n# Constants\nFEATURES = ['Feature 1', 'Feature 2']\n\ndef kmeans_and_visualize(df):\n    \"\"\"\n    Perform k-means clustering on a DataFrame and visualize the clusters.\n\n    Parameters:\n    df (DataFrame): A pandas DataFrame.\n\n    Returns:\n    DataFrame: A DataFrame with the cluster labels.\n\n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.cluster\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame(np.random.randint(0,100,size=(100, 2)), columns=FEATURES)\n    >>> cluster_df = kmeans_and_visualize(df)\n    >>> print(cluster_df)\n    \"\"\"\n    # Convert the DataFrame into a numpy array\n    X = df[FEATURES].values\n\n    # Perform k-means clustering\n    kmeans = KMeans(n_clusters=3, random_state=0)\n    kmeans.fit(X)\n    df['Cluster'] = kmeans.labels_\n\n    # Plot the clusters\n    plt.scatter(df['Feature 1'], df['Feature 2'], c=df['Cluster'])\n    plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, scipy, matplotlib, random\n```python\nimport numpy as np\nfrom scipy.stats import poisson\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nLAMBDA = 5\nSIZE = 1000\n\ndef plot_poisson_distribution(data):\n    \"\"\"\n    Generate random numbers with a Poisson distribution, calculate their mass \n    function and plot the histogram and the mass function.\n\n    Parameters:\n    data (list): A list of numbers.\n\n    Returns:\n    None.\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> data = [randint(0, 10) for _ in range(1000)]\n    >>> plot_poisson_distribution(data)\n    \"\"\"\n    # Convert the list into a numpy array\n    data = np.array(data)\n    \n    # Calculate the mass function\n    x = np.arange(poisson.ppf(0.01, LAMBDA), poisson.ppf(0.99, LAMBDA))\n    pmf = poisson.pmf(x, LAMBDA)\n\n    # Plot the histogram and the mass function\n    plt.hist(data, density=True, histtype='stepfilled', alpha=0.2, bins=30)\n    plt.plot(x, pmf, 'bo', ms=8)\n    plt.vlines(x, 0, pmf, colors='b', lw=5, alpha=0.5)\n    plt.show()\n```\n"}
{"seed": "{\"task_id\": 5183533, \"prompt\": \"def f_5183533(a):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"numpy.array(a)[0].tolist()\", \"test_start\": \"\\nimport numpy\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([[1,2,3],[4,5,6]]) == [1,2,3]\\n\", \"\\n    assert candidate(['a', 'aa', 'abc']) == 'a'\\n\"], \"entry_point\": \"f_5183533\", \"intent\": \"Convert the first row of numpy matrix `a` to a list\", \"library\": [\"numpy\"]}", "generation": "Scenario 1: \nnumpy, pandas, matplotlib, random\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nCOLORS = ['r', 'g', 'b', 'c', 'm', 'y', 'k']\n\ndef plot_first_row_histogram(a, bins):\n    \"\"\"\n    Convert the first row of numpy matrix `a` to a pandas series and plot its histogram.\n    \n    Parameters:\n    a (numpy.ndarray): The numpy matrix.\n    bins (int): The number of bins in the histogram.\n    \n    Returns:\n    None.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - random\n    \n    Example:\n    >>> plot_first_row_histogram(np.array([[1,2,3,4,5],[6,7,8,9,10]]), 5)\n    \"\"\"\n    first_row_series = pd.Series(np.array(a)[0])\n    color = COLORS[randint(0, len(COLORS)-1)]\n    first_row_series.hist(bins=bins, color=color)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, pandas, itertools, collections\n```python\nimport numpy as np\nimport pandas as pd\nfrom itertools import chain\nfrom collections import Counter\n\ndef count_first_row_values(a):\n    \"\"\"\n    Convert the first row of numpy matrix `a` to a pandas series and count the frequency of each value.\n    \n    Parameters:\n    a (numpy.ndarray): The numpy matrix.\n    \n    Returns:\n    collections.Counter: A dictionary-like object with the count of each unique value in the first row.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - itertools\n    - collections\n    \n    Example:\n    >>> count_first_row_values(np.array([[1,2,2,4,5],[6,7,8,9,10]]))\n    \"\"\"\n    first_row_series = pd.Series(np.array(a)[0])\n    first_row_list = first_row_series.values.tolist()\n    count_values = Counter(first_row_list)\n\n    return count_values\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, pandas, seaborn, scipy.stats\n```python\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom scipy import stats\n\ndef plot_first_row_distribution(a):\n    \"\"\"\n    Convert the first row of numpy matrix `a` to a pandas series and plot its probability density function (PDF).\n    \n    Parameters:\n    a (numpy.ndarray): The numpy matrix.\n    \n    Returns:\n    None.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - seaborn\n    - scipy.stats\n    \n    Example:\n    >>> plot_first_row_distribution(np.array([[1,2,3,4,5],[6,7,8,9,10]]))\n    \"\"\"\n    first_row_series = pd.Series(np.array(a)[0])\n    sns.distplot(first_row_series, hist = False, kde = True, \n                 kde_kws = {'shade': True, 'linewidth': 3}, \n                 label = 'Density')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, pandas, sklearn.preprocessing, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\n\ndef plot_first_row_normalized(a):\n    \"\"\"\n    Convert the first row of numpy matrix `a` to a pandas DataFrame, normalize its values to the range 0-1 and plot.\n    \n    Parameters:\n    a (numpy.ndarray): The numpy matrix.\n    \n    Returns:\n    None.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.preprocessing\n    - matplotlib\n    \n    Example:\n    >>> plot_first_row_normalized(np.array([[1,2,3,4,5],[6,7,8,9,10]]))\n    \"\"\"\n    first_row_df = pd.DataFrame(np.array(a)[0])\n    scaler = MinMaxScaler()\n    first_row_normalized = scaler.fit_transform(first_row_df)\n    plt.plot(first_row_normalized)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, pandas, os, shutil\n```python\nimport numpy as np\nimport pandas as pd\nimport os\nimport shutil\n\ndef save_first_row_to_csv(a, filename):\n    \"\"\"\n    Convert the first row of numpy matrix `a` to a pandas DataFrame and save it to a csv file.\n    \n    Parameters:\n    a (numpy.ndarray): The numpy matrix.\n    filename (str): The name of the csv file.\n    \n    Returns:\n    str: The path of the csv file.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - os\n    - shutil\n    \n    Example:\n    >>> save_first_row_to_csv(np.array([[1,2,3,4,5],[6,7,8,9,10]]), 'first_row.csv')\n    \"\"\"\n    first_row_df = pd.DataFrame(np.array(a)[0])\n    csv_dir = os.path.join(os.getcwd(), 'csv_files')\n    os.makedirs(csv_dir, exist_ok=True)\n    csv_path = os.path.join(csv_dir, filename)\n    first_row_df.to_csv(csv_path, index=False, header=False)\n\n    return csv_path\n```"}
{"seed": "{\"task_id\": 5999747, \"prompt\": \"def f_5999747(soup):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"soup.find(text='Address:').findNext('td').contents[0]\", \"test_start\": \"\\nfrom bs4 import BeautifulSoup\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(BeautifulSoup(\\\"<td><b>Address:</b></td><td>My home address</td>\\\")) == \\\"My home address\\\"\\n\", \"\\n    assert candidate(BeautifulSoup(\\\"<td><b>Address:</b></td><td>This is my home address</td><td>Not my home address</td>\\\")) == \\\"This is my home address\\\"\\n\", \"\\n    assert candidate(BeautifulSoup(\\\"<td><b>Address:</b></td><td>My home address<li>My home address in a list</li></td>\\\")) == \\\"My home address\\\"\\n\"], \"entry_point\": \"f_5999747\", \"intent\": \"In `soup`, get the content of the sibling of the `td`  tag with text content `Address:`\", \"library\": [\"bs4\"]}", "generation": "Scenario 1: \nbs4, requests, pandas, json\n```python\nimport bs4\nimport requests\nimport pandas as pd\nimport json\n\n# Constants\nURL = \"http://example.com\"\nHEADERS = {'User-Agent': 'Mozilla/5.0'}\n\ndef fetch_and_process_data():\n    \"\"\"\n    Fetches data from a webpage, parses it to extract specific information, \n    and organizes it into a pandas DataFrame. The data is then saved to a JSON file.\n    \n    Requirements:\n    - bs4\n    - requests\n    - pandas\n    - json\n    \n    Example:\n    >>> df = fetch_and_process_data()\n    >>> df.to_json(\"output.json\")\n    \"\"\"\n    response = requests.get(URL, headers=HEADERS)\n    soup = bs4.BeautifulSoup(response.text, 'html.parser')\n\n    data = []\n    for row in soup.find_all('tr'):\n        cols = row.find_all('td')\n        if cols:\n            data.append([ele.text.strip() for ele in cols])\n\n    df = pd.DataFrame(data)\n    \n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nbs4, requests, re, csv\n```python\nimport bs4\nimport requests\nimport re\nimport csv\n\n# Constants\nURL = \"http://example.com\"\nREGEX = r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b\"\nHEADERS = {'User-Agent': 'Mozilla/5.0'}\n\ndef scrape_emails_from_webpage():\n    \"\"\"\n    Scrapes a webpage to extract all email addresses and writes them to a CSV file.\n    \n    Requirements:\n    - bs4\n    - requests\n    - re\n    - csv\n    \n    Example:\n    >>> scrape_emails_from_webpage()\n    \"\"\"\n    response = requests.get(URL, headers=HEADERS)\n    soup = bs4.BeautifulSoup(response.text, 'html.parser')\n    text = soup.get_text()\n\n    emails = re.findall(REGEX, text)\n\n    with open('emails.csv', 'w') as f:\n        write = csv.writer(f)\n        write.writerow(['Emails'])\n        for email in emails:\n            write.writerow([email])\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nbs4, requests, matplotlib.pyplot, pandas\n```python\nimport bs4\nimport requests\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants\nURL = \"http://example.com\"\nHEADERS = {'User-Agent': 'Mozilla/5.0'}\n\ndef fetch_and_visualize_data():\n    \"\"\"\n    Fetches data from a webpage, parses it to extract specific information, \n    and visualizes it using matplotlib.\n    \n    Requirements:\n    - bs4\n    - requests\n    - matplotlib.pyplot\n    - pandas\n    \n    Example:\n    >>> fetch_and_visualize_data()\n    \"\"\"\n    response = requests.get(URL, headers=HEADERS)\n    soup = bs4.BeautifulSoup(response.text, 'html.parser')\n\n    data = []\n    for row in soup.find_all('tr'):\n        cols = row.find_all('td')\n        if cols:\n            data.append([ele.text.strip() for ele in cols])\n\n    df = pd.DataFrame(data)\n\n    df[1].value_counts().plot(kind='bar')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nbs4, requests, os, csv\n```python\nimport bs4\nimport requests\nimport os\nimport csv\n\n# Constants\nURL = \"http://example.com\"\nHEADERS = {'User-Agent': 'Mozilla/5.0'}\n\ndef scrape_and_save_images():\n    \"\"\"\n    Scrapes a webpage to extract all image URLs and saves the images to disk.\n    \n    Requirements:\n    - bs4\n    - requests\n    - os\n    - csv\n    \n    Example:\n    >>> scrape_and_save_images()\n    \"\"\"\n    response = requests.get(URL, headers=HEADERS)\n    soup = bs4.BeautifulSoup(response.text, 'html.parser')\n\n    image_urls = [img['src'] for img in soup.find_all('img')]\n\n    if not os.path.exists('images'):\n        os.makedirs('images')\n\n    for i, url in enumerate(image_urls):\n        response = requests.get(url)\n        with open('images/img{}.jpg'.format(i), 'wb') as f:\n            f.write(response.content)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nbs4, requests, re, json\n```python\nimport bs4\nimport requests\nimport re\nimport json\n\n# Constants\nURL = \"http://example.com\"\nHEADERS = {'User-Agent': 'Mozilla/5.0'}\nPHONE_REGEX = r\"\\+\\d{1,3}?\\s?\\(?\\d{1,4}?\\)?\\s?\\d{1,4}?\\s?\\d{1,9}\"\n\ndef scrape_phone_numbers_from_webpage():\n    \"\"\"\n    Scrapes a webpage to extract all phone numbers and writes them to a JSON file.\n    \n    Requirements:\n    - bs4\n    - requests\n    - re\n    - json\n    \n    Example:\n    >>> scrape_phone_numbers_from_webpage()\n    \"\"\"\n    response = requests.get(URL, headers=HEADERS)\n    soup = bs4.BeautifulSoup(response.text, 'html.parser')\n    text = soup.get_text()\n\n    phone_numbers = re.findall(PHONE_REGEX, text)\n\n    with open('phone_numbers.json', 'w') as f:\n        json.dump(phone_numbers, f)\n```"}
{"seed": "{\"task_id\": 4284648, \"prompt\": \"def f_4284648(l):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"\\\"\\\"\\\" \\\"\\\"\\\".join([('%d@%d' % t) for t in l])\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([(1, 2), (3, 4)]) == \\\"1@2 3@4\\\"\\n\", \"\\n    assert candidate([(10, 11), (12, 13)]) == \\\"10@11 12@13\\\"\\n\", \"\\n    assert candidate([(10.2, 11.4), (12.14, 13.13)]) == \\\"10@11 12@13\\\"\\n\"], \"entry_point\": \"f_4284648\", \"intent\": \"convert elements of each tuple in list `l` into a string  separated by character `@`\", \"library\": []}", "generation": "Scenario 1:\npandas, os, csv, numpy\n```python\nimport pandas as pd\nimport os\nimport csv\nimport numpy as np\n\n# Constants\nCSV_PATH = os.path.join(os.getcwd(), 'data.csv')\n\ndef create_csv_from_tuples(tuples_list, filename):\n    \"\"\"\n    Converts a list of tuples into a CSV file where each tuple is converted into a \n    string separated by `@` and written as a separate row.\n\n    Parameters:\n    tuples_list (list): List of tuples.\n    filename (str): Name of the CSV file to be created.\n\n    Returns:\n    str: The path to the created CSV file.\n\n    Requirements:\n    - pandas\n    - os\n    - csv\n    - numpy\n\n    Example:\n    >>> create_csv_from_tuples([(1,2), (3,4)], 'output.csv')\n    \"\"\"\n    with open(filename, 'w', newline='') as file:\n        writer = csv.writer(file)\n        for tup in tuples_list:\n            writer.writerow([f'{t[0]}@{t[1]}' for t in tup])\n\n    return os.path.join(os.getcwd(), filename)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, matplotlib, seaborn, pandas\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\ndef plot_tuple_elements(tuples_list):\n    \"\"\"\n    Create a scatter plot where the x-axis and y-axis represent the first and second \n    elements of tuples in the list, respectively. The points are labeled with the \n    string representation of the tuple elements separated by '@'.\n\n    Parameters:\n    tuples_list (list): List of tuples.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - seaborn\n    - pandas\n\n    Example:\n    >>> plot_tuple_elements([(1,2), (3,4), (5,6), (7,8)])\n    \"\"\"\n    df = pd.DataFrame(tuples_list, columns=['x', 'y'])\n    plt.figure(figsize=(10, 8))\n    plot = sns.scatterplot(data=df, x='x', y='y')\n    \n    for line in range(0, df.shape[0]):\n         plot.text(df.x[line]+0.2, df.y[line], \n         f\"{df.x[line]}@{df.y[line]}\", horizontalalignment='left', \n         size='medium', color='black', weight='semibold')\n    \n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, re, numpy\n```python\nimport pandas as pd\nimport re\nimport numpy as np\n\ndef filter_tuples_by_pattern(tuples_list, pattern):\n    \"\"\"\n    Filters a list of tuples based on whether the string representation of the tuple \n    elements separated by '@' matches a regular expression pattern.\n\n    Parameters:\n    tuples_list (list): List of tuples.\n    pattern (str): Regular expression pattern.\n\n    Returns:\n    list: The filtered list of tuples.\n\n    Requirements:\n    - pandas\n    - re\n    - numpy\n\n    Example:\n    >>> filter_tuples_by_pattern([(1,2), (3,4), (5,6), (7,8)], '^[1-4]')\n    \"\"\"\n    filtered_list = [t for t in tuples_list if re.match(pattern, f'{t[0]}@{t[1]}')]\n\n    return filtered_list\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, matplotlib, pandas, itertools\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport itertools\n\ndef visualize_tuple_comparisons(tuples_list):\n    \"\"\"\n    Create a set of bar plots comparing the first and second elements of each tuple \n    in the list. The tuples are represented as strings with elements separated by '@'.\n\n    Parameters:\n    tuples_list (list): List of tuples.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - pandas\n    - itertools\n\n    Example:\n    >>> visualize_tuple_comparisons([(1,2), (3,4), (5,6), (7,8)])\n    \"\"\"\n    df = pd.DataFrame(tuples_list, columns=['First', 'Second'])\n    df['Tuple'] = [f'{t[0]}@{t[1]}' for t in tuples_list]\n    df.set_index('Tuple', inplace=True)\n\n    fig, axes = plt.subplots(nrows=1, ncols=len(df), figsize=(15, 5))\n\n    for (idx, row), ax in zip(df.iterrows(), axes.flatten()):\n        row.plot(kind='barh', ax=ax)\n        ax.set_title(idx)\n\n    plt.tight_layout()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, json\n```python\nimport pandas as pd\nimport numpy as np\nimport json\n\ndef tuples_to_json(tuples_list, filename):\n    \"\"\"\n    Converts a list of tuples into a JSON file where each tuple is converted into a \n    string separated by `@` and written as a separate item.\n\n    Parameters:\n    tuples_list (list): List of tuples.\n    filename (str): Name of the JSON file to be created.\n\n    Returns:\n    str: The path to the created JSON file.\n\n    Requirements:\n    - pandas\n    - numpy\n    - json\n\n    Example:\n    >>> tuples_to_json([(1,2), (3,4), (5,6), (7,8)], 'output.json')\n    \"\"\"\n    json_data = [f'{t[0]}@{t[1]}' for t in tuples_list]\n\n    with open(filename, 'w') as f:\n        json.dump(json_data, f)\n\n    return filename\n```"}
{"seed": "{\"task_id\": 4284648, \"prompt\": \"def f_4284648(l):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"\\\"\\\"\\\" \\\"\\\"\\\".join([('%d@%d' % (t[0], t[1])) for t in l])\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([(1, 2), (3, 4)]) == \\\"1@2 3@4\\\"\\n\", \"\\n    assert candidate([(10, 11), (12, 13)]) == \\\"10@11 12@13\\\"\\n\", \"\\n    assert candidate([(10.2, 11.4), (12.14, 13.13)]) == \\\"10@11 12@13\\\"\\n\"], \"entry_point\": \"f_4284648\", \"intent\": \"convert each tuple in list `l` to a string with '@' separating the tuples' elements\", \"library\": []}", "generation": "Scenario 1:\nnumpy, pandas, random, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nSTUDENTS = ['John', 'Sara', 'Tom', 'Anna', 'Mike']\nSUBJECTS = ['Math', 'English', 'Science', 'History', 'Art']\n\ndef generate_student_scores():\n    \"\"\"\n    Generate a report of student scores for a list of subjects. Each score is represented \n    as a tuple (student, subject) and converted to a string with '@' separating the tuples' elements.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with student scores for the subjects.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> report = generate_student_scores()\n    >>> print(report)\n    >>> report['Score'].hist(bins=10)\n    \"\"\"\n    report_data = []\n\n    for student in STUDENTS:\n        for subject in SUBJECTS:\n            score = randint(0, 100)\n            report_data.append([f'{student}@{subject}', score])\n\n    report_df = pd.DataFrame(report_data, columns=['Student@Subject', 'Score'])\n\n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, math, matplotlib\n```python\nimport numpy as np\nfrom math import sin, cos\nimport matplotlib.pyplot as plt\n\n# Constants\nANGLES = np.linspace(0, 2*np.pi, 100)\n\ndef plot_sine_cosine_curves():\n    \"\"\"\n    Plot sine and cosine curves for a range of angles. Each angle is represented as a tuple \n    (angle, sin(angle) or cos(angle)) and converted to a string with '@' separating the tuples' elements.\n    \n    Requirements:\n    - numpy\n    - math\n    - matplotlib.pyplot\n    \n    Example:\n    >>> plot_sine_cosine_curves()\n    \"\"\"\n    sine_data = [f\"{angle}@{sin(angle)}\" for angle in ANGLES]\n    cosine_data = [f\"{angle}@{cos(angle)}\" for angle in ANGLES]\n\n    plt.figure(figsize=(10,5))\n    plt.plot(sine_data, label='sin')\n    plt.plot(cosine_data, label='cos')\n    plt.legend()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, datetime, pytz\n```python\nimport numpy as np\nfrom datetime import datetime\nimport pytz\n\n# Constants\nDATES = np.array(['2022-01-01', '2022-02-01', '2022-03-01', '2022-04-01', '2022-05-01'])\n\ndef convert_dates_to_utc():\n    \"\"\"\n    Convert a list of dates in a specific timezone to UTC. Each date is represented as a tuple \n    (original_date, utc_date) and converted to a string with '@' separating the tuples' elements.\n\n    Returns:\n    numpy.ndarray: A numpy array with original dates and corresponding UTC dates.\n    \n    Requirements:\n    - numpy\n    - datetime\n    - pytz\n    \n    Example:\n    >>> utc_dates = convert_dates_to_utc()\n    >>> print(utc_dates)\n    \"\"\"\n    timezone = pytz.timezone('America/New_York')\n\n    utc_dates = []\n\n    for date_str in DATES:\n        original_date = datetime.strptime(date_str, '%Y-%m-%d').replace(tzinfo=timezone)\n        utc_date = original_date.astimezone(pytz.UTC)\n        utc_dates.append(f\"{original_date}@{utc_date}\")\n\n    return np.array(utc_dates)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, pandas, sklearn.preprocessing\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\n# Constants\nCATEGORIES = np.array(['dog', 'cat', 'bird', 'fish'])\n\ndef encode_categories():\n    \"\"\"\n    Encode a list of categories into numerical values. Each category is represented as a tuple \n    (category, encoded_value) and converted to a string with '@' separating the tuples' elements.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with original categories and corresponding encoded values.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.preprocessing.LabelEncoder\n    \n    Example:\n    >>> encoded_categories = encode_categories()\n    >>> print(encoded_categories)\n    \"\"\"\n    encoder = LabelEncoder()\n    encoded_values = encoder.fit_transform(CATEGORIES)\n\n    encoded_categories_df = pd.DataFrame(\n        np.array([f\"{category}@{value}\" for category, value in zip(CATEGORIES, encoded_values)]),\n        columns=['Category@Encoded Value']\n    )\n\n    return encoded_categories_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, pandas, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nNUMBERS = np.arange(1, 101)\n\ndef plot_squared_numbers():\n    \"\"\"\n    Plot a graph of numbers and their squares. Each number is represented as a tuple \n    (number, square) and converted to a string with '@' separating the tuples' elements.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    \n    Example:\n    >>> plot_squared_numbers()\n    \"\"\"\n    squared_numbers = [f\"{number}@{number**2}\" for number in NUMBERS]\n\n    plt.figure(figsize=(10,5))\n    plt.plot(squared_numbers)\n    plt.xlabel('Number')\n    plt.ylabel('Square')\n    plt.title('Number vs. Square')\n    plt.show()\n```"}
{"seed": "{\"task_id\": 29696641, \"prompt\": \"def f_29696641(teststr):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"[i for i in teststr if re.search('\\\\\\\\d+[xX]', i)]\", \"test_start\": \"\\nimport re\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(['1 FirstString', '2x Sec String', '3rd String', 'x forString', '5X fifth']) == ['2x Sec String', '5X fifth']\\n\", \"\\n    assert candidate(['1x', '2', '3X', '4x random', '5X random']) == ['1x', '3X', '4x random', '5X random']\\n\", \"\\n    assert candidate(['1x', '2', '3X', '4xrandom', '5Xrandom']) == ['1x', '3X', '4xrandom', '5Xrandom']\\n\"], \"entry_point\": \"f_29696641\", \"intent\": \"Get all matches with regex pattern `\\\\\\\\d+[xX]` in list of string `teststr`\", \"library\": [\"re\"]}", "generation": "Scenario 1: \nnumpy, re, pandas, matplotlib\n```python\nimport numpy as np\nimport re\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nPATTERN = '\\d+[xX]'\nSAMPLE_DATA = ['1x', '2', '3X', '4x random', '5X random', '6X', '7x', '8', '9X', '10x random', '11X random']\n\ndef plot_frequency_of_matches(teststr):\n    \"\"\"\n    Find all matches with regex pattern in a list of strings and plot a bar chart showing the frequency of these matches.\n    \n    Parameters:\n    teststr (list): The list of strings to search.\n    \n    Returns:\n    list: The list of matches.\n    \n    Requirements:\n    - numpy\n    - re\n    - pandas\n    - matplotlib.pyplot\n    \n    Example:\n    >>> plot_frequency_of_matches(SAMPLE_DATA)\n    \"\"\"\n    matches = [i for i in teststr if re.search(PATTERN, i)]\n    \n    df = pd.DataFrame(matches, columns=['Match'])\n    df['Match'].value_counts().plot(kind='bar')\n    \n    plt.show()\n    \n    return matches\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nnumpy, re, os, glob\n```python\nimport numpy as np\nimport re\nimport os\nimport glob\n\n# Constants\nPATTERN = '\\d+[xX]'\nSAMPLE_DIR = './sample_dir'\n\ndef find_files_with_matches(directory):\n    \"\"\"\n    Find all text files in a given directory where the content matches a regex pattern.\n    \n    Parameters:\n    directory (str): The directory to search.\n    \n    Returns:\n    list: The list of files with matches.\n    \n    Requirements:\n    - numpy\n    - re\n    - os\n    - glob\n    \n    Example:\n    >>> find_files_with_matches(SAMPLE_DIR)\n    \"\"\"\n    files_with_matches = []\n    \n    # Get all text files in the directory\n    files = glob.glob(os.path.join(directory, '*.txt'))\n    \n    for file in files:\n        with open(file, 'r') as f:\n            content = f.read()\n            if re.search(PATTERN, content):\n                files_with_matches.append(file)\n    \n    return files_with_matches\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnumpy, re, json, collections\n```python\nimport numpy as np\nimport re\nimport json\nfrom collections import Counter\n\n# Constants\nPATTERN = '\\d+[xX]'\nSAMPLE_JSON = '{\"data\": [\"1x\", \"2\", \"3X\", \"4x random\", \"5X random\", \"6X\", \"7x\", \"8\", \"9X\", \"10x random\", \"11X random\"]}'\n\ndef count_matches_in_json(json_str):\n    \"\"\"\n    Count the frequency of matches with a regex pattern in a list of strings in a JSON string.\n    \n    Parameters:\n    json_str (str): The JSON string.\n    \n    Returns:\n    dict: The dictionary of matches and their frequencies.\n    \n    Requirements:\n    - numpy\n    - re\n    - json\n    - collections.Counter\n    \n    Example:\n    >>> count_matches_in_json(SAMPLE_JSON)\n    \"\"\"\n    data = json.loads(json_str)['data']\n    matches = [i for i in data if re.search(PATTERN, i)]\n    \n    return dict(Counter(matches))\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, re, csv, pandas\n```python\nimport numpy as np\nimport re\nimport csv\nimport pandas as pd\n\n# Constants\nPATTERN = '\\d+[xX]'\nSAMPLE_CSV = './sample.csv'\n\ndef find_matches_in_csv(csv_file):\n    \"\"\"\n    Find all matches with a regex pattern in a list of strings in a CSV file.\n    \n    Parameters:\n    csv_file (str): The CSV file.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the matches.\n    \n    Requirements:\n    - numpy\n    - re\n    - csv\n    - pandas\n    \n    Example:\n    >>> find_matches_in_csv(SAMPLE_CSV)\n    \"\"\"\n    df = pd.read_csv(csv_file)\n    \n    matches = df[df['data'].str.contains(PATTERN)]\n    \n    return matches\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnumpy, re, sqlite3, pandas\n```python\nimport numpy as np\nimport re\nimport sqlite3\nimport pandas as pd\n\n# Constants\nPATTERN = '\\d+[xX]'\nSAMPLE_DB = './sample.db'\n\ndef find_matches_in_db(db_file, table_name, column_name):\n    \"\"\"\n    Find all matches with a regex pattern in a list of strings in a SQL database.\n    \n    Parameters:\n    db_file (str): The SQLite database file.\n    table_name (str): The name of the table to search.\n    column_name (str): The name of the column to search.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the matches.\n    \n    Requirements:\n    - numpy\n    - re\n    - sqlite3\n    - pandas\n    \n    Example:\n    >>> find_matches_in_db(SAMPLE_DB, 'test_table', 'test_column')\n    \"\"\"\n    conn = sqlite3.connect(db_file)\n    df = pd.read_sql_query(f\"SELECT * FROM {table_name}\", conn)\n    \n    matches = df[df[column_name].str.contains(PATTERN)]\n    \n    return matches\n```\n"}
{"seed": "{\"task_id\": 15315452, \"prompt\": \"def f_15315452(df):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"df['A'][(df['B'] > 50) & (df['C'] == 900)]\", \"test_start\": \"\\nimport pandas as pd\\n\\ndef check(candidate):\", \"test\": [\"\\n    df = pd.DataFrame({'A': [7, 7, 4, 4, 7, 7, 3, 9, 6, 3], 'B': [20, 80, 90, 30, 80, 60, 80, 40, 40 ,10], 'C': [300, 700, 100, 900, 200, 800, 900, 100, 100, 600]})\\n    assert candidate(df).to_dict() == {6: 3}\\n\", \"\\n    df1 = pd.DataFrame({'A': [9, 9, 5, 8, 7, 9, 2, 2, 5, 7], 'B': [40, 70, 70, 80, 50, 30, 80, 80, 80, 70], 'C': [300, 700, 900, 900, 200, 900, 700, 400, 300, 800]})\\n    assert candidate(df1).to_dict() == {2: 5, 3: 8}\\n\", \"\\n    df2 = pd.DataFrame({'A': [3, 4, 5, 6], 'B': [-10, 50, 20, 10], 'C': [900, 800, 900, 900]})\\n    assert candidate(df2).to_dict() == {}\\n\"], \"entry_point\": \"f_15315452\", \"intent\": \"select values from column 'A' for which corresponding values in column 'B' will be greater than 50, and in column 'C' - equal 900 in dataframe `df`\", \"library\": [\"pandas\"]}", "generation": "Scenario 1:\npandas, numpy, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['A', 'B', 'C']\nRANGE = np.arange(1, 101)\n\ndef plot_histogram(df):\n    \"\"\"\n    For a given DataFrame `df`, select rows where values in column 'B' are greater than 50,\n    and values in column 'C' are equal to 900. Plot a histogram of values from column 'A' \n    for the selected rows.\n\n    Parameters:\n    df (DataFrame): The input pandas DataFrame.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame({'A': np.random.choice(RANGE, 1000),\n                           'B': np.random.choice(RANGE, 1000),\n                           'C': np.random.choice([900, 800, 700, 600], 1000)})\n    >>> plot_histogram(df)\n    \"\"\"\n    selected = df['A'][(df['B'] > 50) & (df['C'] == 900)]\n    plt.hist(selected, bins=20, edgecolor='black')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, sklearn.preprocessing, seaborn\n```python\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport seaborn as sns\n\n# Constants\nCOLUMNS = ['A', 'B', 'C']\n\ndef plot_standardized_distribution(df):\n    \"\"\"\n    For a given DataFrame `df`, select rows where values in column 'B' are \n    greater than 50, and in column 'C' are equal to 900. Standardize the \n    values from column 'A' for the selected rows and plot the distribution.\n\n    Parameters:\n    df (DataFrame): The input pandas DataFrame.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - sklearn.preprocessing\n    - seaborn\n\n    Example:\n    >>> df = pd.DataFrame({'A': np.random.randint(0, 100, 1000),\n                           'B': np.random.randint(0, 100, 1000),\n                           'C': np.random.choice([900, 800, 700, 600], 1000)})\n    >>> plot_standardized_distribution(df)\n    \"\"\"\n    selected = df['A'][(df['B'] > 50) & (df['C'] == 900)]\n    standardized = StandardScaler().fit_transform(selected.values.reshape(-1, 1))\n    sns.distplot(standardized)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, statsmodels\n```python\nimport pandas as pd\nimport numpy as np\nfrom statsmodels.tsa.stattools import adfuller\n\n# Constants\nCOLUMNS = ['A', 'B', 'C']\n\ndef check_stationarity(df):\n    \"\"\"\n    For a given DataFrame `df`, select rows where values in column 'B' are \n    greater than 50, and in column 'C' are equal to 900. Test the stationarity \n    of the values from column 'A' for the selected rows using the Augmented \n    Dickey-Fuller test.\n\n    Parameters:\n    df (DataFrame): The input pandas DataFrame.\n\n    Returns:\n    bool: True if the series is stationary, False otherwise.\n\n    Requirements:\n    - pandas\n    - numpy\n    - statsmodels.tsa.stattools\n\n    Example:\n    >>> df = pd.DataFrame({'A': np.random.randint(0, 100, 1000),\n                           'B': np.random.randint(0, 100, 1000),\n                           'C': np.random.choice([900, 800, 700, 600], 1000)})\n    >>> check_stationarity(df)\n    \"\"\"\n    selected = df['A'][(df['B'] > 50) & (df['C'] == 900)]\n    result = adfuller(selected)\n    return result[1] < 0.05\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, scipy.stats\n```python\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import chi2_contingency\n\n# Constants\nCOLUMNS = ['A', 'B', 'C']\n\ndef chi_square_test(df):\n    \"\"\"\n    For a given DataFrame `df`, select rows where values in column 'B' are \n    greater than 50, and in column 'C' are equal to 900. Perform a Chi-Square \n    test of independence on the values from column 'A' for the selected rows \n    against the values from column 'B'.\n\n    Parameters:\n    df (DataFrame): The input pandas DataFrame.\n\n    Returns:\n    float: The p-value of the Chi-Square test.\n\n    Requirements:\n    - pandas\n    - numpy\n    - scipy.stats\n\n    Example:\n    >>> df = pd.DataFrame({'A': np.random.choice(['Yes', 'No'], 1000),\n                           'B': np.random.randint(0, 100, 1000),\n                           'C': np.random.choice([900, 800, 700, 600], 1000)})\n    >>> chi_square_test(df)\n    \"\"\"\n    selected = df[(df['B'] > 50) & (df['C'] == 900)][['A', 'B']]\n    contingency_table = pd.crosstab(selected['A'], selected['B'])\n    _, p_value, _, _ = chi2_contingency(contingency_table)\n    return p_value\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, sklearn.model_selection, sklearn.linear_model\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\n# Constants\nCOLUMNS = ['A', 'B', 'C']\n\ndef predict_values(df):\n    \"\"\"\n    For a given DataFrame `df`, select rows where values in column 'B' are \n    greater than 50, and in column 'C' are equal to 900. Use the values from \n    column 'A' for the selected rows to predict the values in column 'B' \n    using linear regression.\n\n    Parameters:\n    df (DataFrame): The input pandas DataFrame.\n\n    Returns:\n    ndarray: The predicted values for column 'B'.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.model_selection\n    - sklearn.linear_model\n\n    Example:\n    >>> df = pd.DataFrame({'A': np.random.randint(0, 100, 1000),\n                           'B': np.random.randint(0, 100, 1000),\n                           'C': np.random.choice([900, 800, 700, 600], 1000)})\n    >>> predict_values(df)\n    \"\"\"\n    selected = df[(df['B'] > 50) & (df['C'] == 900)][['A', 'B']]\n    X_train, X_test, y_train, _ = train_test_split(selected['A'].values.reshape(-1, 1), selected['B'].values, test_size=0.2)\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    predictions = model.predict(X_test)\n    return predictions\n```"}
{"seed": "{\"task_id\": 4642501, \"prompt\": \"def f_4642501(o):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"sorted(o.items())\", \"test_start\": \"\\nimport pandas as pd\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate({1:\\\"abc\\\", 5:\\\"klm\\\", 2:\\\"pqr\\\"}) == [(1, \\\"abc\\\"), (2, \\\"pqr\\\"), (5, \\\"klm\\\")]\\n\", \"\\n    assert candidate({4.221:\\\"uwv\\\", -1.009:\\\"pow\\\"}) == [(-1.009, 'pow'), (4.221, 'uwv')]\\n\", \"\\n    assert candidate({\\\"as2q\\\":\\\"piqr\\\", \\\"#wwq\\\":\\\"say\\\", \\\"Rwc\\\":\\\"koala\\\", \\\"35\\\":\\\"kangaroo\\\"}) == [('#wwq', 'say'), ('35', 'kangaroo'), ('Rwc', 'koala'), ('as2q', 'piqr')]\\n\"], \"entry_point\": \"f_4642501\", \"intent\": \"Sort dictionary `o` in ascending order based on its keys and items\", \"library\": [\"pandas\"]}", "generation": "Scenario 1: \npandas, numpy, collections\n```python\nimport pandas as pd\nimport numpy as np\nfrom collections import Counter\n\n# Constants\nDATA = {\n    'Name': ['Tom', 'Nick', 'John', 'Tom', 'John', 'John', 'Nick', 'Tom', 'John', 'Tom'],\n    'Age': [20, 21, 19, 20, 19, 19, 21, 20, 19, 20],\n    'Score': [85, 79, 92, 88, 90, 92, 81, 86, 90, 85]\n}\n\ndef analyze_student_data(data):\n    \"\"\"\n    Analyze a dictionary of student data. The function should return a dataframe sorted by name and age in ascending order, \n    the average score per student, and the most common age.\n\n    Parameters:\n    data (dict): The student data.\n\n    Returns:\n    DataFrame, Series, int: The sorted dataframe, average scores, and most common age.\n\n    Requirements:\n    - pandas\n    - numpy\n    - collections\n\n    Example:\n    >>> analyze_student_data(DATA)\n    \"\"\"\n    df = pd.DataFrame(data).sort_values(['Name', 'Age'])\n    \n    avg_scores = df.groupby('Name')['Score'].mean()\n    \n    age_counts = Counter(df['Age'])\n    most_common_age = age_counts.most_common(1)[0][0]\n    \n    return df, avg_scores, most_common_age\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, pandas, itertools\n```python\nimport numpy as np\nimport pandas as pd\nimport itertools\n\n# Constants\nMATRIX = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n\ndef sort_matrix_and_find_combinations(matrix):\n    \"\"\"\n    Sort a numpy array in ascending order and find all combinations of two elements in the sorted array.\n\n    Parameters:\n    matrix (numpy.array): The numpy array.\n\n    Returns:\n    numpy.array, list: The sorted array and the combinations.\n\n    Requirements:\n    - numpy\n    - pandas\n    - itertools\n\n    Example:\n    >>> sort_matrix_and_find_combinations(MATRIX)\n    \"\"\"\n    sorted_array = np.sort(matrix, axis=None)\n    \n    combinations = list(itertools.combinations(sorted_array, 2))\n    \n    return sorted_array, combinations\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, sklearn.preprocessing, matplotlib\n```python\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\n\n# Constants\nDATA = {\n    'Name': ['Product A', 'Product B', 'Product C', 'Product D', 'Product E'],\n    'Sales': [1000, 2000, 3000, 4000, 5000],\n    'Profit': [200, 500, 800, 1000, 1200],\n}\n\ndef preprocess_and_visualize_data(data):\n    \"\"\"\n    Preprocess a dictionary of product data by normalizing the sales and profit figures \n    and visualize the normalized data using a bar plot.\n\n    Parameters:\n    data (dict): The product data.\n\n    Returns:\n    DataFrame: The preprocessed dataframe.\n\n    Requirements:\n    - pandas\n    - sklearn.preprocessing\n    - matplotlib\n\n    Example:\n    >>> preprocess_and_visualize_data(DATA)\n    \"\"\"\n    df = pd.DataFrame(data)\n    \n    scaler = MinMaxScaler()\n    df[['Sales', 'Profit']] = scaler.fit_transform(df[['Sales', 'Profit']])\n    \n    df.plot(x='Name', y=['Sales', 'Profit'], kind='bar')\n    plt.show()\n    \n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, pandas, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nDATA = {\n    'X': [1, 2, 3, 4, 5],\n    'Y': [2, 4, 6, 8, 10],\n}\n\ndef calculate_statistics_and_plot_data(data):\n    \"\"\"\n    Calculate the mean and standard deviation of X and Y values in the dictionary and plot a scatter plot.\n\n    Parameters:\n    data (dict): The X and Y values.\n\n    Returns:\n    dict: The mean and standard deviation.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib\n\n    Example:\n    >>> calculate_statistics_and_plot_data(DATA)\n    \"\"\"\n    df = pd.DataFrame(data)\n    \n    statistics = {\n        'X_mean': np.mean(df['X']),\n        'X_std': np.std(df['X']),\n        'Y_mean': np.mean(df['Y']),\n        'Y_std': np.std(df['Y']),\n    }\n    \n    plt.scatter(df['X'], df['Y'])\n    plt.show()\n    \n    return statistics\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, sklearn.model_selection, sklearn.linear_model, numpy\n```python\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\n\n# Constants\nDATA = {\n    'Hours': [2.5, 5.1, 3.2, 8.5, 3.5, 1.5, 9.2, 5.5, 8.3, 2.7],\n    'Scores': [21, 47, 27, 75, 30, 20, 88, 60, 81, 25],\n}\n\ndef train_and_evaluate_model(data):\n    \"\"\"\n    Train a simple linear regression model on the given data and evaluate the model by calculating the mean squared error.\n\n    Parameters:\n    data (dict): The hours of study and scores.\n\n    Returns:\n    float: The mean squared error.\n\n    Requirements:\n    - pandas\n    - sklearn.model_selection\n    - sklearn.linear_model\n    - numpy\n\n    Example:\n    >>> train_and_evaluate_model(DATA)\n    \"\"\"\n    df = pd.DataFrame(data)\n    \n    X = df[['Hours']]\n    y = df['Scores']\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    \n    predictions = model.predict(X_test)\n    \n    mse = np.mean((y_test - predictions) ** 2)\n    \n    return mse\n```\n"}
{"seed": "{\"task_id\": 4642501, \"prompt\": \"def f_4642501(d):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"sorted(d)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate({1:\\\"abc\\\", 5:\\\"klm\\\", 2:\\\"pqr\\\"}) == [1, 2, 5]\\n\", \"\\n    assert candidate({4.221:\\\"uwv\\\", -1.009:\\\"pow\\\"}) == [-1.009, 4.221]\\n\", \"\\n    assert candidate({\\\"as2q\\\":\\\"piqr\\\", \\\"#wwq\\\":\\\"say\\\", \\\"Rwc\\\":\\\"koala\\\", \\\"35\\\":\\\"kangaroo\\\"}) == ['#wwq', '35', 'Rwc', 'as2q']\\n\"], \"entry_point\": \"f_4642501\", \"intent\": \"get sorted list of keys of dict `d`\", \"library\": []}", "generation": "Scenario 1:\nnumpy, pandas, matplotlib, random\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nCATEGORIES = ['Electronics', 'Clothing', 'Groceries', 'Books', 'Sports']\n\ndef generate_sales_report(sales_data_dict):\n    \"\"\"\n    Generate a sales report from the given dictionary and visualize the sales \n    data for different categories using a pie chart.\n    \n    Parameters:\n    sales_data_dict (dict): A dictionary where keys are categories and values are sales.\n\n    Returns:\n    DataFrame: A pandas DataFrame with sales data.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - random\n    \n    Example:\n    >>> sales_data = {'Electronics': 5000, 'Clothing': 4000, 'Groceries': 3000, 'Books': 2000, 'Sports': 1000}\n    >>> report = generate_sales_report(sales_data)\n    >>> print(report)\n    >>> report.plot.pie(y='Sales', labels=report['Category'], autopct='%1.1f%%')\n    \"\"\"\n    report_df = pd.DataFrame(list(sales_data_dict.items()), columns=['Category', 'Sales'])\n\n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncollections, numpy, matplotlib\n```python\nimport collections\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_word_frequency(word_list):\n    \"\"\"\n    Plot a histogram of word frequencies from a list of words using collections and matplotlib.\n    \n    Parameters:\n    word_list (list): A list of words.\n\n    Returns:\n    None\n    \n    Requirements:\n    - collections\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> words = ['apple', 'banana', 'cherry', 'banana', 'cherry', 'cherry']\n    >>> plot_word_frequency(words)\n    \"\"\"\n    counter = collections.Counter(word_list)\n    labels, values = zip(*counter.items())\n\n    indexes = np.arange(len(labels))\n    width = 1\n\n    plt.bar(indexes, values, width)\n    plt.xticks(indexes + width * 0.5, labels)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nitertools, math, random\n```python\nimport itertools\nimport math\nfrom random import shuffle\n\n# Constants\nNUMBERS = list(range(1, 11)) # Numbers from 1 to 10\n\ndef calculate_random_permutation_sum():\n    \"\"\"\n    Shuffle a list of numbers, calculate the sum of the differences between each pair of consecutive numbers, \n    and repeat this process for all permutations. Finally, return the average of these sums.\n    \n    Returns:\n    float: The average sum of differences.\n\n    Requirements:\n    - itertools\n    - math\n    - random\n\n    Example:\n    >>> calculate_random_permutation_sum()\n    \"\"\"\n    shuffle(NUMBERS)\n    permutations = list(itertools.permutations(NUMBERS))\n\n    sum_diffs = 0\n    for perm in permutations:\n        diffs = [abs(perm[i] - perm[i+1]) for i in range(len(perm)-1)]\n        sum_diffs += sum(diffs)\n\n    avg_sum_diffs = sum_diffs / len(permutations)\n    \n    return avg_sum_diffs\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, matplotlib, seaborn, numpy\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\nROWS = 100\n\ndef generate_and_visualize_dataframe():\n    \"\"\"\n    Generate a pandas DataFrame with random values, then visualize the correlation \n    matrix of this DataFrame using seaborn's heatmap.\n    \n    Returns:\n    DataFrame: The generated DataFrame.\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - seaborn\n    - numpy\n\n    Example:\n    >>> df = generate_and_visualize_dataframe()\n    >>> print(df)\n    \"\"\"\n    df = pd.DataFrame(np.random.rand(ROWS, len(COLUMNS)), columns=COLUMNS)\n\n    corr_matrix = df.corr()\n    sns.heatmap(corr_matrix, annot=True)\n    plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\ncollections, random, string\n```python\nimport collections\nimport random\nimport string\n\n# Constants\nLENGTH = 100\n\ndef count_random_string_characters():\n    \"\"\"\n    Generate a random string of a certain length, then count the occurrence of each character using collections.\n\n    Returns:\n    dict: A dictionary where keys are characters and values are counts.\n\n    Requirements:\n    - collections\n    - random\n    - string\n\n    Example:\n    >>> count_random_string_characters()\n    \"\"\"\n    random_string = ''.join(random.choices(string.ascii_uppercase + string.ascii_lowercase, k=LENGTH))\n\n    char_counts = collections.Counter(random_string)\n\n    return dict(char_counts)\n```\n"}
{"seed": "{\"task_id\": 4642501, \"prompt\": \"def f_4642501(d):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"sorted(d.items())\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    d = {'a': [1, 2, 3], 'c': ['one', 'two'], 'b': ['blah', 'bhasdf', 'asdf'], 'd': ['asdf', 'wer', 'asdf', 'zxcv']}\\n    assert candidate(d) == [('a', [1, 2, 3]), ('b', ['blah', 'bhasdf', 'asdf']), ('c', ['one', 'two']), ('d', ['asdf', 'wer', 'asdf', 'zxcv'])]\\n\"], \"entry_point\": \"f_4642501\", \"intent\": \"sort dictionaries `d` by keys\", \"library\": []}", "generation": "Scenario 1: \ncollections, itertools, operator, random\n```python\nfrom collections import Counter\nimport itertools\nimport operator\nimport random\n\n# Constants\nLETTERS = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n\ndef generate_random_dict_and_sort(d):\n    \"\"\"\n    Generate a dictionary where keys are random letters and values are lists of random integers. \n    Then sort the dictionary by the sum of the values in descending order.\n    \n    Parameters:\n    d (dict): The input dictionary.\n    \n    Returns:\n    dict: The sorted dictionary.\n    \n    Requirements:\n    - collections\n    - itertools\n    - operator\n    - random\n    \n    Example:\n    >>> d = {'a': [1, 2, 3], 'c': ['one', 'two'], 'b': ['blah', 'bhasdf', 'asdf'], 'd': ['asdf', 'wer', 'asdf', 'zxcv']}\n    >>> sorted_dict = generate_random_dict_and_sort(d)\n    >>> print(sorted_dict)\n    \"\"\"\n    random_dict = {k: [random.randint(0, 100) for _ in range(random.randint(1, 10))] for k in LETTERS}\n    sorted_dict = dict(sorted(random_dict.items(), key=lambda item: sum(item[1]), reverse=True))\n    return sorted_dict\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \ncollections, operator, random, numpy\n```python\nfrom collections import Counter\nimport operator\nimport random\nimport numpy as np\n\nLETTERS = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n\ndef generate_dict_and_calculate_mean(d):\n    \"\"\"\n    Generate a dictionary where keys are random letters and values are lists of random integers. \n    Then calculate the mean of the values for each key and return a dictionary with these means.\n    \n    Parameters:\n    d (dict): The input dictionary.\n    \n    Returns:\n    dict: The dictionary with the means.\n    \n    Requirements:\n    - collections\n    - operator\n    - random\n    - numpy\n    \n    Example:\n    >>> d = {'a': [1, 2, 3], 'c': ['one', 'two'], 'b': ['blah', 'bhasdf', 'asdf'], 'd': ['asdf', 'wer', 'asdf', 'zxcv']}\n    >>> mean_dict = generate_dict_and_calculate_mean(d)\n    >>> print(mean_dict)\n    \"\"\"\n    random_dict = {k: [random.randint(0, 100) for _ in range(random.randint(1, 10))] for k in LETTERS}\n    mean_dict = {k: np.mean(v) for k, v in random_dict.items()}\n    return mean_dict\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \ncollections, itertools, operator, random\n```python\nfrom collections import Counter\nimport itertools\nimport operator\nimport random\n\nLETTERS = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n\ndef generate_dict_and_count_values(d):\n    \"\"\"\n    Generate a dictionary where keys are random letters and values are lists of random integers. \n    Then count the occurrence of each integer in the values and return a dictionary with these counts.\n    \n    Parameters:\n    d (dict): The input dictionary.\n    \n    Returns:\n    dict: The dictionary with the counts.\n    \n    Requirements:\n    - collections\n    - itertools\n    - operator\n    - random\n    \n    Example:\n    >>> d = {'a': [1, 2, 3], 'c': ['one', 'two'], 'b': ['blah', 'bhasdf', 'asdf'], 'd': ['asdf', 'wer', 'asdf', 'zxcv']}\n    >>> count_dict = generate_dict_and_count_values(d)\n    >>> print(count_dict)\n    \"\"\"\n    random_dict = {k: [random.randint(0, 100) for _ in range(random.randint(1, 10))] for k in LETTERS}\n    count_dict = Counter(itertools.chain.from_iterable(random_dict.values()))\n    return dict(count_dict)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \ncollections, operator, random, math\n```python\nfrom collections import Counter\nimport operator\nimport random\nimport math\n\nLETTERS = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n\ndef generate_dict_and_calculate_sd(d):\n    \"\"\"\n    Generate a dictionary where keys are random letters and values are lists of random integers. \n    Then calculate the standard deviation of the values for each key and return a dictionary with these standard deviations.\n    \n    Parameters:\n    d (dict): The input dictionary.\n    \n    Returns:\n    dict: The dictionary with the standard deviations.\n    \n    Requirements:\n    - collections\n    - operator\n    - random\n    - math\n    \n    Example:\n    >>> d = {'a': [1, 2, 3], 'c': ['one', 'two'], 'b': ['blah', 'bhasdf', 'asdf'], 'd': ['asdf', 'wer', 'asdf', 'zxcv']}\n    >>> sd_dict = generate_dict_and_calculate_sd(d)\n    >>> print(sd_dict)\n    \"\"\"\n    random_dict = {k: [random.randint(0, 100) for _ in range(random.randint(1, 10))] for k in LETTERS}\n    sd_dict = {k: math.sqrt(sum((i - sum(v) / len(v)) ** 2 for i in v) / len(v)) for k, v in random_dict.items()}\n    return sd_dict\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \ncollections, operator, random, statistics\n```python\nfrom collections import Counter\nimport operator\nimport random\nimport statistics\n\nLETTERS = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n\ndef generate_dict_and_calculate_median(d):\n    \"\"\"\n    Generate a dictionary where keys are random letters and values are lists of random integers. \n    Then calculate the median of the values for each key and return a dictionary with these medians.\n    \n    Parameters:\n    d (dict): The input dictionary.\n    \n    Returns:\n    dict: The dictionary with the medians.\n    \n    Requirements:\n    - collections\n    - operator\n    - random\n    - statistics\n    \n    Example:\n    >>> d = {'a': [1, 2, 3], 'c': ['one', 'two'], 'b': ['blah', 'bhasdf', 'asdf'], 'd': ['asdf', 'wer', 'asdf', 'zxcv']}\n    >>> median_dict = generate_dict_and_calculate_median(d)\n    >>> print(median_dict)\n    \"\"\"\n    random_dict = {k: [random.randint(0, 100) for _ in range(random.randint(1, 10))] for k in LETTERS}\n    median_dict = {k: statistics.median(v) for k, v in random_dict.items()}\n    return median_dict\n```\n`GPT_ODEX_BREAK`"}
{"seed": "{\"task_id\": 642154, \"prompt\": \"def f_642154():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"int('1')\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate() == 1\\n\", \"\\n    assert candidate() + 1 == 2\\n\"], \"entry_point\": \"f_642154\", \"intent\": \"convert string \\\"1\\\" into integer\", \"library\": []}", "generation": "Scenario 1:\npandas, numpy, matplotlib, random\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nSTUDENTS = ['John', 'Alice', 'Bob', 'Cindy', 'David']\nSUBJECTS = ['Math', 'English', 'Science', 'History', 'Geography']\n\ndef generate_student_grades():\n    \"\"\"\n    Generate a report of student grades for different subjects and plot the average grades for each student.\n\n    Returns:\n    DataFrame: A pandas DataFrame with student grades.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> report = generate_student_grades()\n    >>> print(report)\n    >>> report.mean().plot(kind='bar')\n    \"\"\"\n    grade_data = []\n\n    for student in STUDENTS:\n        grades = [randint(1, 100) for _ in SUBJECTS]\n        grade_data.append(grades)\n\n    grade_df = pd.DataFrame(grade_data, columns=SUBJECTS, index=STUDENTS)\n\n    return grade_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, re, datetime\n```python\nimport os\nimport re\nfrom datetime import datetime\n\n# Constants\nLOG_DIR = '/var/log/'\n\ndef find_recent_log_file(pattern):\n    \"\"\"\n    Find the most recent log file in a directory that matches a given regex pattern.\n\n    Parameters:\n    pattern (str): The regex pattern to match the log files.\n\n    Returns:\n    str: The path of the most recent log file.\n\n    Requirements:\n    - os\n    - re\n    - datetime\n\n    Example:\n    >>> find_recent_log_file(r'^access.log.[0-9]+$')\n    \"\"\"\n    log_files = [f for f in os.listdir(LOG_DIR) if re.match(pattern, f)]\n    log_files = sorted(log_files, key=lambda f: os.path.getmtime(os.path.join(LOG_DIR, f)), reverse=True)\n\n    return os.path.join(LOG_DIR, log_files[0]) if log_files else None\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ncsv, collections, operator\n```python\nimport csv\nimport collections\nimport operator\n\n# Constants\nCSV_FILE = 'sales.csv'\n\ndef find_top_selling_product():\n    \"\"\"\n    Find the top selling product from a CSV file containing sales data.\n\n    Returns:\n    str: The name of the top selling product.\n\n    Requirements:\n    - csv\n    - collections\n    - operator\n\n    Example:\n    >>> find_top_selling_product()\n    \"\"\"\n    with open(CSV_FILE, 'r') as f:\n        reader = csv.reader(f)\n        next(reader) # skip header\n        sales_data = collections.defaultdict(int)\n        for row in reader:\n            product, quantity = row[0], int(row[1])\n            sales_data[product] += quantity\n\n    top_selling_product = max(sales_data.items(), key=operator.itemgetter(1))[0]\n\n    return top_selling_product\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\njson, urllib.request, datetime\n```python\nimport json\nimport urllib.request\nfrom datetime import datetime\n\n# Constants\nAPI_URL = 'http://api.example.com/data'\n\ndef fetch_latest_data():\n    \"\"\"\n    Fetch the latest data from a JSON API and convert the timestamp to a datetime object.\n\n    Returns:\n    dict: The latest data with the timestamp converted to a datetime object.\n\n    Requirements:\n    - json\n    - urllib.request\n    - datetime\n\n    Example:\n    >>> fetch_latest_data()\n    \"\"\"\n    with urllib.request.urlopen(API_URL) as url:\n        data = json.loads(url.read().decode())\n    latest_data = max(data, key=lambda x: x['timestamp'])\n    latest_data['timestamp'] = datetime.fromtimestamp(latest_data['timestamp'])\n\n    return latest_data\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nsqlite3, pandas, matplotlib\n```python\nimport sqlite3\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nDB_FILE = 'sales.db'\n\ndef plot_monthly_sales():\n    \"\"\"\n    Plot the monthly sales from a SQLite database.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the monthly sales data.\n\n    Requirements:\n    - sqlite3\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = plot_monthly_sales()\n    >>> print(df)\n    >>> df.plot(kind='bar')\n    \"\"\"\n    conn = sqlite3.connect(DB_FILE)\n    query = \"SELECT strftime('%Y-%m', sale_date) as month, SUM(amount) as total_sales FROM sales GROUP BY month\"\n    df = pd.read_sql_query(query, conn)\n    conn.close()\n\n    return df\n```"}
{"seed": "{\"task_id\": 642154, \"prompt\": \"def f_642154(T1):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"[list(map(int, x)) for x in T1]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    T1 = (('13', '17', '18', '21', '32'), ('07', '11', '13', '14', '28'), ('01', '05', '06', '08', '15', '16'))\\n    assert candidate(T1) == [[13, 17, 18, 21, 32], [7, 11, 13, 14, 28], [1, 5, 6, 8, 15, 16]]\\n\"], \"entry_point\": \"f_642154\", \"intent\": \"convert items in `T1` to integers\", \"library\": []}", "generation": "Scenario 1:\npandas, numpy, itertools, random\n```python\nimport pandas as pd\nimport numpy as np\nimport itertools\nfrom random import randint\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\nROW_NUM = 50\nSEED = 2022\n\ndef create_random_dataframe(T1):\n    \"\"\"\n    Convert items in `T1` to integers and create a pandas DataFrame with random \n    numbers, where the number of columns is the sum of the integers and the number of \n    rows is defined by the ROW_NUM constant. \n    \n    Parameters:\n    T1 (tuple): A tuple of tuples, each containing string representations of integers.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with random numbers.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - itertools\n    - random\n    \n    Example:\n    >>> T1 = (('13', '17', '18', '21', '32'), ('07', '11', '13', '14', '28'), ('01', '05', '06', '08', '15', '16'))\n    >>> df = create_random_dataframe(T1)\n    >>> print(df)\n    \"\"\"\n    np.random.seed(SEED)\n    int_list = [list(map(int, x)) for x in T1]\n    flattened_list = list(itertools.chain(*int_list))\n    total_cols = sum(flattened_list)\n\n    data = np.random.randint(0, 100, size=(ROW_NUM, total_cols))\n    df = pd.DataFrame(data, columns=[f'Col_{i+1}' for i in range(total_cols)])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nmatplotlib, itertools, numpy, random\n```python\nimport matplotlib.pyplot as plt\nimport itertools\nfrom random import randint\nimport numpy as np\n\n# Constants\nCOLORS = ['r', 'g', 'b', 'y', 'm', 'c', 'k']\nMARKERS = ['o', 's', '*', '+', 'x', 'D', '|', '_']\n\ndef plot_scatter_graphs(T1):\n    \"\"\"\n    Convert items in `T1` to integers and plot a scatter graph for each tuple in `T1`.\n    The x and y values of the scatter plot are random numbers, and the number of points \n    is the sum of the integers in each tuple.\n    \n    Parameters:\n    T1 (tuple): A tuple of tuples, each containing string representations of integers.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - matplotlib.pyplot\n    - itertools\n    - numpy\n    - random\n    \n    Example:\n    >>> T1 = (('13', '17', '18', '21', '32'), ('07', '11', '13', '14', '28'), ('01', '05', '06', '08', '15', '16'))\n    >>> plot_scatter_graphs(T1)\n    \"\"\"\n    color_marker = list(itertools.product(COLORS, MARKERS))\n\n    for i, tup in enumerate(T1):\n        int_list = list(map(int, tup))\n        total_points = sum(int_list)\n\n        x = np.random.rand(total_points)\n        y = np.random.rand(total_points)\n        color, marker = color_marker[i % len(color_marker)]\n\n        plt.scatter(x, y, color=color, marker=marker)\n        plt.title(f'Graph {i+1}')\n        plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ncollections, itertools, numpy, random\n```python\nfrom collections import Counter\nimport itertools\nfrom random import randint\nimport numpy as np\n\n# Constants\nRANGE = 100\n\ndef count_occurrences(T1):\n    \"\"\"\n    Convert items in `T1` to integers and create a list of random integers. \n    The size of the list is the sum of the integers in `T1`. Count the occurrences \n    of each number in the list using a Counter.\n    \n    Parameters:\n    T1 (tuple): A tuple of tuples, each containing string representations of integers.\n    \n    Returns:\n    Counter: A Counter object with the count of each number in the list.\n    \n    Requirements:\n    - collections.Counter\n    - itertools\n    - numpy\n    - random\n    \n    Example:\n    >>> T1 = (('13', '17', '18', '21', '32'), ('07', '11', '13', '14', '28'), ('01', '05', '06', '08', '15', '16'))\n    >>> counts = count_occurrences(T1)\n    >>> print(counts)\n    \"\"\"\n    int_list = [list(map(int, x)) for x in T1]\n    flattened_list = list(itertools.chain(*int_list))\n    total_nums = sum(flattened_list)\n\n    random_nums = [randint(0, RANGE) for _ in range(total_nums)]\n    counts = Counter(random_nums)\n\n    return counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, itertools, random, statistics\n```python\nimport numpy as np\nimport itertools\nfrom random import randint\nimport statistics\n\n# Constants\nRANGE = 100\n\ndef calculate_statistics(T1):\n    \"\"\"\n    Convert items in `T1` to integers and create a list of random integers. \n    The size of the list is the sum of the integers in `T1`. Calculate and \n    return the mean, median, and mode of the list.\n    \n    Parameters:\n    T1 (tuple): A tuple of tuples, each containing string representations of integers.\n    \n    Returns:\n    tuple: A tuple (mean, median, mode) of the list.\n    \n    Requirements:\n    - numpy\n    - itertools\n    - random\n    - statistics\n    \n    Example:\n    >>> T1 = (('13', '17', '18', '21', '32'), ('07', '11', '13', '14', '28'), ('01', '05', '06', '08', '15', '16'))\n    >>> stats = calculate_statistics(T1)\n    >>> print(stats)\n    \"\"\"\n    int_list = [list(map(int, x)) for x in T1]\n    flattened_list = list(itertools.chain(*int_list))\n    total_nums = sum(flattened_list)\n\n    random_nums = [randint(0, RANGE) for _ in range(total_nums)]\n\n    mean = np.mean(random_nums)\n    median = np.median(random_nums)\n    mode = statistics.mode(random_nums)\n\n    return mean, median, mode\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, itertools, random, scipy.stats\n```python\nimport numpy as np\nimport itertools\nfrom random import randint\nfrom scipy import stats\n\n# Constants\nRANGE = 100\n\ndef calculate_percentile(T1):\n    \"\"\"\n    Convert items in `T1` to integers and create a list of random integers. \n    The size of the list is the sum of the integers in `T1`. Calculate and \n    return the 25th, 50th, and 75th percentiles of the list.\n    \n    Parameters:\n    T1 (tuple): A tuple of tuples, each containing string representations of integers.\n    \n    Returns:\n    tuple: A tuple (p25, p50, p75) of the percentiles of the list.\n    \n    Requirements:\n    - numpy\n    - itertools\n    - random\n    - scipy.stats\n    \n    Example:\n    >>> T1 = (('13', '17', '18', '21', '32'), ('07', '11', '13', '14', '28'), ('01', '05', '06', '08', '15', '16'))\n    >>> percentiles = calculate_percentile(T1)\n    >>> print(percentiles)\n    \"\"\"\n    int_list = [list(map(int, x)) for x in T1]\n    flattened_list = list(itertools.chain(*int_list))\n    total_nums = sum(flattened_list)\n\n    random_nums = [randint(0, RANGE) for _ in range(total_nums)]\n\n    p25 = np.percentile(random_nums, 25)\n    p50 = np.percentile(random_nums, 50)\n    p75 = np.percentile(random_nums, 75)\n\n    return p25, p50, p75\n```\n"}
{"seed": "{\"task_id\": 3777301, \"prompt\": \"def f_3777301():\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"subprocess.call(['./test.sh'])\", \"test_start\": \"\\nimport subprocess\\nfrom unittest.mock import Mock\\n\\ndef check(candidate):\", \"test\": [\"\\n    subprocess.call = Mock()\\n    try:\\n        candidate()\\n    except:\\n        assert False\\n\"], \"entry_point\": \"f_3777301\", \"intent\": \"call a shell script `./test.sh` using subprocess\", \"library\": [\"subprocess\"]}", "generation": "Scenario 1:\nsubprocess, os, sys, json, datetime\n```python\nimport subprocess\nimport os\nimport sys\nimport json\nfrom datetime import datetime\n\n# Constants\nSCRIPT_NAME = 'backup.sh'\nBACKUP_DIR = '/home/user/backup'\nLOG_FILE = '/home/user/backup_log.json'\n\ndef run_backup_script():\n    \"\"\"\n    Run a backup shell script, save the start time, end time, and exit status of the script \n    in a json log file. If the script does not exist or it fails to run, the function will exit.\n\n    Requirements:\n    - subprocess\n    - os\n    - sys\n    - json\n    - datetime\n\n    Example:\n    >>> run_backup_script()\n    \"\"\"\n    log_data = {}\n\n    if not os.path.isfile(SCRIPT_NAME):\n        print(f'Script {SCRIPT_NAME} does not exist.')\n        sys.exit(1)\n\n    start_time = datetime.now()\n    log_data['start_time'] = start_time.strftime('%Y-%m-%d %H:%M:%S')\n\n    try:\n        exit_status = subprocess.call(['./' + SCRIPT_NAME])\n    except Exception as e:\n        print(f'Failed to run {SCRIPT_NAME}: {str(e)}')\n        sys.exit(1)\n\n    end_time = datetime.now()\n    log_data['end_time'] = end_time.strftime('%Y-%m-%d %H:%M:%S')\n    log_data['exit_status'] = exit_status\n\n    with open(LOG_FILE, 'w') as f:\n        json.dump(log_data, f)\n    return log_data\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nsubprocess, time, random, matplotlib.pyplot\n```python\nimport subprocess\nimport time\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nCOMMANDS = ['ls', 'pwd', 'date', 'uname -r', 'df -h']\nITERATIONS = 50\n\ndef command_execution_time():\n    \"\"\"\n    Run a list of shell commands multiple times and plot a histogram of their \n    execution times.\n\n    Requirements:\n    - subprocess\n    - time\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> command_execution_time()\n    \"\"\"\n    execution_times = []\n\n    for _ in range(ITERATIONS):\n        command = COMMANDS[randint(0, len(COMMANDS) - 1)]\n\n        start_time = time.time()\n        subprocess.call(command, shell=True)\n        end_time = time.time()\n\n        execution_times.append(end_time - start_time)\n\n    plt.hist(execution_times, bins=10)\n    plt.title('Command Execution Times')\n    plt.xlabel('Time (s)')\n    plt.ylabel('Frequency')\n    plt.show()\n    return execution_times\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nsubprocess, ftplib, os, sys\n```python\nimport subprocess\nfrom ftplib import FTP\nimport os\nimport sys\n\n# Constants\nFTP_SERVER = 'ftp.dlptest.com'\nFTP_USER = 'dlpuser'\nFTP_PASSWORD = 'rNrKYTX9g7z3RgJRmxWuGHbeu'\nFTP_DIR = '/ftp/test'\n\ndef download_ftp_directory():\n    \"\"\"\n    Download all files from a specific directory on an FTP server using wget in a subprocess.\n\n    Requirements:\n    - subprocess\n    - ftplib\n    - os\n    - sys\n\n    Example:\n    >>> download_ftp_directory()\n    \"\"\"\n    try:\n        ftp = FTP(FTP_SERVER)\n        ftp.login(FTP_USER, FTP_PASSWORD)\n        ftp.cwd(FTP_DIR)\n    except Exception as e:\n        print(f'Failed to connect to FTP server: {str(e)}')\n        sys.exit(1)\n\n    for filename in ftp.nlst():\n        command = f'wget ftp://{FTP_USER}:{FTP_PASSWORD}@{FTP_SERVER}{FTP_DIR}/{filename}'\n        subprocess.call(command, shell=True)\n\n    ftp.quit()\n    return os.listdir('.')\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nsubprocess, configparser, shutil\n```python\nimport subprocess\nimport configparser\nimport shutil\n\n# Constants\nCONFIG_FILE = 'config.ini'\nARCHIVE_DIR = '/home/user/archive'\n\ndef archive_project():\n    \"\"\"\n    Archive the project directory specified in a config file to a zip file \n    using subprocess.\n\n    Requirements:\n    - subprocess\n    - configparser\n    - shutil\n\n    Example:\n    >>> archive_project()\n    \"\"\"\n    config = configparser.ConfigParser()\n    config.read(CONFIG_FILE)\n\n    project_dir = config.get('Project', 'directory')\n\n    if not os.path.isdir(project_dir):\n        print(f'Directory {project_dir} does not exist.')\n        sys.exit(1)\n\n    archive_file = f'{ARCHIVE_DIR}/{os.path.basename(project_dir)}.zip'\n\n    command = f'zip -r {archive_file} {project_dir}'\n    subprocess.call(command, shell=True)\n\n    return os.path.isfile(archive_file)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nsubprocess, csv, os\n```python\nimport subprocess\nimport csv\nimport os\n\n# Constants\nCOMMANDS_FILE = 'commands.csv'\nOUTPUT_DIR = '/home/user/command_outputs'\n\ndef execute_commands_from_csv():\n    \"\"\"\n    Read a list of shell commands from a csv file and execute them using subprocess. \n    Save the output of each command to a separate file.\n\n    Requirements:\n    - subprocess\n    - csv\n    - os\n\n    Example:\n    >>> execute_commands_from_csv()\n    \"\"\"\n    with open(COMMANDS_FILE, 'r') as f:\n        reader = csv.reader(f)\n        commands = list(reader)\n\n    for i, command in enumerate(commands):\n        output_file = f'{OUTPUT_DIR}/command_{i+1}_output.txt'\n        with open(output_file, 'w') as f:\n            subprocess.call(command, shell=True, stdout=f)\n\n    return os.listdir(OUTPUT_DIR)\n```"}
{"seed": "{\"task_id\": 3777301, \"prompt\": \"def f_3777301():\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"subprocess.call(['notepad'])\", \"test_start\": \"\\nimport subprocess\\nfrom unittest.mock import Mock\\n\\ndef check(candidate):\", \"test\": [\"\\n    subprocess.call = Mock()\\n    try:\\n        candidate()\\n    except:\\n        assert False\\n\"], \"entry_point\": \"f_3777301\", \"intent\": \"call a shell script `notepad` using subprocess\", \"library\": [\"subprocess\"]}", "generation": "Scenario 1:\nsubprocess, os, glob, shutil\n```python\nimport subprocess\nimport os\nimport glob\nimport shutil\n\n# Constants\nBACKUP_DIR = '/path/to/backup'\n\ndef backup_and_clear_logs(directory):\n    \"\"\"\n    Back up all log files in a specified directory and clear the original files. \n    The backup is a tar file created by a shell script.\n\n    Parameters:\n    directory (str): The directory that contains the log files.\n\n    Returns:\n    str: The path to the backup file.\n\n    Requirements:\n    - subprocess\n    - glob\n    - shutil\n    - os\n\n    Example:\n    >>> backup_and_clear_logs('/path/to/logs')\n    \"\"\"\n    log_files = glob.glob(os.path.join(directory, '*.log'))\n\n    if not os.path.exists(BACKUP_DIR):\n        os.makedirs(BACKUP_DIR)\n\n    backup_file = os.path.join(BACKUP_DIR, 'logs_backup.tar.gz')\n\n    subprocess.call(['tar', '-czvf', backup_file] + log_files)\n\n    for file in log_files:\n        os.remove(file)\n\n    return backup_file\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nsubprocess, os, psutil, time\n```python\nimport subprocess\nimport os\nimport psutil\nimport time\n\ndef manage_process(process_name):\n    \"\"\"\n    Check if a specific process is running. If it is not running, start it.\n    If it is running, restart it.\n\n    Parameters:\n    process_name (str): The name of the process.\n\n    Requirements:\n    - subprocess\n    - os\n    - psutil\n    - time\n\n    Example:\n    >>> manage_process('notepad')\n    \"\"\"\n    for proc in psutil.process_iter():\n        if proc.name() == process_name:\n            proc.terminate()\n            time.sleep(5)\n\n    subprocess.Popen(process_name)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nsubprocess, csv, os, random\n```python\nimport subprocess\nimport csv\nimport os\nimport random\n\n# Constants\nCSV_FILE = '/path/to/file.csv'\n\ndef split_and_shuffle_csv(file):\n    \"\"\"\n    Split a CSV file into multiple smaller files and shuffle the rows in each file. \n    The splitting is done by a shell script.\n\n    Parameters:\n    file (str): The path to the CSV file.\n\n    Returns:\n    list: The paths to the split files.\n\n    Requirements:\n    - subprocess\n    - csv\n    - os\n    - random\n\n    Example:\n    >>> split_and_shuffle_csv('/path/to/file.csv')\n    \"\"\"\n    subprocess.call(['split', '-n', '5', '-d', file, 'split_'])\n\n    split_files = glob.glob('split_*')\n\n    for file in split_files:\n        with open(file, 'r') as f:\n            reader = csv.reader(f)\n            rows = list(reader)\n\n        random.shuffle(rows)\n\n        with open(file, 'w') as f:\n            writer = csv.writer(f)\n            writer.writerows(rows)\n\n    return split_files\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nsubprocess, os, glob, zipfile\n```python\nimport subprocess\nimport os\nimport glob\nimport zipfile\n\n# Constants\nZIP_DIR = '/path/to/zip'\n\ndef zip_files(directory):\n    \"\"\"\n    Zip all files in a specified directory. The zip operation is done by a shell script.\n\n    Parameters:\n    directory (str): The directory that contains the files.\n\n    Returns:\n    str: The path to the zip file.\n\n    Requirements:\n    - subprocess\n    - glob\n    - zipfile\n    - os\n\n    Example:\n    >>> zip_files('/path/to/files')\n    \"\"\"\n    files = glob.glob(os.path.join(directory, '*'))\n\n    if not os.path.exists(ZIP_DIR):\n        os.makedirs(ZIP_DIR)\n\n    zip_file = os.path.join(ZIP_DIR, 'files.zip')\n\n    subprocess.call(['zip', '-r', zip_file] + files)\n\n    return zip_file\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nsubprocess, os, psutil, platform\n```python\nimport subprocess\nimport os\nimport psutil\nimport platform\n\ndef system_check():\n    \"\"\"\n    Check the system details including OS, architecture, and memory usage. \n    The details are obtained by using shell commands through subprocess.\n\n    Returns:\n    dict: A dictionary with the system details.\n\n    Requirements:\n    - subprocess\n    - os\n    - psutil\n    - platform\n\n    Example:\n    >>> system_check()\n    \"\"\"\n    system_info = {}\n\n    system_info['OS'] = platform.system()\n    system_info['Architecture'] = platform.architecture()[0]\n\n    total_memory = psutil.virtual_memory().total\n    used_memory = psutil.virtual_memory().used\n    system_info['Memory Usage'] = f'{used_memory/total_memory*100:.2f}%'\n\n    return system_info\n```"}
{"seed": "{\"task_id\": 7946798, \"prompt\": \"def f_7946798(l1, l2):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"[val for pair in zip(l1, l2) for val in pair]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([1,2,3], [10,20,30]) == [1,10,2,20,3,30]\\n\", \"\\n    assert candidate([1,2,3], ['c','b','a']) == [1,'c',2,'b',3,'a']\\n\", \"\\n    assert candidate([1,2,3], ['c','b']) == [1,'c',2,'b']\\n\"], \"entry_point\": \"f_7946798\", \"intent\": \"combine lists `l1` and `l2`  by alternating their elements\", \"library\": []}", "generation": "Scenario 1:\npandas, numpy, matplotlib, itertools\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom itertools import zip_longest\n\n# Constants\nN = 100\nCATEGORIES = ['A', 'B', 'C', 'D', 'E']\n\ndef generate_and_visualize_data(l1, l2):\n    \"\"\"\n    Generate a DataFrame using two given lists by alternating their elements,\n    generate random category for each row, and visualize the count of each category.\n    \n    Parameters:\n    l1 (list): The first list.\n    l2 (list): The second list.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with combined data from l1 and l2 and random categories.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - itertools\n    \n    Example:\n    >>> l1 = list(range(10))\n    >>> l2 = list(range(10, 20))\n    >>> df = generate_and_visualize_data(l1, l2)\n    >>> print(df)\n    >>> df['Category'].value_counts().plot(kind='bar')\n    \"\"\"\n    combined = [val for pair in zip_longest(l1, l2) for val in pair if val is not None]\n    categories = np.random.choice(CATEGORIES, len(combined))\n    df = pd.DataFrame({'Value': combined, 'Category': categories})\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncollections, itertools, random\n```python\nimport collections\nimport itertools\nfrom random import choices\n\n# Constants\nK = 10\nPOPULATION = list(range(1, 101))\n\ndef combined_list_freq(l1, l2):\n    \"\"\"\n    Combine two lists by alternating their elements, generate a random sample from the combined list,\n    and calculate the frequency of each element in the sample.\n    \n    Parameters:\n    l1 (list): The first list.\n    l2 (list): The second list.\n    \n    Returns:\n    Counter: A collections Counter object with the frequency of each element in the sample.\n    \n    Requirements:\n    - collections\n    - itertools\n    - random\n    \n    Example:\n    >>> l1 = list(range(10))\n    >>> l2 = list(range(10, 20))\n    >>> freq = combined_list_freq(l1, l2)\n    >>> print(freq)\n    \"\"\"\n    combined = [val for pair in zip_longest(l1, l2) for val in pair if val is not None]\n    sample = choices(POPULATION, k=K)\n    freq = collections.Counter(sample)\n    return freq\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nmatplotlib, numpy, itertools\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom itertools import zip_longest\n\n# Constants\nNPOINTS = 100\n\ndef plot_alternating_series(l1, l2):\n    \"\"\"\n    Combine two lists by alternating their elements and plot the resulting series.\n    \n    Parameters:\n    l1 (list): The first list.\n    l2 (list): The second list.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - matplotlib.pyplot\n    - numpy\n    - itertools\n    \n    Example:\n    >>> l1 = list(range(10))\n    >>> l2 = list(range(10, 20))\n    >>> plot_alternating_series(l1, l2)\n    \"\"\"\n    combined = [val for pair in zip_longest(l1, l2) for val in pair if val is not None]\n    plt.plot(np.arange(len(combined)), combined)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, itertools, math\n```python\nimport numpy as np\nfrom itertools import zip_longest\nimport math\n\n# Constants\nTHRESHOLD = 0.5\n\ndef find_closest_to_threshold(l1, l2):\n    \"\"\"\n    Combine two lists by alternating their elements, calculate the difference from a given threshold,\n    and find the element that is closest to the threshold.\n    \n    Parameters:\n    l1 (list): The first list.\n    l2 (list): The second list.\n    \n    Returns:\n    float: The element closest to the threshold.\n    \n    Requirements:\n    - numpy\n    - itertools\n    - math\n    \n    Example:\n    >>> l1 = list(range(10))\n    >>> l2 = list(range(10, 20))\n    >>> closest = find_closest_to_threshold(l1, l2)\n    >>> print(closest)\n    \"\"\"\n    combined = [val for pair in zip_longest(l1, l2) for val in pair if val is not None]\n    differences = np.abs(np.array(combined) - THRESHOLD)\n    closest_index = np.argmin(differences)\n    return combined[closest_index]\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, itertools, seaborn\n```python\nimport pandas as pd\nfrom itertools import zip_longest\nimport seaborn as sns\n\n# Constants\nCOLUMNS = ['a', 'b', 'c']\n\ndef visualize_combination(l1, l2):\n    \"\"\"\n    Combine two lists by alternating their elements, create a DataFrame, \n    and visualize the distribution of the values in the DataFrame.\n    \n    Parameters:\n    l1 (list): The first list.\n    l2 (list): The second list.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with combined data from l1 and l2.\n    \n    Requirements:\n    - pandas\n    - itertools\n    - seaborn\n    \n    Example:\n    >>> l1 = list(range(10))\n    >>> l2 = list(range(10, 20))\n    >>> df = visualize_combination(l1, l2)\n    >>> print(df)\n    >>> sns.histplot(df)\n    \"\"\"\n    combined = [val for pair in zip_longest(l1, l2) for val in pair if val is not None]\n    df = pd.DataFrame(combined, columns=COLUMNS)\n    return df\n```\n"}
{"seed": "{\"task_id\": 8908287, \"prompt\": \"def f_8908287():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"base64.b64encode(b'data to be encoded')\", \"test_start\": \"\\nimport base64\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate() == b'ZGF0YSB0byBiZSBlbmNvZGVk'\\n\"], \"entry_point\": \"f_8908287\", \"intent\": \"encode string 'data to be encoded'\", \"library\": [\"base64\"]}", "generation": "Scenario 1:\nbase64, hashlib, os, binascii\n```python\nimport base64\nimport hashlib\nimport os\nimport binascii\n\n# Constants\nSALT_LENGTH = 32\n\ndef create_hashed_password(password):\n    \"\"\"\n    Hash a password using the PBKDF2 algorithm and encode both the salt and the hashed password \n    with base64.\n\n    Parameters:\n    password (str): The password to be hashed.\n\n    Returns:\n    tuple: A tuple containing the base64 encoded salt and hashed password.\n\n    Requirements:\n    - base64\n    - hashlib\n    - os\n    - binascii\n\n    Example:\n    >>> salt, hashed_password = create_hashed_password('my_password')\n    >>> print(salt, hashed_password)\n    \"\"\"\n    salt = os.urandom(SALT_LENGTH)\n    hashed_password = hashlib.pbkdf2_hmac('sha256', password.encode(), salt, 100000)\n    return base64.b64encode(salt), base64.b64encode(hashed_password)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nbase64, json, zlib\n```python\nimport base64\nimport json\nimport zlib\n\ndef compress_and_encode_dict(data_dict):\n    \"\"\"\n    Compress a dictionary using zlib and then encode it using base64.\n\n    Parameters:\n    data_dict (dict): The dictionary to be compressed and encoded.\n\n    Returns:\n    str: The base64 encoded compressed dictionary.\n\n    Requirements:\n    - base64\n    - json\n    - zlib\n\n    Example:\n    >>> data = {'key1': 'value1', 'key2': 'value2'}\n    >>> encoded_data = compress_and_encode_dict(data)\n    >>> print(encoded_data)\n    \"\"\"\n    json_str = json.dumps(data_dict)\n    compressed = zlib.compress(json_str.encode())\n    return base64.b64encode(compressed).decode()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nbase64, PIL, io\n```python\nimport base64\nfrom PIL import Image\nimport io\n\ndef encode_image(image_path):\n    \"\"\"\n    Open an image from given path, convert it to RGB and then encode it using base64.\n\n    Parameters:\n    image_path (str): The path to the image file.\n\n    Returns:\n    str: The base64 encoded image string.\n\n    Requirements:\n    - base64\n    - PIL\n    - io\n\n    Example:\n    >>> encoded_image = encode_image('/path/to/image.jpg')\n    >>> print(encoded_image)\n    \"\"\"\n    image = Image.open(image_path).convert('RGB')\n    byte_arr = io.BytesIO()\n    image.save(byte_arr, format='JPEG')\n    byte_data = byte_arr.getvalue()\n    return base64.b64encode(byte_data).decode()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nbase64, cryptography.fernet, getpass\n```python\nimport base64\nfrom cryptography.fernet import Fernet\nfrom getpass import getpass\n\ndef encrypt_and_encode_message(message):\n    \"\"\"\n    Encrypt a message with a symmetric encryption key and then encode the encrypted message \n    using base64.\n\n    Parameters:\n    message (str): The message to be encrypted and encoded.\n\n    Returns:\n    str: The base64 encoded encrypted message.\n\n    Requirements:\n    - base64\n    - cryptography.fernet\n    - getpass\n\n    Example:\n    >>> encrypted_message = encrypt_and_encode_message('Hello, World!')\n    >>> print(encrypted_message)\n    \"\"\"\n    key = getpass(\"Enter encryption key: \")\n    fernet = Fernet(base64.urlsafe_b64encode(key.encode()))\n    encrypted_message = fernet.encrypt(message.encode())\n    return base64.b64encode(encrypted_message).decode()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nbase64, numpy, scipy.misc\n```python\nimport base64\nimport numpy as np\nfrom scipy.misc import imsave, toimage\n\ndef encode_image_from_array(array):\n    \"\"\"\n    Convert a numpy array to an image, save it to a temporary file and then encode the image \n    file using base64.\n\n    Parameters:\n    array (np.array): A 2D or 3D numpy array representing image data.\n\n    Returns:\n    str: The base64 encoded image string.\n\n    Requirements:\n    - base64\n    - numpy\n    - scipy.misc\n\n    Example:\n    >>> array = np.zeros((100, 100))\n    >>> encoded_image = encode_image_from_array(array)\n    >>> print(encoded_image)\n    \"\"\"\n    image = toimage(array)\n    image.save('/tmp/temp_image.png')\n    with open('/tmp/temp_image.png', 'rb') as image_file:\n        return base64.b64encode(image_file.read()).decode()\n```"}
{"seed": "{\"task_id\": 8908287, \"prompt\": \"def f_8908287():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"'data to be encoded'.encode('ascii')\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate() == b'data to be encoded'\\n\"], \"entry_point\": \"f_8908287\", \"intent\": \"encode a string `data to be encoded` to `ascii` encoding\", \"library\": []}", "generation": "Scenario 1: \npandas, os, hashlib, base64\n```python\nimport pandas as pd\nimport os\nimport hashlib\nimport base64\n\n# Constants\nDIR_PATH = '/path/to/your/directory'\n\ndef create_encoded_file_hash_df():\n    \"\"\"\n    Create a pandas DataFrame containing the file names and their encoded hash values\n    for all files in a given directory.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with file names and their encoded hash values.\n    \n    Requirements:\n    - pandas\n    - os\n    - hashlib\n    - base64\n    \n    Example:\n    >>> df = create_encoded_file_hash_df()\n    >>> print(df)\n    \"\"\"\n    file_list = os.listdir(DIR_PATH)\n    data = []\n    \n    for file in file_list:\n        with open(os.path.join(DIR_PATH, file), 'rb') as f:\n            bytes = f.read()\n            readable_hash = hashlib.sha256(bytes).digest()\n            encoded_hash = base64.b64encode(readable_hash).decode('ascii')\n            data.append([file, encoded_hash])\n    \n    df = pd.DataFrame(data, columns=['File Name', 'Encoded Hash'])\n    \n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \njson, base64, datetime\n```python\nimport json\nimport base64\nfrom datetime import datetime\n\n# Constants\nDATE_FORMAT = \"%Y-%m-%d %H:%M:%S\"\n\ndef encode_json_data(data):\n    \"\"\"\n    Encode a Python dictionary into a JSON formatted string, \n    add a timestamp, and then encode the string into 'ascii'.\n    \n    Parameters:\n    data (dict): The Python dictionary to encode.\n    \n    Returns:\n    str: The encoded string.\n    \n    Requirements:\n    - json\n    - base64\n    - datetime\n    \n    Example:\n    >>> data = {'name': 'John', 'age': 30, 'city': 'New York'}\n    >>> encoded_data = encode_json_data(data)\n    >>> print(encoded_data)\n    \"\"\"\n    data['timestamp'] = datetime.now().strftime(DATE_FORMAT)\n    json_data = json.dumps(data)\n    encoded_data = base64.b64encode(json_data.encode('ascii')).decode('ascii')\n    \n    return encoded_data\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nrequests, json, base64\n```python\nimport requests\nimport json\nimport base64\n\n# Constants\nURL = \"http://your-api-url.com\"\n\ndef send_encoded_data(data):\n    \"\"\"\n    Encode a Python dictionary into a JSON formatted string and then encode the string into 'ascii'.\n    Send the encoded string to an API endpoint as a POST request.\n    \n    Parameters:\n    data (dict): The Python dictionary to encode and send.\n    \n    Returns:\n    Response: The response from the API endpoint.\n    \n    Requirements:\n    - requests\n    - json\n    - base64\n    \n    Example:\n    >>> data = {'name': 'John', 'age': 30, 'city': 'New York'}\n    >>> response = send_encoded_data(data)\n    >>> print(response.status_code)\n    \"\"\"\n    json_data = json.dumps(data)\n    encoded_data = base64.b64encode(json_data.encode('ascii')).decode('ascii')\n    response = requests.post(URL, data={\"payload\": encoded_data})\n    \n    return response\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nsklearn.preprocessing, numpy, base64\n```python\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\nimport base64\n\n# Constants\nDATA = np.array([[0, 0], [0, 0], [1, 1], [1, 1]])\n\ndef standardize_and_encode_data(data):\n    \"\"\"\n    Standardize a numpy array using sklearn's StandardScaler, \n    then encode the standardized data into 'ascii'.\n    \n    Parameters:\n    data (numpy array): The numpy array to standardize and encode.\n    \n    Returns:\n    str: The encoded string of the standardized data.\n    \n    Requirements:\n    - sklearn.preprocessing\n    - numpy\n    - base64\n    \n    Example:\n    >>> data = np.array([[0, 0], [0, 0], [1, 1], [1, 1]])\n    >>> encoded_data = standardize_and_encode_data(data)\n    >>> print(encoded_data)\n    \"\"\"\n    scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(data)\n    standardized_data_str = np.array2string(standardized_data)\n    encoded_data = base64.b64encode(standardized_data_str.encode('ascii')).decode('ascii')\n    \n    return encoded_data\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \npandas, io, base64\n```python\nimport pandas as pd\nimport io\nimport base64\n\ndef encode_dataframe_to_csv(df):\n    \"\"\"\n    Encode a pandas DataFrame into a csv formatted string and then encode the string into 'ascii'.\n    \n    Parameters:\n    df (DataFrame): The pandas DataFrame to encode.\n    \n    Returns:\n    str: The encoded string.\n    \n    Requirements:\n    - pandas\n    - io\n    - base64\n    \n    Example:\n    >>> df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n    >>> encoded_csv = encode_dataframe_to_csv(df)\n    >>> print(encoded_csv)\n    \"\"\"\n    csv_buffer = io.StringIO()\n    df.to_csv(csv_buffer)\n    csv_str = csv_buffer.getvalue()\n    encoded_csv = base64.b64encode(csv_str.encode('ascii')).decode('ascii')\n    \n    return encoded_csv\n```"}
{"seed": "{\"task_id\": 7856296, \"prompt\": \"def f_7856296():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"list(csv.reader(open('text.txt', 'r'), delimiter='\\\\t'))\", \"test_start\": \"\\nimport csv \\n\\ndef check(candidate):\", \"test\": [\"\\n    with open('text.txt', 'w', newline='') as csvfile:\\n        spamwriter = csv.writer(csvfile, delimiter='\\t')\\n        spamwriter.writerow(['Spam', 'Lovely Spam', 'Wonderful Spam'])\\n        spamwriter.writerow(['hello', 'world', '!'])\\n\\n    assert candidate() == [['Spam', 'Lovely Spam', 'Wonderful Spam'], ['hello', 'world', '!']]\\n\"], \"entry_point\": \"f_7856296\", \"intent\": \"parse tab-delimited CSV file 'text.txt' into a list\", \"library\": [\"csv\"]}", "generation": "Scenario 1: \npandas, csv, os, random\n```python\nimport pandas as pd\nimport csv\nimport os\nfrom random import randint\n\n# Constants\nDATA_DIR = '/path/to/data/directory'\nCSV_FILES = ['file1.csv', 'file2.csv', 'file3.csv']\n\ndef merge_csv_files():\n    \"\"\"\n    Merge multiple CSV files in a directory into a single pandas DataFrame.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the merged data.\n    \n    Requirements:\n    - pandas\n    - csv\n    - os\n    - random\n    \n    Example:\n    >>> df = merge_csv_files()\n    >>> print(df)\n    \"\"\"\n    merged_df = pd.DataFrame()\n\n    for file in CSV_FILES:\n        file_path = os.path.join(DATA_DIR, file)\n        df = pd.read_csv(file_path)\n        merged_df = pd.concat([merged_df, df])\n\n    return merged_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncsv, os, random, matplotlib.pyplot\n```python\nimport csv\nimport os\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nDATA_DIR = '/path/to/data/directory'\nCSV_FILES = ['file1.csv', 'file2.csv', 'file3.csv']\n\ndef plot_random_csv_file():\n    \"\"\"\n    Plot the data of a randomly selected CSV file from a list.\n    \n    Requirements:\n    - csv\n    - os\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> plot_random_csv_file()\n    \"\"\"\n    file = CSV_FILES[randint(0, len(CSV_FILES)-1)]\n    file_path = os.path.join(DATA_DIR, file)\n\n    with open(file_path, 'r') as f:\n        reader = csv.reader(f)\n        data = list(reader)\n\n    plt.plot(data)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ncsv, os, pandas, numpy\n```python\nimport csv\nimport os\nimport pandas as pd\nimport numpy as np\n\n# Constants\nDATA_DIR = '/path/to/data/directory'\nCSV_FILE = 'file.csv'\n\ndef replace_nan_with_mean():\n    \"\"\"\n    Replace NaN values in a CSV file with the mean of the corresponding column.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the processed data.\n    \n    Requirements:\n    - csv\n    - os\n    - pandas\n    - numpy\n    \n    Example:\n    >>> df = replace_nan_with_mean()\n    >>> print(df)\n    \"\"\"\n    file_path = os.path.join(DATA_DIR, CSV_FILE)\n    df = pd.read_csv(file_path)\n\n    for column in df.columns:\n        if df[column].dtype == np.number:\n            df[column].fillna(df[column].mean(), inplace=True)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ncsv, os, pandas, matplotlib.pyplot\n```python\nimport csv\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nDATA_DIR = '/path/to/data/directory'\nCSV_FILE = 'file.csv'\n\ndef plot_histogram_of_column(column_name):\n    \"\"\"\n    Plot a histogram of a specified column in a CSV file.\n\n    Parameters:\n    column_name (str): The name of the column to plot.\n\n    Requirements:\n    - csv\n    - os\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_histogram_of_column('age')\n    \"\"\"\n    file_path = os.path.join(DATA_DIR, CSV_FILE)\n    df = pd.read_csv(file_path)\n\n    df[column_name].hist()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\ncsv, os, random, pandas\n```python\nimport csv\nimport os\nfrom random import randint\nimport pandas as pd\n\n# Constants\nDATA_DIR = '/path/to/data/directory'\nCSV_FILES = ['file1.csv', 'file2.csv', 'file3.csv']\n\ndef randomly_select_rows_from_csv():\n    \"\"\"\n    Randomly select a specified number of rows from a CSV file.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the selected rows.\n    \n    Requirements:\n    - csv\n    - os\n    - random\n    - pandas\n    \n    Example:\n    >>> df = randomly_select_rows_from_csv()\n    >>> print(df)\n    \"\"\"\n    file = CSV_FILES[randint(0, len(CSV_FILES)-1)]\n    file_path = os.path.join(DATA_DIR, file)\n    df = pd.read_csv(file_path)\n\n    selected_rows = df.sample(n=randint(1, len(df)))\n\n    return selected_rows\n```"}
{"seed": "{\"task_id\": 9035479, \"prompt\": \"def f_9035479(my_object, my_str):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"getattr(my_object, my_str)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    class Student:\\n        id = 9\\n        name = \\\"abc\\\"\\n        grade = 97.08\\n\\n    s = Student()\\n    \\n    assert candidate(s, \\\"name\\\") == \\\"abc\\\"\\n\", \"\\n    class Student:\\n        id = 9\\n        name = \\\"abc\\\"\\n        grade = 97.08\\n\\n    s = Student()\\n    \\n    assert (candidate(s, \\\"grade\\\") - 97.08) < 1e-6\\n\", \"\\n    class Student:\\n        id = 9\\n        name = \\\"abc\\\"\\n        grade = 97.08\\n\\n    s = Student()\\n    \\n    assert (candidate(s, \\\"grade\\\") - 97.07) > 1e-6\\n\", \"\\n    class Student:\\n        id = 9\\n        name = \\\"abc\\\"\\n        grade = 97.08\\n\\n    s = Student()\\n    \\n    assert candidate(s, \\\"id\\\") == 9\\n\"], \"entry_point\": \"f_9035479\", \"intent\": \"Get attribute `my_str` of object `my_object`\", \"library\": []}", "generation": "Scenario 1:\njson, os, argparse, re\n```python\nimport json\nimport os\nimport argparse\nimport re\n\n# Constants\nVALID_JSON_STRUCTURE = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"name\": {\"type\": \"string\"},\n        \"age\": {\"type\": \"number\"},\n        \"email\": {\"type\": \"string\"}\n    },\n    \"required\": [\"name\", \"age\", \"email\"]\n}\n\nEMAIL_REGEX = r\"^[a-z0-9]+[\\._]?[a-z0-9]+[@]\\w+[.]\\w{2,3}$\"\n\ndef validate_json_file(file_path, attribute):\n    \"\"\"\n    Validate the structure and content of a JSON file and retrieve a specified attribute from the JSON object.\n    \n    Parameters:\n    file_path (str): The path to the JSON file.\n    attribute (str): The attribute to retrieve from the JSON object.\n\n    Returns:\n    Any: The value of the specified attribute.\n\n    Requirements:\n    - json\n    - os\n    - argparse\n    - re\n\n    Terminal-based Input-Output Example:\n    $ python3 validate_json_file.py --file_path /path/to/file.json --attribute email\n    $ john.doe@example.com\n    \"\"\"\n    parser = argparse.ArgumentParser(description='Validate JSON file and retrieve attribute.')\n    parser.add_argument('--file_path', type=str, help='Path to the JSON file.')\n    parser.add_argument('--attribute', type=str, help='Attribute to retrieve from the JSON object.')\n    args = parser.parse_args()\n\n    if not os.path.isfile(args.file_path):\n        raise ValueError(f'{args.file_path} does not exist.')\n\n    with open(args.file_path, 'r') as f:\n        data = json.load(f)\n\n    for key in VALID_JSON_STRUCTURE['required']:\n        if key not in data:\n            raise ValueError(f'{key} is missing from the JSON object.')\n        if not isinstance(data[key], VALID_JSON_STRUCTURE['properties'][key]['type']):\n            raise ValueError(f'{key} is not of type {VALID_JSON_STRUCTURE[\"properties\"][key][\"type\"]}.')\n\n    if 'email' in data and not re.fullmatch(EMAIL_REGEX, data['email']):\n        raise ValueError('Email is not valid.')\n\n    return data[args.attribute]\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, scipy.optimize, matplotlib\n```python\nimport numpy as np\nfrom scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\n\n# Constants\nINITIAL_GUESS = [1.0, 0.0, 0.0]\n\ndef fit_sin_wave(data, attribute):\n    \"\"\"\n    Fit a sinusoidal function to a set of data points and plot the original data and the fitted curve.\n\n    Parameters:\n    data (numpy.ndarray): The data points. Each row of the array is expected to represent a point in 2D space, with the first element being the x-coordinate and the second element being the y-coordinate.\n    attribute (str): The attribute to retrieve from the data points.\n\n    Returns:\n    list: The coefficients of the fitted sinusoidal function.\n\n    Requirements:\n    - numpy\n    - scipy.optimize\n    - matplotlib\n\n    Terminal-based Input-Output Example:\n    $ python3 fit_sin_wave.py --data '[[1,1], [2,2], [3,3], [4,4], [5,5]]' --attribute y\n    $ [1.0, 0.0, 0.0]\n    \"\"\"\n    parser = argparse.ArgumentParser(description='Fit a sinusoidal function to data points and plot the result.')\n    parser.add_argument('--data', type=str, help='The data points. Should be a string representation of a 2D array.')\n    parser.add_argument('--attribute', type=str, help='The attribute to retrieve from the data points.')\n    args = parser.parse_args()\n\n    data = np.array(eval(args.data))\n\n    def sin_func(x, a, b, c):\n        return a * np.sin(b * x + c)\n\n    popt, pcov = curve_fit(sin_func, data[:,0], data[:,1], p0=INITIAL_GUESS)\n\n    plt.figure()\n    plt.plot(data[:,0], data[:,1], 'b-', label='data')\n    plt.plot(data[:,0], sin_func(data[:,0], *popt), 'r-', label='fit: a=%5.3f, b=%5.3f, c=%5.3f' % tuple(popt))\n    plt.xlabel('x')\n    plt.ylabel(args.attribute)\n    plt.legend()\n    plt.show()\n\n    return list(popt)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ncsv, pandas, sklearn.linear_model, sklearn.model_selection\n```python\nimport csv\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\n# Constants\nTEST_SIZE = 0.2\nRANDOM_STATE = 42\n\ndef predict(csv_file_path, attribute):\n    \"\"\"\n    Train a linear regression model on a data set and predict the value of a specified attribute.\n\n    Parameters:\n    csv_file_path (str): The path to the CSV file containing the data set.\n    attribute (str): The attribute to predict.\n\n    Returns:\n    float: The predicted value of the specified attribute.\n\n    Requirements:\n    - csv\n    - pandas\n    - sklearn.linear_model\n    - sklearn.model_selection\n\n    Terminal-based Input-Output Example:\n    $ python3 predict.py --csv_file_path /path/to/data.csv --attribute target\n    $ 123.45\n    \"\"\"\n    parser = argparse.ArgumentParser(description='Train a linear regression model and predict an attribute.')\n    parser.add_argument('--csv_file_path', type=str, help='Path to the CSV file containing the data set.')\n    parser.add_argument('--attribute', type=str, help='The attribute to predict.')\n    args = parser.parse_args()\n\n    df = pd.read_csv(args.csv_file_path)\n    X = df.drop(columns=[args.attribute])\n    y = df[args.attribute]\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    return model.predict(X_test)[0]\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nrequests, json, argparse, bs4\n```python\nimport requests\nimport json\nimport argparse\nfrom bs4 import BeautifulSoup\n\n# Constants\nURL = 'https://www.example.com/'\n\ndef scrape_website(url, attribute):\n    \"\"\"\n    Scrape a website for a specified HTML attribute and return its value.\n\n    Parameters:\n    url (str): The URL of the website to scrape.\n    attribute (str): The HTML attribute to scrape for.\n\n    Returns:\n    str: The value of the specified HTML attribute.\n\n    Requirements:\n    - requests\n    - json\n    - argparse\n    - bs4\n\n    Terminal-based Input-Output Example:\n    $ python3 scrape_website.py --url https://www.google.com/ --attribute title\n    $ 'Google'\n    \"\"\"\n    parser = argparse.ArgumentParser(description='Scrape a website for a specified HTML attribute.')\n    parser.add_argument('--url', type=str, help='The URL of the website to scrape.')\n    parser.add_argument('--attribute', type=str, help='The HTML attribute to scrape for.')\n    args = parser.parse_args()\n\n    response = requests.get(args.url)\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    return soup.find(args.attribute).string\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, scipy.stats, matplotlib\n```python\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\n# Constants\nNUM_SAMPLES = 1000\n\ndef plot_normal_distribution(mean, standard_deviation):\n    \"\"\"\n    Generate a random normal distribution with a specified mean and standard deviation, and plot the result.\n\n    Parameters:\n    mean (float): The mean of the distribution.\n    standard_deviation (float): The standard deviation of the distribution.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib\n\n    Terminal-based Input-Output Example:\n    $ python3 plot_normal_distribution.py --mean 0 --standard_deviation 1\n    \"\"\"\n    parser = argparse.ArgumentParser(description='Generate a random normal distribution and plot the result.')\n    parser.add_argument('--mean', type=float, help='The mean of the distribution.')\n    parser.add_argument('--standard_deviation', type=float, help='The standard deviation of the distribution.')\n    args = parser.parse_args()\n\n    data = np.random.normal(loc=args.mean, scale=args.standard_deviation, size=NUM_SAMPLES)\n    plt.hist(data, bins=30, density=True)\n\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, args.mean, args.standard_deviation)\n    plt.plot(x, p, 'k', linewidth=2)\n\n    plt.show()\n```"}
{"seed": "{\"task_id\": 5558418, \"prompt\": \"def f_5558418(LD):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"dict(zip(LD[0], zip(*[list(d.values()) for d in LD])))\", \"test_start\": \"\\nimport collections\\n\\ndef check(candidate):\", \"test\": [\"\\n    employees = [{'name' : 'apple', 'id': 60}, {'name' : 'orange', 'id': 65}]\\n    exp_result = {'name': ('apple', 'orange'), 'id': (60, 65)}\\n    actual_result = candidate(employees)\\n    for key in actual_result:\\n        assert collections.Counter(list(exp_result[key])) == collections.Counter(list(actual_result[key]))\\n\"], \"entry_point\": \"f_5558418\", \"intent\": \"group a list of dicts `LD` into one dict by key\", \"library\": [\"collections\"]}", "generation": "Scenario 1: \ncollections, numpy, pandas, matplotlib\n```python\nimport collections\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nPRODUCTS = ['apple', 'orange', 'banana', 'grape', 'pear']\n\ndef analyze_sales_data(sales_data):\n    \"\"\"\n    Analyze a list of sales data dicts and return summary statistics and a histogram of sales by product.\n\n    Parameters:\n    sales_data (list): A list of dictionaries where each dictionary represents sales data for a product,\n                       e.g., [{'product': 'apple', 'sales': 60}, {'product': 'orange', 'sales': 65}]\n\n    Returns:\n    dict: A dictionary with summary statistics (mean, median, mode) for the sales data.\n    matplotlib.figure.Figure: A histogram of sales by product.\n\n    Requirements:\n    - collections\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> sales_data = [{'product': 'apple', 'sales': 60}, {'product': 'orange', 'sales': 65}]\n    >>> stats, fig = analyze_sales_data(sales_data)\n    >>> print(stats)\n    >>> fig.show()\n    \"\"\"\n    sales = collections.defaultdict(list)\n    for data in sales_data:\n        sales[data['product']].append(data['sales'])\n\n    summary_stats = {}\n    for product in PRODUCTS:\n        sales_array = np.array(sales[product])\n        summary_stats[product] = {\n            'mean': np.mean(sales_array),\n            'median': np.median(sales_array),\n            'mode': collections.Counter(sales_array).most_common(1)[0][0]\n        }\n\n    product_sales_df = pd.DataFrame(sales)\n    fig = product_sales_df.hist(bins=10)\n\n    return summary_stats, fig\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \npandas, matplotlib, collections, re\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport collections\nimport re\n\n# Constants\nID_PATTERN = r'\\d{3}-\\d{2}-\\d{4}'\n\ndef analyze_employee_data(employee_data):\n    \"\"\"\n    Analyze a list of employee data dicts, validate employee IDs using a regular expression, and return a count of\n    employee by job role and a bar chart of employee count by job role.\n\n    Parameters:\n    employee_data (list): A list of dictionaries where each dictionary represents data for an employee,\n                          e.g., [{'name' : 'John', 'id': '123-45-6789', 'role': 'engineer'}]\n\n    Returns:\n    dict: A dictionary with the count of employees by job role.\n    matplotlib.figure.Figure: A bar chart of employee count by job role.\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - collections\n    - re\n\n    Example:\n    >>> employee_data = [{'name' : 'John', 'id': '123-45-6789', 'role': 'engineer'}]\n    >>> role_counts, fig = analyze_employee_data(employee_data)\n    >>> print(role_counts)\n    >>> fig.show()\n    \"\"\"\n    valid_employees = [employee for employee in employee_data if re.match(ID_PATTERN, employee['id'])]\n    \n    role_counts = collections.Counter([employee['role'] for employee in valid_employees])\n\n    employee_df = pd.DataFrame(valid_employees)\n    fig = employee_df['role'].value_counts().plot(kind='bar')\n\n    return dict(role_counts), fig\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, pandas, collections, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nimport collections\nimport matplotlib.pyplot as plt\n\n# Constants\nCOUNTRIES = ['USA', 'Canada', 'UK', 'Germany', 'France', 'Australia']\n\ndef analyze_population_data(population_data):\n    \"\"\"\n    Analyze a list of population data dicts and return summary statistics and a pie chart of populations by country.\n\n    Parameters:\n    population_data (list): A list of dictionaries where each dictionary represents population data for a country,\n                            e.g., [{'country': 'USA', 'population': 331002651}, {'country': 'UK', 'population': 67886011}]\n\n    Returns:\n    dict: A dictionary with summary statistics (mean, median, mode) for the population data.\n    matplotlib.figure.Figure: A pie chart of populations by country.\n\n    Requirements:\n    - numpy\n    - pandas\n    - collections\n    - matplotlib.pyplot\n\n    Example:\n    >>> population_data = [{'country': 'USA', 'population': 331002651}, {'country': 'UK', 'population': 67886011}]\n    >>> stats, fig = analyze_population_data(population_data)\n    >>> print(stats)\n    >>> fig.show()\n    \"\"\"\n    populations = collections.defaultdict(list)\n    for data in population_data:\n        populations[data['country']].append(data['population'])\n\n    summary_stats = {}\n    for country in COUNTRIES:\n        population_array = np.array(populations[country])\n        summary_stats[country] = {\n            'mean': np.mean(population_array),\n            'median': np.median(population_array),\n            'mode': collections.Counter(population_array).most_common(1)[0][0]\n        }\n\n    population_df = pd.DataFrame(population_data)\n    fig = population_df['population'].plot.pie(autopct='%1.1f%%')\n\n    return summary_stats, fig\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ncollections, numpy, pandas, matplotlib\n```python\nimport collections\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nPRODUCTS = ['apple', 'orange', 'banana', 'grape', 'pear']\n\ndef analyze_inventory_data(inventory_data):\n    \"\"\"\n    Analyze a list of inventory data dicts and return summary statistics and a line plot of inventory levels over time.\n\n    Parameters:\n    inventory_data (list): A list of dictionaries where each dictionary represents inventory data for a product at a point in time,\n                           e.g., [{'product': 'apple', 'inventory': 60, 'time': '2021-09-01'}, {'product': 'orange', 'inventory': 65, 'time': '2021-09-01'}]\n\n    Returns:\n    dict: A dictionary with summary statistics (mean, median, mode) for the inventory data.\n    matplotlib.figure.Figure: A line plot of inventory levels over time.\n\n    Requirements:\n    - collections\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> inventory_data = [{'product': 'apple', 'inventory': 60, 'time': '2021-09-01'}, {'product': 'orange', 'inventory': 65, 'time': '2021-09-01'}]\n    >>> stats, fig = analyze_inventory_data(inventory_data)\n    >>> print(stats)\n    >>> fig.show()\n    \"\"\"\n    inventory = collections.defaultdict(list)\n    for data in inventory_data:\n        inventory[data['product']].append(data['inventory'])\n\n    summary_stats = {}\n    for product in PRODUCTS:\n        inventory_array = np.array(inventory[product])\n        summary_stats[product] = {\n            'mean': np.mean(inventory_array),\n            'median': np.median(inventory_array),\n            'mode': collections.Counter(inventory_array).most_common(1)[0][0]\n        }\n\n    inventory_df = pd.DataFrame(inventory_data)\n    inventory_df['time'] = pd.to_datetime(inventory_df['time'])\n    fig = inventory_df.pivot(index='time', columns='product', values='inventory').plot()\n\n    return summary_stats, fig\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \ncollections, pandas, matplotlib, re\n```python\nimport collections\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport re\n\n# Constants\nEMAIL_PATTERN = r'(^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$)'\n\ndef analyze_customer_data(customer_data):\n    \"\"\"\n    Analyze a list of customer data dicts, validate customer emails using a regular expression, and return a count of\n    customer by country and a bar chart of customer count by country.\n\n    Parameters:\n    customer_data (list): A list of dictionaries where each dictionary represents data for a customer,\n                          e.g., [{'name' : 'John', 'email': 'john@domain.com', 'country': 'USA'}]\n\n    Returns:\n    dict: A dictionary with the count of customers by country.\n    matplotlib.figure.Figure: A bar chart of customer count by country.\n\n    Requirements:\n    - collections\n    - pandas\n    - matplotlib.pyplot\n    - re\n\n    Example:\n    >>> customer_data = [{'name' : 'John', 'email': 'john@domain.com', 'country': 'USA'}]\n    >>> country_counts, fig = analyze_customer_data(customer_data)\n    >>> print(country_counts)\n    >>> fig.show()\n    \"\"\"\n    valid_customers = [customer for customer in customer_data if re.match(EMAIL_PATTERN, customer['email'])]\n    \n    country_counts = collections.Counter([customer['country'] for customer in valid_customers])\n\n    customer_df = pd.DataFrame(valid_customers)\n    fig = customer_df['country'].value_counts().plot(kind='bar')\n\n    return dict(country_counts), fig\n```"}
{"seed": "{\"task_id\": 638048, \"prompt\": \"def f_638048(list_of_pairs):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"sum([pair[0] for pair in list_of_pairs])\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([(5, 9), (-1, -2), (4, 2)]) == 8\\n\"], \"entry_point\": \"f_638048\", \"intent\": \"sum the first value in each tuple in a list of tuples `list_of_pairs` in python\", \"library\": []}", "generation": "Scenario 1:\npandas, numpy, matplotlib, random\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nCATEGORIES = ['Fruits', 'Vegetables', 'Dairy', 'Bakery', 'Meat']\n\ndef create_and_visualize_data(list_of_pairs):\n    \"\"\"\n    Create a pandas DataFrame from a list of pairs and visualize the data using a bar plot.\n    \n    Parameters:\n    list_of_pairs (list): A list of pairs, where the first element is the category and \n                          the second element is the value.\n    \n    Returns:\n    DataFrame: A pandas DataFrame.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - random\n    \n    Example:\n    >>> list_of_pairs = [('Fruits', 5), ('Vegetables', 9), ('Dairy', -1), ('Bakery', -2), ('Meat', 4)]\n    >>> df = create_and_visualize_data(list_of_pairs)\n    >>> print(df)\n    \"\"\"\n    df = pd.DataFrame(list_of_pairs, columns=['Category', 'Value'])\n    df['Value'] = df['Value'].apply(lambda x: x if x > 0 else randint(0, 10))\n\n    df.plot(kind='bar', x='Category', y='Value', legend=False)\n    plt.title('Category vs Value')\n    plt.ylabel('Value')\n    plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, itertools, functools\n```python\nimport numpy as np\nfrom itertools import chain\nfrom functools import reduce\n\ndef calculate_product_of_pairs(list_of_pairs):\n    \"\"\"\n    Calculate the product of the second values in each tuple in a list of tuples \n    and return the product as a numpy array.\n    \n    Parameters:\n    list_of_pairs (list): A list of tuples, where the first element is the category \n                          and the second element is the numeric value.\n    \n    Returns:\n    numpy.ndarray: A numpy array with the product of the second values.\n    \n    Requirements:\n    - numpy\n    - itertools\n    - functools\n    \n    Example:\n    >>> list_of_pairs = [('Fruits', 5), ('Vegetables', 9), ('Dairy', -1), ('Bakery', -2), ('Meat', 4)]\n    >>> product_array = calculate_product_of_pairs(list_of_pairs)\n    >>> print(product_array)\n    \"\"\"\n    second_values = [pair[1] for pair in list_of_pairs]\n    product = reduce(np.multiply, second_values)\n    product_array = np.array(product)\n\n    return product_array\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, matplotlib, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nCATEGORIES = ['Fruits', 'Vegetables', 'Dairy', 'Bakery', 'Meat']\n\ndef create_and_visualize_data(list_of_pairs):\n    \"\"\"\n    Create a pandas DataFrame from a list of pairs and visualize the data using a bar plot.\n    \n    Parameters:\n    list_of_pairs (list): A list of tuples, where the first element is the category and \n                          the second element is the value.\n    \n    Returns:\n    DataFrame: A pandas DataFrame.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - seaborn\n    \n    Example:\n    >>> list_of_pairs = [('Fruits', 5), ('Vegetables', 9), ('Dairy', -1), ('Bakery', -2), ('Meat', 4)]\n    >>> df = create_and_visualize_data(list_of_pairs)\n    >>> print(df)\n    \"\"\"\n    df = pd.DataFrame(list_of_pairs, columns=['Category', 'Value'])\n    sns.barplot(x='Category', y='Value', data=df)\n    plt.title('Category vs Value')\n    plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, itertools, collections\n```python\nimport numpy as np\nfrom itertools import chain\nfrom collections import Counter\n\ndef calculate_and_count_values(list_of_pairs):\n    \"\"\"\n    Calculate the sum of the first values and the counter of the second values in each tuple \n    in a list of tuples.\n    \n    Parameters:\n    list_of_pairs (list): A list of tuples, where the first element is a numeric value \n                          and the second element is a category.\n    \n    Returns:\n    tuple: A tuple with the sum and the counter as a dictionary.\n    \n    Requirements:\n    - numpy\n    - itertools\n    - collections\n    \n    Example:\n    >>> list_of_pairs = [(5, 'Fruits'), (9, 'Vegetables'), (-1, 'Dairy'), (-2, 'Bakery'), (4, 'Meat')]\n    >>> sum_values, count_values = calculate_and_count_values(list_of_pairs)\n    >>> print(sum_values, count_values)\n    \"\"\"\n    first_values = [pair[0] for pair in list_of_pairs]\n    second_values = [pair[1] for pair in list_of_pairs]\n\n    sum_values = np.sum(first_values)\n    count_values = dict(Counter(second_values))\n\n    return sum_values, count_values\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, matplotlib, sklearn.preprocessing\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Constants\nCATEGORIES = ['Fruits', 'Vegetables', 'Dairy', 'Bakery', 'Meat']\n\ndef create_and_normalize_data(list_of_pairs):\n    \"\"\"\n    Create a pandas DataFrame from a list of pairs and normalize the data using MinMaxScaler.\n    \n    Parameters:\n    list_of_pairs (list): A list of tuples, where the first element is the category and \n                          the second element is the value.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with normalized data.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.preprocessing.MinMaxScaler\n    \n    Example:\n    >>> list_of_pairs = [('Fruits', 5), ('Vegetables', 9), ('Dairy', -1), ('Bakery', -2), ('Meat', 4)]\n    >>> df = create_and_normalize_data(list_of_pairs)\n    >>> print(df)\n    \"\"\"\n    df = pd.DataFrame(list_of_pairs, columns=['Category', 'Value'])\n\n    scaler = MinMaxScaler()\n    df['Value'] = scaler.fit_transform(df[['Value']])\n\n    return df\n```"}
{"seed": "{\"task_id\": 14950260, \"prompt\": \"def f_14950260():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"ast.literal_eval(\\\"{'code1':1,'code2':1}\\\")\", \"test_start\": \"\\nimport ast\\n\\ndef check(candidate):\", \"test\": [\"\\n    d = candidate()\\n    exp_result = {'code1' : 1, 'code2': 1}\\n    for key in d:\\n        if key not in exp_result:\\n            assert False\\n        else:\\n            assert d[key] == exp_result[key]\\n\"], \"entry_point\": \"f_14950260\", \"intent\": \"convert unicode string u\\\"{'code1':1,'code2':1}\\\" into dictionary\", \"library\": [\"ast\"]}", "generation": "Scenario 1:\nast, json, collections\n```python\nimport ast\nimport json\nfrom collections import Counter\n\n# Constants\nJSON_FILE = 'data.json'\n\ndef parse_and_analyze_json(json_file):\n    \"\"\"\n    Load a JSON file, convert each string representation of a dictionary into a Python dictionary \n    using ast.literal_eval, and analyze the frequency of each key.\n\n    Parameters:\n    json_file (str): The path to the JSON file.\n\n    Returns:\n    collections.Counter: A Counter object with the frequency of each key.\n\n    Requirements:\n    - ast\n    - json\n    - collections.Counter\n\n    Example:\n    >>> parse_and_analyze_json(JSON_FILE)\n    \"\"\"\n    with open(json_file, 'r') as file:\n        data = json.load(file)\n\n    counter = Counter()\n    for item in data:\n        dict_item = ast.literal_eval(item)\n        counter.update(dict_item.keys())\n\n    return counter\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nast, os, glob\n```python\nimport ast\nimport os\nimport glob\n\n# Constants\nDIRECTORY = 'data'\n\ndef batch_convert_files(directory):\n    \"\"\"\n    Convert all unicode string representations of dictionaries in all text files in a \n    specified directory into Python dictionaries.\n\n    Parameters:\n    directory (str): The path to the directory.\n\n    Returns:\n    list: A list of dictionaries.\n\n    Requirements:\n    - ast\n    - os\n    - glob\n\n    Example:\n    >>> batch_convert_files(DIRECTORY)\n    \"\"\"\n    path = os.path.join(directory, '*.txt')\n    files = glob.glob(path)\n\n    results = []\n    for file in files:\n        with open(file, 'r') as f:\n            for line in f:\n                results.append(ast.literal_eval(line.strip()))\n\n    return results\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nast, pandas, seaborn\n```python\nimport ast\nimport pandas as pd\nimport seaborn as sns\n\n# Constants\nCSV_FILE = 'data.csv'\n\ndef analyze_data(csv_file):\n    \"\"\"\n    Read a CSV file, convert the string representations of dictionaries in a specific \n    column into Python dictionaries, and visualize the data with Seaborn.\n\n    Parameters:\n    csv_file (str): The path to the CSV file.\n\n    Requirements:\n    - ast\n    - pandas\n    - seaborn\n\n    Example:\n    >>> analyze_data(CSV_FILE)\n    \"\"\"\n    df = pd.read_csv(csv_file)\n    df['dict_column'] = df['dict_column'].apply(ast.literal_eval)\n    \n    sns.pairplot(df, hue='dict_column')\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nast, requests, bs4\n```python\nimport ast\nimport requests\nfrom bs4 import BeautifulSoup\n\n# Constants\nURL = 'https://example.com'\n\ndef scrape_and_parse(url):\n    \"\"\"\n    Scrape a webpage, find all string representations of dictionaries in the page's \n    text, and convert them into Python dictionaries.\n\n    Parameters:\n    url (str): The URL of the webpage.\n\n    Returns:\n    list: A list of dictionaries.\n\n    Requirements:\n    - ast\n    - requests\n    - bs4\n\n    Example:\n    >>> scrape_and_parse(URL)\n    \"\"\"\n    response = requests.get(url)\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    results = []\n    for script in soup.find_all('script'):\n        try:\n            results.append(ast.literal_eval(script.string))\n        except (ValueError, SyntaxError):\n            continue\n\n    return results\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nast, re, csv\n```python\nimport ast\nimport re\nimport csv\n\n# Constants\nTEXT_FILE = 'data.txt'\n\ndef extract_and_convert(text_file):\n    \"\"\"\n    Extract all string representations of dictionaries from a text file using regular \n    expressions and convert them into Python dictionaries.\n\n    Parameters:\n    text_file (str): The path to the text file.\n\n    Returns:\n    list: A list of dictionaries.\n\n    Requirements:\n    - ast\n    - re\n    - csv\n\n    Example:\n    >>> extract_and_convert(TEXT_FILE)\n    \"\"\"\n    with open(text_file, 'r') as file:\n        text = file.read()\n\n    pattern = re.compile(r\"\\{.*?\\}\")\n    matches = pattern.findall(text)\n\n    results = [ast.literal_eval(match) for match in matches]\n\n    return results\n```"}
{"seed": "{\"task_id\": 11416772, \"prompt\": \"def f_11416772(mystring):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"[word for word in mystring.split() if word.startswith('$')]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    str = \\\"$abc def $efg $hij klm $\\\"\\n    exp_result = ['$abc', '$efg', '$hij', '$']\\n    assert sorted(candidate(str)) == sorted(exp_result)\\n\"], \"entry_point\": \"f_11416772\", \"intent\": \"find all words in a string `mystring` that start with the `$` sign\", \"library\": []}", "generation": "Scenario 1: \nre, nltk, string, collections\n```python\nimport re\nimport nltk\nfrom string import punctuation\nfrom collections import Counter\n\n# Constants\nPUNCTUATION = set(punctuation)\n\ndef count_most_frequent_dollar_words(text):\n    \"\"\"\n    Count the most frequent words in a text that start with the '$' sign and return the top 5.\n\n    Parameters:\n    text (str): The input text.\n\n    Returns:\n    list: A list of tuples where each tuple contains a word and its frequency.\n\n    Requirements:\n    - re\n    - nltk\n    - string\n    - collections\n\n    Example:\n    >>> text = \"$abc def $efg $hij klm $ $abc $abc $hij $hij\"\n    >>> count_most_frequent_dollar_words(text)\n    [('abc', 3), ('hij', 3), ('efg', 1), ('$', 1)]\n    \"\"\"\n    words = nltk.word_tokenize(text)\n    dollar_words = [word for word in words if word.startswith('$') and not all(c in PUNCTUATION for c in word)]\n    freq = Counter(dollar_words)\n    return freq.most_common(5)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nre, nltk, string, matplotlib\n```python\nimport re\nimport nltk\nfrom string import punctuation\nimport matplotlib.pyplot as plt\n\n# Constants\nPUNCTUATION = set(punctuation)\n\ndef plot_dollar_words_frequency(text):\n    \"\"\"\n    Plot a bar chart of the frequency of words in a text that start with the '$' sign.\n\n    Parameters:\n    text (str): The input text.\n\n    Returns:\n    None\n\n    Requirements:\n    - re\n    - nltk\n    - string\n    - matplotlib\n\n    Example:\n    >>> text = \"$abc def $efg $hij klm $ $abc $abc $hij $hij\"\n    >>> plot_dollar_words_frequency(text)\n    \"\"\"\n    words = nltk.word_tokenize(text)\n    dollar_words = [word for word in words if word.startswith('$') and not all(c in PUNCTUATION for c in word)]\n    freq = nltk.FreqDist(dollar_words)\n    \n    plt.figure(figsize=(10, 5))\n    freq.plot(30, cumulative=False)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nre, nltk, string, pandas\n```python\nimport re\nimport nltk\nfrom string import punctuation\nimport pandas as pd\n\n# Constants\nPUNCTUATION = set(punctuation)\n\ndef dollar_words_to_dataframe(text):\n    \"\"\"\n    Convert all words in a text that start with the '$' sign to a pandas DataFrame.\n\n    Parameters:\n    text (str): The input text.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the '$' words and their occurrences.\n\n    Requirements:\n    - re\n    - nltk\n    - string\n    - pandas\n\n    Example:\n    >>> text = \"$abc def $efg $hij klm $ $abc $abc $hij $hij\"\n    >>> dollar_words_to_dataframe(text)\n    \"\"\"\n    words = nltk.word_tokenize(text)\n    dollar_words = [word for word in words if word.startswith('$') and not all(c in PUNCTUATION for c in word)]\n    freq = nltk.FreqDist(dollar_words)\n    df = pd.DataFrame(list(freq.items()), columns=[\"Word\", \"Frequency\"])\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nre, nltk, string, os\n```python\nimport re\nimport nltk\nfrom string import punctuation\nimport os\n\n# Constants\nPUNCTUATION = set(punctuation)\n\ndef save_dollar_words_to_file(text, filename):\n    \"\"\"\n    Save all words in a text that start with the '$' sign to a text file.\n\n    Parameters:\n    text (str): The input text.\n    filename (str): The name of the text file to save the '$' words.\n\n    Returns:\n    str: The path of the saved text file.\n\n    Requirements:\n    - re\n    - nltk\n    - string\n    - os\n\n    Example:\n    >>> text = \"$abc def $efg $hij klm $ $abc $abc $hij $hij\"\n    >>> save_dollar_words_to_file(text, 'dollar_words.txt')\n    \"\"\"\n    words = nltk.word_tokenize(text)\n    dollar_words = [word for word in words if word.startswith('$') and not all(c in PUNCTUATION for c in word)]\n    with open(filename, 'w') as f:\n        for word in dollar_words:\n            f.write(word + '\\n')\n    return os.path.abspath(filename)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nre, nltk, string, csv\n```python\nimport re\nimport nltk\nfrom string import punctuation\nimport csv\n\n# Constants\nPUNCTUATION = set(punctuation)\n\ndef save_dollar_words_to_csv(text, filename):\n    \"\"\"\n    Save all words in a text that start with the '$' sign to a CSV file.\n\n    Parameters:\n    text (str): The input text.\n    filename (str): The name of the CSV file to save the '$' words.\n\n    Returns:\n    str: The path of the saved CSV file.\n\n    Requirements:\n    - re\n    - nltk\n    - string\n    - csv\n\n    Example:\n    >>> text = \"$abc def $efg $hij klm $ $abc $abc $hij $hij\"\n    >>> save_dollar_words_to_csv(text, 'dollar_words.csv')\n    \"\"\"\n    words = nltk.word_tokenize(text)\n    dollar_words = [word for word in words if word.startswith('$') and not all(c in PUNCTUATION for c in word)]\n    with open(filename, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow([\"Word\"])\n        for word in dollar_words:\n            writer.writerow([word])\n    return os.path.abspath(filename)\n```"}
{"seed": "{\"task_id\": 11331982, \"prompt\": \"def f_11331982(text):\\n\\t\", \"suffix\": \"\\n\\treturn text\", \"canonical_solution\": \"text = re.sub('^https?:\\\\\\\\/\\\\\\\\/.*[\\\\\\\\r\\\\\\\\n]*', '', text, flags=re.MULTILINE)\", \"test_start\": \"\\nimport re\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(\\\"https://www.wikipedia.org/ click at\\\") == \\\"\\\"\\n\"], \"entry_point\": \"f_11331982\", \"intent\": \"remove any url within string `text`\", \"library\": [\"re\"]}", "generation": "Scenario 1:\nre, nltk, string\n```python\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom string import punctuation\n\n# Constants\nSTOPWORDS = set(stopwords.words('english'))\nPUNCTUATION = set(punctuation)\n\ndef clean_text(text):\n    \"\"\"\n    Clean the given text by removing URLs, stopwords, and punctuation.\n\n    Parameters:\n    text (str): The text to clean.\n\n    Returns:\n    str: The cleaned text.\n\n    Requirements:\n    - re\n    - nltk.corpus.stopwords\n    - string.punctuation\n\n    Example:\n    >>> clean_text('Visit https://www.python.org for more info. I love to eat apples.')\n    'Visit  info I love eat apples'\n    \"\"\"\n    # Remove URLs\n    text = re.sub('http[s]?://\\S+', '', text)\n\n    # Tokenize the text\n    words = nltk.word_tokenize(text)\n\n    # Remove stopwords and punctuation\n    cleaned_words = [word for word in words if word not in STOPWORDS and word not in PUNCTUATION]\n\n    return ' '.join(cleaned_words)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nre, nltk, collections\n```python\nimport re\nimport nltk\nfrom collections import Counter\n\ndef count_top_words(text, top_n):\n    \"\"\"\n    Count the top N most frequent words in a text after removing URLs.\n\n    Parameters:\n    text (str): The text to analyze.\n    top_n (int): The number of top words to return.\n\n    Returns:\n    list: A list of tuples where each tuple contains a word and its frequency.\n\n    Requirements:\n    - re\n    - nltk\n    - collections.Counter\n\n    Example:\n    >>> count_top_words('Visit https://www.python.org for more info. Python is great. I love Python.', 2)\n    [('Python', 2), ('Visit', 1)]\n    \"\"\"\n    # Remove URLs\n    text = re.sub('http[s]?://\\S+', '', text)\n    \n    # Tokenize the text\n    words = nltk.word_tokenize(text)\n\n    # Count the frequency of each word\n    word_freq = Counter(words)\n\n    return word_freq.most_common(top_n)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nre, nltk, gensim\n```python\nimport re\nimport nltk\nfrom gensim.summarization import summarize\n\ndef generate_summary(text):\n    \"\"\"\n    Generate a summary of a text after removing URLs.\n\n    Parameters:\n    text (str): The text to summarize.\n\n    Returns:\n    str: The summary of the text.\n\n    Requirements:\n    - re\n    - nltk\n    - gensim.summarization.summarize\n\n    Example:\n    >>> generate_summary('Visit https://www.python.org for more info. Python is great. I love Python.')\n    'Python is great. I love Python.'\n    \"\"\"\n    # Remove URLs\n    text = re.sub('http[s]?://\\S+', '', text)\n\n    return summarize(text)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nre, nltk, sklearn\n```python\nimport re\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ndef calculate_tfidf(texts):\n    \"\"\"\n    Calculate the TF-IDF score for each word in a collection of texts after removing URLs.\n\n    Parameters:\n    texts (list of str): The texts to analyze.\n\n    Returns:\n    csr_matrix: A sparse matrix of TF-IDF scores.\n\n    Requirements:\n    - re\n    - sklearn.feature_extraction.text.TfidfVectorizer\n\n    Example:\n    >>> calculate_tfidf(['Visit https://www.python.org for more info.', 'Python is great.', 'I love Python.'])\n    \"\"\"\n    # Remove URLs\n    cleaned_texts = [re.sub('http[s]?://\\S+', '', text) for text in texts]\n\n    vectorizer = TfidfVectorizer()\n    tfidf_matrix = vectorizer.fit_transform(cleaned_texts)\n\n    return tfidf_matrix\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nre, nltk, wordcloud\n```python\nimport re\nimport nltk\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\ndef generate_word_cloud(text):\n    \"\"\"\n    Generate a word cloud from a text after removing URLs.\n\n    Parameters:\n    text (str): The text to analyze.\n\n    Returns:\n    None. Displays the word cloud.\n\n    Requirements:\n    - re\n    - nltk\n    - wordcloud.WordCloud\n    - matplotlib.pyplot\n\n    Example:\n    >>> generate_word_cloud('Visit https://www.python.org for more info. Python is great. I love Python.')\n    \"\"\"\n    # Remove URLs\n    text = re.sub('http[s]?://\\S+', '', text)\n\n    wordcloud = WordCloud().generate(text)\n\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis(\"off\")\n    plt.show()\n```\n\nAbove is the illustration."}
{"seed": "{\"task_id\": 34945274, \"prompt\": \"def f_34945274(A):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"np.where(np.in1d(A, [1, 3, 4]).reshape(A.shape), A, 0)\", \"test_start\": \"\\nimport numpy as np\\n\\ndef check(candidate):\", \"test\": [\"\\n    A = np.array([[6, 8, 1, 3, 4], [-1, 0, 3, 5, 1]])\\n    B = np.array([[0, 0, 1, 3, 4], [0, 0, 3, 0, 1]])\\n    assert np.array_equal(candidate(A), B)\\n\"], \"entry_point\": \"f_34945274\", \"intent\": \"replace all elements in array `A` that are not present in array `[1, 3, 4]` with zeros\", \"library\": [\"numpy\"]}", "generation": "Scenario 1:\nnumpy, pandas, seaborn, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nTARGET_VALUES = np.array([1, 3, 4])\n\ndef plot_distribution_after_replacement(df):\n    \"\"\"\n    Replace all elements in DataFrame columns that are not present in the TARGET_VALUES array with zeros,\n    then plot the distribution of each column after replacement.\n\n    Parameters:\n    df (DataFrame): The input pandas DataFrame.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - pandas\n    - seaborn\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = pd.DataFrame(np.random.randint(0,10,size=(100, 5)), columns=list('ABCDE'))\n    >>> plot_distribution_after_replacement(df)\n    \"\"\"\n    df = df.applymap(lambda x: x if x in TARGET_VALUES else 0)\n    plt.figure(figsize=(10, 5))\n\n    for column in df.columns:\n        sns.kdeplot(df[column], label=column)\n\n    plt.legend()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, pandas, matplotlib, sklearn\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n# Constants\nTARGET_VALUES = np.array([1, 3, 4])\n\ndef linear_regression_after_replacement(df, target_column):\n    \"\"\"\n    Replace all elements in DataFrame columns that are not present in the TARGET_VALUES array with zeros,\n    then perform a linear regression using the target column.\n\n    Parameters:\n    df (DataFrame): The input pandas DataFrame.\n    target_column (str): The target column for the linear regression.\n\n    Returns:\n    LinearRegression: The trained Linear Regression model.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - sklearn.linear_model.LinearRegression\n\n    Example:\n    >>> df = pd.DataFrame(np.random.randint(0,10,size=(100, 5)), columns=list('ABCDE'))\n    >>> model = linear_regression_after_replacement(df, 'E')\n    >>> print(model.coef_)\n    \"\"\"\n    df = df.applymap(lambda x: x if x in TARGET_VALUES else 0)\n\n    X = df.drop(target_column, axis=1)\n    y = df[target_column]\n\n    model = LinearRegression().fit(X, y)\n\n    return model\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, pandas, scipy, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\n# Constants\nTARGET_VALUES = np.array([1, 3, 4])\n\ndef boxcox_transformation_after_replacement(df):\n    \"\"\"\n    Replace all elements in DataFrame columns that are not present in the TARGET_VALUES array with zeros,\n    then perform a Box-Cox transformation on each column and plot the resulting distributions.\n\n    Parameters:\n    df (DataFrame): The input pandas DataFrame.\n\n    Returns:\n    DataFrame: The transformed DataFrame.\n\n    Requirements:\n    - numpy\n    - pandas\n    - scipy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = pd.DataFrame(np.random.randint(1,10,size=(100, 5)), columns=list('ABCDE'))  # Values should be positive for Box-Cox\n    >>> transformed_df = boxcox_transformation_after_replacement(df)\n    >>> print(transformed_df)\n    \"\"\"\n    df = df.applymap(lambda x: x if x in TARGET_VALUES else 0)\n\n    transformed_df = pd.DataFrame()\n\n    for column in df.columns:\n        transformed_data, _ = stats.boxcox(df[column] + 1)  # Add 1 to avoid zero values\n        transformed_df[column] = transformed_data\n\n        sns.kdeplot(transformed_df[column], label=column)\n\n    plt.legend()\n    plt.show()\n\n    return transformed_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, pandas, sklearn, scipy\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nfrom scipy.spatial.distance import cdist\n\n# Constants\nTARGET_VALUES = np.array([1, 3, 4])\n\ndef kmeans_clusters_after_replacement(df):\n    \"\"\"\n    Replace all elements in DataFrame columns that are not present in the TARGET_VALUES array with zeros,\n    then perform K-Means clustering on the processed data and return the optimal number of clusters\n    using the Elbow method.\n\n    Parameters:\n    df (DataFrame): The input pandas DataFrame.\n\n    Returns:\n    int: The optimal number of clusters.\n\n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.cluster.KMeans\n    - scipy.spatial.distance.cdist\n\n    Example:\n    >>> df = pd.DataFrame(np.random.randint(0,10,size=(100, 5)), columns=list('ABCDE'))\n    >>> optimal_clusters = kmeans_clusters_after_replacement(df)\n    >>> print(optimal_clusters)\n    \"\"\"\n    df = df.applymap(lambda x: x if x in TARGET_VALUES else 0)\n\n    distortions = []\n    K = range(1, 10)\n\n    for k in K:\n        kmeanModel = KMeans(n_clusters=k).fit(df)\n        distortions.append(sum(np.min(cdist(df, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) / df.shape[0])\n\n    elbow_point = K[distortions.index(min(distortions))]\n\n    return elbow_point\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, pandas, sklearn, seaborn\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\n\n# Constants\nTARGET_VALUES = np.array([1, 3, 4])\n\ndef feature_importance_after_replacement(df, target_column):\n    \"\"\"\n    Replace all elements in DataFrame columns that are not present in the TARGET_VALUES array with zeros,\n    then train a Random Forest Classifier and plot the feature importance.\n\n    Parameters:\n    df (DataFrame): The input pandas DataFrame.\n    target_column (str): The target column for the Random Forest Classifier.\n\n    Returns:\n    RandomForestClassifier: The trained Random Forest Classifier.\n\n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.ensemble.RandomForestClassifier\n    - seaborn\n    \n    Example:\n    >>> df = pd.DataFrame(np.random.randint(0,10,size=(100, 5)), columns=list('ABCDE'))\n    >>> model = feature_importance_after_replacement(df, 'E')\n    >>> print(model.feature_importances_)\n    \"\"\"\n    df = df.applymap(lambda x: x if x in TARGET_VALUES else 0)\n\n    X = df.drop(target_column, axis=1)\n    y = df[target_column]\n\n    model = RandomForestClassifier().fit(X, y)\n\n    feature_imp = pd.Series(model.feature_importances_,index=X.columns).sort_values(ascending=False)\n    sns.barplot(x=feature_imp, y=feature_imp.index)\n\n    plt.xlabel('Feature Importance Score')\n    plt.ylabel('Features')\n    plt.title(\"Visualizing Important Features\")\n    plt.show()\n\n    return model\n```\n"}
{"seed": "{\"task_id\": 15819980, \"prompt\": \"def f_15819980(a):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"np.mean(a, axis=1)\", \"test_start\": \"\\nimport numpy as np\\n\\ndef check(candidate):\", \"test\": [\"\\n    A = np.array([[6, 8, 1, 3, 4], [-1, 0, 3, 5, 1]])\\n    B = np.array([4.4, 1.6])\\n    assert np.array_equal(candidate(A), B)\\n\"], \"entry_point\": \"f_15819980\", \"intent\": \"calculate mean across dimension in a 2d array `a`\", \"library\": [\"numpy\"]}", "generation": "Scenario 1: \nnumpy, pandas, sklearn.preprocessing, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\n# Constants\nFEATURE_NAMES = ['Feature 1', 'Feature 2', 'Feature 3', 'Feature 4', 'Feature 5']\n\ndef process_and_visualize_data(data_matrix):\n    \"\"\"\n    Standardize a 2D data matrix, calculate the mean of each row, \n    then visualize the distribution of the means.\n\n    Parameters:\n    data_matrix (numpy.array): The 2D data matrix.\n    \n    Returns:\n    pandas.DataFrame: A DataFrame containing the standardized data \n                      and the mean of each row.\n\n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.preprocessing.StandardScaler\n    - matplotlib.pyplot\n\n    Example:\n    >>> data = np.array([[6, 8, 1, 3, 4], [-1, 0, 3, 5, 1]])\n    >>> df = process_and_visualize_data(data)\n    >>> print(df)\n    \"\"\"\n    scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(data_matrix)\n\n    df = pd.DataFrame(standardized_data, columns=FEATURE_NAMES)\n    df['Mean'] = df.mean(axis=1)\n\n    df['Mean'].plot(kind='hist', title='Distribution of Means')\n    plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nnumpy, scipy.stats, matplotlib, pandas\n```python\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import ttest_1samp\nimport matplotlib.pyplot as plt\n\n# Constants\nALPHA = 0.05\n\ndef test_and_plot_means(data_matrix):\n    \"\"\"\n    Calculate the mean of each row in a 2D data matrix, perform a one-sample t-test \n    against the population mean, and plot the means that are significantly different.\n\n    Parameters:\n    data_matrix (numpy.array): The 2D data matrix.\n    \n    Returns:\n    list: A list of indices of the means that are significantly different from the population mean.\n\n    Requirements:\n    - numpy\n    - scipy.stats.ttest_1samp\n    - matplotlib.pyplot\n    - pandas\n\n    Example:\n    >>> data = np.array([[6, 8, 1, 3, 4], [-1, 0, 3, 5, 1]])\n    >>> indices = test_and_plot_means(data)\n    >>> print(indices)\n    \"\"\"\n    means = np.mean(data_matrix, axis=1)\n    population_mean = np.mean(data_matrix)\n\n    _, p_value = ttest_1samp(means, population_mean)\n    significant_indices = np.where(p_value < ALPHA)[0]\n\n    plt.figure(figsize=(10, 5))\n    plt.plot(means, 'ro', label='Means')\n    plt.plot(significant_indices, means[significant_indices], 'bo', label='Significant Means')\n    plt.axhline(y=population_mean, color='g', linestyle='-', label='Population Mean')\n    plt.legend()\n    plt.show()\n\n    return significant_indices.tolist()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnumpy, pandas, seaborn, scipy.stats\n```python\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom scipy.stats import zscore\n\n# Constants\nFEATURE_NAMES = ['Feature 1', 'Feature 2', 'Feature 3', 'Feature 4', 'Feature 5']\n\ndef analyze_and_visualize_data(data_matrix):\n    \"\"\"\n    Calculate the Z-scores of a 2D data matrix, calculate the mean of each row,\n    then visualize the correlation matrix of the Z-scores.\n\n    Parameters:\n    data_matrix (numpy.array): The 2D data matrix.\n    \n    Returns:\n    pandas.DataFrame: A DataFrame containing the Z-scores and the mean of each row.\n\n    Requirements:\n    - numpy\n    - pandas\n    - seaborn\n    - scipy.stats.zscore\n\n    Example:\n    >>> data = np.array([[6, 8, 1, 3, 4], [-1, 0, 3, 5, 1]])\n    >>> df = analyze_and_visualize_data(data)\n    >>> print(df)\n    \"\"\"\n    z_scores = zscore(data_matrix, axis=1)\n    df = pd.DataFrame(z_scores, columns=FEATURE_NAMES)\n    df['Mean'] = df.mean(axis=1)\n\n    correlation_matrix = df.corr()\n    sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\")\n    plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, pandas, matplotlib, scipy.stats\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew\n\n# Constants\nFEATURE_NAMES = ['Feature 1', 'Feature 2', 'Feature 3', 'Feature 4', 'Feature 5']\n\ndef calculate_skewness_and_plot(data_matrix):\n    \"\"\"\n    Calculate the skewness of each row in a 2D data matrix and plot the distribution.\n\n    Parameters:\n    data_matrix (numpy.array): The 2D data matrix.\n    \n    Returns:\n    pandas.DataFrame: A DataFrame containing the skewness of each row.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - scipy.stats.skew\n\n    Example:\n    >>> data = np.array([[6, 8, 1, 3, 4], [-1, 0, 3, 5, 1]])\n    >>> df = calculate_skewness_and_plot(data)\n    >>> print(df)\n    \"\"\"\n    skewness = skew(data_matrix, axis=1)\n    df = pd.DataFrame(skewness, columns=['Skewness'])\n\n    df['Skewness'].plot(kind='hist', title='Distribution of Skewness')\n    plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnumpy, pandas, matplotlib, sklearn.decomposition\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\n# Constants\nFEATURE_NAMES = ['Feature 1', 'Feature 2', 'Feature 3', 'Feature 4', 'Feature 5']\n\ndef apply_pca_and_plot(data_matrix):\n    \"\"\"\n    Apply PCA to a 2D data matrix, calculate the mean of each component,\n    then plot the explained variance ratio.\n\n    Parameters:\n    data_matrix (numpy.array): The 2D data matrix.\n    \n    Returns:\n    pandas.DataFrame: A DataFrame containing the PCA transformed data\n                      and the mean of each component.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - sklearn.decomposition.PCA\n\n    Example:\n    >>> data = np.array([[6, 8, 1, 3, 4], [-1, 0, 3, 5, 1]])\n    >>> df = apply_pca_and_plot(data)\n    >>> print(df)\n    \"\"\"\n    pca = PCA()\n    transformed_data = pca.fit_transform(data_matrix)\n\n    df = pd.DataFrame(transformed_data, columns=[f'Component {i+1}' for i in range(transformed_data.shape[1])])\n    df['Mean'] = df.mean(axis=1)\n\n    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n    plt.xlabel('Number of Components')\n    plt.ylabel('Cumulative Explained Variance')\n    plt.show()\n\n    return df\n```"}
{"seed": "{\"task_id\": 19894365, \"prompt\": \"def f_19894365():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"subprocess.call(['/usr/bin/Rscript', '--vanilla', '/pathto/MyrScript.r'])\", \"test_start\": \"\\nfrom unittest.mock import Mock\\nimport subprocess\\n\\ndef check(candidate):\", \"test\": [\"\\n    subprocess.call = Mock(return_value = 0)\\n    assert candidate() == 0\\n\"], \"entry_point\": \"f_19894365\", \"intent\": \"running r script '/pathto/MyrScript.r' from python\", \"library\": [\"subprocess\"]}", "generation": "Scenario 1: \nsubprocess, os, glob, time\n```python\nimport subprocess\nimport os\nimport glob\nimport time\n\n# Constants\nR_SCRIPTS_DIR = '/pathto/rscripts/'\n\ndef run_all_r_scripts():\n    \"\"\"\n    Run all R scripts in a specified directory and return their execution times.\n\n    Returns:\n    dict: A dictionary with the script names as keys and their execution times as values.\n\n    Requirements:\n    - subprocess\n    - os\n    - glob\n    - time\n\n    Example:\n    >>> run_all_r_scripts()\n    \"\"\"\n    execution_times = {}\n    r_scripts = glob.glob(os.path.join(R_SCRIPTS_DIR, '*.r'))\n\n    for r_script in r_scripts:\n        start_time = time.time()\n        subprocess.call(['/usr/bin/Rscript', '--vanilla', r_script])\n        end_time = time.time()\n        execution_times[os.path.basename(r_script)] = end_time - start_time\n\n    return execution_times\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nsubprocess, pandas, matplotlib.pyplot\n```python\nimport subprocess\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nSCRIPT_PATH = '/pathto/MyrScript.r'\nOUTPUT_FILE = '/pathto/output.csv'\n\ndef run_r_script_and_plot_output():\n    \"\"\"\n    Run an R script that generates a CSV file and plot the output data.\n\n    Requirements:\n    - subprocess\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> run_r_script_and_plot_output()\n    \"\"\"\n    subprocess.call(['/usr/bin/Rscript', '--vanilla', SCRIPT_PATH])\n    data = pd.read_csv(OUTPUT_FILE)\n    data.plot()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nsubprocess, shlex, datetime\n```python\nimport subprocess\nimport shlex\nfrom datetime import datetime\n\n# Constants\nSCRIPT_PATH = '/pathto/MyrScript.r'\nLOG_FILE = '/pathto/log.txt'\n\ndef run_r_script_with_logging():\n    \"\"\"\n    Run an R script and log the start time, end time, and output to a file.\n\n    Requirements:\n    - subprocess\n    - shlex\n    - datetime\n\n    Example:\n    >>> run_r_script_with_logging()\n    \"\"\"\n    start_time = datetime.now()\n    process = subprocess.Popen(shlex.split(f\"/usr/bin/Rscript --vanilla {SCRIPT_PATH}\"),\n                               stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    stdout, stderr = process.communicate()\n    end_time = datetime.now()\n\n    with open(LOG_FILE, 'a') as f:\n        f.write(f'Start Time: {start_time}\\n')\n        f.write(f'End Time: {end_time}\\n')\n        f.write(f'Stdout: {stdout}\\n')\n        f.write(f'Stderr: {stderr}\\n')\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nsubprocess, tempfile, shutil\n```python\nimport subprocess\nimport tempfile\nimport shutil\n\n# Constants\nSCRIPT_PATH = '/pathto/MyrScript.r'\n\ndef run_r_script_in_temp_dir():\n    \"\"\"\n    Run an R script in a temporary directory and clean up after execution.\n\n    Requirements:\n    - subprocess\n    - tempfile\n    - shutil\n\n    Example:\n    >>> run_r_script_in_temp_dir()\n    \"\"\"\n    with tempfile.TemporaryDirectory() as temp_dir:\n        shutil.copy(SCRIPT_PATH, temp_dir)\n        temp_script_path = os.path.join(temp_dir, os.path.basename(SCRIPT_PATH))\n        subprocess.call(['/usr/bin/Rscript', '--vanilla', temp_script_path])\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nsubprocess, os, time, threading\n```python\nimport subprocess\nimport os\nimport time\nimport threading\n\n# Constants\nSCRIPT_PATH = '/pathto/MyrScript.r'\nTIMEOUT = 60\n\ndef run_r_script_with_timeout():\n    \"\"\"\n    Run an R script with a specified timeout. If the script does not finish within the timeout, kill it.\n\n    Requirements:\n    - subprocess\n    - os\n    - time\n    - threading\n\n    Example:\n    >>> run_r_script_with_timeout()\n    \"\"\"\n    def target():\n        subprocess.call(['/usr/bin/Rscript', '--vanilla', SCRIPT_PATH])\n\n    thread = threading.Thread(target=target)\n    thread.start()\n\n    thread.join(TIMEOUT)\n\n    if thread.is_alive():\n        print('Terminating process')\n        os.system(f'pkill -f \"{SCRIPT_PATH}\"')\n        thread.join()\n```\n"}
{"seed": "{\"task_id\": 19894365, \"prompt\": \"def f_19894365():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"subprocess.call('/usr/bin/Rscript --vanilla /pathto/MyrScript.r', shell=True)\", \"test_start\": \"\\nfrom unittest.mock import Mock\\nimport subprocess\\n\\ndef check(candidate):\", \"test\": [\"\\n    subprocess.call = Mock(return_value = 0)\\n    assert candidate() == 0\\n\"], \"entry_point\": \"f_19894365\", \"intent\": \"run r script '/usr/bin/Rscript --vanilla /pathto/MyrScript.r'\", \"library\": [\"subprocess\"]}", "generation": "Scenario 1: \nsubprocess, os, shutil, datetime\n```python\nimport subprocess\nimport os\nimport shutil\nfrom datetime import datetime\n\n# Constants\nR_SCRIPT_PATH = '/usr/bin/Rscript --vanilla /pathto/MyrScript.r'\nBACKUP_PATH = '/backup/'\n\ndef run_and_backup_r_script():\n    \"\"\"\n    Run an R script and then backup the script to a backup directory with current date.\n\n    Returns:\n    int: The subprocess.call return value.\n\n    Requirements:\n    - subprocess\n    - os\n    - shutil\n    - datetime\n\n    Example:\n    >>> run_and_backup_r_script()\n    \"\"\"\n    ret_val = subprocess.call(R_SCRIPT_PATH, shell=True)\n\n    today = datetime.now().strftime('%Y%m%d')\n    backup_file = BACKUP_PATH + 'MyrScript_' + today + '.r'\n    shutil.copy(R_SCRIPT_PATH, backup_file)\n\n    return ret_val\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nsubprocess, os, re, sys\n```python\nimport subprocess\nimport os\nimport re\nimport sys\n\n# Constants\nR_SCRIPT_PATH = '/usr/bin/Rscript --vanilla /pathto/MyrScript.r'\n\ndef run_r_script_and_check_log():\n    \"\"\"\n    Run an R script and check the log. If there is an error message, return the error message.\n\n    Returns:\n    str: The error message in the log.\n\n    Requirements:\n    - subprocess\n    - os\n    - re\n    - sys\n\n    Example:\n    >>> run_r_script_and_check_log()\n    \"\"\"\n    log_file = '/tmp/log.txt'\n    command = R_SCRIPT_PATH + ' > ' + log_file\n    subprocess.call(command, shell=True)\n\n    with open(log_file, 'r') as f:\n        log_content = f.read()\n\n    error_message = re.search('Error: (.*)', log_content)\n    if error_message:\n        return error_message.group(1)\n    else:\n        return \"No error\"\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nsubprocess, os, time, glob\n```python\nimport subprocess\nimport os\nimport time\nimport glob\n\n# Constants\nR_SCRIPT_PATH = '/usr/bin/Rscript --vanilla /pathto/MyrScript.r'\n\ndef run_r_script_and_check_output():\n    \"\"\"\n    Run an R script and check if the output file is generated within the specified duration.\n\n    Returns:\n    bool: True if the output file is generated within the specified duration, False otherwise.\n\n    Requirements:\n    - subprocess\n    - os\n    - time\n    - glob\n\n    Example:\n    >>> run_r_script_and_check_output()\n    \"\"\"\n    subprocess.call(R_SCRIPT_PATH, shell=True)\n\n    start_time = time.time()\n    while time.time() - start_time < 10:\n        if glob.glob('/path_to_output/*.csv'):\n            return True\n        time.sleep(0.1)\n\n    return False\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nsubprocess, os, shlex, shutil\n```python\nimport subprocess\nimport os\nimport shlex\nimport shutil\n\n# Constants\nR_SCRIPT_PATH = '/usr/bin/Rscript --vanilla /pathto/MyrScript.r'\n\ndef run_r_script_in_new_directory():\n    \"\"\"\n    Create a new directory, copy the R script into the new directory, and run the script in the new directory.\n\n    Returns:\n    int: The subprocess.call return value.\n\n    Requirements:\n    - subprocess\n    - os\n    - shlex\n    - shutil\n\n    Example:\n    >>> run_r_script_in_new_directory()\n    \"\"\"\n    new_dir = '/tmp/new_directory'\n    os.makedirs(new_dir)\n\n    new_script_path = os.path.join(new_dir, 'MyrScript.r')\n    shutil.copy(R_SCRIPT_PATH, new_script_path)\n\n    command = shlex.split('Rscript --vanilla ' + new_script_path)\n    return subprocess.call(command)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nsubprocess, os, sys, argparse\n```python\nimport subprocess\nimport os\nimport sys\nimport argparse\n\ndef run_r_script_with_arguments():\n    \"\"\"\n    Parse command line arguments and run the R script with these arguments.\n\n    Returns:\n    int: The subprocess.call return value.\n\n    Requirements:\n    - subprocess\n    - os\n    - sys\n    - argparse\n\n    Example:\n    >>> run_r_script_with_arguments()\n    \"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--arg1', required=True)\n    parser.add_argument('--arg2', required=True)\n    args = parser.parse_args()\n\n    command = 'Rscript --vanilla /pathto/MyrScript.r --arg1 {} --arg2 {}'.format(args.arg1, args.arg2)\n    return subprocess.call(command, shell=True)\n```"}
{"seed": "{\"task_id\": 33058590, \"prompt\": \"def f_33058590(df):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"df.fillna(df.mean(axis=0))\", \"test_start\": \"\\nimport pandas as pd\\nimport numpy as np\\n\\ndef check(candidate):\", \"test\": [\"\\n    df = pd.DataFrame([[1,2,3],[4,5,6],[7.0,np.nan,9.0]], columns=[\\\"c1\\\",\\\"c2\\\",\\\"c3\\\"]) \\n    res = pd.DataFrame([[1,2,3],[4,5,6],[7.0,3.5,9.0]], columns=[\\\"c1\\\",\\\"c2\\\",\\\"c3\\\"])\\n    assert candidate(df).equals(res)\\n\"], \"entry_point\": \"f_33058590\", \"intent\": \"replacing nan in the dataframe `df` with row average\", \"library\": [\"numpy\", \"pandas\"]}", "generation": "Scenario 1:\npandas, numpy, seaborn, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef describe_and_plot_distribution(df):\n    \"\"\"\n    Describe a dataframe and plot a distribution plot for each numeric column \n    after replacing NaN values with the column's average.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n\n    Returns:\n    DataFrame: A pandas DataFrame with statistics.\n\n    Requirements:\n    - pandas\n    - numpy\n    - seaborn\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame([[1,2,3],[4,5,6],[7.0,np.nan,9.0]], columns=[\"c1\",\"c2\",\"c3\"])\n    >>> describe_and_plot_distribution(df)\n    \"\"\"\n    df = df.fillna(df.mean(axis=0))\n    description = df.describe()\n\n    for col in df.select_dtypes(include=[np.number]).columns:\n        sns.displot(df[col], bins=10)\n        plt.show()\n\n    return description\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, sklearn, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\n\ndef normalize_and_boxplot(df):\n    \"\"\"\n    Normalize numeric columns in a DataFrame and draw a boxplot for each column.\n    Missing values are replaced by column's average.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n\n    Returns:\n    DataFrame: A pandas DataFrame after normalization.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.preprocessing.MinMaxScaler\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame([[1,2,3],[4,5,6],[7.0,np.nan,9.0]], columns=[\"c1\",\"c2\",\"c3\"])\n    >>> normalize_and_boxplot(df)\n    \"\"\"\n    df = df.fillna(df.mean(axis=0))\n    scaler = MinMaxScaler()\n    df[df.columns] = scaler.fit_transform(df[df.columns])\n\n    df.boxplot(grid=False, vert=False, fontsize=15)\n    plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, sklearn, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.decomposition import PCA\nimport seaborn as sns\n\ndef pca_and_scatterplot(df):\n    \"\"\"\n    Perform PCA on a DataFrame and draw a scatterplot of the first two principal components.\n    Missing values are replaced by column's average.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the first two principal components.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.decomposition.PCA\n    - seaborn\n\n    Example:\n    >>> df = pd.DataFrame([[1,2,3],[4,5,6],[7.0,np.nan,9.0]], columns=[\"c1\",\"c2\",\"c3\"])\n    >>> pca_and_scatterplot(df)\n    \"\"\"\n    df = df.fillna(df.mean(axis=0))\n    pca = PCA(n_components=2)\n    principalComponents = pca.fit_transform(df)\n    principalDf = pd.DataFrame(data=principalComponents, columns=['principal component 1', 'principal component 2'])\n\n    sns.scatterplot(data=principalDf, x='principal component 1', y='principal component 2')\n    plt.show()\n\n    return principalDf\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, scipy, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import zscore\nimport matplotlib.pyplot as plt\n\ndef zscore_and_histogram(df):\n    \"\"\"\n    Calculate Z-scores for numeric columns in a DataFrame and draw a histogram for each column.\n    Missing values are replaced by column's average.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n\n    Returns:\n    DataFrame: A pandas DataFrame with z-scores.\n\n    Requirements:\n    - pandas\n    - numpy\n    - scipy.stats.zscore\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame([[1,2,3],[4,5,6],[7.0,np.nan,9.0]], columns=[\"c1\",\"c2\",\"c3\"])\n    >>> zscore_and_histogram(df)\n    \"\"\"\n    df = df.fillna(df.mean(axis=0))\n    df = df.apply(zscore)\n\n    df.hist(grid=False, bins=10, layout=(1,df.shape[1]))\n    plt.tight_layout()\n    plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, sklearn, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nimport seaborn as sns\n\ndef standardize_and_heatmap(df):\n    \"\"\"\n    Standardize numeric columns in a DataFrame and draw a heatmap of the correlation matrix.\n    Missing values are replaced by column's average.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n\n    Returns:\n    DataFrame: A pandas DataFrame after standardization.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.preprocessing.StandardScaler\n    - seaborn\n\n    Example:\n    >>> df = pd.DataFrame([[1,2,3],[4,5,6],[7.0,np.nan,9.0]], columns=[\"c1\",\"c2\",\"c3\"])\n    >>> standardize_and_heatmap(df)\n    \"\"\"\n    df = df.fillna(df.mean(axis=0))\n    scaler = StandardScaler()\n    df[df.columns] = scaler.fit_transform(df[df.columns])\n\n    sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n    plt.show()\n\n    return df\n```"}
{"seed": "{\"task_id\": 12400256, \"prompt\": \"def f_12400256():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(1347517370))\", \"test_start\": \"\\nimport time\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate() == \\\"2012-09-13 06:22:50\\\"\\n\"], \"entry_point\": \"f_12400256\", \"intent\": \"Convert unix timestamp '1347517370' to formatted string '%Y-%m-%d %H:%M:%S'\", \"library\": [\"time\"]}", "generation": "Scenario 1: \npandas, datetime, time, os, pathlib\n```python\nimport pandas as pd\nfrom datetime import datetime\nimport time\nimport os\nfrom pathlib import Path\n\n# Constants\nDATE_FORMAT = '%Y-%m-%d %H:%M:%S'\n\ndef get_file_creation_time(file_path):\n    \"\"\"\n    Get the creation time of a file and convert it to a formatted string '%Y-%m-%d %H:%M:%S'.\n    \n    Parameters:\n    file_path (str): The path to the file.\n    \n    Returns:\n    str: The creation time of the file in the format '%Y-%m-%d %H:%M:%S'.\n    \n    Requirements:\n    - pandas\n    - datetime\n    - time\n    - os\n    - pathlib.Path\n    \n    Example:\n    >>> get_file_creation_time('/path/to/file.txt')\n    \"\"\"\n    if not Path(file_path).exists():\n        raise FileNotFoundError(f\"No such file or directory: '{file_path}'\")\n\n    creation_time = os.path.getctime(file_path)\n    formatted_time = datetime.fromtimestamp(creation_time).strftime(DATE_FORMAT)\n    \n    return formatted_time\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ntime, datetime, random, numpy, matplotlib\n```python\nimport time\nfrom datetime import datetime\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nDATE_FORMAT = '%Y-%m-%d %H:%M:%S'\n\ndef generate_random_timestamps(n):\n    \"\"\"\n    Generate n random Unix timestamps and convert them to formatted strings '%Y-%m-%d %H:%M:%S'. \n    Plot a histogram of the distribution of the generated timestamps.\n    \n    Parameters:\n    n (int): The number of timestamps to generate.\n    \n    Returns:\n    list: The list of n formatted timestamps.\n    \n    Requirements:\n    - time\n    - datetime\n    - random\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> generate_random_timestamps(5)\n    \"\"\"\n    timestamps = []\n    for _ in range(n):\n        timestamp = random.randint(0, int(time.time()))\n        formatted_time = datetime.fromtimestamp(timestamp).strftime(DATE_FORMAT)\n        timestamps.append(formatted_time)\n\n    plt.hist([datetime.strptime(t, DATE_FORMAT) for t in timestamps])\n    plt.show()\n    \n    return timestamps\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ndatetime, time, pytz, pandas\n```python\nfrom datetime import datetime\nimport time\nimport pytz\nimport pandas as pd\n\n# Constants\nDATE_FORMAT = '%Y-%m-%d %H:%M:%S'\n\ndef convert_timestamp_to_tz(timestamp, tz):\n    \"\"\"\n    Convert Unix timestamp to a formatted string '%Y-%m-%d %H:%M:%S' in a given timezone.\n    \n    Parameters:\n    timestamp (int): The Unix timestamp.\n    tz (str): The timezone.\n    \n    Returns:\n    str: The timestamp in the format '%Y-%m-%d %H:%M:%S' in the given timezone.\n    \n    Requirements:\n    - datetime\n    - time\n    - pytz\n    - pandas\n    \n    Example:\n    >>> convert_timestamp_to_tz(1347517370, 'America/New_York')\n    \"\"\"\n    dt = datetime.fromtimestamp(timestamp, pytz.timezone(tz))\n    \n    return dt.strftime(DATE_FORMAT)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ndatetime, time, pandas, numpy, matplotlib\n```python\nfrom datetime import datetime\nimport time\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nDATE_FORMAT = '%Y-%m-%d %H:%M:%S'\n\ndef analyze_timestamps(timestamps):\n    \"\"\"\n    Convert a list of Unix timestamps to datetime objects, create a pandas DataFrame, and plot a histogram.\n    \n    Parameters:\n    timestamps (list): The list of Unix timestamps.\n    \n    Returns:\n    DataFrame: A pandas DataFrame containing the original Unix timestamps and the converted datetime objects.\n    \n    Requirements:\n    - datetime\n    - time\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> analyze_timestamps([1347517370, 1475153730, 1602737300])\n    \"\"\"\n    datetimes = [datetime.fromtimestamp(t).strftime(DATE_FORMAT) for t in timestamps]\n    df = pd.DataFrame({'Timestamp': timestamps, 'Datetime': datetimes})\n\n    plt.hist(pd.to_datetime(df['Datetime']))\n    plt.show()\n    \n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\ntime, datetime, pandas, pytz, matplotlib\n```python\nimport time\nfrom datetime import datetime\nimport pandas as pd\nimport pytz\nimport matplotlib.pyplot as plt\n\n# Constants\nDATE_FORMAT = '%Y-%m-%d %H:%M:%S'\nTIMEZONES = ['America/New_York', 'Europe/London', 'Asia/Shanghai', 'Asia/Tokyo', 'Australia/Sydney']\n\ndef analyze_timestamp_across_timezones(timestamp):\n    \"\"\"\n    Convert a Unix timestamp to datetime objects in different timezones, create a pandas DataFrame, and plot a bar chart.\n    \n    Parameters:\n    timestamp (int): The Unix timestamp.\n    \n    Returns:\n    DataFrame: A pandas DataFrame containing the datetime in different timezones.\n    \n    Requirements:\n    - time\n    - datetime\n    - pandas\n    - pytz\n    - matplotlib.pyplot\n    \n    Example:\n    >>> analyze_timestamp_across_timezones(1347517370)\n    \"\"\"\n    datetimes = [datetime.fromtimestamp(timestamp, pytz.timezone(tz)).strftime(DATE_FORMAT) for tz in TIMEZONES]\n    \n    df = pd.DataFrame({'Timezone': TIMEZONES, 'Datetime': datetimes})\n    \n    df['Datetime'] = pd.to_datetime(df['Datetime'])\n    df.plot.bar(x='Timezone', y='Datetime')\n    plt.show()\n    \n    return df\n```"}
{"seed": "{\"task_id\": 23359886, \"prompt\": \"def f_23359886(a):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"a[np.where((a[:, (0)] == 0) * (a[:, (1)] == 1))]\", \"test_start\": \"\\nimport numpy as np\\n\\ndef check(candidate):\", \"test\": [\"\\n    a = np.array([[ 0,  1,  2], [ 3,  4,  5], [ 6,  7,  8], [ 9, 10, 11], [12, 13, 14]])\\n    res = np.array([[0, 1, 2]])\\n    assert np.array_equal(candidate(a), res)\\n\"], \"entry_point\": \"f_23359886\", \"intent\": \"selecting rows in Numpy ndarray 'a', where the value in the first column is 0 and value in the second column is 1\", \"library\": [\"numpy\"]}", "generation": "Scenario 1: \nnumpy, pandas, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['Age', 'Height', 'Weight']\n\ndef select_and_plot(df, age, height):\n    \"\"\"\n    Select rows in pandas DataFrame 'df', where the value in the 'Age' column is greater than 'age' \n    and value in the 'Height' column is less than 'height'. Then plot the selected data.\n\n    Parameters:\n    df (DataFrame): The input DataFrame.\n    age (int): The minimum age for selection.\n    height (int): The maximum height for selection.\n\n    Returns:\n    DataFrame: The selected DataFrame.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame(np.random.randint(0,100,size=(100, 3)), columns=COLUMNS)\n    >>> selected_df = select_and_plot(df, 50, 70)\n    \"\"\"\n    selected_df = df[(df['Age'] > age) & (df['Height'] < height)]\n    selected_df.plot(kind='scatter', x='Age', y='Height')\n    plt.show()\n\n    return selected_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nnumpy, pandas, seaborn\n```python\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n# Constants\nCOLUMNS = ['Country', 'Population', 'GDP']\n\ndef select_and_heatmap(df, population, gdp):\n    \"\"\"\n    Select rows in pandas DataFrame 'df', where the value in the 'Population' column is greater \n    than 'population' and value in the 'GDP' column is greater than 'gdp'. Then generate a heatmap\n    of the correlation matrix of the selected data.\n\n    Parameters:\n    df (DataFrame): The input DataFrame.\n    population (int): The minimum population for selection.\n    gdp (int): The minimum GDP for selection.\n\n    Returns:\n    DataFrame: The selected DataFrame.\n\n    Requirements:\n    - pandas\n    - numpy\n    - seaborn\n\n    Example:\n    >>> df = pd.DataFrame(np.random.randint(0,100,size=(100, 3)), columns=COLUMNS)\n    >>> selected_df = select_and_heatmap(df, 50, 70)\n    \"\"\"\n    selected_df = df[(df['Population'] > population) & (df['GDP'] > gdp)]\n    corr = selected_df.corr()\n    sns.heatmap(corr, annot=True)\n\n    return selected_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnumpy, pandas, sklearn\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\n# Constants\nCOLUMNS = ['Age', 'Height', 'Weight']\n\ndef select_and_standardize(df, age, weight):\n    \"\"\"\n    Select rows in pandas DataFrame 'df', where the value in the 'Age' column is less than 'age' \n    and value in the 'Weight' column is greater than 'weight'. Then standardize the selected data.\n\n    Parameters:\n    df (DataFrame): The input DataFrame.\n    age (int): The maximum age for selection.\n    weight (int): The minimum weight for selection.\n\n    Returns:\n    DataFrame: The selected and standardized DataFrame.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.preprocessing.StandardScaler\n\n    Example:\n    >>> df = pd.DataFrame(np.random.randint(0,100,size=(100, 3)), columns=COLUMNS)\n    >>> selected_df = select_and_standardize(df, 50, 70)\n    \"\"\"\n    selected_df = df[(df['Age'] < age) & (df['Weight'] > weight)]\n    scaler = StandardScaler()\n    selected_df = pd.DataFrame(scaler.fit_transform(selected_df), columns=selected_df.columns)\n\n    return selected_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, pandas, statsmodels\n```python\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Constants\nCOLUMNS = ['Age', 'Height', 'Weight']\n\ndef select_and_regression(df, height, weight):\n    \"\"\"\n    Select rows in pandas DataFrame 'df', where the value in the 'Height' column is greater \n    than 'height' and value in the 'Weight' column is less than 'weight'. Then perform a linear \n    regression using 'Age' as the dependent variable and the other columns as independent variables.\n\n    Parameters:\n    df (DataFrame): The input DataFrame.\n    height (int): The minimum height for selection.\n    weight (int): The maximum weight for selection.\n\n    Returns:\n    statsmodels.regression.linear_model.RegressionResultsWrapper: The regression results.\n\n    Requirements:\n    - pandas\n    - numpy\n    - statsmodels.api\n\n    Example:\n    >>> df = pd.DataFrame(np.random.randint(0,100,size=(100, 3)), columns=COLUMNS)\n    >>> results = select_and_regression(df, 50, 70)\n    \"\"\"\n    selected_df = df[(df['Height'] > height) & (df['Weight'] < weight)]\n    X = selected_df[COLUMNS[1:]]\n    y = selected_df['Age']\n    X = sm.add_constant(X)\n    model = sm.OLS(y, X)\n    results = model.fit()\n\n    return results\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnumpy, pandas, sklearn, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['Age', 'Height', 'Weight']\n\ndef select_and_cluster(df, age, height):\n    \"\"\"\n    Select rows in pandas DataFrame 'df', where the value in the 'Age' column is greater than 'age' \n    and value in the 'Height' column is less than 'height'. Then perform K-means clustering with 3 clusters\n    on the selected data and plot the clusters.\n\n    Parameters:\n    df (DataFrame): The input DataFrame.\n    age (int): The minimum age for selection.\n    height (int): The maximum height for selection.\n\n    Returns:\n    DataFrame: The selected DataFrame with an added 'Cluster' column.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.cluster.KMeans\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame(np.random.randint(0,100,size=(100, 3)), columns=COLUMNS)\n    >>> selected_df = select_and_cluster(df, 50, 70)\n    \"\"\"\n    selected_df = df[(df['Age'] > age) & (df['Height'] < height)]\n    kmeans = KMeans(n_clusters=3)\n    selected_df['Cluster'] = kmeans.fit_predict(selected_df)\n    \n    plt.scatter(selected_df['Age'], selected_df['Height'], c=selected_df['Cluster'])\n    plt.show()\n\n    return selected_df\n```"}
{"seed": "{\"task_id\": 4383082, \"prompt\": \"def f_4383082(words):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"re.split(' +', words)\", \"test_start\": \"\\nimport regex as re\\n\\ndef check(candidate):\", \"test\": [\"\\n    s = \\\"hello world sample text\\\"\\n    res = [\\\"hello\\\", \\\"world\\\", \\\"sample\\\", \\\"text\\\"]\\n    assert candidate(s) == res\\n\"], \"entry_point\": \"f_4383082\", \"intent\": \"separate words delimited by one or more spaces into a list\", \"library\": [\"regex\"]}", "generation": "Scenario 1:\npandas, regex, numpy, matplotlib\n```python\nimport pandas as pd\nimport regex as re\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nSTOPWORDS = [\"a\", \"an\", \"the\", \"in\", \"is\", \"are\"]\n\ndef count_word_frequency(text):\n    \"\"\"\n    Count the frequency of each word in a text, excluding the stopwords.\n    Then plot a histogram of the word frequencies.\n\n    Parameters:\n    text (str): The text to analyze. \n\n    Returns:\n    Series: A pandas Series with word frequencies.\n\n    Requirements:\n    - pandas\n    - regex\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> text = \"This is a sample text. This text contains sample words.\"\n    >>> word_counts = count_word_frequency(text)\n    >>> print(word_counts)\n    >>> word_counts.plot(kind='bar')\n    \"\"\"\n    words = re.split(r'\\s+', text.lower())\n    words = [word for word in words if word not in STOPWORDS]\n\n    word_counts = pd.Series(words).value_counts() \n\n    return word_counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, regex, seaborn, matplotlib\n```python\nimport pandas as pd\nimport regex as re\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMN_NAMES = ['Name', 'Email', 'Age', 'Country']\n\ndef extract_data_from_text(text):\n    \"\"\"\n    Extract data from a text and create a pandas DataFrame. \n    The text contains several lines, each formatted as 'Name: John Doe, Email: john.doe@example.com, Age: 30, Country: USA'.\n    Plot the age distribution using seaborn.\n\n    Parameters:\n    text (str): The text to analyze. \n\n    Returns:\n    DataFrame: A pandas DataFrame with extracted data.\n\n    Requirements:\n    - pandas\n    - regex\n    - seaborn\n    - matplotlib.pyplot\n\n    Example:\n    >>> text = \"Name: John Doe, Email: john.doe@example.com, Age: 30, Country: USA\\nName: Jane Doe, Email: jane.doe@example.com, Age: 25, Country: UK\"\n    >>> df = extract_data_from_text(text)\n    >>> print(df)\n    >>> sns.histplot(data=df, x='Age')\n    \"\"\"\n    pattern = r\"Name: (.*?), Email: (.*?), Age: (.*?), Country: (.*?)\\n\"\n    matches = re.findall(pattern, text)\n\n    data = []\n    for match in matches:\n        data.append(match)\n\n    df = pd.DataFrame(data, columns=COLUMN_NAMES)\n    df['Age'] = df['Age'].astype(int)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n  \nScenario 3:\npandas, regex, sklearn, matplotlib\n```python\nimport pandas as pd\nimport regex as re\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\n\ndef analyze_text(text):\n    \"\"\"\n    Analyze a text by creating a document-term matrix using CountVectorizer.\n    The text contains several sentences, each separated by a period.\n    Then plot a bar chart of the total word counts.\n\n    Parameters:\n    text (str): The text to analyze. \n\n    Returns:\n    DataFrame: A pandas DataFrame with the document-term matrix.\n\n    Requirements:\n    - pandas\n    - regex\n    - sklearn.feature_extraction.text.CountVectorizer\n    - matplotlib.pyplot\n\n    Example:\n    >>> text = \"This is a sample sentence. This sentence contains sample words.\"\n    >>> dtm = analyze_text(text)\n    >>> print(dtm)\n    >>> dtm.sum().plot(kind='bar')\n    \"\"\"\n    sentences = re.split(r'\\.\\s*', text)\n    vectorizer = CountVectorizer()\n    dtm = vectorizer.fit_transform(sentences)\n\n    df = pd.DataFrame(dtm.toarray(), columns=vectorizer.get_feature_names_out())\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, regex, nltk, matplotlib\n```python\nimport pandas as pd\nimport regex as re\nfrom nltk.corpus import stopwords\nimport matplotlib.pyplot as plt\n\n# Constants\nSTOPWORDS = set(stopwords.words('english'))\n\ndef analyze_sentences(text):\n    \"\"\"\n    Analyze a text by counting the number of non-stopword words in each sentence.\n    The text contains several sentences, each separated by a period.\n    Then plot a bar chart of the sentence word counts.\n\n    Parameters:\n    text (str): The text to analyze. \n\n    Returns:\n    Series: A pandas Series with the sentence word counts.\n\n    Requirements:\n    - pandas\n    - regex\n    - nltk.corpus.stopwords\n    - matplotlib.pyplot\n\n    Example:\n    >>> text = \"This is a sample sentence. This sentence contains sample words.\"\n    >>> sentence_counts = analyze_sentences(text)\n    >>> print(sentence_counts)\n    >>> sentence_counts.plot(kind='bar')\n    \"\"\"\n    sentences = re.split(r'\\.\\s*', text)\n    sentence_counts = {}\n\n    for i, sentence in enumerate(sentences):\n        words = re.split(r'\\s+', sentence.lower())\n        words = [word for word in words if word not in STOPWORDS]\n        sentence_counts[f'Sentence {i+1}'] = len(words)\n\n    sentence_counts = pd.Series(sentence_counts)\n\n    return sentence_counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, regex, seaborn, matplotlib\n```python\nimport pandas as pd\nimport regex as re\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef extract_and_plot_data(text):\n    \"\"\"\n    Extract data from a text and create a pandas DataFrame. \n    The text contains several lines, each formatted as 'Score: 85, Category: Math'.\n    Then plot a box plot of the scores for each category using seaborn.\n\n    Parameters:\n    text (str): The text to analyze. \n\n    Returns:\n    DataFrame: A pandas DataFrame with extracted data.\n\n    Requirements:\n    - pandas\n    - regex\n    - seaborn\n    - matplotlib.pyplot\n\n    Example:\n    >>> text = \"Score: 85, Category: Math\\nScore: 90, Category: Science\\nScore: 80, Category: Math\"\n    >>> df = extract_and_plot_data(text)\n    >>> print(df)\n    >>> sns.boxplot(x='Category', y='Score', data=df)\n    \"\"\"\n    pattern = r\"Score: (.*?), Category: (.*?)\\n\"\n    matches = re.findall(pattern, text)\n\n    data = []\n    for match in matches:\n        data.append(match)\n\n    df = pd.DataFrame(data, columns=['Score', 'Category'])\n    df['Score'] = df['Score'].astype(int)\n\n    return df\n```"}
{"seed": "{\"task_id\": 14637696, \"prompt\": \"def f_14637696(words):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"len(max(words, key=len))\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([\\\"hello\\\", \\\"world\\\", \\\"sample\\\", \\\"text\\\", \\\"superballer\\\"]) == 11\\n\"], \"entry_point\": \"f_14637696\", \"intent\": \"length of longest element in list `words`\", \"library\": []}", "generation": "Scenario 1: \ncollections, os, nltk, string\n```python\nfrom collections import Counter\nimport os\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\n\n# Constants\nSTOPWORDS = set(stopwords.words('english'))\n\ndef word_statistics(directory):\n    \"\"\"\n    Analyze a directory of text files and return the most common word and its count, excluding\n    stopwords and punctuation.\n\n    Parameters:\n    directory (str): Path to the directory containing text files.\n\n    Returns:\n    tuple: The most common word and its count.\n\n    Requirements:\n    - collections\n    - os\n    - nltk.corpus.stopwords\n    - string\n\n    Example:\n    >>> word_statistics('./text_files')\n    \"\"\"\n    word_counter = Counter()\n    \n    for file in os.listdir(directory):\n        if file.endswith('.txt'):\n            with open(os.path.join(directory, file), 'r') as f:\n                words = nltk.word_tokenize(f.read().lower())\n                words = [word for word in words if word not in STOPWORDS and word not in string.punctuation]\n                word_counter.update(words)\n    \n    common_word, count = word_counter.most_common(1)[0]\n    return common_word, count\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 2: \npandas, sklearn.model_selection, sklearn.linear_model, numpy\n```python\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\n\ndef model_training(data, target_column):\n    \"\"\"\n    Train a linear regression model using the provided dataset and return the model's score.\n\n    Parameters:\n    data (DataFrame): The input data for training.\n    target_column (str): The column to predict.\n\n    Returns:\n    float: The model's score.\n\n    Requirements:\n    - pandas\n    - sklearn.model_selection.train_test_split\n    - sklearn.linear_model.LinearRegression\n    - numpy\n\n    Example:\n    >>> data = pd.DataFrame({'x1': np.random.rand(100), 'x2': np.random.rand(100), 'y': np.random.rand(100)})\n    >>> model_training(data, 'y')\n    \"\"\"\n    X = data.drop(columns=[target_column])\n    y = data[target_column]\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n    model = LinearRegression().fit(X_train, y_train)\n\n    return model.score(X_test, y_test)\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 3: \npandas, matplotlib.pyplot, seaborn\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef create_heatmap(data, title):\n    \"\"\"\n    Create a heatmap of the correlation matrix of a DataFrame.\n\n    Parameters:\n    data (DataFrame): The input data.\n    title (str): The title of the heatmap.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> data = pd.DataFrame({'A': range(10), 'B': range(10), 'C': range(10)})\n    >>> create_heatmap(data, 'Correlation Heatmap')\n    \"\"\"\n    corr = data.corr()\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(corr, annot=True, cmap='coolwarm', cbar=True)\n    plt.title(title)\n    plt.show()\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, scipy.stats, matplotlib.pyplot\n```python\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef plot_normal_distribution(mu, sigma, num_samples):\n    \"\"\"\n    Plot a normal distribution with given mean and standard deviation and overlay a histogram \n    of randomly generated samples from this distribution.\n\n    Parameters:\n    mu (float): The mean of the distribution.\n    sigma (float): The standard deviation of the distribution.\n    num_samples (int): The number of samples to generate.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_normal_distribution(0, 1, 1000)\n    \"\"\"\n    samples = np.random.normal(mu, sigma, num_samples)\n    plt.hist(samples, bins=30, density=True, alpha=0.6, color='g')\n\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, mu, sigma)\n    plt.plot(x, p, 'k', linewidth=2)\n    \n    plt.title('Normal Distribution')\n    plt.show()\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 5: \nwikipedia, wordcloud, matplotlib.pyplot\n```python\nimport wikipedia\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\ndef create_wordcloud(page_title):\n    \"\"\"\n    Generate a word cloud from the text of a Wikipedia page.\n\n    Parameters:\n    page_title (str): The title of the Wikipedia page.\n\n    Returns:\n    None\n\n    Requirements:\n    - wikipedia\n    - wordcloud.WordCloud\n    - matplotlib.pyplot\n\n    Example:\n    >>> create_wordcloud('Python (programming language)')\n    \"\"\"\n    text = wikipedia.page(page_title).content\n    wordcloud = WordCloud().generate(text)\n\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis('off')\n    plt.show()\n```"}
{"seed": "{\"task_id\": 3933478, \"prompt\": \"def f_3933478(result):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"result[0]['from_user']\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    Contents = [{\\\"hi\\\": 7, \\\"bye\\\": 4, \\\"from_user\\\": 0}, {1: 2, 3: 4, 5: 6}]\\n    assert candidate(Contents) == 0\\n\"], \"entry_point\": \"f_3933478\", \"intent\": \"get the value associated with unicode key 'from_user' of first dictionary in list `result`\", \"library\": []}", "generation": "Scenario 1: \npandas, json, csv, os\n\n```python\nimport pandas as pd\nimport json\nimport csv\nimport os\n\n# Constants\nCSV_FILE_PATH = './data.csv'\nJSON_FILE_PATH = './data.json'\n\ndef save_dict_to_csv_and_json(result):\n    \"\"\"\n    Save the list of dictionaries in `result` to a CSV file and a JSON file.\n    \n    Parameters:\n    result (list): A list of dictionaries.\n    \n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - json\n    - csv\n    - os\n    \n    Example:\n    >>> result = [{\"hi\": 7, \"bye\": 4, \"from_user\": 0}, {1: 2, 3: 4, 5: 6}]\n    >>> save_dict_to_csv_and_json(result)\n    \"\"\"\n    # Save to CSV\n    df = pd.DataFrame(result)\n    df.to_csv(CSV_FILE_PATH, index=False)\n\n    # Save to JSON\n    with open(JSON_FILE_PATH, 'w') as f:\n        json.dump(result, f, indent=4)\n\n    return None\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, matplotlib, math, datetime\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\nfrom datetime import datetime\n\n# Constants\nPLOT_TITLE = 'Square root plot'\nX_LABEL = 'x'\nY_LABEL = 'sqrt(x)'\nTIME_FORMAT = '%Y-%m-%d %H:%M:%S'\n\ndef plot_sqrt_and_annotate(result):\n    \"\"\"\n    Plot the square root function for the values in `result` that are associated with \n    the key 'from_user'. Also, annotate the plot with the current date and time.\n\n    Parameters:\n    result (list): A list of dictionaries.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - matplotlib\n    - math\n    - datetime\n\n    Example:\n    >>> result = [{\"hi\": 7, \"bye\": 4, \"from_user\": 16}, {1: 2, 3: 4, 5: 6}]\n    >>> plot_sqrt_and_annotate(result)\n    \"\"\"\n    # Extract the 'from_user' values\n    from_user_values = [d['from_user'] for d in result if 'from_user' in d]\n\n    # Calculate the square roots\n    square_roots = np.sqrt(from_user_values)\n\n    # Plot the square root function\n    plt.figure()\n    plt.plot(from_user_values, square_roots)\n    plt.title(PLOT_TITLE)\n    plt.xlabel(X_LABEL)\n    plt.ylabel(Y_LABEL)\n\n    # Annotate the plot with the current date and time\n    now = datetime.now()\n    now_str = now.strftime(TIME_FORMAT)\n    plt.annotate(now_str, (0.05, 0.95), xycoords='axes fraction')\n\n    plt.show()\n\n    return None\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nitertools, collections, operator\n\n```python\nimport itertools\nfrom collections import Counter\nimport operator\n\ndef get_most_common_values(result):\n    \"\"\"\n    Get the most common values associated with the key 'from_user' in the list of dictionaries \n    `result`.\n\n    Parameters:\n    result (list): A list of dictionaries.\n\n    Returns:\n    dict: A dictionary with the most common values and their counts.\n\n    Requirements:\n    - itertools\n    - collections\n    - operator\n\n    Example:\n    >>> result = [{\"hi\": 7, \"bye\": 4, \"from_user\": 0}, {\"from_user\": 0}, {\"from_user\": 1}]\n    >>> get_most_common_values(result)\n    \"\"\"\n    from_user_values = [d['from_user'] for d in result if 'from_user' in d]\n\n    counter = Counter(from_user_values)\n    most_common = dict(counter.most_common(1))\n\n    return most_common\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, pandas, datetime\n\n```python\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\n\n# Constants\nDATE_FORMAT = '%Y-%m-%d %H:%M:%S'\n\ndef calculate_statistical_summary(result):\n    \"\"\"\n    Calculate the mean, median, min, max, and standard deviation of the 'from_user' values \n    in `result`. Also, add the current date and time to the summary.\n\n    Parameters:\n    result (list): A list of dictionaries.\n\n    Returns:\n    Series: A pandas Series with the statistical summary.\n\n    Requirements:\n    - numpy\n    - pandas\n    - datetime\n\n    Example:\n    >>> result = [{\"hi\": 7, \"bye\": 4, \"from_user\": 0}, {\"from_user\": 0}, {\"from_user\": 1}]\n    >>> calculate_statistical_summary(result)\n    \"\"\"\n    from_user_values = np.array([d['from_user'] for d in result if 'from_user' in d])\n\n    summary = {\n        'mean': np.mean(from_user_values),\n        'median': np.median(from_user_values),\n        'min': np.min(from_user_values),\n        'max': np.max(from_user_values),\n        'std': np.std(from_user_values),\n        'current_time': datetime.now().strftime(DATE_FORMAT)\n    }\n\n    summary_series = pd.Series(summary)\n\n    return summary_series\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nrandom, matplotlib, seaborn\n\n```python\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nCOLORS = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n\ndef plot_histogram(result):\n    \"\"\"\n    Plot a histogram of the 'from_user' values in `result`. Randomly select the color of the histogram.\n\n    Parameters:\n    result (list): A list of dictionaries.\n\n    Returns:\n    None\n\n    Requirements:\n    - random\n    - matplotlib\n    - seaborn\n\n    Example:\n    >>> result = [{\"hi\": 7, \"bye\": 4, \"from_user\": 0}, {\"from_user\": 0}, {\"from_user\": 1}]\n    >>> plot_histogram(result)\n    \"\"\"\n    from_user_values = [d['from_user'] for d in result if 'from_user' in d]\n\n    color = random.choice(COLORS)\n\n    plt.figure()\n    sns.histplot(from_user_values, color=color)\n    plt.show()\n\n    return None\n```"}
{"seed": "{\"task_id\": 39112645, \"prompt\": \"def f_39112645():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"[line.split() for line in open('File.txt')]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    with open('File.txt','w') as fw:\\n        fw.write(\\\"hi hello cat dog\\\")\\n    assert candidate() == [['hi', 'hello', 'cat', 'dog']]\\n\"], \"entry_point\": \"f_39112645\", \"intent\": \"Retrieve each line from a file 'File.txt' as a list\", \"library\": []}", "generation": "Scenario 1: \nos, sys, re, collections\n```python\nimport os\nimport sys\nimport re\nfrom collections import Counter\n\n# Constants\nFILE_PATH = 'File.txt'\n\ndef count_word_frequency(file_path=FILE_PATH):\n    \"\"\"\n    Count word frequency from a given text file. This function retrieves each line from the file, \n    splits it into words and counts the frequency of each word. The words are normalized to lower case.\n    \n    Parameters:\n    file_path (str): The path to the text file. Defaults to 'File.txt'.\n    \n    Returns:\n    dict: A dictionary with words as keys and their frequencies as values.\n    \n    Requirements:\n    - os\n    - sys\n    - re\n    - collections.Counter\n    \n    Example:\n    >>> count_word_frequency('File.txt')\n    \"\"\"\n    if not os.path.isfile(file_path):\n        print(f\"File not found: {file_path}\", file=sys.stderr)\n        return None\n\n    word_counter = Counter()\n\n    with open(file_path, 'r') as file:\n        for line in file:\n            words = re.findall(r'\\b\\w+\\b', line.lower())\n            word_counter.update(words)\n\n    return dict(word_counter)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, glob, re, collections\n```python\nimport os\nimport glob\nimport re\nfrom collections import Counter\n\n# Constants\nFILE_PATH = '*.txt'\n\ndef count_word_frequency_in_multiple_files(file_path=FILE_PATH):\n    \"\"\"\n    Count word frequency from all text files in a directory. This function retrieves each line from each file, \n    splits it into words and counts the frequency of each word. The words are normalized to lower case.\n    \n    Parameters:\n    file_path (str): The path to the text files. Defaults to '*.txt'.\n    \n    Returns:\n    dict: A dictionary with words as keys and their frequencies as values.\n    \n    Requirements:\n    - os\n    - glob\n    - re\n    - collections.Counter\n    \n    Example:\n    >>> count_word_frequency_in_multiple_files('*.txt')\n    \"\"\"\n    word_counter = Counter()\n\n    for file in glob.glob(file_path):\n        with open(file, 'r') as f:\n            for line in f:\n                words = re.findall(r'\\b\\w+\\b', line.lower())\n                word_counter.update(words)\n\n    return dict(word_counter)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nos, sys, json, re\n```python\nimport os\nimport sys\nimport json\nimport re\n\n# Constants\nFILE_PATH = 'File.txt'\n\ndef convert_file_to_json(file_path=FILE_PATH):\n    \"\"\"\n    Convert a text file to a JSON file. This function retrieves each line from the file, \n    splits it into words and stores the words in a list. The list is then converted to JSON format.\n    \n    Parameters:\n    file_path (str): The path to the text file. Defaults to 'File.txt'.\n    \n    Returns:\n    str: The JSON string.\n    \n    Requirements:\n    - os\n    - sys\n    - json\n    - re\n    \n    Example:\n    >>> convert_file_to_json('File.txt')\n    \"\"\"\n    if not os.path.isfile(file_path):\n        print(f\"File not found: {file_path}\", file=sys.stderr)\n        return None\n\n    lines = []\n\n    with open(file_path, 'r') as file:\n        for line in file:\n            words = re.findall(r'\\b\\w+\\b', line)\n            lines.append(words)\n\n    return json.dumps(lines)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, sys, re, nltk\n```python\nimport os\nimport sys\nimport re\nfrom nltk import word_tokenize\n\n# Constants\nFILE_PATH = 'File.txt'\n\ndef tokenize_file(file_path=FILE_PATH):\n    \"\"\"\n    Tokenize a text file using the NLTK library. This function retrieves each line from the file, \n    tokenizes it into words or punctuation and stores the tokens in a list.\n    \n    Parameters:\n    file_path (str): The path to the text file. Defaults to 'File.txt'.\n    \n    Returns:\n    list: A list of tokens.\n    \n    Requirements:\n    - os\n    - sys\n    - re\n    - nltk.word_tokenize\n    \n    Example:\n    >>> tokenize_file('File.txt')\n    \"\"\"\n    if not os.path.isfile(file_path):\n        print(f\"File not found: {file_path}\", file=sys.stderr)\n        return None\n\n    tokens = []\n\n    with open(file_path, 'r') as file:\n        for line in file:\n            tokens.extend(word_tokenize(line))\n\n    return tokens\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, sys, re, csv\n```python\nimport os\nimport sys\nimport re\nimport csv\n\n# Constants\nFILE_PATH = 'File.txt'\nCSV_PATH = 'File.csv'\n\ndef convert_file_to_csv(file_path=FILE_PATH, csv_path=CSV_PATH):\n    \"\"\"\n    Convert a text file to a CSV file. This function retrieves each line from the file, \n    splits it into words and stores the words in a list. The list is then written to a CSV file.\n    \n    Parameters:\n    file_path (str): The path to the text file. Defaults to 'File.txt'.\n    csv_path (str): The path to the CSV file. Defaults to 'File.csv'.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - os\n    - sys\n    - re\n    - csv\n    \n    Example:\n    >>> convert_file_to_csv('File.txt', 'File.csv')\n    \"\"\"\n    if not os.path.isfile(file_path):\n        print(f\"File not found: {file_path}\", file=sys.stderr)\n        return None\n\n    lines = []\n\n    with open(file_path, 'r') as file:\n        for line in file:\n            words = re.findall(r'\\b\\w+\\b', line)\n            lines.append(words)\n\n    with open(csv_path, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerows(lines)\n```\n"}
{"seed": "{\"task_id\": 1031851, \"prompt\": \"def f_1031851(a):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"dict((v, k) for k, v in a.items())\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    a = {\\\"one\\\": 1, \\\"two\\\": 2}\\n    assert candidate(a) == {1: \\\"one\\\", 2: \\\"two\\\"}\\n\"], \"entry_point\": \"f_1031851\", \"intent\": \"swap keys with values in a dictionary `a`\", \"library\": []}", "generation": "Scenario 1: \npandas, random, matplotlib, seaborn\n```python\nimport pandas as pd\nfrom random import shuffle\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nCARS = ['Ford', 'Toyota', 'Mercedes', 'Tesla', 'BMW']\nCOLORS = ['Red', 'Blue', 'Black', 'White', 'Silver']\n\ndef generate_car_color_distribution(car_dict):\n    \"\"\"\n    Given a dictionary of cars as keys and their colors as values, create a DataFrame\n    and visualize the distribution of car colors in a bar plot.\n    \n    Parameters:\n    car_dict (dict): The dictionary with car brands as keys and their colors as values.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with car brands and their colors.\n    \n    Requirements:\n    - pandas\n    - random\n    - matplotlib.pyplot\n    - seaborn\n    \n    Example:\n    >>> car_dict = {'Ford': 'Red', 'Toyota': 'Blue', 'Mercedes': 'Black', 'Tesla': 'White', 'BMW': 'Silver'}\n    >>> df = generate_car_color_distribution(car_dict)\n    >>> print(df)\n    >>> sns.countplot(x='Color', data=df)\n    \"\"\"\n    car_data = list(car_dict.items())\n    shuffle(car_data)\n    \n    df = pd.DataFrame(car_data, columns=['Car', 'Color'])\n    \n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncollections, operator, itertools\n```python\nfrom collections import Counter\nfrom operator import itemgetter\nimport itertools\n\nWORDS = ['apple', 'banana', 'cherry', 'date', 'elderberry', 'fig', 'grape', 'honeydew']\nLETTERS = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\n\ndef count_letters_in_words(word_dict):\n    \"\"\"\n    Given a dictionary of words as keys and letters as values, count the frequency of each letter in the words.\n    \n    Parameters:\n    word_dict (dict): The dictionary with words as keys and their letters as values.\n    \n    Returns:\n    dict: A dictionary with letters as keys and their frequencies as values.\n    \n    Requirements:\n    - collections.Counter\n    - operator.itemgetter\n    - itertools\n    \n    Example:\n    >>> word_dict = {'apple': 'a', 'banana': 'b', 'cherry': 'c', 'date': 'd', 'elderberry': 'e', 'fig': 'f', 'grape': 'g', 'honeydew': 'h'}\n    >>> counts = count_letters_in_words(word_dict)\n    >>> print(counts)\n    \"\"\"\n    letters = list(itertools.chain.from_iterable(word_dict.keys()))\n    count_dict = dict(Counter(letters))\n    \n    sorted_dict = dict(sorted(count_dict.items(), key=itemgetter(1), reverse=True))\n    \n    return sorted_dict\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \npandas, numpy, matplotlib, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nSTATES = ['California', 'Texas', 'New York', 'Florida', 'Illinois']\n\ndef generate_population_data(state_dict):\n    \"\"\"\n    Given a dictionary of states as keys and their populations as values, create a DataFrame\n    and visualize the distribution of state populations in a bar plot.\n    \n    Parameters:\n    state_dict (dict): The dictionary with states as keys and their populations as values.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with states and their populations.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - seaborn\n    \n    Example:\n    >>> state_dict = {'California': 39538223, 'Texas': 29145505, 'New York': 20201249, 'Florida': 21538187, 'Illinois': 12812508}\n    >>> df = generate_population_data(state_dict)\n    >>> print(df)\n    >>> sns.barplot(x='State', y='Population', data=df)\n    \"\"\"\n    state_data = list(state_dict.items())\n    \n    df = pd.DataFrame(state_data, columns=['State', 'Population'])\n    \n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ncollections, operator, itertools\n```python\nfrom collections import Counter\nfrom operator import itemgetter\nimport itertools\n\nANIMALS = ['cat', 'dog', 'elephant', 'fox', 'giraffe', 'hippo', 'iguana', 'jaguar']\nLETTERS = ['c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']\n\ndef count_letters_in_animals(animal_dict):\n    \"\"\"\n    Given a dictionary of animals as keys and letters as values, count the frequency of each letter in the animals.\n    \n    Parameters:\n    animal_dict (dict): The dictionary with animals as keys and their letters as values.\n    \n    Returns:\n    dict: A dictionary with letters as keys and their frequencies as values.\n    \n    Requirements:\n    - collections.Counter\n    - operator.itemgetter\n    - itertools\n    \n    Example:\n    >>> animal_dict = {'cat': 'c', 'dog': 'd', 'elephant': 'e', 'fox': 'f', 'giraffe': 'g', 'hippo': 'h', 'iguana': 'i', 'jaguar': 'j'}\n    >>> counts = count_letters_in_animals(animal_dict)\n    >>> print(counts)\n    \"\"\"\n    letters = list(itertools.chain.from_iterable(animal_dict.keys()))\n    count_dict = dict(Counter(letters))\n    \n    sorted_dict = dict(sorted(count_dict.items(), key=itemgetter(1), reverse=True))\n    \n    return sorted_dict\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \npandas, numpy, matplotlib, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nCOUNTRIES = ['China', 'India', 'USA', 'Indonesia', 'Pakistan']\n\ndef generate_population_data(country_dict):\n    \"\"\"\n    Given a dictionary of countries as keys and their populations as values, create a DataFrame\n    and visualize the distribution of country populations in a bar plot.\n    \n    Parameters:\n    country_dict (dict): The dictionary with countries as keys and their populations as values.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with countries and their populations.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - seaborn\n    \n    Example:\n    >>> country_dict = {'China': 1444216107, 'India': 1393409038, 'USA': 332915073, 'Indonesia': 276361783, 'Pakistan': 225199937}\n    >>> df = generate_population_data(country_dict)\n    >>> print(df)\n    >>> sns.barplot(x='Country', y='Population', data=df)\n    \"\"\"\n    country_data = list(country_dict.items())\n    \n    df = pd.DataFrame(country_data, columns=['Country', 'Population'])\n    \n    return df\n```"}
{"seed": "{\"task_id\": 8577137, \"prompt\": \"def f_8577137():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"open('path/to/FILE_NAME.ext', 'w')\", \"test_start\": \"\\nimport os\\n\\ndef check(candidate):\", \"test\": [\"\\n    path1 = os.path.join(\\\"\\\", \\\"path\\\")\\n    os.mkdir(path1)\\n    path2 = os.path.join(\\\"path\\\", \\\"to\\\")\\n    os.mkdir(path2)\\n    candidate()\\n    assert os.path.exists('path/to/FILE_NAME.ext')\\n\"], \"entry_point\": \"f_8577137\", \"intent\": \"Open a file `path/to/FILE_NAME.ext` in write mode\", \"library\": [\"os\"]}", "generation": "Scenario 1:\n```python\nimport os\nfrom datetime import datetime\nimport csv\nimport random\n\n# Constants\nFILE_NAME = 'path/to/data.csv'\nDATA = ['Temperature', 'Humidity', 'Pressure']\nRANGE = {\n    'Temperature': (-50, 50),\n    'Humidity': (0, 100),\n    'Pressure': (980, 1040)\n}\n\ndef generate_weather_data(file_name=FILE_NAME):\n    \"\"\"\n    Generate a CSV file with weather data for each hour of the current day.\n\n    Parameters:\n    file_name (str): The path to the CSV file to be created.\n    \n    Returns:\n    str: The path to the created file.\n\n    Requirements:\n    - os\n    - datetime\n    - csv\n    - random\n\n    Example:\n    >>> generate_weather_data()\n    'path/to/data.csv'\n    \"\"\"\n    with open(file_name, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Time'] + DATA)\n        \n        for hour in range(24):\n            row = [f'{hour}:00']\n            for data_type in DATA:\n                min_val, max_val = RANGE[data_type]\n                row.append(random.uniform(min_val, max_val))\n            writer.writerow(row)\n\n    return file_name\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\n```python\nimport os\nimport json\nimport glob\nfrom collections import defaultdict\n\n# Constants\nSOURCE_DIR = 'path/to/source/'\nTARGET_FILE = 'path/to/target.json'\n\ndef aggregate_json_files(source_dir=SOURCE_DIR, target_file=TARGET_FILE):\n    \"\"\"\n    Aggregate data from all JSON files in a directory into one JSON file.\n\n    Parameters:\n    source_dir (str): The directory containing the source JSON files.\n    target_file (str): The path to the target JSON file.\n\n    Returns:\n    str: The path to the created file.\n\n    Requirements:\n    - os\n    - json\n    - glob\n    - collections.defaultdict\n\n    Example:\n    >>> aggregate_json_files()\n    'path/to/target.json'\n    \"\"\"\n    aggregated_data = defaultdict(list)\n\n    for file_name in glob.glob(os.path.join(source_dir, '*.json')):\n        with open(file_name, 'r') as file:\n            data = json.load(file)\n            for key, value in data.items():\n                aggregated_data[key].append(value)\n\n    with open(target_file, 'w') as file:\n        json.dump(aggregated_data, file)\n\n    return target_file\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\n```python\nimport os\nimport sqlite3\nimport csv\n\n# Constants\nCSV_FILE = 'path/to/data.csv'\nDB_FILE = 'path/to/data.db'\nTABLE_NAME = 'WeatherData'\n\ndef csv_to_sqlite(csv_file=CSV_FILE, db_file=DB_FILE, table_name=TABLE_NAME):\n    \"\"\"\n    Convert a CSV file to a SQLite database.\n\n    Parameters:\n    csv_file (str): The path to the CSV file.\n    db_file (str): The path to the SQLite database file.\n    table_name (str): The name of the table to be created in the database.\n\n    Returns:\n    str: The path to the created database file.\n\n    Requirements:\n    - os\n    - sqlite3\n    - csv\n\n    Example:\n    >>> csv_to_sqlite()\n    'path/to/data.db'\n    \"\"\"\n    conn = sqlite3.connect(db_file)\n    cur = conn.cursor()\n\n    with open(csv_file, 'r') as file:\n        reader = csv.reader(file)\n        columns = next(reader)\n        cur.execute(f\"CREATE TABLE {table_name} ({', '.join(columns)});\")\n\n        for row in reader:\n            cur.execute(f\"INSERT INTO {table_name} VALUES ({', '.join('?' for _ in row)});\", row)\n\n    conn.commit()\n    conn.close()\n\n    return db_file\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\n```python\nimport os\nimport shutil\nfrom pathlib import Path\n\n# Constants\nSOURCE_DIR = 'path/to/source/'\nTARGET_DIR = 'path/to/target/'\nEXTENSIONS = ['.jpg', '.png', '.gif']\n\ndef move_images(source_dir=SOURCE_DIR, target_dir=TARGET_DIR, extensions=EXTENSIONS):\n    \"\"\"\n    Move all image files with certain extensions from one directory to another.\n\n    Parameters:\n    source_dir (str): The directory containing the source image files.\n    target_dir (str): The directory to which the image files should be moved.\n    extensions (list): The list of file extensions to be moved.\n\n    Returns:\n    int: The number of moved files.\n\n    Requirements:\n    - os\n    - shutil\n    - pathlib.Path\n\n    Example:\n    >>> move_images()\n    15\n    \"\"\"\n    count = 0\n\n    for extension in extensions:\n        for file_name in Path(source_dir).glob(f'*{extension}'):\n            shutil.move(str(file_name), target_dir)\n            count += 1\n\n    return count\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\n```python\nimport os\nimport zipfile\n\n# Constants\nSOURCE_DIR = 'path/to/source/'\nTARGET_FILE = 'path/to/target.zip'\n\ndef compress_directory(source_dir=SOURCE_DIR, target_file=TARGET_FILE):\n    \"\"\"\n    Compress a directory into a ZIP file.\n\n    Parameters:\n    source_dir (str): The directory to be compressed.\n    target_file (str): The path to the target ZIP file.\n\n    Returns:\n    str: The path to the created ZIP file.\n\n    Requirements:\n    - os\n    - zipfile\n\n    Example:\n    >>> compress_directory()\n    'path/to/target.zip'\n    \"\"\"\n    with zipfile.ZipFile(target_file, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        for root, dirs, files in os.walk(source_dir):\n            for file in files:\n                zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), source_dir))\n\n    return target_file\n```"}
{"seed": "{\"task_id\": 17926273, \"prompt\": \"def f_17926273(df):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"df.groupby(['col1', 'col2'])['col3'].nunique().reset_index()\", \"test_start\": \"\\nimport pandas as pd\\n\\ndef check(candidate):\", \"test\": [\"\\n    data = [[1, 1, 1], [1, 1, 1], [1, 1, 2], [1, 2, 3], [1, 2, 3], [1, 2, 3], \\n            [2, 1, 1], [2, 1, 2], [2, 1, 3], [2, 2, 3], [2, 2, 3], [2, 2, 3]]\\n    expected = [[1, 1, 2], [1, 2, 1], [2, 1, 3], [2, 2, 1]]\\n    df = pd.DataFrame(data, columns = ['col1', 'col2', 'col3'])\\n    expected_df = pd.DataFrame(expected, columns = ['col1', 'col2', 'col3'])\\n    df1 = candidate(df)\\n    assert pd.DataFrame.equals(expected_df, df1)\\n\"], \"entry_point\": \"f_17926273\", \"intent\": \"count distinct values in a column 'col3' of a pandas dataframe `df` group by objects in 'col1' and 'col2'\", \"library\": [\"pandas\"]}", "generation": "Scenario 1:\npandas, numpy, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['col1', 'col2', 'col3']\n\ndef analyze_data_distinct_values(df):\n    \"\"\"\n    Analyze a pandas DataFrame by counting distinct values in a column 'col3' \n    grouped by 'col1' and 'col2', and plot a bar chart for visualizing the data distribution.\n\n    Parameters:\n    df (pandas.DataFrame): The DataFrame to be analyzed.\n\n    Returns:\n    pandas.DataFrame: The DataFrame of the analyzed data.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib\n\n    Example:\n    >>> data = [[1, 1, 1], [1, 1, 1], [1, 1, 2], [1, 2, 3], [1, 2, 3], [1, 2, 3], \n                [2, 1, 1], [2, 1, 2], [2, 1, 3], [2, 2, 3], [2, 2, 3], [2, 2, 3]]\n    >>> df = pd.DataFrame(data, columns=COLUMNS)\n    >>> analyzed_df = analyze_data_distinct_values(df)\n    >>> print(analyzed_df)\n    \"\"\"\n    analyzed_df = df.groupby(COLUMNS[:-1])[COLUMNS[-1]].nunique().reset_index()\n    analyzed_df.plot(kind='bar', x=COLUMNS[:-1], y=COLUMNS[-1])\n    plt.show()\n\n    return analyzed_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n\n# Constants\nCOLUMNS = ['col1', 'col2', 'col3']\n\ndef visualize_data_distinct_values(df):\n    \"\"\"\n    Visualize the distribution of distinct values in a column 'col3' of a pandas DataFrame 'df', \n    grouped by 'col1' and 'col2', using a heatmap.\n\n    Parameters:\n    df (pandas.DataFrame): The DataFrame to be visualized.\n\n    Returns:\n    pandas.DataFrame: The DataFrame of the analyzed data.\n\n    Requirements:\n    - pandas\n    - numpy\n    - seaborn\n\n    Example:\n    >>> data = [[1, 1, 1], [1, 1, 1], [1, 1, 2], [1, 2, 3], [1, 2, 3], [1, 2, 3], \n                [2, 1, 1], [2, 1, 2], [2, 1, 3], [2, 2, 3], [2, 2, 3], [2, 2, 3]]\n    >>> df = pd.DataFrame(data, columns=COLUMNS)\n    >>> analyzed_df = visualize_data_distinct_values(df)\n    >>> print(analyzed_df)\n    \"\"\"\n    analyzed_df = df.groupby(COLUMNS[:-1])[COLUMNS[-1]].nunique().reset_index()\n    analyzed_df = analyzed_df.pivot(*COLUMNS)\n    sns.heatmap(analyzed_df, annot=True)\n    plt.show()\n\n    return analyzed_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['col1', 'col2', 'col3']\n\ndef plot_data_distinct_values(df):\n    \"\"\"\n    Plot a line graph of distinct values in a column 'col3' of a pandas DataFrame 'df', \n    grouped by 'col1' and 'col2'.\n\n    Parameters:\n    df (pandas.DataFrame): The DataFrame to be plotted.\n\n    Returns:\n    pandas.DataFrame: The DataFrame of the analyzed data.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib\n\n    Example:\n    >>> data = [[1, 1, 1], [1, 1, 1], [1, 1, 2], [1, 2, 3], [1, 2, 3], [1, 2, 3], \n                [2, 1, 1], [2, 1, 2], [2, 1, 3], [2, 2, 3], [2, 2, 3], [2, 2, 3]]\n    >>> df = pd.DataFrame(data, columns=COLUMNS)\n    >>> analyzed_df = plot_data_distinct_values(df)\n    >>> print(analyzed_df)\n    \"\"\"\n    analyzed_df = df.groupby(COLUMNS[:-1])[COLUMNS[-1]].nunique().reset_index()\n    analyzed_df.plot(kind='line', x=COLUMNS[:-1], y=COLUMNS[-1])\n    plt.show()\n\n    return analyzed_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n\n# Constants\nCOLUMNS = ['col1', 'col2', 'col3']\n\ndef cluster_map_data_distinct_values(df):\n    \"\"\"\n    Cluster map of distinct values in a column 'col3' of a pandas DataFrame 'df', \n    grouped by 'col1' and 'col2', using seaborn.\n\n    Parameters:\n    df (pandas.DataFrame): The DataFrame to be visualized.\n\n    Returns:\n    pandas.DataFrame: The DataFrame of the analyzed data.\n\n    Requirements:\n    - pandas\n    - numpy\n    - seaborn\n\n    Example:\n    >>> data = [[1, 1, 1], [1, 1, 1], [1, 1, 2], [1, 2, 3], [1, 2, 3], [1, 2, 3], \n                [2, 1, 1], [2, 1, 2], [2, 1, 3], [2, 2, 3], [2, 2, 3], [2, 2, 3]]\n    >>> df = pd.DataFrame(data, columns=COLUMNS)\n    >>> analyzed_df = cluster_map_data_distinct_values(df)\n    >>> print(analyzed_df)\n    \"\"\"\n    analyzed_df = df.groupby(COLUMNS[:-1])[COLUMNS[-1]].nunique().reset_index()\n    analyzed_df = analyzed_df.pivot(*COLUMNS)\n    sns.clustermap(analyzed_df)\n    plt.show()\n\n    return analyzed_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n\n# Constants\nCOLUMNS = ['col1', 'col2', 'col3']\n\ndef dist_plot_data_distinct_values(df):\n    \"\"\"\n    Distribution plot of distinct values in a column 'col3' of a pandas DataFrame 'df', \n    grouped by 'col1' and 'col2', using seaborn.\n\n    Parameters:\n    df (pandas.DataFrame): The DataFrame to be visualized.\n\n    Returns:\n    pandas.DataFrame: The DataFrame of the analyzed data.\n\n    Requirements:\n    - pandas\n    - numpy\n    - seaborn\n\n    Example:\n    >>> data = [[1, 1, 1], [1, 1, 1], [1, 1, 2], [1, 2, 3], [1, 2, 3], [1, 2, 3], \n                [2, 1, 1], [2, 1, 2], [2, 1, 3], [2, 2, 3], [2, 2, 3], [2, 2, 3]]\n    >>> df = pd.DataFrame(data, columns=COLUMNS)\n    >>> analyzed_df = dist_plot_data_distinct_values(df)\n    >>> print(analyzed_df)\n    \"\"\"\n    analyzed_df = df.groupby(COLUMNS[:-1])[COLUMNS[-1]].nunique().reset_index()\n    sns.distplot(analyzed_df[COLUMNS[-1]])\n    plt.show()\n\n    return analyzed_df\n```\n"}
{"seed": "{\"task_id\": 3735814, \"prompt\": \"def f_3735814(dict1):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"any(key.startswith('EMP$$') for key in dict1)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate({'EMP$$': 1, 'EMP$$112': 4}) == True\\n\", \"\\n    assert candidate({'EMP$$': 1, 'EM$$112': 4}) == True\\n\", \"\\n    assert candidate({'EMP$33': 0}) == False\\n\"], \"entry_point\": \"f_3735814\", \"intent\": \"Check if any key in the dictionary `dict1` starts with the string `EMP$$`\", \"library\": []}", "generation": "Scenario 1: \npandas, re, os, matplotlib\n```python\nimport pandas as pd\nimport re\nimport os\nimport matplotlib.pyplot as plt\n\n# Constants\nFILE_PATH = '/path/to/directory'\nPATTERN = 'EMP$$'\n\ndef filter_files_and_plot(file_path=FILE_PATH, pattern=PATTERN):\n    \"\"\"\n    Search for all files in a directory that start with a given pattern and \n    plot the number of files against their size.\n    \n    Parameters:\n    file_path (str): The path to the directory.\n    pattern (str): The pattern to match.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with file names and their sizes.\n    \n    Requirements:\n    - pandas\n    - re\n    - os\n    - matplotlib.pyplot\n    \n    Example:\n    >>> report = filter_files_and_plot()\n    >>> print(report)\n    >>> report['Size'].plot(kind='bar')\n    \"\"\"\n    file_sizes = []\n    for file in os.listdir(file_path):\n        if re.match(pattern, file):\n            file_sizes.append((file, os.path.getsize(os.path.join(file_path, file))))\n\n    df = pd.DataFrame(file_sizes, columns=['File', 'Size'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \ncollections, random, json, datetime\n```python\nimport collections\nimport random\nimport json\nfrom datetime import datetime\n\n# Constants\nEMPLOYEES = ['EMP$$001', 'EMP$$002', 'EMP$$003', 'EMP$$004', 'EMP$$005']\nSTATUS = ['Present', 'Absent', 'Leave', 'On Duty']\n\ndef generate_employee_attendance():\n    \"\"\"\n    Generate a JSON report of employee attendance for a given day.\n    \n    Parameters:\n    None\n    \n    Returns:\n    str: A JSON string of the employee attendance report.\n    \n    Requirements:\n    - collections\n    - random\n    - json\n    - datetime\n    \n    Example:\n    >>> report = generate_employee_attendance()\n    >>> print(report)\n    \"\"\"\n    attendance = collections.defaultdict(str)\n\n    for employee in EMPLOYEES:\n        attendance[employee] = random.choice(STATUS)\n\n    report = {\n        'date': datetime.now().strftime('%Y-%m-%d'),\n        'attendance': dict(attendance)\n    }\n\n    return json.dumps(report, indent=4)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnumpy, pandas, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nEMPLOYEES = ['EMP$$001', 'EMP$$002', 'EMP$$003', 'EMP$$004', 'EMP$$005']\n\ndef simulate_employee_salaries():\n    \"\"\"\n    Simulate employee salaries over a period of time and plot the trend.\n    \n    Parameters:\n    None\n    \n    Returns:\n    DataFrame: A pandas DataFrame with simulated salaries.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    \n    Example:\n    >>> salaries = simulate_employee_salaries()\n    >>> print(salaries)\n    >>> salaries.plot()\n    \"\"\"\n    salaries = pd.DataFrame()\n\n    for employee in EMPLOYEES:\n        salary = np.random.normal(loc=50000, scale=10000, size=12)\n        salaries[employee] = salary\n\n    return salaries\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \ncsv, collections, itertools\n```python\nimport csv\nimport collections\nimport itertools\n\n# Constants\nCSV_FILE = '/path/to/file.csv'\nEMP_PREFIX = 'EMP$$'\n\ndef count_employee_records(csv_file=CSV_FILE, emp_prefix=EMP_PREFIX):\n    \"\"\"\n    Count the number of records for each employee in a CSV file.\n    \n    Parameters:\n    csv_file (str): The path to the CSV file.\n    emp_prefix (str): The prefix of the employee IDs.\n    \n    Returns:\n    dict: A dictionary with the count of records for each employee.\n    \n    Requirements:\n    - csv\n    - collections\n    - itertools\n    \n    Example:\n    >>> counts = count_employee_records()\n    >>> print(counts)\n    \"\"\"\n    counter = collections.Counter()\n\n    with open(csv_file, 'r') as f:\n        reader = csv.reader(f)\n        for row in reader:\n            if row[0].startswith(emp_prefix):\n                counter[row[0]] += 1\n\n    return dict(counter)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \npandas, seaborn, matplotlib\n```python\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nDATA = '/path/to/data.csv'\nEMP_PREFIX = 'EMP$$'\n\ndef plot_employee_data(data=DATA, emp_prefix=EMP_PREFIX):\n    \"\"\"\n    Load a CSV file into a DataFrame, filter the rows where the employee ID starts with a \n    given prefix, and plot a histogram of their ages.\n    \n    Parameters:\n    data (str): The path to the data file.\n    emp_prefix (str): The prefix of the employee IDs.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the filtered data.\n    \n    Requirements:\n    - pandas\n    - seaborn\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = plot_employee_data()\n    >>> print(df)\n    >>> sns.histplot(data=df, x='Age', kde=True)\n    \"\"\"\n    df = pd.read_csv(data)\n    df = df[df['Employee ID'].str.startswith(emp_prefix)]\n    return df\n```"}
{"seed": "{\"task_id\": 3735814, \"prompt\": \"def f_3735814(dict1):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"[value for key, value in list(dict1.items()) if key.startswith('EMP$$')]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert sorted(candidate({'EMP$$': 1, 'EMP$$112': 4})) == [1, 4]\\n\", \"\\n    assert sorted(candidate({'EMP$$': 1, 'EM$$112': 4})) == [1]\\n\", \"\\n    assert sorted(candidate({'EMP$33': 0})) == []\\n\"], \"entry_point\": \"f_3735814\", \"intent\": \"create list of values from dictionary `dict1` that have a key that starts with 'EMP$$'\", \"library\": []}", "generation": "Scenario 1:\ncollections, itertools, random\n```python\nfrom collections import defaultdict\nfrom itertools import islice\nfrom random import randint\n\n# Constants\nNUM_RANGE = (1, 100)\nPREFICES = ['EMP$$', 'MAN$$', 'DEV$$', 'HR$$']\n\ndef generate_employee_data(dict1):\n    \"\"\"\n    Generate a dictionary of employee data. The keys are department codes \n    (start with 'EMP$$') and the values are lists of employee salaries in that department.\n    \n    Parameters:\n    dict1 (dict): A dictionary with department codes as keys and number of employees \n                  as values.\n    \n    Returns:\n    dict: A dictionary with department codes as keys and lists of employee salaries \n          as values.\n    \n    Requirements:\n    - collections\n    - itertools\n    - random\n    \n    Example:\n    >>> d = {'EMP$$': 10, 'MAN$$': 5, 'DEV$$': 8, 'HR$$': 7}\n    >>> emp_data = generate_employee_data(d)\n    >>> print(emp_data)\n    \"\"\"\n    employee_data = defaultdict(list)\n    \n    for prefix, num_employees in dict1.items():\n        if not prefix.startswith('EMP$$'):\n            continue\n\n        salaries = (randint(*NUM_RANGE) for _ in range(num_employees))\n        employee_data[prefix].extend(list(islice(salaries, num_employees)))\n\n    return dict(employee_data)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncollections, random, string\n```python\nimport collections\nimport random\nimport string\n\n# Constants\nPREFICES = ['EMP$$', 'MAN$$', 'DEV$$', 'HR$$']\n\ndef assign_employee_ids(dict1):\n    \"\"\"\n    Assign a unique ID to each employee in a company based on their department \n    (indicated by 'EMP$$'). The ID consists of the department code followed by \n    a random string of 5 letters.\n\n    Parameters:\n    dict1 (dict): A dictionary with department codes as keys and number of employees \n                  as values.\n\n    Returns:\n    list: A list of unique employee IDs.\n\n    Requirements:\n    - collections\n    - random\n    - string\n\n    Example:\n    >>> d = {'EMP$$': 10, 'MAN$$': 5, 'DEV$$': 8, 'HR$$': 7}\n    >>> emp_ids = assign_employee_ids(d)\n    >>> print(emp_ids)\n    \"\"\"\n    employee_ids = []\n    \n    for prefix, num_employees in dict1.items():\n        if not prefix.startswith('EMP$$'):\n            continue\n\n        for _ in range(num_employees):\n            random_str = ''.join(random.choice(string.ascii_uppercase) for _ in range(5))\n            employee_ids.append(f'{prefix}{random_str}')\n\n    return employee_ids\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ncollections, random, matplotlib\n```python\nimport collections\nimport random\nimport matplotlib.pyplot as plt\n\n# Constants\nPREFICES = ['EMP$$', 'MAN$$', 'DEV$$', 'HR$$']\nSALARY_RANGE = (20000, 100000)\n\ndef analyze_emp_salaries(dict1):\n    \"\"\"\n    Analyze the distribution of salaries within the 'EMP$$' department. Generate \n    random salaries for each employee and plot a histogram.\n\n    Parameters:\n    dict1 (dict): A dictionary with department codes as keys and number of employees \n                  as values.\n\n    Returns:\n    None\n\n    Requirements:\n    - collections\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> d = {'EMP$$': 10, 'MAN$$': 5, 'DEV$$': 8, 'HR$$': 7}\n    >>> analyze_emp_salaries(d)\n    \"\"\"\n    emp_salaries = []\n    \n    for prefix, num_employees in dict1.items():\n        if not prefix.startswith('EMP$$'):\n            continue\n\n        for _ in range(num_employees):\n            salary = random.randint(*SALARY_RANGE)\n            emp_salaries.append(salary)\n\n    plt.hist(emp_salaries, bins=10, alpha=0.5)\n    plt.title('Salary Distribution in EMP$$ Department')\n    plt.xlabel('Salary')\n    plt.ylabel('Number of Employees')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ncollections, random, statistics\n```python\nimport collections\nimport random\nimport statistics\n\n# Constants\nPREFICES = ['EMP$$', 'MAN$$', 'DEV$$', 'HR$$']\nAGE_RANGE = (22, 60)\n\ndef calculate_emp_age_statistics(dict1):\n    \"\"\"\n    Calculate the mean, median, and mode of the ages of employees in the 'EMP$$' \n    department. Generate random ages for each employee.\n\n    Parameters:\n    dict1 (dict): A dictionary with department codes as keys and number of employees \n                  as values.\n\n    Returns:\n    tuple: A tuple of mean, median, and mode of employee ages.\n\n    Requirements:\n    - collections\n    - random\n    - statistics\n\n    Example:\n    >>> d = {'EMP$$': 10, 'MAN$$': 5, 'DEV$$': 8, 'HR$$': 7}\n    >>> stats = calculate_emp_age_statistics(d)\n    >>> print(stats)\n    \"\"\"\n    emp_ages = []\n    \n    for prefix, num_employees in dict1.items():\n        if not prefix.startswith('EMP$$'):\n            continue\n\n        for _ in range(num_employees):\n            age = random.randint(*AGE_RANGE)\n            emp_ages.append(age)\n\n    mean_age = statistics.mean(emp_ages)\n    median_age = statistics.median(emp_ages)\n    mode_age = statistics.mode(emp_ages)\n\n    return mean_age, median_age, mode_age\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\ncollections, random, json\n```python\nimport collections\nimport random\nimport json\n\n# Constants\nPREFICES = ['EMP$$', 'MAN$$', 'DEV$$', 'HR$$']\nLEVELS = ['Junior', 'Mid', 'Senior']\n\ndef generate_emp_level_data(dict1):\n    \"\"\"\n    Generate a JSON object of employee level data. The keys are department codes \n    (start with 'EMP$$') and the values are lists of employee levels in that department.\n\n    Parameters:\n    dict1 (dict): A dictionary with department codes as keys and number of employees \n                  as values.\n\n    Returns:\n    str: A JSON object of employee level data.\n\n    Requirements:\n    - collections\n    - random\n    - json\n\n    Example:\n    >>> d = {'EMP$$': 10, 'MAN$$': 5, 'DEV$$': 8, 'HR$$': 7}\n    >>> level_data = generate_emp_level_data(d)\n    >>> print(level_data)\n    \"\"\"\n    level_data = collections.defaultdict(list)\n    \n    for prefix, num_employees in dict1.items():\n        if not prefix.startswith('EMP$$'):\n            continue\n\n        for _ in range(num_employees):\n            level = random.choice(LEVELS)\n            level_data[prefix].append(level)\n\n    return json.dumps(level_data)\n```\n"}
{"seed": "{\"task_id\": 26097916, \"prompt\": \"def f_26097916(sf):\\n\\t\", \"suffix\": \"\\n\\treturn df\", \"canonical_solution\": \"df = pd.DataFrame({'email': sf.index, 'list': sf.values})\", \"test_start\": \"\\nimport pandas as pd\\n\\ndef check(candidate):\", \"test\": [\"\\n    dict = {'email1': [1.0, 5.0, 7.0], 'email2': [4.2, 3.6, -0.9]}\\n    sf = pd.Series(dict)\\n    k = [['email1', [1.0, 5.0, 7.0]], ['email2', [4.2, 3.6, -0.9]]]\\n    df1 = pd.DataFrame(k, columns=['email', 'list'])\\n    df2 = candidate(sf)\\n    assert pd.DataFrame.equals(df1, df2)\\n\"], \"entry_point\": \"f_26097916\", \"intent\": \"convert a pandas series `sf` into a pandas dataframe `df` with columns `email` and `list`\", \"library\": [\"pandas\"]}", "generation": "Scenario 1:\npandas, json, numpy, matplotlib\n```python\nimport pandas as pd\nimport json\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['email', 'list']\n\ndef analyze_email_data(json_file):\n    \"\"\"\n    Load email data from a JSON file, convert it into a pandas DataFrame, \n    calculate the sum and the mean of the list associated with each email, \n    and then plot these values.\n\n    Parameters:\n    json_file (str): The path to the JSON file.\n\n    Returns:\n    DataFrame: A pandas DataFrame with email data.\n\n    Requirements:\n    - pandas\n    - json\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = analyze_email_data('email_data.json')\n    >>> print(df)\n    \"\"\"\n    with open(json_file, 'r') as file:\n        email_data = json.load(file)\n\n    df = pd.DataFrame(email_data, columns=COLUMNS)\n    df['sum'] = df['list'].apply(np.sum)\n    df['mean'] = df['list'].apply(np.mean)\n\n    df[['sum', 'mean']].plot(kind='bar')\n    plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, csv, seaborn, scipy.stats\n```python\nimport pandas as pd\nimport csv\nimport seaborn as sns\nfrom scipy import stats\n\n# Constants\nCOLUMNS = ['email', 'list']\n\ndef analyze_email_csv(csv_file):\n    \"\"\"\n    Load email data from a CSV file, convert it into a pandas DataFrame, \n    calculate the sum, the mean, and the standard deviation of the list \n    associated with each email, and then plot a histogram of the means.\n\n    Parameters:\n    csv_file (str): The path to the CSV file.\n\n    Returns:\n    DataFrame: A pandas DataFrame with email data.\n\n    Requirements:\n    - pandas\n    - csv\n    - seaborn\n    - scipy.stats\n\n    Example:\n    >>> df = analyze_email_csv('email_data.csv')\n    >>> print(df)\n    \"\"\"\n    df = pd.read_csv(csv_file, names=COLUMNS)\n    df['sum'] = df['list'].apply(sum)\n    df['mean'] = df['list'].apply(np.mean)\n    df['std'] = df['list'].apply(np.std)\n\n    sns.histplot(df['mean'], kde=True)\n    plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, os, numpy, matplotlib\n```python\nimport pandas as pd\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['email', 'list']\n\ndef process_email_data(directory):\n    \"\"\"\n    Traverse a directory for CSV files, load email data from each CSV file, \n    convert it into a pandas DataFrame, calculate the sum, the mean, and \n    the median of the list associated with each email, and then plot a histogram \n    of the medians.\n\n    Parameters:\n    directory (str): The path to the directory.\n\n    Returns:\n    None.\n\n    Requirements:\n    - pandas\n    - os\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> process_email_data('data_directory')\n    \"\"\"\n    for filename in os.listdir(directory):\n        if filename.endswith('.csv'):\n            df = pd.read_csv(os.path.join(directory, filename), names=COLUMNS)\n            df['sum'] = df['list'].apply(sum)\n            df['mean'] = df['list'].apply(np.mean)\n            df['median'] = df['list'].apply(np.median)\n\n            df['median'].hist()\n            plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, sqlite3, numpy, matplotlib\n```python\nimport pandas as pd\nimport sqlite3\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['email', 'list']\n\ndef analyze_email_db(db_file):\n    \"\"\"\n    Load email data from a SQLite database, convert it into a pandas DataFrame, \n    calculate the sum, the mean, and the variance of the list associated with \n    each email, and then plot these values.\n\n    Parameters:\n    db_file (str): The path to the SQLite database file.\n\n    Returns:\n    DataFrame: A pandas DataFrame with email data.\n\n    Requirements:\n    - pandas\n    - sqlite3\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = analyze_email_db('email_data.db')\n    >>> print(df)\n    \"\"\"\n    conn = sqlite3.connect(db_file)\n    df = pd.read_sql_query(\"SELECT * FROM EmailData\", conn, columns=COLUMNS)\n    df['sum'] = df['list'].apply(np.sum)\n    df['mean'] = df['list'].apply(np.mean)\n    df['var'] = df['list'].apply(np.var)\n\n    df[['sum', 'mean', 'var']].plot(kind='bar')\n    plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, requests, numpy, matplotlib\n```python\nimport pandas as pd\nimport requests\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['email', 'list']\n\ndef analyze_email_api(api_url):\n    \"\"\"\n    Load email data from a RESTful API, convert it into a pandas DataFrame, \n    calculate the sum, the mean, and the standard deviation of the list \n    associated with each email, and then plot a histogram of the standard deviations.\n\n    Parameters:\n    api_url (str): The URL of the RESTful API.\n\n    Returns:\n    DataFrame: A pandas DataFrame with email data.\n\n    Requirements:\n    - pandas\n    - requests\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = analyze_email_api('https://api.example.com/email_data')\n    >>> print(df)\n    \"\"\"\n    response = requests.get(api_url)\n    email_data = response.json()\n\n    df = pd.DataFrame(email_data, columns=COLUMNS)\n    df['sum'] = df['list'].apply(np.sum)\n    df['mean'] = df['list'].apply(np.mean)\n    df['std'] = df['list'].apply(np.std)\n\n    df['std'].hist()\n    plt.show()\n\n    return df\n```\n"}
{"seed": "{\"task_id\": 4048964, \"prompt\": \"def f_4048964(list):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"'\\\\t'.join(map(str, list))\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(['hello', 'world', '!']) == 'hello\\\\tworld\\\\t!'\\n\", \"\\n    assert candidate([]) == \\\"\\\"\\n\", \"\\n    assert candidate([\\\"mconala\\\"]) == \\\"mconala\\\"\\n\", \"\\n    assert candidate([\\\"MCoNaLa\\\"]) == \\\"MCoNaLa\\\"\\n\"], \"entry_point\": \"f_4048964\", \"intent\": \"concatenate elements of list `list` by tabs `\\t`\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, random, matplotlib, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nNUM_CATEGORIES = 5\nNUM_SAMPLES = 100\nCATEGORIES = ['Category {}'.format(i+1) for i in range(NUM_CATEGORIES)]\n\ndef generate_and_visualize_data():\n    \"\"\"\n    Generate a dataset with five categories, each having 100 samples of random integer values,\n    and visualize the distribution of the values using a boxplot.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the generated data.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - random\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> data = generate_and_visualize_data()\n    >>> print(data)\n    >>> sns.boxplot(data=data, orient='h')\n    \"\"\"\n    data = pd.DataFrame()\n\n    for category in CATEGORIES:\n        data[category] = [randint(0, 100) for _ in range(NUM_SAMPLES)]\n\n    return data\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, shutil, fnmatch, time\n```python\nimport os\nimport shutil\nimport fnmatch\nimport time\n\n# Constants\nFOLDER_PATH = './files/'\nARCHIVE_PATH = './archive/'\nFILE_PATTERN = '*.txt'\n\ndef archive_old_files(days):\n    \"\"\"\n    Move all .txt files in a specified folder that are older than a specified number of days to an archive folder.\n\n    Parameters:\n    days (int): The number of days.\n\n    Requirements:\n    - os\n    - shutil\n    - fnmatch\n    - time\n\n    Example:\n    >>> archive_old_files(30)\n    \"\"\"\n    current_time = time.time()\n\n    for filename in os.listdir(FOLDER_PATH):\n        if fnmatch.fnmatch(filename, FILE_PATTERN):\n            file_path = os.path.join(FOLDER_PATH, filename)\n            file_time = os.path.getmtime(file_path)\n            file_age_days = (current_time - file_time) / (60 * 60 * 24)\n            if file_age_days > days:\n                shutil.move(file_path, ARCHIVE_PATH)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nrequests, bs4, re, pandas\n```python\nimport requests\nfrom bs4 import BeautifulSoup\nimport re\nimport pandas as pd\n\n# Constants\nURL = 'https://www.python.org/downloads/'\n\ndef scrape_python_versions():\n    \"\"\"\n    Scrape the Python.org downloads page and return a DataFrame with Python version and release date.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the scraped data.\n    \n    Requirements:\n    - requests\n    - bs4 (BeautifulSoup)\n    - re\n    - pandas\n\n    Example:\n    >>> versions = scrape_python_versions()\n    >>> print(versions)\n    \"\"\"\n    response = requests.get(URL)\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    version_elements = soup.select('.download-list-widget .list-row-container li')\n    version_data = []\n\n    for version_element in version_elements:\n        version = version_element.find('span', class_='release-number').text.split(' ')[1]\n        date = version_element.find('span', class_='release-date').text\n        version_data.append([version, date])\n\n    version_df = pd.DataFrame(version_data, columns=['Version', 'Release Date'])\n\n    return version_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nmath, random, matplotlib, numpy\n```python\nimport math\nimport random\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nPI = math.pi\nRADIUS = 1\nNUM_POINTS = 1000\n\ndef estimate_pi():\n    \"\"\"\n    Estimate the value of pi using the Monte Carlo method and visualize the points used in the estimation.\n\n    Returns:\n    float: The estimated value of pi.\n    \n    Requirements:\n    - math\n    - random\n    - matplotlib.pyplot\n    - numpy\n\n    Example:\n    >>> pi_estimate = estimate_pi()\n    >>> print(pi_estimate)\n    \"\"\"\n    inside_circle = 0\n    total_points = 0\n    x_inside = []\n    y_inside = []\n    x_outside = []\n    y_outside = []\n\n    for _ in range(NUM_POINTS):\n        x = random.uniform(-RADIUS, RADIUS)\n        y = random.uniform(-RADIUS, RADIUS)\n        distance = math.sqrt(x**2 + y**2)\n        if distance <= RADIUS:\n            inside_circle += 1\n            x_inside.append(x)\n            y_inside.append(y)\n        else:\n            x_outside.append(x)\n            y_outside.append(y)\n        total_points += 1\n\n    plt.scatter(x_inside, y_inside, color='b')\n    plt.scatter(x_outside, y_outside, color='r')\n    plt.gca().set_aspect('equal', adjustable='box')\n\n    pi_estimate = 4 * inside_circle / total_points\n\n    return pi_estimate\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nrequests, json, pandas, matplotlib\n```python\nimport requests\nimport json\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nURL = 'https://api.coindesk.com/v1/bpi/currentprice.json'\n\ndef plot_bitcoin_price():\n    \"\"\"\n    Fetch the current Bitcoin price in different currencies from the CoinDesk API and plot a bar graph.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the Bitcoin price data.\n    \n    Requirements:\n    - requests\n    - json\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> btc_price = plot_bitcoin_price()\n    >>> print(btc_price)\n    >>> btc_price.plot(kind='bar')\n    \"\"\"\n    response = requests.get(URL)\n    data = json.loads(response.text)\n    bpi_data = data['bpi']\n\n    btc_price = pd.DataFrame.from_dict(bpi_data, orient='index')\n\n    return btc_price\n```"}
{"seed": "{\"task_id\": 3182716, \"prompt\": \"def f_3182716():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"'\\\\xd0\\\\xbf\\\\xd1\\\\x80\\\\xd0\\\\xb8'.encode('raw_unicode_escape')\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate() == b'\\\\xd0\\\\xbf\\\\xd1\\\\x80\\\\xd0\\\\xb8'\\n\"], \"entry_point\": \"f_3182716\", \"intent\": \"print unicode string '\\\\xd0\\\\xbf\\\\xd1\\\\x80\\\\xd0\\\\xb8' with utf-8\", \"library\": []}", "generation": "Scenario 1:\npandas, numpy, matplotlib.pyplot, sklearn.preprocessing\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\n\n# Constants\nCOUNTRIES = ['Russia', 'China', 'USA', 'India', 'Brazil']\nAGES = np.arange(18, 60)\nGENDERS = ['Male', 'Female']\n\ndef generate_demographics(num_samples):\n    \"\"\"\n    Generate a demographics dataset containing information about individuals \n    from different countries, their ages, and genders. The genders are encoded \n    using LabelEncoder from sklearn.\n\n    Parameters:\n    num_samples (int): The number of samples to generate.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the demographics data.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.preprocessing.LabelEncoder\n\n    Example:\n    >>> demographics = generate_demographics(100)\n    >>> print(demographics)\n    >>> demographics['Age'].hist(bins=20)\n    >>> plt.show()\n    \"\"\"\n    countries = np.random.choice(COUNTRIES, num_samples)\n    ages = np.random.choice(AGES, num_samples)\n    genders = np.random.choice(GENDERS, num_samples)\n\n    le = LabelEncoder()\n    encoded_genders = le.fit_transform(genders)\n\n    demographics = pd.DataFrame({\n        'Country': countries,\n        'Age': ages,\n        'Gender': encoded_genders\n    })\n\n    return demographics\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, pathlib, shutil, zipfile\n```python\nimport os\nfrom pathlib import Path\nimport shutil\nimport zipfile\n\n# Constants\nBASE_DIR = Path.cwd()\n\ndef manage_files(source_dir, target_dir, archive_name):\n    \"\"\"\n    Move all files from a source directory to a target directory, and create a \n    zip archive of the target directory.\n\n    Parameters:\n    source_dir (str): The source directory.\n    target_dir (str): The target directory.\n    archive_name (str): The name of the zip archive.\n\n    Returns:\n    str: The path to the zip archive.\n\n    Requirements:\n    - os\n    - pathlib.Path\n    - shutil\n    - zipfile\n\n    Example:\n    >>> manage_files('source_dir', 'target_dir', 'archive')\n    \"\"\"\n    source_dir = BASE_DIR / source_dir\n    target_dir = BASE_DIR / target_dir\n    archive_path = BASE_DIR / f\"{archive_name}.zip\"\n\n    for file_name in os.listdir(source_dir):\n        shutil.move(source_dir / file_name, target_dir)\n\n    with zipfile.ZipFile(archive_path, 'w') as zipf:\n        for root, dirs, files in os.walk(target_dir):\n            for file in files:\n                zipf.write(os.path.join(root, file))\n\n    return str(archive_path)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, scipy.stats, matplotlib.pyplot\n```python\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\n# Constants\nSAMPLE_SIZE = 1000\n\ndef plot_normal_distribution(mu, sigma):\n    \"\"\"\n    Generate a sample from a normal distribution with a given mean and standard \n    deviation, and plot its histogram and probability density function.\n\n    Parameters:\n    mu (float): The mean of the distribution.\n    sigma (float): The standard deviation of the distribution.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_normal_distribution(0, 1)\n    \"\"\"\n    sample = np.random.normal(mu, sigma, SAMPLE_SIZE)\n    density = stats.norm(mu, sigma).pdf(sample)\n\n    plt.hist(sample, density=True, bins=30, alpha=0.6, color='g')\n    plt.plot(sample, density, 'r--')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\njson, csv, requests, io\n```python\nimport json\nimport csv\nimport requests\nfrom io import StringIO\n\n# Constants\nCSV_URL = 'https://example.com/data.csv'\nJSON_FILE = 'data.json'\n\ndef download_and_convert_data():\n    \"\"\"\n    Download a CSV file from a URL, convert it to JSON format, and save it to a file.\n\n    Parameters:\n    None\n\n    Returns:\n    str: The path to the JSON file.\n\n    Requirements:\n    - json\n    - csv\n    - requests\n    - io.StringIO\n\n    Example:\n    >>> download_and_convert_data()\n    \"\"\"\n    response = requests.get(CSV_URL)\n    csv_data = csv.reader(StringIO(response.text))\n\n    headers = next(csv_data)\n    json_data = [dict(zip(headers, row)) for row in csv_data]\n\n    with open(JSON_FILE, 'w') as json_file:\n        json.dump(json_data, json_file)\n\n    return JSON_FILE\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, pandas, sklearn.linear_model, matplotlib.pyplot\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\n\n# Constants\nSAMPLE_SIZE = 100\n\ndef linear_regression_plot():\n    \"\"\"\n    Generate a sample dataset, fit a linear regression model to it, and plot \n    the data points and the regression line.\n\n    Parameters:\n    None\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.linear_model.LinearRegression\n    - matplotlib.pyplot\n\n    Example:\n    >>> linear_regression_plot()\n    \"\"\"\n    X = np.random.rand(SAMPLE_SIZE, 1)\n    y = 3 * X + np.random.randn(SAMPLE_SIZE, 1)\n\n    model = LinearRegression()\n    model.fit(X, y)\n\n    plt.scatter(X, y)\n    plt.plot(X, model.predict(X), color='red')\n    plt.show()\n```"}
{"seed": "{\"task_id\": 3182716, \"prompt\": \"def f_3182716():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"'Sopet\\\\xc3\\\\xb3n'.encode('latin-1').decode('utf-8')\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate() == \\\"Sopet\\u00f3n\\\"\\n\"], \"entry_point\": \"f_3182716\", \"intent\": \"Encode a latin character in string `Sopet\\\\xc3\\\\xb3n` properly\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, codecs, re, datetime\n```python\nimport pandas as pd\nimport numpy as np\nimport codecs\nimport re\nfrom datetime import datetime\n\n# Constants\nLATIN_NAMES = ['Sopet\u00f3n', 'M\u00e9ndez', 'G\u00f3mez', 'P\u00e9rez', 'Mu\u00f1oz']\nNAMES = ['Smith', 'Johnson', 'Williams', 'Brown', 'Jones']\n\ndef generate_user_data():\n    \"\"\"\n    Generate a random user data DataFrame with 100 rows. \n    Each row contains: ID (1-100), Name (randomly selected from LATIN_NAMES \n    and NAMES), Date of Birth (randomly generated between 1980 and 2000), \n    and Email (generated based on name and year of birth). \n    All the Latin characters in the names are encoded improperly and need to be fixed.\n\n    Returns:\n    DataFrame: A pandas DataFrame with user data.\n\n    Requirements:\n    - pandas\n    - numpy\n    - codecs\n    - re\n    - datetime\n\n    Example:\n    >>> user_data = generate_user_data()\n    >>> print(user_data)\n    \"\"\"\n    data = []\n    for i in range(1, 101):\n        is_latin = np.random.choice([True, False])\n        if is_latin:\n            name = np.random.choice(LATIN_NAMES)\n        else:\n            name = np.random.choice(NAMES)\n        birth_year = np.random.randint(1980, 2001)\n        dob = datetime(birth_year, np.random.randint(1, 13), np.random.randint(1, 29))\n        email = re.sub(r'\\s+', '.', codecs.encode(name, 'latin-1').decode('utf-8').lower()) + str(birth_year) + '@example.com'\n        data.append([i, name, dob, email])\n\n    df = pd.DataFrame(data, columns=['ID', 'Name', 'Date of Birth', 'Email'])\n    df['Name'] = df['Name'].apply(lambda x: codecs.encode(x, 'latin-1').decode('utf-8'))\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncodecs, random, string, hashlib\n```python\nimport codecs\nimport random\nimport string\nimport hashlib\n\n# Constants\nPASSWORD_LENGTH = 10\nSALT = \"salty\".encode('utf-8')\n\ndef generate_hashed_password():\n    \"\"\"\n    Generate a random password of 10 characters including latin characters, \n    numbers, and symbols. Then, hash the password using SHA256 algorithm after \n    mixing it with a salt.\n\n    Returns:\n    str: The hashed password.\n\n    Requirements:\n    - codecs\n    - random\n    - string\n    - hashlib\n\n    Example:\n    >>> hashed_password = generate_hashed_password()\n    >>> print(hashed_password)\n    \"\"\"\n    password_chars = string.ascii_letters + string.digits + string.punctuation\n    password = ''.join(random.choice(password_chars) for i in range(PASSWORD_LENGTH))\n    password = codecs.encode(password, 'latin-1').decode('utf-8')\n    salted_password = (password + SALT).encode('utf-8')\n    hashed_password = hashlib.sha256(salted_password).hexdigest()\n\n    return hashed_password\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \ncodecs, os, zipfile\n```python\nimport codecs\nimport os\nimport zipfile\n\n# Constants\nDIRECTORY_NAME = 'latin_files'\nFILE_NAMES = ['file1.txt', 'file2.txt', 'file3.txt']\n\ndef zip_latin_files():\n    \"\"\"\n    Create a directory named 'latin_files', create 3 .txt files with latin \n    characters, encode them improperly, and then zip the directory.\n\n    Returns:\n    str: The zipped file name.\n\n    Requirements:\n    - codecs\n    - os\n    - zipfile\n\n    Example:\n    >>> zipped_file = zip_latin_files()\n    >>> print(zipped_file)\n    \"\"\"\n    os.makedirs(DIRECTORY_NAME, exist_ok=True)\n\n    for file_name in FILE_NAMES:\n        with open(os.path.join(DIRECTORY_NAME, file_name), 'w') as f:\n            f.write(codecs.encode('Sopet\u00f3n', 'latin-1').decode('utf-8'))\n\n    zipped_file = DIRECTORY_NAME + '.zip'\n    with zipfile.ZipFile(zipped_file, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        for root, dirs, files in os.walk(DIRECTORY_NAME):\n            for file in files:\n                zipf.write(os.path.join(root, file))\n\n    return zipped_file\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \ncodecs, csv, random\n```python\nimport codecs\nimport csv\nimport random\n\n# Constants\nCSV_FILE = 'names.csv'\nLATIN_NAMES = ['Sopet\u00f3n', 'M\u00e9ndez', 'G\u00f3mez', 'P\u00e9rez', 'Mu\u00f1oz']\nNAMES = ['Smith', 'Johnson', 'Williams', 'Brown', 'Jones']\n\ndef generate_csv_file():\n    \"\"\"\n    Generate a CSV file with 100 rows. Each row contains a name and age (randomly \n    generated between 20 and 50). Half of the names are randomly selected from \n    LATIN_NAMES and encoded improperly, and the other half are selected from NAMES.\n\n    Returns:\n    str: The CSV file name.\n\n    Requirements:\n    - csv\n    - codecs\n    - random\n\n    Example:\n    >>> file_name = generate_csv_file()\n    >>> print(file_name)\n    \"\"\"\n    with open(CSV_FILE, 'w', newline='') as csvfile:\n        fieldnames = ['Name', 'Age']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        writer.writeheader()\n\n        for _ in range(50):\n            writer.writerow({'Name': codecs.encode(random.choice(LATIN_NAMES), 'latin-1').decode('utf-8'), 'Age': random.randint(20, 51)})\n            writer.writerow({'Name': random.choice(NAMES), 'Age': random.randint(20, 51)})\n\n    return CSV_FILE\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \ncodecs, json, requests, random\n```python\nimport codecs\nimport json\nimport requests\nimport random\n\n# Constants\nAPI_URL = 'https://jsonplaceholder.typicode.com/posts'\nPOST_TITLES = ['Sopet\u00f3n', 'M\u00e9ndez', 'G\u00f3mez', 'P\u00e9rez', 'Mu\u00f1oz']\n\ndef post_random_latin_title():\n    \"\"\"\n    Select a random title from POST_TITLES, encode it improperly, and then post \n    it to a dummy API (https://jsonplaceholder.typicode.com/posts).\n\n    Returns:\n    dict: The response from the API (parsed as JSON).\n\n    Requirements:\n    - codecs\n    - json\n    - requests\n    - random\n\n    Example:\n    >>> response = post_random_latin_title()\n    >>> print(response)\n    \"\"\"\n    post_data = {\n        'title': codecs.encode(random.choice(POST_TITLES), 'latin-1').decode('utf-8'),\n        'body': 'Lorem ipsum dolor sit amet, consectetur adipiscing elit.',\n        'userId': 1\n    }\n    response = requests.post(API_URL, data=json.dumps(post_data))\n    return response.json()\n```"}
{"seed": "{\"task_id\": 35622945, \"prompt\": \"def f_35622945(s):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"re.findall('n(?<=[^n]n)n+(?=[^n])(?i)', s)\", \"test_start\": \"\\nimport re \\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(\\\"ncnnnne\\\") == ['nnnn']\\n\", \"\\n    assert candidate(\\\"nn\\\") == []\\n\", \"\\n    assert candidate(\\\"ask\\\") == []\\n\"], \"entry_point\": \"f_35622945\", \"intent\": \"regex, find \\\"n\\\"s only in the middle of string `s`\", \"library\": [\"re\"]}", "generation": "Scenario 3:\nre, itertools, collections\n```python\nimport re\nimport itertools\nimport collections\n\n# Constants\nPATTERNS = ['nnn', 'aaa', 'sss', 'ddd', 'fff']\n\ndef count_patterns_in_string(s):\n    \"\"\"\n    Count occurrences of specified patterns in a string. The patterns are 'nnn', 'aaa', 'sss', 'ddd', 'fff'.\n    \n    Parameters:\n    s (str): The input string.\n    \n    Returns:\n    dict: A dictionary with patterns as keys and their counts as values.\n    \n    Requirements:\n    - re\n    - itertools\n    - collections\n    \n    Example:\n    >>> count_patterns_in_string(\"nnnaaaasssdddeeefffggg\")\n    \"\"\"\n    pattern_counts = collections.defaultdict(int)\n\n    for pattern in PATTERNS:\n        pattern_counts[pattern] = len(re.findall(pattern, s))\n\n    return dict(pattern_counts)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nre, os, glob, time\n```python\nimport re\nimport os\nimport glob\nimport time\n\n# Constants\nFILE_TYPE = '.txt'\nSEARCH_PATTERN = r'\\berror\\b'\n\ndef find_pattern_in_files(dir_path):\n    \"\"\"\n    Search for a specific pattern (the word 'error') in all text files within a directory and its subdirectories.\n    \n    Parameters:\n    dir_path (str): The path of the directory.\n    \n    Returns:\n    dict: A dictionary with file names as keys and the count of occurrences as values.\n    \n    Requirements:\n    - re\n    - os\n    - glob\n    - time\n    \n    Example:\n    >>> find_pattern_in_files(\"/path/to/directory\")\n    \"\"\"\n    result = {}\n    file_paths = glob.glob(f'{dir_path}/**/*{FILE_TYPE}', recursive=True)\n\n    for file_path in file_paths:\n        with open(file_path, 'r') as file:\n            content = file.read()\n        matches = re.findall(SEARCH_PATTERN, content, re.IGNORECASE)\n        if matches:\n            result[os.path.relpath(file_path, dir_path)] = len(matches)\n\n    return result\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nre, pandas, numpy, matplotlib\n```python\nimport re\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nPATTERNS_TO_PLOT = ['nnn', 'aaa', 'sss', 'ddd', 'fff']\n\ndef plot_patterns_in_dataframe(df):\n    \"\"\"\n    Count occurrences of specified patterns in each row of a pandas DataFrame and plot the counts.\n    The DataFrame is assumed to have a column named 'strings' containing the strings to be processed.\n    \n    Parameters:\n    df (DataFrame): The input DataFrame.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - re\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = pd.DataFrame({'strings': ['nnnaaaasssddd', 'aaasssdddnnn', 'sssdddnnnaaa', 'dddnnnaaasss', 'nnnaaasssddd']})\n    >>> plot_patterns_in_dataframe(df)\n    \"\"\"\n    counts = {pattern: [] for pattern in PATTERNS_TO_PLOT}\n    \n    for _, row in df.iterrows():\n        for pattern in PATTERNS_TO_PLOT:\n            counts[pattern].append(len(re.findall(pattern, row['strings'])))\n    \n    for pattern, values in counts.items():\n        plt.plot(values, label=pattern)\n    \n    plt.legend()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 6:\nre, os, json, sys\n```python\nimport re\nimport os\nimport json\nimport sys\n\n# Constants\nPATTERN = r'\\b\\d{4}\\b'\n\ndef extract_years_from_files(dir_path):\n    \"\"\"\n    Extract 4-digit numbers (assumed to be years) from all text files within a directory and its subdirectories.\n    Results are saved in a JSON file in the same directory.\n    \n    Parameters:\n    dir_path (str): The path of the directory.\n    \n    Returns:\n    str: The path of the JSON file with the results.\n    \n    Requirements:\n    - re\n    - os\n    - json\n    - sys\n    \n    Example:\n    >>> extract_years_from_files(\"/path/to/directory\")\n    \"\"\"\n    result = {}\n    file_paths = [os.path.join(root, file) for root, _, files in os.walk(dir_path) for file in files if file.endswith('.txt')]\n\n    for file_path in file_paths:\n        with open(file_path, 'r') as file:\n            content = file.read()\n        matches = re.findall(PATTERN, content)\n        if matches:\n            result[os.path.relpath(file_path, dir_path)] = matches\n\n    output_path = os.path.join(dir_path, 'extracted_years.json')\n    with open(output_path, 'w') as output_file:\n        json.dump(result, output_file, indent=4)\n\n    return output_path\n```\n`GPT_ODEX_BREAK`\n\nScenario 7:\nre, pandas, seaborn, matplotlib\n```python\nimport re\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nPATTERNS = ['nnn', 'aaa', 'sss', 'ddd', 'fff']\n\ndef heatmap_of_patterns_in_dataframe(df):\n    \"\"\"\n    Count occurrences of specified patterns in each row of a pandas DataFrame and plot a heatmap of the counts.\n    The DataFrame is assumed to have a column named 'strings' containing the strings to be processed.\n    \n    Parameters:\n    df (DataFrame): The input DataFrame.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - re\n    - pandas\n    - seaborn\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = pd.DataFrame({'strings': ['nnnaaaasssddd', 'aaasssdddnnn', 'sssdddnnnaaa', 'dddnnnaaasss', 'nnnaaasssddd']})\n    >>> heatmap_of_patterns_in_dataframe(df)\n    \"\"\"\n    counts = pd.DataFrame(columns=PATTERNS)\n    \n    for _, row in df.iterrows():\n        row_counts = {pattern: len(re.findall(pattern, row['strings'])) for pattern in PATTERNS}\n        counts = counts.append(row_counts, ignore_index=True)\n    \n    sns.heatmap(counts, annot=True, cmap='viridis')\n    plt.show()\n```"}
{"seed": "{\"task_id\": 5306756, \"prompt\": \"def f_5306756():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"'{0:.0f}%'.format(1.0 / 3 * 100)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert(candidate() == \\\"33%\\\")\\n\"], \"entry_point\": \"f_5306756\", \"intent\": \"display the float `1/3*100` as a percentage\", \"library\": []}", "generation": "Scenario 1: \nnumpy, pandas, matplotlib, seaborn\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nCATEGORIES = ['A', 'B', 'C', 'D', 'E']\nSAMPLE_SIZE = 1000\n\ndef generate_and_plot_sample_percentages():\n    \"\"\"\n    Generate a random sample of categories, calculate the percentage of each category\n    in the sample, and display the percentages in a bar plot.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the sample data and their percentages.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> sample_df = generate_and_plot_sample_percentages()\n    >>> print(sample_df)\n    \"\"\"\n    sample = np.random.choice(CATEGORIES, SAMPLE_SIZE)\n    sample_df = pd.DataFrame(sample, columns=['Category'])\n    sample_counts = sample_df['Category'].value_counts(normalize=True).mul(100).round(1).astype(str) + '%'\n    \n    plt.figure(figsize=(10,6))\n    sns.barplot(x=sample_counts.index, y=sample_counts.values.astype(float), palette='viridis')\n    plt.xlabel('Category')\n    plt.ylabel('Percentage')\n    plt.title('Category Percentages')\n    plt.show()\n\n    return sample_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nnumpy, pandas, sklearn.model_selection, sklearn.linear_model\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\n# Constants\nNUM_SAMPLES = 500\nRANDOM_SEED = 42\nTEST_SIZE = 0.2\n\ndef generate_and_fit_linear_regression():\n    \"\"\"\n    Generate a dataset with a single feature and a target variable, split the dataset into training and test sets,\n    fit a linear regression model on the training set, and calculate the R-squared score on the test set.\n\n    Returns:\n    float: The R-squared score of the fitted model on the test set.\n\n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.model_selection.train_test_split\n    - sklearn.linear_model.LinearRegression\n\n    Example:\n    >>> r_squared = generate_and_fit_linear_regression()\n    >>> print(r_squared)\n    \"\"\"\n    np.random.seed(RANDOM_SEED)\n    X = np.random.rand(NUM_SAMPLES, 1)\n    y = 3*X.squeeze() + 2 + np.random.randn(NUM_SAMPLES)\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_SEED)\n    \n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    \n    r_squared = model.score(X_test, y_test)\n    \n    return r_squared\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnumpy, pandas, matplotlib, seaborn\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nNUM_PAIRS = 500\nRANDOM_SEED = 42\n\ndef generate_pairs_and_plot_correlation():\n    \"\"\"\n    Generate a pair of normally distributed random variables, calculate the correlation coefficient,\n    and plot a scatter plot with a regression line.\n\n    Returns:\n    float: The correlation coefficient.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> correlation = generate_pairs_and_plot_correlation()\n    >>> print(correlation)\n    \"\"\"\n    np.random.seed(RANDOM_SEED)\n    X = np.random.randn(NUM_PAIRS)\n    Y = 2.5*X + np.random.randn(NUM_PAIRS)\n    df = pd.DataFrame({'X': X, 'Y': Y})\n\n    correlation = df['X'].corr(df['Y'])\n\n    plt.figure(figsize=(8, 6))\n    sns.regplot(x='X', y='Y', data=df, line_kws={'color': 'red'})\n    plt.title(f'Correlation: {correlation:.2f}')\n    plt.show()\n\n    return correlation\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, pandas, sklearn.preprocessing, sklearn.metrics\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\n\n# Constants\nNUM_SAMPLES = 1000\nRANDOM_SEED = 42\n\ndef generate_scale_and_calculate_mse():\n    \"\"\"\n    Generate a dataset, standardize it using sklearn's StandardScaler, and calculate the mean squared error \n    between the original and the standardized datasets.\n\n    Returns:\n    float: The mean squared error.\n\n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.preprocessing.StandardScaler\n    - sklearn.metrics.mean_squared_error\n\n    Example:\n    >>> mse = generate_scale_and_calculate_mse()\n    >>> print(mse)\n    \"\"\"\n    np.random.seed(RANDOM_SEED)\n    data = np.random.randn(NUM_SAMPLES, 1)*5 + 2\n    df = pd.DataFrame(data, columns=['Original'])\n\n    scaler = StandardScaler()\n    scaled_data = scaler.fit_transform(df)\n\n    mse = mean_squared_error(df, scaled_data)\n\n    return mse\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnumpy, pandas, sklearn.model_selection, sklearn.ensemble\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Constants\nNUM_SAMPLES = 1000\nRANDOM_SEED = 42\nCV = 5\n\ndef generate_and_cross_validate_rf():\n    \"\"\"\n    Generate a dataset with multiple features and a target variable, and perform cross-validation \n    on a RandomForestRegressor model using the dataset.\n\n    Returns:\n    float: The mean cross-validation score.\n\n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.model_selection.cross_val_score\n    - sklearn.ensemble.RandomForestRegressor\n\n    Example:\n    >>> cv_score = generate_and_cross_validate_rf()\n    >>> print(cv_score)\n    \"\"\"\n    np.random.seed(RANDOM_SEED)\n    X = np.random.randn(NUM_SAMPLES, 5)\n    y = np.sum(X, axis=1) + np.random.randn(NUM_SAMPLES)\n    \n    model = RandomForestRegressor(n_estimators=100, random_state=RANDOM_SEED)\n    \n    cv_scores = cross_val_score(model, X, y, cv=CV)\n    \n    return np.mean(cv_scores)\n```"}
{"seed": "{\"task_id\": 2878084, \"prompt\": \"def f_2878084(mylist):\\n\\t\", \"suffix\": \"\\n\\treturn mylist\", \"canonical_solution\": \"mylist.sort(key=lambda x: x['title'])\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    input = [\\n        {'title':'New York Times', 'title_url':'New_York_Times','id':4}, \\n        {'title':'USA Today','title_url':'USA_Today','id':6}, \\n        {'title':'Apple News','title_url':'Apple_News','id':2}\\n    ]\\n    res = [\\n        {'title': 'Apple News', 'title_url': 'Apple_News', 'id': 2}, \\n        {'title': 'New York Times', 'title_url': 'New_York_Times', 'id': 4},\\n        {'title': 'USA Today', 'title_url': 'USA_Today', 'id': 6}\\n    ]\\n    assert candidate(input) == res\\n\"], \"entry_point\": \"f_2878084\", \"intent\": \"sort a list of dictionary `mylist` by the key `title`\", \"library\": []}", "generation": "Scenario 1: \npandas, datetime, os, sys, csv\n```python\nimport pandas as pd\nimport os\nimport sys\nimport csv\nfrom datetime import datetime\n\n# Constants\nFILE_PATH = 'data.csv'\nOUTPUT_PATH = 'sorted_data.csv'\n\ndef sort_csv_file(file_path=FILE_PATH, output_path=OUTPUT_PATH, sort_key='title'):\n    \"\"\"\n    Sort a CSV file by a given key and write the sorted data to another CSV file.\n\n    Parameters:\n    file_path (str): The path to the input CSV file.\n    output_path (str): The path to the output CSV file.\n    sort_key (str): The key to sort the CSV file by.\n\n    Returns:\n    str: The path to the output CSV file.\n\n    Requirements:\n    - pandas\n    - os\n    - sys\n    - csv\n    - datetime\n\n    Example:\n    >>> sort_csv_file('data.csv', 'sorted_data.csv', 'title')\n    \"\"\"\n    try:\n        df = pd.read_csv(file_path)\n        df.sort_values(by=[sort_key], inplace=True)\n        df.to_csv(output_path, index=False)\n    except Exception as e:\n        sys.stderr.write(f\"Error while sorting the file: {str(e)}\")\n        sys.exit(1)\n\n    return output_path\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nre, json, operator, sys\n```python\nimport re\nimport json\nimport operator\nimport sys\n\n# Constants\nPATTERN = '[a-zA-Z0-9]+'\n\ndef sort_json_string(json_string, sort_key='title'):\n    \"\"\"\n    Sort a JSON string by a given key.\n\n    Parameters:\n    json_string (str): The JSON string to sort.\n    sort_key (str): The key to sort the JSON string by.\n\n    Returns:\n    str: The sorted JSON string.\n\n    Requirements:\n    - re\n    - json\n    - operator\n    - sys\n\n    Example:\n    >>> json_string = '[{\"title\":\"New York Times\",\"title_url\":\"New_York_Times\",\"id\":4},{\"title\":\"USA Today\",\"title_url\":\"USA_Today\",\"id\":6},{\"title\":\"Apple News\",\"title_url\":\"Apple_News\",\"id\":2}]'\n    >>> sort_json_string(json_string, 'title')\n    \"\"\"\n    try:\n        json_data = json.loads(json_string)\n        json_data.sort(key=operator.itemgetter(sort_key))\n        sorted_json_string = json.dumps(json_data)\n    except json.JSONDecodeError as e:\n        sys.stderr.write(f\"Error while decoding the JSON string: {str(e)}\")\n        sys.exit(1)\n\n    return sorted_json_string\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \npandas, urllib.request, operator\n```python\nimport pandas as pd\nimport urllib.request\nimport operator\n\n# Constants\nURL = \"http://example.com/data.csv\"\n\ndef sort_url_csv(url=URL, sort_key='title'):\n    \"\"\"\n    Download a CSV from a given URL, sort it by a given key, and return it as a pandas DataFrame.\n\n    Parameters:\n    url (str): The URL to the CSV file.\n    sort_key (str): The key to sort the CSV file by.\n\n    Returns:\n    DataFrame: The sorted pandas DataFrame.\n\n    Requirements:\n    - pandas\n    - urllib.request\n    - operator\n\n    Example:\n    >>> sort_url_csv('http://example.com/data.csv', 'title')\n    \"\"\"\n    df = pd.read_csv(url)\n    df.sort_values(by=[sort_key], inplace=True)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, pandas, matplotlib.pyplot, seaborn\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nFILE_PATH = 'data.csv'\n\ndef plot_sorted_distribution(file_path=FILE_PATH, sort_key='title'):\n    \"\"\"\n    Plot the distribution of a sorted CSV file.\n\n    Parameters:\n    file_path (str): The path to the CSV file.\n    sort_key (str): The key to sort the CSV file by.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> plot_sorted_distribution('data.csv', 'title')\n    \"\"\"\n    df = pd.read_csv(file_path)\n    df.sort_values(by=[sort_key], inplace=True)\n    \n    sns.distplot(df[sort_key])\n\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nrandom, pandas, matplotlib.pyplot, seaborn\n```python\nimport random\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nLENGTH = 1000\nRANGE = 100\n\ndef plot_sorted_random_distribution(length=LENGTH, range=RANGE):\n    \"\"\"\n    Generate a list of random numbers, sort it, and plot the distribution.\n\n    Parameters:\n    length (int): The length of the list of random numbers.\n    range (int): The range of the random numbers.\n\n    Returns:\n    None\n\n    Requirements:\n    - random\n    - pandas\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> plot_sorted_random_distribution(1000, 100)\n    \"\"\"\n    random_numbers = [random.randint(1, range) for _ in range(length)]\n    random_numbers.sort()\n\n    sns.distplot(random_numbers)\n\n    plt.show()\n```"}
{"seed": "{\"task_id\": 2878084, \"prompt\": \"def f_2878084(l):\\n\\t\", \"suffix\": \"\\n\\treturn l\", \"canonical_solution\": \"l.sort(key=lambda x: x['title'])\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    input = [\\n        {'title':'New York Times', 'title_url':'New_York_Times','id':4}, \\n        {'title':'USA Today','title_url':'USA_Today','id':6}, \\n        {'title':'Apple News','title_url':'Apple_News','id':2}\\n    ]\\n    res = [\\n        {'title': 'Apple News', 'title_url': 'Apple_News', 'id': 2}, \\n        {'title': 'New York Times', 'title_url': 'New_York_Times', 'id': 4},\\n        {'title': 'USA Today', 'title_url': 'USA_Today', 'id': 6}\\n    ]\\n    assert candidate(input) == res\\n\"], \"entry_point\": \"f_2878084\", \"intent\": \"sort a list `l` of dicts by dict value 'title'\", \"library\": []}", "generation": "Scenario 1:\ncollections, operator, itertools, random\n```python\nfrom collections import defaultdict\nfrom operator import itemgetter\nfrom itertools import groupby\nimport random\n\n# Constants\nCATEGORIES = ['Sports', 'Technology', 'Health', 'Science', 'Business']\n\ndef sort_and_group_news_articles(news_articles):\n    \"\"\"\n    Sort a list of news articles by 'category' and 'title'. The news articles \n    are then grouped by 'category'.\n\n    Parameters:\n    news_articles (list): A list of dictionaries where each dictionary represents \n    a news article with keys 'title', 'title_url', 'id', and 'category'.\n\n    Returns:\n    dict: A dictionary where the keys are categories and the values are lists \n    of articles sorted by 'title' in that category.\n\n    Requirements:\n    - collections.defaultdict\n    - operator.itemgetter\n    - itertools.groupby\n    - random\n\n    Example:\n    >>> articles = [{'title': 'Apple News', 'title_url': 'Apple_News', 'id': 2, 'category': 'Technology'},\n    ...             {'title': 'New York Times', 'title_url': 'New_York_Times', 'id': 4, 'category': 'Sports'},\n    ...             {'title': 'USA Today', 'title_url': 'USA_Today', 'id': 6, 'category': 'Health'}]\n    >>> sorted_articles = sort_and_group_news_articles(articles)\n    >>> print(sorted_articles)\n    \"\"\"\n    news_articles.sort(key=itemgetter('category', 'title'))\n\n    grouped_articles = defaultdict(list)\n    for category, group in groupby(news_articles, key=itemgetter('category')):\n        grouped_articles[category] = list(group)\n\n    return grouped_articles\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, datetime, pytz, matplotlib\n```python\nimport pandas as pd\nfrom datetime import datetime\nimport pytz\nimport matplotlib.pyplot as plt\n\n# Constants\nCATEGORIES = ['Sports', 'Technology', 'Health', 'Science', 'Business']\n\ndef analyze_article_publication_times(articles, timezone):\n    \"\"\"\n    Analyze the publication times of a list of articles: \n    1) Convert 'published_time' to a specified timezone\n    2) Group articles by 'category'\n    3) For each category, calculate the count, mean, min, max publication times\n\n    Parameters:\n    articles (list): A list of dictionaries where each dictionary represents \n    an article with keys 'title', 'title_url', 'id', 'category', and 'published_time' (in UTC).\n    timezone (str): The string representation of the timezone to which the 'published_time' should be converted.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the count, mean, min, max publication times for each category.\n\n    Requirements:\n    - pandas\n    - datetime\n    - pytz\n    - matplotlib.pyplot\n\n    Example:\n    >>> articles = [{'title': 'Apple News', 'title_url': 'Apple_News', 'id': 2, 'category': 'Technology', 'published_time': datetime(2023, 6, 15, 12, 0, 0, tzinfo=pytz.UTC)},\n    ...             {'title': 'New York Times', 'title_url': 'New_York_Times', 'id': 4, 'category': 'Sports', 'published_time': datetime(2023, 6, 16, 12, 0, 0, tzinfo=pytz.UTC)},\n    ...             {'title': 'USA Today', 'title_url': 'USA_Today', 'id': 6, 'category': 'Health', 'published_time': datetime(2023, 6, 17, 12, 0, 0, tzinfo=pytz.UTC)}]\n    >>> analysis_df = analyze_article_publication_times(articles, 'America/New_York')\n    >>> print(analysis_df)\n    \"\"\"\n    tz = pytz.timezone(timezone)\n    for article in articles:\n        article['published_time'] = article['published_time'].astimezone(tz)\n\n    df = pd.DataFrame(articles)\n    df['published_time'] = df['published_time'].dt.hour\n\n    analysis_df = df.groupby('category')['published_time'].agg(['count', 'mean', 'min', 'max'])\n\n    return analysis_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nrandom, pandas, numpy, matplotlib\n```python\nimport random\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCATEGORIES = ['Sports', 'Technology', 'Health', 'Science', 'Business']\n\ndef generate_random_article_data(n):\n    \"\"\"\n    Generate 'n' random article data and return a DataFrame.\n\n    Parameters:\n    n (int): The number of articles to generate.\n\n    Returns:\n    DataFrame: A pandas DataFrame with 'n' articles.\n\n    Requirements:\n    - random\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = generate_random_article_data(10)\n    >>> print(df)\n    >>> df.hist(column='views')\n    \"\"\"\n    data = []\n    for _ in range(n):\n        title = f\"Article {_}\"\n        title_url = f\"Article_{_}\"\n        id = _\n        category = random.choice(CATEGORIES)\n        views = np.random.poisson(1000)\n        data.append({'title': title, 'title_url': title_url, 'id': id, 'category': category, 'views': views})\n\n    df = pd.DataFrame(data)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, random, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\n\n# Constants\nCATEGORIES = ['Sports', 'Technology', 'Health', 'Science', 'Business']\n\ndef generate_and_analyze_article_data(n):\n    \"\"\"\n    Generate 'n' random article data, analyze the number of views by category, \n    and plot the result.\n\n    Parameters:\n    n (int): The number of articles to generate.\n\n    Returns:\n    Series: A pandas Series with the total number of views for each category.\n\n    Requirements:\n    - pandas\n    - numpy\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> views_by_category = generate_and_analyze_article_data(100)\n    >>> print(views_by_category)\n    >>> views_by_category.plot(kind='bar')\n    \"\"\"\n    data = []\n    for _ in range(n):\n        title = f\"Article {_}\"\n        title_url = f\"Article_{_}\"\n        id = _\n        category = random.choice(CATEGORIES)\n        views = np.random.poisson(1000)\n        data.append({'title': title, 'title_url': title_url, 'id': id, 'category': category, 'views': views})\n\n    df = pd.DataFrame(data)\n    views_by_category = df.groupby('category')['views'].sum()\n\n    return views_by_category\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, random, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\n\n# Constants\nCATEGORIES = ['Sports', 'Technology', 'Health', 'Science', 'Business']\n\ndef generate_and_visualize_article_data(n):\n    \"\"\"\n    Generate 'n' random article data, visualize the distribution \n    of views by category, and the correlation between article id and views.\n\n    Parameters:\n    n (int): The number of articles to generate.\n\n    Returns:\n    DataFrame: A pandas DataFrame with 'n' articles.\n\n    Requirements:\n    - pandas\n    - numpy\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = generate_and_visualize_article_data(100)\n    >>> print(df)\n    >>> df.boxplot(column='views', by='category')\n    >>> df.plot(kind='scatter', x='id', y='views')\n    \"\"\"\n    data = []\n    for _ in range(n):\n        title = f\"Article {_}\"\n        title_url = f\"Article_{_}\"\n        id = _\n        category = random.choice(CATEGORIES)\n        views = np.random.poisson(1000)\n        data.append({'title': title, 'title_url': title_url, 'id': id, 'category': category, 'views': views})\n\n    df = pd.DataFrame(data)\n\n    return df\n```\n"}
{"seed": "{\"task_id\": 2878084, \"prompt\": \"def f_2878084(l):\\n\\t\", \"suffix\": \"\\n\\treturn l\", \"canonical_solution\": \"l.sort(key=lambda x: (x['title'], x['title_url'], x['id']))\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    input = [\\n        {'title':'New York Times', 'title_url':'New_York_Times','id':4}, \\n        {'title':'USA Today','title_url':'USA_Today','id':6}, \\n        {'title':'Apple News','title_url':'Apple_News','id':2}\\n    ]\\n    res = [\\n        {'title': 'Apple News', 'title_url': 'Apple_News', 'id': 2}, \\n        {'title': 'New York Times', 'title_url': 'New_York_Times', 'id': 4},\\n        {'title': 'USA Today', 'title_url': 'USA_Today', 'id': 6}\\n    ]\\n    assert candidate(input) == res\\n\"], \"entry_point\": \"f_2878084\", \"intent\": \"sort a list of dictionaries by the value of keys 'title', 'title_url', 'id' in ascending order.\", \"library\": []}", "generation": "Scenario 1: \npandas, random, csv, matplotlib\n```python\nimport pandas as pd\nimport random\nimport csv\nimport matplotlib.pyplot as plt\n\n# Constants\nCATEGORIES = ['Sports', 'Technology', 'Business', 'Politics', 'Entertainment']\nNEWS_SITES = ['New York Times', 'USA Today', 'Apple News', 'CNN', 'BBC']\n\ndef generate_news_data_and_visualize(n):\n    \"\"\"\n    Generate a CSV file with random news data and then visualize the distribution of news sites and categories.\n    \n    Parameters:\n    n (int): The number of news items to generate.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the generated news data.\n    \n    Requirements:\n    - pandas\n    - random\n    - csv\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = generate_news_data_and_visualize(1000)\n    >>> df['Site'].value_counts().plot(kind='bar')\n    >>> df['Category'].value_counts().plot(kind='bar')\n    \"\"\"\n    news_data = []\n    \n    for _ in range(n):\n        site = random.choice(NEWS_SITES)\n        category = random.choice(CATEGORIES)\n        news_data.append({'Site': site, 'Category': category})\n    \n    with open('news_data.csv', 'w', newline='') as csvfile:\n        fieldnames = ['Site', 'Category']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        writer.writeheader()\n        writer.writerows(news_data)\n        \n    df = pd.read_csv('news_data.csv')\n    \n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \npandas, random, csv, matplotlib, numpy\n```python\nimport pandas as pd\nimport random\nimport csv\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nCATEGORIES = ['Sports', 'Technology', 'Business', 'Politics', 'Entertainment']\nNEWS_SITES = ['New York Times', 'USA Today', 'Apple News', 'CNN', 'BBC']\nLIKERT_SCALE = ['Strongly Disagree', 'Disagree', 'Neither Agree nor Disagree', 'Agree', 'Strongly Agree']\n\ndef generate_news_survey_data_and_analyze(n):\n    \"\"\"\n    Generate a CSV file with random news survey data, read it into a pandas DataFrame, \n    and then analyze the distribution of responses and the average response value.\n    \n    Parameters:\n    n (int): The number of survey responses to generate.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the analyzed survey data.\n    \n    Requirements:\n    - pandas\n    - random\n    - csv\n    - matplotlib.pyplot\n    - numpy\n    \n    Example:\n    >>> df = generate_news_survey_data_and_analyze(1000)\n    >>> df['Response'].value_counts().plot(kind='bar')\n    >>> df['Value'].mean()\n    \"\"\"\n    survey_data = []\n    \n    for _ in range(n):\n        site = random.choice(NEWS_SITES)\n        category = random.choice(CATEGORIES)\n        response = random.choice(LIKERT_SCALE)\n        value = LIKERT_SCALE.index(response) + 1  # Assign a numerical value to the response\n        survey_data.append({'Site': site, 'Category': category, 'Response': response, 'Value': value})\n    \n    with open('news_survey_data.csv', 'w', newline='') as csvfile:\n        fieldnames = ['Site', 'Category', 'Response', 'Value']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        writer.writeheader()\n        writer.writerows(survey_data)\n        \n    df = pd.read_csv('news_survey_data.csv')\n    \n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \npandas, matplotlib, csv, random, statistics\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport csv\nimport random\nimport statistics\n\n# Constants\nCOUNTRIES = ['USA', 'UK', 'China', 'India', 'Germany']\nPRODUCTS = ['Product A', 'Product B', 'Product C', 'Product D', 'Product E']\n\ndef generate_sales_data_and_analyze(n):\n    \"\"\"\n    Generate a CSV file with random sales data, read it into a pandas DataFrame, \n    and then analyze the distribution of sales and the average sales per product per country.\n    \n    Parameters:\n    n (int): The number of sales records to generate.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the analyzed sales data.\n    \n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - csv\n    - random\n    - statistics\n    \n    Example:\n    >>> df = generate_sales_data_and_analyze(1000)\n    >>> df.groupby(['Product', 'Country'])['Sales'].mean().plot(kind='bar')\n    \"\"\"\n    sales_data = []\n    \n    for _ in range(n):\n        country = random.choice(COUNTRIES)\n        product = random.choice(PRODUCTS)\n        sales = random.randint(1, 100)\n        sales_data.append({'Country': country, 'Product': product, 'Sales': sales})\n    \n    with open('sales_data.csv', 'w', newline='') as csvfile:\n        fieldnames = ['Country', 'Product', 'Sales']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        writer.writeheader()\n        writer.writerows(sales_data)\n        \n    df = pd.read_csv('sales_data.csv')\n    \n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \npandas, matplotlib, csv, random, numpy\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport csv\nimport random\nimport numpy as np\n\n# Constants\nCOUNTRIES = ['USA', 'UK', 'China', 'India', 'Germany']\nPRODUCTS = ['Product A', 'Product B', 'Product C', 'Product D', 'Product E']\n\ndef generate_and_visualize_profit_data(n):\n    \"\"\"\n    Generate a CSV file with random profit data, read it into a pandas DataFrame, \n    and then visualize the distribution of profit per product per country.\n    \n    Parameters:\n    n (int): The number of profit records to generate.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the generated profit data.\n    \n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - csv\n    - random\n    - numpy\n    \n    Example:\n    >>> df = generate_and_visualize_profit_data(1000)\n    >>> df.groupby(['Product', 'Country'])['Profit'].sum().plot(kind='bar')\n    \"\"\"\n    profit_data = []\n    \n    for _ in range(n):\n        country = random.choice(COUNTRIES)\n        product = random.choice(PRODUCTS)\n        profit = np.random.normal(100, 20)\n        profit_data.append({'Country': country, 'Product': product, 'Profit': profit})\n    \n    with open('profit_data.csv', 'w', newline='') as csvfile:\n        fieldnames = ['Country', 'Product', 'Profit']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        writer.writeheader()\n        writer.writerows(profit_data)\n        \n    df = pd.read_csv('profit_data.csv')\n    \n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \npandas, matplotlib, csv, random, datetime\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport csv\nimport random\nfrom datetime import datetime, timedelta\n\n# Constants\nCOUNTRIES = ['USA', 'UK', 'China', 'India', 'Germany']\nPRODUCTS = ['Product A', 'Product B', 'Product C', 'Product D', 'Product E']\n\ndef generate_and_visualize_sales_over_time(n):\n    \"\"\"\n    Generate a CSV file with random sales data over time, read it into a pandas DataFrame, \n    and then visualize the trend of sales per product per country over time.\n    \n    Parameters:\n    n (int): The number of sales records to generate.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the generated sales data.\n    \n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - csv\n    - random\n    - datetime\n    \n    Example:\n    >>> df = generate_and_visualize_sales_over_time(1000)\n    >>> df.groupby(['Date', 'Product', 'Country'])['Sales'].sum().unstack().plot()\n    \"\"\"\n    sales_data = []\n    start_date = datetime.now()\n    \n    for i in range(n):\n        country = random.choice(COUNTRIES)\n        product = random.choice(PRODUCTS)\n        sales = random.randint(1, 100)\n        date = start_date + timedelta(days=i)\n        sales_data.append({'Date': date, 'Country': country, 'Product': product, 'Sales': sales})\n    \n    with open('sales_over_time.csv', 'w', newline='') as csvfile:\n        fieldnames = ['Date', 'Country', 'Product', 'Sales']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        writer.writeheader()\n        writer.writerows(sales_data)\n        \n    df = pd.read_csv('sales_over_time.csv')\n    df['Date'] = pd.to_datetime(df['Date'])\n    \n    return df\n```"}
{"seed": "{\"task_id\": 9323159, \"prompt\": \"def f_9323159(l1, l2):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"heapq.nlargest(10, range(len(l1)), key=lambda i: abs(l1[i] - l2[i]))\", \"test_start\": \"\\nimport heapq\\n\\ndef check(candidate):\", \"test\": [\"\\n    l1 = [99, 86, 90, 70, 86, 95, 56, 98, 80, 81]\\n    l2 = [21, 11, 21, 1, 26, 40, 4, 50, 34, 37]\\n    res = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\\n    assert candidate(l1, l2) == res\\n\"], \"entry_point\": \"f_9323159\", \"intent\": \"find 10 largest differences between each respective elements of list `l1` and list `l2`\", \"library\": [\"heapq\"]}", "generation": "Scenario 1: \nheapq, numpy, math, matplotlib\n```python\nimport heapq\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\n\n# Constants\nN = 10\n\ndef plot_largest_differences(l1, l2):\n    \"\"\"\n    Find N largest differences between each respective elements of list `l1` and list `l2`,\n    square the differences, take the square root and plot the values.\n\n    Parameters:\n    l1 (list): A list of numbers.\n    l2 (list): A list of numbers.\n\n    Requirements:\n    - heapq\n    - numpy\n    - math\n    - matplotlib.pyplot\n\n    Example:\n    >>> l1 = [99, 86, 90, 70, 86, 95, 56, 98, 80, 81]\n    >>> l2 = [21, 11, 21, 1, 26, 40, 4, 50, 34, 37]\n    >>> plot_largest_differences(l1, l2)\n    \"\"\"\n    largest_diff_indices = heapq.nlargest(N, range(len(l1)), key=lambda i: abs(l1[i] - l2[i]))\n    largest_diffs = [math.sqrt((l1[i] - l2[i])**2) for i in largest_diff_indices]\n\n    plt.plot(largest_diffs)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nheapq, pandas, numpy, matplotlib\n```python\nimport heapq\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nN = 10\n\ndef analyze_and_plot(df):\n    \"\"\"\n    Find N largest differences between each respective elements of two columns in the DataFrame,\n    calculate the logarithm of the differences and plot.\n\n    Parameters:\n    df (pandas.DataFrame): A DataFrame with numerical columns.\n\n    Requirements:\n    - heapq\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame({\n    ...     'col1': [99, 86, 90, 70, 86, 95, 56, 98, 80, 81],\n    ...     'col2': [21, 11, 21, 1, 26, 40, 4, 50, 34, 37]\n    ... })\n    >>> analyze_and_plot(df)\n    \"\"\"\n    l1 = df['col1'].values\n    l2 = df['col2'].values\n\n    largest_diff_indices = heapq.nlargest(N, range(len(l1)), key=lambda i: abs(l1[i] - l2[i]))\n    largest_diffs = [np.log(abs(l1[i] - l2[i])) for i in largest_diff_indices]\n\n    plt.plot(largest_diffs)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nheapq, numpy, pandas, scipy.stats\n```python\nimport heapq\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\n\n# Constants\nN = 10\n\ndef calculate_p_value(df):\n    \"\"\"\n    Find N largest differences between each respective elements of two columns in the DataFrame, \n    calculate the p-value using t-test and return it.\n\n    Parameters:\n    df (pandas.DataFrame): A DataFrame with numerical columns.\n\n    Returns:\n    float: The p-value.\n\n    Requirements:\n    - heapq\n    - numpy\n    - pandas\n    - scipy.stats\n\n    Example:\n    >>> df = pd.DataFrame({\n    ...     'col1': [99, 86, 90, 70, 86, 95, 56, 98, 80, 81],\n    ...     'col2': [21, 11, 21, 1, 26, 40, 4, 50, 34, 37]\n    ... })\n    >>> p_value = calculate_p_value(df)\n    \"\"\"\n    l1 = df['col1'].values\n    l2 = df['col2'].values\n\n    largest_diff_indices = heapq.nlargest(N, range(len(l1)), key=lambda i: abs(l1[i] - l2[i]))\n    largest_diffs = [abs(l1[i] - l2[i]) for i in largest_diff_indices]\n\n    _, p_value = stats.ttest_ind(l1[largest_diff_indices], l2[largest_diff_indices])\n\n    return p_value\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nheapq, numpy, pandas, sklearn.preprocessing\n```python\nimport heapq\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\n# Constants\nN = 10\n\ndef standardize_and_find_largest_differences(df):\n    \"\"\"\n    Standardize two columns in the DataFrame, find N largest differences between \n    each respective elements of the standardized columns and return the indices.\n\n    Parameters:\n    df (pandas.DataFrame): A DataFrame with numerical columns.\n\n    Returns:\n    list: The indices of the N largest differences.\n\n    Requirements:\n    - heapq\n    - numpy\n    - pandas\n    - sklearn.preprocessing\n\n    Example:\n    >>> df = pd.DataFrame({\n    ...     'col1': [99, 86, 90, 70, 86, 95, 56, 98, 80, 81],\n    ...     'col2': [21, 11, 21, 1, 26, 40, 4, 50, 34, 37]\n    ... })\n    >>> indices = standardize_and_find_largest_differences(df)\n    \"\"\"\n    scaler = StandardScaler()\n    df[['col1', 'col2']] = scaler.fit_transform(df[['col1', 'col2']])\n\n    l1 = df['col1'].values\n    l2 = df['col2'].values\n\n    largest_diff_indices = heapq.nlargest(N, range(len(l1)), key=lambda i: abs(l1[i] - l2[i]))\n\n    return largest_diff_indices\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nheapq, numpy, pandas, sklearn.linear_model\n```python\nimport heapq\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\n# Constants\nN = 10\n\ndef fit_regression_and_find_largest_residuals(df):\n    \"\"\"\n    Fit a simple linear regression model on two columns in the DataFrame, \n    find N largest residuals and return the indices.\n\n    Parameters:\n    df (pandas.DataFrame): A DataFrame with numerical columns.\n\n    Returns:\n    list: The indices of the N largest residuals.\n\n    Requirements:\n    - heapq\n    - numpy\n    - pandas\n    - sklearn.linear_model\n\n    Example:\n    >>> df = pd.DataFrame({\n    ...     'col1': [99, 86, 90, 70, 86, 95, 56, 98, 80, 81],\n    ...     'col2': [21, 11, 21, 1, 26, 40, 4, 50, 34, 37]\n    ... })\n    >>> indices = fit_regression_and_find_largest_residuals(df)\n    \"\"\"\n    X = df['col1'].values.reshape(-1, 1)\n    y = df['col2'].values\n\n    model = LinearRegression()\n    model.fit(X, y)\n\n    residuals = y - model.predict(X)\n\n    largest_residual_indices = heapq.nlargest(N, range(len(residuals)), key=lambda i: abs(residuals[i]))\n\n    return largest_residual_indices\n```\n"}
{"seed": "{\"task_id\": 29877663, \"prompt\": \"def f_29877663(soup):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"soup.find_all('span', {'class': 'starGryB sp'})\", \"test_start\": \"\\nimport bs4\\n\\ndef check(candidate):\", \"test\": [\"\\n    html = '''<span class=\\\"starBig sp\\\">4.1</span>\\n             <span class=\\\"starGryB sp\\\">2.9</span>\\n             <span class=\\\"sp starGryB\\\">2.9</span>\\n             <span class=\\\"sp starBig\\\">22</span>'''\\n    soup = bs4.BeautifulSoup(html, features=\\\"html5lib\\\")\\n    res = '''[<span class=\\\"starGryB sp\\\">2.9</span>]'''\\n    assert(str(candidate(soup)) == res)\\n\"], \"entry_point\": \"f_29877663\", \"intent\": \"BeautifulSoup find all 'span' elements in HTML string `soup` with class of 'starGryB sp'\", \"library\": [\"bs4\"]}", "generation": "Scenario 1: \nbs4, requests, re, json\n```python\nimport requests\nfrom bs4 import BeautifulSoup\nimport re\nimport json\n\n# Constants\nURL_PATTERN = \"https://www.example.com/products/{}\"\nHEADERS = {'User-Agent': 'Mozilla/5.0'}\n\ndef extract_product_details(product_ids):\n    \"\"\"\n    Extract product details (name, price, rating) from an e-commerce website for a list of \n    product IDs. The details are extracted from the HTML of the product page using BeautifulSoup.\n\n    Parameters:\n    product_ids (list): The list of product IDs.\n\n    Returns:\n    dict: A dictionary where the keys are the product IDs and the values are another dictionary \n    with keys 'Name', 'Price', and 'Rating' and corresponding values.\n\n    Requirements:\n    - bs4\n    - requests\n    - re\n    - json\n\n    Example:\n    >>> product_ids = ['1234', '5678', '9012']\n    >>> details = extract_product_details(product_ids)\n    >>> print(json.dumps(details, indent=4))\n    \"\"\"\n    details = {}\n\n    for product_id in product_ids:\n        url = URL_PATTERN.format(product_id)\n        response = requests.get(url, headers=HEADERS)\n        soup = BeautifulSoup(response.text, 'html.parser')\n\n        name = soup.find('span', {'class': 'productName'}).text\n        price = float(re.search(r'\\d+\\.\\d+', soup.find('span', {'class': 'productPrice'}).text).group())\n        rating = float(soup.find('span', {'class': 'productRating'}).text)\n\n        details[product_id] = {'Name': name, 'Price': price, 'Rating': rating}\n\n    return details\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nbs4, requests, csv, os\n```python\nimport requests\nfrom bs4 import BeautifulSoup\nimport csv\nimport os\n\n# Constants\nURL = \"https://www.example.com/news\"\nHEADERS = {'User-Agent': 'Mozilla/5.0'}\nCSV_FILE = \"news.csv\"\n\ndef scrape_news():\n    \"\"\"\n    Scrape the latest news headlines and their URLs from a news website and save them \n    in a CSV file. The headlines and URLs are extracted from the HTML of the website \n    using BeautifulSoup.\n\n    Returns:\n    str: The path of the CSV file.\n\n    Requirements:\n    - bs4\n    - requests\n    - csv\n    - os\n\n    Example:\n    >>> csv_file = scrape_news()\n    >>> print(csv_file)\n    \"\"\"\n    response = requests.get(URL, headers=HEADERS)\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    news_items = soup.find_all('div', {'class': 'newsItem'})\n\n    with open(CSV_FILE, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow([\"Headline\", \"URL\"])\n\n        for item in news_items:\n            headline = item.find('h2').text\n            url = item.find('a')['href']\n            writer.writerow([headline, url])\n\n    return os.path.realpath(file.name)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nbs4, requests, pandas, matplotlib.pyplot\n```python\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nURL = \"https://www.example.com/movies\"\nHEADERS = {'User-Agent': 'Mozilla/5.0'}\n\ndef analyze_movie_ratings():\n    \"\"\"\n    Scrape the ratings of movies from a movie website and visualize the distribution of \n    the ratings using a histogram. The ratings are extracted from the HTML of the website \n    using BeautifulSoup.\n\n    Requirements:\n    - bs4\n    - requests\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> analyze_movie_ratings()\n    \"\"\"\n    response = requests.get(URL, headers=HEADERS)\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    movie_items = soup.find_all('div', {'class': 'movieItem'})\n\n    ratings = [float(item.find('span', {'class': 'movieRating'}).text) for item in movie_items]\n\n    df = pd.DataFrame(ratings, columns=['Rating'])\n    df['Rating'].plot(kind='hist', rwidth=0.8, bins=20)\n\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nbs4, requests, csv, datetime\n```python\nimport requests\nfrom bs4 import BeautifulSoup\nimport csv\nfrom datetime import datetime\n\n# Constants\nURL = \"https://www.example.com/blog\"\nHEADERS = {'User-Agent': 'Mozilla/5.0'}\nCSV_FILE = \"blog_posts.csv\"\n\ndef scrape_blog_posts():\n    \"\"\"\n    Scrape the latest blog post titles, their dates, and URLs from a blog and save them \n    in a CSV file. The titles, dates, and URLs are extracted from the HTML of the blog \n    using BeautifulSoup.\n\n    Returns:\n    str: The path of the CSV file.\n\n    Requirements:\n    - bs4\n    - requests\n    - csv\n    - datetime\n\n    Example:\n    >>> csv_file = scrape_blog_posts()\n    >>> print(csv_file)\n    \"\"\"\n    response = requests.get(URL, headers=HEADERS)\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    blog_posts = soup.find_all('div', {'class': 'blogPost'})\n\n    with open(CSV_FILE, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow([\"Title\", \"Date\", \"URL\"])\n\n        for post in blog_posts:\n            title = post.find('h2').text\n            date = datetime.strptime(post.find('span', {'class': 'postDate'}).text, \"%Y-%m-%d\").date()\n            url = post.find('a')['href']\n            writer.writerow([title, date, url])\n\n    return os.path.realpath(file.name)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nbs4, requests, json, os\n```python\nimport requests\nfrom bs4 import BeautifulSoup\nimport json\nimport os\n\n# Constants\nURL = \"https://www.example.com/products\"\nHEADERS = {'User-Agent': 'Mozilla/5.0'}\nJSON_FILE = \"products.json\"\n\ndef scrape_product_details():\n    \"\"\"\n    Scrape the details (name and price) of products from an e-commerce website and save them \n    in a JSON file. The details are extracted from the HTML of the product page using BeautifulSoup.\n\n    Returns:\n    str: The path of the JSON file.\n\n    Requirements:\n    - bs4\n    - requests\n    - json\n    - os\n\n    Example:\n    >>> json_file = scrape_product_details()\n    >>> print(json_file)\n    \"\"\"\n    response = requests.get(URL, headers=HEADERS)\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    product_items = soup.find_all('div', {'class': 'productItem'})\n\n    details = [{'Name': item.find('span', {'class': 'productName'}).text, \n                'Price': float(item.find('span', {'class': 'productPrice'}).text.replace('$', ''))}\n               for item in product_items]\n\n    with open(JSON_FILE, 'w') as file:\n        json.dump(details, file, indent=4)\n\n    return os.path.realpath(file.name)\n```"}
{"seed": "{\"task_id\": 24189150, \"prompt\": \"def f_24189150(df, engine):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"df.to_sql('test', engine)\", \"test_start\": \"\\nimport pandas as pd\\nfrom sqlalchemy import create_engine\\n\\ndef check(candidate):\", \"test\": [\"\\n    engine = create_engine('sqlite://', echo=False)\\n    df = pd.DataFrame({'name' : ['User 1', 'User 2', 'User 3']})\\n    candidate(df, engine)\\n    result = pd.read_sql('SELECT name FROM test', engine)\\n    assert result.equals(df)\\n\"], \"entry_point\": \"f_24189150\", \"intent\": \"write records in dataframe `df` to table 'test' in schema 'a_schema' with `engine`\", \"library\": [\"pandas\", \"sqlalchemy\"]}", "generation": "Scenario 1:\npandas, sqlalchemy, numpy, matplotlib\n```python\nimport pandas as pd\nfrom sqlalchemy import create_engine\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef analyze_and_plot(df, engine, table_name):\n    \"\"\"\n    Write records in dataframe `df` to a SQL table, retrieve the data, perform some basic analysis,\n    and plot the results.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame to be written to the SQL table.\n    engine (Engine): The SQLAlchemy engine instance.\n    table_name (str): The name of the SQL table.\n\n    Returns:\n    DataFrame: A pandas DataFrame with basic statistical analysis results.\n    Plot: A histogram plot of the data.\n\n    Requirements:\n    - pandas\n    - sqlalchemy\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> engine = create_engine('sqlite://', echo=False)\n    >>> df = pd.DataFrame({'value' : np.random.randn(100)})\n    >>> analyze_and_plot(df, engine, 'test')\n    \"\"\"\n    df.to_sql(table_name, engine, if_exists='replace')\n    df_retrieved = pd.read_sql_table(table_name, engine)\n\n    # Perform some basic statistical analysis\n    stats_df = df_retrieved.describe()\n\n    # Plot a histogram\n    df_retrieved.hist(bins=30, edgecolor='black')\n    plt.title('Histogram of Values')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n\n    return stats_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, sqlalchemy, seaborn, matplotlib, numpy\n```python\nimport pandas as pd\nfrom sqlalchemy import create_engine\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef analyze_and_visualize_correlation(df, engine, table_name):\n    \"\"\"\n    Write records in dataframe `df` to a SQL table, retrieve the data, calculate the correlation\n    between columns, and visualize the correlation matrix using a heatmap.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame to be written to the SQL table.\n    engine (Engine): The SQLAlchemy engine instance.\n    table_name (str): The name of the SQL table.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the correlation matrix.\n    Plot: A heatmap of the correlation matrix.\n\n    Requirements:\n    - pandas\n    - sqlalchemy\n    - seaborn\n    - matplotlib.pyplot\n    - numpy\n\n    Example:\n    >>> engine = create_engine('sqlite://', echo=False)\n    >>> df = pd.DataFrame({'A' : np.random.randn(100), 'B' : np.random.randn(100)})\n    >>> analyze_and_visualize_correlation(df, engine, 'test')\n    \"\"\"\n    df.to_sql(table_name, engine, if_exists='replace')\n    df_retrieved = pd.read_sql_table(table_name, engine)\n\n    # Calculate the correlation matrix\n    corr_matrix = df_retrieved.corr()\n\n    # Plot a heatmap of the correlation matrix\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n    plt.title('Correlation Matrix Heatmap')\n\n    return corr_matrix\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, sqlalchemy, sklearn, seaborn, matplotlib\n```python\nimport pandas as pd\nfrom sqlalchemy import create_engine\nfrom sklearn.preprocessing import StandardScaler\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef preprocess_and_visualize(df, engine, table_name):\n    \"\"\"\n    Write records in dataframe `df` to a SQL table, retrieve the data, preprocess the data using\n    standard scaling, and visualize the distributions of the scaled data.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame to be written to the SQL table.\n    engine (Engine): The SQLAlchemy engine instance.\n    table_name (str): The name of the SQL table.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the scaled data.\n    Plot: A distribution plot of the scaled data.\n\n    Requirements:\n    - pandas\n    - sqlalchemy\n    - sklearn.preprocessing\n    - seaborn\n    - matplotlib.pyplot\n\n    Example:\n    >>> engine = create_engine('sqlite://', echo=False)\n    >>> df = pd.DataFrame({'value' : np.random.randn(100)})\n    >>> preprocess_and_visualize(df, engine, 'test')\n    \"\"\"\n    df.to_sql(table_name, engine, if_exists='replace')\n    df_retrieved = pd.read_sql_table(table_name, engine)\n\n    # Preprocess the data using standard scaling\n    scaler = StandardScaler()\n    df_scaled = pd.DataFrame(scaler.fit_transform(df_retrieved), columns=df_retrieved.columns)\n\n    # Visualize the distributions of the scaled data\n    for column in df_scaled.columns:\n        sns.distplot(df_scaled[column])\n        plt.title(f'Distribution of {column} (scaled)')\n        plt.show()\n\n    return df_scaled\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, sqlalchemy, sklearn, matplotlib, seaborn\n```python\nimport pandas as pd\nfrom sqlalchemy import create_engine\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef pca_and_visualize(df, engine, table_name, n_components):\n    \"\"\"\n    Write records in dataframe `df` to a SQL table, retrieve the data, perform PCA (Principal Component Analysis)\n    on the data, and visualize the results.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame to be written to the SQL table.\n    engine (Engine): The SQLAlchemy engine instance.\n    table_name (str): The name of the SQL table.\n    n_components (int): The number of principal components to keep.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the principal components.\n    Plot: A scatter plot of the first two principal components.\n\n    Requirements:\n    - pandas\n    - sqlalchemy\n    - sklearn.decomposition\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> engine = create_engine('sqlite://', echo=False)\n    >>> df = pd.DataFrame({'A' : np.random.randn(100), 'B' : np.random.randn(100)})\n    >>> pca_and_visualize(df, engine, 'test', 2)\n    \"\"\"\n    df.to_sql(table_name, engine, if_exists='replace')\n    df_retrieved = pd.read_sql_table(table_name, engine)\n\n    # Perform PCA on the data\n    pca = PCA(n_components=n_components)\n    pca_result = pca.fit_transform(df_retrieved)\n    df_pca = pd.DataFrame(pca_result, columns=[f'PC{i+1}' for i in range(n_components)])\n\n    # Visualize the first two principal components\n    if n_components > 1:\n        plt.figure(figsize=(10, 8))\n        sns.scatterplot(x='PC1', y='PC2', data=df_pca)\n        plt.title('Scatter Plot of Principal Components')\n        plt.show()\n\n    return df_pca\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, sqlalchemy, sklearn, matplotlib, numpy\n```python\nimport pandas as pd\nfrom sqlalchemy import create_engine\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef cluster_and_visualize(df, engine, table_name, n_clusters):\n    \"\"\"\n    Write records in dataframe `df` to a SQL table, retrieve the data, perform K-Means clustering\n    on the data, and visualize the results.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame to be written to the SQL table.\n    engine (Engine): The SQLAlchemy engine instance.\n    table_name (str): The name of the SQL table.\n    n_clusters (int): The number of clusters for K-Means.\n\n    Returns:\n    ndarray: A numpy array with the cluster labels.\n    Plot: A scatter plot of the data with different colors for each cluster.\n\n    Requirements:\n    - pandas\n    - sqlalchemy\n    - sklearn.cluster\n    - matplotlib.pyplot\n    - numpy\n\n    Example:\n    >>> engine = create_engine('sqlite://', echo=False)\n    >>> df = pd.DataFrame({'A' : np.random.randn(100), 'B' : np.random.randn(100)})\n    >>> cluster_and_visualize(df, engine, 'test', 3)\n    \"\"\"\n    df.to_sql(table_name, engine, if_exists='replace')\n    df_retrieved = pd.read_sql_table(table_name, engine)\n\n    # Perform K-Means clustering on the data\n    kmeans = KMeans(n_clusters=n_clusters)\n    kmeans.fit(df_retrieved)\n    labels = kmeans.labels_\n\n    # Visualize the clusters\n    plt.figure(figsize=(10, 8))\n    plt.scatter(df_retrieved.iloc[:, 0], df_retrieved.iloc[:, 1], c=labels)\n    plt.title('Scatter Plot of Clusters')\n    plt.show()\n\n    return labels\n```\n"}
{"seed": "{\"task_id\": 30766151, \"prompt\": \"def f_30766151(s):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"re.sub('[^(){}[\\\\]]', '', s)\", \"test_start\": \"\\nimport re\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(\\\"(a(vdwvndw){}]\\\") == \\\"((){}]\\\"\\n\", \"\\n    assert candidate(\\\"12345\\\") == \\\"\\\"\\n\"], \"entry_point\": \"f_30766151\", \"intent\": \"Extract brackets from string `s`\", \"library\": [\"re\"]}", "generation": "Scenario 1:\nre, collections, string\n```python\nimport re\nimport collections\nimport string\n\n# Constants\nBRACKET_TYPES = ['()', '{}', '[]']\n\ndef detect_imbalanced_brackets(s):\n    \"\"\"\n    Detect imbalanced brackets in string `s` and return the count of each type of imbalanced brackets.\n\n    Parameters:\n    s (str): The string to check.\n\n    Returns:\n    dict: A dictionary with the bracket types as keys and their imbalanced counts as values.\n\n    Requirements:\n    - re\n    - collections\n    - string\n    \n    Example:\n    >>> detect_imbalanced_brackets(\"((a(vdwvndw){}])\")\n    \"\"\"\n    bracket_str = re.sub('[^(){}[\\]]', '', s)\n    counter = collections.Counter(bracket_str)\n    imbalance_dict = {}\n\n    for bracket_type in BRACKET_TYPES:\n        opening_bracket, closing_bracket = bracket_type\n        imbalance_count = abs(counter[opening_bracket] - counter[closing_bracket])\n        imbalance_dict[bracket_type] = imbalance_count\n\n    return imbalance_dict\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nre, string, random\n```python\nimport re\nimport string\nimport random\n\n# Constants\nBRACKETS = \"(){}[]\"\n\ndef generate_string_with_brackets(length):\n    \"\"\"\n    Generate a random string of a given length, where each character is either a bracket or a lowercase English letter.\n\n    Parameters:\n    length (int): The length of the string to generate.\n\n    Returns:\n    str: The generated string.\n\n    Requirements:\n    - re\n    - string\n    - random\n\n    Example:\n    >>> generate_string_with_brackets(10)\n    \"\"\"\n    return ''.join(random.choice(string.ascii_lowercase + BRACKETS) for _ in range(length))\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nre, os, fnmatch\n```python\nimport re\nimport os\nimport fnmatch\n\n# Constants\nBRACKET_PATTERN = '[(){}[\\]]'\n\ndef find_files_with_brackets_in_name(directory):\n    \"\"\"\n    Find all files in a given directory whose names contain brackets.\n\n    Parameters:\n    directory (str): The directory to search in.\n\n    Returns:\n    list: A list of file names.\n\n    Requirements:\n    - re\n    - os\n    - fnmatch\n\n    Example:\n    >>> find_files_with_brackets_in_name('./')\n    \"\"\"\n    file_list = []\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if re.search(BRACKET_PATTERN, file):\n                file_list.append(os.path.join(root, file))\n\n    return file_list\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nre, json, urllib\n```python\nimport re\nimport json\nimport urllib.request\n\n# Constants\nURL_PATTERN = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n\ndef extract_urls_from_json(json_str):\n    \"\"\"\n    Extract all URLs from a JSON string.\n\n    Parameters:\n    json_str (str): The JSON string.\n\n    Returns:\n    list: A list of URLs.\n\n    Requirements:\n    - re\n    - json\n    - urllib.request\n\n    Example:\n    >>> extract_urls_from_json('{\"name\": \"John\", \"website\": \"https://www.example.com\"}')\n    \"\"\"\n    data = json.loads(json_str)\n    urls = []\n\n    def extract(dictionary):\n        for key, value in dictionary.items():\n            if isinstance(value, dict):\n                extract(value)\n            elif isinstance(value, str) and re.match(URL_PATTERN, value):\n                urls.append(value)\n\n    extract(data)\n    return urls\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nre, numpy, pandas\n```python\nimport re\nimport numpy as np\nimport pandas as pd\n\n# Constants\nBRACKETS_PATTERN = '[(){}[\\]]'\n\ndef count_brackets_in_dataframe(df):\n    \"\"\"\n    Count the total number of brackets in a pandas DataFrame.\n\n    Parameters:\n    df (DataFrame): The DataFrame to process.\n\n    Returns:\n    int: The total number of brackets.\n\n    Requirements:\n    - re\n    - numpy\n    - pandas\n\n    Example:\n    >>> df = pd.DataFrame({'A': ['(a)', 'b', 'c'], 'B': ['d', 'e', '(f)']})\n    >>> count_brackets_in_dataframe(df)\n    \"\"\"\n    return np.sum(df.applymap(lambda x: len(re.findall(BRACKETS_PATTERN, str(x)))).values)\n```\nAbove is the illustration."}
{"seed": "{\"task_id\": 1143379, \"prompt\": \"def f_1143379(L):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"list(dict((x[0], x) for x in L).values())\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    L = [['14', '65', 76], ['2', '5', 6], ['7', '12', 33], ['14', '22', 46]]\\n    res = [['14', '22', 46], ['2', '5', 6], ['7', '12', 33]]\\n    assert(candidate(L) == res)\\n\", \"\\n    assert candidate([\\\"a\\\", \\\"aa\\\", \\\"abc\\\", \\\"bac\\\"]) == [\\\"abc\\\", \\\"bac\\\"]\\n\"], \"entry_point\": \"f_1143379\", \"intent\": \"remove duplicate elements from list 'L'\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, random, itertools, json\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import randint\nimport itertools\nimport json\n\n# Constants\nLETTERS = list('abcdefghijklmnopqrstuvwxyz')\n\ndef generate_dataframes_and_find_common(L):\n    \"\"\"\n    Generate pandas DataFrames from a list of lists 'L' by using each list as a row. Each DataFrame has a random number of columns.\n    Then, find the common rows between all generated DataFrames.\n    \n    Parameters:\n    L (list): Input list of lists.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the common rows between all generated DataFrames.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - random\n    - itertools\n    - json\n    \n    Example:\n    >>> L = [['14', '65', 76], ['2', '5', 6], ['7', '12', 33], ['14', '22', 46]]\n    >>> common_rows = generate_dataframes_and_find_common(L)\n    >>> print(common_rows)\n    \"\"\"\n    dataframes = []\n    \n    for row in L:\n        num_cols = randint(1,len(row))\n        col_names = np.random.choice(LETTERS, num_cols, replace=False)\n        dataframe = pd.DataFrame([dict(zip(np.random.choice(col_names, len(row), replace=False), row))])\n        dataframes.append(dataframe)\n        \n    common_rows = pd.concat(dataframes).groupby(list(itertools.chain(*dataframes[0].columns))).filter(lambda x: len(x) == len(dataframes))\n    \n    return common_rows\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, matplotlib, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nCOLORS = ['red', 'blue', 'green', 'yellow', 'purple']\n\ndef create_dataframe_and_plot_correlation(L):\n    \"\"\"\n    Create a pandas DataFrame from a list of lists 'L' and plot a correlation matrix heatmap of the DataFrame.\n    \n    Parameters:\n    L (list): Input list of lists.\n    \n    Returns:\n    DataFrame: A pandas DataFrame created from 'L'.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - seaborn\n    \n    Example:\n    >>> L = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n    >>> df = create_dataframe_and_plot_correlation(L)\n    >>> print(df)\n    \"\"\"\n    df = pd.DataFrame(L)\n    corr = df.corr()\n    \n    plt.figure(figsize=(10,8))\n    sns.heatmap(corr, annot=True, cmap=COLORS)\n    plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, collections, matplotlib, pandas\n```python\nimport numpy as np\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef analyze_and_visualize_data(L):\n    \"\"\"\n    Analyze a list 'L' by calculating the mean, median, mode, and standard deviation.\n    Visualize the data by plotting a histogram.\n    \n    Parameters:\n    L (list): Input list.\n    \n    Returns:\n    dict: A dictionary with the mean, median, mode, and standard deviation of 'L'.\n    \n    Requirements:\n    - numpy\n    - collections.Counter\n    - matplotlib.pyplot\n    - pandas\n    \n    Example:\n    >>> L = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n    >>> stats = analyze_and_visualize_data(L)\n    >>> print(stats)\n    \"\"\"\n    mean = np.mean(L)\n    median = np.median(L)\n    mode = Counter(L).most_common(1)[0][0]\n    std_dev = np.std(L)\n    \n    plt.hist(L, bins='auto')\n    plt.title('Histogram of Data')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.show()\n    \n    return {'mean': mean, 'median': median, 'mode': mode, 'std_dev': std_dev}\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, random, matplotlib, pandas\n```python\nimport numpy as np\nfrom random import choice\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants\nLETTERS = list('abcdefghijklmnopqrstuvwxyz')\n\ndef create_random_dataframe_and_plot(L):\n    \"\"\"\n    Create a pandas DataFrame from a list 'L' where each element of 'L' is a row and columns are random letters.\n    Then, plot a bar chart of the sum of each column.\n    \n    Parameters:\n    L (list): Input list of lists.\n    \n    Returns:\n    DataFrame: A pandas DataFrame created from 'L'.\n    \n    Requirements:\n    - numpy\n    - random.choice\n    - matplotlib.pyplot\n    - pandas\n    \n    Example:\n    >>> L = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n    >>> df = create_random_dataframe_and_plot(L)\n    >>> print(df)\n    \"\"\"\n    df = pd.DataFrame(L, columns=[choice(LETTERS) for _ in range(len(L[0]))])\n    df.sum().plot(kind='bar')\n    plt.show()\n    \n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, itertools, matplotlib, pandas\n```python\nimport numpy as np\nimport itertools\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef analyze_and_visualize_combinations(L):\n    \"\"\"\n    Generate all combinations of a list 'L' with length 2, create a pandas DataFrame from these combinations and\n    plot a scatter plot of the DataFrame.\n    \n    Parameters:\n    L (list): Input list.\n    \n    Returns:\n    DataFrame: A pandas DataFrame created from the combinations of 'L'.\n    \n    Requirements:\n    - numpy\n    - itertools.combinations\n    - matplotlib.pyplot\n    - pandas\n    \n    Example:\n    >>> L = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n    >>> df = analyze_and_visualize_combinations(L)\n    >>> print(df)\n    \"\"\"\n    combinations = list(itertools.combinations(L, 2))\n    df = pd.DataFrame(combinations, columns=['x', 'y'])\n    \n    plt.scatter(df['x'], df['y'])\n    plt.show()\n    \n    return df\n```\nAbove is the illustration."}
{"seed": "{\"task_id\": 12330522, \"prompt\": \"def f_12330522(file):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"[line.rstrip('\\\\n') for line in file]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    res = ['1', '2', '3']\\n    f = open(\\\"myfile.txt\\\", \\\"a\\\")\\n    f.write(\\\"1\\\\n2\\\\n3\\\")\\n    f.close()\\n    f = open(\\\"myfile.txt\\\", \\\"r\\\")\\n    assert candidate(f) == res\\n\"], \"entry_point\": \"f_12330522\", \"intent\": \"read a file `file` without newlines\", \"library\": []}", "generation": "Scenario 1:\npandas, os, csv, matplotlib\n```python\nimport pandas as pd\nimport os\nimport csv\nimport matplotlib.pyplot as plt\n\n# Constants\nFILE_NAME = 'myfile.csv'\n\ndef generate_summary_statistics(file_name):\n    \"\"\"\n    Generate summary statistics for a csv file and plot the distribution of each numerical column.\n    \n    Parameters:\n    file_name (str): The name of the csv file.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with summary statistics for each column.\n\n    Requirements:\n    - pandas\n    - os\n    - csv\n    - matplotlib.pyplot\n    \n    Example:\n    >>> summary = generate_summary_statistics(FILE_NAME)\n    >>> print(summary)\n    \"\"\"\n    if not os.path.exists(file_name):\n        raise FileNotFoundError(\"File does not exist.\")\n\n    df = pd.read_csv(file_name)\n\n    summary_df = df.describe()\n\n    for col in summary_df.columns:\n        plt.figure()\n        df[col].hist()\n        plt.title(f'Distribution of {col}')\n        plt.show()\n\n    return summary_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncsv, json, os\n```python\nimport csv\nimport json\nimport os\n\n# Constants\nFILE_NAME = 'myfile.csv'\n\ndef convert_csv_to_json(file_name):\n    \"\"\"\n    Convert a csv file to a json file.\n    \n    Parameters:\n    file_name (str): The name of the csv file.\n    \n    Returns:\n    str: The file name of the created json file.\n\n    Requirements:\n    - csv\n    - json\n    - os\n    \n    Example:\n    >>> json_file = convert_csv_to_json(FILE_NAME)\n    >>> print(json_file)\n    \"\"\"\n    if not os.path.exists(file_name):\n        raise FileNotFoundError(\"File does not exist.\")\n\n    data = []\n\n    with open(file_name, 'r') as f:\n        csv_reader = csv.DictReader(f)\n        for row in csv_reader:\n            data.append(row)\n\n    json_file_name = file_name.split('.')[0] + '.json'\n\n    with open(json_file_name, 'w') as f:\n        json.dump(data, f)\n\n    return json_file_name\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, matplotlib, csv\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport csv\n\n# Constants\nFILE_NAME = 'myfile.csv'\n\ndef plot_data_from_file(file_name):\n    \"\"\"\n    Plot data from a csv file.\n    \n    Parameters:\n    file_name (str): The name of the csv file.\n    \n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - csv\n    \n    Example:\n    >>> plot_data_from_file(FILE_NAME)\n    \"\"\"\n    data = np.genfromtxt(file_name, delimiter=',', names=True)\n\n    for col in data.dtype.names:\n        plt.figure()\n        plt.plot(data[col])\n        plt.title(f'Plot of {col}')\n        plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ncsv, collections, numpy\n```python\nimport csv\nimport collections\nimport numpy as np\n\n# Constants\nFILE_NAME = 'myfile.csv'\n\ndef find_most_common_value(file_name):\n    \"\"\"\n    Find the most common value in each column of a csv file.\n    \n    Parameters:\n    file_name (str): The name of the csv file.\n    \n    Returns:\n    dict: A dictionary with column names as keys and most common values as values.\n\n    Requirements:\n    - csv\n    - collections\n    - numpy\n    \n    Example:\n    >>> common_values = find_most_common_value(FILE_NAME)\n    >>> print(common_values)\n    \"\"\"\n    data = np.genfromtxt(file_name, delimiter=',', names=True, dtype=None)\n\n    common_values = {}\n\n    for col in data.dtype.names:\n        counter = collections.Counter(data[col])\n        common_values[col] = counter.most_common(1)[0][0]\n\n    return common_values\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, csv, sklearn.preprocessing\n```python\nimport pandas as pd\nimport csv\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Constants\nFILE_NAME = 'myfile.csv'\n\ndef normalize_data(file_name):\n    \"\"\"\n    Normalize data in a csv file using MinMaxScaler from sklearn.\n    \n    Parameters:\n    file_name (str): The name of the csv file.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with normalized data.\n\n    Requirements:\n    - pandas\n    - csv\n    - sklearn.preprocessing.MinMaxScaler\n    \n    Example:\n    >>> normalized_data = normalize_data(FILE_NAME)\n    >>> print(normalized_data)\n    \"\"\"\n    df = pd.read_csv(file_name)\n\n    scaler = MinMaxScaler()\n    df_normalized = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n\n    return df_normalized\n```\nAbove are the illustrations."}
{"seed": "{\"task_id\": 364621, \"prompt\": \"def f_364621(testlist):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"[i for (i, x) in enumerate(testlist) if (x == 1)]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    testlist = [1,2,3,5,3,1,2,1,6]\\n    assert candidate(testlist) == [0, 5, 7]\\n\", \"\\n    testlist = [0, -1]\\n    assert candidate(testlist) == []\\n\"], \"entry_point\": \"f_364621\", \"intent\": \"get the position of item 1 in `testlist`\", \"library\": []}", "generation": "Scenario 1:\npandas, numpy, random\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import randint\n\n# Constants\nITEMS = ['apple', 'banana', 'orange', 'pineapple', 'blueberry']\nITEM_COUNT = len(ITEMS)\n\ndef get_item_positions_in_dataframe(df, item):\n    \"\"\"\n    Find the positions of a specific item in a Pandas DataFrame.\n\n    Parameters:\n    df (DataFrame): The DataFrame to search.\n    item (str): The item to find.\n\n    Returns:\n    list: A list of tuples. Each tuple contains the row-index and column-name where the item is found.\n\n    Requirements:\n    - pandas\n    - numpy\n    - random\n\n    Example:\n    >>> df = pd.DataFrame(np.random.choice(ITEMS, (5, 5)), columns=list('ABCDE'))\n    >>> print(df)\n    >>> get_item_positions_in_dataframe(df, 'apple')\n    \"\"\"\n    positions = [(index, col) for col in df for index, val in enumerate(df[col]) if val == item]\n    return positions\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, scipy.stats\n```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef get_positions_of_outliers(data):\n    \"\"\"\n    Detect the indices of outliers in a 1D numpy array based on the Z-score.\n\n    Parameters:\n    data (numpy array): The numpy array to check.\n\n    Returns:\n    list: The indices of outliers in the data.\n\n    Requirements:\n    - numpy\n    - scipy.stats\n\n    Example:\n    >>> data = np.array([1, 2, 3, 4, 5, 6, 100])\n    >>> get_positions_of_outliers(data)\n    \"\"\"\n    z_scores = np.abs(norm.ppf(data))\n    outliers = np.where(z_scores > 2)\n    return list(outliers[0])\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, cv2, matplotlib\n```python\nimport numpy as np\nimport cv2\nfrom matplotlib import pyplot as plt\n\ndef get_positions_of_color_in_image(image_path, rgb_color):\n    \"\"\"\n    Find the positions of a specific RGB color in an image.\n\n    Parameters:\n    image_path (str): The path to the image file.\n    rgb_color (tuple): The RGB color to find (in the format (R, G, B)).\n\n    Returns:\n    list: A list of tuples. Each tuple contains the row and column where the color is found.\n\n    Requirements:\n    - numpy\n    - cv2\n    - matplotlib\n\n    Example:\n    >>> get_positions_of_color_in_image('image.jpg', (255, 255, 255))\n    \"\"\"\n    image = cv2.imread(image_path)\n    positions = np.where(np.all(image == rgb_color, axis=-1))\n    return list(zip(positions[0], positions[1]))\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, matplotlib, sklearn.cluster\n```python\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom sklearn.cluster import KMeans\n\ndef find_clusters_in_data(data, n_clusters):\n    \"\"\"\n    Apply KMeans clustering to a 2D numpy array and find the indices of data points in each cluster.\n\n    Parameters:\n    data (numpy array): The 2D numpy array for clustering.\n    n_clusters (int): The number of clusters to form.\n\n    Returns:\n    dict: A dictionary where keys are cluster labels and values are lists of indices for data points in the cluster.\n\n    Requirements:\n    - numpy\n    - matplotlib\n    - sklearn.cluster\n\n    Example:\n    >>> data = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    >>> find_clusters_in_data(data, 2)\n    \"\"\"\n    kmeans = KMeans(n_clusters=n_clusters).fit(data)\n    labels = kmeans.labels_\n    clusters = {i: np.where(labels == i)[0] for i in range(n_clusters)}\n    return clusters\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, pandas, sklearn.preprocessing\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\ndef encode_categorical_data_in_dataframe(df, column_name):\n    \"\"\"\n    Encode the categorical data in a specific column of a DataFrame using LabelEncoder.\n\n    Parameters:\n    df (DataFrame): The DataFrame that contains the data.\n    column_name (str): The name of the column to encode.\n\n    Returns:\n    DataFrame: The DataFrame with the encoded column.\n\n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.preprocessing\n\n    Example:\n    >>> df = pd.DataFrame({'fruit': ['apple', 'banana', 'cherry', 'apple', 'banana']})\n    >>> encode_categorical_data_in_dataframe(df, 'fruit')\n    \"\"\"\n    le = LabelEncoder()\n    df[column_name] = le.fit_transform(df[column_name])\n    return df\n```"}
{"seed": "{\"task_id\": 364621, \"prompt\": \"def f_364621(testlist):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"[i for (i, x) in enumerate(testlist) if (x == 1)]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    testlist = [1,2,3,5,3,1,2,1,6]\\n    assert candidate(testlist) == [0, 5, 7]\\n\", \"\\n    testlist = [0, -1]\\n    assert candidate(testlist) == []\\n\"], \"entry_point\": \"f_364621\", \"intent\": \"get the position of item 1 in `testlist`\", \"library\": []}", "generation": "Scenario 1: \nnumpy, pandas, matplotlib, random\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nITEMS = ['Apple', 'Banana', 'Orange', 'Pear', 'Pineapple']\n\ndef find_item_positions(dataframe, item):\n    \"\"\"\n    Find the positions of a given item in a pandas DataFrame and plot its \n    frequency distribution.\n\n    Parameters:\n    dataframe (DataFrame): The pandas DataFrame.\n    item (str): The item to find.\n\n    Returns:\n    list: A list of positions (row index, column name) where the item is found.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> df = pd.DataFrame([[ITEMS[randint(0, len(ITEMS)-1)] for _ in range(5)] for _ in range(1000)])\n    >>> positions = find_item_positions(df, 'Apple')\n    >>> print(positions)\n    >>> df.apply(pd.Series.value_counts).fillna(0).T.plot(kind='bar', stacked=True)\n    \"\"\"\n    positions = [(i, col) for i in dataframe.index for col in dataframe.columns if dataframe.at[i, col] == item]\n    return positions\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nitertools, numpy, pandas, matplotlib\n```python\nimport itertools\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nITEMS = ['Apple', 'Banana', 'Orange', 'Pear', 'Pineapple']\n\ndef find_item_combinations(dataframe, item):\n    \"\"\"\n    Find all combinations of a given item with other items in a pandas DataFrame \n    and plot the frequency of each combination.\n\n    Parameters:\n    dataframe (DataFrame): The pandas DataFrame.\n    item (str): The item to combine with.\n\n    Returns:\n    dict: A dictionary with combinations as keys and their frequencies as values.\n\n    Requirements:\n    - itertools\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame([[ITEMS[i] for i in np.random.choice(5, 2)] for _ in range(1000)])\n    >>> combinations = find_item_combinations(df, 'Apple')\n    >>> print(combinations)\n    >>> pd.Series(combinations).plot(kind='bar')\n    \"\"\"\n    combinations = {}\n\n    for i in dataframe.index:\n        row_items = set(dataframe.loc[i])\n        if item in row_items:\n            row_items.remove(item)\n            for other_item in row_items:\n                combination = tuple(sorted([item, other_item]))\n                combinations[combination] = combinations.get(combination, 0) + 1\n\n    return combinations\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnumpy, pandas, matplotlib, random\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nITEMS = ['Apple', 'Banana', 'Orange', 'Pear', 'Pineapple']\n\ndef generate_item_frequency(dataframe):\n    \"\"\"\n    Generate the frequency of each item in a pandas DataFrame and plot a bar \n    chart of the frequencies.\n\n    Parameters:\n    dataframe (DataFrame): The pandas DataFrame.\n\n    Returns:\n    Series: A pandas Series with items as index and frequencies as values.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> df = pd.DataFrame([[ITEMS[randint(0, len(ITEMS)-1)] for _ in range(5)] for _ in range(1000)])\n    >>> frequencies = generate_item_frequency(df)\n    >>> print(frequencies)\n    >>> frequencies.plot(kind='bar')\n    \"\"\"\n    frequencies = dataframe.apply(pd.value_counts).sum(axis=1)\n    return frequencies\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, pandas, matplotlib, random\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nITEMS = ['Apple', 'Banana', 'Orange', 'Pear', 'Pineapple']\n\ndef generate_item_pairs(dataframe):\n    \"\"\"\n    Generate all pairs of items in a pandas DataFrame and plot a scatter plot \n    of the pairs.\n\n    Parameters:\n    dataframe (DataFrame): The pandas DataFrame.\n\n    Returns:\n    DataFrame: A pandas DataFrame with pairs of items.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> df = pd.DataFrame([[ITEMS[randint(0, len(ITEMS)-1)] for _ in range(5)] for _ in range(1000)])\n    >>> pairs = generate_item_pairs(df)\n    >>> print(pairs)\n    >>> pairs.plot(kind='scatter', x='Item 1', y='Item 2')\n    \"\"\"\n    pairs = pd.DataFrame([(row[i], row[j]) for row in dataframe.values for i in range(len(row)) for j in range(i+1, len(row))], columns=['Item 1', 'Item 2'])\n    return pairs\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nitertools, numpy, pandas, matplotlib\n```python\nimport itertools\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nITEMS = ['Apple', 'Banana', 'Orange', 'Pear', 'Pineapple']\n\ndef generate_item_combinations(dataframe):\n    \"\"\"\n    Generate all combinations of items in a pandas DataFrame and plot a bar \n    chart of the combination frequencies.\n\n    Parameters:\n    dataframe (DataFrame): The pandas DataFrame.\n\n    Returns:\n    dict: A dictionary with combinations as keys and their frequencies as values.\n\n    Requirements:\n    - itertools\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame([[ITEMS[i] for i in np.random.choice(5, 2)] for _ in range(1000)])\n    >>> combinations = generate_item_combinations(df)\n    >>> print(combinations)\n    >>> pd.Series(combinations).plot(kind='bar')\n    \"\"\"\n    combinations = {}\n\n    for row in dataframe.values:\n        for combination in itertools.combinations(row, 2):\n            combination = tuple(sorted(combination))\n            combinations[combination] = combinations.get(combination, 0) + 1\n\n    return combinations\n```\n"}
{"seed": "{\"task_id\": 364621, \"prompt\": \"def f_364621(testlist, element):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"testlist.index(element)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    testlist = [1,2,3,5,3,1,2,1,6]\\n    assert candidate(testlist, 1) == 0\\n\", \"\\n    testlist = [1,2,3,5,3,1,2,1,6]\\n    try:\\n        candidate(testlist, 14)\\n    except:\\n        assert True\\n\"], \"entry_point\": \"f_364621\", \"intent\": \"get the position of item `element` in list `testlist`\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, random, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import choice\nimport matplotlib.pyplot as plt\n\n# Constants\nPRODUCTS = ['Apple', 'Banana', 'Grapes', 'Orange', 'Pineapple']\n\ndef generate_sales_report(product_list, product):\n    \"\"\"\n    Generate a sales report for a specific product from a given list of products \n    sold over a month (30 days).\n    \n    Parameters:\n    product_list (list): The list of products sold.\n    product (str): The specific product for which report needs to be generated.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with sales report for the product.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> product_sales = [choice(PRODUCTS) for _ in range(1000)]\n    >>> report = generate_sales_report(product_sales, 'Apple')\n    >>> print(report)\n    >>> report.plot(kind='bar')\n    \"\"\"\n    if product not in product_list:\n        raise ValueError(\"The product is not in the product list.\")\n        \n    days = np.arange(1, 31)\n    sales = [product_list.count(product) for _ in days]\n    \n    report_df = pd.DataFrame({'Day': days, 'Sales': sales})\n    \n    return report_df\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 2:\nitertools, numpy, random\n\n```python\nfrom itertools import combinations\nimport numpy as np\nfrom random import sample\n\n# Constants\nNUMBERS = np.arange(1, 101)\n\ndef find_combinations(number_list, element):\n    \"\"\"\n    Find all combinations of 3 numbers from a list that add up to a given element.\n\n    Parameters:\n    number_list (list): The list of numbers.\n    element (int): The number to which the combination of 3 numbers should add up.\n\n    Returns:\n    list: A list of tuples, each containing a combination of 3 numbers that add up to the element.\n\n    Requirements:\n    - itertools\n    - numpy\n    - random\n\n    Example:\n    >>> find_combinations(sample(list(NUMBERS), 50), 10)\n    \"\"\"\n    if element not in number_list:\n        raise ValueError(\"The element is not in the number list.\")\n        \n    combinations_list = list(combinations(number_list, 3))\n    valid_combinations = [comb for comb in combinations_list if sum(comb) == element]\n    \n    return valid_combinations\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, random, matplotlib\n\n```python\nimport numpy as np\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nRANGE_START = 1\nRANGE_END = 100\n\ndef plot_random_walk(element):\n    \"\"\"\n    Generate and plot a random walk of 'element' steps.\n\n    Parameters:\n    element (int): The number of steps in the random walk.\n\n    Returns:\n    None.\n\n    Requirements:\n    - numpy\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_random_walk(1000)\n    \"\"\"\n    if not isinstance(element, int) or element <= 0:\n        raise ValueError(\"Element must be a positive integer.\")\n        \n    steps = np.random.choice([-1, 1], size=element)\n    walk = np.cumsum(steps)\n    \n    plt.figure(figsize=(10, 6))\n    plt.plot(walk)\n    plt.title('Random Walk')\n    plt.show()\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 4: \npandas, numpy, random\n\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import choice\n\n# Constants\nSTUDENTS = range(1, 101)\n\ndef calculate_student_average(test_scores, student):\n    \"\"\"\n    Calculate the average test score for a specific student from a given DataFrame of test scores.\n    \n    Parameters:\n    test_scores (DataFrame): The DataFrame of test scores.\n    student (int): The specific student for which the average score needs to be calculated.\n    \n    Returns:\n    float: The average score for the student.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - random\n    \n    Example:\n    >>> scores = pd.DataFrame({'Student': sample(STUDENTS, 50), 'Score': np.random.randint(50, 101, size=50)})\n    >>> calc_student_average(scores, 10)\n    \"\"\"\n    if student not in test_scores['Student'].values:\n        raise ValueError(\"The student is not in the test scores list.\")\n        \n    student_scores = test_scores[test_scores['Student'] == student]['Score']\n    average_score = student_scores.mean()\n    \n    return average_score\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 5:\ncollections, random\n\n```python\nfrom collections import Counter\nfrom random import choice\n\n# Constants\nLETTERS = 'abcdefghijklmnopqrstuvwxyz'\n\ndef count_letter_frequencies(letter_list, element):\n    \"\"\"\n    Count the frequency of a specific letter in a given list of letters.\n\n    Parameters:\n    letter_list (list): The list of letters.\n    element (str): The specific letter for which the frequency needs to be counted.\n\n    Returns:\n    int: The frequency of the letter.\n\n    Requirements:\n    - collections\n    - random\n\n    Example:\n    >>> letter_list = [choice(LETTERS) for _ in range(1000)]\n    >>> count_letter_frequencies(letter_list, 'a')\n    \"\"\"\n    if element not in letter_list:\n        raise ValueError(\"The element is not in the letter list.\")\n        \n    letter_frequencies = Counter(letter_list)\n    element_frequency = letter_frequencies[element]\n    \n    return element_frequency\n```"}
{"seed": "{\"task_id\": 13145368, \"prompt\": \"def f_13145368(lis):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"max(lis, key=lambda item: item[1])[0]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    lis = [(101, 153), (255, 827), (361, 961)]\\n    assert candidate(lis) == 361\\n\"], \"entry_point\": \"f_13145368\", \"intent\": \"find the first element of the tuple with the maximum second element in a list of tuples `lis`\", \"library\": []}", "generation": "Scenario 1:\npandas, numpy, itertools, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom itertools import groupby\nimport matplotlib.pyplot as plt\n\n# Constants\nDATA = [(101, 153), (255, 827), (361, 961), (255, 1027), (361, 861), (101, 953)]\n\ndef plot_max_elements(data):\n    \"\"\"\n    Plot the maximum second elements of tuples for each group (by first element) \n    in a list of tuples `data`.\n\n    Parameters:\n    data (list): A list of tuples.\n\n    Returns:\n    None. The function plots a bar chart.\n\n    Requirements:\n    - pandas\n    - numpy\n    - itertools\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_max_elements(DATA)\n    \"\"\"\n    data.sort()\n    grouped_data = {key: max([item[1] for item in group]) for key, group in groupby(data, lambda x: x[0])}\n    \n    plt.bar(grouped_data.keys(), grouped_data.values())\n    plt.show()\n\n    return None\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, pandas, matplotlib, seaborn\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nDATA = [(101, 153), (255, 827), (361, 961), (255, 1027), (361, 861), (101, 953)]\n\ndef plot_distribution_of_max_elements(data):\n    \"\"\"\n    Plot the distribution of maximum second elements of tuples for each group \n    (by first element) in a list of tuples `data`.\n\n    Parameters:\n    data (list): A list of tuples.\n\n    Returns:\n    None. The function plots a distribution plot.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> plot_distribution_of_max_elements(DATA)\n    \"\"\"\n    df = pd.DataFrame(data, columns=['Group', 'Value'])\n    max_values = df.groupby('Group')['Value'].max()\n\n    sns.distplot(max_values)\n    plt.show()\n\n    return None\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, matplotlib, scipy\n```python\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\n# Constants\nDATA = [(101, 153), (255, 827), (361, 961), (255, 1027), (361, 861), (101, 953)]\n\ndef plot_regression_of_max_elements(data):\n    \"\"\"\n    Plot a linear regression of the maximum second elements of tuples for each \n    group (by first element) in a list of tuples `data`.\n\n    Parameters:\n    data (list): A list of tuples.\n\n    Returns:\n    None. The function plots a linear regression plot.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - scipy\n\n    Example:\n    >>> plot_regression_of_max_elements(DATA)\n    \"\"\"\n    df = pd.DataFrame(data, columns=['Group', 'Value'])\n    max_values = df.groupby('Group')['Value'].max().reset_index()\n\n    slope, intercept, r_value, p_value, std_err = stats.linregress(max_values['Group'], max_values['Value'])\n\n    plt.scatter(max_values['Group'], max_values['Value'])\n    plt.plot(max_values['Group'], intercept + slope * max_values['Group'], 'r')\n    plt.show()\n\n    return None\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, pandas, matplotlib, scipy\n```python\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\n# Constants\nDATA = [(101, 153), (255, 827), (361, 961), (255, 1027), (361, 861), (101, 953)]\n\ndef plot_correlation_of_max_elements(data):\n    \"\"\"\n    Plot a correlation matrix of the maximum second elements of tuples for each \n    group (by first element) in a list of tuples `data`.\n\n    Parameters:\n    data (list): A list of tuples.\n\n    Returns:\n    None. The function plots a correlation matrix.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - scipy\n\n    Example:\n    >>> plot_correlation_of_max_elements(DATA)\n    \"\"\"\n    df = pd.DataFrame(data, columns=['Group', 'Value'])\n    max_values = df.groupby('Group')['Value'].max().reset_index()\n\n    corr_matrix = max_values.corr()\n\n    plt.matshow(corr_matrix)\n    plt.colorbar()\n    plt.show()\n\n    return None\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, matplotlib, seaborn\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nDATA = [(101, 153), (255, 827), (361, 961), (255, 1027), (361, 861), (101, 953)]\n\ndef plot_heatmap_of_max_elements(data):\n    \"\"\"\n    Plot a heatmap of the maximum second elements of tuples for each \n    group (by first element) in a list of tuples `data`.\n\n    Parameters:\n    data (list): A list of tuples.\n\n    Returns:\n    None. The function plots a heatmap.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> plot_heatmap_of_max_elements(DATA)\n    \"\"\"\n    df = pd.DataFrame(data, columns=['Group', 'Value'])\n    max_values = df.groupby('Group')['Value'].max().reset_index()\n\n    pivot_table = pd.pivot_table(max_values, values='Value', index=['Group'])\n\n    sns.heatmap(pivot_table, cmap='coolwarm')\n    plt.show()\n\n    return None\n```\n"}
{"seed": "{\"task_id\": 13145368, \"prompt\": \"def f_13145368(lis):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"max(lis, key=itemgetter(1))[0]\", \"test_start\": \"\\nfrom operator import itemgetter \\n\\ndef check(candidate):\", \"test\": [\"\\n    lis = [(101, 153), (255, 827), (361, 961)]\\n    assert candidate(lis) == 361\\n\"], \"entry_point\": \"f_13145368\", \"intent\": \"get the item at index 0 from the tuple that has maximum value at index 1 in list `lis`\", \"library\": [\"operator\"]}", "generation": "Scenario 1: \npandas, numpy, matplotlib, operator\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom operator import itemgetter\n\n# Constants\nCOLUMNS = ['Product ID', 'Sales', 'Profit']\n\n# Sample data\nDATA = [(101, 153, 50), (255, 827, 200), (361, 961, 300), (420, 123, 30), (550, 789, 250)]\n\ndef plot_max_profit_product(data):\n    \"\"\"\n    Visualize the product which generates the maximum profit and its corresponding sales.\n    \n    Parameters:\n    data (list of tuples): The data with each tuple containing product ID, sales and profit.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - operator\n\n    Example:\n    >>> plot_max_profit_product(DATA)\n    \"\"\"\n    df = pd.DataFrame(data, columns=COLUMNS)\n    max_profit_product = max(data, key=itemgetter(2))[0]\n    max_profit_product_data = df[df['Product ID'] == max_profit_product]\n\n    plt.bar(max_profit_product_data['Product ID'], max_profit_product_data['Sales'], label='Sales')\n    plt.plot(max_profit_product_data['Product ID'], max_profit_product_data['Profit'], color='red', label='Profit')\n    plt.xlabel('Product ID')\n    plt.ylabel('Amount')\n    plt.title(f'Product {max_profit_product} Sales and Profit')\n    plt.legend()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nnumpy, operator, matplotlib, random\n```python\nimport numpy as np\nfrom operator import itemgetter\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nRANGE_START = 1\nRANGE_END = 100\nSIZE = 50\n\n# Generating a list of tuples\nDATA = [(randint(RANGE_START, RANGE_END), randint(RANGE_START, RANGE_END)) for _ in range(SIZE)]\n\ndef plot_max_tuple(data):\n    \"\"\"\n    Plot the tuple with the maximum value at index 1 from a list of tuples.\n    \n    Parameters:\n    data (list of tuples): The data with each tuple containing two integers.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - numpy\n    - operator\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> plot_max_tuple(DATA)\n    \"\"\"\n    max_tuple = max(data, key=itemgetter(1))\n    tuples = np.array(data)\n    x = tuples[:,0]\n    y = tuples[:,1]\n\n    plt.scatter(x, y, label='Data')\n    plt.scatter(*max_tuple, color='red', label='Max Tuple')\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.title('Max Tuple Highlighted')\n    plt.legend()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ncollections, operator, matplotlib, random\n```python\nimport collections\nfrom operator import itemgetter\nimport matplotlib.pyplot as plt\nfrom random import choice\n\n# Constants\nLETTERS = 'abcdefghijklmnopqrstuvwxyz'\nSIZE = 1000\n\n# Generating a list of tuples\nDATA = [(choice(LETTERS), randint(1, 100)) for _ in range(SIZE)]\n\ndef plot_most_common_letter(data):\n    \"\"\"\n    Plot the frequency of letters and highlight the letter with the maximum value at index 1.\n    \n    Parameters:\n    data (list of tuples): The data with each tuple containing a letter and an integer.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - collections\n    - operator\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> plot_most_common_letter(DATA)\n    \"\"\"\n    letter_counts = collections.Counter([item[0] for item in data])\n    max_value_letter = max(data, key=itemgetter(1))[0]\n\n    letters, counts = zip(*letter_counts.items())\n    plt.bar(letters, counts, label='Letter Counts')\n\n    if max_value_letter in letter_counts:\n        plt.bar(max_value_letter, letter_counts[max_value_letter], color='red', label='Max Value Letter')\n\n    plt.xlabel('Letter')\n    plt.ylabel('Count')\n    plt.title('Letter Counts with Max Value Letter Highlighted')\n    plt.legend()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, operator, matplotlib, random\n```python\nimport numpy as np\nfrom operator import itemgetter\nimport matplotlib.pyplot as plt\nfrom random import uniform\n\n# Constants\nSIZE = 1000\n\n# Generating a list of tuples\nDATA = [(uniform(0, 1), uniform(0, 1)) for _ in range(SIZE)]\n\ndef plot_point_with_max_y(data):\n    \"\"\"\n    Plot a scatter plot of points and highlight the point with the maximum y-value.\n    \n    Parameters:\n    data (list of tuples): The data with each tuple containing two floats.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - numpy\n    - operator\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> plot_point_with_max_y(DATA)\n    \"\"\"\n    max_y_point = max(data, key=itemgetter(1))\n    points = np.array(data)\n    x = points[:,0]\n    y = points[:,1]\n\n    plt.scatter(x, y, label='Points')\n    plt.scatter(*max_y_point, color='red', label='Max Y Point')\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.title('Points with Max Y Point Highlighted')\n    plt.legend()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, operator, matplotlib, random\n```python\nimport pandas as pd\nfrom operator import itemgetter\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nCATEGORIES = ['A', 'B', 'C', 'D', 'E']\nSIZE = 100\n\n# Generating a list of tuples\nDATA = [(choice(CATEGORIES), randint(1, 100)) for _ in range(SIZE)]\n\ndef plot_category_with_max_value(data):\n    \"\"\"\n    Plot a bar chart of category counts and highlight the category with the maximum value at index 1.\n    \n    Parameters:\n    data (list of tuples): The data with each tuple containing a category and an integer.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - pandas\n    - operator\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> plot_category_with_max_value(DATA)\n    \"\"\"\n    df = pd.DataFrame(data, columns=['Category', 'Value'])\n    category_counts = df['Category'].value_counts()\n    max_value_category = max(data, key=itemgetter(1))[0]\n\n    categories = category_counts.index\n    counts = category_counts.values\n    colors = ['red' if category == max_value_category else 'blue' for category in categories]\n\n    plt.bar(categories, counts, color=colors, label='Category Counts')\n    plt.xlabel('Category')\n    plt.ylabel('Count')\n    plt.title('Category Counts with Max Value Category Highlighted')\n    plt.legend()\n    plt.show()\n```\n"}
{"seed": "{\"task_id\": 2689189, \"prompt\": \"def f_2689189():\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"time.sleep(1)\", \"test_start\": \"\\nimport time\\n\\ndef check(candidate):\", \"test\": [\"\\n    t1 = time.time()\\n    candidate()\\n    t2 = time.time()\\n    assert t2 - t1 > 1\\n\"], \"entry_point\": \"f_2689189\", \"intent\": \"Make a delay of 1 second\", \"library\": [\"time\"]}", "generation": "Scenario 1:\ntime, numpy, pandas, matplotlib\n```python\nimport time\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nMEASUREMENTS = 10\n\ndef calculate_time_delays():\n    \"\"\"\n    Make a delay of 1 second for a specified number of times, calculate the actual \n    delay for each iteration, store it in a pandas DataFrame, and plot the delays.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the delay times.\n\n    Requirements:\n    - time\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> delays = calculate_time_delays()\n    >>> print(delays)\n    >>> delays.plot(kind='bar')\n    \"\"\"\n    delay_times = []\n\n    for _ in range(MEASUREMENTS):\n        t1 = time.time()\n        time.sleep(1)\n        t2 = time.time()\n        delay_times.append(t2 - t1)\n\n    delay_df = pd.DataFrame(delay_times, columns=['Delay Time'])\n\n    return delay_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ntime, os, sys\n```python\nimport time\nimport os\nimport sys\n\n# Constants\nDELAY = 1\n\ndef delay_print_stdout():\n    \"\"\"\n    Make a delay of 1 second, then print a message to stdout. Repeat this process \n    until the script is interrupted with a KeyboardInterrupt.\n\n    Requirements:\n    - time\n    - os\n    - sys\n\n    Example:\n    >>> delay_print_stdout()\n    \"\"\"\n    try:\n        while True:\n            time.sleep(DELAY)\n            sys.stdout.write('1 second has passed\\n')\n            sys.stdout.flush()\n    except KeyboardInterrupt:\n        sys.stdout.write('Script interrupted\\n')\n        sys.stdout.flush()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ntime, random, matplotlib\n```python\nimport time\nimport random\nimport matplotlib.pyplot as plt\n\n# Constants\nINTERVALS = 100\n\ndef delay_and_plot():\n    \"\"\"\n    Make a delay of 1 second, generate a random number, and plot the numbers as a \n    function of time.\n\n    Requirements:\n    - time\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> delay_and_plot()\n    \"\"\"\n    times = []\n    numbers = []\n\n    try:\n        for _ in range(INTERVALS):\n            time.sleep(1)\n            times.append(time.time())\n            numbers.append(random.random())\n    except KeyboardInterrupt:\n        print('Interrupted by user')\n\n    plt.plot(times, numbers)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ntime, threading\n```python\nimport time\nimport threading\n\n# Constants\nTHREADS = 5\n\ndef delay_in_threads():\n    \"\"\"\n    Make a delay of 1 second in a number of separate threads.\n\n    Requirements:\n    - time\n    - threading\n\n    Example:\n    >>> delay_in_threads()\n    \"\"\"\n    def delay():\n        time.sleep(1)\n        print(f'Delay in thread {threading.current_thread().name} completed')\n\n    for i in range(THREADS):\n        t = threading.Thread(target=delay, name=str(i))\n        t.start()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\ntime, numpy, scipy.stats\n```python\nimport time\nimport numpy as np\nimport scipy.stats as stats\n\n# Constants\nSAMPLES = 1000\n\ndef delay_and_stats():\n    \"\"\"\n    Make a delay of 1 second, measure the actual delay, and calculate the statistical \n    properties of the delay times.\n\n    Returns:\n    tuple: The mean and standard deviation of the delay times.\n\n    Requirements:\n    - time\n    - numpy\n    - scipy.stats\n\n    Example:\n    >>> mean, std = delay_and_stats()\n    >>> print(f'Mean: {mean}, Std: {std}')\n    \"\"\"\n    delay_times = []\n\n    for _ in range(SAMPLES):\n        t1 = time.time()\n        time.sleep(1)\n        t2 = time.time()\n        delay_times.append(t2 - t1)\n\n    delay_times = np.array(delay_times)\n\n    mean = np.mean(delay_times)\n    std = np.std(delay_times)\n\n    return mean, std\n```"}
{"seed": "{\"task_id\": 12485244, \"prompt\": \"def f_12485244(L):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"\\\"\\\"\\\", \\\"\\\"\\\".join('(' + ', '.join(i) + ')' for i in L)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    L = [(\\\"abc\\\", \\\"def\\\"), (\\\"hij\\\", \\\"klm\\\")]\\n    assert candidate(L) == '(abc, def), (hij, klm)'\\n\"], \"entry_point\": \"f_12485244\", \"intent\": \"convert list of tuples `L` to a string\", \"library\": []}", "generation": "Scenario 1:\npandas, os, json, collections\n```python\nimport pandas as pd\nimport os\nimport json\nfrom collections import Counter\n\n# Constants\njson_dir_path = './json_files/'\n\ndef get_most_common_words(json_dir_path, word_count):\n    \"\"\"\n    Load JSON files from a directory, parse the text content, and find the most common words.\n\n    Parameters:\n    json_dir_path (str): The directory path where JSON files are stored.\n    word_count (int): The number of most common words to return.\n\n    Returns:\n    list: A list of tuples with the most common words and their counts.\n\n    Requirements:\n    - pandas\n    - os\n    - json\n    - collections.Counter\n\n    Example:\n    >>> get_most_common_words('./json_files/', 10)\n    \"\"\"\n    word_counter = Counter()\n    \n    for filename in os.listdir(json_dir_path):\n        if filename.endswith('.json'):\n            with open(os.path.join(json_dir_path, filename), 'r') as f:\n                data = json.load(f)\n                text = data.get('text', '')\n                words = pd.Series(text.split())\n                word_counter += Counter(words)\n                \n    return word_counter.most_common(word_count)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, itertools, string\n```python\nimport numpy as np\nfrom itertools import product\nimport string\n\n# Constants\nALPHABETS = list(string.ascii_lowercase)\n\ndef generate_strings(length, seed):\n    \"\"\"\n    Generate a list of all possible strings of a given length from a set of characters (lowercase english alphabets)\n    using a specific seed for reproducibility.\n\n    Parameters:\n    length (int): The length of the strings to generate.\n    seed (int): The seed for the random number generator.\n\n    Returns:\n    list: A list of generated strings.\n\n    Requirements:\n    - numpy\n    - itertools.product\n    - string\n\n    Example:\n    >>> generate_strings(2, 123)\n    \"\"\"\n    np.random.seed(seed)\n    all_combinations = [''.join(p) for p in product(ALPHABETS, repeat=length)]\n    return np.random.choice(all_combinations, size=10).tolist()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nre, nltk, collections\n```python\nimport re\nfrom nltk.corpus import stopwords\nfrom collections import Counter\n\n# Constants\nSTOPWORDS = set(stopwords.words('english'))\n\ndef count_non_stopwords(text):\n    \"\"\"\n    Count the non-stopwords in a given text.\n\n    Parameters:\n    text (str): The text to analyze.\n\n    Returns:\n    dict: A dictionary with non-stopwords and their counts.\n\n    Requirements:\n    - re\n    - nltk.corpus.stopwords\n    - collections.Counter\n\n    Example:\n    >>> count_non_stopwords('This is a sample text for testing.')\n    \"\"\"\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    non_stopwords = [word for word in words if word not in STOPWORDS]\n    return dict(Counter(non_stopwords))\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, scipy.stats, matplotlib\n```python\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\n# Constants\nSAMPLE_SIZE = 1000\nMU = 0\nSIGMA = 1\n\ndef plot_normal_distribution(mu=MU, sigma=SIGMA, sample_size=SAMPLE_SIZE):\n    \"\"\"\n    Generate a sample from a normal distribution with a given mean and standard deviation, \n    and plot the histogram along with the probability density function.\n\n    Parameters:\n    mu (float): The mean of the normal distribution.\n    sigma (float): The standard deviation of the normal distribution.\n    sample_size (int): The size of the sample to generate.\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_normal_distribution(0, 1, 1000)\n    \"\"\"\n    sample = np.random.normal(mu, sigma, sample_size)\n    \n    plt.hist(sample, bins=30, density=True, alpha=0.5, label='Sample Histogram')\n    \n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, mu, sigma)\n    plt.plot(x, p, 'k', linewidth=2, label='Normal PDF')\n    \n    plt.title(\"Normal Distribution with $\\mu = %0.2f, \\sigma = %0.2f$\" % (mu, sigma))\n    plt.legend()\n    \n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\ndatetime, pytz, pandas, matplotlib\n```python\nimport pandas as pd\nfrom datetime import datetime\nimport pytz\nimport matplotlib.pyplot as plt\n\n# Constants\nDATE_RANGE_START = '2020-01-01'\nDATE_RANGE_END = '2020-12-31'\nFREQ = 'H'\nTIMEZONES = ['UTC', 'Europe/London', 'America/New_York']\n\ndef plot_timezone_difference(start=DATE_RANGE_START, end=DATE_RANGE_END, freq=FREQ, timezones=TIMEZONES):\n    \"\"\"\n    Generate a plot showing the difference in hours between different timezones and UTC over a date range.\n\n    Parameters:\n    start (str): The start of the date range in 'YYYY-MM-DD' format.\n    end (str): The end of the date range in 'YYYY-MM-DD' format.\n    freq (str): The frequency of the date range.\n    timezones (list): A list of timezones to include in the plot.\n\n    Requirements:\n    - pandas\n    - datetime\n    - pytz\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_timezone_difference('2020-01-01', '2020-12-31', 'H', ['UTC', 'Europe/London', 'America/New_York'])\n    \"\"\"\n    date_range = pd.date_range(start=start, end=end, freq=freq)\n    df = pd.DataFrame(date_range, columns=['UTC'])\n\n    for tz in timezones:\n        df[tz] = df['UTC'].dt.tz_convert(tz).dt.hour - df['UTC'].dt.hour\n\n    df.plot(x='UTC', y=timezones, title='Timezone Difference from UTC')\n    plt.ylabel('Hours')\n    plt.show()\n```"}
{"seed": "{\"task_id\": 755857, \"prompt\": \"def f_755857():\\n\\t\", \"suffix\": \"\\n\\treturn b\", \"canonical_solution\": \"b = models.CharField(max_length=7, default='0000000', editable=False)\", \"test_start\": \"\\nfrom django.db import models\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate().get_default() == '0000000'\\n\"], \"entry_point\": \"f_755857\", \"intent\": \"Django set default value of field `b` equal to '0000000'\", \"library\": [\"django\"]}", "generation": "Scenario 3:\nDjango, hashlib, random, string\n```python\nfrom django.db import models\nimport hashlib\nimport random\nimport string\n\n# Constants\nDEFAULT_LENGTH = 10\nSALT = \"django_example\"\n\nclass User(models.Model):\n    \"\"\"\n    A user model with a secure default password.\n\n    Requirements:\n    - django.db.models\n    - hashlib\n    - random\n    - string\n\n    Example:\n    >>> user = User()\n    >>> user.password\n    \"\"\"\n    def create_default_password():\n        \"\"\"\n        Create a secure default password.\n        \"\"\"\n        raw_password = ''.join([random.choice(string.ascii_letters + string.digits) for _ in range(DEFAULT_LENGTH)])\n        hashed_password = hashlib.sha256((SALT + raw_password).encode('utf-8')).hexdigest()\n\n        return hashed_password\n\n    username = models.CharField(max_length=50)\n    password = models.CharField(max_length=64, default=create_default_password, editable=False)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nDjango, random, string, datetime\n```python\nfrom django.db import models\nimport random\nimport string\nfrom datetime import datetime, timedelta\n\nclass Document(models.Model):\n    \"\"\"\n    A document model with a default expiry date.\n\n    Requirements:\n    - django.db.models\n    - random\n    - string\n    - datetime\n\n    Example:\n    >>> doc = Document()\n    >>> doc.expiry_date\n    \"\"\"\n    def create_default_expiry_date():\n        \"\"\"\n        Create a default expiry date 30 days from now.\n        \"\"\"\n        return datetime.now() + timedelta(days=30)\n\n    title = models.CharField(max_length=100)\n    content = models.TextField()\n    expiry_date = models.DateTimeField(default=create_default_expiry_date)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nDjango, numpy, random\n```python\nfrom django.db import models\nimport numpy as np\nimport random\n\n# Constants\nRATING_CHOICES = np.arange(1, 6)\n\nclass Product(models.Model):\n    \"\"\"\n    A product model with a default random rating.\n\n    Requirements:\n    - django.db.models\n    - numpy\n    - random\n\n    Example:\n    >>> product = Product()\n    >>> product.rating\n    \"\"\"\n    def create_default_rating():\n        \"\"\"\n        Create a default random rating between 1 and 5.\n        \"\"\"\n        return random.choice(RATING_CHOICES)\n\n    name = models.CharField(max_length=100)\n    description = models.TextField()\n    price = models.DecimalField(max_digits=5, decimal_places=2)\n    rating = models.IntegerField(default=create_default_rating)\n```\n`GPT_ODEX_BREAK`\n\nScenario 6:\nDjango, datetime, pytz\n```python\nfrom django.db import models\nfrom datetime import datetime\nimport pytz\n\nclass Appointment(models.Model):\n    \"\"\"\n    An appointment model with a default date and time in UTC.\n\n    Requirements:\n    - django.db.models\n    - datetime\n    - pytz\n\n    Example:\n    >>> appointment = Appointment()\n    >>> appointment.date_time\n    \"\"\"\n    def create_default_date_time():\n        \"\"\"\n        Create a default date and time in UTC.\n        \"\"\"\n        return datetime.now(pytz.UTC)\n\n    patient = models.CharField(max_length=100)\n    doctor = models.CharField(max_length=100)\n    date_time = models.DateTimeField(default=create_default_date_time)\n```\n`GPT_ODEX_BREAK`\n\nScenario 7:\nDjango, random, hashlib, base64\n```python\nfrom django.db import models\nimport random\nimport hashlib\nimport base64\n\n# Constants\nDEFAULT_LENGTH = 10\n\nclass APIKey(models.Model):\n    \"\"\"\n    An API key model with a default random key.\n\n    Requirements:\n    - django.db.models\n    - random\n    - hashlib\n    - base64\n\n    Example:\n    >>> api_key = APIKey()\n    >>> api_key.key\n    \"\"\"\n    def create_default_key():\n        \"\"\"\n        Create a default random key.\n        \"\"\"\n        raw_key = ''.join([chr(random.randint(0, 255)) for _ in range(DEFAULT_LENGTH)])\n        hashed_key = hashlib.sha256(raw_key.encode('utf-8')).digest()\n        key = base64.b64encode(hashed_key).decode('utf-8')\n\n        return key\n\n    user = models.CharField(max_length=50)\n    key = models.CharField(max_length=44, default=create_default_key, editable=False)\n```"}
{"seed": "{\"task_id\": 16193578, \"prompt\": \"def f_16193578(list5):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"sorted(list5, key = lambda x: (degrees(x), x))\", \"test_start\": \"\\nfrom math import degrees\\n\\ndef check(candidate):\", \"test\": [\"\\n    list5 = [4, 1, 2, 3, 9, 5]\\n    assert candidate(list5) == [1, 2, 3, 4, 5, 9]\\n\"], \"entry_point\": \"f_16193578\", \"intent\": \"Sort lis `list5` in ascending order based on the degrees value of its elements\", \"library\": [\"math\"]}", "generation": "Scenario 1: \nmath, pandas, numpy, matplotlib\n```python\nimport math\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nELEMENTS = ['Hydrogen', 'Helium', 'Lithium', 'Beryllium', 'Boron', \n            'Carbon', 'Nitrogen', 'Oxygen', 'Fluorine', 'Neon']\n\ndef sort_and_plot_elements_based_on_degrees(list5):\n    \"\"\"\n    Sort the given list in ascending order based on the degrees value of its elements, \n    create a pandas DataFrame for the sorted list and the corresponding elements, \n    and plot a bar graph of the DataFrame.\n\n    Parameters:\n    list5 (list): The list to be sorted.\n\n    Returns:\n    DataFrame: A pandas DataFrame with sorted values and corresponding elements.\n\n    Requirements:\n    - math\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> list5 = [4, 1, 2, 3, 9, 5]\n    >>> df = sort_and_plot_elements_based_on_degrees(list5)\n    >>> print(df)\n    >>> df.plot(x='Elements', y='Degrees', kind='bar')\n    \"\"\"\n    sorted_list = sorted(list5, key = lambda x: (math.degrees(x), x))\n    degrees = [math.degrees(i) for i in sorted_list]\n    elements = [ELEMENTS[i-1] for i in sorted_list]\n    df = pd.DataFrame({'Degrees': degrees, 'Elements': elements})\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nmath, random, sympy, numpy\n```python\nimport math\nimport random\nfrom sympy import isprime\nimport numpy as np\n\n# Constants\nNUM_RANGE = range(1, 101)\n\ndef filter_prime_and_sort_by_degrees(list5):\n    \"\"\"\n    Filter the prime numbers from the given list, sort the prime numbers in ascending \n    order based on their degrees value, and return the sorted list.\n\n    Parameters:\n    list5 (list): The list to be filtered and sorted.\n\n    Returns:\n    list: The sorted list of prime numbers.\n\n    Requirements:\n    - math\n    - random\n    - sympy\n    - numpy\n\n    Example:\n    >>> list5 = random.sample(NUM_RANGE, 50)\n    >>> sorted_primes = filter_prime_and_sort_by_degrees(list5)\n    >>> print(sorted_primes)\n    \"\"\"\n    primes = [i for i in list5 if isprime(i)]\n    sorted_primes = sorted(primes, key = lambda x: (math.degrees(x), x))\n    return sorted_primes\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nmath, pandas, numpy, seaborn\n```python\nimport math\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n\n# Constants\nELEMENTS = ['Hydrogen', 'Helium', 'Lithium', 'Beryllium', 'Boron', \n            'Carbon', 'Nitrogen', 'Oxygen', 'Fluorine', 'Neon']\n\ndef sort_elements_and_plot_distribution(list5):\n    \"\"\"\n    Sort the given list in ascending order based on the degrees value of its elements, \n    create a pandas DataFrame for the sorted list and the corresponding elements, \n    and plot a distribution plot of the degrees values.\n\n    Parameters:\n    list5 (list): The list to be sorted.\n\n    Returns:\n    DataFrame: A pandas DataFrame with sorted values and corresponding elements.\n\n    Requirements:\n    - math\n    - pandas\n    - numpy\n    - seaborn\n\n    Example:\n    >>> list5 = [4, 1, 2, 3, 9, 5]\n    >>> df = sort_elements_and_plot_distribution(list5)\n    >>> print(df)\n    >>> sns.distplot(df['Degrees'])\n    \"\"\"\n    sorted_list = sorted(list5, key = lambda x: (math.degrees(x), x))\n    degrees = [math.degrees(i) for i in sorted_list]\n    elements = [ELEMENTS[i-1] for i in sorted_list]\n    df = pd.DataFrame({'Degrees': degrees, 'Elements': elements})\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nmath, random, statistics, matplotlib\n```python\nimport math\nimport random\nimport statistics\nimport matplotlib.pyplot as plt\n\n# Constants\nNUM_RANGE = range(1, 101)\n\ndef sort_and_visualize_statistics(list5):\n    \"\"\"\n    Sort the given list in ascending order based on the degrees value of its elements, \n    calculate the mean, median, and mode of the sorted list, and plot a histogram \n    of the sorted list.\n\n    Parameters:\n    list5 (list): The list to be sorted.\n\n    Returns:\n    tuple: A tuple with the mean, median, and mode of the sorted list.\n\n    Requirements:\n    - math\n    - random\n    - statistics\n    - matplotlib.pyplot\n\n    Example:\n    >>> list5 = random.sample(NUM_RANGE, 50)\n    >>> stats = sort_and_visualize_statistics(list5)\n    >>> print(stats)\n    >>> plt.hist(list5, bins=10)\n    \"\"\"\n    sorted_list = sorted(list5, key = lambda x: (math.degrees(x), x))\n    mean = statistics.mean(sorted_list)\n    median = statistics.median(sorted_list)\n    mode = statistics.mode(sorted_list)\n    return (mean, median, mode)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nmath, random, numpy, matplotlib\n```python\nimport math\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nNUM_RANGE = range(1, 101)\n\ndef sort_and_calculate_cumulative_sum(list5):\n    \"\"\"\n    Sort the given list in ascending order based on the degrees value of its elements, \n    calculate the cumulative sum of the sorted list, and plot a line graph of the \n    cumulative sum.\n\n    Parameters:\n    list5 (list): The list to be sorted.\n\n    Returns:\n    numpy array: The cumulative sum of the sorted list.\n\n    Requirements:\n    - math\n    - random\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> list5 = random.sample(NUM_RANGE, 50)\n    >>> cumsum = sort_and_calculate_cumulative_sum(list5)\n    >>> print(cumsum)\n    >>> plt.plot(cumsum)\n    \"\"\"\n    sorted_list = sorted(list5, key = lambda x: (math.degrees(x), x))\n    cumsum = np.cumsum(sorted_list)\n    return cumsum\n```"}
{"seed": "{\"task_id\": 16041405, \"prompt\": \"def f_16041405(l):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"(n for n in l)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    generator = candidate([1,2,3,5])\\n    assert str(type(generator)) == \\\"<class 'generator'>\\\"\\n    assert [x for x in generator] == [1, 2, 3, 5]\\n\"], \"entry_point\": \"f_16041405\", \"intent\": \"convert a list `l` into a generator object\", \"library\": []}", "generation": "Scenario 1: \npandas, itertools, statistics, collections\n```python\nimport pandas as pd\nimport itertools\nfrom statistics import mean\nfrom collections import Counter\n\n# Constants\nDATA = {'Name': ['Tom', 'Nick', 'John', 'Tom', 'John'],\n        'Age': [20, 21, 19, 20, 18],\n        'Score': [85, 79, 90, 88, 82]}\n\ndf = pd.DataFrame(DATA)\n\ndef analyze_dataframe(df):\n    \"\"\"\n    Convert a pandas DataFrame into a dictionary of generator objects, \n    where each generator produces a sequence of tuples containing a \n    unique name and the corresponding average score for that name.\n\n    Parameters:\n    df (DataFrame): The DataFrame to analyze.\n\n    Returns:\n    dict: A dictionary of generator objects.\n\n    Requirements:\n    - pandas\n    - itertools\n    - statistics\n    - collections\n\n    Example:\n    >>> analyze_dataframe(df)\n    \"\"\"\n    grouped = df.groupby('Name')\n    result_dict = {}\n    for name, group in grouped:\n        result_dict[name] = ((name, mean(score)) for score in group['Score'])\n\n    return result_dict\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nrandom, itertools, math\n```python\nimport random\nimport itertools\nimport math\n\n# Constants\nRANGE_START = 1\nRANGE_END = 100\nPAIRS_COUNT = 10\n\ndef generate_random_pairs():\n    \"\"\"\n    Generate a generator object that produces a sequence of tuples. \n    Each tuple contains two random numbers and their Euclidean distance.\n\n    Returns:\n    generator: A generator object.\n\n    Requirements:\n    - random\n    - itertools\n    - math\n\n    Example:\n    >>> pairs = generate_random_pairs()\n    >>> print(next(pairs))  # prints a tuple like (23, 89, 66.0)\n    \"\"\"\n    pairs = ((random.randint(RANGE_START, RANGE_END), random.randint(RANGE_START, RANGE_END)) for _ in range(PAIRS_COUNT))\n    return ((x, y, math.sqrt((x - y) ** 2)) for x, y in pairs)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, itertools, matplotlib.pyplot\n```python\nimport numpy as np\nimport itertools\nimport matplotlib.pyplot as plt\n\n# Constants\nRANGE_START = -10\nRANGE_END = 10\nSTEP = 0.1\n\ndef plot_sine_cosine():\n    \"\"\"\n    Generate a generator object that produces a sequence of tuples. \n    Each tuple contains x, sin(x), and cos(x) values. \n    Plot the sine and cosine functions using these values.\n\n    Returns:\n    generator: A generator object.\n\n    Requirements:\n    - numpy\n    - itertools\n    - matplotlib.pyplot\n\n    Example:\n    >>> data = plot_sine_cosine()\n    >>> print(next(data))  # prints a tuple like (-10.0, -0.5440211108893698, -0.8390715290764524)\n    \"\"\"\n    x_values = np.arange(RANGE_START, RANGE_END, STEP)\n    data = ((x, np.sin(x), np.cos(x)) for x in x_values)\n    for x, sin_x, cos_x in data:\n        plt.scatter(x, sin_x, color='b')\n        plt.scatter(x, cos_x, color='r')\n    plt.show()\n    return data\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nrandom, itertools, collections, statistics\n```python\nimport random\nimport itertools\nfrom collections import Counter\nfrom statistics import mode\n\n# Constants\nLIST_LENGTH = 1000\nRANGE_START = 1\nRANGE_END = 10\n\ndef analyze_random_list():\n    \"\"\"\n    Generate a random list of integers. Convert this list into a generator object\n    that produces a sequence of tuples, where each tuple contains a number from the \n    list and its frequency in the list. Find and print the mode of the list.\n\n    Returns:\n    generator: A generator object.\n\n    Requirements:\n    - random\n    - itertools\n    - collections\n    - statistics\n\n    Example:\n    >>> numbers = analyze_random_list()\n    >>> print(next(numbers))  # prints a tuple like (1, 100)\n    \"\"\"\n    random_list = [random.randint(RANGE_START, RANGE_END) for _ in range(LIST_LENGTH)]\n    counter = Counter(random_list)\n    numbers = ((number, count) for number, count in counter.items())\n    print('Mode:', mode(random_list))\n    return numbers\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, itertools, math, matplotlib.pyplot\n```python\nimport numpy as np\nimport itertools\nimport math\nimport matplotlib.pyplot as plt\n\n# Constants\nRANGE_START = 0\nRANGE_END = 10\nSTEP = 0.1\n\ndef plot_exponential_function():\n    \"\"\"\n    Generate a generator object that produces a sequence of tuples. \n    Each tuple contains x and e^x values. Plot the exponential function using these values.\n\n    Returns:\n    generator: A generator object.\n\n    Requirements:\n    - numpy\n    - itertools\n    - math\n    - matplotlib.pyplot\n\n    Example:\n    >>> data = plot_exponential_function()\n    >>> print(next(data))  # prints a tuple like (0.0, 1.0)\n    \"\"\"\n    x_values = np.arange(RANGE_START, RANGE_END, STEP)\n    data = ((x, math.exp(x)) for x in x_values)\n    for x, exp_x in data:\n        plt.scatter(x, exp_x, color='b')\n    plt.show()\n    return data\n```"}
{"seed": "{\"task_id\": 18837607, \"prompt\": \"def f_18837607(oldlist, removelist):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"[v for i, v in enumerate(oldlist) if i not in removelist]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([\\\"asdf\\\",\\\"ghjk\\\",\\\"qwer\\\",\\\"tyui\\\"], [1,3]) == ['asdf', 'qwer']\\n\", \"\\n    assert candidate([1,2,3,4,5], [0,4]) == [2,3,4]\\n\"], \"entry_point\": \"f_18837607\", \"intent\": \"remove elements from list `oldlist` that have an index number mentioned in list `removelist`\", \"library\": []}", "generation": "Scenario 1:\nnumpy, pandas, random, matplotlib.pyplot\n```python\nimport numpy as np\nimport pandas as pd\nfrom random import sample\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef generate_dataset_and_sample_columns(n_rows, remove_cols):\n    \"\"\"\n    Generate a DataFrame with random values and randomly remove some columns \n    based on the provided indices.\n    \n    Parameters:\n    n_rows (int): The number of rows in the DataFrame.\n    remove_cols (list of int): The indices of columns to be removed.\n    \n    Returns:\n    DataFrame: The resulting DataFrame after removal of columns.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = generate_dataset_and_sample_columns(10, [1, 3])\n    >>> print(df)\n    \"\"\"\n    df = pd.DataFrame(np.random.randint(0,100,size=(n_rows, len(COLUMNS))), columns=COLUMNS)\n    df = df.drop(df.columns[remove_cols], axis=1)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, pandas, matplotlib.pyplot\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef generate_dataset_and_plot_histogram(n_rows, plot_cols):\n    \"\"\"\n    Generate a DataFrame with random values and plot histograms for some columns \n    based on the provided indices.\n    \n    Parameters:\n    n_rows (int): The number of rows in the DataFrame.\n    plot_cols (list of int): The indices of columns to be plotted.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    \n    Example:\n    >>> generate_dataset_and_plot_histogram(100, [1, 3])\n    \"\"\"\n    df = pd.DataFrame(np.random.randint(0,100,size=(n_rows, len(COLUMNS))), columns=COLUMNS)\n    \n    for i in plot_cols:\n        df[COLUMNS[i]].plot(kind='hist', title=COLUMNS[i])\n        plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, pandas, matplotlib.pyplot\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef generate_dataset_and_plot_correlation(n_rows, plot_cols):\n    \"\"\"\n    Generate a DataFrame with random values and plot correlation matrix for some columns \n    based on the provided indices.\n    \n    Parameters:\n    n_rows (int): The number of rows in the DataFrame.\n    plot_cols (list of int): The indices of columns to be plotted.\n    \n    Returns:\n    DataFrame: The correlation matrix of the selected columns.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    \n    Example:\n    >>> corr = generate_dataset_and_plot_correlation(100, [1, 3])\n    >>> print(corr)\n    \"\"\"\n    df = pd.DataFrame(np.random.randint(0,100,size=(n_rows, len(COLUMNS))), columns=COLUMNS)\n    corr = df.iloc[:, plot_cols].corr()\n    plt.matshow(corr)\n    plt.show()\n    \n    return corr\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, pandas, seaborn\n```python\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef generate_dataset_and_boxplot(n_rows, boxplot_cols):\n    \"\"\"\n    Generate a DataFrame with random values and plot boxplots for some columns \n    based on the provided indices.\n    \n    Parameters:\n    n_rows (int): The number of rows in the DataFrame.\n    boxplot_cols (list of int): The indices of columns to be plotted.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - seaborn\n    \n    Example:\n    >>> generate_dataset_and_boxplot(100, [1, 3])\n    \"\"\"\n    df = pd.DataFrame(np.random.randint(0,100,size=(n_rows, len(COLUMNS))), columns=COLUMNS)\n    \n    for i in boxplot_cols:\n        sns.boxplot(x=df[COLUMNS[i]])\n        plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, pandas, sklearn.preprocessing\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef generate_dataset_and_scale(n_rows, scale_cols):\n    \"\"\"\n    Generate a DataFrame with random values and scale some columns \n    based on the provided indices using StandardScaler.\n    \n    Parameters:\n    n_rows (int): The number of rows in the DataFrame.\n    scale_cols (list of int): The indices of columns to be scaled.\n    \n    Returns:\n    DataFrame: The resulting DataFrame after scaling the selected columns.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.preprocessing.StandardScaler\n    \n    Example:\n    >>> df = generate_dataset_and_scale(100, [1, 3])\n    >>> print(df)\n    \"\"\"\n    df = pd.DataFrame(np.random.randint(0,100,size=(n_rows, len(COLUMNS))), columns=COLUMNS)\n    \n    for i in scale_cols:\n        scaler = StandardScaler()\n        df[COLUMNS[i]] = scaler.fit_transform(df[[COLUMNS[i]]])\n    \n    return df\n```"}
{"seed": "{\"task_id\": 4710067, \"prompt\": \"def f_4710067():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"open('yourfile.txt', 'w')\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    fw = candidate()\\n    assert fw.name == \\\"yourfile.txt\\\"\\n    assert fw.mode == 'w'\\n\"], \"entry_point\": \"f_4710067\", \"intent\": \"Open a file `yourfile.txt` in write mode\", \"library\": []}", "generation": "Scenario 1:\nos, csv, random\n```python\nimport os\nimport csv\nimport random\n\n# Constants\nFIELDS = ['Name', 'Age', 'Gender', 'Country']\n\ndef generate_random_data_csv(file_path, num_rows):\n    \"\"\"\n    Generate a CSV file at a given file path with random data for a predefined\n    set of fields. The number of rows in the CSV file is determined by num_rows.\n\n    Parameters:\n    file_path (str): The file path where the CSV file should be created.\n    num_rows (int): The number of rows of random data to generate.\n\n    Returns:\n    str: The file path of the generated CSV file.\n\n    Requirements:\n    - os\n    - csv\n    - random\n\n    Example:\n    >>> generate_random_data_csv('/tmp/data.csv', 100)\n    \"\"\"\n    with open(file_path, 'w', newline='') as csv_file:\n        writer = csv.DictWriter(csv_file, fieldnames=FIELDS)\n        writer.writeheader()\n\n        for _ in range(num_rows):\n            writer.writerow({\n                'Name': ''.join(random.choices('ABCDEFGHIJKLMNOPQRSTUVWXYZ', k=5)),\n                'Age': random.randint(20, 60),\n                'Gender': random.choice(['Male', 'Female']),\n                'Country': random.choice(['USA', 'UK', 'Canada', 'Australia', 'India'])\n            })\n\n    return file_path\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, json, random, datetime\n```python\nimport os\nimport json\nimport random\nfrom datetime import datetime, timedelta\n\n# Constants\nUSERS = ['Alice', 'Bob', 'Charlie', 'Dave', 'Eve']\n\ndef generate_user_activity_log(file_path, num_entries):\n    \"\"\"\n    Generate a JSON file at a given file path with random user activity data.\n    The number of entries in the JSON file is determined by num_entries.\n\n    Parameters:\n    file_path (str): The file path where the JSON file should be created.\n    num_entries (int): The number of entries of random data to generate.\n\n    Returns:\n    str: The file path of the generated JSON file.\n\n    Requirements:\n    - os\n    - json\n    - random\n    - datetime\n\n    Example:\n    >>> generate_user_activity_log('/tmp/log.json', 100)\n    \"\"\"\n    log_entries = []\n    current_time = datetime.now()\n    for _ in range(num_entries):\n        user = random.choice(USERS)\n        action = random.choice(['login', 'logout', 'view_page', 'edit_profile', 'post_message'])\n        timestamp = current_time.strftime('%Y-%m-%dT%H:%M:%S')\n        log_entries.append({'user': user, 'action': action, 'timestamp': timestamp})\n        current_time -= timedelta(minutes=random.randint(1, 60))\n\n    with open(file_path, 'w') as json_file:\n        json.dump(log_entries, json_file, indent=4)\n\n    return file_path\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, pandas, numpy\n```python\nimport os\nimport pandas as pd\nimport numpy as np\n\n# Constants\nDATA_DIMENSIONS = 5\n\ndef generate_random_data_frame(file_path, num_rows):\n    \"\"\"\n    Generate a CSV file at a given file path with random numeric data for a \n    predefined number of dimensions. The number of rows in the CSV file is \n    determined by num_rows.\n\n    Parameters:\n    file_path (str): The file path where the CSV file should be created.\n    num_rows (int): The number of rows of random data to generate.\n\n    Returns:\n    str: The file path of the generated CSV file.\n\n    Requirements:\n    - os\n    - pandas\n    - numpy\n\n    Example:\n    >>> generate_random_data_frame('/tmp/data.csv', 100)\n    \"\"\"\n    df = pd.DataFrame(np.random.rand(num_rows, DATA_DIMENSIONS), \n                      columns=[f'Feature_{i+1}' for i in range(DATA_DIMENSIONS)])\n\n    df.to_csv(file_path, index=False)\n\n    return file_path\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, sqlite3, random\n```python\nimport os\nimport sqlite3\nimport random\n\n# Constants\nUSERS = ['Alice', 'Bob', 'Charlie', 'Dave', 'Eve']\n\ndef generate_user_db(db_path, num_entries):\n    \"\"\"\n    Generate a SQLite database at a given file path with random user data.\n    The number of entries in the database is determined by num_entries.\n\n    Parameters:\n    db_path (str): The file path where the SQLite database should be created.\n    num_entries (int): The number of entries of random data to generate.\n\n    Returns:\n    str: The file path of the generated SQLite database.\n\n    Requirements:\n    - os\n    - sqlite3\n    - random\n\n    Example:\n    >>> generate_user_db('/tmp/users.db', 100)\n    \"\"\"\n    conn = sqlite3.connect(db_path)\n    c = conn.cursor()\n\n    c.execute('''\n        CREATE TABLE users\n        (id INTEGER PRIMARY KEY, name TEXT, age INTEGER, country TEXT)\n    ''')\n\n    for _ in range(num_entries):\n        user = random.choice(USERS)\n        age = random.randint(20, 60)\n        country = random.choice(['USA', 'UK', 'Canada', 'Australia', 'India'])\n        c.execute('INSERT INTO users (name, age, country) VALUES (?, ?, ?)', (user, age, country))\n\n    conn.commit()\n    conn.close()\n\n    return db_path\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, csv, random, faker\n```python\nimport os\nimport csv\nimport random\nfrom faker import Faker\n\n# Faker object\nfake = Faker()\n\ndef generate_fake_people_csv(file_path, num_rows):\n    \"\"\"\n    Generate a CSV file at a given file path with fake people data.\n    The number of rows in the CSV file is determined by num_rows.\n\n    Parameters:\n    file_path (str): The file path where the CSV file should be created.\n    num_rows (int): The number of rows of random data to generate.\n\n    Returns:\n    str: The file path of the generated CSV file.\n\n    Requirements:\n    - os\n    - csv\n    - random\n    - faker\n\n    Example:\n    >>> generate_fake_people_csv('/tmp/people.csv', 100)\n    \"\"\"\n    with open(file_path, 'w', newline='') as csv_file:\n        writer = csv.writer(csv_file)\n        writer.writerow(['Name', 'Age', 'Address', 'Email'])\n\n        for _ in range(num_rows):\n            name = fake.name()\n            age = random.randint(20, 60)\n            address = fake.address().replace('\\n', ', ')\n            email = fake.email()\n            writer.writerow([name, age, address, email])\n\n    return file_path\n```\n"}
{"seed": "{\"task_id\": 7373219, \"prompt\": \"def f_7373219(obj, attr):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"getattr(obj, attr)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    class Student:\\n        student_id = \\\"\\\"\\n        student_name = \\\"\\\"\\n\\n        def __init__(self, student_id=101, student_name=\\\"Adam\\\"):\\n            self.student_id = student_id\\n            self.student_name = student_name\\n\\n    student = Student()\\n\\n    assert(candidate(student, 'student_name') == \\\"Adam\\\")\\n    assert(candidate(student, 'student_id') == 101)\\n\", \"\\n    class Student:\\n        student_id = \\\"\\\"\\n        student_name = \\\"\\\"\\n\\n        def __init__(self, student_id=101, student_name=\\\"Adam\\\"):\\n            self.student_id = student_id\\n            self.student_name = student_name\\n\\n    student = Student()\\n\\n    try:\\n        value = candidate(student, 'student_none')\\n    except: \\n        assert True\\n\"], \"entry_point\": \"f_7373219\", \"intent\": \"get attribute 'attr' from object `obj`\", \"library\": []}", "generation": "Scenario 3: \ncollections, itertools, random, copy\n```python\nimport collections\nimport itertools\nimport random\nimport copy\n\n# Constants\nCOLORS = ['Red', 'Green', 'Blue', 'Yellow', 'White']\nSHAPES = ['Square', 'Circle', 'Triangle', 'Rectangle', 'Pentagon']\n\n# Sample data\nclass Object:\n    color = \"\"\n    shape = \"\"\n\n    def __init__(self, color=random.choice(COLORS), shape=random.choice(SHAPES)):\n        self.color = color\n        self.shape = shape\n\ndef count_object_attributes(obj_list, attr):\n    \"\"\"\n    Count the frequency of each value of attribute 'attr' among a list of objects.\n\n    Parameters:\n    obj_list (list): The list of objects.\n    attr (str): The attribute to count.\n\n    Returns:\n    collections.Counter: The frequency count of each value of the attribute.\n\n    Requirements:\n    - collections\n    - itertools\n    - random\n    - copy\n    \n    Example:\n    >>> obj_list = [Object() for _ in range(100)]\n    >>> count = count_object_attributes(obj_list, 'color')\n    >>> print(count)\n    \"\"\"\n    attr_values = [getattr(obj, attr) for obj in obj_list]\n    count = collections.Counter(attr_values)\n\n    return count\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, scipy.stats, matplotlib, random\n```python\nimport numpy as np\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nimport random\n\n# Constants\nMU = 0\nSTD = 1\n\n# Sample data\nclass Object:\n    value = 0\n\n    def __init__(self, value=random.gauss(MU, STD)):\n        self.value = value\n\ndef plot_distribution(obj_list, attr):\n    \"\"\"\n    Plot the histogram and the fitted normal distribution curve of the attribute 'attr' \n    among a list of objects.\n\n    Parameters:\n    obj_list (list): The list of objects.\n    attr (str): The attribute to plot.\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib\n    - random\n    \n    Example:\n    >>> obj_list = [Object() for _ in range(1000)]\n    >>> plot_distribution(obj_list, 'value')\n    \"\"\"\n    attr_values = [getattr(obj, attr) for obj in obj_list]\n\n    # Plot histogram\n    plt.hist(attr_values, bins=30, density=True, alpha=0.6, color='g')\n\n    # Plot the PDF.\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, MU, STD)\n    plt.plot(x, p, 'k', linewidth=2)\n\n    title = \"Fit results: mu = %.2f,  std = %.2f\" % (MU, STD)\n    plt.title(title)\n\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nheapq, random, itertools\n```python\nimport heapq\nimport random\nimport itertools\n\n# Constants\nTOP_N = 5\n\n# Sample data\nclass Object:\n    value = 0\n\n    def __init__(self, value=random.randint(1, 100)):\n        self.value = value\n\ndef top_n_values(obj_list, attr):\n    \"\"\"\n    Find the top N values of attribute 'attr' among a list of objects.\n\n    Parameters:\n    obj_list (list): The list of objects.\n    attr (str): The attribute to find the top N values.\n\n    Returns:\n    list: The top N values.\n\n    Requirements:\n    - heapq\n    - random\n    - itertools\n    \n    Example:\n    >>> obj_list = [Object() for _ in range(100)]\n    >>> top_values = top_n_values(obj_list, 'value')\n    >>> print(top_values)\n    \"\"\"\n    attr_values = [getattr(obj, attr) for obj in obj_list]\n    top_values = heapq.nlargest(TOP_N, attr_values)\n\n    return top_values\n```\n`GPT_ODEX_BREAK`\n\nScenario 6: \nsys, os, time, random\n```python\nimport sys\nimport os\nimport time\nimport random\n\n# Constants\nSLEEP_TIME = 1\n\n# Sample data\nclass Object:\n    name = \"\"\n\n    def __init__(self, name=str(random.randint(1, 100))):\n        self.name = name\n\ndef print_attribute_every_second(obj, attr):\n    \"\"\"\n    Print the attribute 'attr' of an object every second until the user interrupts the process.\n\n    Parameters:\n    obj (Object): The object.\n    attr (str): The attribute to print.\n\n    Requirements:\n    - sys\n    - os\n    - time\n    - random\n    \n    Example:\n    >>> obj = Object()\n    >>> print_attribute_every_second(obj, 'name')\n    \"\"\"\n    while True:\n        try:\n            print(getattr(obj, attr))\n            time.sleep(SLEEP_TIME)\n        except KeyboardInterrupt:\n            print(\"\\nInterrupted by user\")\n            break\n```\n`GPT_ODEX_BREAK`\n\nScenario 7: \nrandom, numpy, matplotlib\n```python\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nNUM_BINS = 30\n\n# Sample data\nclass Object:\n    value = 0\n\n    def __init__(self, value=random.gauss(0, 1)):\n        self.value = value\n\ndef histogram_of_attribute(obj_list, attr):\n    \"\"\"\n    Generate a histogram of the attribute 'attr' among a list of objects.\n\n    Parameters:\n    obj_list (list): The list of objects.\n    attr (str): The attribute to generate a histogram.\n\n    Requirements:\n    - random\n    - numpy\n    - matplotlib\n    \n    Example:\n    >>> obj_list = [Object() for _ in range(1000)]\n    >>> histogram_of_attribute(obj_list, 'value')\n    \"\"\"\n    attr_values = [getattr(obj, attr) for obj in obj_list]\n\n    # Generate histogram\n    plt.hist(attr_values, bins=NUM_BINS, alpha=0.5)\n\n    plt.title('Histogram of attribute values')\n    plt.xlabel('Attribute Value')\n    plt.ylabel('Count')\n\n    plt.show()\n```"}
{"seed": "{\"task_id\": 8171751, \"prompt\": \"def f_8171751():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"reduce(lambda a, b: a + b, (('aa',), ('bb',), ('cc',)))\", \"test_start\": \"\\nfrom functools import reduce\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate() == ('aa', 'bb', 'cc')\\n\"], \"entry_point\": \"f_8171751\", \"intent\": \"convert tuple of tuples `(('aa',), ('bb',), ('cc',))` to tuple\", \"library\": [\"functools\"]}", "generation": "Scenario 1:\npandas, functools, statistics, random\n```python\nimport pandas as pd\nfrom functools import reduce\nimport statistics\nimport random\n\n# Constants\nSTUDENTS = ['Alice', 'Bob', 'Charlie', 'David', 'Eve']\nSUBJECTS = ['Math', 'Physics', 'Chemistry', 'Biology', 'English']\n\ndef generate_student_grades():\n    \"\"\"\n    Generate a report of grades for a list of students across various subjects. \n    The report should also include the average grade for each student.\n    \n    Parameters:\n    None\n    \n    Returns:\n    DataFrame: A pandas DataFrame with grades for the students.\n    \n    Requirements:\n    - pandas\n    - functools.reduce\n    - statistics\n    - random\n    \n    Example:\n    >>> report = generate_student_grades()\n    >>> print(report)\n    \"\"\"\n    report_data = []\n\n    for student in STUDENTS:\n        grades = [random.randint(0, 100) for _ in SUBJECTS]\n        avg_grade = statistics.mean(grades)\n        report_data.append((student,) + tuple(grades) + (avg_grade,))\n\n    report_df = pd.DataFrame(report_data, columns=['Student'] + SUBJECTS + ['Average Grade'])\n\n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nfunctools, itertools, random, string\n```python\nfrom functools import reduce\nfrom itertools import chain\nimport random\nimport string\n\n# Constants\nLETTERS = string.ascii_lowercase\n\ndef random_string_combinations(n):\n    \"\"\"\n    Generate a tuple of random string combinations. Each string contains lowercase \n    letters and its length is between 1 and n.\n\n    Parameters:\n    n (int): The maximum length of the string.\n\n    Returns:\n    tuple: A tuple of all combinations.\n\n    Requirements:\n    - functools.reduce\n    - itertools\n    - random\n    - string\n\n    Example:\n    >>> random_string_combinations(3)\n    \"\"\"\n    all_combinations = []\n\n    for i in range(1, n + 1):\n        combinations = [''.join(random.choices(LETTERS, k=i)) for _ in range(n)]\n        all_combinations.append(tuple(combinations))\n\n    return reduce(lambda a, b: a + b, all_combinations)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nfunctools, itertools, collections\n```python\nfrom functools import reduce\nfrom itertools import combinations\nfrom collections import Counter\n\n# Constants\nWORDS = ['apple', 'banana', 'cherry', 'date', 'elderberry']\n\ndef word_combinations():\n    \"\"\"\n    Generate all combinations of words and return the count of unique letters \n    appearing in each combination.\n\n    Parameters:\n    None\n\n    Returns:\n    dict: A dictionary with combinations as keys and counts as values.\n\n    Requirements:\n    - functools.reduce\n    - itertools\n    - collections\n\n    Example:\n    >>> word_combinations()\n    \"\"\"\n    all_combinations = reduce(lambda a, b: a + b, [list(combinations(WORDS, i)) for i in range(1, len(WORDS) + 1)])\n\n    combination_counts = {combination: len(set(''.join(combination))) for combination in all_combinations}\n\n    return combination_counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nfunctools, itertools, math\n```python\nfrom functools import reduce\nfrom itertools import permutations\nimport math\n\n# Constants\nNUMBERS = [1, 2, 3, 4, 5]\n\ndef calculate_permutation_sums():\n    \"\"\"\n    Generate all permutations of a list of numbers and calculate the sum of \n    the factorials of each number in each permutation.\n\n    Parameters:\n    None\n\n    Returns:\n    list: A list of sums.\n\n    Requirements:\n    - functools.reduce\n    - itertools\n    - math\n\n    Example:\n    >>> calculate_permutation_sums()\n    \"\"\"\n    all_permutations = list(permutations(NUMBERS))\n\n    sums = [reduce(lambda a, b: a + b, [math.factorial(n) for n in permutation]) for permutation in all_permutations]\n\n    return sums\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nfunctools, itertools, numpy, random\n```python\nfrom functools import reduce\nfrom itertools import product\nimport numpy as np\nimport random\n\n# Constants\nSHAPE = (3, 3)\n\ndef generate_random_matrix_operations():\n    \"\"\"\n    Generate a matrix of random numbers and calculate the sum of the products of \n    all possible pairs of numbers in the matrix.\n\n    Parameters:\n    None\n\n    Returns:\n    int: The sum.\n\n    Requirements:\n    - functools.reduce\n    - itertools\n    - numpy\n    - random\n\n    Example:\n    >>> generate_random_matrix_operations()\n    \"\"\"\n    matrix = np.random.randint(1, 10, SHAPE)\n\n    all_pairs = list(product(*[row for row in matrix]))\n\n    sum_of_products = reduce(lambda a, b: a + b, [np.prod(pair) for pair in all_pairs])\n\n    return sum_of_products\n```"}
{"seed": "{\"task_id\": 8171751, \"prompt\": \"def f_8171751():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"list(map(lambda a: a[0], (('aa',), ('bb',), ('cc',))))\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate() == ['aa', 'bb', 'cc']\\n\"], \"entry_point\": \"f_8171751\", \"intent\": \"convert tuple of tuples `(('aa',), ('bb',), ('cc',))` to list in one line\", \"library\": []}", "generation": "Scenario 1: \npandas, matplotlib.pyplot, numpy, itertools\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport itertools\n\n# Constants\nCATEGORIES = ['A', 'B', 'C', 'D', 'E']\n\ndef generate_and_plot_data():\n    \"\"\"\n    Generate a DataFrame with random data for categories and plot a bar chart \n    for the mean value of each category.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - itertools\n    \n    Example:\n    >>> df = generate_and_plot_data()\n    >>> print(df.head())\n    >>> df.groupby('Category').mean().plot(kind='bar')\n    \"\"\"\n    data = [(cat, np.random.rand()) for cat in itertools.cycle(CATEGORIES) for _ in range(100)]\n\n    df = pd.DataFrame(data, columns=['Category', 'Value'])\n\n    return df\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncollections, string, random\n```python\nfrom collections import Counter\nimport string\nimport random\n\n# Constants\nLETTERS = string.ascii_lowercase\n\ndef generate_letter_counts(n):\n    \"\"\"\n    Generate a count of random letters.\n\n    Parameters:\n    n (int): The number of random letters to generate.\n\n    Returns:\n    Counter: A collections.Counter object with the count of each letter.\n\n    Requirements:\n    - collections\n    - string\n    - random\n\n    Example:\n    >>> letter_counts = generate_letter_counts(1000)\n    >>> print(letter_counts)\n    \"\"\"\n    letters = [random.choice(LETTERS) for _ in range(n)]\n    letter_counts = Counter(letters)\n\n    return letter_counts\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 3:\nre, string, random\n```python\nimport re\nimport string\nimport random\n\n# Constants\nPATTERN = r'[A-Za-z0-9]{5}'\n\ndef generate_and_find_pattern(n):\n    \"\"\"\n    Generate a string of random characters and find all occurrences of a pattern.\n\n    Parameters:\n    n (int): The length of the random string to generate.\n\n    Returns:\n    list: A list of all occurrences of the pattern.\n\n    Requirements:\n    - re\n    - string\n    - random\n\n    Example:\n    >>> patterns = generate_and_find_pattern(1000)\n    >>> print(patterns)\n    \"\"\"\n    rand_str = ''.join(random.choice(string.printable) for _ in range(n))\n    matches = re.findall(PATTERN, rand_str)\n\n    return matches\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, scipy.stats, matplotlib.pyplot\n```python\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef generate_and_plot_normal_distribution(mu, sigma):\n    \"\"\"\n    Generate a normal distribution with a given mean and standard deviation and plot a histogram.\n\n    Parameters:\n    mu (float): The mean of the distribution.\n    sigma (float): The standard deviation of the distribution.\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Example:\n    >>> generate_and_plot_normal_distribution(0, 1)\n    \"\"\"\n    s = np.random.normal(mu, sigma, 1000)\n\n    count, bins, ignored = plt.hist(s, 30, density=True)\n    plt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) * np.exp( - (bins - mu)**2 / (2 * sigma**2) ), linewidth=2, color='r')\n    plt.show()\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 5:\ncollections, random, string\n```python\nfrom collections import defaultdict\nimport random\nimport string\n\n# Constants\nLETTERS = string.ascii_lowercase\n\ndef generate_letter_dict(n):\n    \"\"\"\n    Generate a defaultdict of lists with random letters.\n\n    Parameters:\n    n (int): The number of random letters to generate.\n\n    Returns:\n    defaultdict: A collections.defaultdict object with the lists of letters.\n\n    Requirements:\n    - collections\n    - random\n    - string\n\n    Example:\n    >>> letter_dict = generate_letter_dict(1000)\n    >>> print(letter_dict)\n    \"\"\"\n    letter_dict = defaultdict(list)\n    for _ in range(n):\n        letter = random.choice(LETTERS)\n        letter_dict[letter].append(letter)\n\n    return letter_dict\n```\n"}
{"seed": "{\"task_id\": 28986489, \"prompt\": \"def f_28986489(df):\\n\\t\", \"suffix\": \"\\n\\treturn df\", \"canonical_solution\": \"df['range'].replace(',', '-', inplace=True)\", \"test_start\": \"\\nimport pandas as pd\\n\\ndef check(candidate):\", \"test\": [\"\\n    df = pd.DataFrame({'range' : [\\\",\\\", \\\"(50,290)\\\", \\\",,,\\\"]})\\n    res = pd.DataFrame({'range' : [\\\"-\\\", \\\"(50,290)\\\", \\\",,,\\\"]})\\n    assert candidate(df).equals(res)\\n\"], \"entry_point\": \"f_28986489\", \"intent\": \"replace a characters in a column of a dataframe `df`\", \"library\": [\"pandas\"]}", "generation": "Scenario 1:\npandas, random, seaborn, matplotlib\n```python\nimport pandas as pd\nimport random\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nCATEGORIES = ['A', 'B', 'C', 'D', 'E']\nRANGE_VALUES = [str(random.randint(1, 100)) + ',' + str(random.randint(101, 200)) for _ in range(1000)]\n\ndef plot_category_distribution(df):\n    \"\"\"\n    Replace ',' with '-' in 'range' column of a dataframe 'df' and plot \n    distribution of 'range' across 'category' column.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n\n    Returns:\n    DataFrame: The modified DataFrame.\n\n    Requirements:\n    - pandas\n    - random\n    - seaborn\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame({'range' : RANGE_VALUES, 'category': [random.choice(CATEGORIES) for _ in range(1000)]})\n    >>> df = plot_category_distribution(df)\n    \"\"\"\n    df['range'].replace(',', '-', inplace=True)\n    plt.figure(figsize=(10,6))\n    sns.countplot(data=df, x='category')\n    plt.show()\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, matplotlib, sklearn.preprocessing\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\n\n# Constants\nRANGE_VALUES = [str(np.random.normal()) + ',' + str(np.random.normal()) for _ in range(1000)]\nLABELS = ['Label_' + str(i) for i in range(4)]\n\ndef encode_and_visualize(df):\n    \"\"\"\n    Replace ',' with '-' in 'range' column of a dataframe 'df', encode 'label' \n    column and plot a histogram of 'range'.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n\n    Returns:\n    DataFrame: The modified DataFrame.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.preprocessing.LabelEncoder\n\n    Example:\n    >>> df = pd.DataFrame({'range' : RANGE_VALUES, 'label': np.random.choice(LABELS, 1000)})\n    >>> df = encode_and_visualize(df)\n    \"\"\"\n    df['range'].replace(',', '-', inplace=True)\n    encoder = LabelEncoder()\n    df['label'] = encoder.fit_transform(df['label'])\n    df['range'].astype(str).hist()\n    plt.show()\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, matplotlib, scipy.stats\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n# Constants\nRANGE_VALUES = [str(np.random.normal()) + ',' + str(np.random.normal()) for _ in range(1000)]\n\ndef plot_normal_distribution(df):\n    \"\"\"\n    Replace ',' with '-' in 'range' column of a dataframe 'df' and plot a \n    histogram of 'range' with a normal distribution fit.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n\n    Returns:\n    DataFrame: The modified DataFrame.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - scipy.stats.norm\n\n    Example:\n    >>> df = pd.DataFrame({'range' : RANGE_VALUES})\n    >>> df = plot_normal_distribution(df)\n    \"\"\"\n    df['range'].replace(',', '-', inplace=True)\n    df['range'].astype(str).plot(kind='hist', density=True)\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, df['range'].mean(), df['range'].std())\n    plt.plot(x, p, 'k', linewidth=2)\n    plt.show()\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, matplotlib, sklearn.preprocessing\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Constants\nRANGE_VALUES = [str(np.random.normal()) + ',' + str(np.random.normal()) for _ in range(1000)]\n\ndef scale_and_plot(df):\n    \"\"\"\n    Replace ',' with '-' in 'range' column of a dataframe 'df', scale 'range' \n    column and plot a histogram of 'range'.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n\n    Returns:\n    DataFrame: The modified DataFrame.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.preprocessing.MinMaxScaler\n\n    Example:\n    >>> df = pd.DataFrame({'range' : RANGE_VALUES})\n    >>> df = scale_and_plot(df)\n    \"\"\"\n    df['range'].replace(',', '-', inplace=True)\n    scaler = MinMaxScaler()\n    df['range'] = scaler.fit_transform(df[['range']])\n    df['range'].plot(kind='hist')\n    plt.show()\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, matplotlib, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nRANGE_VALUES = [str(np.random.normal()) + ',' + str(np.random.normal()) for _ in range(1000)]\n\ndef plot_density(df):\n    \"\"\"\n    Replace ',' with '-' in 'range' column of a dataframe 'df' and plot a \n    density plot of 'range'.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n\n    Returns:\n    DataFrame: The modified DataFrame.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> df = pd.DataFrame({'range' : RANGE_VALUES})\n    >>> df = plot_density(df)\n    \"\"\"\n    df['range'].replace(',', '-', inplace=True)\n    sns.kdeplot(df['range'], shade=True)\n    plt.show()\n    return df\n```\n"}
{"seed": "{\"task_id\": 19339, \"prompt\": \"def f_19339():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"zip(*[('a', 1), ('b', 2), ('c', 3), ('d', 4)])\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert [a for a in candidate()] == [('a', 'b', 'c', 'd'), (1, 2, 3, 4)]\\n\"], \"entry_point\": \"f_19339\", \"intent\": \"unzip the list `[('a', 1), ('b', 2), ('c', 3), ('d', 4)]`\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, itertools, collections\n```python\nimport pandas as pd\nimport numpy as np\nimport itertools\nfrom collections import Counter\n\n# Constants\nSAMPLE_DATA = [('apple', 5), ('banana', 3), ('apple', 6), ('banana', 4),\n               ('cherry', 5), ('banana', 2), ('apple', 4), ('cherry', 5)]\n\ndef calculate_fruit_statistics():\n    \"\"\"\n    Unzip a list of fruits and their counts, calculate the total count for each \n    fruit and the average count.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the total count and average count for each fruit.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - itertools\n    - collections.Counter\n    \n    Example:\n    >>> report = calculate_fruit_statistics()\n    >>> print(report)\n    \"\"\"\n    fruits, counts = zip(*SAMPLE_DATA)\n    total_counts = Counter(fruits)\n    avg_counts = {fruit: np.mean([count for fruit_, count in SAMPLE_DATA if fruit_ == fruit])\n                  for fruit in total_counts.keys()}\n\n    report_df = pd.DataFrame(list(zip(total_counts.values(), avg_counts.values())),\n                             index=total_counts.keys(),\n                             columns=['Total Count', 'Average Count'])\n\n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, scipy.stats, sklearn.preprocessing\n```python\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import zscore\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Constants\nSAMPLE_DATA = [('A', 100, 0.5), ('B', 200, 0.6), ('C', 150, 0.7), ('D', 180, 0.8)]\n\ndef normalize_data():\n    \"\"\"\n    Unzip a list of items, their counts and weights, normalize the counts and weights \n    using z-score and min-max normalization respectively.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the normalized counts and weights for each item.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - scipy.stats.zscore\n    - sklearn.preprocessing.MinMaxScaler\n    \n    Example:\n    >>> report = normalize_data()\n    >>> print(report)\n    \"\"\"\n    items, counts, weights = zip(*SAMPLE_DATA)\n    counts_normalized = zscore(counts)\n    scaler = MinMaxScaler()\n    weights_normalized = scaler.fit_transform(np.array(weights).reshape(-1, 1))\n\n    report_df = pd.DataFrame(list(zip(counts_normalized, weights_normalized.flatten())),\n                             index=items,\n                             columns=['Normalized Count', 'Normalized Weight'])\n\n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, matplotlib, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nSAMPLE_DATA = [('A', 1, 10), ('B', 2, 20), ('C', 3, 30), ('D', 4, 40)]\n\ndef plot_data():\n    \"\"\"\n    Unzip a list of items, their x and y values, create a DataFrame and plot the \n    values using seaborn.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - seaborn\n    \n    Example:\n    >>> plot_data()\n    \"\"\"\n    items, x_values, y_values = zip(*SAMPLE_DATA)\n    df = pd.DataFrame(list(zip(x_values, y_values)), index=items, columns=['X', 'Y'])\n    sns.lineplot(data=df, palette=\"tab10\", linewidth=2.5)\n\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, itertools, sklearn.cluster\n```python\nimport numpy as np\nimport itertools\nfrom sklearn.cluster import KMeans\n\n# Constants\nSAMPLE_DATA = [('A', 1, 1), ('B', 2, 2), ('C', 3, 3), ('D', 4, 4)]\n\ndef cluster_data():\n    \"\"\"\n    Unzip a list of items and their 2D coordinates, perform KMeans clustering and \n    return the labels.\n    \n    Returns:\n    ndarray: A numpy array with the cluster labels for each item.\n    \n    Requirements:\n    - numpy\n    - itertools\n    - sklearn.cluster.KMeans\n    \n    Example:\n    >>> labels = cluster_data()\n    >>> print(labels)\n    \"\"\"\n    items, x_values, y_values = zip(*SAMPLE_DATA)\n    coordinates = np.array(list(zip(x_values, y_values)))\n\n    kmeans = KMeans(n_clusters=2, random_state=0).fit(coordinates)\n    labels = kmeans.labels_\n\n    return labels\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, itertools, sklearn.decomposition, matplotlib\n```python\nimport numpy as np\nimport itertools\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\n# Constants\nSAMPLE_DATA = [('A', 1, 1, 1), ('B', 2, 2, 2), ('C', 3, 3, 3), ('D', 4, 4, 4)]\n\ndef perform_pca():\n    \"\"\"\n    Unzip a list of items and their 3D coordinates, perform PCA to reduce the \n    dimensionality to 2D and plot the 2D points.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - numpy\n    - itertools\n    - sklearn.decomposition.PCA\n    - matplotlib.pyplot\n    \n    Example:\n    >>> perform_pca()\n    \"\"\"\n    items, x_values, y_values, z_values = zip(*SAMPLE_DATA)\n    coordinates = np.array(list(zip(x_values, y_values, z_values)))\n\n    pca = PCA(n_components=2)\n    coordinates_2d = pca.fit_transform(coordinates)\n\n    plt.scatter(*zip(*coordinates_2d))\n    plt.show()\n```"}
{"seed": "{\"task_id\": 19339, \"prompt\": \"def f_19339(original):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"([a for (a, b) in original], [b for (a, b) in original])\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    original = [('a', 1), ('b', 2), ('c', 3), ('d', 4)]\\n    assert candidate(original) == (['a', 'b', 'c', 'd'], [1, 2, 3, 4])\\n\", \"\\n    original2 = [([], 1), ([], 2), (5, 3), (6, 4)]\\n    assert candidate(original2) == ([[], [], 5, 6], [1, 2, 3, 4])\\n\"], \"entry_point\": \"f_19339\", \"intent\": \"unzip list `original`\", \"library\": []}", "generation": "Scenario 1:\npandas, numpy, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['Name', 'Age', 'Score']\n\ndef analyze_data(original):\n    \"\"\"\n    Create a DataFrame from the list `original`, calculate basic statistics for \n    numeric columns, and plot a histogram for each numeric column.\n\n    Parameters:\n    original (list): The original list with tuples to be unzipped into a DataFrame.\n\n    Returns:\n    DataFrame: A pandas DataFrame for the unzipped data.\n    dict: Basic statistics for numeric columns (age and score).\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> original = [('Tom', 25, 85), ('Jerry', 22, 95), ('Spike', 30, 75), ('Tyke', 20, 90)]\n    >>> df, stats = analyze_data(original)\n    >>> print(df)\n    >>> print(stats)\n    >>> df['Age'].plot(kind='hist')\n    >>> df['Score'].plot(kind='hist')\n    \"\"\"\n    df = pd.DataFrame(original, columns=COLUMNS)\n\n    stats = df.describe().to_dict()\n\n    return df, stats\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, matplotlib, scipy\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\ndef plot_distribution(original):\n    \"\"\"\n    Create a numpy array from the list `original`, calculate basic statistics,\n    and plot histogram and probability density function (PDF) of the array.\n\n    Parameters:\n    original (list): The original list with tuples to be unzipped into a numpy array.\n\n    Returns:\n    np.array: A numpy array for the unzipped data.\n    dict: Basic statistics for the array.\n    \n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - scipy.stats\n\n    Example:\n    >>> original = [('a', 1), ('b', 2), ('c', 3), ('d', 4)]\n    >>> arr, stats = plot_distribution(original)\n    >>> print(arr)\n    >>> print(stats)\n    >>> plt.hist(arr, density=True)\n    >>> xmin, xmax = plt.xlim()\n    >>> x = np.linspace(xmin, xmax, 100)\n    >>> p = stats.norm.pdf(x, stats['mean'], stats['std'])\n    >>> plt.plot(x, p, 'k', linewidth=2)\n    >>> plt.show()\n    \"\"\"\n    arr = np.array([b for (a, b) in original])\n\n    stats = {\n        'mean': np.mean(arr),\n        'std': np.std(arr),\n        'min': np.min(arr),\n        'max': np.max(arr)\n    }\n\n    return arr, stats\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n\ndef visualize_data(original):\n    \"\"\"\n    Create a DataFrame from the list `original`, compute correlation matrix, \n    and plot a heatmap for the correlation matrix.\n\n    Parameters:\n    original (list): The original list with tuples to be unzipped into a DataFrame.\n\n    Returns:\n    DataFrame: A pandas DataFrame for the unzipped data.\n    np.array: Correlation matrix.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - seaborn\n\n    Example:\n    >>> original = [('Tom', 25, 85), ('Jerry', 22, 95), ('Spike', 30, 75), ('Tyke', 20, 90)]\n    >>> df, corr = visualize_data(original)\n    >>> print(df)\n    >>> print(corr)\n    >>> sns.heatmap(corr, annot=True)\n    >>> plt.show()\n    \"\"\"\n    df = pd.DataFrame(original, columns=['Name', 'Age', 'Score'])\n\n    corr = df.corr().values\n\n    return df, corr\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, matplotlib, sklearn\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\n\ndef normalize_plot(original):\n    \"\"\"\n    Create a numpy array from the list `original`, normalize the array, \n    and plot the original and normalized arrays.\n\n    Parameters:\n    original (list): The original list with tuples to be unzipped into a numpy array.\n\n    Returns:\n    np.array: A numpy array for the original data.\n    np.array: Normalized array.\n    \n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.preprocessing\n\n    Example:\n    >>> original = [('a', 1), ('b', 2), ('c', 3), ('d', 4)]\n    >>> arr, norm_arr = normalize_plot(original)\n    >>> print(arr)\n    >>> print(norm_arr)\n    >>> plt.plot(arr, label='Original')\n    >>> plt.plot(norm_arr, label='Normalized')\n    >>> plt.legend()\n    >>> plt.show()\n    \"\"\"\n    arr = np.array([b for (a, b) in original])\n\n    norm_arr = preprocessing.normalize([arr])[0]\n\n    return arr, norm_arr\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, matplotlib, scipy\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.fft import fft\n\ndef calculate_fft(original):\n    \"\"\"\n    Create a numpy array from the list `original`, calculate Fast Fourier Transform (FFT), \n    and plot the original and FFT data.\n\n    Parameters:\n    original (list): The original list with tuples to be unzipped into a numpy array.\n\n    Returns:\n    np.array: A numpy array for the original data.\n    np.array: FFT data.\n    \n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - scipy.fft\n\n    Example:\n    >>> original = [('a', 1), ('b', 2), ('c', 3), ('d', 4)]\n    >>> arr, fft_data = calculate_fft(original)\n    >>> print(arr)\n    >>> print(fft_data)\n    >>> plt.plot(arr, label='Original')\n    >>> plt.plot(np.abs(fft_data), label='FFT')\n    >>> plt.legend()\n    >>> plt.show()\n    \"\"\"\n    arr = np.array([b for (a, b) in original])\n\n    fft_data = fft(arr)\n\n    return arr, fft_data\n```"}
{"seed": "{\"task_id\": 19339, \"prompt\": \"def f_19339(original):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"((a for (a, b) in original), (b for (a, b) in original))\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    original = [('a', 1), ('b', 2), ('c', 3), ('d', 4)]\\n    result = candidate(original)\\n    assert [a for gen in result for a in gen] == ['a','b','c','d',1,2,3,4]\\n\", \"\\n    original2 = [([], 1), ([], 2), (5, 3), (6, 4)]\\n    result2 = candidate(original2)\\n    assert [a for gen in result2 for a in gen] == [[], [], 5, 6, 1, 2, 3, 4]\\n\"], \"entry_point\": \"f_19339\", \"intent\": \"unzip list `original` and return a generator\", \"library\": []}", "generation": "Scenario 1:\npandas, itertools, random, matplotlib\n```python\nimport pandas as pd\nfrom itertools import cycle\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nCATEGORIES = ['Fruit', 'Vegetable', 'Dairy', 'Meat', 'Bakery']\nITEMS = ['Apple', 'Carrot', 'Milk', 'Chicken', 'Bread']\n\ndef generate_shopping_list(n_items):\n    \"\"\"\n    Generate a shopping list of n items with a cyclic category and a random item.\n    \n    Parameters:\n    n_items (int): The number of items in the shopping list.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the shopping list.\n    \n    Requirements:\n    - pandas\n    - itertools\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> shopping_list = generate_shopping_list(10)\n    >>> print(shopping_list)\n    >>> shopping_list['Category'].value_counts().plot(kind='bar')\n    \"\"\"\n    category_cycle = cycle(CATEGORIES)\n    shopping_data = []\n\n    for _ in range(n_items):\n        category = next(category_cycle)\n        item = ITEMS[randint(0, len(ITEMS)-1)]\n        shopping_data.append([category, item])\n\n    shopping_df = pd.DataFrame(shopping_data, columns=['Category', 'Item'])\n\n    return shopping_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, itertools, matplotlib, math\n```python\nimport numpy as np\nfrom itertools import cycle\nimport matplotlib.pyplot as plt\nimport math\n\n# Constants\nANGLES = np.arange(0, 2*np.pi, 0.01)\n\ndef generate_sine_wave_series(n_waves):\n    \"\"\"\n    Generate a series of n sine waves with increasing frequency.\n    \n    Parameters:\n    n_waves (int): The number of sine waves in the series.\n    \n    Returns:\n    list: A list of numpy arrays with the y values of the sine waves.\n    \n    Requirements:\n    - numpy\n    - itertools\n    - matplotlib.pyplot\n    - math\n\n    Example:\n    >>> sine_waves = generate_sine_wave_series(5)\n    >>> for i, wave in enumerate(sine_waves):\n    >>>     plt.plot(ANGLES, wave, label=f'Wave {i+1}')\n    >>> plt.legend()\n    \"\"\"\n    frequency_cycle = cycle(range(1, n_waves+1))\n    sine_wave_series = []\n\n    for _ in range(n_waves):\n        frequency = next(frequency_cycle)\n        wave = np.sin(frequency * ANGLES)\n        sine_wave_series.append(wave)\n\n    return sine_wave_series\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nitertools, re, string\n```python\nfrom itertools import zip_longest\nimport re\nimport string\n\n# Constants\nPUNCTUATION = string.punctuation\n\ndef remove_punctuation(text1, text2):\n    \"\"\"\n    Remove punctuation from two texts and return the cleaned texts.\n    \n    Parameters:\n    text1, text2 (str): The original texts.\n    \n    Returns:\n    tuple: A tuple with the cleaned texts.\n    \n    Requirements:\n    - itertools\n    - re\n    - string\n\n    Example:\n    >>> text1, text2 = remove_punctuation(\"Hello, world!\", \"How's it going?\")\n    >>> print(text1, text2)\n    \"\"\"\n    cleaned_texts = []\n\n    for text in [text1, text2]:\n        cleaned_text = re.sub('['+re.escape(PUNCTUATION)+']', '', text)\n        cleaned_texts.append(cleaned_text)\n\n    return tuple(cleaned_texts)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nitertools, collections, random\n```python\nfrom itertools import cycle\nfrom collections import Counter\nfrom random import choice\n\n# Constants\nCOLORS = ['Red', 'Green', 'Blue', 'Yellow', 'Purple']\n\ndef generate_color_pattern(n_colors):\n    \"\"\"\n    Generate a color pattern of n colors with a cyclic color and a random color.\n    \n    Parameters:\n    n_colors (int): The number of colors in the pattern.\n    \n    Returns:\n    list: A list with the color pattern.\n    \n    Requirements:\n    - itertools\n    - collections\n    - random\n\n    Example:\n    >>> color_pattern = generate_color_pattern(10)\n    >>> print(color_pattern)\n    >>> print(Counter(color_pattern))\n    \"\"\"\n    color_cycle = cycle(COLORS)\n    color_pattern = []\n\n    for _ in range(n_colors):\n        color = next(color_cycle) if _ % 2 == 0 else choice(COLORS)\n        color_pattern.append(color)\n\n    return color_pattern\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, itertools, random, statistics\n```python\nimport pandas as pd\nfrom itertools import cycle\nfrom random import randint\nimport statistics\n\n# Constants\nSTUDENTS = ['Alice', 'Bob', 'Charlie', 'David', 'Eve']\nGRADES = range(1, 11)\n\ndef generate_grade_report(n_grades):\n    \"\"\"\n    Generate a grade report of n grades with a cyclic student and a random grade.\n    \n    Parameters:\n    n_grades (int): The number of grades in the report.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the grade report.\n    \n    Requirements:\n    - pandas\n    - itertools\n    - random\n    - statistics\n\n    Example:\n    >>> grade_report = generate_grade_report(15)\n    >>> print(grade_report)\n    >>> print(\"Average grades:\", grade_report.groupby('Student')['Grade'].apply(statistics.mean))\n    \"\"\"\n    student_cycle = cycle(STUDENTS)\n    grade_data = []\n\n    for _ in range(n_grades):\n        student = next(student_cycle)\n        grade = randint(min(GRADES), max(GRADES))\n        grade_data.append([student, grade])\n\n    grade_df = pd.DataFrame(grade_data, columns=['Student', 'Grade'])\n\n    return grade_df\n```"}
{"seed": "{\"task_id\": 19339, \"prompt\": \"def f_19339():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"zip(*[('a', 1), ('b', 2), ('c', 3), ('d', 4), ('e',)])\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert list(candidate()) == [('a', 'b', 'c', 'd', 'e')]\\n\"], \"entry_point\": \"f_19339\", \"intent\": \"unzip list `[('a', 1), ('b', 2), ('c', 3), ('d', 4), ('e', )]`\", \"library\": []}", "generation": "Scenario 1:\npandas, numpy, itertools\n```python\nimport pandas as pd\nimport numpy as np\nimport itertools\n\n# Constants\nDATA_LIST = [('a', 1, 2.1), ('b', 2, 3.2), ('c', 3, 4.3), ('d', 4, 5.4), ('e', 5, 6.5)]\n\ndef unzip_and_process_data():\n    \"\"\"\n    Unzip the list [('a', 1, 2.1), ('b', 2, 3.2), ('c', 3, 4.3), ('d', 4, 5.4), ('e', 5, 6.5)] \n    and calculate the mean of the numerical values for each position. The results are stored in a pandas DataFrame.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the mean values.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - itertools\n\n    Example:\n    >>> df = unzip_and_process_data()\n    >>> print(df)\n    \"\"\"\n    unzipped_data = list(itertools.zip_longest(*DATA_LIST, fillvalue=np.nan))\n    mean_values = [np.nanmean(column) for column in unzipped_data[1:]]\n    df = pd.DataFrame(mean_values, columns=['Mean Value'], index=['Position {}'.format(i+1) for i in range(len(mean_values))])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, matplotlib.pyplot, itertools\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport itertools\n\n# Constants\nDATA_LIST = [('a', 1, 2), ('b', 2, 3), ('c', 3, 4), ('d', 4, 5), ('e', 5, 6)]\n\ndef unzip_and_plot_data():\n    \"\"\"\n    Unzip the list [('a', 1, 2), ('b', 2, 3), ('c', 3, 4), ('d', 4, 5), ('e', 5, 6)] \n    and plot the numerical values for each position.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - itertools\n\n    Example:\n    >>> unzip_and_plot_data()\n    \"\"\"\n    unzipped_data = list(itertools.zip_longest(*DATA_LIST, fillvalue=np.nan))\n    for i, column in enumerate(unzipped_data[1:], start=1):\n        plt.plot(column, label='Position {}'.format(i))\n    plt.legend()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, os, itertools\n```python\nimport numpy as np\nimport os\nimport itertools\n\n# Constants\nDATA_LIST = [('a', 1, 2), ('b', 2, 3), ('c', 3, 4), ('d', 4, 5), ('e', 5, 6)]\nFILE_NAME = 'mean_values.txt'\n\ndef unzip_and_save_data():\n    \"\"\"\n    Unzip the list [('a', 1, 2), ('b', 2, 3), ('c', 3, 4), ('d', 4, 5), ('e', 5, 6)] \n    and calculate the mean of the numerical values for each position. The results are saved in a text file.\n\n    Requirements:\n    - numpy\n    - os\n    - itertools\n\n    Example:\n    >>> unzip_and_save_data()\n    \"\"\"\n    unzipped_data = list(itertools.zip_longest(*DATA_LIST, fillvalue=np.nan))\n    mean_values = [np.nanmean(column) for column in unzipped_data[1:]]\n\n    with open(FILE_NAME, 'w') as f:\n        for i, mean_value in enumerate(mean_values, start=1):\n            f.write('Position {}: {}\\n'.format(i, mean_value))\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, itertools, sys\n```python\nimport numpy as np\nimport itertools\nimport sys\n\n# Constants\nDATA_LIST = [('a', 1, 2), ('b', 2, 3), ('c', 3, 4), ('d', 4, 5), ('e', 5, 6)]\n\ndef unzip_and_print_data():\n    \"\"\"\n    Unzip the list [('a', 1, 2), ('b', 2, 3), ('c', 3, 4), ('d', 4, 5), ('e', 5, 6)] \n    and calculate the mean of the numerical values for each position. The results are printed to the standard output.\n\n    Requirements:\n    - numpy\n    - itertools\n    - sys\n\n    Example:\n    >>> unzip_and_print_data()\n    \"\"\"\n    unzipped_data = list(itertools.zip_longest(*DATA_LIST, fillvalue=np.nan))\n    mean_values = [np.nanmean(column) for column in unzipped_data[1:]]\n\n    for i, mean_value in enumerate(mean_values, start=1):\n        sys.stdout.write('Position {}: {}\\n'.format(i, mean_value))\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, itertools, json\n```python\nimport numpy as np\nimport itertools\nimport json\n\n# Constants\nDATA_LIST = [('a', 1, 2), ('b', 2, 3), ('c', 3, 4), ('d', 4, 5), ('e', 5, 6)]\nJSON_FILE_NAME = 'mean_values.json'\n\ndef unzip_and_export_data():\n    \"\"\"\n    Unzip the list [('a', 1, 2), ('b', 2, 3), ('c', 3, 4), ('d', 4, 5), ('e', 5, 6)] \n    and calculate the mean of the numerical values for each position. The results are exported to a JSON file.\n\n    Requirements:\n    - numpy\n    - itertools\n    - json\n\n    Example:\n    >>> unzip_and_export_data()\n    \"\"\"\n    unzipped_data = list(itertools.zip_longest(*DATA_LIST, fillvalue=np.nan))\n    mean_values = [np.nanmean(column) for column in unzipped_data[1:]]\n\n    with open(JSON_FILE_NAME, 'w') as f:\n        json.dump({'Position {}'.format(i+1): mean_value for i, mean_value in enumerate(mean_values)}, f)\n```\n`GPT_ODEX_BREAK`"}
{"seed": "{\"task_id\": 19339, \"prompt\": \"def f_19339():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"list(zip_longest(('a', 1), ('b', 2), ('c', 3), ('d', 4), ('e',)))\", \"test_start\": \"\\nfrom itertools import zip_longest\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert(candidate() == [('a', 'b', 'c', 'd', 'e'), (1, 2, 3, 4, None)])\\n\"], \"entry_point\": \"f_19339\", \"intent\": \"unzip list `[('a', 1), ('b', 2), ('c', 3), ('d', 4), ('e', )]` and fill empty results with None\", \"library\": [\"itertools\"]}", "generation": "Scenario 1:\n```python\nimport csv\nimport os\nfrom itertools import zip_longest\n\n# Constants\nCSV_PATH = 'sample.csv'\nFIELDS = ['Name', 'Age', 'Occupation']\n\ndef write_to_csv(data):\n    \"\"\"\n    Write a list of tuples to a CSV file using csv.writer. The tuples are \n    unzipped and empty results are filled with None.\n\n    Parameters:\n    data (list): A list of tuples.\n\n    Returns:\n    str: The path of the CSV file.\n\n    Requirements:\n    - csv\n    - os\n    - itertools\n\n    Example:\n    >>> data = [('John', 25, 'Engineer'), ('Alice', 30, 'Doctor'), ('Bob', )]\n    >>> write_to_csv(data)\n    'sample.csv'\n    \"\"\"\n    with open(CSV_PATH, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(FIELDS)\n        writer.writerows(zip_longest(*data, fillvalue=None))\n    \n    return os.path.abspath(CSV_PATH)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\n```python\nimport numpy as np\nfrom itertools import zip_longest\nfrom scipy.spatial import distance\n\n# Constants\nPOINTS = [(1, 2), (3, 4), (5, 6), (7, 8)]\n\ndef calculate_euclidean_distance(points):\n    \"\"\"\n    Calculate the Euclidean distance between consecutive points in a list.\n\n    Parameters:\n    points (list): A list of tuples representing points in 2D space.\n\n    Returns:\n    list: A list of distances.\n\n    Requirements:\n    - numpy\n    - itertools\n    - scipy.spatial\n\n    Example:\n    >>> calculate_euclidean_distance(POINTS)\n    [2.8284271247461903, 2.8284271247461903, 2.8284271247461903]\n    \"\"\"\n    distances = []\n    for point1, point2 in zip_longest(points, points[1:]):\n        if point2 is not None:\n            distances.append(distance.euclidean(point1, point2))\n            \n    return distances\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\n```python\nimport matplotlib.pyplot as plt\nfrom itertools import zip_longest\n\n# Constants\nCOLORS = ['red', 'green', 'blue', 'yellow', 'purple']\n\ndef plot_data(data, labels):\n    \"\"\"\n    Plot a list of data with different colors. If there are more data series \n    than colors, cycle through the colors.\n\n    Parameters:\n    data (list): A list of lists, each representing a series of data.\n    labels (list): A list of labels for the data series.\n\n    Returns:\n    None\n\n    Requirements:\n    - matplotlib.pyplot\n    - itertools\n\n    Example:\n    >>> data = [[1, 2, 3, 4, 5], [2, 3, 4, 5, 6], [3, 4, 5, 6, 7]]\n    >>> labels = ['Series 1', 'Series 2', 'Series 3']\n    >>> plot_data(data, labels)\n    \"\"\"\n    for series, label, color in zip_longest(data, labels, COLORS, fillvalue='black'):\n        plt.plot(series, label=label, color=color)\n        \n    plt.legend()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\n```python\nfrom itertools import zip_longest\nimport pandas as pd\n\n# Constants\nCOLUMNS = ['Name', 'Age', 'Occupation']\n\ndef create_dataframe(data):\n    \"\"\"\n    Create a pandas DataFrame from a list of tuples. The tuples are unzipped\n    and empty results are filled with None.\n\n    Parameters:\n    data (list): A list of tuples.\n\n    Returns:\n    DataFrame: A pandas DataFrame.\n\n    Requirements:\n    - pandas\n    - itertools\n\n    Example:\n    >>> data = [('John', 25, 'Engineer'), ('Alice', 30, 'Doctor'), ('Bob', )]\n    >>> df = create_dataframe(data)\n    >>> print(df)\n    \"\"\"\n    df = pd.DataFrame(list(zip_longest(*data, fillvalue=None)), columns=COLUMNS)\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\n```python\nimport numpy as np\nfrom itertools import zip_longest\n\n# Constants\nARRAYS = [np.array([1, 2, 3]), np.array([4, 5, 6]), np.array([7, 8, 9])]\n\ndef calculate_dot_product(arrays):\n    \"\"\"\n    Calculate the dot product of multiple arrays. If there are uneven arrays, \n    fill the empty results with 1.\n\n    Parameters:\n    arrays (list): A list of numpy arrays.\n\n    Returns:\n    numpy array: The result of the dot product.\n\n    Requirements:\n    - numpy\n    - itertools\n\n    Example:\n    >>> calculate_dot_product(ARRAYS)\n    array([32, 64, 96])\n    \"\"\"\n    product = None\n    for array1, array2 in zip_longest(arrays, arrays[1:], fillvalue=np.array([1])):\n        if product is None:\n            product = np.dot(array1, array2)\n        else:\n            product = np.dot(product, array2)\n            \n    return product\n```"}
{"seed": "{\"task_id\": 1960516, \"prompt\": \"def f_1960516():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"json.dumps('3.9')\", \"test_start\": \"\\nimport json\\n\\ndef check(candidate):\", \"test\": [\"\\n    data = candidate()\\n    assert json.loads(data) == '3.9'\\n\"], \"entry_point\": \"f_1960516\", \"intent\": \"encode `Decimal('3.9')` to a JSON string\", \"library\": [\"json\"]}", "generation": "Scenario 1: \njson, os, pandas, datetime, pytz\n```python\nimport json\nimport os\nfrom pandas.io.json import json_normalize\nimport pandas as pd\nfrom datetime import datetime\nimport pytz\n\n# Constants\nJSON_FILE_PATH = 'data.json'\n\ndef save_and_read_json_data(utc_datetime, data):\n    \"\"\"\n    Save the given data as a JSON file, then read the JSON file and return a DataFrame.\n    \n    Parameters:\n    utc_datetime (datetime): The datetime in UTC.\n    data (dict): The data to be saved.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the data from the JSON file.\n    \n    Requirements:\n    - json\n    - os\n    - pandas\n    - datetime\n    - pytz\n    \n    Example:\n    >>> utc_time = datetime(2023, 6, 15, 12, 0, 0, tzinfo=pytz.UTC)\n    >>> data = {'utc_time': str(utc_time), 'temperature': 23.5}\n    >>> df = save_and_read_json_data(utc_time, data)\n    >>> print(df)\n    \"\"\"\n    # Save the data as a JSON file\n    with open(JSON_FILE_PATH, 'w') as f:\n        json.dump(data, f)\n\n    # Read the JSON file and return a DataFrame\n    with open(JSON_FILE_PATH, 'r') as f:\n        data = json.load(f)\n        df = pd.json_normalize(data)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\njson, datetime, pytz, decimal, math\n```python\nimport json\nfrom datetime import datetime\nimport pytz\nfrom decimal import Decimal\nimport math\n\n# Constants\nPRECISION = 2\n\ndef calculate_and_encode_square_root(utc_datetime, decimal_value):\n    \"\"\"\n    Calculate the square root of the given decimal value up to a certain precision, \n    then encode the result as a JSON string.\n    \n    Parameters:\n    utc_datetime (datetime): The datetime in UTC.\n    decimal_value (Decimal): The decimal value.\n    \n    Returns:\n    str: The square root of the decimal value encoded as a JSON string.\n    \n    Requirements:\n    - json\n    - datetime\n    - pytz\n    - decimal\n    - math\n    \n    Example:\n    >>> utc_time = datetime(2023, 6, 15, 12, 0, 0, tzinfo=pytz.UTC)\n    >>> decimal_value = Decimal('3.9')\n    >>> json_str = calculate_and_encode_square_root(utc_time, decimal_value)\n    >>> print(json_str)\n    \"\"\"\n    # Calculate the square root of the decimal value\n    square_root = round(math.sqrt(decimal_value), PRECISION)\n    \n    # Encode the result as a JSON string\n    json_str = json.dumps(str(square_root))\n    \n    return json_str\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \njson, datetime, pytz, random, hashlib\n```python\nimport json\nfrom datetime import datetime\nimport pytz\nimport random\nimport hashlib\n\n# Constants\nSALT = 'salt'\nPASSWORD_LENGTH = 10\n\ndef generate_and_encode_password(utc_datetime):\n    \"\"\"\n    Generate a random password, then hash and encode the password as a JSON string.\n    \n    Parameters:\n    utc_datetime (datetime): The datetime in UTC.\n    \n    Returns:\n    str: The hashed password encoded as a JSON string.\n    \n    Requirements:\n    - json\n    - datetime\n    - pytz\n    - random\n    - hashlib\n    \n    Example:\n    >>> utc_time = datetime(2023, 6, 15, 12, 0, 0, tzinfo=pytz.UTC)\n    >>> password_json_str = generate_and_encode_password(utc_time)\n    >>> print(password_json_str)\n    \"\"\"\n    # Generate a random password\n    password = ''.join(random.choice('abcdefghijklmnopqrstuvwxyz0123456789') for _ in range(PASSWORD_LENGTH))\n    \n    # Hash the password\n    hashed_password = hashlib.sha256((password + SALT).encode('utf-8')).hexdigest()\n    \n    # Encode the hashed password as a JSON string\n    password_json_str = json.dumps(hashed_password)\n    \n    return password_json_str\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \njson, pandas, datetime, pytz, random\n```python\nimport json\nfrom pandas.io.json import json_normalize\nimport pandas as pd\nfrom datetime import datetime\nimport pytz\nimport random\n\n# Constants\nDATA = [{'name': 'John', 'age': 30, 'city': 'New York'},\n        {'name': 'Peter', 'age': 35, 'city': 'London'},\n        {'name': 'Susan', 'age': 25, 'city': 'Sydney'}]\n\ndef choose_and_encode_random_person(utc_datetime):\n    \"\"\"\n    Choose a random person from a list of people, then encode the person's data as a JSON string.\n    \n    Parameters:\n    utc_datetime (datetime): The datetime in UTC.\n    \n    Returns:\n    str: The person's data encoded as a JSON string.\n    \n    Requirements:\n    - json\n    - pandas\n    - datetime\n    - pytz\n    - random\n    \n    Example:\n    >>> utc_time = datetime(2023, 6, 15, 12, 0, 0, tzinfo=pytz.UTC)\n    >>> person_json_str = choose_and_encode_random_person(utc_time)\n    >>> print(person_json_str)\n    \"\"\"\n    # Choose a random person\n    person = random.choice(DATA)\n    \n    # Encode the person's data as a JSON string\n    person_json_str = json.dumps(person)\n    \n    return person_json_str\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \njson, requests, datetime, pytz\n```python\nimport json\nimport requests\nfrom datetime import datetime\nimport pytz\n\n# Constants\nURL = 'https://api.example.com/data'\n\ndef fetch_and_encode_data(utc_datetime):\n    \"\"\"\n    Fetch data from a URL, then encode the data as a JSON string.\n    \n    Parameters:\n    utc_datetime (datetime): The datetime in UTC.\n    \n    Returns:\n    str: The data encoded as a JSON string.\n    \n    Requirements:\n    - json\n    - requests\n    - datetime\n    - pytz\n    \n    Example:\n    >>> utc_time = datetime(2023, 6, 15, 12, 0, 0, tzinfo=pytz.UTC)\n    >>> data_json_str = fetch_and_encode_data(utc_time)\n    >>> print(data_json_str)\n    \"\"\"\n    # Fetch the data from the URL\n    response = requests.get(URL)\n    data = response.json()\n    \n    # Encode the data as a JSON string\n    data_json_str = json.dumps(data)\n    \n    return data_json_str\n```\n"}
{"seed": "{\"task_id\": 1024847, \"prompt\": \"def f_1024847(d):\\n\\t\", \"suffix\": \"\\n\\treturn d\", \"canonical_solution\": \"d['mynewkey'] = 'mynewvalue'\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate({'key': 'value'}) == {'key': 'value', 'mynewkey': 'mynewvalue'}\\n\"], \"entry_point\": \"f_1024847\", \"intent\": \"Add key \\\"mynewkey\\\" to dictionary `d` with value \\\"mynewvalue\\\"\", \"library\": []}", "generation": "Scenario 1: \npandas, json, os, glob\n```python\nimport pandas as pd\nimport json\nimport os\nimport glob\n\n# Constants\nKEY = 'mynewkey'\nVALUE = 'mynewvalue'\n\ndef add_key_to_json_files(directory):\n    \"\"\"\n    Add a new key-value pair to all JSON files in a specified directory and save the updated JSON files.\n\n    Parameters:\n    directory (str): The directory containing the JSON files.\n\n    Returns:\n    int: The number of JSON files updated.\n\n    Requirements:\n    - pandas\n    - json\n    - os\n    - glob\n\n    Example:\n    >>> add_key_to_json_files('./json_files')\n    \"\"\"\n    files = glob.glob(os.path.join(directory, '*.json'))\n    updated_files = 0\n\n    for file in files:\n        with open(file, 'r+') as f:\n            data = json.load(f)\n            if KEY not in data:\n                data[KEY] = VALUE\n                f.seek(0)\n                json.dump(data, f)\n                updated_files += 1\n\n    return updated_files\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncollections, numpy, seaborn, matplotlib\n```python\nimport collections\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nKEY = 'mynewkey'\nVALUE = 'mynewvalue'\n\ndef add_key_to_dict_and_plot_distribution(dictionary):\n    \"\"\"\n    Add a new key-value pair to the dictionary and plot the distribution of values.\n\n    Parameters:\n    dictionary (dict): The dictionary to be updated.\n\n    Returns:\n    dict: The updated dictionary.\n\n    Requirements:\n    - collections\n    - numpy\n    - seaborn\n    - matplotlib\n\n    Example:\n    >>> add_key_to_dict_and_plot_distribution({'key1': 10, 'key2': 20, 'key3': 30})\n    \"\"\"\n    dictionary[KEY] = VALUE\n\n    counter = collections.Counter(dictionary.values())\n    values, frequencies = zip(*counter.items())\n\n    plt.figure(figsize=(10, 6))\n    sns.barplot(list(values), list(frequencies), palette='viridis')\n    plt.title('Distribution of Values in the Dictionary')\n    plt.xlabel('Values')\n    plt.ylabel('Frequency')\n    plt.show()\n\n    return dictionary\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nrandom, numpy, matplotlib\n```python\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nKEY = 'mynewkey'\nVALUE = 'mynewvalue'\n\ndef add_key_to_dict_and_generate_random_data(dictionary, n=100):\n    \"\"\"\n    Add a new key-value pair to the dictionary and generate a random dataset of size n \n    that follows a normal distribution where the mean and standard deviation are \n    set to the value associated with the new key.\n\n    Parameters:\n    dictionary (dict): The dictionary to be updated.\n    n (int): The size of the random dataset to be generated.\n\n    Returns:\n    np.array: The generated random dataset.\n\n    Requirements:\n    - random\n    - numpy\n    - matplotlib\n\n    Example:\n    >>> add_key_to_dict_and_generate_random_data({'key1': 10, 'key2': 20}, n=500)\n    \"\"\"\n    dictionary[KEY] = VALUE\n\n    data = np.random.normal(loc=dictionary[KEY], scale=dictionary[KEY], size=n)\n\n    plt.hist(data, bins=30, density=True)\n    plt.title('Random Data Distribution')\n    plt.show()\n\n    return data\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, sklearn, matplotlib\n```python\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\n\n# Constants\nKEY = 'mynewkey'\nVALUE = 'mynewvalue'\n\ndef add_key_to_dict_and_perform_regression(dictionary, dataset):\n    \"\"\"\n    Add a new key-value pair to the dictionary and perform linear regression on a \n    dataset using the new key as a feature.\n\n    Parameters:\n    dictionary (dict): The dictionary to be updated.\n    dataset (pandas.DataFrame): The dataset for regression.\n\n    Returns:\n    float: The R-squared score of the model.\n\n    Requirements:\n    - pandas\n    - sklearn.model_selection\n    - sklearn.linear_model\n    - matplotlib\n\n    Example:\n    >>> add_key_to_dict_and_perform_regression({'key1': 10, 'key2': 20}, my_dataset)\n    \"\"\"\n    dictionary[KEY] = VALUE\n\n    X = dataset[dictionary.keys()]\n    y = dataset['target']\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    y_pred = model.predict(X_test)\n\n    plt.scatter(y_test, y_pred)\n    plt.xlabel('True Values')\n    plt.ylabel('Predictions')\n    plt.title('Linear Regression Results')\n    plt.grid(True)\n    plt.show()\n\n    score = model.score(X_test, y_test)\n\n    return score\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nKEY = 'mynewkey'\nVALUE = 'mynewvalue'\n\ndef add_key_to_dict_and_create_dataframe(dictionary):\n    \"\"\"\n    Add a new key-value pair to the dictionary and create a pandas DataFrame \n    using the dictionary keys as column names and the values as the values of the first row.\n\n    Parameters:\n    dictionary (dict): The dictionary to be updated.\n\n    Returns:\n    pandas.DataFrame: The created DataFrame.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib\n\n    Example:\n    >>> add_key_to_dict_and_create_dataframe({'key1': 10, 'key2': 20})\n    \"\"\"\n    dictionary[KEY] = VALUE\n\n    df = pd.DataFrame([dictionary])\n\n    df.plot(kind='bar')\n    plt.title('Bar Plot of Dictionary Values')\n    plt.xlabel('Row Index')\n    plt.ylabel('Value')\n    plt.show()\n\n    return df\n```"}
{"seed": "{\"task_id\": 1024847, \"prompt\": \"def f_1024847(data):\\n\\t\", \"suffix\": \"\\n\\treturn data\", \"canonical_solution\": \"data.update({'a': 1, })\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate({'key': 'value'}) == {'key': 'value', 'a': 1}\\n\", \"\\n    assert candidate({'key': 'value', 'a' : 2}) == {'key': 'value', 'a': 1}\\n\"], \"entry_point\": \"f_1024847\", \"intent\": \"Add key 'a' to dictionary `data` with value 1\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, matplotlib, sklearn\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Constants\nFEATURES = ['Feature1', 'Feature2', 'Feature3', 'Feature4', 'Feature5']\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the given data by adding a new feature 'a' with value 1, scaling \n    the numerical features to range [0, 1] and plotting a correlation heatmap.\n    \n    Parameters:\n    data (dict): The input data as a dictionary.\n    \n    Returns:\n    DataFrame: A pandas DataFrame after preprocessing.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib\n    - sklearn.preprocessing.MinMaxScaler\n\n    Example:\n    >>> data = {'Feature1': [1, 2, 3], 'Feature2': [2, 3, 4], 'Feature3': [3, 4, 5],\n                'Feature4': [4, 5, 6], 'Feature5': [5, 6, 7]}\n    >>> preprocess_data(data)\n    \"\"\"\n    # Convert data to DataFrame\n    df = pd.DataFrame(data)\n\n    # Add new feature 'a' with value 1\n    df['a'] = 1\n\n    # Scale the numerical features to range [0, 1]\n    scaler = MinMaxScaler()\n    df[FEATURES] = scaler.fit_transform(df[FEATURES])\n\n    # Plot a correlation heatmap\n    corr = df.corr()\n    plt.figure(figsize=(10, 8))\n    plt.matshow(corr, fignum=1)\n    plt.xticks(range(len(corr.columns)), corr.columns)\n    plt.yticks(range(len(corr.columns)), corr.columns)\n    plt.colorbar()\n    plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \ncollections, re, json, os\n```python\nimport collections\nimport re\nimport json\nimport os\n\n# Constants\nJSON_FILE_NAME = 'data.json'\n\ndef update_data(data):\n    \"\"\"\n    Add a new key 'a' with value 1 to the given dictionary `data`, \n    calculate the frequency of values in `data` and save the updated \n    `data` and the frequency dictionary into a JSON file.\n\n    Parameters:\n    data (dict): The input data as a dictionary.\n\n    Returns:\n    str: The path of the JSON file.\n\n    Requirements:\n    - collections\n    - re\n    - json\n    - os\n\n    Example:\n    >>> data = {'key1': 'value1', 'key2': 'value2', 'key3': 'value1'}\n    >>> update_data(data)\n    \"\"\"\n    # Add new key 'a' with value 1\n    data['a'] = 1\n\n    # Calculate the frequency of values in `data`\n    freq = collections.Counter(data.values())\n\n    # Save the updated `data` and the `freq` into a JSON file\n    json_data = {'data': data, 'freq': dict(freq)}\n    json_file_path = os.path.join(os.getcwd(), JSON_FILE_NAME)\n    with open(json_file_path, 'w') as json_file:\n        json.dump(json_data, json_file)\n\n    return json_file_path\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnetworkx, matplotlib, random, numpy\n```python\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport random\nimport numpy as np\n\n# Constants\nN_NODES = 10\n\ndef generate_and_visualize_graph(data):\n    \"\"\"\n    Add a new key 'a' with value 1 to the given dictionary `data`,\n    generate a random graph with `data` as node attributes, and visualize the graph.\n\n    Parameters:\n    data (dict): The input data as a dictionary.\n\n    Returns:\n    Graph: The generated NetworkX graph.\n\n    Requirements:\n    - networkx\n    - matplotlib\n    - random\n    - numpy\n\n    Example:\n    >>> data = {'key1': 'value1', 'key2': 'value2', 'key3': 'value1'}\n    >>> graph = generate_and_visualize_graph(data)\n    \"\"\"\n    # Add new key 'a' with value 1\n    data['a'] = 1\n\n    # Generate a random graph with `data` as node attributes\n    G = nx.gnp_random_graph(N_NODES, 0.5, seed=1)\n    for node in G.nodes():\n        G.nodes[node].update(data)\n\n    # Visualize the graph\n    pos = nx.spring_layout(G, seed=1)\n    labels = nx.get_node_attributes(G, 'a')\n    nx.draw(G, pos, with_labels=True, labels=labels)\n    plt.show()\n\n    return G\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, scipy, matplotlib\n```python\nimport numpy as np\nfrom scipy import fftpack\nimport matplotlib.pyplot as plt\n\n# Constants\nSAMPLE_RATE = 8000\n\ndef perform_fft(data):\n    \"\"\"\n    Add a new key 'a' with value 1 to the given dictionary `data`,\n    generate a signal based on the values in `data`, perform a Fast Fourier \n    Transform (FFT) on the signal, and plot the FFT.\n\n    Parameters:\n    data (dict): The input data as a dictionary.\n\n    Returns:\n    ndarray: The FFT of the signal.\n\n    Requirements:\n    - numpy\n    - scipy.fftpack\n    - matplotlib\n\n    Example:\n    >>> data = {'key1': 1, 'key2': 2, 'key3': 3}\n    >>> fft = perform_fft(data)\n    \"\"\"\n    # Add new key 'a' with value 1\n    data['a'] = 1\n\n    # Generate a signal based on the values in `data`\n    signal = np.array(list(data.values()))\n    time = np.linspace(0, 2, 2 * SAMPLE_RATE, False)\n    signal = np.sin(time * signal * np.pi)\n\n    # Perform a Fast Fourier Transform (FFT) on the signal\n    fft = fftpack.fft(signal)\n\n    # Plot the FFT\n    plt.figure(figsize=(12, 6))\n    plt.plot(np.abs(fft))\n    plt.title('FFT of the Signal')\n    plt.xlabel('Frequency [Hz]')\n    plt.ylabel('Frequency Spectrum Magnitude')\n    plt.show()\n\n    return fft\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \npandas, seaborn, sklearn, matplotlib\n```python\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\n\n# Constants\nTARGET_VARIABLE = 'a'\n\ndef perform_linear_regression(data):\n    \"\"\"\n    Add a new key 'a' with value 1 to the given dictionary `data`, convert `data`\n    into a pandas DataFrame, perform a train-test split, fit a linear regression \n    model, and plot the regression line.\n\n    Parameters:\n    data (dict): The input data as a dictionary.\n\n    Returns:\n    LinearRegression: The fitted linear regression model.\n\n    Requirements:\n    - pandas\n    - seaborn\n    - sklearn.model_selection.train_test_split\n    - sklearn.linear_model.LinearRegression\n    - matplotlib\n\n    Example:\n    >>> data = {'x': [1, 2, 3, 4, 5], 'y': [2, 3, 4, 5, 6]}\n    >>> model = perform_linear_regression(data)\n    \"\"\"\n    # Add new key 'a' with value 1\n    data['a'] = 1\n\n    # Convert `data` into a pandas DataFrame\n    df = pd.DataFrame(data)\n\n    # Perform a train-test split\n    X_train, X_test, y_train, y_test = train_test_split(df.drop([TARGET_VARIABLE], axis=1), df[TARGET_VARIABLE], test_size=0.2, random_state=1)\n\n    # Fit a linear regression model\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    # Plot the regression line\n    plt.figure(figsize=(8, 6))\n    sns.regplot(x=X_train, y=y_train, line_kws={'color': 'red'})\n    plt.title('Linear Regression Line')\n    plt.show()\n\n    return model\n```"}
{"seed": "{\"task_id\": 1024847, \"prompt\": \"def f_1024847(data):\\n\\t\", \"suffix\": \"\\n\\treturn data\", \"canonical_solution\": \"data.update(dict(a=1))\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate({'key': 'value'}) == {'key': 'value', 'a': 1}\\n\", \"\\n    assert candidate({'key': 'value', 'a' : 2}) == {'key': 'value', 'a': 1}\\n\"], \"entry_point\": \"f_1024847\", \"intent\": \"Add key 'a' to dictionary `data` with value 1\", \"library\": []}", "generation": "Scenario 1:\npandas, csv, json, matplotlib\n```python\nimport pandas as pd\nimport csv\nimport json\nimport matplotlib.pyplot as plt\n\n# Constants\nCSV_FILE_PATH = 'data.csv'\nJSON_FILE_PATH = 'data.json'\n\ndef process_data(data_dict):\n    \"\"\"\n    Add key 'a' to dictionary `data_dict` with value 1, convert it to DataFrame, save it to a CSV and JSON file,\n    and plot a bar graph of the data.\n\n    Parameters:\n    data_dict (dict): The dictionary to be processed.\n\n    Returns:\n    dict: The processed dictionary.\n\n    Requirements:\n    - pandas\n    - csv\n    - json\n    - matplotlib.pyplot\n\n    Example:\n    >>> process_data({'key': 'value'})\n    \"\"\"\n    # Add the key 'a' with value 1\n    data_dict.update(dict(a=1))\n\n    # Convert the dictionary to DataFrame\n    df = pd.DataFrame([data_dict])\n\n    # Save the DataFrame to a CSV file\n    df.to_csv(CSV_FILE_PATH, index=False)\n\n    # Save the DataFrame to a JSON file\n    df.to_json(JSON_FILE_PATH, orient='records')\n\n    # Plot a bar graph of the data\n    df.plot(kind='bar')\n    plt.show()\n\n    return data_dict\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncollections, operator, os, shutil\n```python\nimport collections\nimport operator\nimport os\nimport shutil\n\n# Constants\nDIRECTORY_PATH = 'dir_path'\nBACKUP_DIRECTORY_PATH = 'backup_dir_path'\n\ndef add_key_and_backup_files(data_dict):\n    \"\"\"\n    Add key 'a' to dictionary `data_dict` with value 1, count the frequency of the values, sort by the frequency,\n    and backup files from a directory to another directory.\n\n    Parameters:\n    data_dict (dict): The dictionary to be processed.\n\n    Returns:\n    dict: The processed dictionary.\n\n    Requirements:\n    - collections\n    - operator\n    - os\n    - shutil\n\n    Example:\n    >>> add_key_and_backup_files({'key': 'value'})\n    \"\"\"\n    # Add the key 'a' with value 1\n    data_dict.update(dict(a=1))\n\n    # Count the frequency of the values\n    counter = collections.Counter(data_dict.values())\n\n    # Sort the dictionary by the frequency\n    sorted_dict = sorted(counter.items(), key=operator.itemgetter(1), reverse=True)\n\n    # Backup files\n    if os.path.isdir(DIRECTORY_PATH):\n        shutil.copytree(DIRECTORY_PATH, BACKUP_DIRECTORY_PATH)\n\n    return data_dict\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, scipy, sklearn, matplotlib\n```python\nimport numpy as np\nfrom scipy import stats\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\n\n# Constants\nSCALER_RANGE = (0, 1)\n\ndef add_key_and_analyze_data(data_dict):\n    \"\"\"\n    Add key 'a' to dictionary `data_dict` with value 1, perform statistical analysis on the values,\n    normalize the values, and plot a histogram.\n\n    Parameters:\n    data_dict (dict): The dictionary to be processed.\n\n    Returns:\n    dict: The processed dictionary.\n\n    Requirements:\n    - numpy\n    - scipy\n    - sklearn.preprocessing\n    - matplotlib.pyplot\n\n    Example:\n    >>> add_key_and_analyze_data({'key': 5, 'another_key': 10})\n    \"\"\"\n    # Add the key 'a' with value 1\n    data_dict.update(dict(a=1))\n\n    # Convert the values to a numpy array\n    values = np.array(list(data_dict.values()))\n\n    # Perform statistical analysis\n    mean = np.mean(values)\n    median = np.median(values)\n    mode = stats.mode(values)\n\n    # Normalize the values\n    scaler = MinMaxScaler(feature_range=SCALER_RANGE)\n    normalized_values = scaler.fit_transform(values.reshape(-1, 1))\n\n    # Plot a histogram of the normalized values\n    plt.hist(normalized_values)\n    plt.show()\n\n    return data_dict\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nrandom, string, hashlib, time\n```python\nimport random\nimport string\nimport hashlib\nimport time\n\n# Constants\nSALT_LENGTH = 5\n\ndef add_key_and_hash_data(data_dict):\n    \"\"\"\n    Add key 'a' to dictionary `data_dict` with value 1, generate a random salt, concatenate the salt with the values,\n    hash the concatenated string, and timestamp the process.\n\n    Parameters:\n    data_dict (dict): The dictionary to be processed.\n\n    Returns:\n    dict: The processed dictionary.\n\n    Requirements:\n    - random\n    - string\n    - hashlib\n    - time\n\n    Example:\n    >>> add_key_and_hash_data({'key': 'value'})\n    \"\"\"\n    # Add the key 'a' with value 1\n    data_dict.update(dict(a=1))\n\n    # Generate a random salt\n    salt = ''.join(random.choice(string.ascii_lowercase) for _ in range(SALT_LENGTH))\n\n    # Concatenate the salt with the values and hash the concatenated string\n    for key in data_dict.keys():\n        data_dict[key] = hashlib.sha256((str(data_dict[key]) + salt).encode()).hexdigest()\n\n    # Timestamp the process\n    data_dict['timestamp'] = time.time()\n\n    return data_dict\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nrequests, bs4, re, csv\n```python\nimport requests\nfrom bs4 import BeautifulSoup\nimport re\nimport csv\n\n# Constants\nURL = 'https://example.com'\nCSV_FILE_PATH = 'data.csv'\n\ndef add_key_and_scrape_website(data_dict):\n    \"\"\"\n    Add key 'a' to dictionary `data_dict` with value 1, scrape a website, find all links on the website,\n    and write the links to a CSV file.\n\n    Parameters:\n    data_dict (dict): The dictionary to be processed.\n\n    Returns:\n    dict: The processed dictionary.\n\n    Requirements:\n    - requests\n    - bs4.BeautifulSoup\n    - re\n    - csv\n\n    Example:\n    >>> add_key_and_scrape_website({'key': 'value'})\n    \"\"\"\n    # Add the key 'a' with value 1\n    data_dict.update(dict(a=1))\n\n    # Scrape the website\n    response = requests.get(URL)\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    # Find all links on the website\n    links = [a['href'] for a in soup.find_all('a', href=re.compile('^http'))]\n\n    # Write the links to a CSV file\n    with open(CSV_FILE_PATH, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerows(links)\n\n    return data_dict\n```"}
{"seed": "{\"task_id\": 1024847, \"prompt\": \"def f_1024847(data):\\n\\t\", \"suffix\": \"\\n\\treturn data\", \"canonical_solution\": \"data.update(a=1)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate({'key': 'value'}) == {'key': 'value', 'a': 1}\\n\", \"\\n    assert candidate({'key': 'value', 'a' : 2}) == {'key': 'value', 'a': 1}\\n\"], \"entry_point\": \"f_1024847\", \"intent\": \"Add key 'a' to dictionary `data` with value 1\", \"library\": []}", "generation": "Scenario 1:\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\n# Constants\nPCA_COMPONENTS = 2\n\ndef standardize_and_reduce(data):\n    \"\"\"\n    Standardize the given DataFrame and then apply PCA to reduce its dimensionality.\n\n    Parameters:\n    data (DataFrame): The input data.\n\n    Returns:\n    DataFrame: The transformed data.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.preprocessing.StandardScaler\n    - sklearn.decomposition.PCA\n\n    Example:\n    >>> data = pd.DataFrame(np.random.randint(0,100,size=(100, 4)), columns=list('ABCD'))\n    >>> transformed_data = standardize_and_reduce(data)\n    >>> print(transformed_data)\n    \"\"\"\n    scaler = StandardScaler()\n    data_scaled = scaler.fit_transform(data)\n\n    pca = PCA(n_components=PCA_COMPONENTS)\n    data_reduced = pca.fit_transform(data_scaled)\n\n    return pd.DataFrame(data_reduced)\n\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\n# Constants\nTEST_SIZE = 0.2\nRANDOM_STATE = 42\n\ndef train_and_evaluate(data, target):\n    \"\"\"\n    Split the given DataFrame into train and test sets, then train a RandomForestRegressor \n    and evaluate it using mean squared error.\n\n    Parameters:\n    data (DataFrame): The input data.\n    target (str): The target column.\n\n    Returns:\n    float: The mean squared error of the model on the test set.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.model_selection.train_test_split\n    - sklearn.ensemble.RandomForestRegressor\n    - sklearn.metrics.mean_squared_error\n\n    Example:\n    >>> data = pd.DataFrame(np.random.randint(0,100,size=(100, 5)), columns=list('ABCDE'))\n    >>> mse = train_and_evaluate(data, 'E')\n    >>> print(mse)\n    \"\"\"\n    X = data.drop(target, axis=1)\n    y = data[target]\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n\n    model = RandomForestRegressor()\n    model.fit(X_train, y_train)\n\n    y_pred = model.predict(X_test)\n\n    return mean_squared_error(y_test, y_pred)\n\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\n```python\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nPLOT_SIZE = (10, 10)\n\ndef plot_correlation_matrix(data):\n    \"\"\"\n    Compute and plot the correlation matrix of the given DataFrame.\n\n    Parameters:\n    data (DataFrame): The input data.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - seaborn\n    - matplotlib.pyplot\n\n    Example:\n    >>> data = pd.DataFrame(np.random.randint(0,100,size=(100, 5)), columns=list('ABCDE'))\n    >>> plot_correlation_matrix(data)\n    \"\"\"\n    corr = data.corr()\n\n    plt.figure(figsize=PLOT_SIZE)\n    sns.heatmap(corr, annot=True)\n\n    plt.show()\n\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\n```python\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import chi2_contingency\n\ndef perform_chi_square_test(data, col1, col2):\n    \"\"\"\n    Perform a Chi-Square test of independence between two categorical variables in the given DataFrame.\n\n    Parameters:\n    data (DataFrame): The input data.\n    col1 (str): The first column.\n    col2 (str): The second column.\n\n    Returns:\n    float: The p-value from the Chi-Square test.\n\n    Requirements:\n    - pandas\n    - numpy\n    - scipy.stats.chi2_contingency\n\n    Example:\n    >>> data = pd.DataFrame({\n    ...     'A': np.random.choice(['Yes', 'No'], size=100),\n    ...     'B': np.random.choice(['Yes', 'No'], size=100)\n    ... })\n    >>> p_value = perform_chi_square_test(data, 'A', 'B')\n    >>> print(p_value)\n    \"\"\"\n    contingency_table = pd.crosstab(data[col1], data[col2])\n\n    chi2, p, dof, expected = chi2_contingency(contingency_table)\n\n    return p\n\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cluster import KMeans\n\n# Constants\nN_CLUSTERS = 3\n\ndef perform_kmeans_clustering(data):\n    \"\"\"\n    Perform K-Means clustering on the given DataFrame.\n\n    Parameters:\n    data (DataFrame): The input data.\n\n    Returns:\n    array: The cluster labels for each data point.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.cluster.KMeans\n\n    Example:\n    >>> data = pd.DataFrame(np.random.randint(0,100,size=(100, 5)), columns=list('ABCDE'))\n    >>> labels = perform_kmeans_clustering(data)\n    >>> print(labels)\n    \"\"\"\n    kmeans = KMeans(n_clusters=N_CLUSTERS)\n    kmeans.fit(data)\n\n    return kmeans.labels_\n\n```"}
{"seed": "{\"task_id\": 35837346, \"prompt\": \"def f_35837346(matrix):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"max([max(i) for i in matrix])\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([[1,2,3],[4,5,6],[7,8,9]]) == 9\\n\", \"\\n    assert candidate([[1.3,2.8],[4.2,10],[7.9,8.1,5]]) == 10\\n\"], \"entry_point\": \"f_35837346\", \"intent\": \"find maximal value in matrix `matrix`\", \"library\": []}", "generation": "Scenario 1: \nnumpy, pandas, matplotlib, random\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nCATEGORIES = ['Category1', 'Category2', 'Category3', 'Category4', 'Category5']\n\ndef generate_and_analyse_data(matrix):\n    \"\"\"\n    Generate a pandas DataFrame from the given matrix, add a random category for each row,\n    and return the category with the maximum total sum of the row values. Also, plot a bar graph of \n    the total sums of the row values for each category.\n    \n    Parameters:\n    matrix (list): A list of lists representing a matrix.\n    \n    Returns:\n    str: The category with the maximum total sum of the row values.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - random\n    \n    Example:\n    >>> generate_and_analyse_data([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    \"\"\"\n    df = pd.DataFrame(matrix)\n    df['Category'] = [CATEGORIES[randint(0, len(CATEGORIES)-1)] for _ in range(df.shape[0])]\n    df_grouped = df.groupby('Category').sum()\n    df_grouped['Total'] = df_grouped.sum(axis=1)\n    df_grouped['Total'].plot(kind='bar')\n    plt.show()\n\n    return df_grouped['Total'].idxmax()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, scipy.stats, matplotlib\n```python\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef calculate_and_plot_distribution(matrix):\n    \"\"\"\n    Calculate the distribution of the maximum values of each row in the matrix,\n    plot the histogram and the kernel density estimation of the distribution,\n    and return the skewness and the kurtosis of the distribution.\n    \n    Parameters:\n    matrix (list): A list of lists representing a matrix.\n    \n    Returns:\n    tuple: The skewness and the kurtosis of the distribution.\n    \n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n    \n    Example:\n    >>> calculate_and_plot_distribution([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    \"\"\"\n    max_values = [max(row) for row in matrix]\n    \n    plt.hist(max_values, bins=10, density=True, alpha=0.6, color='g')\n    \n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, np.mean(max_values), np.std(max_values))\n    plt.plot(x, p, 'k', linewidth=2)\n    plt.show()\n\n    skewness = stats.skew(max_values)\n    kurtosis = stats.kurtosis(max_values)\n\n    return skewness, kurtosis\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, pandas, seaborn\n```python\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef analyze_correlation(matrix):\n    \"\"\"\n    Generate a pandas DataFrame from the given matrix, calculate the correlation matrix,\n    and plot a heatmap of the correlation matrix.\n    \n    Parameters:\n    matrix (list): A list of lists representing a matrix.\n    \n    Returns:\n    DataFrame: The correlation matrix.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - seaborn\n    \n    Example:\n    >>> analyze_correlation([[1, 2, 3, 4, 5], [2, 3, 4, 5, 6], [3, 4, 5, 6, 7]])\n    \"\"\"\n    df = pd.DataFrame(matrix, columns=COLUMNS)\n    correlation_matrix = df.corr()\n    sns.heatmap(correlation_matrix, annot=True)\n    plt.show()\n\n    return correlation_matrix\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, pandas, sklearn.preprocessing\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndef standardize_matrix(matrix):\n    \"\"\"\n    Generate a pandas DataFrame from the given matrix, standardize the features (columns)\n    to have zero mean and unit variance, and return the standardized DataFrame.\n    \n    Parameters:\n    matrix (list): A list of lists representing a matrix.\n    \n    Returns:\n    DataFrame: The standardized DataFrame.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.preprocessing.StandardScaler\n    \n    Example:\n    >>> standardize_matrix([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    \"\"\"\n    df = pd.DataFrame(matrix)\n    scaler = StandardScaler()\n    df_standardized = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n\n    return df_standardized\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, pandas, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef plot_cumulative_sum(matrix):\n    \"\"\"\n    Generate a pandas DataFrame from the given matrix, calculate the cumulative sum of each column,\n    and plot the cumulative sums.\n    \n    Parameters:\n    matrix (list): A list of lists representing a matrix.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    \n    Example:\n    >>> plot_cumulative_sum([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    \"\"\"\n    df = pd.DataFrame(matrix)\n    df_cumsum = df.cumsum()\n    df_cumsum.plot()\n    plt.show()\n```"}
{"seed": "{\"task_id\": 20457038, \"prompt\": \"def f_20457038(answer):\\n\\t\", \"suffix\": \"\\n\\treturn answer\", \"canonical_solution\": \"answer = str(round(answer, 2))\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(2.34351) == \\\"2.34\\\"\\n\", \"\\n    assert candidate(99.375) == \\\"99.38\\\"\\n\", \"\\n    assert candidate(4.1) == \\\"4.1\\\"\\n\", \"\\n    assert candidate(3) == \\\"3\\\"\\n\"], \"entry_point\": \"f_20457038\", \"intent\": \"Round number `answer` to 2 precision after the decimal point\", \"library\": []}", "generation": "Scenario 1:\npandas, numpy, math, itertools\n```python\nimport pandas as pd\nimport numpy as np\nimport math\nfrom itertools import combinations\n\ndef find_optimal_portfolio(stock_returns_df, precision=2):\n    \"\"\"\n    Given a DataFrame of historical returns for different stocks, find the portfolio \n    (i.e., the weights of each stock in the portfolio) that minimizes the standard \n    deviation (risk) of the portfolio return, subject to the constraint that the \n    total weight is 1 (i.e., the sum of all weights is 1). The weights should be \n    rounded to the specified precision.\n\n    Parameters:\n    stock_returns_df (DataFrame): A DataFrame where each column represents a different \n                                  stock, and each row contains the historical returns \n                                  for that stock.\n    precision (int): The number of decimal places to which to round the weights.\n\n    Returns:\n    dict: A dictionary where keys are the stock names and values are the optimal \n          weights (rounded to the specified precision).\n\n    Requirements:\n    - pandas\n    - numpy\n    - math\n    - itertools\n\n    Example:\n    >>> stock_returns = pd.DataFrame({\n            'AAPL': [0.15, 0.22, 0.19],\n            'GOOGL': [0.24, 0.29, 0.27],\n            'AMZN': [0.35, 0.33, 0.36],\n        })\n    >>> find_optimal_portfolio(stock_returns)\n    \"\"\"\n    stocks = stock_returns_df.columns\n    n = len(stocks)\n\n    min_std = float('inf')\n    optimal_portfolio = None\n\n    for weights in combinations(np.linspace(0, 1, 1/10**precision), n):\n        if not math.isclose(sum(weights), 1, abs_tol=10**-precision):\n            continue\n        portfolio_return = stock_returns_df.mul(weights).sum(axis=1)\n        portfolio_std = portfolio_return.std()\n        if portfolio_std < min_std:\n            min_std = portfolio_std\n            optimal_portfolio = dict(zip(stocks, weights))\n\n    return optimal_portfolio\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, math, sympy\n```python\nimport numpy as np\nimport math\nfrom sympy import symbols, solve\n\ndef solve_quadratic_equation(precision=2):\n    \"\"\"\n    Solve a quadratic equation in the form of ax^2 + bx + c = 0, where a, b, and c \n    are randomly generated numbers between -10 and 10. The solutions are rounded to \n    the specified precision.\n\n    Parameters:\n    precision (int): The number of decimal places to which to round the solutions.\n\n    Returns:\n    tuple: A tuple of two solutions (rounded to the specified precision).\n\n    Requirements:\n    - numpy\n    - math\n    - sympy\n\n    Example:\n    >>> solve_quadratic_equation()\n    \"\"\"\n    a = np.random.uniform(-10, 10)\n    b = np.random.uniform(-10, 10)\n    c = np.random.uniform(-10, 10)\n\n    x = symbols('x')\n    equation = a * x**2 + b * x + c\n\n    solutions = solve(equation, x)\n    solutions = [round(float(solution), precision) for solution in solutions]\n\n    return tuple(solutions)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, matplotlib, sklearn\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\ndef fit_and_plot_linear_regression(data_df, target_col, precision=2):\n    \"\"\"\n    Fit a linear regression model on the given DataFrame, and plot the regression \n    line along with the original data points. The coefficients of the regression \n    line are rounded to the specified precision.\n\n    Parameters:\n    data_df (DataFrame): A DataFrame where each column represents a different \n                         predictor variable, and the target_col contains the \n                         target variable.\n    target_col (str): The name of the target variable column.\n    precision (int): The number of decimal places to which to round the coefficients.\n\n    Returns:\n    dict: A dictionary where keys are the predictor names and values are the \n          coefficients of the regression line (rounded to the specified precision).\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib\n    - sklearn\n\n    Example:\n    >>> data = pd.DataFrame({\n            'x': [1, 2, 3, 4, 5],\n            'y': [2, 3, 5, 7, 9],\n        })\n    >>> fit_and_plot_linear_regression(data, 'y')\n    \"\"\"\n    X = data_df.drop(target_col, axis=1)\n    y = data_df[target_col]\n\n    model = LinearRegression().fit(X, y)\n    coefficients = dict(zip(X.columns, np.round(model.coef_, precision)))\n\n    plt.scatter(X, y)\n    plt.plot(X, model.predict(X), color='red')\n    plt.show()\n\n    return coefficients\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, matplotlib, scipy\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.fft import fft\n\ndef compute_and_plot_fft(signal, precision=2):\n    \"\"\"\n    Compute the one-dimensional n-point discrete Fourier Transform (DFT) and \n    the inverse of this transform for a real or complex sequence (signal) using \n    the Fast Fourier Transform (FFT) algorithm, and plot the original signal \n    and the transformed signal. The transformed signal values are rounded to \n    the specified precision.\n\n    Parameters:\n    signal (array): An array representing the signal.\n    precision (int): The number of decimal places to which to round the transformed signal values.\n\n    Returns:\n    list: A list of transformed signal values (rounded to the specified precision).\n\n    Requirements:\n    - numpy\n    - matplotlib\n    - scipy\n\n    Example:\n    >>> compute_and_plot_fft(np.array([0., 1., 0., -1.]))\n    \"\"\"\n    transformed_signal = fft(signal)\n    transformed_signal = np.round(transformed_signal, precision).tolist()\n\n    fig, ax = plt.subplots(2, 1)\n    ax[0].plot(signal)\n    ax[0].set_title('Original Signal')\n    ax[1].plot(transformed_signal)\n    ax[1].set_title('Transformed Signal')\n    plt.show()\n\n    return transformed_signal\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, math, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nimport math\nimport seaborn as sns\n\ndef compute_and_plot_correlation_matrix(data_df, precision=2):\n    \"\"\"\n    Compute the correlation matrix for a DataFrame, and plot the correlation \n    matrix as a heatmap. The correlation values are rounded to the specified precision.\n\n    Parameters:\n    data_df (DataFrame): A DataFrame where each column represents a different variable.\n    precision (int): The number of decimal places to which to round the correlation values.\n\n    Returns:\n    DataFrame: A DataFrame representing the correlation matrix (rounded to the specified precision).\n\n    Requirements:\n    - pandas\n    - numpy\n    - math\n    - seaborn\n\n    Example:\n    >>> data = pd.DataFrame({\n            'x': [1, 2, 3, 4, 5],\n            'y': [2, 3, 5, 7, 9],\n            'z': [3, 4, 6, 8, 10]\n        })\n    >>> compute_and_plot_correlation_matrix(data)\n    \"\"\"\n    correlation_matrix = data_df.corr().round(precision)\n\n    sns.heatmap(correlation_matrix, annot=True)\n    plt.show()\n\n    return correlation_matrix\n```"}
{"seed": "{\"task_id\": 2890896, \"prompt\": \"def f_2890896(s):\\n\\t\", \"suffix\": \"\\n\\treturn ip\", \"canonical_solution\": \"ip = re.findall('[0-9]+(?:\\\\\\\\.[0-9]+){3}', s)\", \"test_start\": \"\\nimport re\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(\\\"<html><head><title>Current IP Check</title></head><body>Current IP Address: 165.91.15.131</body></html>\\\") == [\\\"165.91.15.131\\\"]\\n\", \"\\n    assert candidate(\\\"<html><head><title>Current IP Check</title></head><body>Current IP Address: 165.91.15.131 and this is not a IP Address: 165.91.15</body></html>\\\") == [\\\"165.91.15.131\\\"]\\n\", \"\\n    assert candidate(\\\"<html><head><title>Current IP Check</title></head><body>Current IP Address: 192.168.1.1 & this is another IP address: 192.168.1.2</body></html>\\\") == [\\\"192.168.1.1\\\", \\\"192.168.1.2\\\"]\\n\"], \"entry_point\": \"f_2890896\", \"intent\": \"extract ip address `ip` from an html string `s`\", \"library\": [\"re\"]}", "generation": "Scenario 1: \nre, bs4, urllib, json\n```python\nimport re\nfrom bs4 import BeautifulSoup\nimport urllib.request\nimport json\n\n# Constants\nURL = 'http://example.com'\nIP_REGEX = r'[0-9]+(?:\\.[0-9]+){3}'\n\ndef fetch_and_extract_ip():\n    \"\"\"\n    Fetch an HTML page from a URL and extract all IP addresses from the page.\n    \n    Returns:\n    list: A list of extracted IP addresses.\n    \n    Requirements:\n    - re\n    - bs4.BeautifulSoup\n    - urllib.request\n    - json\n    \n    Example:\n    >>> fetch_and_extract_ip()\n    \"\"\"\n    try:\n        response = urllib.request.urlopen(URL)\n        html = response.read()\n        soup = BeautifulSoup(html, 'html.parser')\n        text = soup.get_text()\n        ips = re.findall(IP_REGEX, text)\n        return ips\n    except Exception as e:\n        return str(e)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nre, os, collections\n```python\nimport re\nimport os\nfrom collections import Counter\n\n# Constants\nFOLDER_PATH = './log_files'\nIP_REGEX = r'[0-9]+(?:\\.[0-9]+){3}'\n\ndef count_ips_in_logs():\n    \"\"\"\n    Scan a directory for log files and count the occurrences of each IP address in all files.\n    \n    Returns:\n    dict: A dictionary with IP addresses as keys and their counts as values.\n    \n    Requirements:\n    - re\n    - os\n    - collections.Counter\n    \n    Example:\n    >>> count_ips_in_logs()\n    \"\"\"\n    counter = Counter()\n    for filename in os.listdir(FOLDER_PATH):\n        if filename.endswith('.log'):\n            with open(os.path.join(FOLDER_PATH, filename)) as file:\n                content = file.read()\n                ips = re.findall(IP_REGEX, content)\n                counter.update(ips)\n    return dict(counter)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nre, socket, json\n```python\nimport re\nimport socket\nimport json\n\n# Constants\nIP_REGEX = r'[0-9]+(?:\\.[0-9]+){3}'\n\ndef get_hostname_from_ips(ip_addresses):\n    \"\"\"\n    Given a list of IP addresses, return a dictionary mapping each IP address to its hostname.\n    \n    Parameters:\n    ip_addresses (list): A list of IP addresses.\n    \n    Returns:\n    dict: A dictionary with IP addresses as keys and their hostnames as values.\n    \n    Requirements:\n    - re\n    - socket\n    - json\n    \n    Example:\n    >>> get_hostname_from_ips(['8.8.8.8', '8.8.4.4'])\n    \"\"\"\n    hostnames = {}\n    for ip in ip_addresses:\n        if re.match(IP_REGEX, ip):\n            try:\n                hostname = socket.gethostbyaddr(ip)[0]\n                hostnames[ip] = hostname\n            except socket.herror:\n                hostnames[ip] = None\n    return hostnames\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nre, urllib, json\n```python\nimport re\nimport urllib.request\nimport json\n\n# Constants\nAPI_URL = 'https://api.ipify.org?format=json'\nIP_REGEX = r'[0-9]+(?:\\.[0-9]+){3}'\n\ndef get_public_ip():\n    \"\"\"\n    Fetch the public IP address of the current machine from an API.\n    \n    Returns:\n    str: The public IP address.\n    \n    Requirements:\n    - re\n    - urllib.request\n    - json\n    \n    Example:\n    >>> get_public_ip()\n    \"\"\"\n    try:\n        response = urllib.request.urlopen(API_URL)\n        data = json.loads(response.read())\n        ip = data['ip']\n        if re.match(IP_REGEX, ip):\n            return ip\n        else:\n            return 'Invalid IP address received'\n    except Exception as e:\n        return str(e)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nre, pandas, csv\n```python\nimport re\nimport pandas as pd\nimport csv\n\n# Constants\nCSV_FILE = 'data.csv'\nIP_REGEX = r'[0-9]+(?:\\.[0-9]+){3}'\n\ndef extract_ips_from_csv():\n    \"\"\"\n    Extract all IP addresses from a CSV file and return them as a list.\n    \n    Returns:\n    list: A list of extracted IP addresses.\n    \n    Requirements:\n    - re\n    - pandas\n    - csv\n    \n    Example:\n    >>> extract_ips_from_csv()\n    \"\"\"\n    df = pd.read_csv(CSV_FILE)\n    all_ips = []\n    for column in df.columns:\n        ips = df[column].apply(lambda x: re.findall(IP_REGEX, str(x)))\n        all_ips.extend(ips)\n    return all_ips\n```\n"}
{"seed": "{\"task_id\": 29836836, \"prompt\": \"def f_29836836(df):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"df.groupby('A').filter(lambda x: len(x) > 1)\", \"test_start\": \"\\nimport pandas as pd\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(pd.DataFrame([[1, 2], [1, 4], [5, 6]], columns=['A', 'B'])).equals(pd.DataFrame([[1, 2], [1, 4]], columns=['A', 'B'])) is True\\n\", \"\\n    assert candidate(pd.DataFrame([[1, 2], [1, 4], [1, 6]], columns=['A', 'B'])).equals(pd.DataFrame([[1, 2], [1, 4], [1, 6]], columns=['A', 'B'])) is True\\n\"], \"entry_point\": \"f_29836836\", \"intent\": \"filter dataframe `df` by values in column `A` that appear more than once\", \"library\": [\"pandas\"]}", "generation": "Scenario 1:\npandas, numpy, matplotlib.pyplot, random\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import choice\n\n# Constants\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D']\nMATCHES = 10\n\ndef simulate_and_visualize_matches():\n    \"\"\"\n    Simulate a number of matches between teams and visualize the results with \n    bar charts for each team showing the number of wins, losses, and draws.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the results of the matches.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> results = simulate_and_visualize_matches()\n    >>> print(results)\n    \"\"\"\n    results_data = []\n\n    for _ in range(MATCHES):\n        team1, team2 = np.random.choice(TEAMS, 2, replace=False)\n        result = choice(['Win', 'Loss', 'Draw'])\n        results_data.append([team1, team2, result])\n\n    results_df = pd.DataFrame(results_data, columns=['Team 1', 'Team 2', 'Result'])\n\n    for team in TEAMS:\n        team_df = results_df[(results_df['Team 1'] == team) | (results_df['Team 2'] == team)]\n        wins = len(team_df[team_df['Result'] == 'Win'])\n        losses = len(team_df[team_df['Result'] == 'Loss'])\n        draws = len(team_df[team_df['Result'] == 'Draw'])\n        plt.bar(['Wins', 'Losses', 'Draws'], [wins, losses, draws])\n        plt.title(f'Results for {team}')\n        plt.show()\n\n    return results_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, matplotlib, seaborn, numpy\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Constants\nFEATURES = ['Feature 1', 'Feature 2', 'Feature 3', 'Feature 4', 'Feature 5']\nSAMPLES = 100\n\ndef generate_and_visualize_data():\n    \"\"\"\n    Generate a dataset with a number of features and samples, and visualize the \n    distribution and correlation of the features using histograms and a heatmap.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the generated data.\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - seaborn\n    - numpy\n\n    Example:\n    >>> data = generate_and_visualize_data()\n    >>> print(data)\n    \"\"\"\n    data = pd.DataFrame(np.random.rand(SAMPLES, len(FEATURES)), columns=FEATURES)\n\n    # Plot histograms\n    data.hist(figsize=(10, 10), bins=50)\n    plt.show()\n\n    # Plot heatmap of correlations\n    corr = data.corr()\n    sns.heatmap(corr, annot=True, cmap='coolwarm')\n    plt.show()\n\n    return data\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, sklearn.preprocessing, sklearn.model_selection, sklearn.linear_model\n```python\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\n# Constants\nFEATURES = ['Feature 1', 'Feature 2', 'Feature 3', 'Feature 4', 'Feature 5']\nTARGET = 'Target'\n\ndef preprocess_and_train_model(df):\n    \"\"\"\n    Preprocess a pandas DataFrame by standardizing the features, split the data \n    into training and testing sets, and fit a linear regression model.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame with the data.\n\n    Returns:\n    LinearRegression: The trained linear regression model.\n\n    Requirements:\n    - pandas\n    - sklearn.preprocessing\n    - sklearn.model_selection\n    - sklearn.linear_model\n\n    Example:\n    >>> df = pd.DataFrame(np.random.rand(100, 6), columns=FEATURES+[TARGET])\n    >>> model = preprocess_and_train_model(df)\n    >>> print(model.coef_)\n    \"\"\"\n    # Standardize the features\n    scaler = StandardScaler()\n    df[FEATURES] = scaler.fit_transform(df[FEATURES])\n\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(df[FEATURES], df[TARGET], test_size=0.2, random_state=42)\n\n    # Fit a linear regression model\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    return model\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, seaborn, matplotlib.pyplot, statistics\n```python\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport statistics\n\n# Constants\nCATEGORIES = ['Category 1', 'Category 2', 'Category 3', 'Category 4', 'Category 5']\nVALUES = 'Values'\n\ndef analyze_and_visualize_category_values(df):\n    \"\"\"\n    Analyze a pandas DataFrame with categories and values, calculate the mean \n    value for each category, and visualize the distribution of values for each \n    category with box plots.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame with the data.\n\n    Returns:\n    dict: A dictionary with the mean value for each category.\n\n    Requirements:\n    - pandas\n    - seaborn\n    - matplotlib.pyplot\n    - statistics\n\n    Example:\n    >>> df = pd.DataFrame({'Category': np.random.choice(CATEGORIES, 100), 'Values': np.random.rand(100)})\n    >>> means = analyze_and_visualize_category_values(df)\n    >>> print(means)\n    \"\"\"\n    means = {}\n\n    for category in CATEGORIES:\n        category_values = df[df['Category'] == category][VALUES]\n        means[category] = statistics.mean(category_values)\n\n        sns.boxplot(x='Category', y=VALUES, data=df[df['Category'] == category])\n        plt.title(f'Distribution of values for {category}')\n        plt.show()\n\n    return means\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, sklearn.cluster, matplotlib.pyplot, numpy\n```python\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nFEATURES = ['Feature 1', 'Feature 2']\nCLUSTERS = 3\n\ndef cluster_and_visualize_data(df):\n    \"\"\"\n    Cluster a pandas DataFrame with KMeans and visualize the clusters with a \n    scatter plot.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame with the data.\n\n    Returns:\n    KMeans: The KMeans model fitted to the data.\n\n    Requirements:\n    - pandas\n    - sklearn.cluster\n    - matplotlib.pyplot\n    - numpy\n\n    Example:\n    >>> df = pd.DataFrame(np.random.rand(100, 2), columns=FEATURES)\n    >>> model = cluster_and_visualize_data(df)\n    >>> print(model.cluster_centers_)\n    \"\"\"\n    # Cluster the data\n    kmeans = KMeans(n_clusters=CLUSTERS, random_state=42)\n    kmeans.fit(df[FEATURES])\n\n    # Visualize the clusters\n    plt.scatter(df[FEATURES[0]], df[FEATURES[1]], c=kmeans.labels_, cmap='viridis')\n    plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red')\n    plt.show()\n\n    return kmeans\n```\n"}
{"seed": "{\"task_id\": 2545397, \"prompt\": \"def f_2545397(myfile):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"[x for x in myfile if x != '']\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    with open('./tmp.txt', 'w') as fw: \\n        for s in [\\\"hello\\\", \\\"world\\\", \\\"!!!\\\"]:\\n            fw.write(f\\\"{s}\\\\n\\\")\\n\\n    with open('./tmp.txt', 'r') as myfile:\\n        lines = candidate(myfile)\\n        assert isinstance(lines, list)\\n        assert len(lines) == 3\\n        assert lines[0].strip() == \\\"hello\\\"\\n\"], \"entry_point\": \"f_2545397\", \"intent\": \"append each line in file `myfile` into a list\", \"library\": []}", "generation": "Scenario 1:\nos, csv, pandas, numpy\n```python\nimport os\nimport csv\nimport pandas as pd\nimport numpy as np\n\n# Constants\nCSV_FILES_PATH = './csv_files/'\n\ndef aggregate_csv_data():\n    \"\"\"\n    Aggregate data from all csv files in a specific directory, compute the mean of \n    each numeric column and return a dictionary with column names as keys and \n    computed means as values.\n\n    Returns:\n    dict: A dictionary with column names as keys and computed means as values.\n\n    Requirements:\n    - os\n    - csv\n    - pandas\n    - numpy\n\n    Example:\n    >>> aggregate_csv_data()\n    \"\"\"\n    aggregated_data = {}\n\n    for filename in os.listdir(CSV_FILES_PATH):\n        if filename.endswith('.csv'):\n            file_path = os.path.join(CSV_FILES_PATH, filename)\n            df = pd.read_csv(file_path)\n\n            for column in df.columns:\n                if np.issubdtype(df[column].dtype, np.number):\n                    if column not in aggregated_data:\n                        aggregated_data[column] = df[column].mean()\n                    else:\n                        aggregated_data[column] = (aggregated_data[column] + df[column].mean()) / 2\n\n    return aggregated_data\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, json, collections\n```python\nimport os\nimport json\nfrom collections import Counter\n\n# Constants\nJSON_FILES_PATH = './json_files/'\nKEY = 'name'\n\ndef count_key_occurrences():\n    \"\"\"\n    Count occurrences of a specific key in all json files in a specific directory \n    and return a dictionary with values of the key as keys and their counts as values.\n\n    Returns:\n    dict: A dictionary with values of the key as keys and their counts as values.\n\n    Requirements:\n    - os\n    - json\n    - collections.Counter\n\n    Example:\n    >>> count_key_occurrences()\n    \"\"\"\n    key_values = []\n\n    for filename in os.listdir(JSON_FILES_PATH):\n        if filename.endswith('.json'):\n            file_path = os.path.join(JSON_FILES_PATH, filename)\n            with open(file_path, 'r') as json_file:\n                data = json.load(json_file)\n                if KEY in data:\n                    key_values.append(data[KEY])\n\n    return dict(Counter(key_values))\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nstring, random, os, shutil\n```python\nimport string\nimport random\nimport os\nimport shutil\n\n# Constants\nSOURCE_DIR = './source_dir/'\nDEST_DIR = './dest_dir/'\nFILE_EXT = '.txt'\n\ndef move_random_files(num_files):\n    \"\"\"\n    Move a specific number of random files with a specific extension from a source \n    directory to a destination directory.\n\n    Parameters:\n    num_files (int): The number of files to move.\n\n    Requirements:\n    - string\n    - random\n    - os\n    - shutil\n\n    Example:\n    >>> move_random_files(5)\n    \"\"\"\n    files = [f for f in os.listdir(SOURCE_DIR) if f.endswith(FILE_EXT)]\n    random_files = random.sample(files, num_files)\n\n    for file in random_files:\n        shutil.move(os.path.join(SOURCE_DIR, file), DEST_DIR)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nglob, csv, json, pandas\n```python\nimport glob\nimport csv\nimport json\nimport pandas as pd\n\n# Constants\nCSV_FILES_PATH = './csv_files/*.csv'\nJSON_OUTPUT_FILE = './output.json'\n\ndef convert_csv_to_json():\n    \"\"\"\n    Convert all csv files in a specific path to a single json file.\n\n    Requirements:\n    - glob\n    - csv\n    - json\n    - pandas\n\n    Example:\n    >>> convert_csv_to_json()\n    \"\"\"\n    data = []\n\n    for filename in glob.glob(CSV_FILES_PATH):\n        df = pd.read_csv(filename)\n        data.append(df.to_dict())\n\n    with open(JSON_OUTPUT_FILE, 'w') as json_file:\n        json.dump(data, json_file, indent=4)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\ncollections, json, os\n```python\nimport collections\nimport json\nimport os\n\n# Constants\nJSON_FILES_PATH = './json_files/'\n\ndef count_total_key_occurrences():\n    \"\"\"\n    Count total occurrences of all keys in all json files in a specific directory \n    and return a dictionary with keys as keys and their counts as values.\n\n    Returns:\n    dict: A dictionary with keys as keys and their counts as values.\n\n    Requirements:\n    - collections\n    - json\n    - os\n\n    Example:\n    >>> count_total_key_occurrences()\n    \"\"\"\n    key_counts = collections.defaultdict(int)\n\n    for filename in os.listdir(JSON_FILES_PATH):\n        if filename.endswith('.json'):\n            file_path = os.path.join(JSON_FILES_PATH, filename)\n            with open(file_path, 'r') as json_file:\n                data = json.load(json_file)\n                for key in data.keys():\n                    key_counts[key] += 1\n\n    return dict(key_counts)\n```"}
{"seed": "{\"task_id\": 2545397, \"prompt\": \"def f_2545397():\\n\\t\", \"suffix\": \"\\n\\treturn lst\", \"canonical_solution\": \"lst = list(map(int, open('filename.txt').readlines()))\", \"test_start\": \"\\nimport pandas as pd\\n\\ndef check(candidate):\", \"test\": [\"\\n    with open('./filename.txt', 'w') as fw: \\n        for s in [\\\"1\\\", \\\"2\\\", \\\"100\\\"]:\\n            fw.write(f\\\"{s}\\\\n\\\")\\n\\n    assert candidate() == [1, 2, 100]\\n\"], \"entry_point\": \"f_2545397\", \"intent\": \"Get a list of integers `lst` from a file `filename.txt`\", \"library\": [\"pandas\"]}", "generation": "Scenario 1:\npandas, numpy, matplotlib, os\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\n# Constants\nDATA_DIR = './data'\n\ndef fetch_and_plot_data(filename):\n    \"\"\"\n    Fetch data from a CSV file in a specific directory, calculate some basic statistics,\n    and plot a histogram of the data.\n\n    Parameters:\n    filename (str): The name of the CSV file (without extension).\n\n    Returns:\n    pd.DataFrame: The fetched data.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - os\n\n    Example:\n    >>> fetch_and_plot_data('data_file')\n    \"\"\"\n    file_path = os.path.join(DATA_DIR, f'{filename}.csv')\n    data = pd.read_csv(file_path)\n\n    print(f'Mean: {np.mean(data)}')\n    print(f'Standard Deviation: {np.std(data)}')\n    \n    data.hist(bins=20)\n    plt.show()\n    \n    return data\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, itertools, math\n```python\nimport pandas as pd\nimport numpy as np\nimport itertools\nimport math\n\ndef find_closest_pair(filename):\n    \"\"\"\n    Find the closest pair of points in a list of points loaded from a CSV file.\n\n    Parameters:\n    filename (str): The name of the CSV file (without extension).\n\n    Returns:\n    tuple: The closest pair of points.\n\n    Requirements:\n    - pandas\n    - numpy\n    - itertools\n    - math\n\n    Example:\n    >>> find_closest_pair('points')\n    \"\"\"\n    def distance(point1, point2):\n        return math.sqrt((point1[0]-point2[0])**2 + (point1[1]-point2[1])**2)\n\n    data = pd.read_csv(f'{filename}.csv').values\n    points = [tuple(point) for point in data]\n    \n    closest_pair = min(itertools.combinations(points, 2), key=lambda pair: distance(*pair))\n\n    return closest_pair\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, matplotlib, seaborn, numpy\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\ndef plot_correlation_matrix(filename):\n    \"\"\"\n    Plot the correlation matrix of a dataset loaded from a CSV file.\n\n    Parameters:\n    filename (str): The name of the CSV file (without extension).\n\n    Returns:\n    np.ndarray: The correlation matrix.\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - seaborn\n    - numpy\n\n    Example:\n    >>> plot_correlation_matrix('data')\n    \"\"\"\n    data = pd.read_csv(f'{filename}.csv')\n    corr_mat = data.corr()\n    \n    plt.figure(figsize=(10, 10))\n    sns.heatmap(corr_mat, annot=True, cmap='coolwarm')\n    plt.show()\n\n    return corr_mat.values\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, sklearn.preprocessing, json\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nimport json\n\ndef preprocess_and_save(filename):\n    \"\"\"\n    Preprocess a dataset loaded from a CSV file: handle missing values, scale numerical features,\n    and save the preprocessed data to a JSON file.\n\n    Parameters:\n    filename (str): The name of the CSV file (without extension).\n\n    Returns:\n    str: The name of the JSON file with the preprocessed data.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.preprocessing\n    - json\n\n    Example:\n    >>> preprocess_and_save('data')\n    \"\"\"\n    data = pd.read_csv(f'{filename}.csv')\n    data.fillna(data.mean(), inplace=True)\n    \n    numeric_cols = data.select_dtypes(include=[np.number]).columns\n    scaler = StandardScaler()\n    data[numeric_cols] = scaler.fit_transform(data[numeric_cols])\n    \n    output_filename = f'{filename}_preprocessed.json'\n    data.to_json(output_filename, orient='records')\n\n    return output_filename\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, sklearn.ensemble, sklearn.metrics\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\ndef train_and_evaluate(filename, target_col):\n    \"\"\"\n    Train a random forest classifier on a dataset loaded from a CSV file and evaluate its accuracy.\n\n    Parameters:\n    filename (str): The name of the CSV file (without extension).\n    target_col (str): The name of the target column.\n\n    Returns:\n    float: The accuracy of the classifier.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.ensemble\n    - sklearn.metrics\n\n    Example:\n    >>> train_and_evaluate('data', 'target')\n    \"\"\"\n    data = pd.read_csv(f'{filename}.csv')\n    X = data.drop(target_col, axis=1)\n    y = data[target_col]\n    \n    clf = RandomForestClassifier()\n    clf.fit(X, y)\n    \n    y_pred = clf.predict(X)\n    accuracy = accuracy_score(y, y_pred)\n\n    return accuracy\n```"}
{"seed": "{\"task_id\": 35420052, \"prompt\": \"def f_35420052(plt, mappable, ax3):\\n\\t\", \"suffix\": \"\\n\\treturn plt\", \"canonical_solution\": \"plt.colorbar(mappable=mappable, cax=ax3)\", \"test_start\": \"\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom obspy.core.trace import Trace\\nfrom obspy.imaging.spectrogram import spectrogram\\n\\ndef check(candidate):\", \"test\": [\"\\n    spl1 = Trace(data=np.arange(0, 10))\\n    fig = plt.figure()\\n    ax1 = fig.add_axes([0.1, 0.75, 0.7, 0.2]) #[left bottom width height]\\n    ax2 = fig.add_axes([0.1, 0.1, 0.7, 0.60], sharex=ax1)\\n    ax3 = fig.add_axes([0.83, 0.1, 0.03, 0.6])\\n\\n    #make time vector\\n    t = np.arange(spl1.stats.npts) / spl1.stats.sampling_rate\\n\\n    #plot waveform (top subfigure)    \\n    ax1.plot(t, spl1.data, 'k')\\n\\n    #plot spectrogram (bottom subfigure)\\n    spl2 = spl1\\n    fig = spl2.spectrogram(show=False, axes=ax2, wlen=10)\\n    mappable = ax2.images[0]\\n    candidate(plt, mappable, ax3)\\n    \\n    im=ax2.images\\n    assert im[-1].colorbar is not None\\n\"], \"entry_point\": \"f_35420052\", \"intent\": \"add color bar with image `mappable` to plot `plt`\", \"library\": [\"matplotlib\", \"numpy\", \"obspy\"]}", "generation": "Scenario 1:\nmatplotlib, numpy, pandas, seaborn\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n# Constants\nCOLUMNS = ['Feature1', 'Feature2', 'Feature3', 'Feature4', 'Target']\n\ndef generate_heatmap_with_colorbar(df):\n    \"\"\"\n    Generate a heatmap for a pandas DataFrame with a colorbar.\n    \n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n    \n    Returns:\n    None: The function only plots the heatmap.\n    \n    Requirements:\n    - matplotlib.pyplot\n    - numpy\n    - pandas\n    - seaborn\n    \n    Example:\n    >>> df = pd.DataFrame(np.random.rand(5,5), columns=COLUMNS)\n    >>> generate_heatmap_with_colorbar(df)\n    \"\"\"\n    plt.figure(figsize=(10, 8))\n\n    # Generate a mask for the upper triangle\n    mask = np.triu(np.ones_like(df.corr(), dtype=bool))\n\n    # Generate a custom diverging colormap\n    cmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n    # Draw the heatmap with the mask and correct aspect ratio\n    mappable = sns.heatmap(df.corr(), mask=mask, cmap=cmap, vmax=.3, center=0,\n                           square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n\n    plt.colorbar(mappable=mappable.collections[0])\n\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nmatplotlib, numpy, scipy, seaborn\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import norm\nimport seaborn as sns\n\n# Constants\nNUM_SAMPLES = 1000\n\ndef plot_normal_distribution_with_colorbar(mu, sigma):\n    \"\"\"\n    Plot a normal distribution given mean and standard deviation with a colorbar.\n    \n    Parameters:\n    mu (float): The mean of the distribution.\n    sigma (float): The standard deviation of the distribution.\n    \n    Returns:\n    None: The function only plots the distribution.\n    \n    Requirements:\n    - matplotlib.pyplot\n    - numpy\n    - scipy.stats\n    - seaborn\n    \n    Example:\n    >>> plot_normal_distribution_with_colorbar(0, 1)\n    \"\"\"\n    # Generate samples from the normal distribution\n    samples = np.random.normal(mu, sigma, NUM_SAMPLES)\n\n    # Generate a KDE plot\n    mappable = sns.kdeplot(samples, shade=True)\n\n    # Add a colorbar to the plot\n    plt.colorbar(mappable=mappable.collections[0])\n\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nmatplotlib, numpy, pandas, seaborn\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n# Constants\nCOLUMNS = ['Feature1', 'Feature2', 'Feature3', 'Feature4', 'Target']\n\ndef generate_pairplot_with_colorbar(df):\n    \"\"\"\n    Generate a pairplot for a pandas DataFrame with a colorbar.\n    \n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n    \n    Returns:\n    None: The function only plots the pairplot.\n    \n    Requirements:\n    - matplotlib.pyplot\n    - numpy\n    - pandas\n    - seaborn\n    \n    Example:\n    >>> df = pd.DataFrame(np.random.rand(5,5), columns=COLUMNS)\n    >>> generate_pairplot_with_colorbar(df)\n    \"\"\"\n    # Generate a pairplot\n    pair_plot = sns.pairplot(df)\n\n    # Add a colorbar to the plot\n    for ax in pair_plot.axes.flatten():\n        if ax.collections:\n            plt.colorbar(mappable=ax.collections[0], ax=ax)\n\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nmatplotlib, numpy, pandas, seaborn\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n# Constants\nCOLUMNS = ['Feature1', 'Feature2', 'Feature3', 'Feature4', 'Target']\n\ndef generate_boxplot_with_colorbar(df):\n    \"\"\"\n    Generate a boxplot for a pandas DataFrame with a colorbar.\n    \n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n    \n    Returns:\n    None: The function only plots the boxplot.\n    \n    Requirements:\n    - matplotlib.pyplot\n    - numpy\n    - pandas\n    - seaborn\n    \n    Example:\n    >>> df = pd.DataFrame(np.random.rand(5,5), columns=COLUMNS)\n    >>> generate_boxplot_with_colorbar(df)\n    \"\"\"\n    # Generate a boxplot\n    box_plot = sns.boxplot(data=df)\n\n    # Add a colorbar to the plot\n    if box_plot.collections:\n        plt.colorbar(mappable=box_plot.collections[0])\n\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nmatplotlib, numpy, pandas, seaborn\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n# Constants\nCOLUMNS = ['Feature1', 'Feature2', 'Feature3', 'Feature4', 'Target']\n\ndef generate_violinplot_with_colorbar(df):\n    \"\"\"\n    Generate a violinplot for a pandas DataFrame with a colorbar.\n    \n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n    \n    Returns:\n    None: The function only plots the violinplot.\n    \n    Requirements:\n    - matplotlib.pyplot\n    - numpy\n    - pandas\n    - seaborn\n    \n    Example:\n    >>> df = pd.DataFrame(np.random.rand(5,5), columns=COLUMNS)\n    >>> generate_violinplot_with_colorbar(df)\n    \"\"\"\n    # Generate a violinplot\n    violin_plot = sns.violinplot(data=df)\n\n    # Add a colorbar to the plot\n    if violin_plot.collections:\n        plt.colorbar(mappable=violin_plot.collections[0])\n\n    plt.show()\n```\n"}
{"seed": "{\"task_id\": 29903025, \"prompt\": \"def f_29903025(df):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"Counter(' '.join(df['text']).split()).most_common(100)\", \"test_start\": \"\\nimport pandas as pd\\nfrom collections import Counter\\n \\ndef check(candidate):\", \"test\": [\"\\n    df = pd.DataFrame({\\\"text\\\": [\\n      'Python is a high-level, general-purpose programming language.', \\n      'Its design philosophy emphasizes code readability with the use of significant indentation. Python is dynamically-typed and garbage-collected.'\\n    ]})\\n    assert candidate(df) == [('Python', 2),('is', 2),('a', 1),('high-level,', 1),('general-purpose', 1),\\n        ('programming', 1),('language.', 1),('Its', 1),('design', 1),('philosophy', 1),('emphasizes', 1),\\n        ('code', 1),('readability', 1),('with', 1), ('the', 1),('use', 1),('of', 1),('significant', 1),\\n        ('indentation.', 1),('dynamically-typed', 1),('and', 1),('garbage-collected.', 1)]\\n\"], \"entry_point\": \"f_29903025\", \"intent\": \"count most frequent 100 words in column 'text' of dataframe `df`\", \"library\": [\"collections\", \"pandas\"]}", "generation": "Scenario 1:\ncollections, pandas, matplotlib, numpy, re\n```python\nfrom collections import Counter\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport re\n\n# Constants\nTOP_WORDS = 10\n\ndef plot_top_words(df, column):\n    \"\"\"\n    Count the most frequent words in a specified column of a DataFrame,\n    filter out non-alphabetic characters, and plot the top words in a bar chart.\n\n    Parameters:\n    df (DataFrame): the DataFrame\n    column (str): the column containing the text\n\n    Returns:\n    list: The top words and their counts\n\n    Requirements:\n    - pandas\n    - collections\n    - matplotlib.pyplot\n    - numpy\n    - re\n\n    Example:\n    >>> df = pd.DataFrame({\"text\": [\n    ...   'Python is a high-level, general-purpose programming language.',\n    ...   'Its design philosophy emphasizes code readability with the use of significant indentation. Python is dynamically-typed and garbage-collected.'\n    ... ]})\n    >>> top_words = plot_top_words(df, 'text')\n    >>> print(top_words)\n    \"\"\"\n    # Combine all text and split into words\n    words = ' '.join(df[column]).split()\n\n    # Remove non-alphabetic characters\n    words = [re.sub(r'\\W+', '', word) for word in words]\n\n    # Count word frequencies\n    word_counts = Counter(words)\n\n    # Get the most common words\n    top_words = word_counts.most_common(TOP_WORDS)\n\n    # Plot the top words\n    plt.bar(*zip(*top_words))\n    plt.show()\n\n    return top_words\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, sklearn, matplotlib, numpy\n```python\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nN_TOPICS = 5\nN_TOP_WORDS = 10\n\ndef extract_topics(df, column):\n    \"\"\"\n    Extract topics from a text column in a DataFrame using Latent Dirichlet Allocation (LDA)\n    and plot the top words in each topic.\n\n    Parameters:\n    df (DataFrame): the DataFrame\n    column (str): the column containing the text\n\n    Returns:\n    list: The top words in each topic\n\n    Requirements:\n    - pandas\n    - sklearn.feature_extraction.text.CountVectorizer\n    - sklearn.decomposition.LatentDirichletAllocation\n    - matplotlib.pyplot\n    - numpy\n\n    Example:\n    >>> df = pd.DataFrame({\"text\": [\n    ...   \"Python is a high-level, general-purpose programming language.\",\n    ...   \"Its design philosophy emphasizes code readability with the use of significant indentation. Python is dynamically-typed and garbage-collected.\"\n    ... ]})\n    >>> topics = extract_topics(df, 'text')\n    >>> print(topics)\n    \"\"\"\n    vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n    tf = vectorizer.fit_transform(df[column])\n    lda = LatentDirichletAllocation(n_components=N_TOPICS, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n    tf_feature_names = vectorizer.get_feature_names_out()\n    topics = []\n\n    for topic_idx, topic in enumerate(lda.components_):\n        top_words = [tf_feature_names[i] for i in topic.argsort()[:-N_TOP_WORDS - 1:-1]]\n        topics.append(top_words)\n\n        # Plot top words\n        plt.figure(figsize=(10,10))\n        plt.imshow(wordcloud)\n        plt.axis(\"off\")\n        plt.show()\n\n    return topics\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ncollections, pandas, matplotlib, numpy, nltk\n```python\nfrom collections import Counter\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom nltk.corpus import stopwords\n\n# Constants\nTOP_WORDS = 10\n\n# Load English stopwords\nstop_words = set(stopwords.words('english'))\n\ndef plot_top_non_stopwords(df, column):\n    \"\"\"\n    Count the most frequent non-stopwords in a specified column of a DataFrame,\n    filter out non-alphabetic characters, and plot the top words in a bar chart.\n\n    Parameters:\n    df (DataFrame): the DataFrame\n    column (str): the column containing the text\n\n    Returns:\n    list: The top non-stopwords and their counts\n\n    Requirements:\n    - pandas\n    - collections\n    - matplotlib.pyplot\n    - numpy\n    - nltk.corpus.stopwords\n\n    Example:\n    >>> df = pd.DataFrame({\"text\": [\n    ...   'Python is a high-level, general-purpose programming language.',\n    ...   'Its design philosophy emphasizes code readability with the use of significant indentation. Python is dynamically-typed and garbage-collected.'\n    ... ]})\n    >>> top_words = plot_top_non_stopwords(df, 'text')\n    >>> print(top_words)\n    \"\"\"\n    # Combine all text and split into words\n    words = ' '.join(df[column]).split()\n\n    # Remove non-alphabetic characters and stopwords\n    words = [word for word in words if word.isalpha() and word not in stop_words]\n\n    # Count word frequencies\n    word_counts = Counter(words)\n\n    # Get the most common words\n    top_words = word_counts.most_common(TOP_WORDS)\n\n    # Plot the top words\n    plt.bar(*zip(*top_words))\n    plt.show()\n\n    return top_words\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, collections, nltk, matplotlib, numpy\n```python\nimport pandas as pd\nfrom collections import Counter\nfrom nltk.tokenize import word_tokenize\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nTOP_NOUNS = 10\n\ndef plot_top_nouns(df, column):\n    \"\"\"\n    Tokenize a text column in a DataFrame, count the most frequent nouns,\n    and plot the top nouns in a bar chart.\n\n    Parameters:\n    df (DataFrame): the DataFrame\n    column (str): the column containing the text\n\n    Returns:\n    list: The top nouns and their counts\n\n    Requirements:\n    - pandas\n    - collections\n    - nltk.tokenize.word_tokenize\n    - matplotlib.pyplot\n    - numpy\n\n    Example:\n    >>> df = pd.DataFrame({\"text\": [\n    ...   'Python is a high-level, general-purpose programming language.',\n    ...   'Its design philosophy emphasizes code readability with the use of significant indentation. Python is dynamically-typed and garbage-collected.'\n    ... ]})\n    >>> top_nouns = plot_top_nouns(df, 'text')\n    >>> print(top_nouns)\n    \"\"\"\n    # Combine all text and tokenize\n    tokens = word_tokenize(' '.join(df[column]))\n\n    # Filter out non-nouns\n    nouns = [token for token, pos in nltk.pos_tag(tokens) if pos.startswith('N')]\n\n    # Count noun frequencies\n    noun_counts = Counter(nouns)\n\n    # Get the most common nouns\n    top_nouns = noun_counts.most_common(TOP_NOUNS)\n\n    # Plot the top nouns\n    plt.bar(*zip(*top_nouns))\n    plt.show()\n\n    return top_nouns\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\ncollections, pandas, matplotlib, numpy, nltk\n```python\nfrom collections import Counter\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom nltk.stem import PorterStemmer\n\n# Constants\nTOP_WORDS = 10\n\n# Initialize stemmer\nstemmer = PorterStemmer()\n\ndef plot_top_stems(df, column):\n    \"\"\"\n    Stem a text column in a DataFrame, count the most frequent stems,\n    and plot the top stems in a bar chart.\n\n    Parameters:\n    df (DataFrame): the DataFrame\n    column (str): the column containing the text\n\n    Returns:\n    list: The top stems and their counts\n\n    Requirements:\n    - pandas\n    - collections\n    - matplotlib.pyplot\n    - numpy\n    - nltk.stem.PorterStemmer\n\n    Example:\n    >>> df = pd.DataFrame({\"text\": [\n    ...   'Python is a high-level, general-purpose programming language.',\n    ...   'Its design philosophy emphasizes code readability with the use of significant indentation. Python is dynamically-typed and garbage-collected.'\n    ... ]})\n    >>> top_stems = plot_top_stems(df, 'text')\n    >>> print(top_stems)\n    \"\"\"\n    # Combine all text and split into words\n    words = ' '.join(df[column]).split()\n\n    # Stem words\n    stems = [stemmer.stem(word) for word in words]\n\n    # Count stem frequencies\n    stem_counts = Counter(stems)\n\n    # Get the most common stems\n    top_stems = stem_counts.most_common(TOP_WORDS)\n\n    # Plot the top stems\n    plt.bar(*zip(*top_stems))\n    plt.show()\n\n    return top_stems\n```"}
{"seed": "{\"task_id\": 7378180, \"prompt\": \"def f_7378180():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"list(itertools.combinations((1, 2, 3), 2))\", \"test_start\": \"\\nimport itertools\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate() == [(1, 2), (1, 3), (2, 3)]\\n\"], \"entry_point\": \"f_7378180\", \"intent\": \"generate all 2-element subsets of tuple `(1, 2, 3)`\", \"library\": [\"itertools\"]}", "generation": "Scenario 1: \nitertools, random, numpy, matplotlib\n```python\nimport itertools\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nELEMENTS = (1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\nSUBSET_SIZE = 2\n\ndef plot_element_combinations():\n    \"\"\"\n    Generate all 2-element subsets of a tuple and plot a histogram of \n    the sums of the subsets.\n\n    Requirements:\n    - itertools\n    - random\n    - numpy\n    - matplotlib\n    \n    Example:\n    >>> plot_element_combinations()\n    \"\"\"\n    combinations = list(itertools.combinations(ELEMENTS, SUBSET_SIZE))\n    sums = [sum(combination) for combination in combinations]\n    plt.hist(sums, bins=range(min(sums), max(sums) + 1))\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nitertools, random, statistics\n```python\nimport itertools\nimport random\nimport statistics\n\n# Constants\nELEMENTS = (1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\nSUBSET_SIZE = 2\n\ndef calculate_combination_statistics():\n    \"\"\"\n    Generate all 2-element subsets of a tuple and calculate the mean, median, \n    and mode of the sums of the subsets.\n\n    Returns:\n    dict: A dictionary with the mean, median, and mode.\n\n    Requirements:\n    - itertools\n    - random\n    - statistics\n    \n    Example:\n    >>> calculate_combination_statistics()\n    \"\"\"\n    combinations = list(itertools.combinations(ELEMENTS, SUBSET_SIZE))\n    sums = [sum(combination) for combination in combinations]\n    return {\n        'mean': statistics.mean(sums),\n        'median': statistics.median(sums),\n        'mode': statistics.mode(sums)\n    }\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nitertools, random, collections\n```python\nimport itertools\nimport random\nimport collections\n\n# Constants\nELEMENTS = (1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\nSUBSET_SIZE = 2\n\ndef count_combination_sums():\n    \"\"\"\n    Generate all 2-element subsets of a tuple and count the occurrences of \n    each sum in the subsets.\n\n    Returns:\n    dict: A dictionary with the sums and their counts.\n\n    Requirements:\n    - itertools\n    - random\n    - collections\n    \n    Example:\n    >>> count_combination_sums()\n    \"\"\"\n    combinations = list(itertools.combinations(ELEMENTS, SUBSET_SIZE))\n    sums = [sum(combination) for combination in combinations]\n    return collections.Counter(sums)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nitertools, random, pandas\n```python\nimport itertools\nimport random\nimport pandas as pd\n\n# Constants\nELEMENTS = (1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\nSUBSET_SIZE = 2\n\ndef create_combination_dataframe():\n    \"\"\"\n    Generate all 2-element subsets of a tuple and create a pandas DataFrame \n    with the subsets and their sums.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the subsets and their sums.\n\n    Requirements:\n    - itertools\n    - random\n    - pandas\n    \n    Example:\n    >>> create_combination_dataframe()\n    \"\"\"\n    combinations = list(itertools.combinations(ELEMENTS, SUBSET_SIZE))\n    sums = [sum(combination) for combination in combinations]\n    return pd.DataFrame({'Combination': combinations, 'Sum': sums})\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nitertools, random, math\n```python\nimport itertools\nimport random\nimport math\n\n# Constants\nELEMENTS = (1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\nSUBSET_SIZE = 2\n\ndef calculate_combination_product():\n    \"\"\"\n    Generate all 2-element subsets of a tuple and calculate the product of \n    the sums of the subsets.\n\n    Returns:\n    int: The product of the sums.\n\n    Requirements:\n    - itertools\n    - random\n    - math\n    \n    Example:\n    >>> calculate_combination_product()\n    \"\"\"\n    combinations = list(itertools.combinations(ELEMENTS, SUBSET_SIZE))\n    sums = [sum(combination) for combination in combinations]\n    product = math.prod(sums)\n    return product\n```"}
{"seed": "{\"task_id\": 4530069, \"prompt\": \"def f_4530069():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"datetime.now(pytz.utc)\", \"test_start\": \"\\nimport pytz\\nimport time\\nfrom datetime import datetime, timezone\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert (candidate() - datetime(1970, 1, 1).replace(tzinfo=timezone.utc)).total_seconds() - time.time() <= 1\\n\"], \"entry_point\": \"f_4530069\", \"intent\": \"get a value of datetime.today() in the UTC time zone\", \"library\": [\"datetime\", \"pytz\", \"time\"]}", "generation": "Scenario 3:\npandas, pytz, datetime, random, matplotlib, seaborn\n```python\nimport pandas as pd\nimport pytz\nfrom datetime import datetime\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nCOUNTRIES = ['USA', 'UK', 'China', 'Japan', 'Australia']\nCURRENCY_RATES = [1.0, 0.73, 6.45, 110.6, 1.29]\n\n# Time zones for the countries\nTIMEZONES = {\n    'USA': 'America/New_York',\n    'UK': 'Europe/London',\n    'China': 'Asia/Shanghai',\n    'Japan': 'Asia/Tokyo',\n    'Australia': 'Australia/Sydney'\n}\n\ndef generate_currency_report(utc_datetime):\n    \"\"\"\n    Generate a report of currency rates for a list of countries across various \n    time zones at a given time (UTC).\n    \n    Parameters:\n    utc_datetime (datetime): The datetime in UTC.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with currency rates for the countries.\n    \n    Requirements:\n    - pandas\n    - pytz\n    - datetime\n    - random\n    - matplotlib.pyplot\n    - seaborn\n    \n    Example:\n    >>> utc_time = datetime(2023, 6, 15, 12, 0, 0, tzinfo=pytz.UTC)\n    >>> report = generate_currency_report(utc_time)\n    >>> print(report)\n    >>> sns.barplot(x='Country', y='Currency Rate', data=report)\n    \"\"\"\n    report_data = []\n\n    for i, country in enumerate(COUNTRIES):\n        country_tz = pytz.timezone(TIMEZONES[country])\n        country_time = utc_datetime.astimezone(country_tz)\n        currency_rate = CURRENCY_RATES[i] + (randint(-100, 100)/10000)\n        report_data.append([country, country_time, currency_rate])\n\n    report_df = pd.DataFrame(report_data, columns=['Country', 'Local Time', 'Currency Rate'])\n\n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \npytz, datetime, numpy, dateutil, math\n```python\nimport pytz\nfrom datetime import datetime\nimport numpy as np\nfrom dateutil.parser import parse\nimport math\n\n# Constants\nSOLAR_CYCLE_YEARS = np.array([1986, 1996, 2008, 2019])\n\ndef calculate_solar_activity(date_str, from_tz, to_tz):\n    \"\"\"\n    Calculate the solar activity based on the date and time considering \n    the solar cycle of 11 years.\n\n    Parameters:\n    date_str (str): The date string in \"yyyy-mm-dd hh:mm:ss\" format.\n    from_tz (str): The timezone of the given date string.\n    to_tz (str): The timezone to which the given date and time should be converted.\n\n    Returns:\n    float: The solar activity between 0 and 1.\n\n    Requirements:\n    - datetime\n    - pytz\n    - numpy\n    - dateutil.parser\n    - math\n\n    Example:\n    >>> calculate_solar_activity('1970-01-01 00:00:00', 'UTC', 'America/New_York')\n    \"\"\"\n    from_tz = pytz.timezone(from_tz)\n    to_tz = pytz.timezone(to_tz)\n    given_date = parse(date_str).replace(tzinfo=from_tz)\n    converted_date = given_date.astimezone(to_tz)\n\n    solar_cycle_year = SOLAR_CYCLE_YEARS[np.argmin(np.abs(SOLAR_CYCLE_YEARS - converted_date.year))]\n    years_since_solar_cycle_year = abs(converted_date.year - solar_cycle_year)\n\n    solar_activity = math.cos(math.pi * years_since_solar_cycle_year / 11)\n\n    return solar_activity\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, pytz, datetime, random, matplotlib, seaborn\n```python\nimport pandas as pd\nimport pytz\nfrom datetime import datetime\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nCITIES = ['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']\nTRAFFIC_LEVELS = ['Low', 'Medium', 'High', 'Very High']\n\n# Time zones for the cities\nTIMEZONES = {\n    'New York': 'America/New_York',\n    'London': 'Europe/London',\n    'Beijing': 'Asia/Shanghai',\n    'Tokyo': 'Asia/Tokyo',\n    'Sydney': 'Australia/Sydney'\n}\n\ndef generate_traffic_report(utc_datetime):\n    \"\"\"\n    Generate a report of traffic levels for a list of cities across various \n    time zones at a given time (UTC).\n    \n    Parameters:\n    utc_datetime (datetime): The datetime in UTC.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with traffic levels for the cities.\n    \n    Requirements:\n    - pandas\n    - pytz\n    - datetime\n    - random\n    - matplotlib.pyplot\n    - seaborn\n    \n    Example:\n    >>> utc_time = datetime(2023, 6, 15, 12, 0, 0, tzinfo=pytz.UTC)\n    >>> report = generate_traffic_report(utc_time)\n    >>> print(report)\n    >>> sns.countplot(x='Traffic Level', data=report, order=['Low', 'Medium', 'High', 'Very High'])\n    \"\"\"\n    report_data = []\n\n    for city in CITIES:\n        city_tz = pytz.timezone(TIMEZONES[city])\n        city_time = utc_datetime.astimezone(city_tz)\n        traffic_level = TRAFFIC_LEVELS[randint(0, len(TRAFFIC_LEVELS)-1)]\n        report_data.append([city, city_time, traffic_level])\n\n    report_df = pd.DataFrame(report_data, columns=['City', 'Local Time', 'Traffic Level'])\n\n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 6:\npytz, datetime, numpy, dateutil, math\n```python\nimport pytz\nfrom datetime import datetime\nimport numpy as np\nfrom dateutil.parser import parse\nimport math\n\n# Constants\nMOON_PHASES_YEARS = np.array([1987, 1994, 2001, 2008, 2015, 2022])\n\ndef calculate_moon_phase(date_str, from_tz, to_tz):\n    \"\"\"\n    Calculate the moon phase based on the date and time considering \n    the moon phase cycle of 7 years.\n\n    Parameters:\n    date_str (str): The date string in \"yyyy-mm-dd hh:mm:ss\" format.\n    from_tz (str): The timezone of the given date string.\n    to_tz (str): The timezone to which the given date and time should be converted.\n\n    Returns:\n    float: The moon phase between 0 and 1.\n\n    Requirements:\n    - datetime\n    - pytz\n    - numpy\n    - dateutil.parser\n    - math\n\n    Example:\n    >>> calculate_moon_phase('1970-01-01 00:00:00', 'UTC', 'America/New_York')\n    \"\"\"\n    from_tz = pytz.timezone(from_tz)\n    to_tz = pytz.timezone(to_tz)\n    given_date = parse(date_str).replace(tzinfo=from_tz)\n    converted_date = given_date.astimezone(to_tz)\n\n    moon_phase_year = MOON_PHASES_YEARS[np.argmin(np.abs(MOON_PHASES_YEARS - converted_date.year))]\n    years_since_moon_phase_year = abs(converted_date.year - moon_phase_year)\n\n    moon_phase = math.sin(math.pi * years_since_moon_phase_year / 7)\n\n    return moon_phase\n```\n`GPT_ODEX_BREAK`"}
{"seed": "{\"task_id\": 4842956, \"prompt\": \"def f_4842956(list1):\\n\\t\", \"suffix\": \"\\n\\treturn list2\", \"canonical_solution\": \"list2 = [x for x in list1 if x != []]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([[\\\"a\\\"], [], [\\\"b\\\"]]) == [[\\\"a\\\"], [\\\"b\\\"]]\\n\", \"\\n    assert candidate([[], [1,2,3], [], [\\\"b\\\"]]) == [[1,2,3], [\\\"b\\\"]]\\n\"], \"entry_point\": \"f_4842956\", \"intent\": \"Get a new list `list2`by removing empty list from a list of lists `list1`\", \"library\": []}", "generation": "Scenario 1: \npandas, datetime, random, matplotlib, json, os\n```python\nimport pandas as pd\nfrom datetime import datetime, timedelta\nfrom random import randint, choice\nimport matplotlib.pyplot as plt\nimport json\nimport os\n\n# Constants\nITEMS = ['Coffee', 'Tea', 'Bagel', 'Muffin', 'Croissant']\nOUTLET_LOCATIONS = ['Outlet1', 'Outlet2', 'Outlet3', 'Outlet4', 'Outlet5']\n\ndef daily_sales_report(day):\n    \"\"\"\n    Generate a daily sales report for a chain of coffee shops. The report should\n    contain the total sales for each item for each outlet. The sales data is stored\n    as json files in a directory called 'sales_data'. Each file is named in the format\n    'sales_yyyy-mm-dd.json' and contains a list of transactions. Each transaction is \n    a dictionary with 'item', 'quantity', and 'outlet' keys.\n\n    Parameters:\n    day (str): The day for which the report is to be generated in 'yyyy-mm-dd' format.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the total sales for each item for each outlet.\n\n    Requirements:\n    - pandas\n    - datetime\n    - random\n    - matplotlib.pyplot\n    - json\n    - os\n\n    Example:\n    >>> report = daily_sales_report('2023-06-15')\n    >>> print(report)\n    >>> report.plot(kind='bar')\n    \"\"\"\n    # Check if the sales data for the given day exists\n    filename = f'sales_data/sales_{day}.json'\n    if not os.path.exists(filename):\n        raise FileNotFoundError(f'Sales data for {day} not found.')\n\n    # Load the sales data\n    with open(filename, 'r') as file:\n        transactions = json.load(file)\n\n    # Prepare the sales report\n    sales_data = []\n    for transaction in transactions:\n        sales_data.append([transaction['item'], transaction['quantity'], transaction['outlet']])\n\n    sales_df = pd.DataFrame(sales_data, columns=['Item', 'Quantity', 'Outlet'])\n    sales_report = sales_df.groupby(['Item', 'Outlet']).sum()\n\n    return sales_report\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, pandas, random, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nfrom random import choice, randint\nimport matplotlib.pyplot as plt\n\n# Constants\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPLAYERS = ['Player 1', 'Player 2', 'Player 3', 'Player 4', 'Player 5',\n           'Player 6', 'Player 7', 'Player 8', 'Player 9', 'Player 10']\n\ndef generate_match_data(n_matches):\n    \"\"\"\n    Generate data for a series of soccer matches. Each match is between two teams and each \n    team has a set of players. The data includes the team names, player names, and the number \n    of goals scored by each player in each match. The data is returned as a pandas DataFrame \n    and also visualized using a bar plot.\n\n    Parameters:\n    n_matches (int): The number of matches.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the match data.\n\n    Requirements:\n    - numpy\n    - pandas\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> match_data = generate_match_data(5)\n    >>> print(match_data)\n    >>> match_data.groupby('Player')['Goals'].sum().plot(kind='bar')\n    \"\"\"\n    match_data = []\n    for i in range(n_matches):\n        team1, team2 = np.random.choice(TEAMS, size=2, replace=False)\n        for player in PLAYERS:\n            team = choice([team1, team2])\n            goals = randint(0, 3)\n            match_data.append([i+1, team, player, goals])\n\n    df = pd.DataFrame(match_data, columns=['Match', 'Team', 'Player', 'Goals'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \npandas, datetime, random, matplotlib.pyplot, seaborn\n```python\nimport pandas as pd\nfrom datetime import datetime, timedelta\nfrom random import randint, choice\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nUSERS = ['User1', 'User2', 'User3', 'User4', 'User5']\nACTIVITIES = ['Walking', 'Running', 'Cycling', 'Swimming', 'Weight Training']\n\ndef generate_user_activity_data(n_days):\n    \"\"\"\n    Generate user activity data for a fitness app. The data includes the user name, \n    the activity, the duration of the activity in minutes, and the calories burned. \n    The data is returned as a pandas DataFrame and also visualized using a scatter plot.\n\n    Parameters:\n    n_days (int): The number of days for which the data is to be generated.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the user activity data.\n\n    Requirements:\n    - pandas\n    - datetime\n    - random\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> activity_data = generate_user_activity_data(7)\n    >>> print(activity_data)\n    >>> sns.scatterplot(data=activity_data, x='Duration', y='Calories', hue='User')\n    \"\"\"\n    start_date = datetime.now() - timedelta(days=n_days)\n    activity_data = []\n    for i in range(n_days):\n        user = choice(USERS)\n        activity = choice(ACTIVITIES)\n        duration = randint(10, 60)\n        calories = duration * randint(5, 10)\n        activity_data.append([start_date + timedelta(days=i), user, activity, duration, calories])\n\n    df = pd.DataFrame(activity_data, columns=['Date', 'User', 'Activity', 'Duration', 'Calories'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, pandas, random, matplotlib.pyplot, seaborn\n```python\nimport numpy as np\nimport pandas as pd\nfrom random import choice, randint\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nCOUNTRIES = ['Country A', 'Country B', 'Country C', 'Country D', 'Country E']\nPRODUCTS = ['Product 1', 'Product 2', 'Product 3', 'Product 4', 'Product 5']\n\ndef generate_sales_data(n_days):\n    \"\"\"\n    Generate sales data for a global company. The data includes the country, the product, \n    the quantity sold, and the sales revenue. The data is returned as a pandas DataFrame \n    and also visualized using a scatter plot.\n\n    Parameters:\n    n_days (int): The number of days for which the data is to be generated.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the sales data.\n\n    Requirements:\n    - numpy\n    - pandas\n    - random\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> sales_data = generate_sales_data(30)\n    >>> print(sales_data)\n    >>> sns.scatterplot(data=sales_data, x='Quantity', y='Revenue', hue='Country')\n    \"\"\"\n    sales_data = []\n    for i in range(n_days):\n        country = choice(COUNTRIES)\n        product = choice(PRODUCTS)\n        quantity = randint(1, 100)\n        revenue = quantity * randint(10, 100)\n        sales_data.append([i+1, country, product, quantity, revenue])\n\n    df = pd.DataFrame(sales_data, columns=['Day', 'Country', 'Product', 'Quantity', 'Revenue'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, datetime, random, matplotlib.pyplot, seaborn\n```python\nimport pandas as pd\nfrom datetime import datetime, timedelta\nfrom random import choice, randint\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nSTUDENTS = ['Student1', 'Student2', 'Student3', 'Student4', 'Student5']\nCOURSES = ['Math', 'Science', 'History', 'English', 'Art']\n\ndef generate_student_grade_data(n_semesters):\n    \"\"\"\n    Generate student grade data for a school. The data includes the student name, \n    the course, the semester, and the grade. The data is returned as a pandas DataFrame \n    and also visualized using a box plot.\n\n    Parameters:\n    n_semesters (int): The number of semesters for which the data is to be generated.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the student grade data.\n\n    Requirements:\n    - pandas\n    - datetime\n    - random\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> grade_data = generate_student_grade_data(3)\n    >>> print(grade_data)\n    >>> sns.boxplot(data=grade_data, x='Course', y='Grade', hue='Student')\n    \"\"\"\n    grade_data = []\n    for i in range(1, n_semesters+1):\n        for student in STUDENTS:\n            for course in COURSES:\n                grade = randint(50, 100)\n                grade_data.append([i, student, course, grade])\n\n    df = pd.DataFrame(grade_data, columns=['Semester', 'Student', 'Course', 'Grade'])\n\n    return df\n```\n"}
{"seed": "{\"task_id\": 4842956, \"prompt\": \"def f_4842956(list1):\\n\\t\", \"suffix\": \"\\n\\treturn list2\", \"canonical_solution\": \"list2 = [x for x in list1 if x]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([[\\\"a\\\"], [], [\\\"b\\\"]]) == [[\\\"a\\\"], [\\\"b\\\"]]\\n\", \"\\n    assert candidate([[], [1,2,3], [], [\\\"b\\\"]]) == [[1,2,3], [\\\"b\\\"]]\\n\"], \"entry_point\": \"f_4842956\", \"intent\": \"Create `list2` to contain the lists from list `list1` excluding the empty lists from `list1`\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, random, itertools\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import randint\nimport itertools\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef generate_dataframe(list_of_lists):\n    \"\"\"\n    Generate a DataFrame from a list of lists. For each list in `list_of_lists`, \n    Generate a new row in the DataFrame. If a list is empty, fill the row with \n    random integers instead.\n    \n    Parameters:\n    list_of_lists (list): The list of lists.\n    \n    Returns:\n    DataFrame: A pandas DataFrame.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - random\n    - itertools\n    \n    Example:\n    >>> generate_dataframe([[1, 2, 3, 4, 5], [], [6, 7, 8, 9, 10]])\n    \"\"\"\n    data = []\n    for list_ in list_of_lists:\n        if list_:\n            data.append(list_)\n        else:\n            data.append([randint(0, 100) for _ in range(len(COLUMNS))])\n\n    df = pd.DataFrame(data, columns=COLUMNS)\n    \n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncollections, itertools, random\n```python\nfrom collections import Counter\nimport itertools\nfrom random import sample\n\n# Constants\nALPHABET = 'abcdefghijklmnopqrstuvwxyz'\n\ndef count_letters(list_of_lists):\n    \"\"\"\n    Count the frequency of each letter in a list of lists. If a list is empty, \n    fill it with a random sample from the alphabet and then count the letters.\n    \n    Parameters:\n    list_of_lists (list): The list of lists.\n    \n    Returns:\n    Counter: A Counter object with the frequency of each letter.\n    \n    Requirements:\n    - collections.Counter\n    - itertools\n    - random.sample\n    \n    Example:\n    >>> count_letters([['a', 'b', 'c'], [], ['d', 'e', 'f']])\n    \"\"\"\n    flattened_list = list(itertools.chain(*list_of_lists))\n\n    if [] in list_of_lists:\n        flattened_list += sample(ALPHABET, 10)\n\n    counter = Counter(flattened_list)\n    \n    return counter\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, random, seaborn, matplotlib\n```python\nimport numpy as np\nfrom random import randint\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nCATEGORIES = ['A', 'B', 'C', 'D', 'E']\n\ndef generate_histogram(list_of_lists):\n    \"\"\"\n    Generate a histogram for a list of lists. If a list is empty, fill it with \n    random integers and then generate the histogram.\n    \n    Parameters:\n    list_of_lists (list): The list of lists.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - numpy\n    - random\n    - seaborn\n    - matplotlib.pyplot\n    \n    Example:\n    >>> generate_histogram([[1, 2, 3], [], [4, 5, 6]])\n    \"\"\"\n    data = []\n    for list_ in list_of_lists:\n        if list_:\n            data += list_\n        else:\n            data += [randint(0, 100) for _ in range(len(CATEGORIES))]\n\n    sns.histplot(data)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, random, sklearn.preprocessing\n```python\nimport numpy as np\nfrom random import randint\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Constants\nRANGE = (0, 1)\n\ndef scale_lists(list_of_lists):\n    \"\"\"\n    Scale the values in a list of lists to a specified range using MinMaxScaler. \n    If a list is empty, fill it with random integers and then scale the values.\n    \n    Parameters:\n    list_of_lists (list): The list of lists.\n    \n    Returns:\n    list: A list of lists with scaled values.\n    \n    Requirements:\n    - numpy\n    - random\n    - sklearn.preprocessing.MinMaxScaler\n    \n    Example:\n    >>> scale_lists([[1, 2, 3], [], [4, 5, 6]])\n    \"\"\"\n    data = []\n    for list_ in list_of_lists:\n        if list_:\n            data.append(list_)\n        else:\n            data.append([randint(0, 100) for _ in range(5)])\n    \n    scaler = MinMaxScaler(feature_range=RANGE)\n    scaled_data = scaler.fit_transform(data)\n    \n    return scaled_data.tolist()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, random, scipy.stats\n```python\nimport numpy as np\nfrom random import randint\nfrom scipy import stats\n\n# Constants\nSIZE = 5\n\ndef calculate_statistics(list_of_lists):\n    \"\"\"\n    Calculate the mean, median, and mode of the values in a list of lists. \n    If a list is empty, fill it with random integers and then calculate the statistics.\n    \n    Parameters:\n    list_of_lists (list): The list of lists.\n    \n    Returns:\n    dict: A dictionary with the mean, median, and mode.\n    \n    Requirements:\n    - numpy\n    - random\n    - scipy.stats\n    \n    Example:\n    >>> calculate_statistics([[1, 2, 3], [], [4, 5, 6]])\n    \"\"\"\n    data = []\n    for list_ in list_of_lists:\n        if list_:\n            data += list_\n        else:\n            data += [randint(0, 100) for _ in range(SIZE)]\n    \n    return {\n        'mean': np.mean(data),\n        'median': np.median(data),\n        'mode': stats.mode(data)[0][0]\n    }\n```"}
{"seed": "{\"task_id\": 9262278, \"prompt\": \"def f_9262278(data):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"HttpResponse(data, content_type='application/json')\", \"test_start\": \"\\nimport os\\nimport json\\nfrom django.http import HttpResponse\\nfrom django.conf import settings\\nif not settings.configured:\\n    settings.configure(DEBUG=True)\\n\\ndef check(candidate):\", \"test\": [\"\\n    if settings.DEBUG:\\n        assert candidate(json.dumps({\\\"Sample-Key\\\": \\\"Sample-Value\\\"})).content == b'{\\\"Sample-Key\\\": \\\"Sample-Value\\\"}'\\n\", \"\\n    if settings.DEBUG:\\n        assert candidate(json.dumps({\\\"Sample-Key\\\": \\\"Sample-Value\\\"}))['content-type'] == 'application/json'\\n\"], \"entry_point\": \"f_9262278\", \"intent\": \"Django response with JSON `data`\", \"library\": [\"django\", \"json\", \"os\"]}", "generation": "Scenario 1:\nDjango, json, os, hashlib, base64\n```python\nimport os\nimport json\nfrom django.http import HttpResponse\nfrom django.conf import settings\nimport hashlib\nimport base64\n\nif not settings.configured:\n    settings.configure(DEBUG=True)\n\ndef generate_hashed_response(data):\n    \"\"\"\n    Generate a Django HttpResponse with JSON data and a hashed content based \n    on the provided data. The hash is added to the HTTP headers.\n    \n    Parameters:\n    data (str): The data to be included in the response body.\n    \n    Returns:\n    HttpResponse: A Django HttpResponse with JSON data and hashed content.\n    \n    Requirements:\n    - django\n    - json\n    - os\n    - hashlib\n    - base64\n\n    Example:\n    >>> generate_hashed_response(json.dumps({\"Sample-Key\": \"Sample-Value\"}))\n    \"\"\"\n    response = HttpResponse(data, content_type='application/json')\n\n    # Generate a hash of the data\n    hash_object = hashlib.sha256(data.encode())\n    content_hash = base64.b64encode(hash_object.digest()).decode()\n\n    # Add the content hash to the response headers\n    response['Content-Hash'] = content_hash\n\n    return response\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nDjango, json, os, random, time\n```python\nimport os\nimport json\nfrom django.http import HttpResponse\nfrom django.conf import settings\nimport random\nimport time\n\nif not settings.configured:\n    settings.configure(DEBUG=True)\n\ndef generate_delayed_response(data, min_delay, max_delay):\n    \"\"\"\n    Generate a Django HttpResponse with JSON data after a random delay to \n    simulate network latency.\n    \n    Parameters:\n    data (str): The data to be included in the response body.\n    min_delay (int): The minimum delay in seconds.\n    max_delay (int): The maximum delay in seconds.\n    \n    Returns:\n    HttpResponse: A Django HttpResponse with JSON data.\n    \n    Requirements:\n    - django\n    - json\n    - os\n    - random\n    - time\n\n    Example:\n    >>> generate_delayed_response(json.dumps({\"Sample-Key\": \"Sample-Value\"}), 1, 5)\n    \"\"\"\n    # Generate a random delay\n    delay = random.uniform(min_delay, max_delay)\n\n    # Wait for the delay\n    time.sleep(delay)\n\n    response = HttpResponse(data, content_type='application/json')\n\n    return response\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nDjango, json, os, datetime, pytz\n```python\nimport os\nimport json\nfrom django.http import HttpResponse\nfrom django.conf import settings\nfrom datetime import datetime\nimport pytz\n\nif not settings.configured:\n    settings.configure(DEBUG=True)\n\ndef generate_timestamped_response(data):\n    \"\"\"\n    Generate a Django HttpResponse with JSON data, including a timestamp in \n    the HTTP headers.\n    \n    Parameters:\n    data (str): The data to be included in the response body.\n    \n    Returns:\n    HttpResponse: A Django HttpResponse with JSON data and timestamp.\n    \n    Requirements:\n    - django\n    - json\n    - os\n    - datetime\n    - pytz\n\n    Example:\n    >>> generate_timestamped_response(json.dumps({\"Sample-Key\": \"Sample-Value\"}))\n    \"\"\"\n    response = HttpResponse(data, content_type='application/json')\n\n    # Generate a timestamp\n    now = datetime.now(pytz.UTC)\n    timestamp = now.isoformat()\n\n    # Add the timestamp to the response headers\n    response['Timestamp'] = timestamp\n\n    return response\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nDjango, json, os, uuid\n```python\nimport os\nimport json\nfrom django.http import HttpResponse\nfrom django.conf import settings\nimport uuid\n\nif not settings.configured:\n    settings.configure(DEBUG=True)\n\ndef generate_uuid_response(data):\n    \"\"\"\n    Generate a Django HttpResponse with JSON data, including a UUID in the \n    HTTP headers for request tracking.\n    \n    Parameters:\n    data (str): The data to be included in the response body.\n    \n    Returns:\n    HttpResponse: A Django HttpResponse with JSON data and UUID.\n    \n    Requirements:\n    - django\n    - json\n    - os\n    - uuid\n\n    Example:\n    >>> generate_uuid_response(json.dumps({\"Sample-Key\": \"Sample-Value\"}))\n    \"\"\"\n    response = HttpResponse(data, content_type='application/json')\n\n    # Generate a UUID\n    request_uuid = uuid.uuid4()\n\n    # Add the UUID to the response headers\n    response['UUID'] = str(request_uuid)\n\n    return response\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nDjango, json, os, re\n```python\nimport os\nimport json\nfrom django.http import HttpResponse\nfrom django.conf import settings\nimport re\n\nif not settings.configured:\n    settings.configure(DEBUG=True)\n\ndef generate_sanitized_response(data):\n    \"\"\"\n    Generate a Django HttpResponse with JSON data, where the data has been \n    sanitized to remove any email addresses.\n    \n    Parameters:\n    data (str): The data to be included in the response body.\n    \n    Returns:\n    HttpResponse: A Django HttpResponse with sanitized JSON data.\n    \n    Requirements:\n    - django\n    - json\n    - os\n    - re\n\n    Example:\n    >>> generate_sanitized_response(json.dumps({\"email\": \"test@example.com\"}))\n    \"\"\"\n    # Sanitize the data\n    sanitized_data = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '', data)\n\n    response = HttpResponse(sanitized_data, content_type='application/json')\n\n    return response\n```"}
{"seed": "{\"task_id\": 17284947, \"prompt\": \"def f_17284947(example_str):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"re.findall('(.*?)\\\\\\\\[.*?\\\\\\\\]', example_str)\", \"test_start\": \"\\nimport re \\n\\ndef check(candidate):  \", \"test\": [\"\\n    list_elems = candidate(\\\"Josie Smith [3996 COLLEGE AVENUE, SOMETOWN, MD 21003]Mugsy Dog Smith [2560 OAK ST, GLENMEADE, WI 14098]\\\")\\n    assert \\\"\\\".join(list_elems).strip() == 'Josie Smith Mugsy Dog Smith'\\n\"], \"entry_point\": \"f_17284947\", \"intent\": \"get all text that is not enclosed within square brackets in string `example_str`\", \"library\": [\"re\"]}", "generation": "Scenario 1: \npandas, re, nltk, sklearn, matplotlib\n```python\nimport pandas as pd\nimport re\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\n\n# Constants\nSTOPWORDS = set(stopwords.words('english'))\n\ndef analyze_text_data(text_data):\n    \"\"\"\n    Extract all text that is not enclosed within square brackets in a pandas DataFrame column \n    and perform a word count analysis excluding English stopwords.\n\n    Parameters:\n    text_data (DataFrame): The pandas DataFrame with a 'text' column.\n\n    Returns:\n    DataFrame: A pandas DataFrame with word counts.\n\n    Requirements:\n    - pandas\n    - re\n    - nltk.corpus.stopwords\n    - sklearn.feature_extraction.text.CountVectorizer\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame({'text': ['Josie Smith [3996 COLLEGE AVENUE, SOMETOWN, MD 21003]', \n    >>>                            'Mugsy Dog Smith [2560 OAK ST, GLENMEADE, WI 14098]']})\n    >>> word_counts = analyze_text_data(df)\n    >>> print(word_counts)\n    >>> word_counts.plot(kind='bar')\n    \"\"\"\n    text_data['text'] = text_data['text'].apply(lambda x: ' '.join(re.findall('(.*?)\\\\[.*?\\\\]', x)))\n    text_data['text'] = text_data['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in STOPWORDS]))\n\n    vectorizer = CountVectorizer()\n    X = vectorizer.fit_transform(text_data['text'])\n\n    word_counts = pd.DataFrame(data=X.toarray(), columns=vectorizer.get_feature_names_out())\n\n    return word_counts.sum().sort_values(ascending=False)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nre, os, shutil, datetime\n```python\nimport re\nimport os\nimport shutil\nfrom datetime import datetime\n\n# Constants\nDATE_FORMAT = '%Y%m%d%H%M%S'\n\ndef organize_files_by_content(directory):\n    \"\"\"\n    Organize files in a directory based on the first text that is not enclosed within square brackets in their content.\n    Move the files to subdirectories named after this text.\n\n    Parameters:\n    directory (str): The directory path.\n\n    Returns:\n    str: The directory path with organized files.\n\n    Requirements:\n    - re\n    - os\n    - shutil\n    - datetime\n\n    Example:\n    >>> organize_files_by_content('./my_directory')\n    \"\"\"\n    for filename in os.listdir(directory):\n        with open(os.path.join(directory, filename), 'r') as file:\n            content = file.read()\n            match = re.search('(.*?)\\\\[.*?\\\\]', content)\n            if match:\n                subdirectory = match.group(1).strip()\n\n                if not os.path.exists(os.path.join(directory, subdirectory)):\n                    os.makedirs(os.path.join(directory, subdirectory))\n\n                new_filename = f\"{filename.split('.')[0]}_{datetime.now().strftime(DATE_FORMAT)}.{filename.split('.')[1]}\"\n                shutil.move(os.path.join(directory, filename), os.path.join(directory, subdirectory, new_filename))\n\n    return directory\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nre, openpyxl, pandas\n```python\nimport re\nimport openpyxl\nimport pandas as pd\n\ndef extract_data_from_excel(file_path):\n    \"\"\"\n    Extract all data from an Excel file that is not enclosed within square brackets and put it in a pandas DataFrame.\n\n    Parameters:\n    file_path (str): The path to the Excel file.\n\n    Returns:\n    DataFrame: The pandas DataFrame with the extracted data.\n\n    Requirements:\n    - re\n    - openpyxl\n    - pandas\n\n    Example:\n    >>> df = extract_data_from_excel('./my_file.xlsx')\n    >>> print(df)\n    \"\"\"\n    wb = openpyxl.load_workbook(file_path, read_only=True)\n    ws = wb.active\n\n    data = []\n\n    for row in ws.iter_rows():\n        row_data = []\n        for cell in row:\n            if cell.value:\n                row_data.append(' '.join(re.findall('(.*?)\\\\[.*?\\\\]', str(cell.value))))\n        data.append(row_data)\n\n    df = pd.DataFrame(data)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nre, sklearn, numpy\n```python\nimport re\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ndef calculate_tfidf(example_str):\n    \"\"\"\n    Extract all text that is not enclosed within square brackets in a string and calculate the TF-IDF scores.\n\n    Parameters:\n    example_str (str): The input string.\n\n    Returns:\n    dict: A dictionary with words as keys and TF-IDF scores as values.\n\n    Requirements:\n    - re\n    - sklearn.feature_extraction.text.TfidfVectorizer\n    - numpy\n\n    Example:\n    >>> tfidf_scores = calculate_tfidf(\"Josie Smith [3996 COLLEGE AVENUE, SOMETOWN, MD 21003] Mugsy Dog Smith [2560 OAK ST, GLENMEADE, WI 14098]\")\n    >>> print(tfidf_scores)\n    \"\"\"\n    text = ' '.join(re.findall('(.*?)\\\\[.*?\\\\]', example_str))\n\n    tfidf_vectorizer = TfidfVectorizer()\n    tfidf_matrix = tfidf_vectorizer.fit_transform([text])\n\n    feature_names = tfidf_vectorizer.get_feature_names_out()\n    tfidf_scores = dict(zip(feature_names, np.squeeze(tfidf_matrix.toarray())))\n\n    return tfidf_scores\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nre, nltk, matplotlib\n```python\nimport re\nimport nltk\nimport matplotlib.pyplot as plt\nfrom nltk.probability import FreqDist\n\ndef plot_word_frequency(example_str):\n    \"\"\"\n    Extract all text that is not enclosed within square brackets in a string and plot a frequency distribution of words.\n\n    Parameters:\n    example_str (str): The input string.\n\n    Returns:\n    None\n\n    Requirements:\n    - re\n    - nltk\n    - nltk.probability.FreqDist\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_word_frequency(\"Josie Smith [3996 COLLEGE AVENUE, SOMETOWN, MD 21003] Mugsy Dog Smith [2560 OAK ST, GLENMEADE, WI 14098]\")\n    \"\"\"\n    text = ' '.join(re.findall('(.*?)\\\\[.*?\\\\]', example_str))\n\n    words = nltk.word_tokenize(text)\n    fdist = FreqDist(words)\n\n    fdist.plot(30, cumulative=False)\n    plt.show()\n```\nAbove is the illustration."}
{"seed": "{\"task_id\": 17284947, \"prompt\": \"def f_17284947(example_str):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"re.findall('(.*?)(?:\\\\\\\\[.*?\\\\\\\\]|$)', example_str)\", \"test_start\": \"\\nimport re \\n\\ndef check(candidate): \", \"test\": [\"\\n    list_elems = candidate(\\\"Josie Smith [3996 COLLEGE AVENUE, SOMETOWN, MD 21003]Mugsy Dog Smith [2560 OAK ST, GLENMEADE, WI 14098]\\\")\\n    assert \\\"\\\".join(list_elems).strip() == 'Josie Smith Mugsy Dog Smith'\\n\"], \"entry_point\": \"f_17284947\", \"intent\": \"Use a regex to get all text in a string `example_str` that is not surrounded by square brackets\", \"library\": [\"re\"]}", "generation": "Scenario 1: \nre, pandas, os, pathlib\n```python\nimport re\nimport pandas as pd\nimport os\nfrom pathlib import Path\n\n# Constants\nDATA_DIRECTORY = Path('/path/to/your/data/directory')\n\ndef extract_names_from_files(directory=DATA_DIRECTORY):\n    \"\"\"\n    Extract all names from text files within a specified directory that are not surrounded by square brackets.\n    The names are then stored into a pandas DataFrame.\n\n    Parameters:\n    directory (Path): The directory where the text files are stored.\n\n    Returns:\n    DataFrame: A pandas DataFrame with extracted names from all files.\n\n    Requirements:\n    - re\n    - pandas\n    - os\n    - pathlib\n\n    Example:\n    >>> names_df = extract_names_from_files()\n    >>> print(names_df)\n    \"\"\"\n    files = [f for f in os.listdir(directory) if f.endswith('.txt')]\n    names = []\n\n    for file in files:\n        with open(directory / file, 'r') as f:\n            text = f.read()\n            names.extend(re.findall('(.*?)(?:\\\\[.*?\\\\]|$)', text))\n\n    names_df = pd.DataFrame(names, columns=['Name'])\n\n    return names_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nre, pandas, numpy, matplotlib\n```python\nimport re\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nTEXT = \"Josie Smith [3996 COLLEGE AVENUE, SOMETOWN, MD 21003]Mugsy Dog Smith [2560 OAK ST, GLENMEADE, WI 14098]\"\n\ndef plot_name_frequencies(text=TEXT):\n    \"\"\"\n    Extract all names from a string that are not surrounded by square brackets, count the frequency of each name,\n    and plot a bar chart of the name frequencies.\n\n    Parameters:\n    text (str): The text from which to extract names.\n\n    Returns:\n    Series: A pandas Series with the frequency of each name.\n\n    Requirements:\n    - re\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> name_freqs = plot_name_frequencies()\n    >>> print(name_freqs)\n    >>> name_freqs.plot(kind='bar')\n    \"\"\"\n    names = re.findall('(.*?)(?:\\\\[.*?\\\\]|$)', text)\n    name_freqs = pd.Series(names).value_counts()\n\n    return name_freqs\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nre, nltk, collections\n```python\nimport re\nfrom nltk.tokenize import word_tokenize\nfrom collections import Counter\n\n# Constants\nTEXT = \"Josie Smith [3996 COLLEGE AVENUE, SOMETOWN, MD 21003]Mugsy Dog Smith [2560 OAK ST, GLENMEADE, WI 14098]\"\n\ndef count_word_frequencies(text=TEXT):\n    \"\"\"\n    Extract all names from a string that are not surrounded by square brackets, tokenize the names into words, \n    and count the frequency of each word.\n\n    Parameters:\n    text (str): The text from which to extract names and count word frequencies.\n\n    Returns:\n    Counter: A counter object with the frequency of each word.\n\n    Requirements:\n    - re\n    - nltk.tokenize\n    - collections\n\n    Example:\n    >>> word_freqs = count_word_frequencies()\n    >>> print(word_freqs)\n    \"\"\"\n    names = re.findall('(.*?)(?:\\\\[.*?\\\\]|$)', text)\n    words = word_tokenize(' '.join(names))\n    word_freqs = Counter(words)\n\n    return word_freqs\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nre, json, requests\n```python\nimport re\nimport json\nimport requests\n\n# Constants\nURL = \"https://api.example.com/data\"\n\ndef fetch_and_extract_names(url=URL):\n    \"\"\"\n    Fetch data from a specified URL, extract all names from the data that are not surrounded by square brackets.\n\n    Parameters:\n    url (str): The URL from which to fetch data.\n\n    Returns:\n    list: A list of extracted names.\n\n    Requirements:\n    - re\n    - json\n    - requests\n\n    Example:\n    >>> names = fetch_and_extract_names()\n    >>> print(names)\n    \"\"\"\n    response = requests.get(url)\n    data = json.loads(response.text)\n\n    names = re.findall('(.*?)(?:\\\\[.*?\\\\]|$)', data)\n\n    return names\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nre, os, smtplib\n```python\nimport re\nimport os\nimport smtplib\n\n# Constants\nTEXT = \"Josie Smith [3996 COLLEGE AVENUE, SOMETOWN, MD 21003]Mugsy Dog Smith [2560 OAK ST, GLENMEADE, WI 14098]\"\nSMTP_SERVER = \"smtp.gmail.com\"\nSMTP_PORT = 587\nEMAIL_ADDRESS = \"your.email@gmail.com\"\nEMAIL_PASSWORD = \"your.password\"\n\ndef send_extracted_names(text=TEXT, smtp_server=SMTP_SERVER, smtp_port=SMTP_PORT, email_address=EMAIL_ADDRESS, email_password=EMAIL_PASSWORD):\n    \"\"\"\n    Extract all names from a string that are not surrounded by square brackets, and send the names in an email.\n\n    Parameters:\n    text (str): The text from which to extract names.\n    smtp_server (str): The SMTP server to use for sending the email.\n    smtp_port (int): The port to use for the SMTP server.\n    email_address (str): The email address from which to send the email.\n    email_password (str): The password for the email address.\n\n    Requirements:\n    - re\n    - os\n    - smtplib\n\n    Example:\n    >>> send_extracted_names()\n    \"\"\"\n    names = re.findall('(.*?)(?:\\\\[.*?\\\\]|$)', text)\n\n    message = 'Subject: Extracted Names\\n\\n' + '\\n'.join(names)\n\n    server = smtplib.SMTP(smtp_server, smtp_port)\n    server.starttls()\n    server.login(email_address, email_password)\n    server.sendmail(email_address, email_address, message)\n    server.quit()\n```\n"}
{"seed": "{\"task_id\": 14182339, \"prompt\": \"def f_14182339():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"re.findall('\\\\\\\\(.+?\\\\\\\\)|\\\\\\\\w', '(zyx)bc')\", \"test_start\": \"\\nimport re \\n\\ndef check(candidate):    \", \"test\": [\"\\n    assert candidate() == ['(zyx)', 'b', 'c']\\n\"], \"entry_point\": \"f_14182339\", \"intent\": \"get whatever is between parentheses as a single match, and any char outside as an individual match in string '(zyx)bc'\", \"library\": [\"re\"]}", "generation": "Scenario 1: \nre, os, pathlib, glob\n```python\nimport re\nimport os\nfrom pathlib import Path\nimport glob\n\n# Constants\nFILE_PATTERN = '*.txt'\nREGEX_PATTERN = r'\\(.+?\\)|\\w'\n\ndef extract_matches_from_files(directory_path):\n    \"\"\"\n    Extracts matches from all text files in a directory based on a regex pattern.\n    It gets whatever is between parentheses as a single match, and any char outside \n    as an individual match in string.\n    \n    Parameters:\n    directory_path (str): The path to the directory.\n    \n    Returns:\n    dict: A dictionary where keys are file names and values are lists of matches.\n\n    Requirements:\n    - re\n    - os\n    - pathlib.Path\n    - glob.glob\n    \n    Example:\n    >>> matches = extract_matches_from_files('/path/to/directory')\n    >>> print(matches)\n    \"\"\"\n    match_dict = {}\n    file_paths = glob.glob(os.path.join(directory_path, FILE_PATTERN))\n\n    for file_path in file_paths:\n        with open(file_path, 'r') as file:\n            content = file.read()\n            matches = re.findall(REGEX_PATTERN, content)\n            match_dict[Path(file_path).name] = matches\n\n    return match_dict\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nre, urllib, bs4, requests\n```python\nimport re\nfrom urllib.parse import urljoin\nfrom bs4 import BeautifulSoup\nimport requests\n\n# Constants\nREGEX_PATTERN = r'\\(.+?\\)|\\w'\nBASE_URL = 'https://example.com/'\n\ndef extract_matches_from_webpage(url):\n    \"\"\"\n    Extracts matches from a webpage based on a regex pattern.\n    It gets whatever is between parentheses as a single match, and any char outside \n    as an individual match in string.\n    \n    Parameters:\n    url (str): The url of the webpage.\n    \n    Returns:\n    list: A list of matches.\n\n    Requirements:\n    - re\n    - urllib.parse.urljoin\n    - bs4.BeautifulSoup\n    - requests\n    \n    Example:\n    >>> matches = extract_matches_from_webpage('https://example.com/page')\n    >>> print(matches)\n    \"\"\"\n    url = urljoin(BASE_URL, url)\n    response = requests.get(url)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    text = soup.get_text()\n    matches = re.findall(REGEX_PATTERN, text)\n\n    return matches\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nre, pandas, numpy\n```python\nimport re\nimport pandas as pd\nimport numpy as np\n\n# Constants\nREGEX_PATTERN = r'\\(.+?\\)|\\w'\nDATA = pd.DataFrame({\n    'Text': ['(zyx)bc', '(abc)def', 'ghi(jkl)', 'mno(pqr)stu', 'vwx(yz)'],\n    'Number': np.arange(5)\n})\n\ndef extract_matches_from_dataframe(data):\n    \"\"\"\n    Extracts matches from a pandas DataFrame column based on a regex pattern.\n    It gets whatever is between parentheses as a single match, and any char outside \n    as an individual match in string.\n    \n    Parameters:\n    data (DataFrame): The pandas DataFrame.\n    \n    Returns:\n    DataFrame: A DataFrame with matches as a new column.\n\n    Requirements:\n    - re\n    - pandas\n    - numpy\n    \n    Example:\n    >>> matches_df = extract_matches_from_dataframe(DATA)\n    >>> print(matches_df)\n    \"\"\"\n    data['Matches'] = data['Text'].apply(lambda x: re.findall(REGEX_PATTERN, x))\n\n    return data\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nre, csv, collections\n```python\nimport re\nimport csv\nfrom collections import Counter\n\n# Constants\nREGEX_PATTERN = r'\\(.+?\\)|\\w'\nCSV_FILE = 'data.csv'\n\ndef count_matches_from_csv(file_path):\n    \"\"\"\n    Counts the matches from a CSV file based on a regex pattern.\n    It gets whatever is between parentheses as a single match, and any char outside \n    as an individual match in string.\n    \n    Parameters:\n    file_path (str): The path to the CSV file.\n    \n    Returns:\n    dict: The dictionary with counts of matches.\n\n    Requirements:\n    - re\n    - csv\n    - collections.Counter\n    \n    Example:\n    >>> counts = count_matches_from_csv(CSV_FILE)\n    >>> print(counts)\n    \"\"\"\n    with open(file_path, 'r') as file:\n        reader = csv.reader(file)\n        text = ' '.join(row[0] for row in reader)\n        matches = re.findall(REGEX_PATTERN, text)\n\n    counts = Counter(matches)\n\n    return dict(counts)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nre, json, os\n```python\nimport re\nimport json\nimport os\n\n# Constants\nREGEX_PATTERN = r'\\(.+?\\)|\\w'\n\ndef extract_matches_from_json(file_path):\n    \"\"\"\n    Extracts matches from a JSON file based on a regex pattern.\n    It gets whatever is between parentheses as a single match, and any char outside \n    as an individual match in string.\n    \n    Parameters:\n    file_path (str): The path to the JSON file.\n    \n    Returns:\n    dict: The dictionary with file name as key and matches as values.\n\n    Requirements:\n    - re\n    - json\n    - os\n    \n    Example:\n    >>> matches = extract_matches_from_json('data.json')\n    >>> print(matches)\n    \"\"\"\n    with open(file_path, 'r') as file:\n        data = json.load(file)\n        text = ' '.join(data.values())\n        matches = re.findall(REGEX_PATTERN, text)\n\n    match_dict = {os.path.basename(file_path): matches}\n\n    return match_dict\n```\n"}
{"seed": "{\"task_id\": 14182339, \"prompt\": \"def f_14182339():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"re.findall('\\\\\\\\((.*?)\\\\\\\\)|(\\\\\\\\w)', '(zyx)bc')\", \"test_start\": \"\\nimport re \\n\\ndef check(candidate): \", \"test\": [\"\\n    assert candidate() == [('zyx', ''), ('', 'b'), ('', 'c')]\\n\"], \"entry_point\": \"f_14182339\", \"intent\": \"match regex '\\\\\\\\((.*?)\\\\\\\\)|(\\\\\\\\w)' with string '(zyx)bc'\", \"library\": [\"re\"]}", "generation": "Scenario 1: \nre, os, glob, pandas\n```python\nimport re\nimport os\nimport glob\nimport pandas as pd\n\n# Constants\nFILE_DIR = '/path/to/files'\nFILE_PATTERN = '*.log'\nLOG_PATTERN = r'ERROR (\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2},\\d{3}): (.*?)$'\n\ndef find_errors_in_logs():\n    \"\"\"\n    Find all ERROR logs in the log files in a directory and return them in a pandas DataFrame.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the timestamp and error message of the ERROR logs.\n    \n    Requirements:\n    - re\n    - os\n    - glob\n    - pandas\n    \n    Example:\n    >>> errors = find_errors_in_logs()\n    >>> print(errors)\n    \"\"\"\n    errors = []\n\n    for file in glob.glob(os.path.join(FILE_DIR, FILE_PATTERN)):\n        with open(file, 'r') as f:\n            for line in f:\n                match = re.match(LOG_PATTERN, line)\n                if match:\n                    timestamp, message = match.groups()\n                    errors.append([timestamp, message])\n\n    errors_df = pd.DataFrame(errors, columns=['Timestamp', 'Message'])\n\n    return errors_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nre, collections, nltk\n```python\nimport re\nfrom collections import Counter\nfrom nltk.corpus import stopwords\n\n# Constants\nTEXT = 'This is a sample text with some words. Some words appear more than once. This is just a sample.'\nSTOPWORDS = set(stopwords.words('english'))\n\ndef count_non_stopwords():\n    \"\"\"\n    Count the number of non-stopword words in a text.\n    \n    Returns:\n    dict: A dictionary with the words and their counts.\n    \n    Requirements:\n    - re\n    - collections\n    - nltk.corpus.stopwords\n    \n    Example:\n    >>> count = count_non_stopwords()\n    >>> print(count)\n    \"\"\"\n    words = re.findall(r'\\b\\w+\\b', TEXT)\n    non_stopwords = [word for word in words if word not in STOPWORDS]\n    count = dict(Counter(non_stopwords))\n\n    return count\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nre, requests, bs4, pandas\n```python\nimport re\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\n# Constants\nURL = 'https://www.example.com'\n\ndef scrape_website():\n    \"\"\"\n    Scrape a website and extract all the URLs and their texts in a pandas DataFrame.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the URLs and their texts.\n    \n    Requirements:\n    - re\n    - requests\n    - bs4.BeautifulSoup\n    - pandas\n    \n    Example:\n    >>> urls = scrape_website()\n    >>> print(urls)\n    \"\"\"\n    response = requests.get(URL)\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    urls = []\n\n    for a in soup.find_all('a', href=True):\n        href = a['href']\n        text = a.text\n        urls.append([href, text])\n\n    urls_df = pd.DataFrame(urls, columns=['URL', 'Text'])\n\n    return urls_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nre, nltk, sklearn\n```python\nimport re\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Constants\nDOCUMENTS = ['This is the first document.', 'This document is the second document.', 'And this is the third one.', 'Is this the first document?']\n\ndef calculate_tfidf():\n    \"\"\"\n    Calculate the TF-IDF scores of the words in a list of documents.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the TF-IDF scores.\n    \n    Requirements:\n    - re\n    - nltk.tokenize\n    - sklearn.feature_extraction.text.TfidfVectorizer\n    \n    Example:\n    >>> tfidf = calculate_tfidf()\n    >>> print(tfidf)\n    \"\"\"\n    vectorizer = TfidfVectorizer(tokenizer=word_tokenize)\n    tfidf_matrix = vectorizer.fit_transform(DOCUMENTS)\n    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n\n    return tfidf_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nre, pandas, matplotlib\n```python\nimport re\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nDATA = {\n    'Name': ['John Doe', 'Jane Doe', 'Alice', 'Bob'],\n    'Email': ['john.doe@example.com', 'jane_doe@example.com', 'alice@example.com', 'bob@example.com'],\n    'Age': [25, 30, 22, 27]\n}\nDF = pd.DataFrame(DATA)\n\ndef plot_age_distribution():\n    \"\"\"\n    Plot the age distribution of people with a certain email pattern.\n    \n    Returns:\n    Series: A pandas Series with the ages and their counts.\n    \n    Requirements:\n    - re\n    - pandas\n    - matplotlib.pyplot\n    \n    Example:\n    >>> ages = plot_age_distribution()\n    >>> print(ages)\n    >>> ages.plot(kind='bar')\n    \"\"\"\n    pattern = r'^[a-z]+\\.[a-z]+@example\\.com$'\n    ages = DF[DF['Email'].str.match(pattern)]['Age'].value_counts().sort_index()\n\n    return ages\n```"}
{"seed": "{\"task_id\": 14182339, \"prompt\": \"def f_14182339():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"re.findall('\\\\\\\\(.*?\\\\\\\\)|\\\\\\\\w', '(zyx)bc')\", \"test_start\": \"\\nimport re \\n\\ndef check(candidate): \", \"test\": [\"\\n    assert candidate() == ['(zyx)', 'b', 'c']\\n\"], \"entry_point\": \"f_14182339\", \"intent\": \"match multiple regex patterns with the alternation operator `|` in a string `(zyx)bc`\", \"library\": [\"re\"]}", "generation": "Scenario 1: \nre, os, glob, pathlib\n```python\nimport re\nimport os\nimport glob\nfrom pathlib import Path\n\n# Constants\nFILE_EXTENSIONS = ['*.txt', '*.md', '*.csv']\n\ndef find_files_with_pattern(pattern, directory, extensions=FILE_EXTENSIONS):\n    \"\"\"\n    Find all files in a given directory that contain a regex pattern in their content.\n    \n    Parameters:\n    pattern (str): The regex pattern to match.\n    directory (str): The directory to search in.\n    extensions (list): The file extensions to consider. Default is ['*.txt', '*.md', '*.csv'].\n    \n    Returns:\n    list: A list of file paths that contain the pattern.\n    \n    Requirements:\n    - os\n    - glob\n    - pathlib\n    - re\n\n    Example:\n    >>> find_files_with_pattern('hello', '/home/user/documents')\n    \"\"\"\n    matched_files = []\n    for ext in extensions:\n        files = glob.glob(os.path.join(directory, ext))\n        for file in files:\n            with open(file, 'r') as f:\n                content = f.read()\n                if re.search(pattern, content):\n                    matched_files.append(Path(file).resolve())\n    return matched_files\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nre, pandas, numpy, matplotlib.pyplot\n```python\nimport re\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Sample DataFrame\n# df = pd.DataFrame({\n#     'Text': ['Hello world', 'Python is cool', 'Regex is fun', 'Hello again', 'Python rocks']\n# })\n\ndef count_pattern_occurrences(df, column, pattern):\n    \"\"\"\n    Count the occurrences of a regex pattern in a pandas DataFrame column and plot a histogram.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n    column (str): The column in which to search for the pattern.\n    pattern (str): The regex pattern to match.\n\n    Returns:\n    Series: A pandas Series with the counts of the pattern in each row.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - re\n\n    Example:\n    >>> df = pd.DataFrame({\n    ...     'Text': ['Hello world', 'Python is cool', 'Regex is fun', 'Hello again', 'Python rocks']\n    ... })\n    >>> count_pattern_occurrences(df, 'Text', 'Python')\n    \"\"\"\n    counts = df[column].apply(lambda x: len(re.findall(pattern, x)))\n    df['Pattern Count'] = counts\n    df['Pattern Count'].plot(kind='hist', rwidth=0.8)\n    plt.show()\n    return counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nre, urllib.request, bs4.BeautifulSoup, collections\n```python\nimport re\nimport urllib.request\nfrom bs4 import BeautifulSoup\nfrom collections import Counter\n\ndef count_words_in_web_page(url, pattern):\n    \"\"\"\n    Count the occurrences of words in a web page that match a regex pattern.\n\n    Parameters:\n    url (str): The URL of the web page.\n    pattern (str): The regex pattern to match words.\n\n    Returns:\n    Counter: A Counter object with the counts of words that match the pattern.\n    \n    Requirements:\n    - urllib.request\n    - bs4.BeautifulSoup\n    - collections\n    - re\n\n    Example:\n    >>> count_words_in_web_page('https://www.example.com', '\\\\b[a-zA-Z0-9]{5,}\\\\b')\n    \"\"\"\n    response = urllib.request.urlopen(url)\n    html = response.read().decode()\n    soup = BeautifulSoup(html, 'html.parser')\n    text = soup.get_text()\n    words = re.findall(pattern, text)\n    return Counter(words)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nre, requests, json, csv\n```python\nimport re\nimport requests\nimport json\nimport csv\n\n# Constants\nAPI_URL = 'https://api.example.com/data'\n\ndef extract_data_from_api(pattern):\n    \"\"\"\n    Make a GET request to an API, extract data that matches a regex pattern, and write it to a CSV file.\n\n    Parameters:\n    pattern (str): The regex pattern to match.\n\n    Returns:\n    str: The path to the CSV file.\n\n    Requirements:\n    - requests\n    - json\n    - csv\n    - re\n\n    Example:\n    >>> extract_data_from_api('\\\\b[A-Z0-9._%+-]+@[A-Z0-9.-]+\\\\.[A-Z]{2,}\\\\b')\n    \"\"\"\n    response = requests.get(API_URL)\n    data = json.loads(response.text)\n    matched_data = [re.findall(pattern, str(item)) for item in data]\n    with open('matched_data.csv', 'w') as f:\n        writer = csv.writer(f)\n        writer.writerows(matched_data)\n    return os.path.abspath('matched_data.csv')\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nre, pandas, matplotlib.pyplot, seaborn\n```python\nimport re\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Sample DataFrame\n# df = pd.DataFrame({\n#     'Text': ['Hello world', 'Python is cool', 'Regex is fun', 'Hello again', 'Python rocks']\n# })\n\ndef visualize_pattern_occurrences(df, column, pattern):\n    \"\"\"\n    Visualize the occurrences of a regex pattern in a pandas DataFrame column using a bar plot.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n    column (str): The column in which to search for the pattern.\n    pattern (str): The regex pattern to match.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - seaborn\n    - re\n\n    Example:\n    >>> df = pd.DataFrame({\n    ...     'Text': ['Hello world', 'Python is cool', 'Regex is fun', 'Hello again', 'Python rocks']\n    ... })\n    >>> visualize_pattern_occurrences(df, 'Text', 'Hello')\n    \"\"\"\n    counts = df[column].apply(lambda x: len(re.findall(pattern, x)))\n    df['Pattern Count'] = counts\n    plt.figure(figsize=(10,5))\n    sns.barplot(x=df.index, y=df['Pattern Count'])\n    plt.xlabel('Index')\n    plt.ylabel('Pattern Count')\n    plt.title('Occurrences of pattern in DataFrame')\n    plt.show()\n```"}
{"seed": "{\"task_id\": 7126916, \"prompt\": \"def f_7126916(elements):\\n\\t\", \"suffix\": \"\\n\\treturn elements\", \"canonical_solution\": \"elements = ['%{0}%'.format(element) for element in elements]\", \"test_start\": \"\\ndef check(candidate): \", \"test\": [\"\\n    elements = ['abc', 'def', 'ijk', 'mno']\\n    assert candidate(elements) == ['%abc%', '%def%', '%ijk%', '%mno%']\\n\", \"\\n    elements = [1, 2, 3, 4, 500]\\n    assert candidate(elements) == ['%1%', '%2%', '%3%', '%4%', '%500%']\\n\"], \"entry_point\": \"f_7126916\", \"intent\": \"formate each string cin list `elements` into pattern '%{0}%'\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, matplotlib, random\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import choice\n\n# Constants\nELEMENTS = ['abc', 'def', 'ijk', 'mno']\nPATTERNS = ['%{}%', '#{}#', '@{}@', '&{}&']\n\ndef pattern_distribution(elements):\n    \"\"\"\n    Format each string in the list `elements` into a pattern randomly chosen from `PATTERNS`,\n    and return a pandas DataFrame containing each element and its corresponding pattern, as well \n    as a bar plot showing the distribution of patterns.\n    \n    Parameters:\n    elements (List[str]): The list of elements.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with elements and their corresponding patterns.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - random\n    \n    Example:\n    >>> patterns_df = pattern_distribution(ELEMENTS)\n    >>> print(patterns_df)\n    >>> patterns_df['Pattern'].value_counts().plot(kind='bar')\n    \"\"\"\n    pattern_data = []\n\n    for element in elements:\n        pattern = choice(PATTERNS)\n        formatted_element = pattern.format(element)\n        pattern_data.append([element, formatted_element, pattern])\n\n    patterns_df = pd.DataFrame(pattern_data, columns=['Element', 'Formatted Element', 'Pattern'])\n\n    return patterns_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, re, collections, matplotlib\n```python\nimport pandas as pd\nimport re\nimport collections\nimport matplotlib.pyplot as plt\n\n# Constants\nELEMENTS = ['abc', 'def', 'ijk', 'mno']\nPATTERN = re.compile(r'%(.*)%')\n\ndef pattern_analysis(elements):\n    \"\"\"\n    Analyze the list `elements` and return a pandas DataFrame containing each element and \n    the matched pattern. Also return a bar plot showing the distribution of pattern length.\n    \n    Parameters:\n    elements (List[str]): The list of elements.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with elements and their corresponding pattern length.\n    \n    Requirements:\n    - pandas\n    - re\n    - collections\n    - matplotlib.pyplot\n    \n    Example:\n    >>> patterns_df = pattern_analysis(ELEMENTS)\n    >>> print(patterns_df)\n    >>> patterns_df['Pattern Length'].value_counts().plot(kind='bar')\n    \"\"\"\n    pattern_data = []\n\n    for element in elements:\n        match = PATTERN.match(element)\n        if match:\n            matched_pattern = match.group(1)\n            pattern_length = len(matched_pattern)\n            pattern_data.append([element, matched_pattern, pattern_length])\n\n    patterns_df = pd.DataFrame(pattern_data, columns=['Element', 'Matched Pattern', 'Pattern Length'])\n\n    return patterns_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nre, random, string\n```python\nimport re\nimport random\nimport string\n\n# Constants\nELEMENTS = ['abc', 'def', 'ijk', 'mno']\nPATTERN = re.compile(r'%(.*)%')\n\ndef random_pattern(elements):\n    \"\"\"\n    Format each string in the list `elements` into a pattern '%{0}%', where {0} is a \n    randomly generated string of length 5.\n    \n    Parameters:\n    elements (List[str]): The list of elements.\n    \n    Returns:\n    List[str]: The list of elements formatted with random patterns.\n    \n    Requirements:\n    - re\n    - random\n    - string\n    \n    Example:\n    >>> random_patterns = random_pattern(ELEMENTS)\n    >>> print(random_patterns)\n    \"\"\"\n    random_patterns = []\n\n    for element in elements:\n        random_str = ''.join(random.choices(string.ascii_letters + string.digits, k=5))\n        pattern = '%{}%'.format(random_str)\n        random_patterns.append(pattern)\n\n    return random_patterns\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, seaborn, matplotlib\n```python\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nELEMENTS = ['abc', 'def', 'ijk', 'mno']\n\ndef count_character(elements):\n    \"\"\"\n    Count the number of characters in each element in the list `elements` and return a \n    pandas DataFrame showing the count. Also return a bar plot of the count.\n\n    Parameters:\n    elements (List[str]): The list of elements.\n\n    Returns:\n    DataFrame: A pandas DataFrame with elements and their character count.\n\n    Requirements:\n    - pandas\n    - seaborn\n    - matplotlib.pyplot\n\n    Example:\n    >>> count_df = count_character(ELEMENTS)\n    >>> print(count_df)\n    >>> sns.barplot(x='Element', y='Count', data=count_df)\n    \"\"\"\n    count_data = []\n\n    for element in elements:\n        count = len(element)\n        count_data.append([element, count])\n\n    count_df = pd.DataFrame(count_data, columns=['Element', 'Count'])\n\n    return count_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nre, string, random\n```python\nimport re\nimport string\nimport random\n\n# Constants\nELEMENTS = ['abc', 'def', 'ijk', 'mno']\nPATTERN = r'%{}%'\n\ndef replace_character(elements):\n    \"\"\"\n    Replace each character in each element in the list `elements` with a random character \n    and format the element into a pattern '%{0}%'.\n\n    Parameters:\n    elements (List[str]): The list of elements.\n\n    Returns:\n    List[str]: The list of formatted elements with replaced characters.\n\n    Requirements:\n    - re\n    - string\n    - random\n\n    Example:\n    >>> replaced_elements = replace_character(ELEMENTS)\n    >>> print(replaced_elements)\n    \"\"\"\n    replaced_elements = []\n\n    for element in elements:\n        replaced = ''.join(random.choice(string.ascii_letters) for _ in element)\n        formatted = PATTERN.format(replaced)\n        replaced_elements.append(formatted)\n\n    return replaced_elements\n```\n"}
{"seed": "{\"task_id\": 3595685, \"prompt\": \"def f_3595685():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"subprocess.Popen(['background-process', 'arguments'])\", \"test_start\": \"\\nimport subprocess\\nfrom unittest.mock import Mock\\n\\ndef check(candidate):\", \"test\": [\"\\n    subprocess.Popen = Mock(return_value = 0)\\n    assert candidate() == 0\\n\"], \"entry_point\": \"f_3595685\", \"intent\": \"Open a background process 'background-process' with arguments 'arguments'\", \"library\": [\"subprocess\"]}", "generation": "Scenario 1:\nsubprocess, os, shutil, time\n```python\nimport subprocess\nimport os\nimport shutil\nimport time\n\n# Constants\nBACKUP_DIR = '/path/to/backup/dir'\n\ndef backup_and_delete_folder(src_folder):\n    \"\"\"\n    Backup a source folder to a specified location and then delete the source folder. \n    This operation is done as a background process.\n\n    Parameters:\n    src_folder (str): The path of the source folder to be backed up and deleted.\n\n    Returns:\n    int: The return code of the subprocess.\n\n    Requirements:\n    - subprocess\n    - os\n    - shutil\n    - time\n\n    Example:\n    >>> backup_and_delete_folder('/path/to/source/folder')\n    \"\"\"\n    # Check if source folder exists\n    if not os.path.isdir(src_folder):\n        raise ValueError(f\"Source folder '{src_folder}' does not exist.\")\n\n    # Backup folder\n    backup_folder = os.path.join(BACKUP_DIR, os.path.basename(src_folder))\n    shutil.copytree(src_folder, backup_folder)\n\n    # Delete source folder in a background process\n    process = subprocess.Popen(['rm', '-r', src_folder])\n\n    # Wait for the process to complete\n    while process.poll() is None:\n        time.sleep(1)\n\n    return process.returncode\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nsubprocess, os, sys, time\n```python\nimport subprocess\nimport os\nimport sys\nimport time\n\ndef run_python_script(script_path, *args):\n    \"\"\"\n    Run a Python script as a background process with given arguments.\n\n    Parameters:\n    script_path (str): The path of the Python script to be run.\n    *args: The arguments to be passed to the script.\n\n    Returns:\n    int: The return code of the subprocess.\n\n    Requirements:\n    - subprocess\n    - os\n    - sys\n    - time\n\n    Example:\n    >>> run_python_script('/path/to/script.py', 'arg1', 'arg2')\n    \"\"\"\n    # Check if script exists\n    if not os.path.isfile(script_path):\n        raise ValueError(f\"Script '{script_path}' does not exist.\")\n\n    # Run script in a background process\n    process = subprocess.Popen([sys.executable, script_path, *args])\n\n    # Wait for the process to complete\n    while process.poll() is None:\n        time.sleep(1)\n\n    return process.returncode\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nsubprocess, os, signal, time\n```python\nimport subprocess\nimport os\nimport signal\nimport time\n\ndef stop_all_processes(process_name):\n    \"\"\"\n    Stop all running processes with a given name.\n\n    Parameters:\n    process_name (str): The name of the processes to be stopped.\n\n    Returns:\n    int: The number of processes stopped.\n\n    Requirements:\n    - subprocess\n    - os\n    - signal\n    - time\n\n    Example:\n    >>> stop_all_processes('python')\n    \"\"\"\n    # Find all processes with the given name\n    pids = subprocess.check_output(['pgrep', process_name]).split()\n\n    # Send SIGTERM signal to each process\n    for pid in pids:\n        os.kill(int(pid), signal.SIGTERM)\n\n    # Wait for processes to stop\n    time.sleep(1)\n\n    return len(pids)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nsubprocess, platform, os, time\n```python\nimport subprocess\nimport platform\nimport os\nimport time\n\ndef open_webpage(url):\n    \"\"\"\n    Open a webpage in the default web browser.\n\n    Parameters:\n    url (str): The URL of the webpage to be opened.\n\n    Returns:\n    int: The return code of the subprocess.\n\n    Requirements:\n    - subprocess\n    - platform\n    - os\n    - time\n\n    Example:\n    >>> open_webpage('https://www.google.com')\n    \"\"\"\n    # Determine the command to use based on the operating system\n    if platform.system() == 'Darwin':  # macOS\n        cmd = 'open'\n    elif platform.system() == 'Windows':\n        cmd = 'start'\n    else:  # Linux and other UNIX-like systems\n        cmd = 'xdg-open'\n\n    # Open webpage in a background process\n    process = subprocess.Popen([cmd, url])\n\n    # Wait for the process to complete\n    while process.poll() is None:\n        time.sleep(1)\n\n    return process.returncode\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nsubprocess, os, shutil, glob\n```python\nimport subprocess\nimport os\nimport shutil\nfrom glob import glob\n\n# Constants\nCOMPRESSED_DIR = '/path/to/compressed/dir'\n\ndef compress_and_move_files(src_folder, dst_folder):\n    \"\"\"\n    Compress all files in a source folder and move them to a destination folder. \n    This operation is done as a background process.\n\n    Parameters:\n    src_folder (str): The path of the source folder containing the files to be compressed.\n    dst_folder (str): The path of the destination folder where the compressed files will be moved.\n\n    Returns:\n    int: The return code of the subprocess.\n\n    Requirements:\n    - subprocess\n    - os\n    - shutil\n    - glob\n\n    Example:\n    >>> compress_and_move_files('/path/to/source/folder', '/path/to/destination/folder')\n    \"\"\"\n    # Check if source and destination folders exist\n    if not os.path.isdir(src_folder):\n        raise ValueError(f\"Source folder '{src_folder}' does not exist.\")\n    if not os.path.isdir(dst_folder):\n        raise ValueError(f\"Destination folder '{dst_folder}' does not exist.\")\n\n    # Compress files in a background process\n    for file in glob(os.path.join(src_folder, '*')):\n        process = subprocess.Popen(['gzip', file])\n\n    # Wait for the process to complete\n    while process.poll() is None:\n        time.sleep(1)\n\n    # Move compressed files to destination folder\n    for file in glob(os.path.join(src_folder, '*.gz')):\n        shutil.move(file, dst_folder)\n\n    return process.returncode\n```"}
{"seed": "{\"task_id\": 18453566, \"prompt\": \"def f_18453566(mydict, mykeys):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"[mydict[x] for x in mykeys]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    mydict = {'one': 1, 'two': 2, 'three': 3}\\n    mykeys = ['three', 'one']\\n    assert candidate(mydict, mykeys) == [3, 1]\\n\", \"\\n    mydict = {'one': 1.0, 'two': 2.0, 'three': 3.0}\\n    mykeys = ['one']\\n    assert candidate(mydict, mykeys) == [1.0]\\n\"], \"entry_point\": \"f_18453566\", \"intent\": \"get list of values from dictionary 'mydict' w.r.t. list of keys 'mykeys'\", \"library\": []}", "generation": "Scenario 1: \npandas, math, numpy, matplotlib\n```python\nimport pandas as pd\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['Product', 'Quantity', 'Price']\n\ndef calculate_and_plot_profit(product_dict, product_keys):\n    \"\"\"\n    Generate a profit report for a list of products using a given product dictionary that contains\n    quantity and price of each product. Then, plot a bar graph of the profit for each product.\n    \n    Parameters:\n    product_dict (dict): The dictionary containing product details.\n    product_keys (list): The list of product keys.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with profit for each product.\n    \n    Requirements:\n    - pandas\n    - math\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> product_dict = {'Apple': [100, 2.5], 'Orange': [80, 3.5], 'Banana': [120, 1.5]}\n    >>> product_keys = ['Apple', 'Banana']\n    >>> report = calculate_and_plot_profit(product_dict, product_keys)\n    >>> print(report)\n    >>> report['Profit'].plot(kind='bar')\n    \"\"\"\n    data = []\n\n    for key in product_keys:\n        quantity, price = product_dict[key]\n        profit = quantity * price\n        data.append([key, quantity, price, profit])\n\n    df = pd.DataFrame(data, columns=COLUMNS + ['Profit'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncollections, matplotlib, numpy, pandas\n```python\nimport collections\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# Constants\nWORDS = ['the', 'be', 'to', 'of', 'and', 'a', 'in', 'that', 'have', 'I']\n\ndef plot_word_frequencies(text_dict, word_keys):\n    \"\"\"\n    Calculate the frequencies of given words in a text dictionary and plot a bar graph.\n\n    Parameters:\n    text_dict (dict): The dictionary containing word frequencies.\n    word_keys (list): The list of words.\n\n    Returns:\n    None\n\n    Requirements:\n    - collections\n    - matplotlib.pyplot\n    - numpy\n    - pandas\n\n    Example:\n    >>> text_dict = collections.Counter(['the', 'be', 'to', 'the', 'and', 'a', 'in', 'the', 'that', 'have', 'I'])\n    >>> word_keys = ['the', 'and', 'I']\n    >>> plot_word_frequencies(text_dict, word_keys)\n    \"\"\"\n    frequencies = [text_dict[word] for word in word_keys]\n    word_series = pd.Series(frequencies, index=word_keys)\n\n    word_series.plot(kind='bar')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, matplotlib, sklearn\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Constants\nCOLUMNS = ['Feature1', 'Feature2', 'Feature3']\n\ndef normalize_and_plot(data_dict, data_keys):\n    \"\"\"\n    Normalize the given data using MinMax normalization and plot the normalized values.\n\n    Parameters:\n    data_dict (dict): The dictionary containing data.\n    data_keys (list): The list of keys.\n\n    Returns:\n    DataFrame: A pandas DataFrame with normalized data.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.preprocessing\n\n    Example:\n    >>> data_dict = {'A': [1, 2, 3], 'B': [4, 5, 6], 'C': [7, 8, 9]}\n    >>> data_keys = ['A', 'B']\n    >>> normalized_data = normalize_and_plot(data_dict, data_keys)\n    >>> print(normalized_data)\n    \"\"\"\n    data = np.array([data_dict[key] for key in data_keys]).T\n    scaler = MinMaxScaler()\n    normalized_data = scaler.fit_transform(data)\n    df = pd.DataFrame(normalized_data, columns=data_keys)\n    \n    df.plot()\n    plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ncollections, matplotlib, numpy, pandas\n```python\nimport collections\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# Constants\nWORDS = ['the', 'be', 'to', 'of', 'and', 'a', 'in', 'that', 'have', 'I']\n\ndef plot_word_occurrences_in_sentences(sentences_dict, word_keys):\n    \"\"\"\n    Calculate the occurrences of given words in a collection of sentences and plot a bar graph.\n\n    Parameters:\n    sentences_dict (dict): The dictionary containing sentences.\n    word_keys (list): The list of words.\n\n    Returns:\n    None\n\n    Requirements:\n    - collections\n    - matplotlib.pyplot\n    - numpy\n    - pandas\n\n    Example:\n    >>> sentences_dict = {'Sentence1': 'the quick brown fox', 'Sentence2': 'jumps over the lazy dog', 'Sentence3': 'the dog is brown'}\n    >>> word_keys = ['the', 'dog']\n    >>> plot_word_occurrences_in_sentences(sentences_dict, word_keys)\n    \"\"\"\n    word_counts = collections.Counter(' '.join(sentences_dict.values()).split())\n    frequencies = [word_counts[word] for word in word_keys]\n    word_series = pd.Series(frequencies, index=word_keys)\n\n    word_series.plot(kind='bar')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, scipy, matplotlib, pandas\n```python\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants\nCOLUMNS = ['X', 'Y']\n\ndef calculate_and_plot_correlation(data_dict, data_keys):\n    \"\"\"\n    Calculate the correlation of two data series and plot a scatter graph.\n\n    Parameters:\n    data_dict (dict): The dictionary containing data.\n    data_keys (list): The list of keys.\n\n    Returns:\n    float: The correlation coefficient.\n\n    Requirements:\n    - numpy\n    - scipy\n    - matplotlib.pyplot\n    - pandas\n\n    Example:\n    >>> data_dict = {'X': [1, 2, 3, 4, 5], 'Y': [2, 3, 5, 7, 8]}\n    >>> data_keys = ['X', 'Y']\n    >>> correlation = calculate_and_plot_correlation(data_dict, data_keys)\n    >>> print(correlation)\n    \"\"\"\n    x = data_dict[data_keys[0]]\n    y = data_dict[data_keys[1]]\n    correlation, _ = stats.pearsonr(x, y)\n    \n    plt.scatter(x, y)\n    plt.show()\n\n    return correlation\n```"}
{"seed": "{\"task_id\": 12692135, \"prompt\": \"def f_12692135():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"dict([('Name', 'Joe'), ('Age', 22)])\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate() == {'Name': 'Joe', 'Age': 22}\\n\"], \"entry_point\": \"f_12692135\", \"intent\": \"convert list `[('Name', 'Joe'), ('Age', 22)]` into a dictionary\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, random, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import choice, randint\nimport matplotlib.pyplot as plt\n\n# Constants\nSTUDENTS = ['Joe', 'Amy', 'Mark', 'Sara', 'John', 'Emily', 'Zoe', 'Matt']\nCOURSES = ['Math', 'Physics', 'Chemistry', 'Biology', 'English', 'History', 'Geography', 'Computer Science']\n\ndef generate_student_grades():\n    \"\"\"\n    Generate a DataFrame of students' grades in various courses.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with students' grades in various courses.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> grades = generate_student_grades()\n    >>> print(grades)\n    >>> grades['Average Grade'].plot(kind='bar')\n    \"\"\"\n    students_data = []\n\n    for student in STUDENTS:\n        grades = [randint(0, 100) for _ in COURSES]\n        average_grade = np.mean(grades)\n        students_data.append([student] + grades + [average_grade])\n\n    columns = ['Name'] + COURSES + ['Average Grade']\n    grades_df = pd.DataFrame(students_data, columns=columns)\n\n    return grades_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \ncollections, json, random\n```python\nfrom collections import Counter\nimport json\nimport random\n\n# Constants\nLETTERS = 'abcdefghijklmnopqrstuvwxyz'\nWORDS = ['apple', 'banana', 'cherry', 'date', 'elderberry', 'fig', 'grape', 'honeydew']\n\ndef generate_word_counts(n):\n    \"\"\"\n    Generate a json file with counts of n randomly selected words from a list.\n    \n    Parameters:\n    n (int): The number of words to select from the list.\n    \n    Returns:\n    str: The name of the json file generated.\n    \n    Requirements:\n    - collections\n    - json\n    - random\n\n    Example:\n    >>> generate_word_counts(5)\n    \"\"\"\n    selected_words = [random.choice(WORDS) for _ in range(n)]\n    counts = Counter(selected_words)\n    file_name = 'word_counts.json'\n\n    with open(file_name, 'w') as f:\n        json.dump(dict(counts), f)\n\n    return file_name\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \ndatetime, collections, matplotlib.pyplot\n```python\nfrom datetime import datetime, timedelta\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\n\n# Constants\nDAYS = 7\n\ndef generate_weekly_activity():\n    \"\"\"\n    Generate a bar plot of the number of activities done each day of the week.\n\n    Returns:\n    None\n    \n    Requirements:\n    - datetime\n    - collections\n    - matplotlib.pyplot\n\n    Example:\n    >>> generate_weekly_activity()\n    \"\"\"\n    now = datetime.now()\n    activity_counts = defaultdict(int)\n\n    for i in range(DAYS):\n        day = (now - timedelta(days=i)).strftime('%A')\n        activity_counts[day] += 1\n\n    days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n    counts = [activity_counts[day] for day in days]\n\n    plt.bar(days, counts)\n    plt.xlabel('Day of the Week')\n    plt.ylabel('Number of Activities')\n    plt.title('Weekly Activity')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \npandas, numpy, sklearn.preprocessing\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\n\n# Constants\nCATEGORIES = ['A', 'B', 'C', 'D', 'E']\n\ndef encode_categories(data):\n    \"\"\"\n    Encode categorical variable into numerical values using sklearn's LabelEncoder.\n    \n    Parameters:\n    data (list): The categorical data to encode.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the original and encoded values.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.preprocessing\n\n    Example:\n    >>> encode_categories(['A', 'B', 'C', 'A', 'D', 'E', 'B', 'C'])\n    \"\"\"\n    le = LabelEncoder()\n    encoded = le.fit_transform(data)\n    df = pd.DataFrame({'Category': data, 'Encoded': encoded})\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nos, shutil, random\n```python\nimport os\nimport shutil\nfrom random import choice\n\n# Constants\nFILES = ['file1.txt', 'file2.txt', 'file3.txt', 'file4.txt', 'file5.txt']\nDEST_DIR = 'dest_dir'\n\ndef move_random_file(src_dir):\n    \"\"\"\n    Move a random file from the source directory to the destination directory.\n    \n    Parameters:\n    src_dir (str): The source directory.\n    \n    Returns:\n    str: The name of the file moved.\n    \n    Requirements:\n    - os\n    - shutil\n    - random\n\n    Example:\n    >>> move_random_file('src_dir')\n    \"\"\"\n    file_name = choice(FILES)\n    src_file = os.path.join(src_dir, file_name)\n    dest_file = os.path.join(DEST_DIR, file_name)\n\n    shutil.move(src_file, dest_file)\n\n    return file_name\n```\n"}
{"seed": "{\"task_id\": 14401047, \"prompt\": \"def f_14401047(data):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"data.mean(axis=1).reshape(data.shape[0], -1)\", \"test_start\": \"\\nimport numpy as np\\n\\ndef check(candidate):\", \"test\": [\"\\n    data = np.array([[1, 2, 3, 4, 4, 3, 7, 1], [6, 2, 3, 4, 3, 4, 4, 1]])\\n    expected_res = np.array([[3.125], [3.375]])\\n    assert np.array_equal(candidate(data), expected_res)\\n\"], \"entry_point\": \"f_14401047\", \"intent\": \"average each two columns of array `data`\", \"library\": [\"numpy\"]}", "generation": "Scenario 1:\npandas, numpy, statistics, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport statistics\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMN_NAMES = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']\n\ndef plot_column_averages(data):\n    \"\"\"\n    Calculate the average of each two columns in the data, plot the averages and\n    return the DataFrame with added 'Average' column.\n\n    Parameters:\n    data (numpy.array): 2D array with eight columns.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the added 'Average' column.\n\n    Requirements:\n    - pandas\n    - numpy\n    - statistics\n    - matplotlib.pyplot\n\n    Example:\n    >>> data = np.array([[1, 2, 3, 4, 4, 3, 7, 1], [6, 2, 3, 4, 3, 4, 4, 1]])\n    >>> df = plot_column_averages(data)\n    >>> print(df)\n    \"\"\"\n    df = pd.DataFrame(data, columns=COLUMN_NAMES)\n    df['Average'] = df.mean(axis=1)\n\n    plt.plot(df['Average'])\n    plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, sklearn.preprocessing, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMN_NAMES = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']\n\ndef normalize_and_plot(data):\n    \"\"\"\n    Normalize the data using MinMaxScaler, calculate the average of each two \n    columns, plot the averages and return the DataFrame with added 'Average' column.\n\n    Parameters:\n    data (numpy.array): 2D array with eight columns.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the added 'Average' column.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.preprocessing\n    - matplotlib.pyplot\n\n    Example:\n    >>> data = np.array([[1, 2, 3, 4, 4, 3, 7, 1], [6, 2, 3, 4, 3, 4, 4, 1]])\n    >>> df = normalize_and_plot(data)\n    >>> print(df)\n    \"\"\"\n    scaler = MinMaxScaler()\n    normalized_data = scaler.fit_transform(data)\n\n    df = pd.DataFrame(normalized_data, columns=COLUMN_NAMES)\n    df['Average'] = df.mean(axis=1)\n\n    plt.plot(df['Average'])\n    plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, seaborn, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMN_NAMES = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']\n\ndef plot_heatmap(data):\n    \"\"\"\n    Calculate the average of each two columns in the data, plot a heatmap of the \n    correlations between columns and return the DataFrame with added 'Average' column.\n\n    Parameters:\n    data (numpy.array): 2D array with eight columns.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the added 'Average' column.\n\n    Requirements:\n    - pandas\n    - numpy\n    - seaborn\n    - matplotlib.pyplot\n\n    Example:\n    >>> data = np.array([[1, 2, 3, 4, 4, 3, 7, 1], [6, 2, 3, 4, 3, 4, 4, 1]])\n    >>> df = plot_heatmap(data)\n    >>> print(df)\n    \"\"\"\n    df = pd.DataFrame(data, columns=COLUMN_NAMES)\n    df['Average'] = df.mean(axis=1)\n\n    correlation = df.corr()\n    sns.heatmap(correlation, annot=True, cmap='coolwarm')\n    plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, sklearn.decomposition, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMN_NAMES = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']\n\ndef pca_decomposition(data):\n    \"\"\"\n    Perform PCA decomposition on the data, calculate the average of each two \n    columns, plot the results and return the DataFrame with added 'Average' column.\n\n    Parameters:\n    data (numpy.array): 2D array with eight columns.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the added 'Average' column.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.decomposition\n    - matplotlib.pyplot\n\n    Example:\n    >>> data = np.array([[1, 2, 3, 4, 4, 3, 7, 1], [6, 2, 3, 4, 3, 4, 4, 1]])\n    >>> df = pca_decomposition(data)\n    >>> print(df)\n    \"\"\"\n    pca = PCA(n_components=2)\n    pca_data = pca.fit_transform(data)\n\n    df = pd.DataFrame(pca_data, columns=['PC1', 'PC2'])\n    df['Average'] = df.mean(axis=1)\n\n    plt.scatter(df['PC1'], df['PC2'])\n    plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, seaborn, scipy.stats\n```python\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom scipy import stats\n\n# Constants\nCOLUMN_NAMES = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']\n\ndef plot_distribution_and_test_normality(data):\n    \"\"\"\n    Calculate the average of each two columns in the data, plot the distribution \n    of averages, perform a normality test and return the DataFrame with added 'Average' column.\n\n    Parameters:\n    data (numpy.array): 2D array with eight columns.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the added 'Average' column.\n\n    Requirements:\n    - pandas\n    - numpy\n    - seaborn\n    - scipy.stats\n\n    Example:\n    >>> data = np.array([[1, 2, 3, 4, 4, 3, 7, 1], [6, 2, 3, 4, 3, 4, 4, 1]])\n    >>> df = plot_distribution_and_test_normality(data)\n    >>> print(df)\n    \"\"\"\n    df = pd.DataFrame(data, columns=COLUMN_NAMES)\n    df['Average'] = df.mean(axis=1)\n\n    sns.distplot(df['Average'], hist = False, kde = True, kde_kws = {'linewidth': 3})\n    plt.show()\n\n    k2, p = stats.normaltest(df['Average'])\n    print(\"p = {:g}\".format(p))\n\n    return df\n```"}
{"seed": "{\"task_id\": 18886596, \"prompt\": \"def f_18886596(s):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"s.replace('\\\"', '\\\\\\\"')\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    s = 'This sentence has some \\\"quotes\\\" in it'\\n    assert candidate(s) == 'This sentence has some \\\\\\\"quotes\\\\\\\" in it'\\n\"], \"entry_point\": \"f_18886596\", \"intent\": \"double backslash escape all double quotes in string `s`\", \"library\": []}", "generation": "Scenario 1: \nre, json, os, glob\n```python\nimport re\nimport json\nimport os\nimport glob\n\n# Constants\nJSON_FILES_PATH = './json_files/'\n\ndef escape_quotes_in_json_files():\n    \"\"\"\n    Double backslash escape all double quotes in all JSON files in a specific directory.\n\n    Requirements:\n    - re\n    - json\n    - os\n    - glob\n\n    Example:\n    >>> escape_quotes_in_json_files()\n    \"\"\"\n    json_files = glob.glob(JSON_FILES_PATH + '*.json')\n\n    for json_file in json_files:\n        with open(json_file, 'r') as file:\n            data = json.load(file)\n\n        escaped_data = json.dumps(data, ensure_ascii=False)\n        escaped_data = re.sub(r'(?<!\\\\)\"', r'\\\\\"', escaped_data)\n\n        with open(json_file, 'w') as file:\n            file.write(escaped_data)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nre, pandas, glob, csv\n```python\nimport re\nimport pandas as pd\nimport glob\nimport csv\n\n# Constants\nCSV_FILES_PATH = './csv_files/'\n\ndef escape_quotes_in_csv_files():\n    \"\"\"\n    Double backslash escape all double quotes in all CSV files in a specific directory.\n\n    Requirements:\n    - re\n    - pandas\n    - glob\n    - csv\n\n    Example:\n    >>> escape_quotes_in_csv_files()\n    \"\"\"\n    csv_files = glob.glob(CSV_FILES_PATH + '*.csv')\n\n    for csv_file in csv_files:\n        data = pd.read_csv(csv_file, quoting=csv.QUOTE_NONE, escapechar='\\\\')\n\n        for column in data.columns:\n            data[column] = data[column].apply(lambda x: re.sub(r'(?<!\\\\)\"', r'\\\\\"', str(x)))\n\n        data.to_csv(csv_file, index=False, quoting=csv.QUOTE_NONE, escapechar='\\\\')\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nre, docx, glob\n```python\nimport re\nfrom docx import Document\nimport glob\n\n# Constants\nDOCX_FILES_PATH = './docx_files/'\n\ndef escape_quotes_in_docx_files():\n    \"\"\"\n    Double backslash escape all double quotes in all Word (.docx) files in a specific directory.\n\n    Requirements:\n    - re\n    - docx\n    - glob\n\n    Example:\n    >>> escape_quotes_in_docx_files()\n    \"\"\"\n    docx_files = glob.glob(DOCX_FILES_PATH + '*.docx')\n\n    for docx_file in docx_files:\n        document = Document(docx_file)\n\n        for paragraph in document.paragraphs:\n            paragraph.text = re.sub(r'(?<!\\\\)\"', r'\\\\\"', paragraph.text)\n\n        document.save(docx_file)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nre, openpyxl, glob\n```python\nimport re\nfrom openpyxl import load_workbook\nimport glob\n\n# Constants\nXLSX_FILES_PATH = './xlsx_files/'\n\ndef escape_quotes_in_xlsx_files():\n    \"\"\"\n    Double backslash escape all double quotes in all Excel (.xlsx) files in a specific directory.\n\n    Requirements:\n    - re\n    - openpyxl\n    - glob\n\n    Example:\n    >>> escape_quotes_in_xlsx_files()\n    \"\"\"\n    xlsx_files = glob.glob(XLSX_FILES_PATH + '*.xlsx')\n\n    for xlsx_file in xlsx_files:\n        workbook = load_workbook(filename=xlsx_file)\n\n        for sheet in workbook.sheetnames:\n            for row in workbook[sheet].iter_rows(values_only=True):\n                for cell in row:\n                    if isinstance(cell, str):\n                        cell = re.sub(r'(?<!\\\\)\"', r'\\\\\"', cell)\n\n        workbook.save(xlsx_file)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nre, BeautifulSoup, glob, requests\n```python\nimport re\nfrom bs4 import BeautifulSoup\nimport glob\nimport requests\n\n# Constants\nHTML_FILES_PATH = './html_files/'\n\ndef escape_quotes_in_html_files():\n    \"\"\"\n    Double backslash escape all double quotes in all HTML files in a specific directory.\n\n    Requirements:\n    - re\n    - BeautifulSoup\n    - glob\n    - requests\n\n    Example:\n    >>> escape_quotes_in_html_files()\n    \"\"\"\n    html_files = glob.glob(HTML_FILES_PATH + '*.html')\n\n    for html_file in html_files:\n        with open(html_file, 'r', encoding='utf-8') as file:\n            content = file.read()\n\n        soup = BeautifulSoup(content, 'html.parser')\n\n        for tag in soup():\n            tag.string = re.sub(r'(?<!\\\\)\"', r'\\\\\"', tag.string)\n\n        with open(html_file, 'w', encoding='utf-8') as file:\n            file.write(str(soup))\n```"}
{"seed": "{\"task_id\": 5932059, \"prompt\": \"def f_5932059(s):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"re.split('(\\\\\\\\W+)', s)\", \"test_start\": \"\\nimport re \\n\\ndef check(candidate):\", \"test\": [\"\\n    s = \\\"this is  a\\\\nsentence\\\"\\n    assert candidate(s) == ['this', ' ', 'is', '  ', 'a', '\\\\n', 'sentence']\\n\"], \"entry_point\": \"f_5932059\", \"intent\": \"split a string `s` into a list of words and whitespace\", \"library\": [\"re\"]}", "generation": "Scenario 1: \nnltk, re, string, collections\n```python\nimport nltk\nimport re\nimport string\nfrom collections import Counter\n\n# Constants\nSTOPWORDS = nltk.corpus.stopwords.words('english')\n\ndef word_frequency_in_text(text):\n    \"\"\"\n    Calculate the frequency of non-stop words in a given text string after splitting \n    it into words and removing punctuation marks.\n\n    Parameters:\n    text (str): The text string.\n\n    Returns:\n    dict: A dictionary with words as keys and their frequencies as values.\n\n    Requirements:\n    - nltk\n    - re\n    - string\n    - collections.Counter\n\n    Example:\n    >>> word_frequency_in_text('This is a sample text. This text is for testing.')\n    \"\"\"\n    words = re.split(r'\\W+', text.lower())\n    words = [word for word in words if word not in STOPWORDS and word != '']\n    word_freq = dict(Counter(words))\n\n    return word_freq\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nre, pandas, csv\n```python\nimport re\nimport pandas as pd\nimport csv\n\ndef extract_data_from_logs(log_file):\n    \"\"\"\n    Extract certain data from a log file and save it to a CSV file. Each line in the \n    log file follows this format: \"ERROR/INFO: [yyyy-mm-dd hh:mm:ss] - Message\"\n\n    Parameters:\n    log_file (str): The path to the log file.\n\n    Returns:\n    None\n\n    Requirements:\n    - re\n    - pandas\n    - csv\n\n    Example:\n    >>> extract_data_from_logs('server.log')\n    \"\"\"\n    log_data = []\n    with open(log_file, 'r') as f:\n        for line in f:\n            match = re.search(r'(ERROR|INFO): \\[(.*?)\\] - (.*)', line)\n            if match:\n                log_data.append(match.groups())\n\n    df = pd.DataFrame(log_data, columns=['Type', 'Timestamp', 'Message'])\n    df.to_csv('log_data.csv', index=False, quoting=csv.QUOTE_NONNUMERIC)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nre, os, shutil\n```python\nimport re\nimport os\nimport shutil\n\ndef organize_files_by_extension(directory):\n    \"\"\"\n    Organize files in a directory by their extensions. Create a new directory for each \n    extension and move the files to their corresponding directories.\n\n    Parameters:\n    directory (str): The path to the directory.\n\n    Returns:\n    None\n\n    Requirements:\n    - re\n    - os\n    - shutil\n\n    Example:\n    >>> organize_files_by_extension('/home/user/Documents')\n    \"\"\"\n    for filename in os.listdir(directory):\n        match = re.search(r'\\.(.*?)$', filename)\n        if match:\n            ext_dir = os.path.join(directory, match.group(1))\n            if not os.path.exists(ext_dir):\n                os.mkdir(ext_dir)\n            shutil.move(os.path.join(directory, filename), ext_dir)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nre, pandas, numpy, matplotlib\n```python\nimport re\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_word_length_distribution(text):\n    \"\"\"\n    Plot the distribution of word lengths in a given text string.\n\n    Parameters:\n    text (str): The text string.\n\n    Returns:\n    None\n\n    Requirements:\n    - re\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_word_length_distribution('This is a sample text for testing.')\n    \"\"\"\n    words = re.split(r'\\W+', text)\n    word_lengths = [len(word) for word in words if word != '']\n\n    df = pd.DataFrame(word_lengths, columns=['Word Length'])\n    df['Word Length'].plot(kind='hist', rwidth=0.8)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nre, pandas, seaborn, sklearn, matplotlib\n```python\nimport re\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\n\ndef visualize_top_n_grams(text, n, top_k):\n    \"\"\"\n    Visualize the top K n-grams in a given text string.\n\n    Parameters:\n    text (str): The text string.\n    n (int): The value of n for the n-grams.\n    top_k (int): The number of top n-grams to visualize.\n\n    Returns:\n    None\n\n    Requirements:\n    - re\n    - pandas\n    - seaborn\n    - sklearn.feature_extraction.text.CountVectorizer\n    - matplotlib.pyplot\n\n    Example:\n    >>> visualize_top_n_grams('This is a sample text for testing.', 2, 5)\n    \"\"\"\n    words = re.split(r'\\W+', text.lower())\n    vec = CountVectorizer(ngram_range=(n, n)).fit(words)\n    bag_of_words = vec.transform(words)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n    top_df = pd.DataFrame(words_freq[:top_k], columns=['n-gram', 'Frequency'])\n\n    sns.barplot(x='n-gram', y='Frequency', data=top_df)\n    plt.show()\n```"}
{"seed": "{\"task_id\": 9938130, \"prompt\": \"def f_9938130(df):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"df.plot(kind='barh', stacked=True)\", \"test_start\": \"\\nimport pandas as pd\\n\\ndef check(candidate):\", \"test\": [\"\\n    data = [[1, 3], [2, 5], [4, 8]]\\n    df = pd.DataFrame(data, columns = ['a', 'b'])\\n    assert str(candidate(df)).split('(')[0] == 'AxesSubplot'\\n\"], \"entry_point\": \"f_9938130\", \"intent\": \"plotting stacked barplots on a panda data frame `df`\", \"library\": [\"pandas\"]}", "generation": "Scenario 1: \nNumpy, Matplotlib, Seaborn, Pandas\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n# Constants\nCATEGORIES = ['A', 'B', 'C', 'D', 'E']\n\ndef generate_and_plot_data():\n    \"\"\"\n    Generate a pandas DataFrame with random numeric data and plot a stacked \n    bar plot using seaborn for categories 'A', 'B', 'C', 'D', 'E'.\n\n    Returns:\n    seaborn.axisgrid.FacetGrid: The FacetGrid object with the plot.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> plot = generate_and_plot_data()\n    >>> plt.show()\n    \"\"\"\n    data = pd.DataFrame(np.random.rand(5, 5)*100, columns=CATEGORIES)\n\n    plot = sns.catplot(data=data, kind='bar', stacked=True)\n\n    return plot\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, matplotlib, numpy, seaborn\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\n# Constants\nLABELS = ['Label1', 'Label2', 'Label3', 'Label4', 'Label5']\n\ndef plot_stacked_bargraph_with_data():\n    \"\"\"\n    Generate a pandas DataFrame with random numeric data for 5 labels and plot a stacked \n    bar graph using matplotlib.\n\n    Returns:\n    matplotlib.figure.Figure: The Figure object with the plot.\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - numpy\n    - seaborn\n\n    Example:\n    >>> fig = plot_stacked_bargraph_with_data()\n    >>> plt.show()\n    \"\"\"\n    data = pd.DataFrame(np.random.rand(5, 5), columns=LABELS)\n\n    fig, ax = plt.subplots()\n\n    data.plot(kind='bar', stacked=True, ax=ax)\n\n    return fig\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, matplotlib, random\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nLABELS = ['Category1', 'Category2', 'Category3', 'Category4', 'Category5']\n\ndef generate_random_dataframe_and_plot():\n    \"\"\"\n    Generate a pandas DataFrame with random integer data for 5 categories and plot a stacked \n    bar graph using matplotlib.\n\n    Returns:\n    matplotlib.figure.Figure: The Figure object with the plot.\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> fig = generate_random_dataframe_and_plot()\n    >>> plt.show()\n    \"\"\"\n    data = pd.DataFrame({label: [randint(0, 100) for _ in range(5)] for label in LABELS})\n\n    fig, ax = plt.subplots()\n\n    data.plot(kind='bar', stacked=True, ax=ax)\n\n    return fig\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, matplotlib, random\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nLABELS = ['Type1', 'Type2', 'Type3', 'Type4', 'Type5']\n\ndef generate_dataframe_and_plot_horizontal():\n    \"\"\"\n    Generate a pandas DataFrame with random integer data for 5 types and plot a stacked \n    bar graph horizontally using matplotlib.\n\n    Returns:\n    matplotlib.figure.Figure: The Figure object with the plot.\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> fig = generate_dataframe_and_plot_horizontal()\n    >>> plt.show()\n    \"\"\"\n    data = pd.DataFrame({label: [randint(0, 100) for _ in range(5)] for label in LABELS})\n\n    fig, ax = plt.subplots()\n\n    data.plot(kind='barh', stacked=True, ax=ax)\n\n    return fig\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, matplotlib, numpy\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nLABELS = ['Group1', 'Group2', 'Group3', 'Group4', 'Group5']\n\ndef generate_and_plot_dataframe():\n    \"\"\"\n    Generate a pandas DataFrame with random float data for 5 groups and plot a stacked \n    bar graph using matplotlib.\n\n    Returns:\n    matplotlib.figure.Figure: The Figure object with the plot.\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - numpy\n\n    Example:\n    >>> fig = generate_and_plot_dataframe()\n    >>> plt.show()\n    \"\"\"\n    data = pd.DataFrame(np.random.rand(5, 5), columns=LABELS)\n\n    fig, ax = plt.subplots()\n\n    data.plot(kind='bar', stacked=True, ax=ax)\n\n    return fig\n```"}
{"seed": "{\"task_id\": 35945473, \"prompt\": \"def f_35945473(myDictionary):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"{i[1]: i[0] for i in list(myDictionary.items())}\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate({'a' : 'b', 'c' : 'd'}) == {'b': 'a', 'd': 'c'}\\n\"], \"entry_point\": \"f_35945473\", \"intent\": \"reverse the keys and values in a dictionary `myDictionary`\", \"library\": []}", "generation": "Scenario 1: \ncollections, random, itertools\n```python\nimport collections\nimport random\nimport itertools\n\n# Constants\nANIMALS = ['Cat', 'Dog', 'Elephant', 'Lion', 'Tiger', 'Bear', 'Giraffe']\n\ndef reverse_and_count_animal_names(animal_dict):\n    \"\"\"\n    Reverse the keys and values in a dictionary `animal_dict` and count the occurrences of each animal name.\n\n    Parameters:\n    animal_dict (dict): The dictionary with keys as names and values as animal names.\n\n    Returns:\n    tuple: A tuple where the first element is a reversed dictionary and the second element is a counter with animal \n    name occurrences.\n\n    Requirements:\n    - collections\n    - random\n    - itertools\n\n    Example:\n    >>> animal_dict = {'John': 'Cat', 'Alice': 'Dog', 'Bob': 'Elephant', 'Charlie': 'Lion', 'David': 'Tiger'}\n    >>> reverse_and_count_animal_names(animal_dict)\n    \"\"\"\n    reversed_dict = {v: k for k, v in animal_dict.items()}\n    animal_counter = collections.Counter(itertools.chain.from_iterable([[v] * random.randint(1, 10) for v in ANIMALS]))\n    \n    return reversed_dict, animal_counter\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \ncollections, random, matplotlib\n```python\nimport collections\nimport random\nimport matplotlib.pyplot as plt\n\n# Constants\nFRUITS = ['Apple', 'Banana', 'Cherry', 'Date', 'Elderberry', 'Fig', 'Grape']\n\ndef reverse_and_plot_fruit_counts(fruit_dict):\n    \"\"\"\n    Reverse the keys and values in a dictionary `fruit_dict` and plot the counts of each fruit.\n\n    Parameters:\n    fruit_dict (dict): The dictionary with keys as people's names and values as fruit names.\n\n    Returns:\n    dict: A dictionary with fruit names as keys and their counts as values.\n\n    Requirements:\n    - collections\n    - random\n    - matplotlib\n\n    Example:\n    >>> fruit_dict = {'John': 'Apple', 'Alice': 'Banana', 'Bob': 'Cherry', 'Charlie': 'Date', 'David': 'Elderberry'}\n    >>> reverse_and_plot_fruit_counts(fruit_dict)\n    \"\"\"\n    reversed_dict = {v: k for k, v in fruit_dict.items()}\n    fruit_counter = collections.Counter(itertools.chain.from_iterable([[v] * random.randint(1, 10) for v in FRUITS]))\n    \n    plt.bar(fruit_counter.keys(), fruit_counter.values())\n    plt.show()\n\n    return fruit_counter\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nrandom, numpy, matplotlib\n```python\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCITIES = ['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']\n\ndef reverse_and_plot_city_population(city_dict):\n    \"\"\"\n    Reverse the keys and values in a dictionary `city_dict` and plot the population of each city.\n\n    Parameters:\n    city_dict (dict): The dictionary with keys as people's names and values as city names.\n\n    Returns:\n    dict: A dictionary with city names as keys and their population as values.\n\n    Requirements:\n    - random\n    - numpy\n    - matplotlib\n\n    Example:\n    >>> city_dict = {'John': 'New York', 'Alice': 'London', 'Bob': 'Beijing', 'Charlie': 'Tokyo', 'David': 'Sydney'}\n    >>> reverse_and_plot_city_population(city_dict)\n    \"\"\"\n    reversed_dict = {v: k for k, v in city_dict.items()}\n    city_population = {city: np.random.randint(1000000, 10000000) for city in CITIES}\n    \n    plt.bar(city_population.keys(), city_population.values())\n    plt.show()\n\n    return city_population\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \ncollections, random, pandas\n```python\nimport collections\nimport random\nimport pandas as pd\n\n# Constants\nVEGETABLES = ['Carrot', 'Potato', 'Tomato', 'Cabbage', 'Spinach']\n\ndef reverse_and_calculate_vegetable_statistics(vegetable_dict):\n    \"\"\"\n    Reverse the keys and values in a dictionary `vegetable_dict` and calculate the statistics of each \n    vegetable's occurrences.\n\n    Parameters:\n    vegetable_dict (dict): The dictionary with keys as people's names and values as vegetable names.\n\n    Returns:\n    DataFrame: A pandas DataFrame with vegetable names and their statistics.\n\n    Requirements:\n    - collections\n    - random\n    - pandas\n\n    Example:\n    >>> vegetable_dict = {'John': 'Carrot', 'Alice': 'Potato', 'Bob': 'Tomato', 'Charlie': 'Cabbage', 'David': 'Spinach'}\n    >>> reverse_and_calculate_vegetable_statistics(vegetable_dict)\n    \"\"\"\n    reversed_dict = {v: k for k, v in vegetable_dict.items()}\n    vegetable_counter = collections.Counter(itertools.chain.from_iterable([[v] * random.randint(1, 10) for v in VEGETABLES]))\n    \n    statistics_df = pd.DataFrame.from_dict(vegetable_counter, orient='index', columns=['Count'])\n    statistics_df['Percentage'] = statistics_df['Count'] / statistics_df['Count'].sum() * 100\n\n    return statistics_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nrandom, numpy, pandas\n```python\nimport random\nimport numpy as np\nimport pandas as pd\n\n# Constants\nCOUNTRIES = ['USA', 'UK', 'China', 'Japan', 'Australia']\n\ndef reverse_and_calculate_country_gdp(country_dict):\n    \"\"\"\n    Reverse the keys and values in a dictionary `country_dict` and calculate the GDP of each country.\n\n    Parameters:\n    country_dict (dict): The dictionary with keys as people's names and values as country names.\n\n    Returns:\n    DataFrame: A pandas DataFrame with country names and their GDP.\n\n    Requirements:\n    - random\n    - numpy\n    - pandas\n\n    Example:\n    >>> country_dict = {'John': 'USA', 'Alice': 'UK', 'Bob': 'China', 'Charlie': 'Japan', 'David': 'Australia'}\n    >>> reverse_and_calculate_country_gdp(country_dict)\n    \"\"\"\n    reversed_dict = {v: k for k, v in country_dict.items()}\n    country_gdp = {country: np.random.randint(1000000000, 100000000000) for country in COUNTRIES}\n    \n    gdp_df = pd.DataFrame.from_dict(country_gdp, orient='index', columns=['GDP'])\n\n    return gdp_df\n```\n"}
{"seed": "{\"task_id\": 30729735, \"prompt\": \"def f_30729735(myList):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"[i for i, j in enumerate(myList) if 'how' in j.lower() or 'what' in j.lower()]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(['abc', 'how', 'what', 'def']) == [1, 2]\\n\"], \"entry_point\": \"f_30729735\", \"intent\": \"finding the index of elements containing substring 'how' and 'what' in a list of strings 'myList'.\", \"library\": []}", "generation": "Scenario 1: \npandas, re, matplotlib, numpy\n```python\nimport pandas as pd\nimport re\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Sample data\nDATA = {\n    'Title': ['How to code?', 'What is Python?', 'The art of programming', 'How to cook?', 'What is life?'],\n    'Views': [1000, 500, 200, 300, 800],\n    'Likes': [500, 250, 100, 150, 600]\n}\n\ndf = pd.DataFrame(DATA)\n\ndef plot_interesting_videos(df):\n    \"\"\"\n    Find the videos with titles containing 'how' or 'what', calculate and plot \n    the like ratio (Likes / Views) for these videos.\n\n    Parameters:\n    df (DataFrame): A pandas DataFrame with video data (Title, Views, Likes).\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - re\n    - matplotlib.pyplot\n    - numpy\n\n    Example:\n    >>> plot_interesting_videos(df)\n    \"\"\"\n    pattern = re.compile(r'(how|what)', re.IGNORECASE)\n    interesting_videos = df[df['Title'].apply(lambda x: bool(pattern.search(x)))]\n    interesting_videos['Like Ratio'] = interesting_videos['Likes'] / interesting_videos['Views']\n\n    plt.bar(interesting_videos['Title'], interesting_videos['Like Ratio'])\n    plt.ylabel('Like Ratio')\n    plt.xticks(rotation='vertical')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \npandas, re, nltk, string\n```python\nimport pandas as pd\nimport re\nimport nltk\nfrom string import punctuation\n\n# Sample data\nDATA = {\n    'Title': ['How to code?', 'What is Python?', 'The art of programming', 'How to cook?', 'What is life?'],\n    'Content': ['This is a tutorial about coding...', 'Python is a programming language...', 'Programming is an art...', 'This is a cooking tutorial...', 'Life is complicated...']\n}\n\ndf = pd.DataFrame(DATA)\n\ndef count_interesting_words(df):\n    \"\"\"\n    Find the articles with titles containing 'how' or 'what', tokenize the content \n    of these articles and count the frequency of each word (excluding punctuation).\n\n    Parameters:\n    df (DataFrame): A pandas DataFrame with article data (Title, Content).\n\n    Returns:\n    dict: A dictionary with word frequencies.\n\n    Requirements:\n    - pandas\n    - re\n    - nltk\n    - string\n\n    Example:\n    >>> count_interesting_words(df)\n    \"\"\"\n    pattern = re.compile(r'(how|what)', re.IGNORECASE)\n    interesting_articles = df[df['Title'].apply(lambda x: bool(pattern.search(x)))]\n\n    word_freq = {}\n    for content in interesting_articles['Content']:\n        tokens = nltk.word_tokenize(content)\n        for token in tokens:\n            if token not in punctuation:\n                if token not in word_freq:\n                    word_freq[token] = 1\n                else:\n                    word_freq[token] += 1\n\n    return word_freq\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nre, os, shutil\n```python\nimport re\nimport os\nimport shutil\n\n# Directory\nDIRECTORY = './'\n\ndef move_interesting_files(directory):\n    \"\"\"\n    Find the files with file names containing 'how' or 'what' in a directory, \n    create a new directory named 'Interesting Files' and move these files into the new directory.\n\n    Parameters:\n    directory (str): The directory path.\n\n    Returns:\n    None\n\n    Requirements:\n    - re\n    - os\n    - shutil\n\n    Example:\n    >>> move_interesting_files(DIRECTORY)\n    \"\"\"\n    pattern = re.compile(r'(how|what)', re.IGNORECASE)\n    interesting_files = [file for file in os.listdir(directory) if pattern.search(file)]\n\n    if not os.path.exists('Interesting Files'):\n        os.mkdir('Interesting Files')\n\n    for file in interesting_files:\n        shutil.move(os.path.join(directory, file), 'Interesting Files')\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \npandas, re, matplotlib.pyplot, sklearn.feature_extraction.text\n```python\nimport pandas as pd\nimport re\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Sample data\nDATA = {\n    'Title': ['How to code?', 'What is Python?', 'The art of programming', 'How to cook?', 'What is life?'],\n    'Content': ['This is a tutorial about coding...', 'Python is a programming language...', 'Programming is an art...', 'This is a cooking tutorial...', 'Life is complicated...']\n}\n\ndf = pd.DataFrame(DATA)\n\ndef analyze_interesting_articles(df):\n    \"\"\"\n    Find the articles with titles containing 'how' or 'what', calculate and plot \n    the TF-IDF (Term Frequency-Inverse Document Frequency) scores for the content of these articles.\n\n    Parameters:\n    df (DataFrame): A pandas DataFrame with article data (Title, Content).\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - re\n    - matplotlib.pyplot\n    - sklearn.feature_extraction.text\n\n    Example:\n    >>> analyze_interesting_articles(df)\n    \"\"\"\n    pattern = re.compile(r'(how|what)', re.IGNORECASE)\n    interesting_articles = df[df['Title'].apply(lambda x: bool(pattern.search(x)))]\n\n    vectorizer = TfidfVectorizer()\n    X = vectorizer.fit_transform(interesting_articles['Content'])\n    tfidf_scores = np.array(X.sum(axis=0))[0]\n\n    plt.bar(vectorizer.get_feature_names(), tfidf_scores)\n    plt.ylabel('TF-IDF Score')\n    plt.xticks(rotation='vertical')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \npandas, re, matplotlib.pyplot, sklearn.cluster\n```python\nimport pandas as pd\nimport re\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Sample data\nDATA = {\n    'Title': ['How to code?', 'What is Python?', 'The art of programming', 'How to cook?', 'What is life?'],\n    'Content': ['This is a tutorial about coding...', 'Python is a programming language...', 'Programming is an art...', 'This is a cooking tutorial...', 'Life is complicated...']\n}\n\ndf = pd.DataFrame(DATA)\n\ndef cluster_interesting_articles(df):\n    \"\"\"\n    Find the articles with titles containing 'how' or 'what', vectorize the content \n    of these articles using CountVectorizer and apply KMeans clustering.\n\n    Parameters:\n    df (DataFrame): A pandas DataFrame with article data (Title, Content).\n\n    Returns:\n    list: A list of cluster labels for the articles.\n\n    Requirements:\n    - pandas\n    - re\n    - matplotlib.pyplot\n    - sklearn.cluster\n    - sklearn.feature_extraction.text\n\n    Example:\n    >>> cluster_interesting_articles(df)\n    \"\"\"\n    pattern = re.compile(r'(how|what)', re.IGNORECASE)\n    interesting_articles = df[df['Title'].apply(lambda x: bool(pattern.search(x)))]\n\n    vectorizer = CountVectorizer()\n    X = vectorizer.fit_transform(interesting_articles['Content'])\n\n    kmeans = KMeans(n_clusters=2)\n    kmeans.fit(X)\n\n    return kmeans.labels_\n```"}
{"seed": "{\"task_id\": 1303243, \"prompt\": \"def f_1303243(obj):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"isinstance(obj, str)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('python3') == True\\n\", \"\\n    assert candidate(1.23) == False\\n\"], \"entry_point\": \"f_1303243\", \"intent\": \"check if object `obj` is a string\", \"library\": []}", "generation": "Scenario 1:\npandas, re, nltk, sklearn\n```python\nimport pandas as pd\nimport re\nimport nltk\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Constants\nSTOPWORDS = nltk.corpus.stopwords.words('english')\n\ndef analyze_text_data(dataframe, text_column):\n    \"\"\"\n    Analyze a text column in a pandas DataFrame. Remove stop words, numbers and punctuation, \n    and then vectorize the text data.\n\n    Parameters:\n    dataframe (DataFrame): The pandas DataFrame.\n    text_column (str): The name of the text column.\n\n    Returns:\n    DataFrame: A DataFrame with the vectorized text data.\n\n    Requirements:\n    - pandas\n    - re\n    - nltk\n    - sklearn.feature_extraction.text\n\n    Example:\n    >>> df = pd.DataFrame({'text': ['This is a test.', 'Python is cool!', 'nltk and sklearn are useful for text analysis.']})\n    >>> analyze_text_data(df, 'text')\n    \"\"\"\n    def preprocess_text(text):\n        text = text.lower()\n        text = re.sub(r'\\d+', '', text)\n        text = re.sub(r'\\W+', ' ', text)\n        text = ' '.join(word for word in text.split() if word not in STOPWORDS)\n        return text\n\n    dataframe[text_column] = dataframe[text_column].apply(preprocess_text)\n    vectorizer = CountVectorizer()\n    vectorized_data = vectorizer.fit_transform(dataframe[text_column])\n\n    return pd.DataFrame(vectorized_data.toarray(), columns=vectorizer.get_feature_names_out())\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\njson, os, glob, shutil\n```python\nimport json\nimport os\nimport glob\nimport shutil\n\n# Constants\nARCHIVE_DIR = 'archive'\n\ndef archive_json_files(directory):\n    \"\"\"\n    Archive all JSON files in a directory by moving them to a designated archive directory.\n\n    Parameters:\n    directory (str): The directory where the JSON files are located.\n\n    Returns:\n    bool: True if successful, False otherwise.\n\n    Requirements:\n    - json\n    - os\n    - glob\n    - shutil\n\n    Example:\n    >>> archive_json_files('data')\n    \"\"\"\n    if not os.path.exists(ARCHIVE_DIR):\n        os.makedirs(ARCHIVE_DIR)\n\n    json_files = glob.glob(os.path.join(directory, '*.json'))\n    for json_file in json_files:\n        try:\n            shutil.move(json_file, ARCHIVE_DIR)\n        except Exception as e:\n            print(f'Unable to move {json_file} due to {str(e)}')\n            return False\n\n    return True\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, matplotlib, scipy\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n# Constants\nNUM_SAMPLES = 1000\n\ndef generate_normal_distribution(mu, sigma):\n    \"\"\"\n    Generate a normal distribution with given mean and standard deviation and plot its histogram and QQ-plot.\n\n    Parameters:\n    mu (float): The mean of the normal distribution.\n    sigma (float): The standard deviation of the normal distribution.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - scipy.stats\n\n    Example:\n    >>> generate_normal_distribution(0, 1)\n    \"\"\"\n    samples = np.random.normal(mu, sigma, NUM_SAMPLES)\n\n    plt.figure(figsize=(12, 6))\n    plt.subplot(1, 2, 1)\n    plt.hist(samples, bins=30, density=True, alpha=0.6, color='g')\n\n    plt.subplot(1, 2, 2)\n    stats.probplot(samples, dist=\"norm\", plot=plt)\n\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ncollections, string, random\n```python\nimport collections\nimport string\nimport random\n\n# Constants\nNUM_SAMPLES = 1000\n\ndef generate_random_string(length):\n    \"\"\"\n    Generate a random string of a given length and compute the frequency of each character.\n\n    Parameters:\n    length (int): The length of the random string.\n\n    Returns:\n    dict: A dictionary with the frequency of each character.\n\n    Requirements:\n    - collections\n    - string\n    - random\n\n    Example:\n    >>> generate_random_string(100)\n    \"\"\"\n    random_string = ''.join(random.choice(string.ascii_letters) for _ in range(length))\n\n    char_freq = collections.Counter(random_string)\n\n    return dict(char_freq)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nsqlite3, pandas, csv\n```python\nimport sqlite3\nimport pandas as pd\nimport csv\n\n# Constants\nDATABASE_NAME = 'test.db'\nTABLE_NAME = 'test_table'\n\ndef import_csv_to_sqlite(csv_file):\n    \"\"\"\n    Import a CSV file into a SQLite database.\n\n    Parameters:\n    csv_file (str): The path to the CSV file.\n\n    Returns:\n    DataFrame: A pandas DataFrame representation of the imported data.\n\n    Requirements:\n    - sqlite3\n    - pandas\n    - csv\n\n    Example:\n    >>> import_csv_to_sqlite('data.csv')\n    \"\"\"\n    conn = sqlite3.connect(DATABASE_NAME)\n    cursor = conn.cursor()\n\n    with open(csv_file, 'r') as f:\n        dr = csv.DictReader(f)\n        cols = dr.fieldnames\n        cursor.execute(f'DROP TABLE IF EXISTS {TABLE_NAME}')\n        cursor.execute(f'CREATE TABLE {TABLE_NAME} ({\", \".join([f\"{col}\" for col in cols])})')\n        for row in dr:\n            cursor.execute(f'INSERT INTO {TABLE_NAME} VALUES ({\", \".join([\"?\" for _ in cols])})', list(row.values()))\n\n    conn.commit()\n    dataframe = pd.read_sql_query(f'SELECT * from {TABLE_NAME}', conn)\n\n    conn.close()\n\n    return dataframe\n```"}
{"seed": "{\"task_id\": 1303243, \"prompt\": \"def f_1303243(o):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"isinstance(o, str)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(\\\"hello\\\") == True\\n\", \"\\n    assert candidate(123) == False\\n\", \"\\n    assert candidate([]) == False\\n\", \"\\n    assert candidate({\\\"aa\\\", \\\"v\\\"}) == False\\n\", \"\\n    assert candidate(\\\"123\\\") == True\\n\"], \"entry_point\": \"f_1303243\", \"intent\": \"check if object `o` is a string\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, matplotlib, random\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import choice\n\n# Constants\nDATA_TYPES = [str, int, float, list, tuple, dict, set]\n\ndef generate_random_data_frame(rows, columns):\n    \"\"\"\n    Generate a pandas DataFrame with random data and random data types. \n    \n    Parameters:\n    rows (int): The number of rows in the DataFrame.\n    columns (int): The number of columns in the DataFrame.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with random data.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - random.choice\n    \n    Example:\n    >>> df = generate_random_data_frame(5, 3)\n    >>> print(df)\n    >>> df.dtypes.plot(kind='bar')\n    \"\"\"\n    data = {}\n\n    for col in range(columns):\n        data_type = choice(DATA_TYPES)\n        if data_type == str:\n            data['col' + str(col)] = [''.join(np.random.choice(list('abcdefghijklmnopqrstuvwxyz'), size=5)) for _ in range(rows)]\n        else:\n            data['col' + str(col)] = np.random.choice([data_type(i) for i in range(10)], size=rows)\n\n    df = pd.DataFrame(data)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, scipy.stats, matplotlib\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n# Constants\nSAMPLE_SIZE = 1000\n\ndef plot_normal_distribution(mu, sigma):\n    \"\"\"\n    Generate a plot of a normal distribution with a given mean and standard deviation.\n    \n    Parameters:\n    mu (float): The mean of the normal distribution.\n    sigma (float): The standard deviation of the normal distribution.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - scipy.stats\n    \n    Example:\n    >>> plot_normal_distribution(0, 1)\n    \"\"\"\n    sample = np.random.normal(mu, sigma, SAMPLE_SIZE)\n    density = stats.gaussian_kde(sample)\n\n    x = np.linspace(min(sample), max(sample), SAMPLE_SIZE)\n    plt.plot(x, density(x))\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, json, os\n```python\nimport pandas as pd\nimport json\nimport os\n\n# Constants\nFILE_PATH = './data.json'\n\ndef check_json_data_type(file_path):\n    \"\"\"\n    Check if the data in a JSON file is a list of dictionaries (objects in JavaScript).\n    \n    Parameters:\n    file_path (str): The path to the JSON file.\n    \n    Returns:\n    bool: True if the data is a list of dictionaries, False otherwise.\n    \n    Requirements:\n    - pandas\n    - json\n    - os\n    \n    Example:\n    >>> check_json_data_type('./data.json')\n    \"\"\"\n    if not os.path.exists(file_path):\n        return False\n\n    with open(file_path, 'r') as file:\n        data = json.load(file)\n\n    if isinstance(data, list) and all(isinstance(item, dict) for item in data):\n        return True\n\n    return False\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, matplotlib, math\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\n\n# Constants\nSAMPLE_SIZE = 1000\n\ndef plot_sin_cos_wave(frequency):\n    \"\"\"\n    Generate a plot of a sine and cosine wave with a given frequency.\n    \n    Parameters:\n    frequency (float): The frequency of the wave.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - math\n    \n    Example:\n    >>> plot_sin_cos_wave(1)\n    \"\"\"\n    x = np.linspace(0, 2 * math.pi, SAMPLE_SIZE)\n    y_sin = np.sin(frequency * x)\n    y_cos = np.cos(frequency * x)\n\n    plt.plot(x, y_sin, label='sin')\n    plt.plot(x, y_cos, label='cos')\n    plt.legend()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nDATA_SIZE = 100\nBAR_COLOR = ['blue', 'green', 'red', 'cyan', 'magenta', 'yellow', 'black']\n\ndef plot_random_data_histogram(data_size):\n    \"\"\"\n    Generate a histogram for random numerical data.\n    \n    Parameters:\n    data_size (int): The size of the data.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> plot_random_data_histogram(100)\n    \"\"\"\n    data = np.random.randn(data_size)\n    plt.hist(data, bins=np.arange(-3, 4, 0.5), color=np.random.choice(BAR_COLOR), edgecolor='black')\n    plt.show()\n```"}
{"seed": "{\"task_id\": 1303243, \"prompt\": \"def f_1303243(o):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"(type(o) is str)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(\\\"hello\\\") == True\\n\", \"\\n    assert candidate(123) == False\\n\", \"\\n    assert candidate([]) == False\\n\", \"\\n    assert candidate({\\\"aa\\\", \\\"v\\\"}) == False\\n\", \"\\n    assert candidate(\\\"123\\\") == True\\n\"], \"entry_point\": \"f_1303243\", \"intent\": \"check if object `o` is a string\", \"library\": []}", "generation": "Scenario 1:\npandas, numpy, matplotlib, re\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re\n\n# Constants\nPATTERN = re.compile(r\"[a-zA-Z]\")\n\ndef analyse_string_content(dataframe, column):\n    \"\"\"\n    Check if object in a specific column of a pandas DataFrame is a string and \n    plot the distribution of string length, the count of non-alphabetic \n    characters, and the count of alphabetic characters.\n\n    Parameters:\n    dataframe (DataFrame): The pandas DataFrame.\n    column (str): The name of the column.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - re\n\n    Example:\n    >>> df = pd.DataFrame({'Name': ['John', 'Doe', '123', 456, None], 'Age': [25, 30, 35, 40, 45]})\n    >>> analyse_string_content(df, 'Name')\n    \"\"\"\n    string_lengths = []\n    non_alpha_counts = []\n    alpha_counts = []\n\n    for index, row in dataframe.iterrows():\n        if type(row[column]) is str:\n            count_alpha = len(PATTERN.findall(row[column]))\n            string_lengths.append(len(row[column]))\n            alpha_counts.append(count_alpha)\n            non_alpha_counts.append(len(row[column]) - count_alpha)\n        else:\n            string_lengths.append(0)\n            non_alpha_counts.append(0)\n            alpha_counts.append(0)\n\n    plt.figure(figsize=(15, 5))\n\n    plt.subplot(131)\n    plt.hist(string_lengths, bins=np.arange(min(string_lengths), max(string_lengths) + 1) - 0.5, edgecolor='black')\n    plt.title('String Length Distribution')\n    plt.xlabel('Length')\n    plt.ylabel('Count')\n\n    plt.subplot(132)\n    plt.hist(non_alpha_counts, bins=np.arange(min(non_alpha_counts), max(non_alpha_counts) + 1) - 0.5, edgecolor='black')\n    plt.title('Non-Alphabetic Character Count Distribution')\n    plt.xlabel('Non-Alphabetic Character Count')\n    plt.ylabel('Count')\n\n    plt.subplot(133)\n    plt.hist(alpha_counts, bins=np.arange(min(alpha_counts), max(alpha_counts) + 1) - 0.5, edgecolor='black')\n    plt.title('Alphabetic Character Count Distribution')\n    plt.xlabel('Alphabetic Character Count')\n    plt.ylabel('Count')\n\n    plt.tight_layout()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\njson, os, pathlib, glob\n```python\nimport json\nimport os\nfrom pathlib import Path\nimport glob\n\ndef find_files_containing_string(directory, string):\n    \"\"\"\n    Check if a string exists in JSON files in a given directory and its subdirectories.\n\n    Parameters:\n    directory (str): The directory path.\n    string (str): The string to find.\n\n    Returns:\n    list: A list of file paths which contain the string.\n\n    Requirements:\n    - json\n    - os\n    - pathlib\n    - glob\n\n    Example:\n    >>> find_files_containing_string('./', 'target')\n    \"\"\"\n    json_files = list(Path(directory).rglob(\"*.json\"))\n    found_files = []\n\n    for file in json_files:\n        with open(file, 'r') as f:\n            if string in json.load(f):\n                found_files.append(str(file))\n\n    return found_files\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, matplotlib, sklearn.preprocessing, sklearn.model_selection, sklearn.linear_model\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\ndef predict_with_linear_regression(X, y, test_size=0.2, random_state=None):\n    \"\"\"\n    Check if the target variable is continuous and predict it using a linear regression model.\n\n    Parameters:\n    X (array-like): The input data.\n    y (array-like): The target variable.\n    test_size (float, optional): The proportion of the dataset to include in the test split. Default is 0.2.\n    random_state (int, optional): The seed used by the random number generator. Default is None.\n\n    Returns:\n    tuple: A tuple of (model, X_train, X_test, y_train, y_test, y_pred).\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.preprocessing\n    - sklearn.model_selection\n    - sklearn.linear_model\n\n    Example:\n    >>> X = np.random.rand(100, 1)\n    >>> y = 3 * X + np.random.randn(100, 1)\n    >>> model, X_train, X_test, y_train, y_test, y_pred = predict_with_linear_regression(X, y)\n    >>> plt.scatter(X_test, y_test)\n    >>> plt.plot(X_test, y_pred, color='red')\n    \"\"\"\n    if np.issubdtype(y.dtype, np.number) and len(y.shape) == 1:\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n        scaler = StandardScaler().fit(X_train)\n        X_train_scaled = scaler.transform(X_train)\n        X_test_scaled = scaler.transform(X_test)\n\n        model = LinearRegression().fit(X_train_scaled, y_train)\n        y_pred = model.predict(X_test_scaled)\n\n        return model, X_train, X_test, y_train, y_test, y_pred\n\n    else:\n        raise ValueError(\"The target variable is not continuous.\")\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, matplotlib, sklearn.preprocessing, sklearn.decomposition\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\ndef perform_pca(dataframe, n_components=2):\n    \"\"\"\n    Check if the DataFrame contains only numeric columns and perform PCA.\n\n    Parameters:\n    dataframe (DataFrame): The pandas DataFrame.\n    n_components (int, optional): The number of components for PCA. Default is 2.\n\n    Returns:\n    DataFrame: A DataFrame of the PCA results.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.preprocessing\n    - sklearn.decomposition\n\n    Example:\n    >>> df = pd.DataFrame(np.random.rand(100, 5), columns=list('ABCDE'))\n    >>> pca_results = perform_pca(df)\n    >>> plt.scatter(pca_results['PC1'], pca_results['PC2'])\n    \"\"\"\n    if dataframe.select_dtypes(include=[np.number]).shape[1] == dataframe.shape[1]:\n        scaler = StandardScaler().fit(dataframe)\n        dataframe_scaled = scaler.transform(dataframe)\n\n        pca = PCA(n_components=n_components)\n        pca_results = pca.fit_transform(dataframe_scaled)\n\n        return pd.DataFrame(data=pca_results, columns=['PC' + str(i) for i in range(1, n_components+1)])\n\n    else:\n        raise ValueError(\"The DataFrame contains non-numeric columns.\")\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, matplotlib, seaborn, sklearn.datasets\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\n\ndef plot_iris_dataset():\n    \"\"\"\n    Load the iris dataset and plot a pairplot.\n\n    Parameters:\n    None\n\n    Returns:\n    DataFrame: A DataFrame of the iris dataset.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - seaborn\n    - sklearn.datasets\n\n    Example:\n    >>> df = plot_iris_dataset()\n    >>> sns.pairplot(df, hue=\"species\")\n    \"\"\"\n    iris = load_iris()\n    df = pd.DataFrame(data=np.c_[iris['data'], iris['target']], columns=iris['feature_names'] + ['species'])\n\n    return df\n```"}
{"seed": "{\"task_id\": 1303243, \"prompt\": \"def f_1303243(obj_to_test):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"isinstance(obj_to_test, str)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(\\\"hello\\\") == True\\n\", \"\\n    assert candidate(123) == False\\n\", \"\\n    assert candidate([]) == False\\n\", \"\\n    assert candidate({\\\"aa\\\", \\\"v\\\"}) == False\\n\", \"\\n    assert candidate(\\\"123\\\") == True\\n\"], \"entry_point\": \"f_1303243\", \"intent\": \"check if `obj_to_test` is a string\", \"library\": []}", "generation": "Scenario 1:\ncollections, json, re\n```python\nimport collections\nimport json\nimport re\n\ndef check_json_string(json_obj):\n    \"\"\"\n    Check if the given object is a valid JSON string and if it contains only alphanumeric characters and spaces.\n\n    Parameters:\n    json_obj (str): The JSON string to be tested.\n\n    Returns:\n    bool: True if the JSON string is valid and only contains alphanumeric characters and spaces, False otherwise.\n\n    Requirements:\n    - collections\n    - json\n    - re\n\n    Example:\n    >>> check_json_string('{\"name\": \"John Doe\", \"age\": 30}')\n    \"\"\"\n    try:\n        parsed_json = json.loads(json_obj)\n\n        if not isinstance(parsed_json, collections.abc.Mapping):\n            return False\n\n        for key, value in parsed_json.items():\n            if not re.match(r'^[\\w\\s]*$', key) or not re.match(r'^[\\w\\s]*$', str(value)):\n                return False\n\n        return True\n    except json.JSONDecodeError:\n        return False\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, pandas, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef plot_correlation_matrix(df):\n    \"\"\"\n    Calculate and plot the correlation matrix of a given DataFrame.\n\n    Parameters:\n    df (pandas.DataFrame): The DataFrame to be tested.\n\n    Returns:\n    numpy.ndarray: The correlation matrix.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib\n\n    Example:\n    >>> df = pd.DataFrame(np.random.rand(10, 5), columns=['A', 'B', 'C', 'D', 'E'])\n    >>> plot_correlation_matrix(df)\n    \"\"\"\n    correlation_matrix = df.corr().values\n\n    plt.matshow(correlation_matrix)\n    plt.colorbar()\n    plt.show()\n\n    return correlation_matrix\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, pathlib, hashlib\n```python\nimport os\nimport pathlib\nimport hashlib\n\ndef check_file_sha256(file_path):\n    \"\"\"\n    Calculate and return the SHA256 hash of a given file.\n\n    Parameters:\n    file_path (str): The path to the file.\n\n    Returns:\n    str: The SHA256 hash of the file.\n\n    Requirements:\n    - os\n    - pathlib\n    - hashlib\n\n    Example:\n    >>> check_file_sha256('./test.txt')\n    \"\"\"\n    file_path = pathlib.Path(file_path)\n\n    if not file_path.is_file():\n        return None\n\n    sha256_hash = hashlib.sha256()\n\n    with file_path.open('rb') as f:\n        for chunk in iter(lambda: f.read(4096), b\"\"):\n            sha256_hash.update(chunk)\n\n    return sha256_hash.hexdigest()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nmultiprocessing, time, random\n```python\nimport multiprocessing\nimport time\nimport random\n\ndef simulate_cpu_intensive_tasks(n_tasks):\n    \"\"\"\n    Simulate n CPU-intensive tasks using multiprocessing and return the time it took to complete all tasks.\n\n    Parameters:\n    n_tasks (int): The number of tasks to simulate.\n\n    Returns:\n    float: The time it took to complete all tasks.\n\n    Requirements:\n    - multiprocessing\n    - time\n    - random\n\n    Example:\n    >>> simulate_cpu_intensive_tasks(10)\n    \"\"\"\n    def cpu_intensive_task(n):\n        while n > 0:\n            n -= 1\n\n    tasks = [random.randint(10**6, 10**7) for _ in range(n_tasks)]\n\n    start_time = time.time()\n\n    with multiprocessing.Pool() as pool:\n        pool.map(cpu_intensive_task, tasks)\n\n    end_time = time.time()\n\n    return end_time - start_time\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, sklearn, numpy\n```python\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\n\ndef train_model(df, target_col):\n    \"\"\"\n    Train a RandomForestClassifier using the given DataFrame and return the accuracy of the model.\n\n    Parameters:\n    df (pandas.DataFrame): The DataFrame used to train the model.\n    target_col (str): The name of the target column.\n\n    Returns:\n    float: The accuracy of the model.\n\n    Requirements:\n    - pandas\n    - sklearn.model_selection\n    - sklearn.ensemble\n    - sklearn.metrics\n    - numpy\n\n    Example:\n    >>> df = pd.DataFrame(np.random.randint(0, 2, size=(100, 5)), columns=list('ABCDE'))\n    >>> train_model(df, 'E')\n    \"\"\"\n    X = df.drop(columns=[target_col])\n    y = df[target_col]\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    model = RandomForestClassifier()\n    model.fit(X_train, y_train)\n\n    y_pred = model.predict(X_test)\n\n    return accuracy_score(y_test, y_pred)\n```"}
{"seed": "{\"task_id\": 8177079, \"prompt\": \"def f_8177079(list1, list2):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"list2.extend(list1)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    a, b = [1, 2, 3], [4, 5, 6]\\n    candidate(a, b)\\n    assert b == [4, 5, 6, 1, 2, 3]\\n\", \"\\n    a, c = [1, 2, 3], [7, 8, 9]\\n    candidate(a, c)\\n    assert c == [7, 8, 9, 1, 2, 3] \\n\", \"\\n    b = [4, 5, 6, 1, 2, 3]\\n    c = [7, 8, 9, 1, 2, 3] \\n    candidate(b, c)\\n    assert c == [7, 8, 9, 1, 2, 3, 4, 5, 6, 1, 2, 3]\\n\"], \"entry_point\": \"f_8177079\", \"intent\": \"append list `list1` to `list2`\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, matplotlib, sklearn, random\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom random import randint, uniform\n\n# Constants\nFEATURES = ['Feature_{}'.format(i) for i in range(1, 11)]\nTARGET = 'Target'\n\ndef generate_and_analyze_data(list1, list2):\n    \"\"\"\n    Generate a pandas DataFrame using two lists as input, perform a linear regression analysis, \n    and then visualize the results.\n    \n    Parameters:\n    list1 (list): The list of values to be used as the independent variables.\n    list2 (list): The list of values to be used as the dependent variable.\n    \n    Returns:\n    float: The mean squared error of the linear regression model.\n    plot: A scatter plot of the actual vs. predicted values.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.model_selection.train_test_split\n    - sklearn.linear_model.LinearRegression\n    - sklearn.metrics.mean_squared_error\n    - random.uniform\n\n    Example:\n    >>> list1 = [[randint(1, 100) for _ in range(10)] for _ in range(1000)]\n    >>> list2 = [sum(row) + uniform(-50, 50) for row in list1]\n    >>> mse = generate_and_analyze_data(list1, list2)\n    >>> print('Mean Squared Error:', mse)\n    \"\"\"\n    df = pd.DataFrame(list1, columns=FEATURES)\n    df[TARGET] = list2\n\n    X_train, X_test, y_train, y_test = train_test_split(df[FEATURES], df[TARGET], test_size=0.2, random_state=42)\n    model = LinearRegression().fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n\n    mse = mean_squared_error(y_test, y_pred)\n\n    plt.scatter(y_test, y_pred)\n    plt.xlabel('Actual Values')\n    plt.ylabel('Predicted Values')\n    plt.show()\n\n    return mse\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, matplotlib, itertools, math\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom itertools import product\nfrom math import sin, cos\n\n# Constants\nRADIUS = 10\n\ndef draw_lissajous_figures(list1, list2):\n    \"\"\"\n    Draw Lissajous figures using two lists of frequencies.\n    \n    Parameters:\n    list1 (list): The list of frequencies for the x-axis.\n    list2 (list): The list of frequencies for the y-axis.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - itertools.product\n    - math.sin\n    - math.cos\n\n    Example:\n    >>> list1 = [1, 2, 3]\n    >>> list2 = [4, 5, 6]\n    >>> draw_lissajous_figures(list1, list2)\n    \"\"\"\n    t = np.linspace(0, 2*np.pi, 1000)\n\n    for f1, f2 in product(list1, list2):\n        x = RADIUS * np.sin(f1 * t)\n        y = RADIUS * np.sin(f2 * t)\n        plt.plot(x, y)\n        \n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ncollections, numpy, matplotlib\n```python\nimport collections\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef count_elements_and_plot_histogram(list1, list2):\n    \"\"\"\n    Count the number of occurrences of each element in two lists and plot a histogram.\n    \n    Parameters:\n    list1 (list): The first list of elements.\n    list2 (list): The second list of elements.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - collections\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> list1 = ['apple', 'banana', 'cherry', 'apple', 'banana', 'cherry']\n    >>> list2 = ['apple', 'banana', 'cherry', 'cherry', 'cherry', 'cherry']\n    >>> count_elements_and_plot_histogram(list1, list2)\n    \"\"\"\n    combined = list1 + list2\n    counter = collections.Counter(combined)\n\n    labels, values = zip(*counter.items())\n\n    indexes = np.arange(len(labels))\n    width = 0.5\n\n    plt.bar(indexes, values, width)\n    plt.xticks(indexes + width * 0.5, labels)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, matplotlib, sklearn\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\n\ndef perform_kmeans_clustering_and_visualize(list1, list2):\n    \"\"\"\n    Perform K-means clustering using two lists as input and visualize the results.\n    \n    Parameters:\n    list1 (list): The list of values to be used as the first feature.\n    list2 (list): The list of values to be used as the second feature.\n    \n    Returns:\n    KMeans: The fitted KMeans object.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.cluster.KMeans\n\n    Example:\n    >>> list1 = [1, 2, 3, 4, 5]\n    >>> list2 = [5, 4, 3, 2, 1]\n    >>> kmeans = perform_kmeans_clustering_and_visualize(list1, list2)\n    \"\"\"\n    df = pd.DataFrame({'Feature1': list1, 'Feature2': list2})\n\n    kmeans = KMeans(n_clusters=3)\n    kmeans.fit(df)\n\n    df['Cluster'] = kmeans.labels_\n\n    plt.scatter(df['Feature1'], df['Feature2'], c=df['Cluster'])\n    plt.show()\n\n    return kmeans\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, random, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import shuffle\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLORS = ['red', 'green', 'blue', 'yellow', 'purple']\n\ndef shuffle_and_visualize_data(list1, list2):\n    \"\"\"\n    Shuffle two lists, create a pandas DataFrame, and visualize the data.\n    \n    Parameters:\n    list1 (list): The list of values to be used for the x-axis.\n    list2 (list): The list of values to be used for the y-axis.\n    \n    Returns:\n    DataFrame: The shuffled pandas DataFrame.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - random.shuffle\n    - matplotlib.pyplot\n\n    Example:\n    >>> list1 = [1, 2, 3, 4, 5]\n    >>> list2 = [5, 4, 3, 2, 1]\n    >>> df = shuffle_and_visualize_data(list1, list2)\n    >>> print(df)\n    \"\"\"\n    combined = list(zip(list1, list2))\n    shuffle(combined)\n    list1, list2 = zip(*combined)\n\n    df = pd.DataFrame({'X': list1, 'Y': list2})\n\n    for i in range(len(COLORS)):\n        subset = df[i::len(COLORS)]\n        plt.scatter(subset['X'], subset['Y'], color=COLORS[i])\n\n    plt.show()\n\n    return df\n```"}
{"seed": "{\"task_id\": 8177079, \"prompt\": \"def f_8177079(mylog, list1):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"list1.extend(mylog)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    a, b = [1, 2, 3], [4, 5, 6]\\n    candidate(a, b)\\n    assert b == [4, 5, 6, 1, 2, 3]\\n\", \"\\n    a, c = [1, 2, 3], [7, 8, 9]\\n    candidate(a, c)\\n    assert c == [7, 8, 9, 1, 2, 3] \\n\", \"\\n    b = [4, 5, 6, 1, 2, 3]\\n    c = [7, 8, 9, 1, 2, 3] \\n    candidate(b, c)\\n    assert c == [7, 8, 9, 1, 2, 3, 4, 5, 6, 1, 2, 3]\\n\"], \"entry_point\": \"f_8177079\", \"intent\": \"append list `mylog` to `list1`\", \"library\": []}", "generation": "Scenario 1: \npandas, random, matplotlib, numpy\n```python\nimport pandas as pd\nfrom random import sample\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nLETTERS = list('ABCDEFGHIJKLMNOPQRSTUVWXYZ')\n\ndef create_random_dataframe(mylog, list1):\n    \"\"\"\n    Create a random DataFrame with `mylog` rows and `list1` columns. The column names are randomly \n    sampled from the English alphabet and the values are random integers between 0 and 100.\n\n    Parameters:\n    mylog (int): The number of rows.\n    list1 (int): The number of columns.\n\n    Returns:\n    DataFrame: A pandas DataFrame.\n\n    Requirements:\n    - pandas\n    - random.sample\n    - matplotlib.pyplot\n    - numpy\n\n    Example:\n    >>> df = create_random_dataframe(5, 3)\n    >>> print(df)\n    >>> df.describe().loc[['mean', 'std']].plot(kind='bar')\n    \"\"\"\n    cols = sample(LETTERS, list1)\n    data = np.random.randint(0, 100, size=(mylog, list1))\n    df = pd.DataFrame(data, columns=cols)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, random, matplotlib\n```python\nimport pandas as pd\nfrom random import choices\n\n# Constants\nCATEGORIES = ['A', 'B', 'C', 'D', 'E']\n\ndef generate_categorical_data(mylog, list1):\n    \"\"\"\n    Generate a DataFrame with `mylog` rows and `list1` columns, filled with categorical data.\n    The categories are ['A', 'B', 'C', 'D', 'E'].\n\n    Parameters:\n    mylog (int): The number of rows.\n    list1 (int): The number of columns.\n\n    Returns:\n    DataFrame: A pandas DataFrame.\n\n    Requirements:\n    - pandas\n    - random.choices\n    - matplotlib\n\n    Example:\n    >>> df = generate_categorical_data(5, 3)\n    >>> print(df)\n    >>> df.apply(pd.Series.value_counts).plot(kind='bar', stacked=True)\n    \"\"\"\n    data = [choices(CATEGORIES, k=mylog) for _ in range(list1)]\n    df = pd.DataFrame(data).transpose()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, pandas, matplotlib, random\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nCHARS = list('abcdefghijklmnopqrstuvwxyz')\n\ndef create_random_timeseries(mylog, list1):\n    \"\"\"\n    Create a random time series DataFrame with `mylog` timesteps and `list1` features.\n    The feature names are randomly sampled from the English alphabet.\n\n    Parameters:\n    mylog (int): The number of timesteps.\n    list1 (int): The number of features.\n\n    Returns:\n    DataFrame: A pandas DataFrame.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> df = create_random_timeseries(100, 5)\n    >>> print(df)\n    >>> df.plot()\n    \"\"\"\n    index = pd.date_range(end='1/1/2022', periods=mylog)\n    columns = sample(CHARS, list1)\n    data = np.random.randn(mylog, list1)\n    df = pd.DataFrame(data, index=index, columns=columns)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, random, matplotlib.pyplot\n```python\nimport pandas as pd\nfrom random import choices\nimport matplotlib.pyplot as plt\n\n# Constants\nANIMALS = ['Dog', 'Cat', 'Rabbit', 'Bird', 'Fish']\n\ndef generate_pet_data(mylog, list1):\n    \"\"\"\n    Generate a DataFrame with `mylog` rows and `list1` columns, filled with pet data.\n    The pets are ['Dog', 'Cat', 'Rabbit', 'Bird', 'Fish'].\n\n    Parameters:\n    mylog (int): The number of rows.\n    list1 (int): The number of columns.\n\n    Returns:\n    DataFrame: A pandas DataFrame.\n\n    Requirements:\n    - pandas\n    - random.choices\n    - matplotlib\n\n    Example:\n    >>> df = generate_pet_data(5, 3)\n    >>> print(df)\n    >>> df.apply(pd.Series.value_counts).plot(kind='bar', stacked=True)\n    \"\"\"\n    data = [choices(ANIMALS, k=mylog) for _ in range(list1)]\n    df = pd.DataFrame(data).transpose()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef generate_numeric_data(mylog, list1):\n    \"\"\"\n    Generate a DataFrame with `mylog` rows and `list1` columns, filled with numerical data.\n    The numerical data are random integers between 1 and 100.\n\n    Parameters:\n    mylog (int): The number of rows.\n    list1 (int): The number of columns.\n\n    Returns:\n    DataFrame: A pandas DataFrame.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib\n\n    Example:\n    >>> df = generate_numeric_data(5, 3)\n    >>> print(df)\n    >>> df.describe().loc[['mean', 'std']].plot(kind='bar')\n    \"\"\"\n    data = np.random.randint(1, 100, size=(mylog, list1))\n    df = pd.DataFrame(data)\n\n    return df\n```"}
{"seed": "{\"task_id\": 8177079, \"prompt\": \"def f_8177079(a, c):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"c.extend(a)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    a, b = [1, 2, 3], [4, 5, 6]\\n    candidate(a, b)\\n    assert b == [4, 5, 6, 1, 2, 3]\\n\", \"\\n    a, c = [1, 2, 3], [7, 8, 9]\\n    candidate(a, c)\\n    assert c == [7, 8, 9, 1, 2, 3] \\n\", \"\\n    b = [4, 5, 6, 1, 2, 3]\\n    c = [7, 8, 9, 1, 2, 3] \\n    candidate(b, c)\\n    assert c == [7, 8, 9, 1, 2, 3, 4, 5, 6, 1, 2, 3]\\n\"], \"entry_point\": \"f_8177079\", \"intent\": \"append list `a` to `c`\", \"library\": []}", "generation": "Scenario 1: \npandas, matplotlib.pyplot, numpy, random\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom random import randint\n\n# Constants\nCATEGORIES = ['Electronics', 'Clothing', 'Beauty', 'Books', 'Groceries']\nNUM_MONTHS = 12\nYEARS = [2019, 2020, 2021]\n\ndef generate_sales_report():\n    \"\"\"\n    Generate a sales report for a list of product categories over the past three years, \n    and plot the monthly sales trend for each category.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with sales data for the categories.\n    \n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - numpy\n    - random\n\n    Example:\n    >>> report = generate_sales_report()\n    >>> print(report)\n    >>> for category in CATEGORIES:\n    >>>     report[category].plot()\n    >>> plt.legend()\n    >>> plt.show()\n    \"\"\"\n    sales_data = {}\n\n    for category in CATEGORIES:\n        monthly_sales = np.random.randint(50, 500, size=(len(YEARS)*NUM_MONTHS))\n        sales_data[category] = monthly_sales\n\n    sales_df = pd.DataFrame(sales_data)\n\n    return sales_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \ncollections, random, string, json\n```python\nfrom collections import Counter\nfrom random import choice\nimport string\nimport json\n\n# Constants\nLETTERS = string.ascii_lowercase\nNUM_SAMPLES = 1000\n\ndef generate_letter_frequencies():\n    \"\"\"\n    Generate a frequency distribution of lowercase letters in a randomly generated \n    sample, and save the distribution to a JSON file.\n    \n    Returns:\n    str: The name of the JSON file.\n\n    Requirements:\n    - collections\n    - random\n    - string\n    - json\n\n    Example:\n    >>> filename = generate_letter_frequencies()\n    >>> with open(filename, 'r') as file:\n    >>>     print(json.load(file))\n    \"\"\"\n    samples = [choice(LETTERS) for _ in range(NUM_SAMPLES)]\n    frequencies = Counter(samples)\n    filename = 'frequencies.json'\n\n    with open(filename, 'w') as file:\n        json.dump(frequencies, file)\n\n    return filename\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnumpy, matplotlib.pyplot, scipy.stats\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n# Constants\nSAMPLE_SIZE = 1000\nMU = 0\nSIGMA = 1\n\ndef plot_normal_distribution():\n    \"\"\"\n    Generate a sample from a normal distribution with given mean and standard deviation, \n    and plot the histogram of the sample along with the probability density function.\n    \n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - scipy.stats\n\n    Example:\n    >>> plot_normal_distribution()\n    \"\"\"\n    sample = np.random.normal(MU, SIGMA, SAMPLE_SIZE)\n    x = np.linspace(norm.ppf(0.01), norm.ppf(0.99), 100)\n    plt.hist(sample, density=True, histtype='stepfilled', alpha=0.2)\n    plt.plot(x, norm.pdf(x), 'r-', lw=5, alpha=0.6)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nrandom, time, threading\n```python\nfrom random import randint\nimport time\nimport threading\n\n# Constants\nNUM_TASKS = 5\nMIN_SLEEP_TIME = 1\nMAX_SLEEP_TIME = 5\n\ndef execute_random_tasks():\n    \"\"\"\n    Execute a number of tasks in separate threads, where each task sleeps for a random \n    amount of time between a given range.\n\n    Returns:\n    list: A list of the durations for each task.\n\n    Requirements:\n    - random\n    - time\n    - threading\n\n    Example:\n    >>> durations = execute_random_tasks()\n    >>> print(durations)\n    \"\"\"\n    durations = []\n\n    def task():\n        sleep_time = randint(MIN_SLEEP_TIME, MAX_SLEEP_TIME)\n        time.sleep(sleep_time)\n        durations.append(sleep_time)\n\n    threads = [threading.Thread(target=task) for _ in range(NUM_TASKS)]\n\n    for thread in threads:\n        thread.start()\n\n    for thread in threads:\n        thread.join()\n\n    return durations\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nrequests, bs4, csv\n```python\nimport requests\nfrom bs4 import BeautifulSoup\nimport csv\n\n# Constants\nURL = 'https://example.com'\n\ndef scrape_and_save_data():\n    \"\"\"\n    Scrape data from a webpage and save the data to a CSV file.\n    \n    Returns:\n    str: The name of the CSV file.\n\n    Requirements:\n    - requests\n    - bs4\n    - csv\n\n    Example:\n    >>> filename = scrape_and_save_data()\n    >>> with open(filename, 'r') as file:\n    >>>     reader = csv.reader(file)\n    >>>     for row in reader:\n    >>>         print(row)\n    \"\"\"\n    response = requests.get(URL)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    data = [] # assume appropriate scraping logic here\n    filename = 'data.csv'\n\n    with open(filename, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerows(data)\n\n    return filename\n```"}
{"seed": "{\"task_id\": 8177079, \"prompt\": \"def f_8177079(mylog, list1):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"for line in mylog:\\n\\t    list1.append(line)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    a, b = [1, 2, 3], [4, 5, 6]\\n    candidate(a, b)\\n    assert b == [4, 5, 6, 1, 2, 3]\\n\", \"\\n    a, c = [1, 2, 3], [7, 8, 9]\\n    candidate(a, c)\\n    assert c == [7, 8, 9, 1, 2, 3] \\n\", \"\\n    b = [4, 5, 6, 1, 2, 3]\\n    c = [7, 8, 9, 1, 2, 3] \\n    candidate(b, c)\\n    assert c == [7, 8, 9, 1, 2, 3, 4, 5, 6, 1, 2, 3]\\n\"], \"entry_point\": \"f_8177079\", \"intent\": \"append items in list `mylog` to `list1`\", \"library\": []}", "generation": "Scenario 1:\npandas, random, matplotlib, math\n```python\nimport pandas as pd\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport math\n\n# Constants\nWORDS = ['apple', 'banana', 'cherry', 'dates', 'elderberry']\nCOUNTS = [10, 50, 100, 200, 500, 1000, 2000, 5000]\n\ndef generate_word_frequency(mylog, list1):\n    \"\"\"\n    Generates a word frequency report from 'mylog' list and appends it to 'list1' list.\n    The word frequency is represented as a pandas DataFrame with columns 'Word' and 'Frequency'.\n    It then plots the word frequency in a bar chart.\n\n    Parameters:\n    mylog (list): The list of words.\n    list1 (list): The list to append the DataFrame to.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - random\n    - matplotlib.pyplot\n    - math\n\n    Example:\n    >>> mylog = ['apple', 'banana', 'cherry', 'apple', 'banana', 'apple']\n    >>> list1 = []\n    >>> generate_word_frequency(mylog, list1)\n    >>> print(list1[0])\n    \"\"\"\n    word_frequency = pd.DataFrame({\n        'Word': WORDS,\n        'Frequency': [mylog.count(word) for word in WORDS]\n    })\n\n    list1.append(word_frequency)\n\n    word_frequency.plot(kind='bar', x='Word', y='Frequency', logy=True)\n\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, matplotlib, itertools\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport itertools\n\n# Constants\nMARKERS = itertools.cycle(('+', 'o', '*', 's', '.', 'x', 'D'))\n\ndef plot_vector_sum(mylog, list1):\n    \"\"\"\n    Appends the sum of vectors from 'mylog' list to 'list1' list and plots the vectors \n    and their sum using matplotlib. Each vector is represented by a 2-element list.\n\n    Parameters:\n    mylog (list): The list of vectors.\n    list1 (list): The list to append the sum of vectors to.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - itertools\n\n    Example:\n    >>> mylog = [[1, 2], [3, 4], [5, 6]]\n    >>> list1 = []\n    >>> plot_vector_sum(mylog, list1)\n    >>> print(list1[0])\n    \"\"\"\n    vectors = np.array(mylog)\n    vector_sum = np.sum(vectors, axis=0)\n    list1.append(vector_sum.tolist())\n\n    for vector, marker in zip(vectors, MARKERS):\n        plt.quiver(0, 0, vector[0], vector[1], angles='xy', scale_units='xy', scale=1, marker=marker)\n\n    plt.quiver(0, 0, vector_sum[0], vector_sum[1], angles='xy', scale_units='xy', scale=1, color='r')\n    plt.xlim(-1, np.max(vectors) + 1)\n    plt.ylim(-1, np.max(vectors) + 1)\n\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, scipy.stats, matplotlib\n```python\nimport numpy as np\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\n\n# Constants\nPERCENTILES = [25, 50, 75]\n\ndef generate_statistics(mylog, list1):\n    \"\"\"\n    Generates statistics from 'mylog' list and appends them to 'list1' list.\n    The statistics include mean, median, mode, and percentiles.\n    It then plots a histogram of the data with the mean, median, and mode.\n\n    Parameters:\n    mylog (list): The list of numbers.\n    list1 (list): The list to append the statistics to.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Example:\n    >>> mylog = [1, 2, 3, 2, 1, 2, 2, 3, 1, 2, 3, 1, 2, 3, 4, 5, 6]\n    >>> list1 = []\n    >>> generate_statistics(mylog, list1)\n    >>> print(list1[0])\n    \"\"\"\n    data = np.array(mylog)\n    mean = np.mean(data)\n    median = np.median(data)\n    mode = stats.mode(data).mode[0]\n    percentiles = np.percentile(data, PERCENTILES)\n\n    statistics = {\n        'Mean': mean,\n        'Median': median,\n        'Mode': mode,\n        'Percentiles': percentiles.tolist()\n    }\n\n    list1.append(statistics)\n\n    plt.hist(data, bins='auto', alpha=0.7)\n    plt.axvline(mean, color='r', linestyle='dashed', linewidth=1)\n    plt.axvline(median, color='g', linestyle='dashed', linewidth=1)\n    plt.axvline(mode, color='b', linestyle='dashed', linewidth=1)\n\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, matplotlib, statistics\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statistics as stats\n\n# Constants\nSERIES_LENGTH = 100\n\ndef generate_time_series(mylog, list1):\n    \"\"\"\n    Generates a random walk time series from 'mylog' list and appends it to 'list1' list.\n    It then plots the time series using matplotlib.\n\n    Parameters:\n    mylog (list): The list of numbers.\n    list1 (list): The list to append the time series to.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - statistics\n\n    Example:\n    >>> mylog = [1, 2, 3, 4, 5]\n    >>> list1 = []\n    >>> generate_time_series(mylog, list1)\n    >>> print(list1[0])\n    \"\"\"\n    mylog = np.array(mylog)\n    random_walk = np.cumsum(mylog - stats.mean(mylog))\n    list1.append(random_walk.tolist())\n\n    plt.plot(random_walk)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nscipy.spatial, numpy, matplotlib\n```python\nfrom scipy.spatial import distance\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nDIMENSIONS = 2\n\ndef calculate_distance(mylog, list1):\n    \"\"\"\n    Calculates the Euclidean distance between pairs of points in 'mylog' list and appends them to 'list1' list.\n    Each point is represented by a 2-element list.\n    It then plots the points and their distances using matplotlib.\n\n    Parameters:\n    mylog (list): The list of points.\n    list1 (list): The list to append the distances to.\n\n    Returns:\n    None\n\n    Requirements:\n    - scipy.spatial\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> mylog = [[0, 0], [1, 1], [2, 2], [3, 3]]\n    >>> list1 = []\n    >>> calculate_distance(mylog, list1)\n    >>> print(list1[0])\n    \"\"\"\n    points = np.array(mylog)\n    distances = [distance.euclidean(points[i], points[i+1]) for i in range(len(points)-1)]\n    list1.append(distances)\n\n    for i in range(len(points)-1):\n        plt.plot(points[[i, i+1], 0], points[[i, i+1], 1], 'bo-')\n\n    plt.show()\n```"}
{"seed": "{\"task_id\": 4126227, \"prompt\": \"def f_4126227(a, b):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"b.append((a[0][0], a[0][2]))\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    a = [(1,2,3),(4,5,6)]\\n    b = [(0,0)]\\n    candidate(a, b)\\n    assert(b == [(0, 0), (1, 3)])\\n\"], \"entry_point\": \"f_4126227\", \"intent\": \"append a tuple of elements from list `a` with indexes '[0][0] [0][2]' to list `b`\", \"library\": []}", "generation": "Scenario 1:\npandas, matplotlib.pyplot, random\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\n\n# Constants\nCOLUMNS = ['A', 'B', 'C']\nROWS = 10\n\ndef generate_dataset_and_plot(a, b):\n    \"\"\"\n    Generate a pandas DataFrame using lists 'a' and 'b', and plot the values of column 'A' \n    and column 'C' using matplotlib.\n\n    Parameters:\n    a (list): A list of tuples.\n    b (list): A list of tuples.\n\n    Returns:\n    DataFrame: A pandas DataFrame.\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> a = [(1,2,3),(4,5,6)]\n    >>> b = [(0,0)]\n    >>> df = generate_dataset_and_plot(a, b)\n    >>> print(df)\n    >>> df[['A', 'C']].plot(kind='bar')\n    \"\"\"\n    # Extend the list 'b' with tuples from 'a' at indexes '[0][0]' and '[0][2]'\n    b.append((a[0][0], a[0][2]))\n\n    # Repeat the process to generate a DataFrame with a specified number of rows\n    for _ in range(ROWS - len(b)):\n        b.append((random.randint(0, 10), random.randint(0, 10)))\n\n    df = pd.DataFrame(b, columns=COLUMNS)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, pandas\n```python\nimport numpy as np\nimport pandas as pd\n\n# Constants\nCOLUMNS = ['A', 'B', 'C']\nROWS = 10\n\ndef generate_dataframe_and_calc_std(a, b):\n    \"\"\"\n    Generate a pandas DataFrame using lists 'a' and 'b', and calculate the standard deviation \n    of column 'A' and column 'C' using numpy.\n\n    Parameters:\n    a (list): A list of tuples.\n    b (list): A list of tuples.\n\n    Returns:\n    dict: A dictionary with standard deviations.\n\n    Requirements:\n    - pandas\n    - numpy\n\n    Example:\n    >>> a = [(1,2,3),(4,5,6)]\n    >>> b = [(0,0)]\n    >>> stds = generate_dataframe_and_calc_std(a, b)\n    >>> print(stds)\n    \"\"\"\n    b.append((a[0][0], a[0][2]))\n\n    # Generate random data to populate the DataFrame\n    for _ in range(ROWS - len(b)):\n        b.append((np.random.randint(0, 10), np.random.randint(0, 10)))\n\n    df = pd.DataFrame(b, columns=COLUMNS)\n\n    # Calculate standard deviations\n    stds = {\n        'A': np.std(df['A']),\n        'C': np.std(df['C']),\n    }\n\n    return stds\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nitertools, collections\n```python\nfrom itertools import combinations\nfrom collections import Counter\n\ndef count_combinations(a, b):\n    \"\"\"\n    Append a tuple of elements from list 'a' with indexes '[0][0]' and '[0][2]' to list 'b', \n    generate all two-element combinations of 'b' and count the occurrences of each combination.\n\n    Parameters:\n    a (list): A list of tuples.\n    b (list): A list of tuples.\n\n    Returns:\n    Counter: A Counter object with the combinations and their counts.\n\n    Requirements:\n    - itertools\n    - collections\n\n    Example:\n    >>> a = [(1,2,3),(4,5,6)]\n    >>> b = [(0,0)]\n    >>> counts = count_combinations(a, b)\n    >>> print(counts)\n    \"\"\"\n    b.append((a[0][0], a[0][2]))\n\n    # Generate all combinations\n    combs = list(combinations(b, 2))\n\n    # Count the combinations\n    counts = Counter(combs)\n\n    return counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, matplotlib.pyplot\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nNUM_POINTS = 100\n\ndef generate_scatter_plot(a, b):\n    \"\"\"\n    Append a tuple of elements from list 'a' with indexes '[0][0]' and '[0][2]' to list 'b', \n    generate a scatter plot with 'b' as the x-coordinates and random y-coordinates.\n\n    Parameters:\n    a (list): A list of tuples.\n    b (list): A list of tuples.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> a = [(1,2,3),(4,5,6)]\n    >>> b = [(0,0)]\n    >>> generate_scatter_plot(a, b)\n    >>> plt.show()\n    \"\"\"\n    b.append((a[0][0], a[0][2]))\n\n    # Generate random y-coordinates\n    y = np.random.rand(NUM_POINTS)\n\n    plt.scatter(b, y)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, matplotlib.pyplot\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['A', 'B']\n\ndef generate_dataframe_and_plot_histogram(a, b):\n    \"\"\"\n    Append a tuple of elements from list 'a' with indexes '[0][0]' and '[0][2]' to list 'b', \n    generate a pandas DataFrame from 'b', and plot a histogram of column 'A' values.\n\n    Parameters:\n    a (list): A list of tuples.\n    b (list): A list of tuples.\n\n    Returns:\n    DataFrame: A pandas DataFrame.\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> a = [(1,2,3),(4,5,6)]\n    >>> b = [(0,0)]\n    >>> df = generate_dataframe_and_plot_histogram(a, b)\n    >>> print(df)\n    >>> df['A'].plot(kind='hist')\n    >>> plt.show()\n    \"\"\"\n    b.append((a[0][0], a[0][2]))\n\n    df = pd.DataFrame(b, columns=COLUMNS)\n\n    return df\n```"}
{"seed": "{\"task_id\": 34902378, \"prompt\": \"def f_34902378(app):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"app.config['SECRET_KEY'] = 'Your_secret_string'\", \"test_start\": \"\\nfrom flask import Flask\\n\\ndef check(candidate):\", \"test\": [\"\\n    app = Flask(\\\"test\\\")\\n    candidate(app)\\n    assert app.config['SECRET_KEY'] == 'Your_secret_string'\\n\"], \"entry_point\": \"f_34902378\", \"intent\": \"Initialize `SECRET_KEY` in flask config with `Your_secret_string `\", \"library\": [\"flask\"]}", "generation": "Scenario 1: \nflask, os, hashlib\n```python\nfrom flask import Flask\nimport os\nimport hashlib\n\n# Constants\nSALT = 'Your_salt_string'\n\ndef set_flask_config(app):\n    \"\"\"\n    Initialize `SECRET_KEY` in flask config with a hashed version of an environment \n    variable combined with a salt. If the environment variable doesn't exist, \n    use a default string.\n\n    Parameters:\n    app (Flask): The flask application.\n\n    Requirements:\n    - flask\n    - os\n    - hashlib\n\n    Example:\n    >>> app = Flask(\"test\")\n    >>> set_flask_config(app)\n    \"\"\"\n    secret_string = os.getenv('SECRET_STRING', 'Your_secret_string')\n    hashed_string = hashlib.sha256((secret_string + SALT).encode()).hexdigest()\n    app.config['SECRET_KEY'] = hashed_string\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nflask, os, sqlite3\n```python\nfrom flask import Flask\nimport os\nimport sqlite3\n\ndef initialize_flask_with_db(app):\n    \"\"\"\n    Initialize a Flask application with a SQLite database. The database file is \n    retrieved from an environment variable. If the variable doesn't exist, use \n    a default string.\n\n    Parameters:\n    app (Flask): The flask application.\n\n    Requirements:\n    - flask\n    - os\n    - sqlite3\n\n    Example:\n    >>> app = Flask(\"test\")\n    >>> initialize_flask_with_db(app)\n    \"\"\"\n    db_file = os.getenv('DATABASE_FILE', 'default.db')\n    conn = sqlite3.connect(db_file)\n    app.config['DATABASE'] = conn\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nflask, os, logging\n```python\nfrom flask import Flask\nimport os\nimport logging\n\ndef initialize_flask_with_logging(app):\n    \"\"\"\n    Initialize a Flask application with logging. The log file is retrieved from an \n    environment variable. If the variable doesn't exist, use a default string.\n\n    Parameters:\n    app (Flask): The flask application.\n\n    Requirements:\n    - flask\n    - os\n    - logging\n\n    Example:\n    >>> app = Flask(\"test\")\n    >>> initialize_flask_with_logging(app)\n    \"\"\"\n    log_file = os.getenv('LOG_FILE', 'default.log')\n    logging.basicConfig(filename=log_file, level=logging.INFO)\n    app.config['LOGGING'] = logging\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nflask, os, flask_sqlalchemy\n```python\nfrom flask import Flask\nimport os\nfrom flask_sqlalchemy import SQLAlchemy\n\ndef initialize_flask_with_sqlalchemy(app):\n    \"\"\"\n    Initialize a Flask application with SQLAlchemy. The database URI is retrieved from \n    an environment variable. If the variable doesn't exist, use a default sqlite database.\n\n    Parameters:\n    app (Flask): The flask application.\n\n    Requirements:\n    - flask\n    - os\n    - flask_sqlalchemy\n\n    Example:\n    >>> app = Flask(\"test\")\n    >>> initialize_flask_with_sqlalchemy(app)\n    \"\"\"\n    db_uri = os.getenv('DATABASE_URI', 'sqlite:///default.db')\n    app.config['SQLALCHEMY_DATABASE_URI'] = db_uri\n    db = SQLAlchemy(app)\n    return db\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nflask, os, flask_mail\n```python\nfrom flask import Flask\nimport os\nfrom flask_mail import Mail\n\ndef initialize_flask_with_mail(app):\n    \"\"\"\n    Initialize a Flask application with Flask-Mail. The email server details are retrieved \n    from environment variables. If the variables don't exist, use default values.\n\n    Parameters:\n    app (Flask): The flask application.\n\n    Requirements:\n    - flask\n    - os\n    - flask_mail\n\n    Example:\n    >>> app = Flask(\"test\")\n    >>> initialize_flask_with_mail(app)\n    \"\"\"\n    app.config['MAIL_SERVER'] = os.getenv('MAIL_SERVER', 'localhost')\n    app.config['MAIL_PORT'] = os.getenv('MAIL_PORT', 25)\n    app.config['MAIL_USE_TLS'] = os.getenv('MAIL_USE_TLS', False)\n    app.config['MAIL_USERNAME'] = os.getenv('MAIL_USERNAME', None)\n    app.config['MAIL_PASSWORD'] = os.getenv('MAIL_PASSWORD', None)\n    mail = Mail(app)\n    return mail\n```"}
{"seed": "{\"task_id\": 22799300, \"prompt\": \"def f_22799300(out):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"pd.DataFrame(out.tolist(), columns=['out-1', 'out-2'], index=out.index)\", \"test_start\": \"\\nimport numpy as np\\nimport pandas as pd\\nfrom scipy import stats\\n\\ndef check(candidate):\", \"test\": [\"\\n    df = pd.DataFrame(dict(x=np.random.randn(100), y=np.repeat(list(\\\"abcd\\\"), 25)))\\n    out = df.groupby(\\\"y\\\").x.apply(stats.ttest_1samp, 0)\\n    test = pd.DataFrame(out.tolist())\\n    test.columns = ['out-1', 'out-2']\\n    test.index = out.index\\n    res = candidate(out)\\n    assert(test.equals(res))\\n\"], \"entry_point\": \"f_22799300\", \"intent\": \"unpack a series of tuples in pandas `out` into a DataFrame with column names 'out-1' and 'out-2'\", \"library\": [\"numpy\", \"pandas\", \"scipy\"]}", "generation": "Scenario 1: \npandas, numpy, sklearn, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMN_NAMES = ['Feature-1', 'Feature-2']\n\ndef find_clusters(out):\n    \"\"\"\n    Fit a k-means clustering model to a pandas series of tuples 'out' and plot the clusters.\n\n    Parameters:\n    out (pd.Series): A pandas series of tuples.\n    \n    Returns:\n    np.ndarray: The cluster labels.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.cluster\n    - matplotlib.pyplot\n\n    Example:\n    >>> out = pd.Series([(3, 4), (2, 3), (6, 7), (8, 9), (1, 2), (5, 6)])\n    >>> labels = find_clusters(out)\n    >>> print(labels)\n    \"\"\"\n    df = pd.DataFrame(out.tolist(), columns=COLUMN_NAMES)\n    kmeans = KMeans(n_clusters=2)\n    kmeans.fit(df)\n\n    plt.scatter(df[COLUMN_NAMES[0]], df[COLUMN_NAMES[1]], c=kmeans.labels_)\n    plt.show()\n\n    return kmeans.labels_\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, statsmodels, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport seaborn as sns\n\n# Constants\nCOLUMN_NAMES = ['Feature-1', 'Feature-2']\n\ndef linear_regression(out):\n    \"\"\"\n    Perform a simple linear regression on a pandas series of tuples 'out' and plot the regression line.\n\n    Parameters:\n    out (pd.Series): A pandas series of tuples.\n\n    Returns:\n    statsmodels.regression.linear_model.RegressionResultsWrapper: The regression results.\n\n    Requirements:\n    - pandas\n    - numpy\n    - statsmodels.api\n    - seaborn\n\n    Example:\n    >>> out = pd.Series([(3, 4), (2, 3), (6, 7), (8, 9), (1, 2), (5, 6)])\n    >>> results = linear_regression(out)\n    >>> print(results.summary())\n    \"\"\"\n    df = pd.DataFrame(out.tolist(), columns=COLUMN_NAMES)\n    X = df[COLUMN_NAMES[0]]\n    y = df[COLUMN_NAMES[1]]\n    X = sm.add_constant(X)\n\n    model = sm.OLS(y, X)\n    results = model.fit()\n\n    sns.regplot(x=COLUMN_NAMES[0], y=COLUMN_NAMES[1], data=df)\n    plt.show()\n\n    return results\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, sklearn, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMN_NAMES = ['Feature-1', 'Feature-2']\n\ndef perform_pca(out):\n    \"\"\"\n    Perform principal component analysis (PCA) on a pandas series of tuples 'out' and plot the transformed data.\n\n    Parameters:\n    out (pd.Series): A pandas series of tuples.\n\n    Returns:\n    np.ndarray: The transformed data.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.decomposition\n    - matplotlib.pyplot\n\n    Example:\n    >>> out = pd.Series([(3, 4), (2, 3), (6, 7), (8, 9), (1, 2), (5, 6)])\n    >>> transformed_data = perform_pca(out)\n    >>> print(transformed_data)\n    \"\"\"\n    df = pd.DataFrame(out.tolist(), columns=COLUMN_NAMES)\n    pca = PCA(n_components=2)\n    transformed_data = pca.fit_transform(df)\n\n    plt.scatter(transformed_data[:, 0], transformed_data[:, 1])\n    plt.show()\n\n    return transformed_data\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, scipy, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom scipy.interpolate import interp1d\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMN_NAMES = ['Feature-1', 'Feature-2']\n\ndef interpolate_data(out):\n    \"\"\"\n    Perform interpolation on a pandas series of tuples 'out' and plot the interpolated data.\n\n    Parameters:\n    out (pd.Series): A pandas series of tuples.\n\n    Returns:\n    interp1d: The interpolation function.\n\n    Requirements:\n    - pandas\n    - numpy\n    - scipy.interpolate\n    - matplotlib.pyplot\n\n    Example:\n    >>> out = pd.Series([(1, 2), (3, 4), (5, 6), (7, 8), (9, 10), (11, 12)])\n    >>> f = interpolate_data(out)\n    >>> print(f(2.5))\n    \"\"\"\n    df = pd.DataFrame(out.tolist(), columns=COLUMN_NAMES)\n    f = interp1d(df[COLUMN_NAMES[0]], df[COLUMN_NAMES[1]])\n\n    xnew = np.linspace(min(df[COLUMN_NAMES[0]]), max(df[COLUMN_NAMES[0]]), num=100, endpoint=True)\n    plt.plot(df[COLUMN_NAMES[0]], df[COLUMN_NAMES[1]], 'o', xnew, f(xnew), '-')\n    plt.show()\n\n    return f\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, sklearn, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMN_NAMES = ['Feature-1', 'Feature-2']\n\ndef random_forest_regression(out):\n    \"\"\"\n    Fit a random forest regression model to a pandas series of tuples 'out' and plot the feature importance.\n\n    Parameters:\n    out (pd.Series): A pandas series of tuples.\n\n    Returns:\n    RandomForestRegressor: The fitted model.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.ensemble\n    - matplotlib.pyplot\n\n    Example:\n    >>> out = pd.Series([(3, 4), (2, 3), (6, 7), (8, 9), (1, 2), (5, 6)])\n    >>> model = random_forest_regression(out)\n    >>> print(model.feature_importances_)\n    \"\"\"\n    df = pd.DataFrame(out.tolist(), columns=COLUMN_NAMES)\n    X = df[[COLUMN_NAMES[0]]]\n    y = df[COLUMN_NAMES[1]]\n\n    model = RandomForestRegressor()\n    model.fit(X, y)\n\n    plt.bar([COLUMN_NAMES[0]], model.feature_importances_)\n    plt.show()\n\n    return model\n```"}
{"seed": "{\"task_id\": 1762484, \"prompt\": \"def f_1762484(stocks_list):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"[x for x in range(len(stocks_list)) if stocks_list[x] == 'MSFT']\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    stocks_list = ['AAPL', 'MSFT', 'GOOG', 'MSFT', 'MSFT']\\n    assert(candidate(stocks_list) == [1,3,4])\\n\", \"\\n    stocks_list = ['AAPL', 'MSXT', 'GOOG', 'MSAT', 'SFT']\\n    assert(candidate(stocks_list) == [])\\n\"], \"entry_point\": \"f_1762484\", \"intent\": \"find the index of an element 'MSFT' in a list `stocks_list`\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, random, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import choices\n\n# Constants\nSTOCKS = ['AAPL', 'MSFT', 'GOOG', 'TSLA', 'AMZN']\nVOLUME = list(range(1000, 10000, 100))\n\ndef simulate_stock_market(days):\n    \"\"\"\n    Simulate a stock market for a specific number of days, generating random volumes for a list of stocks.\n\n    Parameters:\n    days (int): The number of days to simulate.\n\n    Returns:\n    DataFrame: A pandas DataFrame with simulated stock volumes.\n\n    Requirements:\n    - pandas\n    - numpy\n    - random\n\n    Example:\n    >>> simulate_stock_market(7)\n    \"\"\"\n    data = []\n    for _ in range(days):\n        volumes = choices(VOLUME, k=len(STOCKS))\n        data.append(volumes)\n\n    df = pd.DataFrame(data, columns=STOCKS)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, random, matplotlib\n```python\nimport numpy as np\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nSTOCKS = ['AAPL', 'MSFT', 'GOOG', 'TSLA', 'AMZN']\n\ndef plot_stock_prices(prices):\n    \"\"\"\n    Given a list of stock prices, plot the prices over time for each stock.\n\n    Parameters:\n    prices (list): A list of lists, where each sublist contains the prices for a stock over time.\n\n    Returns:\n    None: This function does not return a value. It plots the prices using matplotlib.\n\n    Requirements:\n    - numpy\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> prices = [[randint(150, 200) for _ in range(7)] for _ in range(len(STOCKS))]\n    >>> plot_stock_prices(prices)\n    \"\"\"\n    for i in range(len(STOCKS)):\n        plt.plot(prices[i], label=STOCKS[i])\n\n    plt.legend()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, random\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import randint\n\n# Constants\nSTOCKS = ['AAPL', 'MSFT', 'GOOG', 'TSLA', 'AMZN']\nDAYS = list(range(1, 31))\n\ndef generate_stock_data(days):\n    \"\"\"\n    Generate a dataframe of random stock prices for a given number of days.\n\n    Parameters:\n    days (int): The number of days for which to generate stock data.\n\n    Returns:\n    DataFrame: A pandas DataFrame with random stock prices.\n\n    Requirements:\n    - pandas\n    - numpy\n    - random\n\n    Example:\n    >>> generate_stock_data(7)\n    \"\"\"\n    data = {stock: [randint(150, 200) for _ in range(days)] for stock in STOCKS}\n    df = pd.DataFrame(data, index=DAYS)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, random, matplotlib\n```python\nimport numpy as np\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nSTOCKS = ['AAPL', 'MSFT', 'GOOG', 'TSLA', 'AMZN']\n\ndef plot_stock_histogram(prices):\n    \"\"\"\n    Given a list of stock prices, plot a histogram of the prices for each stock.\n\n    Parameters:\n    prices (list): A list of lists, where each sublist contains the prices for a stock.\n\n    Returns:\n    None: This function does not return a value. It plots a histogram using matplotlib.\n\n    Requirements:\n    - numpy\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> prices = [[randint(150, 200) for _ in range(1000)] for _ in range(len(STOCKS))]\n    >>> plot_stock_histogram(prices)\n    \"\"\"\n    for i in range(len(STOCKS)):\n        plt.hist(prices[i], bins=10, alpha=0.5, label=STOCKS[i])\n\n    plt.legend()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, random\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import randint\n\n# Constants\nSTOCKS = ['AAPL', 'MSFT', 'GOOG', 'TSLA', 'AMZN']\n\ndef generate_stock_report(days):\n    \"\"\"\n    Generate a dataframe of random stock prices and volumes for a given number of days, and calculate the average price and total volume for each stock.\n\n    Parameters:\n    days (int): The number of days for which to generate stock data.\n\n    Returns:\n    DataFrame: A pandas DataFrame with random stock prices and volumes, and the calculated average price and total volume.\n\n    Requirements:\n    - pandas\n    - numpy\n    - random\n\n    Example:\n    >>> generate_stock_report(7)\n    \"\"\"\n    prices = {f'{stock} Price': [randint(150, 200) for _ in range(days)] for stock in STOCKS}\n    volumes = {f'{stock} Volume': [randint(1000, 10000) for _ in range(days)] for stock in STOCKS}\n\n    df = pd.DataFrame({**prices, **volumes})\n\n    for stock in STOCKS:\n        df[f'{stock} Avg Price'] = df[f'{stock} Price'].mean()\n        df[f'{stock} Total Volume'] = df[f'{stock} Volume'].sum()\n\n    return df\n```"}
{"seed": "{\"task_id\": 3464359, \"prompt\": \"def f_3464359(ax, labels):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"ax.set_xticklabels(labels, rotation=45)\", \"test_start\": \"\\nimport matplotlib.pyplot as plt \\n\\ndef check(candidate):\", \"test\": [\"\\n    fig, ax = plt.subplots()\\n    ax.plot([1, 2, 3, 4], [1, 4, 2, 3])\\n    ret = candidate(ax, [f\\\"#{i}\\\" for i in range(7)])\\n    assert [tt.get_rotation() == 45.0 for tt in ret]\\n\"], \"entry_point\": \"f_3464359\", \"intent\": \"rotate the xtick `labels` of matplotlib plot `ax` by `45` degrees to make long labels readable\", \"library\": [\"matplotlib\"]}", "generation": "Scenario 1:\npandas, matplotlib, numpy, random\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom random import sample\n\n# Constants\nCATEGORIES = ['Technology', 'Healthcare', 'Finance', 'Real Estate', 'Consumer Discretionary']\n\ndef plot_random_sample(df, n_samples):\n    \"\"\"\n    Plot a bar chart of a random sample of n items from a DataFrame, \n    and rotate the xtick labels by 45 degrees to make long labels readable.\n\n    Parameters:\n    df (DataFrame): The input pandas DataFrame. It should have a 'Category' column.\n    n_samples (int): The number of samples to select.\n\n    Returns:\n    None.\n\n    Requirements:\n    - pandas\n    - matplotlib\n    - numpy\n    - random\n\n    Example:\n    >>> df = pd.DataFrame({\n    ...     'Company': ['Apple', 'Google', 'Microsoft', 'Amazon', 'Facebook'],\n    ...     'Category': ['Technology', 'Technology', 'Technology', 'Consumer Discretionary', 'Technology'],\n    ...     'Market Cap (Billion USD)': [2271, 1561, 2023, 1696, 928]\n    ... })\n    >>> plot_random_sample(df, 3)\n    \"\"\"\n    df_sample = df.sample(n_samples)\n    ax = df_sample.plot(kind='bar', x='Company', y='Market Cap (Billion USD)', legend=False)\n    ax.set_xticklabels(df_sample['Company'], rotation=45)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, matplotlib, seaborn, numpy\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\ndef plot_heatmap(df):\n    \"\"\"\n    Plot a heatmap of a DataFrame, and rotate the xtick labels by 45 degrees to make long labels readable.\n\n    Parameters:\n    df (DataFrame): The input pandas DataFrame.\n\n    Returns:\n    None.\n\n    Requirements:\n    - pandas\n    - matplotlib\n    - seaborn\n    - numpy\n\n    Example:\n    >>> df = pd.DataFrame(\n    ...     np.random.rand(10, 12),\n    ...     columns=['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'],\n    ...     index=[f'Product {i}' for i in range(10)]\n    ... )\n    >>> plot_heatmap(df)\n    \"\"\"\n    plt.figure(figsize=(12, 8))\n    ax = sns.heatmap(df, cmap='YlGnBu')\n    ax.set_xticklabels(df.columns, rotation=45)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, matplotlib, numpy, random\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom random import randint\n\ndef plot_random_walk(n_steps):\n    \"\"\"\n    Plot a random walk of n steps, and rotate the xtick labels by 45 degrees to make long labels readable.\n\n    Parameters:\n    n_steps (int): The number of steps in the random walk.\n\n    Returns:\n    None.\n\n    Requirements:\n    - pandas\n    - matplotlib\n    - numpy\n    - random\n\n    Example:\n    >>> plot_random_walk(100)\n    \"\"\"\n    walk = pd.Series(np.random.randn(n_steps)).cumsum()\n    ax = walk.plot()\n    xticks = np.arange(0, n_steps, step=5)\n    ax.set_xticks(xticks)\n    ax.set_xticklabels([f'Step {i}' for i in xticks], rotation=45)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, matplotlib, numpy, random\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom random import randint\n\ndef plot_random_data(n_points):\n    \"\"\"\n    Plot a scatter plot of n random data points, and rotate the xtick labels by 45 degrees to make long labels readable.\n\n    Parameters:\n    n_points (int): The number of data points.\n\n    Returns:\n    None.\n\n    Requirements:\n    - pandas\n    - matplotlib\n    - numpy\n    - random\n\n    Example:\n    >>> plot_random_data(100)\n    \"\"\"\n    df = pd.DataFrame(np.random.rand(n_points, 2), columns=['x', 'y'])\n    ax = df.plot.scatter(x='x', y='y')\n    xticks = np.linspace(0, 1, num=6)\n    ax.set_xticks(xticks)\n    ax.set_xticklabels([f'{i:.2f}' for i in xticks], rotation=45)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, matplotlib, numpy, random\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom random import randint\n\ndef plot_histogram(df, column):\n    \"\"\"\n    Plot a histogram of a column in a DataFrame, and rotate the xtick labels by 45 degrees to make long labels readable.\n\n    Parameters:\n    df (DataFrame): The input pandas DataFrame.\n    column (str): The column to plot.\n\n    Returns:\n    None.\n\n    Requirements:\n    - pandas\n    - matplotlib\n    - numpy\n    - random\n\n    Example:\n    >>> df = pd.DataFrame(np.random.randn(1000), columns=['Value'])\n    >>> plot_histogram(df, 'Value')\n    \"\"\"\n    ax = df[column].plot.hist(bins=20)\n    xticks = np.linspace(df[column].min(), df[column].max(), num=6)\n    ax.set_xticks(xticks)\n    ax.set_xticklabels([f'{i:.2f}' for i in xticks], rotation=45)\n    plt.show()\n```\n"}
{"seed": "{\"task_id\": 875968, \"prompt\": \"def f_875968(s):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"re.sub('[^\\\\\\\\w]', ' ', s)\", \"test_start\": \"\\nimport re\\n\\ndef check(candidate):\", \"test\": [\"\\n    s = \\\"how much for the maple syrup? $20.99? That's ridiculous!!!\\\"\\n    assert candidate(s) == 'how much for the maple syrup   20 99  That s ridiculous   '\\n\"], \"entry_point\": \"f_875968\", \"intent\": \"remove symbols from a string `s`\", \"library\": [\"re\"]}", "generation": "Scenario 1:\nnltk, re, string, collections\n```python\nimport nltk \nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \nimport re\nimport string\nfrom collections import Counter\n\n# Constants\nSTOPWORDS = set(stopwords.words('english'))\n\ndef count_frequency(sentence):\n    \"\"\"\n    Remove symbols and stopwords from a sentence and return the frequency of each word.\n    \n    Parameters:\n    sentence (str): The sentence to process.\n    \n    Returns:\n    dict: A dictionary with words as keys and their frequencies as values.\n    \n    Requirements:\n    - nltk\n    - re\n    - string\n    - collections.Counter\n    \n    Example:\n    >>> count_frequency(\"how much for the maple syrup? $20.99? That's ridiculous!!!\")\n    \"\"\"\n    sentence = re.sub('[^\\w]', ' ', sentence)\n    word_tokens = word_tokenize(sentence)\n    filtered_sentence = [w for w in word_tokens if not w in STOPWORDS] \n    frequency = Counter(filtered_sentence)\n    \n    return dict(frequency)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nre, string, difflib\n```python\nimport re\nimport string\nimport difflib\n\n# Constants\nALPHABET = string.ascii_lowercase\n\ndef find_closest_word(word, text):\n    \"\"\"\n    Remove symbols from a text and find the closest word in the text to a given word.\n    \n    Parameters:\n    word (str): The word to find the closest match for.\n    text (str): The text to search in.\n    \n    Returns:\n    str: The closest word in the text to the given word.\n    \n    Requirements:\n    - re\n    - string\n    - difflib\n    \n    Example:\n    >>> find_closest_word('aple', \"how much for the maple syrup? $20.99? That's ridiculous!!!\")\n    \"\"\"\n    text = re.sub('[^\\w]', ' ', text)\n    words = text.split()\n    closest_word = difflib.get_close_matches(word, words, n=1)\n    \n    return closest_word[0] if closest_word else None\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nre, string, random\n```python\nimport re\nimport string\nimport random\n\n# Constants\nPUNCTUATIONS = string.punctuation\n\ndef shuffle_characters(sentence):\n    \"\"\"\n    Remove symbols from a sentence and shuffle the characters in each word.\n    \n    Parameters:\n    sentence (str): The sentence to process.\n    \n    Returns:\n    str: The sentence with the characters in each word shuffled.\n    \n    Requirements:\n    - re\n    - string\n    - random\n    \n    Example:\n    >>> shuffle_characters(\"how much for the maple syrup? $20.99? That's ridiculous!!!\")\n    \"\"\"\n    sentence = re.sub('[^\\w]', ' ', sentence)\n    words = sentence.split()\n    shuffled_sentence = ' '.join([''.join(random.sample(word, len(word))) for word in words])\n    \n    return shuffled_sentence\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nre, string, nltk, sklearn\n```python\nimport re\nimport string\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndef word_vector(sentence):\n    \"\"\"\n    Remove symbols from a sentence and convert it into a word vector.\n    \n    Parameters:\n    sentence (str): The sentence to process.\n    \n    Returns:\n    array: A word vector representation of the sentence.\n    \n    Requirements:\n    - re\n    - string\n    - nltk\n    - sklearn.feature_extraction.text.CountVectorizer\n    \n    Example:\n    >>> word_vector(\"how much for the maple syrup? $20.99? That's ridiculous!!!\")\n    \"\"\"\n    sentence = re.sub('[^\\w]', ' ', sentence)\n    word_tokens = word_tokenize(sentence)\n    vectorizer = CountVectorizer().fit(word_tokens)\n    vector = vectorizer.transform(word_tokens).toarray()\n    \n    return vector\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nre, string, matplotlib, wordcloud\n```python\nimport re\nimport string\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\n\ndef generate_wordcloud(sentence):\n    \"\"\"\n    Remove symbols from a sentence and generate a word cloud.\n    \n    Parameters:\n    sentence (str): The sentence to process.\n    \n    Returns:\n    WordCloud: A word cloud object.\n    \n    Requirements:\n    - re\n    - string\n    - matplotlib.pyplot\n    - wordcloud.WordCloud\n    \n    Example:\n    >>> wc = generate_wordcloud(\"how much for the maple syrup? $20.99? That's ridiculous!!!\")\n    >>> plt.imshow(wc, interpolation='bilinear')\n    >>> plt.axis(\"off\")\n    >>> plt.show()\n    \"\"\"\n    sentence = re.sub('[^\\w]', ' ', sentence)\n    wordcloud = WordCloud().generate(sentence)\n    \n    return wordcloud\n```"}
{"seed": "{\"task_id\": 34750084, \"prompt\": \"def f_34750084(s):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"re.findall(\\\"'\\\\\\\\\\\\\\\\[0-7]{1,3}'\\\", s)\", \"test_start\": \"\\nimport re\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(r\\\"char x = '\\\\077';\\\") == [\\\"'\\\\\\\\077'\\\"]\\n\"], \"entry_point\": \"f_34750084\", \"intent\": \"Find octal characters matches from a string `s` using regex\", \"library\": [\"re\"]}", "generation": "Scenario 1:\nre, pandas, matplotlib, numpy\n```python\nimport re\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nPATTERN = r\"'\\\\[0-7]{1,3}'\"\n\ndef analyze_octal_chars(text):\n    \"\"\"\n    Find all octal characters matches from a string `text`, count their occurrences \n    and visualize the top 10 most frequent octal characters using a bar chart.\n\n    Parameters:\n    text (str): The input string.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the octal characters and their counts.\n\n    Requirements:\n    - re\n    - pandas\n    - matplotlib.pyplot\n    - numpy\n\n    Example:\n    >>> analyze_octal_chars(\"char x = '\\\\077'; char y = '\\\\045'; ...\")\n    \"\"\"\n    matches = re.findall(PATTERN, text)\n    counts = pd.Series(matches).value_counts()\n    counts_top10 = counts[:10]\n\n    counts_top10.plot(kind='bar')\n\n    return counts_top10.to_frame(name='Count')\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nre, nltk, collections, pandas\n```python\nimport re\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nimport pandas as pd\n\n# Constants\nPATTERN = r\"'\\\\[0-7]{1,3}'\"\nSTOPWORDS = set(stopwords.words('english'))\n\ndef count_non_stopwords(text):\n    \"\"\"\n    Remove all octal characters matches from a string `text` and count the frequency of non-stopwords.\n\n    Parameters:\n    text (str): The input string.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the non-stopwords and their counts.\n\n    Requirements:\n    - re\n    - nltk.corpus\n    - collections\n    - pandas\n\n    Example:\n    >>> count_non_stopwords(\"char x = '\\\\077'; The quick brown fox ...\")\n    \"\"\"\n    text_no_octal = re.sub(PATTERN, \"\", text)\n    words = re.findall(r'\\b\\w+\\b', text_no_octal)\n    non_stopwords = [word for word in words if word not in STOPWORDS]\n    counts = Counter(non_stopwords)\n\n    return pd.DataFrame.from_dict(counts, orient='index', columns=['Count'])\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nre, csv, urllib.request, os\n```python\nimport re\nimport csv\nimport urllib.request\nimport os\n\n# Constants\nPATTERN = r\"'\\\\[0-7]{1,3}'\"\nURL = 'http://example.com/data.csv'\nFILE_PATH = '/tmp/data.csv'\n\ndef download_and_process_file():\n    \"\"\"\n    Download a CSV file from a URL, replace all octal character matches with empty string \n    and save the processed data to a new file.\n\n    Requirements:\n    - re\n    - csv\n    - urllib.request\n    - os\n\n    Example:\n    >>> download_and_process_file()\n    \"\"\"\n    urllib.request.urlretrieve(URL, FILE_PATH)\n    \n    with open(FILE_PATH, 'r') as file:\n        data = file.read()\n        processed_data = re.sub(PATTERN, '', data)\n    \n    os.remove(FILE_PATH)\n\n    with open(FILE_PATH, 'w') as file:\n        file.write(processed_data)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nre, pandas, numpy, matplotlib\n```python\nimport re\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nPATTERN = r\"'\\\\[0-7]{1,3}'\"\n\ndef analyze_text(text):\n    \"\"\"\n    Replace all octal character matches in a string `text` with spaces,\n    then perform word frequency analysis and visualize the top 10 most frequent words using a bar chart.\n\n    Parameters:\n    text (str): The input string.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the words and their counts.\n\n    Requirements:\n    - re\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> analyze_text(\"char x = '\\\\077'; The quick brown fox ...\")\n    \"\"\"\n    text_no_octal = re.sub(PATTERN, ' ', text)\n    words = pd.Series(text_no_octal.split())\n    counts = words.value_counts()\n    counts_top10 = counts[:10]\n\n    counts_top10.plot(kind='bar')\n\n    return counts_top10.to_frame(name='Count')\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nre, os, sys, hashlib\n```python\nimport re\nimport os\nimport sys\nimport hashlib\n\n# Constants\nPATTERN = r\"'\\\\[0-7]{1,3}'\"\n\ndef process_files(directory):\n    \"\"\"\n    Traverse a directory, find all text files, remove all octal character matches from each file \n    and compute the MD5 hash of the processed content.\n\n    Parameters:\n    directory (str): The directory path.\n\n    Returns:\n    dict: A dictionary with file paths as keys and their MD5 hashes as values.\n\n    Requirements:\n    - re\n    - os\n    - sys\n    - hashlib\n\n    Example:\n    >>> process_files('/path/to/directory')\n    \"\"\"\n    result = {}\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if file.endswith('.txt'):\n                path = os.path.join(root, file)\n                with open(path, 'r') as f:\n                    data = f.read()\n                    processed_data = re.sub(PATTERN, '', data)\n                    md5_hash = hashlib.md5(processed_data.encode()).hexdigest()\n                    result[path] = md5_hash\n    return result\n```\n"}
{"seed": "{\"task_id\": 13209288, \"prompt\": \"def f_13209288(input):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"re.split(r'[ ](?=[A-Z]+\\\\b)', input)\", \"test_start\": \"\\nimport re\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('HELLO there HOW are YOU') == ['HELLO there', 'HOW are', 'YOU']\\n\", \"\\n    assert candidate('hELLO there HoW are YOU') == ['hELLO there HoW are', 'YOU']\\n\", \"\\n    assert candidate('7 is a NUMBER') == ['7 is a', 'NUMBER']\\n\", \"\\n    assert candidate('NUMBER 7') == ['NUMBER 7']\\n\"], \"entry_point\": \"f_13209288\", \"intent\": \"split string `input` based on occurrences of regex pattern '[ ](?=[A-Z]+\\\\\\\\b)'\", \"library\": [\"re\"]}", "generation": "Scenario 1:\npandas, re, nltk, sklearn, matplotlib\n```python\nimport pandas as pd\nimport re\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\n\n# Constants\nSTOPWORDS = set(stopwords.words('english'))\n\ndef get_most_common_words(texts):\n    \"\"\"\n    Split each text in `texts` based on occurrences of regex pattern '[ ](?=[A-Z]+\\\\b)' and \n    get the most common words that are not in the English stopwords list.\n\n    Parameters:\n    texts (list): The list of texts.\n\n    Returns:\n    list: The list of most common words.\n\n    Requirements:\n    - pandas\n    - re\n    - nltk.corpus\n    - sklearn.feature_extraction.text\n    - matplotlib.pyplot\n\n    Example:\n    >>> texts = ['HELLO there HOW are YOU', 'hELLO there HoW are YOU', '7 is a NUMBER', 'NUMBER 7']\n    >>> get_most_common_words(texts)\n    \"\"\"\n    split_texts = []\n    for text in texts:\n        split_texts += re.split(r'[ ](?=[A-Z]+\\b)', text)\n\n    cv = CountVectorizer(stop_words=STOPWORDS)\n    count_vector = cv.fit_transform(split_texts)\n    words_freq = pd.DataFrame(count_vector.toarray(), columns=cv.get_feature_names_out()).sum()\n\n    most_common_words = words_freq.nlargest(10)\n\n    most_common_words.plot(kind='bar')\n\n    return most_common_words.index.tolist()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nre, os, glob, PIL, matplotlib\n```python\nimport re\nimport os\nimport glob\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\n# Constants\nIMAGE_EXTENSIONS = ['.jpg', '.jpeg', '.png', '.bmp']\n\ndef visualize_images_in_folder(folder_path):\n    \"\"\"\n    Find all images in a folder based on occurrences of regex pattern '[ ](?=[A-Z]+\\b)' in their names \n    and visualize them using matplotlib.\n\n    Parameters:\n    folder_path (str): The path of the folder.\n\n    Requirements:\n    - re\n    - os\n    - glob\n    - PIL\n    - matplotlib.pyplot\n\n    Example:\n    >>> visualize_images_in_folder('/path/to/folder')\n    \"\"\"\n    images = []\n    for ext in IMAGE_EXTENSIONS:\n        images += glob.glob(os.path.join(folder_path, '*' + ext))\n\n    matched_images = [img for img in images if re.search(r'[ ](?=[A-Z]+\\b)', os.path.basename(img))]\n\n    fig, axs = plt.subplots(1, len(matched_images))\n\n    for i, img_path in enumerate(matched_images):\n        img = Image.open(img_path)\n        axs[i].imshow(img)\n        axs[i].set_title(os.path.basename(img_path))\n\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nre, pandas, sklearn, matplotlib\n```python\nimport re\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nimport matplotlib.pyplot as plt\n\n# Constants\nDATASET_PATH = '/path/to/dataset.csv'\n\ndef preprocess_dataset():\n    \"\"\"\n    Load a dataset from a CSV file, split each string in the 'Name' column based on occurrences \n    of regex pattern '[ ](?=[A-Z]+\\b)', and encode the 'Label' column using LabelEncoder. \n    Also, visualize the distribution of labels.\n\n    Requirements:\n    - re\n    - pandas\n    - sklearn.preprocessing\n    - matplotlib.pyplot\n\n    Example:\n    >>> preprocess_dataset()\n    \"\"\"\n    df = pd.read_csv(DATASET_PATH)\n\n    df['Name'] = df['Name'].apply(lambda x: ' '.join(re.split(r'[ ](?=[A-Z]+\\b)', x)))\n\n    le = LabelEncoder()\n    df['Label'] = le.fit_transform(df['Label'])\n\n    df['Label'].value_counts().plot(kind='bar')\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nre, os, shutil, pathlib\n```python\nimport re\nimport os\nimport shutil\nfrom pathlib import Path\n\n# Constants\nSOURCE_FOLDER = '/path/to/source/folder'\nDEST_FOLDER = '/path/to/destination/folder'\n\ndef move_files_based_on_pattern():\n    \"\"\"\n    Move all files from a source folder to a destination folder based on occurrences \n    of regex pattern '[ ](?=[A-Z]+\\b)' in their names.\n\n    Parameters:\n    None\n\n    Requirements:\n    - re\n    - os\n    - shutil\n    - pathlib\n\n    Example:\n    >>> move_files_based_on_pattern()\n    \"\"\"\n    files = os.listdir(SOURCE_FOLDER)\n\n    for file in files:\n        if re.search(r'[ ](?=[A-Z]+\\b)', file):\n            shutil.move(Path(SOURCE_FOLDER) / file, Path(DEST_FOLDER) / file)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nre, nltk, wordcloud, matplotlib\n```python\nimport re\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\n# Constants\nTEXTS = ['HELLO there HOW are YOU', 'hELLO there HoW are YOU', '7 is a NUMBER', 'NUMBER 7']\nSTOPWORDS = set(stopwords.words('english'))\n\ndef generate_wordcloud():\n    \"\"\"\n    Split each text in `TEXTS` based on occurrences of regex pattern '[ ](?=[A-Z]+\\b)' and \n    generate a word cloud of the texts excluding the English stopwords.\n\n    Parameters:\n    None\n\n    Requirements:\n    - re\n    - nltk.corpus\n    - wordcloud\n    - matplotlib.pyplot\n\n    Example:\n    >>> generate_wordcloud()\n    \"\"\"\n    split_texts = ' '.join([re.split(r'[ ](?=[A-Z]+\\b)', text) for text in TEXTS])\n\n    wc = WordCloud(stopwords=STOPWORDS).generate(split_texts)\n\n    plt.imshow(wc, interpolation='bilinear')\n    plt.axis('off')\n    plt.show()\n```"}
{"seed": "{\"task_id\": 13209288, \"prompt\": \"def f_13209288(input):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"re.split('[ ](?=[A-Z])', input)\", \"test_start\": \"\\nimport re\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('HELLO there HOW are YOU') == ['HELLO there', 'HOW are', 'YOU']\\n\", \"\\n    assert candidate('hELLO there HoW are YOU') == ['hELLO there', 'HoW are', 'YOU']\\n\", \"\\n    assert candidate('7 is a NUMBER') == ['7 is a', 'NUMBER']\\n\", \"\\n    assert candidate('NUMBER 7') == ['NUMBER 7']\\n\"], \"entry_point\": \"f_13209288\", \"intent\": \"Split string `input` at every space followed by an upper-case letter\", \"library\": [\"re\"]}", "generation": "Scenario 1: \nre, nltk, collections, matplotlib\n```python\nimport re\nimport nltk\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef plot_word_frequency(input):\n    \"\"\"\n    Tokenize the input string using the nltk library, count the frequency of each \n    word using the collections library, and plot the word frequency distribution \n    using matplotlib.\n\n    Parameters:\n    input (str): The input string.\n\n    Returns:\n    dict: A dictionary with words as keys and their frequencies as values.\n\n    Requirements:\n    - re\n    - nltk\n    - collections\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_word_frequency('HELLO there HOW are YOU')\n    \"\"\"\n    # Remove punctuation and convert to lowercase\n    input = re.sub(r'[^\\w\\s]', '', input).lower()\n\n    # Tokenize the string\n    tokens = nltk.word_tokenize(input)\n\n    # Count the frequency of each word\n    word_freq = Counter(tokens)\n\n    # Plot the word frequency distribution\n    plt.bar(word_freq.keys(), word_freq.values())\n    plt.show()\n\n    return dict(word_freq)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nre, pandas, numpy, matplotlib\n```python\nimport re\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCATEGORIES = ['A', 'B', 'C', 'D', 'E']\n\ndef analyze_data(input):\n    \"\"\"\n    Extract numbers and categories from the input string, create a pandas DataFrame \n    with these data, calculate descriptive statistics using numpy, and visualize \n    the data using matplotlib.\n\n    Parameters:\n    input (str): The input string.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the extracted data.\n\n    Requirements:\n    - re\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> analyze_data('A:100 B:200 C:300 D:400 E:500')\n    \"\"\"\n    data = re.findall(r'([A-E]):(\\d+)', input)\n    data = [(category, int(number)) for category, number in data]\n\n    df = pd.DataFrame(data, columns=['Category', 'Number'])\n\n    # Calculate descriptive statistics\n    stats = df['Number'].describe()\n\n    # Create a bar plot of the data\n    plt.bar(df['Category'], df['Number'])\n    plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nre, os, shutil, glob\n```python\nimport re\nimport os\nimport shutil\nimport glob\n\ndef organize_files(input, destination):\n    \"\"\"\n    Match file names in the current directory to the regex pattern in the input string, \n    and move the matched files to the destination directory.\n\n    Parameters:\n    input (str): The regex pattern to match file names.\n    destination (str): The destination directory.\n\n    Returns:\n    list: A list of moved file names.\n\n    Requirements:\n    - re\n    - os\n    - shutil\n    - glob\n\n    Example:\n    >>> organize_files('*.txt', '/path/to/destination')\n    \"\"\"\n    matched_files = []\n\n    for file in glob.glob('*'):\n        if re.match(input, file):\n            shutil.move(file, os.path.join(destination, file))\n            matched_files.append(file)\n\n    return matched_files\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nre, hashlib, base64\n```python\nimport re\nimport hashlib\nimport base64\n\ndef secure_data(input, salt):\n    \"\"\"\n    Extract the password from the input string, salt it, hash it using the SHA256 \n    algorithm, and encode it using base64.\n\n    Parameters:\n    input (str): The input string containing a password.\n    salt (str): The salt.\n\n    Returns:\n    str: The base64 encoded hashed password.\n\n    Requirements:\n    - re\n    - hashlib\n    - base64\n\n    Example:\n    >>> secure_data('username:password', 'salt')\n    \"\"\"\n    password = re.search(r':(.+)', input).group(1)\n\n    # Salt and hash the password\n    hashed_password = hashlib.sha256((password + salt).encode()).digest()\n\n    # Encode the hashed password using base64\n    secure_password = base64.b64encode(hashed_password).decode()\n\n    return secure_password\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nre, json, requests\n```python\nimport re\nimport json\nimport requests\n\ndef fetch_data(input):\n    \"\"\"\n    Extract an API endpoint from the input string, send a GET request to the endpoint, \n    and return the response data in JSON format.\n\n    Parameters:\n    input (str): The input string containing an API endpoint.\n\n    Returns:\n    dict: The response data.\n\n    Requirements:\n    - re\n    - json\n    - requests\n\n    Example:\n    >>> fetch_data('Fetch data from https://api.example.com/data')\n    \"\"\"\n    endpoint = re.search(r'https?:\\/\\/[^ ]+', input).group()\n\n    response = requests.get(endpoint)\n\n    return response.json()\n```\nAbove is the illustration."}
{"seed": "{\"task_id\": 24642040, \"prompt\": \"def f_24642040(url, files, headers, data):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"requests.post(url, files=files, headers=headers, data=data)\", \"test_start\": \"\\nimport requests\\nfrom unittest.mock import Mock\\n\\ndef check(candidate):\", \"test\": [\"\\n    requests.post = Mock()\\n    try:\\n        candidate('https://www.google.com', ['a.txt'], {'accept': 'text/json'}, {'name': 'abc'})\\n    except:\\n        assert False\\n\"], \"entry_point\": \"f_24642040\", \"intent\": \"send multipart encoded file `files` to url `url` with headers `headers` and metadata `data`\", \"library\": [\"requests\"]}", "generation": "Scenario 1:\nrequests, os, json, time\n```python\nimport requests\nimport os\nimport json\nimport time\n\n# Constants\nHEADERS = {\n    'accept': 'text/json',\n    'Content-Type': 'application/json'\n}\n\ndef upload_files_and_metadata_to_server(url, directory, metadata):\n    \"\"\"\n    Upload all files from a given directory to the specified server URL along with \n    associated metadata. The function also handles the rate limiting by pausing for \n    1 second after each upload.\n\n    Parameters:\n    url (str): The server URL.\n    directory (str): The directory containing the files to be uploaded.\n    metadata (dict): The metadata to be associated with the files.\n\n    Returns:\n    list: A list of status codes for the upload responses.\n\n    Requirements:\n    - requests\n    - os\n    - json\n    - time\n\n    Example:\n    >>> upload_files_and_metadata_to_server('https://www.example.com', './uploads', {'userId': 'abc'})\n    \"\"\"\n    files = os.listdir(directory)\n    status_codes = []\n\n    for file in files:\n        with open(os.path.join(directory, file), 'rb') as f:\n            files = {'file': f}\n            response = requests.post(url, files=files, headers=HEADERS, data=json.dumps(metadata))\n            status_codes.append(response.status_code)\n            time.sleep(1)\n\n    return status_codes\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nrequests, json, pandas, matplotlib\n```python\nimport requests\nimport json\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nHEADERS = {\n    'accept': 'application/json'\n}\n\ndef fetch_and_visualize_api_data(url, parameters):\n    \"\"\"\n    Fetch data from a given REST API endpoint with provided parameters, \n    visualize the data using pandas and matplotlib, and return the response data.\n\n    Parameters:\n    url (str): The API endpoint URL.\n    parameters (dict): The parameters to be sent with the GET request.\n\n    Returns:\n    dict: The response data.\n\n    Requirements:\n    - requests\n    - json\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> fetch_and_visualize_api_data('https://api.example.com/data', {'param1': 'value1'})\n    \"\"\"\n    response = requests.get(url, params=parameters, headers=HEADERS)\n    data = json.loads(response.text)\n\n    df = pd.DataFrame(data)\n    df.plot(kind='bar')\n\n    return data\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nrequests, os, zipfile, shutil\n```python\nimport requests\nimport os\nimport zipfile\nimport shutil\n\n# Constants\nHEADERS = {\n    'accept': 'application/octet-stream'\n}\n\ndef download_and_extract_zip(url, destination_directory):\n    \"\"\"\n    Download a zip file from a given URL, extract it to a specified directory, \n    and return the names of the extracted files.\n\n    Parameters:\n    url (str): The URL of the zip file.\n    destination_directory (str): The directory where the zip file should be extracted.\n\n    Returns:\n    list: The names of the extracted files.\n\n    Requirements:\n    - requests\n    - os\n    - zipfile\n    - shutil\n\n    Example:\n    >>> download_and_extract_zip('https://www.example.com/file.zip', './downloads')\n    \"\"\"\n    response = requests.get(url, headers=HEADERS)\n    zip_path = os.path.join(destination_directory, 'file.zip')\n\n    with open(zip_path, 'wb') as f:\n        f.write(response.content)\n\n    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n        zip_ref.extractall(destination_directory)\n\n    extracted_files = os.listdir(destination_directory)\n    extracted_files.remove('file.zip')\n\n    return extracted_files\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nrequests, os, json, csv\n```python\nimport requests\nimport os\nimport json\nimport csv\n\n# Constants\nHEADERS = {\n    'accept': 'application/json'\n}\n\ndef fetch_and_write_to_csv(url, parameters, csv_file_path):\n    \"\"\"\n    Fetch data from a given URL with provided parameters, write the data \n    into a CSV file, and return the status code of the request.\n\n    Parameters:\n    url (str): The URL to fetch the data from.\n    parameters (dict): The parameters to be sent with the GET request.\n    csv_file_path (str): The path to the CSV file.\n\n    Returns:\n    int: The status code of the request.\n\n    Requirements:\n    - requests\n    - os\n    - json\n    - csv\n\n    Example:\n    >>> fetch_and_write_to_csv('https://api.example.com/data', {'param1': 'value1'}, './data.csv')\n    \"\"\"\n    response = requests.get(url, params=parameters, headers=HEADERS)\n    data = json.loads(response.text)\n\n    with open(csv_file_path, 'w', newline='') as csvfile:\n        writer = csv.DictWriter(csvfile, fieldnames=data[0].keys())\n        writer.writeheader()\n        for row in data:\n            writer.writerow(row)\n\n    return response.status_code\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nrequests, json, pandas, seaborn, matplotlib\n```python\nimport requests\nimport json\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nHEADERS = {\n    'accept': 'application/json'\n}\n\ndef fetch_api_data_and_plot_correlation(url, parameters):\n    \"\"\"\n    Fetch data from a given API endpoint with provided parameters, convert \n    the data into a pandas DataFrame, and plot a heatmap to represent the \n    correlation between numeric features.\n\n    Parameters:\n    url (str): The API endpoint URL.\n    parameters (dict): The parameters to be sent with the GET request.\n\n    Returns:\n    DataFrame: The pandas DataFrame containing the data.\n\n    Requirements:\n    - requests\n    - json\n    - pandas\n    - seaborn\n    - matplotlib.pyplot\n\n    Example:\n    >>> fetch_api_data_and_plot_correlation('https://api.example.com/data', {'param1': 'value1'})\n    \"\"\"\n    response = requests.get(url, params=parameters, headers=HEADERS)\n    data = json.loads(response.text)\n\n    df = pd.DataFrame(data)\n    corr = df.corr()\n\n    sns.heatmap(corr, annot=True, cmap='coolwarm')\n    plt.show()\n\n    return df\n```\n"}
{"seed": "{\"task_id\": 4290716, \"prompt\": \"def f_4290716(filename, bytes_):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"open(filename, 'wb').write(bytes_)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    bytes_ = b'68 65 6c 6c 6f'\\n    candidate(\\\"tmpfile\\\", bytes_)\\n\\n    with open(\\\"tmpfile\\\", 'rb') as fr:\\n        assert fr.read() == bytes_\\n\"], \"entry_point\": \"f_4290716\", \"intent\": \"write bytes `bytes_` to a file `filename` in python 3\", \"library\": []}", "generation": "Scenario 1: \nhashlib, os, sys\n```python\nimport hashlib\nimport os\nimport sys\n\ndef write_and_hash(filename, bytes_, hash_algorithm):\n    \"\"\"\n    Write bytes to a file and return the hash of the content using a specified \n    hash algorithm.\n\n    Parameters:\n    filename (str): The name of the file to write to.\n    bytes_ (bytes): The bytes to write to the file.\n    hash_algorithm (str): The name of the hash algorithm to use. Must be supported by hashlib.\n\n    Returns:\n    str: The hexdigest of the hash of the file content.\n\n    Requirements:\n    - hashlib\n    - os\n    - sys\n\n    Example:\n    >>> bytes_ = b'68 65 6c 6c 6f'\n    >>> write_and_hash(\"tmpfile\", bytes_, 'sha256')\n    \"\"\"\n    with open(filename, 'wb') as f:\n        f.write(bytes_)\n\n    hash_func = getattr(hashlib, hash_algorithm)()\n\n    with open(filename, 'rb') as f:\n        for block in iter(lambda: f.read(4096), b''):\n            hash_func.update(block)\n\n    return hash_func.hexdigest()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, shutil, csv\n```python\nimport os\nimport shutil\nimport csv\n\ndef move_and_log(filename, bytes_, destination, log_file):\n    \"\"\"\n    Write bytes to a file, move the file to a specified destination, and log the action in a CSV log file.\n\n    Parameters:\n    filename (str): The name of the file to write to.\n    bytes_ (bytes): The bytes to write to the file.\n    destination (str): The directory to move the file to.\n    log_file (str): The name of the CSV file to log the action in.\n\n    Returns:\n    str: The path of the moved file.\n\n    Requirements:\n    - os\n    - shutil\n    - csv\n\n    Example:\n    >>> bytes_ = b'68 65 6c 6c 6f'\n    >>> move_and_log(\"tmpfile\", bytes_, \"/tmp\", \"log.csv\")\n    \"\"\"\n    with open(filename, 'wb') as f:\n        f.write(bytes_)\n\n    shutil.move(filename, destination)\n    moved_file = os.path.join(destination, filename)\n\n    with open(log_file, 'a', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow([filename, moved_file])\n\n    return moved_file\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, zipfile, base64\n```python\nimport os\nimport zipfile\nimport base64\n\ndef write_zip_and_encode(filename, bytes_, zipname):\n    \"\"\"\n    Write bytes to a file, compress the file into a zip, and return the base64 encoding of the zip file.\n\n    Parameters:\n    filename (str): The name of the file to write to.\n    bytes_ (bytes): The bytes to write to the file.\n    zipname (str): The name of the zip file to create.\n\n    Returns:\n    str: The base64 encoding of the zip file.\n\n    Requirements:\n    - os\n    - zipfile\n    - base64\n\n    Example:\n    >>> bytes_ = b'68 65 6c 6c 6f'\n    >>> write_zip_and_encode(\"tmpfile\", bytes_, \"archive.zip\")\n    \"\"\"\n    with open(filename, 'wb') as f:\n        f.write(bytes_)\n\n    with zipfile.ZipFile(zipname, 'w') as zipf:\n        zipf.write(filename)\n\n    with open(zipname, 'rb') as f:\n        encoded_zip = base64.b64encode(f.read()).decode()\n\n    return encoded_zip\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, pyAesCrypt, getpass\n```python\nimport os\nimport pyAesCrypt\nimport getpass\n\n# Constants\nBUFFER_SIZE = 64 * 1024\n\ndef write_encrypt_and_delete(filename, bytes_):\n    \"\"\"\n    Write bytes to a file, encrypt the file using a password, and delete the original file.\n\n    Parameters:\n    filename (str): The name of the file to write to.\n    bytes_ (bytes): The bytes to write to the file.\n\n    Returns:\n    str: The name of the encrypted file.\n\n    Requirements:\n    - os\n    - pyAesCrypt\n    - getpass\n\n    Example:\n    >>> bytes_ = b'68 65 6c 6c 6f'\n    >>> write_encrypt_and_delete(\"tmpfile\", bytes_)\n    \"\"\"\n    with open(filename, 'wb') as f:\n        f.write(bytes_)\n\n    password = getpass.getpass(\"Enter encryption password: \")\n    encrypted_filename = f\"{filename}.aes\"\n\n    pyAesCrypt.encryptFile(filename, encrypted_filename, password, BUFFER_SIZE)\n    os.remove(filename)\n\n    return encrypted_filename\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, argparse, logging\n```python\nimport os\nimport argparse\nimport logging\n\ndef command_line_write(filename, bytes_):\n    \"\"\"\n    Command line utility to write bytes to a file and log the action.\n\n    Parameters:\n    filename (str): The name of the file to write to.\n    bytes_ (bytes): The bytes to write to the file.\n\n    Returns:\n    None\n\n    Requirements:\n    - os\n    - argparse\n    - logging\n\n    Example command line usage:\n    >>> python write_utility.py --filename tmpfile --bytes 68656c6c6f\n    \"\"\"\n    parser = argparse.ArgumentParser(description='Write bytes to a file.')\n    parser.add_argument('--filename', required=True, help='The name of the file to write to.')\n    parser.add_argument('--bytes', required=True, help='The bytes to write to the file.')\n    args = parser.parse_args()\n\n    filename = args.filename\n    bytes_ = bytes.fromhex(args.bytes)\n\n    with open(filename, 'wb') as f:\n        f.write(bytes_)\n\n    logging.basicConfig(filename='file_write.log', level=logging.INFO)\n    logging.info(f'Wrote {len(bytes_)} bytes to {filename}.')\n```"}
{"seed": "{\"task_id\": 33078554, \"prompt\": \"def f_33078554(lst, dct):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"[dct[k] for k in lst]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(['c', 'd', 'a', 'b', 'd'], {'a': '3', 'b': '3', 'c': '5', 'd': '3'}) == ['5', '3', '3', '3', '3'] \\n\", \"\\n    assert candidate(['c', 'd', 'a', 'b', 'd'], {'a': 3, 'b': 3, 'c': 5, 'd': 3}) == [5, 3, 3, 3, 3] \\n\", \"\\n    assert candidate(['c', 'd', 'a', 'b'], {'a': 3, 'b': 3, 'c': 5, 'd': 3}) == [5, 3, 3, 3]\\n\"], \"entry_point\": \"f_33078554\", \"intent\": \"get a list from a list `lst` with values mapped into a dictionary `dct`\", \"library\": []}", "generation": "Scenario 1:\npandas, numpy, matplotlib, sklearn\n\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\n# Constants\nFEATURES = ['feature1', 'feature2', 'feature3', 'feature4', 'feature5']\nTARGET = 'target'\n\ndef preprocess_and_visualize_data(df, dict_mapping):\n    \"\"\"\n    Preprocess a DataFrame by replacing certain values using a dictionary mapping, \n    standardize the features and plot a histogram of the target variable.\n    \n    Parameters:\n    df (DataFrame): The input DataFrame.\n    dict_mapping (dict): A dictionary for replacing values in df.\n    \n    Returns:\n    DataFrame: The preprocessed DataFrame.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.preprocessing.StandardScaler\n    \n    Example:\n    >>> df = pd.DataFrame({'feature1': ['a', 'b', 'c', 'd', 'e'], 'target': [1, 2, 3, 4, 5]})\n    >>> dict_mapping = {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5}\n    >>> preprocess_and_visualize_data(df, dict_mapping)\n    \"\"\"\n    \n    # Replace values using dictionary mapping\n    df = df.replace(dict_mapping)\n    \n    # Standardize the features\n    scaler = StandardScaler()\n    df[FEATURES] = scaler.fit_transform(df[FEATURES])\n    \n    # Plot histogram of the target variable\n    df[TARGET].plot.hist(bins=50)\n    plt.show()\n    \n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, scipy.stats\n\n```python\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\n\n# Constants\nFEATURES = ['feature1', 'feature2', 'feature3', 'feature4', 'feature5']\n\ndef calculate_statistics(df, dct):\n    \"\"\"\n    For each feature in a DataFrame, calculate and print the mean, median, mode, \n    and variance after replacing certain values using a dictionary mapping.\n    \n    Parameters:\n    df (DataFrame): The input DataFrame.\n    dct (dict): A dictionary for replacing values in df.\n    \n    Returns:\n    dict: A dictionary of statistics for each feature.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - scipy.stats\n    \n    Example:\n    >>> df = pd.DataFrame({'feature1': ['a', 'b', 'c', 'd', 'e'], 'feature2': [1, 2, 3, 4, 5]})\n    >>> dct = {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5}\n    >>> calculate_statistics(df, dct)\n    \"\"\"\n    \n    # Replace values using dictionary mapping\n    df = df.replace(dct)\n    \n    statistics = {}\n    \n    for feature in FEATURES:\n        # Calculate statistics\n        mean = np.mean(df[feature])\n        median = np.median(df[feature])\n        mode = stats.mode(df[feature])[0][0]\n        variance = np.var(df[feature])\n        \n        # Store statistics in dictionary\n        statistics[feature] = {'mean': mean, 'median': median, 'mode': mode, 'variance': variance}\n        \n    return statistics\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, sklearn.preprocessing\n\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\n\n# Constants\nCOLUMNS = ['column1', 'column2', 'column3', 'column4', 'column5']\n\ndef encode_and_standardize(df, dct):\n    \"\"\"\n    Encode categorical features and standardize numerical features of a DataFrame \n    after replacing certain values using a dictionary mapping.\n    \n    Parameters:\n    df (DataFrame): The input DataFrame.\n    dct (dict): A dictionary for replacing values in df.\n    \n    Returns:\n    DataFrame: The preprocessed DataFrame.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.preprocessing.LabelEncoder\n    \n    Example:\n    >>> df = pd.DataFrame({'column1': ['a', 'b', 'c', 'd', 'e'], 'column2': ['v', 'w', 'x', 'y', 'z']})\n    >>> dct = {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'v': 6, 'w': 7, 'x': 8, 'y': 9, 'z': 10}\n    >>> encode_and_standardize(df, dct)\n    \"\"\"\n    \n    # Replace values using dictionary mapping\n    df = df.replace(dct)\n    \n    # Encode categorical features\n    for column in COLUMNS:\n        if df[column].dtype == 'object':\n            le = LabelEncoder()\n            df[column] = le.fit_transform(df[column])\n            \n    # Standardize numerical features\n    df = (df - df.mean()) / df.std()\n    \n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, matplotlib.pyplot\n\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['column1', 'column2', 'column3', 'column4', 'column5']\n\ndef plot_histograms(df, dct):\n    \"\"\"\n    Replace certain values in a DataFrame using a dictionary mapping and plot a \n    histogram for each column.\n    \n    Parameters:\n    df (DataFrame): The input DataFrame.\n    dct (dict): A dictionary for replacing values in df.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = pd.DataFrame({'column1': [1, 2, 3, 4, 5], 'column2': [6, 7, 8, 9, 10]})\n    >>> dct = {1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'v', 7: 'w', 8: 'x', 9: 'y', 10: 'z'}\n    >>> plot_histograms(df, dct)\n    \"\"\"\n    \n    # Replace values using dictionary mapping\n    df = df.replace(dct)\n    \n    # Plot a histogram for each column\n    for column in COLUMNS:\n        df[column].plot.hist(bins=50)\n        plt.title(column)\n        plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, scipy.stats\n\n```python\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\n\n# Constants\nCOLUMNS = ['column1', 'column2', 'column3', 'column4', 'column5']\n\ndef calculate_correlation(df, dct):\n    \"\"\"\n    Replace certain values in a DataFrame using a dictionary mapping and calculate \n    the Pearson correlation coefficient between each pair of columns.\n    \n    Parameters:\n    df (DataFrame): The input DataFrame.\n    dct (dict): A dictionary for replacing values in df.\n    \n    Returns:\n    DataFrame: A DataFrame with the correlation coefficients.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - scipy.stats\n    \n    Example:\n    >>> df = pd.DataFrame({'column1': [1, 2, 3, 4, 5], 'column2': [6, 7, 8, 9, 10]})\n    >>> dct = {1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'v', 7: 'w', 8: 'x', 9: 'y', 10: 'z'}\n    >>> calculate_correlation(df, dct)\n    \"\"\"\n    \n    # Replace values using dictionary mapping\n    df = df.replace(dct)\n    \n    # Calculate the correlation matrix\n    correlation_matrix = df.corr(method='pearson')\n    \n    return correlation_matrix\n```\n"}
{"seed": "{\"task_id\": 15247628, \"prompt\": \"def f_15247628(x):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"x['name'][x.duplicated('name')]\", \"test_start\": \"\\nimport pandas as pd \\n\\ndef check(candidate): \", \"test\": [\"\\n    assert candidate(pd.DataFrame([{'name': 'willy', 'age': 10}, {'name': 'wilson', 'age': 11}, {'name': 'zoe', 'age': 10}])).tolist() == [] \\n\", \"\\n    assert candidate(pd.DataFrame([{'name': 'willy', 'age': 10}, {'name': 'willy', 'age': 11}, {'name': 'zoe', 'age': 10}])).tolist() == ['willy'] \\n\", \"\\n    assert candidate(pd.DataFrame([{'name': 'willy', 'age': 11}, {'name': 'willy', 'age': 11}, {'name': 'zoe', 'age': 10}])).tolist() == ['willy'] \\n\", \"\\n    assert candidate(pd.DataFrame([{'name': 'Willy', 'age': 11}, {'name': 'willy', 'age': 11}, {'name': 'zoe', 'age': 10}])).tolist() == []\\n\"], \"entry_point\": \"f_15247628\", \"intent\": \"find duplicate names in column 'name' of the dataframe `x`\", \"library\": [\"pandas\"]}", "generation": "Scenario 1: \npandas, numpy, matplotlib.pyplot, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nCOLUMNS = ['Name', 'Age', 'Country', 'Score']\n\ndef visualize_data_distribution(df):\n    \"\"\"\n    Plot a histogram of the scores in the DataFrame, excluding rows with duplicate names.\n    Also, plot a boxplot to visualize the score distribution in each country, considering only unique names.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame containing the data.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> data = pd.DataFrame([{'Name': 'James', 'Age': 30, 'Country': 'USA', 'Score': 85},\n                             {'Name': 'James', 'Age': 35, 'Country': 'USA', 'Score': 90},\n                             {'Name': 'Lily', 'Age': 28, 'Country': 'Canada', 'Score': 92},\n                             {'Name': 'Sam', 'Age': 40, 'Country': 'UK', 'Score': 88},\n                             {'Name': 'Nick', 'Age': 50, 'Country': 'Australia', 'Score': 80}])\n    >>> visualize_data_distribution(data)\n    \"\"\"\n    df = df.drop_duplicates(subset='Name')\n\n    plt.figure(figsize=(10, 5))\n    plt.subplot(1, 2, 1)\n    sns.histplot(df['Score'], bins=10)\n    plt.title('Histogram of Scores')\n\n    plt.subplot(1, 2, 2)\n    sns.boxplot(x='Country', y='Score', data=df)\n    plt.title('Boxplot of Scores by Country')\n\n    plt.tight_layout()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, random, collections, datetime\n```python\nimport pandas as pd\nimport random\nimport collections\nfrom datetime import datetime\n\n# Constants\nCATEGORIES = ['Electronics', 'Fashion', 'Home', 'Beauty', 'Sports']\n\ndef generate_sales_report(df):\n    \"\"\"\n    Generate a sales report from a DataFrame, excluding rows with duplicate customer names. \n    The report should include the total sales and the most popular category.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame containing the data.\n\n    Returns:\n    dict: A dictionary with total sales and most popular category.\n\n    Requirements:\n    - pandas\n    - random\n    - collections\n    - datetime\n\n    Example:\n    >>> data = pd.DataFrame([{'Customer': 'John', 'Category': 'Electronics', 'Sales': 500},\n                             {'Customer': 'John', 'Category': 'Fashion', 'Sales': 200},\n                             {'Customer': 'Mary', 'Category': 'Home', 'Sales': 300},\n                             {'Customer': 'Peter', 'Category': 'Beauty', 'Sales': 400},\n                             {'Customer': 'Nick', 'Category': 'Sports', 'Sales': 600}])\n    >>> report = generate_sales_report(data)\n    >>> print(report)\n    \"\"\"\n    df = df.drop_duplicates(subset='Customer')\n\n    total_sales = df['Sales'].sum()\n    popular_category = collections.Counter(df['Category']).most_common(1)[0][0]\n\n    return {'Total Sales': total_sales, 'Most Popular Category': popular_category}\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \npandas, numpy, scipy.stats, matplotlib.pyplot\n```python\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef analyze_data(df):\n    \"\"\"\n    Perform a linear regression between 'Age' and 'Score' in the DataFrame, excluding rows with duplicate names.\n    Plot the regression line and the scatter plot of the data.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame containing the data.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Example:\n    >>> data = pd.DataFrame([{'Name': 'James', 'Age': 30, 'Score': 85},\n                             {'Name': 'James', 'Age': 35, 'Score': 90},\n                             {'Name': 'Lily', 'Age': 28, 'Score': 92},\n                             {'Name': 'Sam', 'Age': 40, 'Score': 88},\n                             {'Name': 'Nick', 'Age': 50, 'Score': 80}])\n    >>> analyze_data(data)\n    \"\"\"\n    df = df.drop_duplicates(subset='Name')\n\n    slope, intercept, r_value, _, _ = stats.linregress(df['Age'], df['Score'])\n\n    plt.figure(figsize=(8, 6))\n    plt.scatter(df['Age'], df['Score'], label='Data')\n    plt.plot(df['Age'], intercept + slope * df['Age'], 'r', label='Fitted line')\n    plt.xlabel('Age')\n    plt.ylabel('Score')\n    plt.title('Linear Regression')\n    plt.legend()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, sklearn.model_selection, sklearn.ensemble, sklearn.metrics\n```python\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\ndef predict_category(df):\n    \"\"\"\n    Predict the category based on 'Age' and 'Score' in the DataFrame, excluding rows with duplicate names.\n    Use a Random Forest Classifier for the prediction.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame containing the data.\n\n    Returns:\n    float: The accuracy of the prediction.\n\n    Requirements:\n    - pandas\n    - sklearn.model_selection\n    - sklearn.ensemble\n    - sklearn.metrics\n\n    Example:\n    >>> data = pd.DataFrame([{'Name': 'James', 'Age': 30, 'Score': 85, 'Category': 'Electronics'},\n                             {'Name': 'James', 'Age': 35, 'Score': 90, 'Category': 'Fashion'},\n                             {'Name': 'Lily', 'Age': 28, 'Score': 92, 'Category': 'Home'},\n                             {'Name': 'Sam', 'Age': 40, 'Score': 88, 'Category': 'Beauty'},\n                             {'Name': 'Nick', 'Age': 50, 'Score': 80, 'Category': 'Sports'}])\n    >>> accuracy = predict_category(data)\n    >>> print(accuracy)\n    \"\"\"\n    df = df.drop_duplicates(subset='Name')\n\n    X = df[['Age', 'Score']]\n    y = df['Category']\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    model = RandomForestClassifier(random_state=42)\n    model.fit(X_train, y_train)\n    predictions = model.predict(X_test)\n\n    accuracy = accuracy_score(y_test, predictions)\n\n    return accuracy\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \npandas, numpy, matplotlib.pyplot, sklearn.preprocessing\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\ndef visualize_age_score(df):\n    \"\"\"\n    Standardize 'Age' and 'Score' in the DataFrame, excluding rows with duplicate names.\n    Plot a scatter plot of the standardized 'Age' and 'Score'.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame containing the data.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.preprocessing\n\n    Example:\n    >>> data = pd.DataFrame([{'Name': 'James', 'Age': 30, 'Score': 85},\n                             {'Name': 'James', 'Age': 35, 'Score': 90},\n                             {'Name': 'Lily', 'Age': 28, 'Score': 92},\n                             {'Name': 'Sam', 'Age': 40, 'Score': 88},\n                             {'Name': 'Nick', 'Age': 50, 'Score': 80}])\n    >>> visualize_age_score(data)\n    \"\"\"\n    df = df.drop_duplicates(subset='Name')\n\n    scaler = StandardScaler()\n\n    df[['Age', 'Score']] = scaler.fit_transform(df[['Age', 'Score']])\n\n    plt.figure(figsize=(8, 6))\n    plt.scatter(df['Age'], df['Score'])\n    plt.xlabel('Age (standardized)')\n    plt.ylabel('Score (standardized)')\n    plt.title('Scatter Plot of Standardized Age and Score')\n    plt.show()\n```"}
{"seed": "{\"task_id\": 783897, \"prompt\": \"def f_783897():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"round(1.923328437452, 3)\", \"test_start\": \"\\ndef check(candidate): \", \"test\": [\"\\n    assert candidate() == 1.923\\n\"], \"entry_point\": \"f_783897\", \"intent\": \"truncate float 1.923328437452 to 3 decimal places\", \"library\": []}", "generation": "Scenario 1:\nnumpy, pandas, random, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nfrom random import uniform\nimport matplotlib.pyplot as plt\n\n# Constants\nN_DATA_POINTS = 1000\nMIN_VALUE = 0.0\nMAX_VALUE = 10.0\n\ndef generate_and_plot_data():\n    \"\"\"\n    Generate a random dataset of floating point numbers, truncate each value to 3 decimal \n    places, and plot a histogram of the data.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the generated data.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> data = generate_and_plot_data()\n    >>> print(data)\n    >>> data.plot(kind='hist', bins=20)\n    \"\"\"\n    data = [round(uniform(MIN_VALUE, MAX_VALUE), 3) for _ in range(N_DATA_POINTS)]\n    data_df = pd.DataFrame(data, columns=['Value'])\n\n    return data_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, pandas, random, seaborn\n```python\nimport numpy as np\nimport pandas as pd\nfrom random import uniform\nimport seaborn as sns\n\n# Constants\nN_DATA_POINTS = 10000\nMIN_VALUE = 0.0\nMAX_VALUE = 10.0\n\ndef generate_and_plot_kde():\n    \"\"\"\n    Generate a random dataset of floating point numbers, truncate each value to 3 decimal \n    places, and plot a KDE (Kernel Density Estimate) of the data.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the generated data.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - random\n    - seaborn\n\n    Example:\n    >>> data = generate_and_plot_kde()\n    >>> print(data)\n    >>> sns.kdeplot(data['Value'])\n    \"\"\"\n    data = [round(uniform(MIN_VALUE, MAX_VALUE), 3) for _ in range(N_DATA_POINTS)]\n    data_df = pd.DataFrame(data, columns=['Value'])\n\n    return data_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, pandas, random, scipy.stats\n```python\nimport numpy as np\nimport pandas as pd\nfrom random import uniform\nfrom scipy import stats\n\n# Constants\nN_DATA_POINTS = 5000\nMIN_VALUE = 0.0\nMAX_VALUE = 10.0\n\ndef generate_and_compute_stats():\n    \"\"\"\n    Generate a random dataset of floating point numbers, truncate each value to 3 decimal \n    places, and compute statistical measures (mean, median, mode) of the data.\n\n    Returns:\n    dict: A dictionary with statistical measures as keys and their corresponding values.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - random\n    - scipy.stats\n\n    Example:\n    >>> stats = generate_and_compute_stats()\n    >>> print(stats)\n    \"\"\"\n    data = [round(uniform(MIN_VALUE, MAX_VALUE), 3) for _ in range(N_DATA_POINTS)]\n    data_df = pd.DataFrame(data, columns=['Value'])\n\n    mean = data_df['Value'].mean()\n    median = data_df['Value'].median()\n    mode = stats.mode(data_df['Value'].values)[0][0]\n\n    return {'mean': mean, 'median': median, 'mode': mode}\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, pandas, random, sklearn.preprocessing\n```python\nimport numpy as np\nimport pandas as pd\nfrom random import uniform\nfrom sklearn.preprocessing import StandardScaler\n\n# Constants\nN_DATA_POINTS = 5000\nMIN_VALUE = 0.0\nMAX_VALUE = 10.0\n\ndef generate_and_normalize_data():\n    \"\"\"\n    Generate a random dataset of floating point numbers, truncate each value to 3 decimal \n    places, and normalize the data using standard scaling (mean=0, std=1).\n\n    Returns:\n    DataFrame: A pandas DataFrame with the normalized data.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - random\n    - sklearn.preprocessing\n\n    Example:\n    >>> normalized_data = generate_and_normalize_data()\n    >>> print(normalized_data)\n    \"\"\"\n    data = [round(uniform(MIN_VALUE, MAX_VALUE), 3) for _ in range(N_DATA_POINTS)]\n    data_df = pd.DataFrame(data, columns=['Value'])\n\n    scaler = StandardScaler()\n    normalized_data = scaler.fit_transform(data_df[['Value']])\n\n    return pd.DataFrame(normalized_data, columns=['Normalized Value'])\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, pandas, random, sklearn.model_selection\n```python\nimport numpy as np\nimport pandas as pd\nfrom random import uniform\nfrom sklearn.model_selection import train_test_split\n\n# Constants\nN_DATA_POINTS = 10000\nMIN_VALUE = 0.0\nMAX_VALUE = 10.0\nTEST_SIZE = 0.2\n\ndef generate_and_split_data():\n    \"\"\"\n    Generate a random dataset of floating point numbers, truncate each value to 3 decimal \n    places, and split the data into train and test sets.\n\n    Returns:\n    tuple: A tuple with two pandas DataFrames (train set, test set).\n    \n    Requirements:\n    - numpy\n    - pandas\n    - random\n    - sklearn.model_selection\n\n    Example:\n    >>> train_data, test_data = generate_and_split_data()\n    >>> print(train_data)\n    >>> print(test_data)\n    \"\"\"\n    data = [round(uniform(MIN_VALUE, MAX_VALUE), 3) for _ in range(N_DATA_POINTS)]\n    data_df = pd.DataFrame(data, columns=['Value'])\n\n    train_data, test_data = train_test_split(data_df, test_size=TEST_SIZE)\n\n    return train_data, test_data\n```"}
{"seed": "{\"task_id\": 22859493, \"prompt\": \"def f_22859493(li):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"sorted(li, key=lambda x: datetime.strptime(x[1], '%d/%m/%Y'), reverse=True)\", \"test_start\": \"\\nfrom datetime import datetime\\n\\ndef check(candidate): \", \"test\": [\"\\n    assert candidate([['name', '01/03/2012', 'job'], ['name', '02/05/2013', 'job'], ['name', '03/08/2014', 'job']]) == [['name', '03/08/2014', 'job'], ['name', '02/05/2013', 'job'], ['name', '01/03/2012', 'job']] \\n\", \"\\n    assert candidate([['name', '01/03/2012', 'job'], ['name', '02/05/2012', 'job'], ['name', '03/08/2012', 'job']]) == [['name', '03/08/2012', 'job'], ['name', '02/05/2012', 'job'], ['name', '01/03/2012', 'job']] \\n\", \"\\n    assert candidate([['name', '01/03/2012', 'job'], ['name', '02/03/2012', 'job'], ['name', '03/03/2012', 'job']]) == [['name', '03/03/2012', 'job'], ['name', '02/03/2012', 'job'], ['name', '01/03/2012', 'job']] \\n\", \"\\n    assert candidate([['name', '03/03/2012', 'job'], ['name', '03/03/2012', 'job'], ['name', '03/03/2012', 'job']]) == [['name', '03/03/2012', 'job'], ['name', '03/03/2012', 'job'], ['name', '03/03/2012', 'job']] \\n\"], \"entry_point\": \"f_22859493\", \"intent\": \"sort list `li` in descending order based on the date value in second element of each list in list `li`\", \"library\": [\"datetime\"]}", "generation": "Scenario 1: \npandas, datetime, matplotlib, numpy\n```python\nimport pandas as pd\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nDATE_FORMAT = '%d/%m/%Y'\nFIELDS = ['Name', 'Date', 'Job', 'Salary']\n\n# Sample data\n# data = [['John', '01/03/2012', 'Engineer', 5000], ['Jane', '02/05/2013', 'Doctor', 7000], ['Joe', '03/08/2014', 'Lawyer', 6000]]\n\ndef plot_salary_trend(data):\n    \"\"\"\n    Plot a line graph of salary trend over the years for different jobs.\n    \n    Parameters:\n    data (list): A list of lists, where each list contains name, date, job, and salary.\n    \n    Returns:\n    None. The function plots a graph.\n    \n    Requirements:\n    - pandas\n    - datetime\n    - matplotlib.pyplot\n    - numpy\n    \n    Example:\n    >>> data = [['John', '01/03/2012', 'Engineer', 5000], ['Jane', '02/05/2013', 'Doctor', 7000], ['Joe', '03/08/2014', 'Lawyer', 6000]]\n    >>> plot_salary_trend(data)\n    \"\"\"\n    df = pd.DataFrame(data, columns=FIELDS)\n    df['Date'] = pd.to_datetime(df['Date'], format=DATE_FORMAT)\n    df.sort_values(by='Date', inplace=True)\n\n    jobs = df['Job'].unique()\n    for job in jobs:\n        job_df = df[df['Job'] == job]\n        plt.plot(job_df['Date'], job_df['Salary'], label=job)\n\n    plt.legend()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ndatetime, random, matplotlib.pyplot, numpy\n```python\nfrom datetime import datetime\nimport random\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nDATE_FORMAT = '%d/%m/%Y'\nFIELDS = ['Name', 'Date', 'Job']\n\n# Sample data\n# data = [['John', '01/03/2012', 'Engineer'], ['Jane', '02/05/2013', 'Doctor'], ['Joe', '03/08/2014', 'Lawyer']]\n\ndef job_distribution(data):\n    \"\"\"\n    Plot a pie chart showing job distribution in the given data.\n    \n    Parameters:\n    data (list): A list of lists, where each list contains name, date, and job.\n    \n    Returns:\n    None. The function plots a pie chart.\n    \n    Requirements:\n    - datetime\n    - random\n    - matplotlib.pyplot\n    - numpy\n    \n    Example:\n    >>> data = [['John', '01/03/2012', 'Engineer'], ['Jane', '02/05/2013', 'Doctor'], ['Joe', '03/08/2014', 'Lawyer']]\n    >>> job_distribution(data)\n    \"\"\"\n    job_count = {}\n    for row in data:\n        if row[2] in job_count:\n            job_count[row[2]] += 1\n        else:\n            job_count[row[2]] = 1\n\n    labels = list(job_count.keys())\n    sizes = list(job_count.values())\n    colors = [plt.cm.Spectral(i/float(len(labels))) for i in range(len(labels))]\n        \n    plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)\n    plt.axis('equal')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ndatetime, pandas, matplotlib.pyplot, seaborn\n```python\nfrom datetime import datetime\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nDATE_FORMAT = '%d/%m/%Y'\nFIELDS = ['Name', 'Date', 'Job', 'Age']\n\n# Sample data\n# data = [['John', '01/03/2012', 'Engineer', 25], ['Jane', '02/05/2013', 'Doctor', 30], ['Joe', '03/08/2014', 'Lawyer', 35]]\n\ndef age_job_relation(data):\n    \"\"\"\n    Plot a bar graph showing the average age for each job type.\n    \n    Parameters:\n    data (list): A list of lists, where each list contains name, date, job, and age.\n    \n    Returns:\n    None. The function plots a bar graph.\n    \n    Requirements:\n    - datetime\n    - pandas\n    - matplotlib.pyplot\n    - seaborn\n    \n    Example:\n    >>> data = [['John', '01/03/2012', 'Engineer', 25], ['Jane', '02/05/2013', 'Doctor', 30], ['Joe', '03/08/2014', 'Lawyer', 35]]\n    >>> age_job_relation(data)\n    \"\"\"\n    df = pd.DataFrame(data, columns=FIELDS)\n    df['Date'] = pd.to_datetime(df['Date'], format=DATE_FORMAT)\n    df.sort_values(by='Date', inplace=True)\n\n    average_age = df.groupby('Job')['Age'].mean()\n\n    sns.barplot(x=average_age.index, y=average_age.values, palette=\"Blues_d\")\n    plt.xlabel('Job')\n    plt.ylabel('Average Age')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ndatetime, pandas, matplotlib.pyplot, seaborn\n```python\nfrom datetime import datetime\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nDATE_FORMAT = '%d/%m/%Y'\nFIELDS = ['Name', 'Date', 'Job', 'Performance']\n\n# Sample data\n# data = [['John', '01/03/2012', 'Engineer', 80], ['Jane', '02/05/2013', 'Doctor', 85], ['Joe', '03/08/2014', 'Lawyer', 90]]\n\ndef performance_over_time(data):\n    \"\"\"\n    Plot a line graph of performance over time for different jobs.\n    \n    Parameters:\n    data (list): A list of lists, where each list contains name, date, job, and performance.\n    \n    Returns:\n    None. The function plots a line graph.\n    \n    Requirements:\n    - datetime\n    - pandas\n    - matplotlib.pyplot\n    - seaborn\n    \n    Example:\n    >>> data = [['John', '01/03/2012', 'Engineer', 80], ['Jane', '02/05/2013', 'Doctor', 85], ['Joe', '03/08/2014', 'Lawyer', 90]]\n    >>> performance_over_time(data)\n    \"\"\"\n    df = pd.DataFrame(data, columns=FIELDS)\n    df['Date'] = pd.to_datetime(df['Date'], format=DATE_FORMAT)\n    df.sort_values(by='Date', inplace=True)\n\n    jobs = df['Job'].unique()\n    for job in jobs:\n        job_df = df[df['Job'] == job]\n        plt.plot(job_df['Date'], job_df['Performance'], label=job)\n\n    plt.legend()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\ndatetime, pandas, matplotlib.pyplot, seaborn\n```python\nfrom datetime import datetime\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nDATE_FORMAT = '%d/%m/%Y'\nFIELDS = ['Name', 'Date', 'Job', 'Absence']\n\n# Sample data\n# data = [['John', '01/03/2012', 'Engineer', 5], ['Jane', '02/05/2013', 'Doctor', 3], ['Joe', '03/08/2014', 'Lawyer', 2]]\n\ndef absence_job_relation(data):\n    \"\"\"\n    Plot a bar graph showing the total absence for each job type.\n    \n    Parameters:\n    data (list): A list of lists, where each list contains name, date, job, and absence.\n    \n    Returns:\n    None. The function plots a bar graph.\n    \n    Requirements:\n    - datetime\n    - pandas\n    - matplotlib.pyplot\n    - seaborn\n    \n    Example:\n    >>> data = [['John', '01/03/2012', 'Engineer', 5], ['Jane', '02/05/2013', 'Doctor', 3], ['Joe', '03/08/2014', 'Lawyer', 2]]\n    >>> absence_job_relation(data)\n    \"\"\"\n    df = pd.DataFrame(data, columns=FIELDS)\n    df['Date'] = pd.to_datetime(df['Date'], format=DATE_FORMAT)\n    df.sort_values(by='Date', inplace=True)\n\n    total_absence = df.groupby('Job')['Absence'].sum()\n\n    sns.barplot(x=total_absence.index, y=total_absence.values, palette=\"Blues_d\")\n    plt.xlabel('Job')\n    plt.ylabel('Total Absence')\n    plt.show()\n```\n"}
{"seed": "{\"task_id\": 29394552, \"prompt\": \"def f_29394552(ax):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"ax.set_rlabel_position(135)\", \"test_start\": \"\\nimport matplotlib.pyplot as plt \\n\\ndef check(candidate): \", \"test\": [\"\\n    ax = plt.subplot(111, polar=True)\\n    candidate(ax)\\n    assert ax.properties()['rlabel_position'] == 135.0\\n\"], \"entry_point\": \"f_29394552\", \"intent\": \"place the radial ticks in plot `ax` at 135 degrees\", \"library\": [\"matplotlib\"]}", "generation": "Scenario 1:\nmatplotlib, numpy, random\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\n\n# Constants\nCOLORS = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n\ndef plot_random_function(ax):\n    \"\"\"\n    Generate a random mathematical function and plot it on the polar plot `ax` \n    with radial ticks placed at a randomly chosen position between 0 and 180 degrees.\n\n    Parameters:\n    ax (matplotlib.axes._subplots.AxesSubplot): The ax to plot on.\n\n    Returns:\n    str: The color of the plotted function.\n\n    Requirements:\n    - matplotlib.pyplot\n    - numpy\n    - random\n\n    Example:\n    >>> fig = plt.figure()\n    >>> ax = fig.add_subplot(111, polar=True)\n    >>> color = plot_random_function(ax)\n    >>> plt.show()\n    \"\"\"\n    x = np.linspace(0, 2 * np.pi, 1000)\n    y = np.sin(random.randint(1, 10)*x)\n\n    color = random.choice(COLORS)\n    ax.plot(x, y, color=color)\n    ax.set_rlabel_position(random.randint(0, 180))\n\n    return color\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nmatplotlib, numpy, random\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\n\n# Constants\nFUNCTIONS = [np.sin, np.cos, np.tan]\n\ndef plot_function_with_ticks(ax, func_index):\n    \"\"\"\n    Plot a function from a list of functions on the polar plot `ax` \n    with radial ticks placed at a position corresponding to the function's index multiplied by 45 degrees.\n\n    Parameters:\n    ax (matplotlib.axes._subplots.AxesSubplot): The ax to plot on.\n    func_index (int): The index of the function in the FUNCTIONS list.\n\n    Returns:\n    None.\n\n    Requirements:\n    - matplotlib.pyplot\n    - numpy\n    - random\n\n    Example:\n    >>> fig = plt.figure()\n    >>> ax = fig.add_subplot(111, polar=True)\n    >>> plot_function_with_ticks(ax, 1)\n    >>> plt.show()\n    \"\"\"\n    x = np.linspace(0, 2 * np.pi, 1000)\n    y = FUNCTIONS[func_index](x)\n\n    ax.plot(x, y)\n    ax.set_rlabel_position(func_index * 45)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nmatplotlib, numpy, math\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport math\n\ndef plot_spiral(ax, num_turns):\n    \"\"\"\n    Plot a spiral on the polar plot `ax` with the number of turns specified by `num_turns`.\n    The radial ticks are placed at a position corresponding to the number of turns multiplied by 45 degrees.\n\n    Parameters:\n    ax (matplotlib.axes._subplots.AxesSubplot): The ax to plot on.\n    num_turns (int): The number of turns for the spiral.\n\n    Returns:\n    None.\n\n    Requirements:\n    - matplotlib.pyplot\n    - numpy\n    - math\n\n    Example:\n    >>> fig = plt.figure()\n    >>> ax = fig.add_subplot(111, polar=True)\n    >>> plot_spiral(ax, 3)\n    >>> plt.show()\n    \"\"\"\n    r = np.linspace(0, num_turns * 2 * math.pi, 1000)\n    theta = r\n\n    ax.plot(theta, r)\n    ax.set_rlabel_position(num_turns * 45)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nmatplotlib, numpy\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_random_polar_scatter(ax, num_points):\n    \"\"\"\n    Generate `num_points` random points and plot them on the polar plot `ax`.\n    The radial ticks are placed at a position corresponding to the number of points divided by 10 degrees.\n\n    Parameters:\n    ax (matplotlib.axes._subplots.AxesSubplot): The ax to plot on.\n    num_points (int): The number of random points to generate.\n\n    Returns:\n    None.\n\n    Requirements:\n    - matplotlib.pyplot\n    - numpy\n\n    Example:\n    >>> fig = plt.figure()\n    >>> ax = fig.add_subplot(111, polar=True)\n    >>> plot_random_polar_scatter(ax, 100)\n    >>> plt.show()\n    \"\"\"\n    r = np.random.rand(num_points)\n    theta = 2 * np.pi * np.random.rand(num_points)\n\n    ax.scatter(theta, r)\n    ax.set_rlabel_position(num_points / 10)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nmatplotlib, numpy\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_circle_with_radius(ax, radius):\n    \"\"\"\n    Plot a circle with a specified radius on the polar plot `ax`.\n    The radial ticks are placed at a position corresponding to the radius multiplied by 45 degrees.\n\n    Parameters:\n    ax (matplotlib.axes._subplots.AxesSubplot): The ax to plot on.\n    radius (float): The radius of the circle.\n\n    Returns:\n    None.\n\n    Requirements:\n    - matplotlib.pyplot\n    - numpy\n\n    Example:\n    >>> fig = plt.figure()\n    >>> ax = fig.add_subplot(111, polar=True)\n    >>> plot_circle_with_radius(ax, 1.5)\n    >>> plt.show()\n    \"\"\"\n    theta = np.linspace(0, 2 * np.pi, 1000)\n\n    ax.plot(theta, radius * np.ones_like(theta))\n    ax.set_rlabel_position(radius * 45)\n```"}
{"seed": "{\"task_id\": 3320406, \"prompt\": \"def f_3320406(my_path):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"os.path.isabs(my_path)\", \"test_start\": \"\\nimport os\\n\\ndef check(candidate): \", \"test\": [\"\\n    assert candidate('.') == False \\n\", \"\\n    assert candidate('/') == True \\n\", \"\\n    assert candidate('/usr') == True\\n\"], \"entry_point\": \"f_3320406\", \"intent\": \"check if path `my_path` is an absolute path\", \"library\": [\"os\"]}", "generation": "Scenario 1: \nos, glob, shutil, time\n```python\nimport os\nimport glob\nimport shutil\nimport time\n\n# Constants\nFILE_EXTENSIONS = ['.txt', '.csv', '.xlsx', '.docx', '.pdf']\n\ndef archive_old_files(my_path, days_old):\n    \"\"\"\n    Archive files that are older than a specified number of days in a given directory.\n\n    Parameters:\n    my_path (str): The directory path.\n    days_old (int): The number of days old the files are.\n\n    Returns:\n    str: The path of the archive.\n\n    Requirements:\n    - os\n    - glob\n    - shutil\n    - time\n\n    Example:\n    >>> archive_old_files('/usr/my_directory', 30)\n    \"\"\"\n    archive_dir = os.path.join(my_path, 'archive')\n    os.makedirs(archive_dir, exist_ok=True)\n\n    for ext in FILE_EXTENSIONS:\n        files = glob.glob(os.path.join(my_path, '*' + ext))\n        for file in files:\n            if os.path.isfile(file) and os.path.getmtime(file) < time.time() - days_old * 86400:\n                shutil.move(file, archive_dir)\n\n    return archive_dir\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, pathlib, sys, datetime\n```python\nimport os\nfrom pathlib import Path\nimport sys\nfrom datetime import datetime\n\n# Constants\nFOLDER_NAME_FORMAT = '%Y%m%d'\n\ndef create_backup(my_path):\n    \"\"\"\n    Create a backup of a directory with the current date as the folder name.\n\n    Parameters:\n    my_path (str): The directory path to be backed up.\n\n    Returns:\n    str: The path of the backup.\n\n    Requirements:\n    - os\n    - pathlib.Path\n    - sys\n    - datetime.datetime\n\n    Example:\n    >>> create_backup('/usr/my_directory')\n    \"\"\"\n    if not os.path.isabs(my_path):\n        print(f\"Path {my_path} is not absolute. Please provide an absolute path.\")\n        sys.exit(1)\n\n    backup_dir = os.path.join(my_path, datetime.now().strftime(FOLDER_NAME_FORMAT))\n    Path(backup_dir).mkdir(parents=True, exist_ok=True)\n    \n    for item in os.listdir(my_path):\n        s = os.path.join(my_path, item)\n        d = os.path.join(backup_dir, item)\n        if os.path.isdir(s):\n            shutil.copytree(s, d, False, None)\n        else:\n            shutil.copy2(s, d)\n\n    return backup_dir\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, shutil, glob, fnmatch\n```python\nimport os\nimport shutil\nimport glob\nimport fnmatch\n\n# Constants\nIGNORED_FILES = ['*.tmp', '*.bak', '*.swp']\n\ndef clean_directory(my_path):\n    \"\"\"\n    Clean a directory by removing ignored files.\n\n    Parameters:\n    my_path (str): The directory path.\n\n    Returns:\n    None\n\n    Requirements:\n    - os\n    - shutil\n    - glob\n    - fnmatch\n\n    Example:\n    >>> clean_directory('/usr/my_directory')\n    \"\"\"\n    for root, dirnames, filenames in os.walk(my_path):\n        for pattern in IGNORED_FILES:\n            for filename in fnmatch.filter(filenames, pattern):\n                os.remove(os.path.join(root, filename))\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, os.path, csv, collections\n```python\nimport os\nimport os.path\nimport csv\nimport collections\n\n# Constants\nFILE_NAME = 'file_sizes.csv'\n\ndef generate_file_size_report(my_path):\n    \"\"\"\n    Generate a report of file sizes in a directory and write it to a CSV file.\n\n    Parameters:\n    my_path (str): The directory path.\n\n    Returns:\n    str: The path of the CSV file.\n\n    Requirements:\n    - os\n    - os.path\n    - csv\n    - collections\n\n    Example:\n    >>> generate_file_size_report('/usr/my_directory')\n    \"\"\"\n    file_sizes = collections.defaultdict(int)\n\n    for dirpath, dirnames, filenames in os.walk(my_path):\n        for f in filenames:\n            fp = os.path.join(dirpath, f)\n            file_sizes[f] += os.path.getsize(fp)\n\n    with open(os.path.join(my_path, FILE_NAME), 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(['File Name', 'Size'])\n        for row in file_sizes.items():\n            writer.writerow(row)\n\n    return os.path.join(my_path, FILE_NAME)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, os.path, hashlib, json\n```python\nimport os\nimport os.path\nimport hashlib\nimport json\n\n# Constants\nHASH_FILE_NAME = 'hashes.json'\n\ndef generate_file_hashes(my_path):\n    \"\"\"\n    Generate hashes for all the files in a directory and store them in a JSON file.\n\n    Parameters:\n    my_path (str): The directory path.\n\n    Returns:\n    str: The path of the JSON file.\n\n    Requirements:\n    - os\n    - os.path\n    - hashlib\n    - json\n\n    Example:\n    >>> generate_file_hashes('/usr/my_directory')\n    \"\"\"\n    file_hashes = {}\n\n    for dirpath, dirnames, filenames in os.walk(my_path):\n        for f in filenames:\n            fp = os.path.join(dirpath, f)\n            with open(fp, 'rb') as file:\n                file_hashes[f] = hashlib.md5(file.read()).hexdigest()\n\n    with open(os.path.join(my_path, HASH_FILE_NAME), 'w') as json_file:\n        json.dump(file_hashes, json_file, indent=4)\n\n    return os.path.join(my_path, HASH_FILE_NAME)\n```"}
{"seed": "{\"task_id\": 2212433, \"prompt\": \"def f_2212433(yourdict):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"len(list(yourdict.keys()))\", \"test_start\": \"\\ndef check(candidate): \", \"test\": [\"\\n    assert candidate({'a': 1, 'b': 2, 'c': 3}) == 3 \\n\", \"\\n    assert candidate({'a': 2, 'c': 3}) == 2\\n\"], \"entry_point\": \"f_2212433\", \"intent\": \"get number of keys in dictionary `yourdict`\", \"library\": []}", "generation": "Scenario 1: \ncollections, itertools, random\n```python\nimport collections\nfrom itertools import chain\nfrom random import choice\n\n# Constants\nLETTERS = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']\n\ndef make_random_dict(n_keys, n_values):\n    \"\"\"\n    Create a Python dictionary with a random number of keys and values, \n    where keys are strings and values are integers.\n\n    Parameters:\n    n_keys (int): The number of keys to generate.\n    n_values (int): The number of values to generate.\n\n    Returns:\n    dict: A Python dictionary.\n\n    Example:\n    >>> make_random_dict(3, 5)\n    {'a': [1, 2, 3, 4, 5], 'b': [1, 2, 3, 4, 5], 'c': [1, 2, 3, 4, 5]}\n    \"\"\"\n    keys = [choice(LETTERS) for _ in range(n_keys)]\n    values = list(range(1, n_values + 1))\n    return dict(collections.OrderedDict((k, values) for k in keys))\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nre, itertools, collections\n```python\nimport re\nfrom itertools import groupby\nfrom collections import Counter\n\ndef count_words(sentence):\n    \"\"\"\n    Count the occurrence of each word in a sentence and return the result as a dictionary.\n\n    Parameters:\n    sentence (str): The sentence to count the words in.\n\n    Returns:\n    dict: A dictionary where the keys are the words and the values are the counts.\n\n    Example:\n    >>> count_words(\"apple banana apple orange orange orange\")\n    {'apple': 2, 'banana': 1, 'orange': 3}\n    \"\"\"\n    words = re.findall(r'\\b\\w+\\b', sentence)\n    return dict(Counter(words))\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnumpy, itertools, random\n```python\nimport numpy as np\nfrom itertools import combinations\nfrom random import randint\n\ndef generate_random_pairs(n):\n    \"\"\"\n    Generate a list of random pairs of integers within a range of n.\n\n    Parameters:\n    n (int): The range of integers to generate pairs from.\n\n    Returns:\n    list: A list of tuple pairs.\n\n    Example:\n    >>> generate_random_pairs(5)\n    [(1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (3, 4)]\n    \"\"\"\n    numbers = np.arange(1, n + 1)\n    pairs = list(combinations(numbers, 2))\n    return pairs\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nrandom, itertools, math\n```python\nimport random\nfrom itertools import combinations\nimport math\n\ndef find_closest_pair(n):\n    \"\"\"\n    Generate n random points in a 2D space and find the pair that is closest to each other.\n\n    Parameters:\n    n (int): The number of points to generate.\n\n    Returns:\n    tuple: A tuple of the form ((x1, y1), (x2, y2)), which are the coordinates of the closest pair.\n\n    Example:\n    >>> find_closest_pair(5)\n    ((0.1, 0.2), (0.3, 0.4))\n    \"\"\"\n    points = [(random.random(), random.random()) for i in range(n)]\n    closest_pair = min(combinations(points, 2), key=lambda pair: math.hypot(pair[0][0]-pair[1][0], pair[0][1]-pair[1][1]))\n    return closest_pair\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nrandom, collections, itertools\n```python\nimport random\nfrom collections import Counter\nfrom itertools import chain\n\n# Constants\nCARDS = ['2', '3', '4', '5', '6', '7', '8', '9', '10', 'J', 'Q', 'K', 'A']\n\ndef draw_poker_hand():\n    \"\"\"\n    Draw a 5-card poker hand from a deck of 52 cards.\n\n    Returns:\n    list: A list of 5 cards.\n\n    Example:\n    >>> draw_poker_hand()\n    ['2', '3', '4', '5', '6']\n    \"\"\"\n    return random.sample(CARDS, 5)\n```"}
{"seed": "{\"task_id\": 2212433, \"prompt\": \"def f_2212433(yourdictfile):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"len(set(open(yourdictfile).read().split()))\", \"test_start\": \"\\ndef check(candidate): \", \"test\": [\"\\n    with open('dict.txt', 'w') as fw:\\n        for w in [\\\"apple\\\", \\\"banana\\\", \\\"tv\\\", \\\"apple\\\", \\\"phone\\\"]:\\n            fw.write(f\\\"{w}\\\\n\\\")\\n    assert candidate('dict.txt') == 4\\n\"], \"entry_point\": \"f_2212433\", \"intent\": \"count the number of keys in dictionary `yourdictfile`\", \"library\": []}", "generation": "Scenario 1:\npandas, collections, os\n```python\nimport pandas as pd\nfrom collections import Counter\nimport os\n\n# Constants\nFILE_DIR = './yourdictfiles/'\n\ndef count_words_in_files():\n    \"\"\"\n    Count the number of unique words across multiple dictionary files in a given directory.\n\n    Returns:\n    int: The number of unique words.\n\n    Requirements:\n    - pandas\n    - collections.Counter\n    - os\n\n    Example:\n    >>> count_words_in_files()\n    \"\"\"\n    word_counts = Counter()\n\n    for file_name in os.listdir(FILE_DIR):\n        if not file_name.endswith('.txt'):\n            continue\n        with open(os.path.join(FILE_DIR, file_name), 'r') as file:\n            words = file.read().split()\n            word_counts.update(words)\n\n    return len(word_counts)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, collections, os, matplotlib.pyplot\n```python\nimport pandas as pd\nfrom collections import Counter\nimport os\nimport matplotlib.pyplot as plt\n\n# Constants\nFILE_DIR = './yourdictfiles/'\n\ndef plot_word_frequency():\n    \"\"\"\n    Count the frequency of words across multiple dictionary files in a given directory \n    and plot the top 10 most frequent words.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - collections.Counter\n    - os\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_word_frequency()\n    \"\"\"\n    word_counts = Counter()\n\n    for file_name in os.listdir(FILE_DIR):\n        if not file_name.endswith('.txt'):\n            continue\n        with open(os.path.join(FILE_DIR, file_name), 'r') as file:\n            words = file.read().split()\n            word_counts.update(words)\n\n    # Get the top 10 most common words\n    top_10 = word_counts.most_common(10)\n    words, counts = zip(*top_10)\n\n    plt.bar(words, counts)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, collections, os, csv\n```python\nimport pandas as pd\nfrom collections import Counter\nimport os\nimport csv\n\n# Constants\nFILE_DIR = './yourdictfiles/'\n\ndef export_word_counts_to_csv(filename):\n    \"\"\"\n    Count the frequency of words across multiple dictionary files in a given directory \n    and export the counts to a CSV file.\n\n    Parameters:\n    filename (str): The name of the output CSV file.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - collections.Counter\n    - os\n    - csv\n\n    Example:\n    >>> export_word_counts_to_csv('word_counts.csv')\n    \"\"\"\n    word_counts = Counter()\n\n    for file_name in os.listdir(FILE_DIR):\n        if not file_name.endswith('.txt'):\n            continue\n        with open(os.path.join(FILE_DIR, file_name), 'r') as file:\n            words = file.read().split()\n            word_counts.update(words)\n\n    with open(filename, 'w') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Word', 'Count'])\n        writer.writerows(word_counts.items())\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, collections, os, json\n```python\nimport pandas as pd\nfrom collections import Counter\nimport os\nimport json\n\n# Constants\nFILE_DIR = './yourdictfiles/'\n\ndef export_word_counts_to_json(filename):\n    \"\"\"\n    Count the frequency of words across multiple dictionary files in a given directory \n    and export the counts to a JSON file.\n\n    Parameters:\n    filename (str): The name of the output JSON file.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - collections.Counter\n    - os\n    - json\n\n    Example:\n    >>> export_word_counts_to_json('word_counts.json')\n    \"\"\"\n    word_counts = Counter()\n\n    for file_name in os.listdir(FILE_DIR):\n        if not file_name.endswith('.txt'):\n            continue\n        with open(os.path.join(FILE_DIR, file_name), 'r') as file:\n            words = file.read().split()\n            word_counts.update(words)\n\n    with open(filename, 'w') as file:\n        json.dump(dict(word_counts), file)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, collections, os, nltk\n```python\nimport pandas as pd\nfrom collections import Counter\nimport os\nfrom nltk.corpus import stopwords\n\n# Constants\nFILE_DIR = './yourdictfiles/'\nSTOPWORDS = set(stopwords.words('english'))\n\ndef count_non_stopwords_in_files():\n    \"\"\"\n    Count the number of unique non-stop words across multiple dictionary files in a given directory.\n\n    Returns:\n    int: The number of unique non-stop words.\n\n    Requirements:\n    - pandas\n    - collections.Counter\n    - os\n    - nltk.corpus.stopwords\n\n    Example:\n    >>> count_non_stopwords_in_files()\n    \"\"\"\n    word_counts = Counter()\n\n    for file_name in os.listdir(FILE_DIR):\n        if not file_name.endswith('.txt'):\n            continue\n        with open(os.path.join(FILE_DIR, file_name), 'r') as file:\n            words = [word for word in file.read().split() if word not in STOPWORDS]\n            word_counts.update(words)\n\n    return len(word_counts)\n```"}
{"seed": "{\"task_id\": 20067636, \"prompt\": \"def f_20067636(df):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"df.groupby('id').first()\", \"test_start\": \"\\nimport pandas as pd \\n\\ndef check(candidate): \", \"test\": [\"\\n    df = pd.DataFrame({\\n        'id': [1, 1, 1, 2, 2, 3, 3, 3, 3, 4, 4, 5, 6, 6, 6, 7, 7], \\n        'value': ['first', 'second', 'second', 'first', 'second', 'first', 'third', 'fourth', 'fifth', 'second', 'fifth', 'first', 'first', 'second', 'third', 'fourth', 'fifth']\\n    })\\n    assert candidate(df).to_dict() == {'value': {1: 'first', 2: 'first', 3: 'first', 4: 'second', 5: 'first', 6: 'first', 7: 'fourth'}}\\n\"], \"entry_point\": \"f_20067636\", \"intent\": \"pandas dataframe `df` get first row of each group by 'id'\", \"library\": [\"pandas\"]}", "generation": "Scenario 1: \npandas, numpy, matplotlib.pyplot, sklearn.preprocessing\n\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Sample Data\n# Assume df is dataframe which has columns ['id', 'age', 'income']\n\ndef scale_and_plot(df):\n    \"\"\"\n    Scale the 'age' and 'income' columns between 0 and 1 for each group by 'id' in pandas \n    dataframe df and plot a histogram of the 'income' column after scaling.\n    \n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n    \n    Returns:\n    DataFrame: The pandas DataFrame after scaling 'age' and 'income' columns.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.preprocessing\n    \n    Example:\n    >>> df = pd.DataFrame({\n    ...     'id': [1, 1, 2, 2, 3, 3],\n    ...     'age': [25, 26, 35, 36, 28, 29],\n    ...     'income': [50000, 60000, 70000, 80000, 90000, 100000]\n    ... })\n    >>> df_scaled = scale_and_plot(df)\n    >>> print(df_scaled)\n    >>> plt.hist(df_scaled['income'], bins=10)\n    \"\"\"\n    scaler = MinMaxScaler()\n\n    df_grouped = df.groupby('id').apply(lambda x: pd.DataFrame(scaler.fit_transform(x[['age', 'income']]), columns=['age', 'income'], index=x.index))\n\n    return df_grouped\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \npandas, datetime, matplotlib.pyplot\n\n```python\nimport pandas as pd\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\n\n# Sample Data\n# Assume df is dataframe which has columns ['id', 'timestamp', 'value']\n\ndef plot_value_over_time(df, id):\n    \"\"\"\n    Plot the 'value' over time for a given 'id' in pandas dataframe df.\n    \n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n    id (int): The id for which the plot is to be created.\n    \n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - datetime\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = pd.DataFrame({\n    ...     'id': [1, 1, 2, 2, 3, 3],\n    ...     'timestamp': ['2021-01-01', '2021-02-01', '2021-01-01', '2021-02-01', '2021-01-01', '2021-02-01'],\n    ...     'value': [50, 60, 70, 80, 90, 100]\n    ... })\n    >>> df['timestamp'] = pd.to_datetime(df['timestamp'])\n    >>> plot_value_over_time(df, 1)\n    \"\"\"\n    df_sub = df[df['id'] == id]\n    plt.plot(df_sub['timestamp'], df_sub['value'])\n    plt.xlabel('Time')\n    plt.ylabel('Value')\n    plt.title(f'Value over time for id {id}')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \npandas, numpy, datetime, matplotlib.pyplot\n\n```python\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\n\n# Sample Data\n# Assume df is dataframe which has columns ['id', 'timestamp', 'value']\n\ndef plot_cumulative_value(df, id):\n    \"\"\"\n    Plot the cumulative 'value' over time for a given 'id' in pandas dataframe df.\n    \n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n    id (int): The id for which the plot is to be created.\n    \n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - numpy\n    - datetime\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = pd.DataFrame({\n    ...     'id': [1, 1, 2, 2, 3, 3],\n    ...     'timestamp': ['2021-01-01', '2021-02-01', '2021-01-01', '2021-02-01', '2021-01-01', '2021-02-01'],\n    ...     'value': [50, 60, 70, 80, 90, 100]\n    ... })\n    >>> df['timestamp'] = pd.to_datetime(df['timestamp'])\n    >>> plot_cumulative_value(df, 1)\n    \"\"\"\n    df_sub = df[df['id'] == id]\n    df_sub['cumulative_value'] = np.cumsum(df_sub['value'])\n    plt.plot(df_sub['timestamp'], df_sub['cumulative_value'])\n    plt.xlabel('Time')\n    plt.ylabel('Cumulative Value')\n    plt.title(f'Cumulative Value over time for id {id}')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \npandas, numpy, matplotlib.pyplot, sklearn.preprocessing\n\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\n# Sample Data\n# Assume df is dataframe which has columns ['id', 'age', 'income']\n\ndef standardize_and_plot(df):\n    \"\"\"\n    Standardize the 'age' and 'income' columns for each group by 'id' in pandas \n    dataframe df and plot a scatter plot of the 'age' and 'income' after standardization.\n    \n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n    \n    Returns:\n    DataFrame: The pandas DataFrame after standardizing 'age' and 'income' columns.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.preprocessing\n    \n    Example:\n    >>> df = pd.DataFrame({\n    ...     'id': [1, 1, 2, 2, 3, 3],\n    ...     'age': [25, 26, 35, 36, 28, 29],\n    ...     'income': [50000, 60000, 70000, 80000, 90000, 100000]\n    ... })\n    >>> df_standardized = standardize_and_plot(df)\n    >>> print(df_standardized)\n    >>> plt.scatter(df_standardized['age'], df_standardized['income'])\n    \"\"\"\n    scaler = StandardScaler()\n\n    df_grouped = df.groupby('id').apply(lambda x: pd.DataFrame(scaler.fit_transform(x[['age', 'income']]), columns=['age', 'income'], index=x.index))\n\n    return df_grouped\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \npandas, matplotlib.pyplot\n\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Sample Data\n# Assume df is dataframe which has columns ['id', 'value']\n\ndef plot_value_distribution(df):\n    \"\"\"\n    Plot a bar plot of the value counts of the 'value' column in pandas dataframe df.\n    \n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n    \n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = pd.DataFrame({\n    ...     'id': [1, 1, 2, 2, 3, 3],\n    ...     'value': ['A', 'B', 'A', 'B', 'A', 'B']\n    ... })\n    >>> plot_value_distribution(df)\n    \"\"\"\n    value_counts = df['value'].value_counts()\n    plt.bar(value_counts.index, value_counts.values)\n    plt.xlabel('Value')\n    plt.ylabel('Count')\n    plt.title('Value Distribution')\n    plt.show()\n```\n"}
{"seed": "{\"task_id\": 40924332, \"prompt\": \"def f_40924332(df):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"pd.concat([df[0].apply(pd.Series), df[1]], axis=1)\", \"test_start\": \"\\nimport numpy as np\\nimport pandas as pd \\n\\ndef check(callerFunction):\", \"test\": [\"\\n    assert callerFunction(pd.DataFrame([[[8, 10, 12], 'A'], [[7, 9, 11], 'B']])).equals(pd.DataFrame([[8,10,12,'A'], [7,9,11,'B']], columns=[0,1,2,1]))\\n\", \"\\n    assert callerFunction(pd.DataFrame([[[8, 10, 12], 'A'], [[7, 11], 'B']])).equals(pd.DataFrame([[8.0,10.0,12.0,'A'], [7.0,11.0,np.nan,'B']], columns=[0,1,2,1]))\\n\", \"\\n    assert callerFunction(pd.DataFrame([[[8, 10, 12]], [[7, 9, 11], 'B']])).equals(pd.DataFrame([[8,10,12,None], [7,9,11,'B']], columns=[0,1,2,1]))\\n\"], \"entry_point\": \"f_40924332\", \"intent\": \"split a list in first column into multiple columns keeping other columns as well in pandas data frame `df`\", \"library\": [\"numpy\", \"pandas\"]}", "generation": "Scenario 1:\npandas, numpy, datetime, matplotlib, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nCOLUMNS = ['Date', 'Value']\n\n# Sample data\n# df = pd.DataFrame([['2021-01-01', [8, 10, 12]], ['2021-01-02', [7, 9, 11]]],\n#                   columns=COLUMNS)\n\ndef analyze_and_plot(df):\n    \"\"\"\n    This function splits a list in the 'Value' column into multiple columns, calculates \n    the mean of these columns, and plots a line graph of the mean values against the dates.\n\n    Parameters:\n    df (DataFrame): A pandas DataFrame with a 'Date' column and a 'Value' column \n                    where 'Value' contains lists of numbers.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the mean values.\n\n    Requirements:\n    - pandas\n    - numpy\n    - datetime\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> df = pd.DataFrame([['2021-01-01', [8, 10, 12]], ['2021-01-02', [7, 9, 11]]],\n                          columns=COLUMNS)\n    >>> mean_df = analyze_and_plot(df)\n    >>> print(mean_df)\n    \"\"\"\n    df['Date'] = pd.to_datetime(df['Date'])\n    df = pd.concat([df['Date'], df['Value'].apply(pd.Series)], axis=1)\n    \n    df['Mean'] = df.iloc[:,1:].mean(axis=1)\n\n    plt.figure(figsize=(10,6))\n    sns.lineplot(x='Date', y='Mean', data=df)\n    plt.title('Mean Value Over Time')\n    plt.xlabel('Date')\n    plt.ylabel('Mean Value')\n    plt.show()\n    \n    return df[['Date', 'Mean']]\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, sklearn.preprocessing, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['Date', 'Value']\n\n# Sample data\n# df = pd.DataFrame([['2021-01-01', [8, 10, 12]], ['2021-01-02', [7, 9, 11]]],\n#                   columns=COLUMNS)\n\ndef preprocess_and_visualize(df):\n    \"\"\"\n    This function splits a list in the 'Value' column into multiple columns, scales \n    these columns using StandardScaler, and visualizes the scaled data using a bar plot.\n\n    Parameters:\n    df (DataFrame): A pandas DataFrame with a 'Date' column and a 'Value' column \n                    where 'Value' contains lists of numbers.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the scaled data.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.preprocessing\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame([['2021-01-01', [8, 10, 12]], ['2021-01-02', [7, 9, 11]]],\n                          columns=COLUMNS)\n    >>> scaled_df = preprocess_and_visualize(df)\n    >>> print(scaled_df)\n    \"\"\"\n    df['Date'] = pd.to_datetime(df['Date'])\n    df = pd.concat([df['Date'], df['Value'].apply(pd.Series)], axis=1)\n    \n    scaler = StandardScaler()\n    df.iloc[:,1:] = scaler.fit_transform(df.iloc[:,1:])\n    \n    df.set_index('Date').plot(kind='bar', stacked=True)\n    plt.title('Scaled Values Over Time')\n    plt.xlabel('Date')\n    plt.ylabel('Scaled Value')\n    plt.show()\n    \n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, scipy.stats, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import zscore\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['Date', 'Value']\n\n# Sample data\n# df = pd.DataFrame([['2021-01-01', [8, 10, 12]], ['2021-01-02', [7, 9, 11]]],\n#                   columns=COLUMNS)\n\ndef detect_outliers(df):\n    \"\"\"\n    This function splits a list in the 'Value' column into multiple columns, calculates \n    the Z-scores of these columns, and visualizes the data using a box plot to detect outliers.\n\n    Parameters:\n    df (DataFrame): A pandas DataFrame with a 'Date' column and a 'Value' column \n                    where 'Value' contains lists of numbers.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the Z-scores.\n\n    Requirements:\n    - pandas\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame([['2021-01-01', [8, 10, 12]], ['2021-01-02', [7, 9, 11]]],\n                          columns=COLUMNS)\n    >>> zscore_df = detect_outliers(df)\n    >>> print(zscore_df)\n    \"\"\"\n    df['Date'] = pd.to_datetime(df['Date'])\n    df = pd.concat([df['Date'], df['Value'].apply(pd.Series)], axis=1)\n    \n    df.iloc[:,1:] = df.iloc[:,1:].apply(zscore)\n    \n    df.set_index('Date').boxplot()\n    plt.title('Z-Scores Over Time')\n    plt.xlabel('Date')\n    plt.ylabel('Z-Score')\n    plt.show()\n    \n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, scipy.stats, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import pearsonr\nimport seaborn as sns\n\n# Constants\nCOLUMNS = ['Date', 'Value']\n\n# Sample data\n# df = pd.DataFrame([['2021-01-01', [8, 10, 12]], ['2021-01-02', [7, 9, 11]]],\n#                   columns=COLUMNS)\n\ndef analyze_correlation(df):\n    \"\"\"\n    This function splits a list in the 'Value' column into multiple columns, calculates \n    the Pearson correlation coefficient between these columns, and visualizes the correlation \n    matrix using a heatmap.\n\n    Parameters:\n    df (DataFrame): A pandas DataFrame with a 'Date' column and a 'Value' column \n                    where 'Value' contains lists of numbers.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the correlation coefficients.\n\n    Requirements:\n    - pandas\n    - numpy\n    - scipy.stats\n    - seaborn\n\n    Example:\n    >>> df = pd.DataFrame([['2021-01-01', [8, 10, 12]], ['2021-01-02', [7, 9, 11]]],\n                          columns=COLUMNS)\n    >>> corr_df = analyze_correlation(df)\n    >>> print(corr_df)\n    \"\"\"\n    df['Date'] = pd.to_datetime(df['Date'])\n    df = pd.concat([df['Date'], df['Value'].apply(pd.Series)], axis=1)\n    \n    corr_df = df.iloc[:,1:].corr(method=pearsonr)\n    \n    sns.heatmap(corr_df, annot=True, cmap='coolwarm')\n    plt.title('Correlation Heatmap')\n    plt.show()\n    \n    return corr_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, sklearn.decomposition, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['Date', 'Value']\n\n# Sample data\n# df = pd.DataFrame([['2021-01-01', [8, 10, 12]], ['2021-01-02', [7, 9, 11]]],\n#                   columns=COLUMNS)\n\ndef perform_pca(df):\n    \"\"\"\n    This function splits a list in the 'Value' column into multiple columns, performs \n    Principal Component Analysis (PCA) on these columns, and visualizes the explained \n    variance ratio of the principal components using a bar plot.\n\n    Parameters:\n    df (DataFrame): A pandas DataFrame with a 'Date' column and a 'Value' column \n                    where 'Value' contains lists of numbers.\n\n    Returns:\n    ndarray: The explained variance ratio of the principal components.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.decomposition\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame([['2021-01-01', [8, 10, 12]], ['2021-01-02', [7, 9, 11]]],\n                          columns=COLUMNS)\n    >>> explained_variance_ratio = perform_pca(df)\n    >>> print(explained_variance_ratio)\n    \"\"\"\n    df['Date'] = pd.to_datetime(df['Date'])\n    df = pd.concat([df['Date'], df['Value'].apply(pd.Series)], axis=1)\n    \n    pca = PCA()\n    pca.fit(df.iloc[:,1:])\n    \n    explained_variance_ratio = pca.explained_variance_ratio_\n    \n    plt.bar(range(len(explained_variance_ratio)), explained_variance_ratio)\n    plt.title('Explained Variance Ratio of Principal Components')\n    plt.xlabel('Principal Component')\n    plt.ylabel('Explained Variance Ratio')\n    plt.show()\n    \n    return explained_variance_ratio\n```\n"}
{"seed": "{\"task_id\": 30759776, \"prompt\": \"def f_30759776(data):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"re.findall('src=\\\"js/([^\\\"]*\\\\\\\\bjquery\\\\\\\\b[^\\\"]*)\\\"', data)\", \"test_start\": \"\\nimport re \\n\\ndef check(candidate):\", \"test\": [\"\\n    data = '<script type=\\\"text/javascript\\\" src=\\\"js/jquery-1.9.1.min.js\\\"/><script type=\\\"text/javascript\\\" src=\\\"js/jquery-migrate-1.2.1.min.js\\\"/><script type=\\\"text/javascript\\\" src=\\\"js/jquery-ui.min.js\\\"/><script type=\\\"text/javascript\\\" src=\\\"js/abc_bsub.js\\\"/><script type=\\\"text/javascript\\\" src=\\\"js/abc_core.js\\\"/>            <script type=\\\"text/javascript\\\" src=\\\"js/abc_explore.js\\\"/><script type=\\\"text/javascript\\\" src=\\\"js/abc_qaa.js\\\"/>'\\n    assert candidate(data) == ['jquery-1.9.1.min.js', 'jquery-migrate-1.2.1.min.js', 'jquery-ui.min.js']\\n\"], \"entry_point\": \"f_30759776\", \"intent\": \"extract attributes 'src=\\\"js/([^\\\"]*\\\\\\\\bjquery\\\\\\\\b[^\\\"]*)\\\"' from string `data`\", \"library\": [\"re\"]}", "generation": "Scenario 1: \nre, urllib.request, bs4, os\n```python\nimport re\nimport urllib.request\nfrom bs4 import BeautifulSoup\nimport os\n\ndef download_jquery_files(url):\n    \"\"\"\n    Download all jQuery files from a given webpage.\n\n    Parameters:\n    url (str): The url of the webpage.\n\n    Returns:\n    list: A list of filenames of the downloaded jQuery files.\n\n    Requirements:\n    - re\n    - urllib.request\n    - BeautifulSoup\n    - os\n\n    Example:\n    >>> download_jquery_files('http://example.com')\n    \"\"\"\n    # Get page content\n    with urllib.request.urlopen(url) as response:\n        html = response.read()\n\n    # Parse the html content\n    soup = BeautifulSoup(html, 'html.parser')\n    \n    # Find all script tags\n    script_tags = soup.find_all('script')\n\n    # Extract jQuery files\n    jquery_files = []\n    for tag in script_tags:\n        src = tag.get('src')\n        if src and 'jquery' in src:\n            jquery_files.append(src)\n\n    # Download jQuery files\n    downloaded_files = []\n    for file in jquery_files:\n        filename = os.path.basename(file)\n        urllib.request.urlretrieve(file, filename)\n        downloaded_files.append(filename)\n\n    return downloaded_files\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nre, pandas, matplotlib.pyplot, seaborn\n```python\nimport re\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nJS_FILE_PATTERNS = ['jquery', 'react', 'vue', 'angular']\n\ndef visualize_js_usage(html_data):\n    \"\"\"\n    Visualize the usage of different JavaScript libraries in a given html string.\n\n    Parameters:\n    html_data (str): The html string.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the usage of JavaScript libraries.\n\n    Requirements:\n    - re\n    - pandas\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> html_data = '<script src=\"js/jquery.min.js\"></script><script src=\"js/react.min.js\"></script><script src=\"js/vue.min.js\"></script>'\n    >>> visualize_js_usage(html_data)\n    \"\"\"\n    # Extract js files\n    js_files = re.findall('src=\"js/([^.]*).js\"', html_data)\n\n    # Count the usage of different js libraries\n    usage = {pattern: 0 for pattern in JS_FILE_PATTERNS}\n    for file in js_files:\n        for pattern in JS_FILE_PATTERNS:\n            if pattern in file:\n                usage[pattern] += 1\n\n    # Convert the usage to DataFrame\n    usage_df = pd.DataFrame(list(usage.items()), columns=['Library', 'Usage'])\n\n    # Visualize the usage\n    plt.figure(figsize=(10, 6))\n    sns.barplot(x='Library', y='Usage', data=usage_df)\n    plt.title('Usage of JavaScript Libraries')\n    plt.show()\n\n    return usage_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nre, os, shutil, sys\n```python\nimport re\nimport os\nimport shutil\nimport sys\n\ndef remove_jquery_files(directory):\n    \"\"\"\n    Remove all jQuery files from a given directory.\n\n    Parameters:\n    directory (str): The directory path.\n\n    Returns:\n    int: The number of files removed.\n\n    Requirements:\n    - re\n    - os\n    - shutil\n    - sys\n\n    Example:\n    >>> remove_jquery_files('/path/to/directory')\n    \"\"\"\n    # Check if directory exists\n    if not os.path.exists(directory):\n        print(f\"Directory '{directory}' does not exist.\")\n        sys.exit()\n\n    # Get all files in the directory\n    files = os.listdir(directory)\n\n    # Remove jQuery files\n    removed_files = 0\n    for file in files:\n        if 'jquery' in file and file.endswith('.js'):\n            os.remove(os.path.join(directory, file))\n            removed_files += 1\n\n    return removed_files\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nre, urllib.request, urllib.parse, tldextract\n```python\nimport re\nimport urllib.request\nimport urllib.parse\nfrom tldextract import extract\n\ndef find_external_jquery_links(html_data):\n    \"\"\"\n    Find all external jQuery links in a given html string.\n\n    Parameters:\n    html_data (str): The html string.\n\n    Returns:\n    list: A list of external jQuery links.\n\n    Requirements:\n    - re\n    - urllib.request\n    - urllib.parse\n    - tldextract\n\n    Example:\n    >>> html_data = '<script src=\"https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js\"></script><script src=\"js/react.min.js\"></script>'\n    >>> find_external_jquery_links(html_data)\n    \"\"\"\n    # Extract all links\n    links = re.findall('src=\"(https?://[^\"]*)\"', html_data)\n\n    # Filter jQuery links\n    jquery_links = [link for link in links if 'jquery' in link]\n\n    return jquery_links\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nre, requests, bs4, time\n```python\nimport re\nimport requests\nfrom bs4 import BeautifulSoup\nimport time\n\ndef check_jquery_update(url):\n    \"\"\"\n    Check if a webpage's jQuery version is up to date.\n\n    Parameters:\n    url (str): The url of the webpage.\n\n    Returns:\n    str: The jQuery version of the webpage.\n\n    Requirements:\n    - re\n    - requests\n    - BeautifulSoup\n    - time\n\n    Example:\n    >>> check_jquery_update('http://example.com')\n    \"\"\"\n    # Get page content\n    response = requests.get(url)\n    html = response.text\n\n    # Parse the html content\n    soup = BeautifulSoup(html, 'html.parser')\n    \n    # Find all script tags\n    script_tags = soup.find_all('script')\n\n    # Extract jQuery version\n    jquery_version = None\n    for tag in script_tags:\n        src = tag.get('src')\n        if src and 'jquery' in src:\n            jquery_version = re.findall('jquery-([0-9.]*).min.js', src)[0]\n            break\n\n    # Check if the version is up to date\n    current_version = requests.get('https://code.jquery.com/').text\n    current_version = re.findall('jquery-([0-9.]*).min.js', current_version)[0]\n\n    if jquery_version != current_version:\n        print(f\"The webpage's jQuery version ({jquery_version}) is not up to date. The current version is {current_version}.\")\n\n    return jquery_version\n```"}
{"seed": "{\"task_id\": 25388796, \"prompt\": \"def f_25388796():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"sum(int(float(item)) for item in [_f for _f in ['', '3.4', '', '', '1.0'] if _f])\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate() == 4\\n\"], \"entry_point\": \"f_25388796\", \"intent\": \"Sum integers contained in strings in list `['', '3.4', '', '', '1.0']`\", \"library\": []}", "generation": "Scenario 1:\npandas, numpy, statistics, random\n```python\nimport pandas as pd\nimport numpy as np\nfrom statistics import mean\nfrom random import randint\n\n# Constants\nFIELDS = ['Physics', 'Math', 'Chemistry', 'Biology', 'English', 'History']\nSTUDENTS = ['Student_' + str(i) for i in range(1, 101)]\n\ndef generate_students_report():\n    \"\"\"\n    Generate a report of students' grades in different subjects. Then, calculate \n    the average grade for each student and each subject.\n\n    Returns:\n    DataFrame: A pandas DataFrame with students' grades and averages.\n\n    Requirements:\n    - pandas\n    - numpy\n    - statistics\n    - random\n\n    Example:\n    >>> report = generate_students_report()\n    >>> print(report)\n    \"\"\"\n    report_data = {field: [randint(0, 100) for _ in STUDENTS] for field in FIELDS}\n\n    df = pd.DataFrame(report_data, index=STUDENTS)\n    \n    df['Average Grade'] = df.apply(mean, axis=1)\n    df.loc['Average'] = df.apply(mean)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, csv, random, statistics\n```python\nimport os\nimport csv\nfrom random import randint\nfrom statistics import mean\n\n# Constants\nCOLUMNS = ['Name', 'Age', 'Height', 'Weight']\nPEOPLE_COUNT = 100\n\ndef generate_people_report(filename):\n    \"\"\"\n    Generate a CSV report of people's Name, Age, Height, and Weight. Then, calculate \n    the average Age, Height, and Weight.\n\n    Parameters:\n    filename (str): The name of the CSV file to be written.\n\n    Returns:\n    str: The path of the written CSV file.\n\n    Requirements:\n    - os\n    - csv\n    - random\n    - statistics\n\n    Example:\n    >>> filename = 'people_report.csv'\n    >>> generate_people_report(filename)\n    >>> # Check the generated CSV file\n    \"\"\"\n    filepath = os.path.join(os.getcwd(), filename)\n    with open(filepath, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(COLUMNS)\n\n        data = [\n            ['Person_' + str(i), randint(20, 50), randint(150, 200), randint(50, 100)] \n            for i in range(1, PEOPLE_COUNT+1)\n        ]\n        writer.writerows(data)\n\n        averages = ['Average', mean([row[1] for row in data]), \n                    mean([row[2] for row in data]), mean([row[3] for row in data])]\n        writer.writerow(averages)\n\n    return filepath\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nrandom, numpy, matplotlib.pyplot\n```python\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nDISTRIBUTION_SIZE = 1000\n\ndef plot_random_distribution():\n    \"\"\"\n    Generate a distribution of random numbers and plot its histogram.\n\n    Requirements:\n    - random\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_random_distribution()\n    \"\"\"\n    distribution = [random.gauss(0, 1) for _ in range(DISTRIBUTION_SIZE)]\n    plt.hist(distribution, bins=30, edgecolor='black')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, random, matplotlib.pyplot\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nCATEGORIES = ['A', 'B', 'C', 'D', 'E']\n\ndef generate_category_distribution():\n    \"\"\"\n    Generate a distribution of categories and plot its pie chart.\n\n    Returns:\n    DataFrame: A pandas DataFrame with category distribution.\n\n    Requirements:\n    - pandas\n    - numpy\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = generate_category_distribution()\n    >>> print(df)\n    >>> df.plot(kind='pie', y='Count', autopct='%1.1f%%')\n    \"\"\"\n    distribution = {category: randint(0, 100) for category in CATEGORIES}\n    df = pd.DataFrame(list(distribution.items()), columns=['Category', 'Count'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nrandom, math, numpy, matplotlib.pyplot\n```python\nimport random\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nPOINTS_COUNT = 1000\nRADIUS = 1\n\ndef plot_random_points_in_circle():\n    \"\"\"\n    Generate random points inside a circle and plot them.\n\n    Requirements:\n    - random\n    - math\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_random_points_in_circle()\n    \"\"\"\n    points = [(RADIUS * math.sqrt(random.random()) * math.cos(2 * math.pi * random.random()), \n               RADIUS * math.sqrt(random.random()) * math.sin(2 * math.pi * random.random())) \n              for _ in range(POINTS_COUNT)]\n\n    plt.scatter(*zip(*points), s=1)\n    plt.gca().set_aspect('equal', adjustable='box')\n    plt.show()\n```"}
{"seed": "{\"task_id\": 804995, \"prompt\": \"def f_804995():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"subprocess.Popen(['c:\\\\\\\\Program Files\\\\\\\\VMware\\\\\\\\VMware Server\\\\\\\\vmware-cmd.bat'])\", \"test_start\": \"\\nimport subprocess\\nfrom unittest.mock import Mock\\n\\ndef check(candidate):\", \"test\": [\"\\n    subprocess.Popen = Mock(return_value = 0)\\n    assert candidate() == 0\\n\"], \"entry_point\": \"f_804995\", \"intent\": \"Call a subprocess with arguments `c:\\\\\\\\Program Files\\\\\\\\VMware\\\\\\\\VMware Server\\\\\\\\vmware-cmd.bat` that may contain spaces\", \"library\": [\"subprocess\"]}", "generation": "Scenario 1:\nsubprocess, os, time, random\n```python\nimport subprocess\nimport os\nimport time\nimport random\n\n# Constants\nDIRECTORY = 'c:\\\\\\\\Program Files\\\\\\\\VMware\\\\\\\\VMware Server\\\\'\nFILES = [\"file1.bat\", \"file2.bat\", \"file3.bat\", \"file4.bat\"]\n\ndef execute_random_file():\n    \"\"\"\n    Choose a random file from a list of files in a specific directory and execute it as a subprocess.\n    \n    Returns:\n    int: The exit code of the subprocess.\n\n    Requirements:\n    - subprocess\n    - os\n    - time\n    - random\n\n    Example:\n    >>> execute_random_file()\n    \"\"\"\n    file = random.choice(FILES)\n    file_path = os.path.join(DIRECTORY, file)\n    process = subprocess.Popen(file_path)\n    time.sleep(1)  # wait for the process to start\n    return process.poll()  # return the exit code\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nsubprocess, os, shutil, sys\n```python\nimport subprocess\nimport os\nimport shutil\nimport sys\n\n# Constants\nDIRECTORY = 'c:\\\\\\\\Program Files\\\\\\\\VMware\\\\\\\\VMware Server\\\\'\nBACKUP_DIRECTORY = 'c:\\\\\\\\Program Files\\\\\\\\VMware\\\\\\\\VMware Server\\\\\\\\Backup\\\\'\n\ndef backup_and_execute(filename):\n    \"\"\"\n    Backup a specific file in a directory and execute it as a subprocess.\n    \n    Parameters:\n    filename (str): The name of the file to be backed up and executed.\n\n    Returns:\n    int: The exit code of the subprocess.\n\n    Requirements:\n    - subprocess\n    - os\n    - shutil\n    - sys\n\n    Example:\n    >>> backup_and_execute('vmware-cmd.bat')\n    \"\"\"\n    file_path = os.path.join(DIRECTORY, filename)\n    backup_path = os.path.join(BACKUP_DIRECTORY, filename)\n\n    # Backup the file\n    try:\n        shutil.copy(file_path, backup_path)\n    except Exception as e:\n        print(f\"Failed to backup the file: {e}\", file=sys.stderr)\n        return -1\n\n    # Execute the file as a subprocess\n    process = subprocess.Popen(file_path)\n    return process.poll()  # return the exit code\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nsubprocess, os, time, threading\n```python\nimport subprocess\nimport os\nimport time\nimport threading\n\n# Constants\nDIRECTORY = 'c:\\\\\\\\Program Files\\\\\\\\VMware\\\\\\\\VMware Server\\\\'\nFILES = [\"file1.bat\", \"file2.bat\", \"file3.bat\", \"file4.bat\"]\n\ndef execute_all_files():\n    \"\"\"\n    Execute all files in a directory as subprocesses concurrently.\n    \n    Returns:\n    list: The exit codes of the subprocesses.\n\n    Requirements:\n    - subprocess\n    - os\n    - time\n    - threading\n\n    Example:\n    >>> execute_all_files()\n    \"\"\"\n    exit_codes = []\n\n    def execute_file(file):\n        file_path = os.path.join(DIRECTORY, file)\n        process = subprocess.Popen(file_path)\n        time.sleep(1)  # wait for the process to start\n        exit_codes.append(process.poll())  # store the exit code\n\n    # Start a thread for each file\n    threads = [threading.Thread(target=execute_file, args=(file,)) for file in FILES]\n    for thread in threads:\n        thread.start()\n\n    # Wait for all threads to finish\n    for thread in threads:\n        thread.join()\n\n    return exit_codes\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nsubprocess, os, sys, glob\n```python\nimport subprocess\nimport os\nimport sys\nimport glob\n\n# Constants\nDIRECTORY = 'c:\\\\\\\\Program Files\\\\\\\\VMware\\\\\\\\VMware Server\\\\'\n\ndef execute_all_bat_files():\n    \"\"\"\n    Find all .bat files in a directory and execute them as subprocesses.\n    \n    Returns:\n    list: The exit codes of the subprocesses.\n\n    Requirements:\n    - subprocess\n    - os\n    - sys\n    - glob\n\n    Example:\n    >>> execute_all_bat_files()\n    \"\"\"\n    exit_codes = []\n    file_paths = glob.glob(os.path.join(DIRECTORY, '*.bat'))\n\n    for file_path in file_paths:\n        process = subprocess.Popen(file_path)\n        exit_code = process.poll()\n\n        if exit_code is not None:\n            exit_codes.append(exit_code)\n        else:\n            print(f\"Failed to execute the file: {file_path}\", file=sys.stderr)\n\n    return exit_codes\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nsubprocess, os, time, sys, signal\n```python\nimport subprocess\nimport os\nimport time\nimport sys\nimport signal\n\n# Constants\nDIRECTORY = 'c:\\\\\\\\Program Files\\\\\\\\VMware\\\\\\\\VMware Server\\\\'\n\ndef execute_and_terminate(filename, timeout):\n    \"\"\"\n    Execute a specific file as a subprocess and terminate it after a given timeout.\n    \n    Parameters:\n    filename (str): The name of the file to be executed.\n    timeout (int): The time in seconds after which the subprocess should be terminated.\n\n    Returns:\n    int: The exit code of the subprocess.\n\n    Requirements:\n    - subprocess\n    - os\n    - time\n    - sys\n    - signal\n\n    Example:\n    >>> execute_and_terminate('vmware-cmd.bat', 5)\n    \"\"\"\n    file_path = os.path.join(DIRECTORY, filename)\n    process = subprocess.Popen(file_path)\n\n    try:\n        time.sleep(timeout)\n        process.terminate()\n        return process.poll()  # return the exit code\n    except Exception as e:\n        print(f\"Failed to terminate the process: {e}\", file=sys.stderr)\n        return -1\n```\n"}
{"seed": "{\"task_id\": 26441253, \"prompt\": \"def f_26441253(q):\\n\\t\", \"suffix\": \"\\n\\treturn q\", \"canonical_solution\": \"for n in [1,3,4,2]: q.put((-n, n))\", \"test_start\": \"\\nfrom queue import PriorityQueue\\n\\ndef check(candidate):\", \"test\": [\"\\n    q = PriorityQueue()\\n    q = candidate(q)\\n    expected = [4, 3, 2, 1]\\n    for i in range(0, len(expected)):\\n        assert q.get()[1] == expected[i]\\n\"], \"entry_point\": \"f_26441253\", \"intent\": \"reverse a priority queue `q` in python without using classes\", \"library\": [\"queue\"]}", "generation": "Scenario 1: \ncollections, itertools, queue, random\n```python\nimport collections\nimport itertools\nfrom queue import PriorityQueue\nfrom random import shuffle\n\n# Constants\nTEAMS = ['Team 1', 'Team 2', 'Team 3', 'Team 4', 'Team 5']\nPOINTS = [10, 20, 30, 40, 50]\n\ndef sports_ranking():\n    \"\"\"\n    Generate a random sports ranking and sort it using a priority queue. \n\n    Requirements:\n    - collections\n    - itertools\n    - queue\n    - random\n\n    Example:\n    >>> ranking = sports_ranking()\n    >>> print(ranking)\n    \"\"\"\n    shuffled_points = POINTS.copy()\n    shuffle(shuffled_points)\n    ranking = dict(zip(TEAMS, shuffled_points))\n\n    sorted_ranking = PriorityQueue()\n    for team, points in ranking.items():\n        sorted_ranking.put((-points, team))\n\n    sorted_ranking_dict = collections.OrderedDict()\n    while not sorted_ranking.empty():\n        points, team = sorted_ranking.get()\n        sorted_ranking_dict[team] = -points\n\n    return sorted_ranking_dict\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nheapq, random, queue\n```python\nimport heapq\nfrom random import randint\nfrom queue import PriorityQueue\n\n# Constants\nNUMBERS = [randint(1, 100) for _ in range(20)]\n\ndef find_k_largest_numbers(k):\n    \"\"\"\n    Find the k largest numbers in a list using heapq and priority queue.\n\n    Parameters:\n    k (int): The number of largest elements to find.\n\n    Returns:\n    list: The list of k largest numbers.\n\n    Requirements:\n    - heapq\n    - random\n    - queue\n\n    Example:\n    >>> find_k_largest_numbers(5)\n    \"\"\"\n    pq = PriorityQueue()\n    for num in NUMBERS:\n        if len(pq.queue) < k:\n            pq.put(num)\n        else:\n            if num > pq.queue[0]:\n                heapq.heapreplace(pq.queue, num)\n\n    return sorted([pq.get() for _ in range(k)], reverse=True)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nrandom, queue, bisect\n```python\nimport random\nfrom queue import PriorityQueue\nimport bisect\n\n# Constants\nNUMBERS = [random.randint(1, 100) for _ in range(20)]\n\ndef insert_into_sorted_priority_queue(num):\n    \"\"\"\n    Insert a number into a sorted priority queue.\n\n    Parameters:\n    num (int): The number to insert.\n\n    Returns:\n    list: The sorted list after insertion.\n\n    Requirements:\n    - random\n    - queue\n    - bisect\n\n    Example:\n    >>> insert_into_sorted_priority_queue(50)\n    \"\"\"\n    pq = PriorityQueue()\n    for n in NUMBERS:\n        pq.put(n)\n    \n    sorted_list = sorted([pq.get() for _ in range(pq.qsize())])\n\n    bisect.insort(sorted_list, num)\n\n    return sorted_list\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nheapq, queue, random\n```python\nimport heapq\nfrom queue import PriorityQueue\nfrom random import randint\n\n# Constants\nNUMBERS = [randint(-100, 100) for _ in range(20)]\n\ndef find_k_smallest_absolute_numbers(k):\n    \"\"\"\n    Find the k smallest numbers in a list based on their absolute values \n    using heapq and priority queue.\n\n    Parameters:\n    k (int): The number of smallest elements to find.\n\n    Returns:\n    list: The list of k smallest numbers.\n\n    Requirements:\n    - heapq\n    - queue\n    - random\n\n    Example:\n    >>> find_k_smallest_absolute_numbers(5)\n    \"\"\"\n    pq = PriorityQueue()\n    for num in NUMBERS:\n        if len(pq.queue) < k:\n            pq.put((abs(num), num))\n        else:\n            if abs(num) < pq.queue[0][0]:\n                heapq.heapreplace(pq.queue, (abs(num), num))\n\n    return [pq.get()[1] for _ in range(k)]\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \ncollections, queue, random\n```python\nimport collections\nfrom queue import PriorityQueue\nfrom random import randint\n\n# Constants\nLETTERS = ['a', 'b', 'c', 'd', 'e']\n\ndef letter_frequency():\n    \"\"\"\n    Generate a random string and count the frequency of each letter using a priority queue. \n\n    Requirements:\n    - collections\n    - queue\n    - random\n\n    Example:\n    >>> freq = letter_frequency()\n    >>> print(freq)\n    \"\"\"\n    string = ''.join([LETTERS[randint(0, len(LETTERS)-1)] for _ in range(100)])\n\n    freq = collections.Counter(string)\n\n    pq = PriorityQueue()\n    for letter, count in freq.items():\n        pq.put((-count, letter))\n\n    sorted_freq = collections.OrderedDict()\n    while not pq.empty():\n        count, letter = pq.get()\n        sorted_freq[letter] = -count\n\n    return sorted_freq\n```"}
{"seed": "{\"task_id\": 18897261, \"prompt\": \"def f_18897261(df):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"df['group'].plot(kind='bar', color=['r', 'g', 'b', 'r', 'g', 'b', 'r'])\", \"test_start\": \"\\nimport pandas as pd\\n\\ndef check(candidate):\", \"test\": [\"\\n    df = pd.DataFrame([1, 3, 4, 5, 7, 9], columns = ['group'])\\n    a = candidate(df)\\n    assert 'AxesSubplot' in str(type(a))\\n\"], \"entry_point\": \"f_18897261\", \"intent\": \"make a barplot of data in column `group` of dataframe `df` colour-coded according to list `color`\", \"library\": [\"pandas\"]}", "generation": "Scenario 1: \npandas, numpy, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLORS = ['r', 'g', 'b']\n\ndef plot_groups(df, group_col, value_col):\n    \"\"\"\n    Make a bar plot of data in multiple groups with error bars.\n\n    Parameters:\n    df (DataFrame): Input DataFrame.\n    group_col (str): Name of the column to group by.\n    value_col (str): Name of the column to plot values for.\n\n    Returns:\n    AxesSubplot: A matplotlib axes object.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame({'Group': ['A', 'B', 'A', 'B', 'A', 'B'], 'Value': [1, 2, 3, 4, 5, 6]})\n    >>> plot = plot_groups(df, 'Group', 'Value')\n    >>> plt.show()\n    \"\"\"\n    group_mean = df.groupby(group_col)[value_col].mean()\n    group_std = df.groupby(group_col)[value_col].std()\n\n    ax = group_mean.plot(kind='bar', yerr=group_std, color=COLORS[:len(group_mean)], capsize=4)\n\n    return ax\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \npandas, seaborn, matplotlib\n```python\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef plot_distribution(df, col):\n    \"\"\"\n    Plot a histogram and a boxplot for a specific column in the DataFrame.\n\n    Parameters:\n    df (DataFrame): Input DataFrame.\n    col (str): Column name to plot.\n\n    Returns:\n    Figure: A matplotlib figure object.\n\n    Requirements:\n    - pandas\n    - seaborn\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame({'value': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]})\n    >>> fig = plot_distribution(df, 'value')\n    >>> plt.show()\n    \"\"\"\n    fig, axes = plt.subplots(nrows=2, ncols=1)\n\n    sns.histplot(df[col], kde=True, ax=axes[0])\n    sns.boxplot(x=df[col], ax=axes[1])\n\n    return fig\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \npandas, matplotlib\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLORS = ['r', 'g', 'b', 'y', 'm']\n\ndef pie_chart(df, col):\n    \"\"\"\n    Plot a pie chart of the count of unique values in a specific column.\n\n    Parameters:\n    df (DataFrame): Input DataFrame.\n    col (str): Column name to plot.\n\n    Returns:\n    AxesSubplot: A matplotlib axes object.\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame({'fruit': ['apple', 'banana', 'orange', 'apple', 'banana', 'banana']})\n    >>> plot = pie_chart(df, 'fruit')\n    >>> plt.show()\n    \"\"\"\n    value_counts = df[col].value_counts()\n    ax = value_counts.plot(kind='pie', colors=COLORS[:len(value_counts)], autopct='%1.1f%%')\n\n    return ax\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \npandas, matplotlib\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef plot_time_series(df, time_col, value_col):\n    \"\"\"\n    Plot a time series from a DataFrame.\n\n    Parameters:\n    df (DataFrame): Input DataFrame.\n    time_col (str): Name of the column containing time data.\n    value_col (str): Name of the column containing value data.\n\n    Returns:\n    AxesSubplot: A matplotlib axes object.\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame({'Time': pd.date_range(start='1/1/2020', periods=5), 'Value': [1, 2, 3, 4, 5]})\n    >>> plot = plot_time_series(df, 'Time', 'Value')\n    >>> plt.show()\n    \"\"\"\n    df = df.sort_values(time_col)\n    ax = df.plot(x=time_col, y=value_col, style='.-')\n\n    return ax\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \npandas, matplotlib, seaborn\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef scatterplot_with_regression(df, col1, col2):\n    \"\"\"\n    Plot a scatter plot with a regression line for two columns from a DataFrame.\n\n    Parameters:\n    df (DataFrame): Input DataFrame.\n    col1 (str): Name of the first column.\n    col2 (str): Name of the second column.\n\n    Returns:\n    AxesSubplot: A seaborn axes object.\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> df = pd.DataFrame({'X': [1, 2, 3, 4, 5], 'Y': [2, 4, 6, 8, 10]})\n    >>> plot = scatterplot_with_regression(df, 'X', 'Y')\n    >>> plt.show()\n    \"\"\"\n    ax = sns.regplot(x=col1, y=col2, data=df)\n\n    return ax\n```\n"}
{"seed": "{\"task_id\": 373194, \"prompt\": \"def f_373194(data):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"re.findall('([a-fA-F\\\\\\\\d]{32})', data)\", \"test_start\": \"\\nimport re \\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('6f96cfdfe5ccc627cadf24b41725caa4 gorilla') ==         ['6f96cfdfe5ccc627cadf24b41725caa4']\\n\"], \"entry_point\": \"f_373194\", \"intent\": \"find all matches of regex pattern '([a-fA-F\\\\\\\\d]{32})' in string `data`\", \"library\": [\"re\"]}", "generation": "Scenario 1: \npandas, re, numpy, matplotlib\n```python\nimport pandas as pd\nimport re\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nPATTERN = r\"([a-fA-F\\d]{32})\"\n\ndef find_and_count_patterns_in_dataframe(df, column):\n    \"\"\"\n    Find all matches of the regex pattern '([a-fA-F\\d]{32})' in a pandas \n    DataFrame column and count the occurrence of each unique match in the data.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n    column (str): The column in which to find the pattern.\n\n    Returns:\n    Series: A pandas Series with counts of each unique match.\n\n    Requirements:\n    - pandas\n    - re\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> data = pd.DataFrame({\"text\": [\"6f96cfdfe5ccc627cadf24b41725caa4 gorilla\", \n    ...                               \"6f96cfdfe5ccc627cadf24b41725caa4 banana\",\n    ...                               \"1234567890abcdef1234567890abcdef apple\"]})\n    >>> counts = find_and_count_patterns_in_dataframe(data, \"text\")\n    >>> print(counts)\n    >>> counts.plot(kind='bar')\n    \"\"\"\n    matches = df[column].apply(lambda x: re.findall(PATTERN, x))\n    flattened_matches = np.concatenate(matches.values)\n    counts = pd.Series(flattened_matches).value_counts()\n    \n    return counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nre, os, hashlib, sys\n```python\nimport re\nimport os\nimport hashlib\nimport sys\n\n# Constants\nPATTERN = r\"([a-fA-F\\d]{32})\"\n\ndef find_and_verify_md5_in_files(file_paths):\n    \"\"\"\n    Find all matches of the regex pattern '([a-fA-F\\d]{32})' in a list of files. \n    Verifies if the found MD5 hash matches the MD5 hash of the file. \n\n    Parameters:\n    file_paths (list): List of file paths.\n\n    Returns:\n    dict: A dictionary with file paths as keys and verification results as values.\n\n    Requirements:\n    - re\n    - os\n    - hashlib\n    - sys\n\n    Example:\n    >>> result = find_and_verify_md5_in_files(['/path/to/file1', '/path/to/file2'])\n    >>> print(result)\n    \"\"\"\n    results = {}\n    \n    for file_path in file_paths:\n        with open(file_path, \"rb\") as f:\n            data = f.read()\n        \n        matches = re.findall(PATTERN, data.decode('utf-8', 'ignore'))\n        \n        if not matches:\n            results[file_path] = \"No MD5 hash found in file\"\n            continue\n\n        md5_hash = hashlib.md5(data).hexdigest()\n        \n        if md5_hash in matches:\n            results[file_path] = \"MD5 hash matches file content\"\n        else:\n            results[file_path] = \"MD5 hash does not match file content\"\n\n    return results\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nre, pandas, seaborn, matplotlib\n```python\nimport re\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nPATTERN = r\"([a-fA-F\\d]{32})\"\n\ndef visualize_pattern_in_dataframe(df, column):\n    \"\"\"\n    Find all matches of the regex pattern '([a-fA-F\\d]{32})' in a pandas \n    DataFrame column and visualize the count of each unique match.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n    column (str): The column in which to find the pattern.\n\n    Returns:\n    None\n\n    Requirements:\n    - re\n    - pandas\n    - seaborn\n    - matplotlib.pyplot\n\n    Example:\n    >>> data = pd.DataFrame({\"text\": [\"6f96cfdfe5ccc627cadf24b41725caa4 gorilla\", \n    ...                               \"6f96cfdfe5ccc627cadf24b41725caa4 banana\",\n    ...                               \"1234567890abcdef1234567890abcdef apple\"]})\n    >>> visualize_pattern_in_dataframe(data, \"text\")\n    \"\"\"\n    matches = df[column].apply(lambda x: re.findall(PATTERN, x))\n    flattened_matches = pd.Series([item for sublist in matches for item in sublist])\n    counts = flattened_matches.value_counts().reset_index()\n    counts.columns = ['Pattern', 'Count']\n    \n    plt.figure(figsize=(10,6))\n    sns.barplot(x='Pattern', y='Count', data=counts)\n    plt.title('Count of unique pattern matches')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nre, pathlib, glob, csv\n```python\nimport re\nfrom pathlib import Path\nimport glob\nimport csv\n\n# Constants\nPATTERN = r\"([a-fA-F\\d]{32})\"\n\ndef find_patterns_in_csv_files(directory):\n    \"\"\"\n    Find all matches of the regex pattern '([a-fA-F\\d]{32})' in all .csv files in a directory.\n\n    Parameters:\n    directory (str): The directory to search for .csv files.\n\n    Returns:\n    dict: A dictionary with file paths as keys and list of matches as values.\n\n    Requirements:\n    - re\n    - pathlib.Path\n    - glob\n    - csv\n\n    Example:\n    >>> result = find_patterns_in_csv_files('/path/to/directory')\n    >>> print(result)\n    \"\"\"\n    results = {}\n    csv_files = Path(directory).rglob('*.csv')\n\n    for csv_file in csv_files:\n        with open(csv_file, 'r') as f:\n            reader = csv.reader(f)\n            data = \" \".join(row for row in reader)\n            matches = re.findall(PATTERN, data)\n            results[csv_file] = matches\n\n    return results\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nre, requests, BeautifulSoup\n```python\nimport re\nimport requests\nfrom bs4 import BeautifulSoup\n\n# Constants\nPATTERN = r\"([a-fA-F\\d]{32})\"\nURL = 'https://www.example.com'\n\ndef find_patterns_in_webpage(url=URL):\n    \"\"\"\n    Find all matches of the regex pattern '([a-fA-F\\d]{32})' in a webpage.\n\n    Parameters:\n    url (str): The URL of the webpage to scrape.\n\n    Returns:\n    list: A list of all matches.\n\n    Requirements:\n    - re\n    - requests\n    - BeautifulSoup\n\n    Example:\n    >>> matches = find_patterns_in_webpage('https://www.example.com')\n    >>> print(matches)\n    \"\"\"\n    response = requests.get(url)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    text = soup.get_text()\n    matches = re.findall(PATTERN, text)\n\n    return matches\n```\n"}
{"seed": "{\"task_id\": 518021, \"prompt\": \"def f_518021(my_list):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"len(my_list)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([]) == 0\\n\", \"\\n    assert candidate([1]) == 1\\n\", \"\\n    assert candidate([1, 2]) == 2\\n\"], \"entry_point\": \"f_518021\", \"intent\": \"Get the length of list `my_list`\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, random, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import randint, sample\nimport matplotlib.pyplot as plt\n\n# Constants\nCATEGORIES = ['Electronics', 'Fashion', 'Home', 'Beauty', 'Sports']\nPRODUCTS = ['Product '+str(i) for i in range(1, 101)]\n\ndef generate_sales_report(product_list):\n    \"\"\"\n    Generate a sales report for a list of products across various categories. \n    The report includes the quantity sold and revenue generated for each product.\n    \n    Parameters:\n    product_list (list): The list of products.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with sales data for the products.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> report = generate_sales_report(PRODUCTS)\n    >>> print(report)\n    >>> report['Quantity Sold'].plot(kind='bar')\n    \"\"\"\n    report_data = []\n\n    for product in product_list:\n        category = CATEGORIES[randint(0, len(CATEGORIES)-1)]\n        quantity_sold = randint(1, 100)\n        revenue = quantity_sold * randint(10, 100)\n        report_data.append([product, category, quantity_sold, revenue])\n\n    report_df = pd.DataFrame(report_data, columns=['Product', 'Category', 'Quantity Sold', 'Revenue'])\n\n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, random, itertools\n```python\nimport numpy as np\nfrom random import sample\nimport itertools\n\n# Constants\nNUMBERS = list(range(1, 101))\n\ndef generate_combinations(number_list, r):\n    \"\"\"\n    Generate all possible combinations of r elements from a given list of numbers.\n    \n    Parameters:\n    number_list (list): The list of numbers.\n    r (int): The number of elements in each combination.\n    \n    Returns:\n    list: A list of tuples, each tuple representing a combination.\n    \n    Requirements:\n    - numpy\n    - random\n    - itertools\n    \n    Example:\n    >>> combinations = generate_combinations(NUMBERS, 3)\n    >>> print(combinations)\n    \"\"\"\n    return list(itertools.combinations(number_list, r))\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, sklearn, numpy, matplotlib\n```python\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nFEATURES = ['feature '+str(i) for i in range(1, 11)]\nTARGET = 'target'\n\ndef generate_linear_model(df):\n    \"\"\"\n    Train a linear regression model on a given DataFrame.\n    \n    Parameters:\n    df (DataFrame): The DataFrame with features and target.\n    \n    Returns:\n    LinearRegression: The trained linear regression model.\n    \n    Requirements:\n    - pandas\n    - sklearn.model_selection\n    - sklearn.linear_model\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = pd.DataFrame(np.random.rand(100, 11), columns=FEATURES+[TARGET])\n    >>> model = generate_linear_model(df)\n    >>> print(model.coef_)\n    \"\"\"\n    X = df[FEATURES]\n    y = df[TARGET]\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    return model\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, random, matplotlib, seaborn\n```python\nimport numpy as np\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nNUMBERS = list(range(1, 101))\n\ndef generate_histogram(number_list, bins):\n    \"\"\"\n    Generate a histogram for a given list of numbers.\n    \n    Parameters:\n    number_list (list): The list of numbers.\n    bins (int): The number of bins for the histogram.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - numpy\n    - random\n    - matplotlib.pyplot\n    - seaborn\n    \n    Example:\n    >>> generate_histogram(NUMBERS, 10)\n    \"\"\"\n    plt.hist(number_list, bins=bins)\n    plt.title('Histogram')\n    plt.xlabel('Number')\n    plt.ylabel('Frequency')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, sklearn, matplotlib\n```python\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nFEATURES = ['feature '+str(i) for i in range(1, 11)]\n\ndef generate_kmeans_clusters(df, n_clusters):\n    \"\"\"\n    Apply K-means clustering on a given DataFrame and plot the clusters.\n    \n    Parameters:\n    df (DataFrame): The DataFrame with features.\n    n_clusters (int): The number of clusters for K-means.\n    \n    Returns:\n    KMeans: The K-means model.\n    \n    Requirements:\n    - pandas\n    - sklearn.cluster\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = pd.DataFrame(np.random.rand(100, 10), columns=FEATURES)\n    >>> model = generate_kmeans_clusters(df, 3)\n    >>> print(model.cluster_centers_)\n    \"\"\"\n    X = df[FEATURES]\n\n    model = KMeans(n_clusters=n_clusters, random_state=42)\n    model.fit(X)\n\n    plt.scatter(df[FEATURES[0]], df[FEATURES[1]], c=model.labels_)\n    plt.title('K-means Clusters')\n    plt.xlabel(FEATURES[0])\n    plt.ylabel(FEATURES[1])\n    plt.show()\n\n    return model\n```"}
{"seed": "{\"task_id\": 518021, \"prompt\": \"def f_518021(l):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"len(l)\", \"test_start\": \"\\nimport numpy as np \\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([]) == 0\\n\", \"\\n    assert candidate(np.array([1])) == 1\\n\", \"\\n    assert candidate(np.array([1, 2])) == 2\\n\"], \"entry_point\": \"f_518021\", \"intent\": \"Getting the length of array `l`\", \"library\": [\"numpy\"]}", "generation": "Scenario 1:\nnumpy, scipy.stats, matplotlib\n```python\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef plot_histogram_with_fit(l):\n    \"\"\"\n    Plot a histogram of the given array with a Gaussian fit.\n\n    Parameters:\n    l (numpy array): The input array.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Example:\n    >>> l = np.array([1, 2, 2, 3, 3, 3, 4, 4, 4, 4])\n    >>> plot_histogram_with_fit(l)\n    \"\"\"\n    plt.hist(l, bins='auto', density=True, alpha=0.6, color='g')\n\n    mu, std = stats.norm.fit(l)\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, mu, std)\n    plt.plot(x, p, 'k', linewidth=2)\n    title = \"Fit results: mu = %.2f,  std = %.2f\" % (mu, std)\n    plt.title(title)\n\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, pandas, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef plot_time_series(l, dates):\n    \"\"\"\n    Plot a time series of the given array.\n\n    Parameters:\n    l (numpy array): The input array.\n    dates (list): A list of dates corresponding to the elements in l.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> l = np.array([1, 2, 3, 4, 5])\n    >>> dates = pd.date_range(start='1/1/2020', periods=5)\n    >>> plot_time_series(l, dates)\n    \"\"\"\n    ts = pd.Series(l, index=dates)\n    ts.plot()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, sklearn.preprocessing, pandas\n```python\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\n\ndef scale_and_display(l):\n    \"\"\"\n    Scale the input array to range [0, 1] and display as a DataFrame.\n\n    Parameters:\n    l (numpy array): The input array.\n\n    Returns:\n    DataFrame: A pandas DataFrame of the scaled array.\n\n    Requirements:\n    - numpy\n    - sklearn.preprocessing\n    - pandas\n\n    Example:\n    >>> l = np.array([10, 20, 30, 40, 50])\n    >>> df = scale_and_display(l)\n    >>> print(df)\n    \"\"\"\n    scaler = MinMaxScaler()\n    l_scaled = scaler.fit_transform(l.reshape(-1, 1))\n    df = pd.DataFrame(l_scaled, columns=['Scaled Values'])\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, scipy.optimize, matplotlib\n```python\nimport numpy as np\nfrom scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\n\ndef fit_curve_and_plot(l, x_data):\n    \"\"\"\n    Fit a curve to the given array and plot the results.\n\n    Parameters:\n    l (numpy array): The input array.\n    x_data (numpy array): The x values corresponding to l.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - scipy.optimize\n    - matplotlib.pyplot\n\n    Example:\n    >>> l = np.array([1, 4, 9, 16, 25])\n    >>> x_data = np.array([1, 2, 3, 4, 5])\n    >>> fit_curve_and_plot(l, x_data)\n    \"\"\"\n    def func(x, a, b):\n        return a * x**2 + b\n\n    params, params_covariance = curve_fit(func, x_data, l)\n\n    plt.figure(figsize=(6, 4))\n    plt.scatter(x_data, l, label='Data')\n    plt.plot(x_data, func(x_data, params[0], params[1]), label='Fitted function')\n\n    plt.legend(loc='best')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, sklearn.decomposition, matplotlib\n```python\nimport numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\ndef perform_pca_and_plot(l):\n    \"\"\"\n    Perform Principal Component Analysis (PCA) on the given array and plot the first two principal components.\n\n    Parameters:\n    l (numpy array): The input array.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - sklearn.decomposition\n    - matplotlib.pyplot\n\n    Example:\n    >>> l = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    >>> perform_pca_and_plot(l)\n    \"\"\"\n    pca = PCA(n_components=2)\n    principalComponents = pca.fit_transform(l)\n    \n    plt.figure(figsize=(6, 4))\n    plt.scatter(principalComponents[:, 0], principalComponents[:, 1])\n    plt.xlabel('First Principal Component')\n    plt.ylabel('Second Principal Component')\n    plt.title('PCA Result')\n    plt.show()\n```\nAbove are the illustrations.\n"}
{"seed": "{\"task_id\": 518021, \"prompt\": \"def f_518021(s):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"len(s)\", \"test_start\": \"\\nimport numpy as np \\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([]) == 0\\n\", \"\\n    assert candidate(np.array([1])) == 1\\n\", \"\\n    assert candidate(np.array([1, 2])) == 2\\n\"], \"entry_point\": \"f_518021\", \"intent\": \"Getting the length of array `s`\", \"library\": [\"numpy\"]}", "generation": "Scenario 1:\npandas, numpy, matplotlib, random\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\n\ndef generate_and_plot_data(length):\n    \"\"\"\n    Generate a pandas DataFrame with given length and random data, then plot the data.\n\n    Parameters:\n    length (int): The length of the DataFrame to be generated.\n\n    Returns:\n    DataFrame: A pandas DataFrame with random data.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> df = generate_and_plot_data(100)\n    >>> print(df)\n    \"\"\"\n    data = np.random.randint(0,100,size=(length, len(COLUMNS)))\n    df = pd.DataFrame(data, columns=COLUMNS)\n\n    df.plot(kind='line')\n    plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, scipy.stats, matplotlib\n```python\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\n# Constants\nMU = 0\nSIGMA = 1\n\ndef generate_normal_distribution(length):\n    \"\"\"\n    Generate a normal distribution with given length, and plot the histogram and the probability density function.\n\n    Parameters:\n    length (int): The length of the distribution to be generated.\n\n    Returns:\n    array: A numpy array with the normal distribution.\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Example:\n    >>> distribution = generate_normal_distribution(1000)\n    >>> print(distribution)\n    \"\"\"\n    distribution = np.random.normal(MU, SIGMA, length)\n    count, bins, ignored = plt.hist(distribution, 30, density=True)\n    plt.plot(bins, norm.pdf(bins, MU, SIGMA), linewidth=2, color='r')\n    plt.show()\n\n    return distribution\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, matplotlib, sklearn.preprocessing\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef scale_and_plot_data(length):\n    \"\"\"\n    Generate a numpy array with given length, scale the data to a range of 0-1, and plot both the original and scaled data.\n\n    Parameters:\n    length (int): The length of the array to be generated.\n\n    Returns:\n    array: A numpy array with the scaled data.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.preprocessing\n\n    Example:\n    >>> scaled_data = scale_and_plot_data(100)\n    >>> print(scaled_data)\n    \"\"\"\n    data = np.random.randint(0,100,length)\n    scaler = MinMaxScaler()\n    scaled_data = scaler.fit_transform(data.reshape(-1, 1))\n\n    plt.figure(figsize=(12,6))\n    plt.subplot(1, 2, 1)\n    plt.plot(data)\n    plt.title('Original Data')\n    plt.subplot(1, 2, 2)\n    plt.plot(scaled_data)\n    plt.title('Scaled Data')\n    plt.show()\n\n    return scaled_data.flatten()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, matplotlib, seaborn\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef generate_and_plot_heatmap(length):\n    \"\"\"\n    Generate a 2D numpy array with given length, and plot the heatmap.\n\n    Parameters:\n    length (int): The length of the array to be generated.\n\n    Returns:\n    array: A 2D numpy array with random data.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> array = generate_and_plot_heatmap(10)\n    >>> print(array)\n    \"\"\"\n    array = np.random.rand(length, length)\n    sns.heatmap(array, cmap='viridis')\n    plt.show()\n\n    return array\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, pandas, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\n\ndef generate_and_plot_cdf(length):\n    \"\"\"\n    Generate a pandas DataFrame with given length, calculate the cumulative distribution function (CDF), and plot the CDF.\n\n    Parameters:\n    length (int): The length of the DataFrame to be generated.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the CDF.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> cdf = generate_and_plot_cdf(100)\n    >>> print(cdf)\n    \"\"\"\n    data = np.random.randint(0,100,size=(length, len(COLUMNS)))\n    df = pd.DataFrame(data, columns=COLUMNS)\n\n    df = df.apply(lambda x: x.value_counts().sort_index().cumsum())\n    df.plot()\n    plt.show()\n\n    return df\n```"}
{"seed": "{\"task_id\": 518021, \"prompt\": \"def f_518021(my_tuple):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"len(my_tuple)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(()) == 0\\n\", \"\\n    assert candidate(('aa', 'wfseg', '')) == 3\\n\", \"\\n    assert candidate(('apple',)) == 1\\n\"], \"entry_point\": \"f_518021\", \"intent\": \"Getting the length of `my_tuple`\", \"library\": []}", "generation": "Scenario 1:\ncollections, csv, os, pandas\n```python\nimport collections\nimport csv\nimport os\nimport pandas as pd\n\n# Constants\nDIR_PATH = './data'\nCSV_FILES = [f for f in os.listdir(DIR_PATH) if f.endswith('.csv')]\n\ndef count_column_values(my_tuple):\n    \"\"\"\n    Count the occurrences of each value in the specified columns across multiple CSV files.\n\n    Parameters:\n    my_tuple (tuple): The tuple of column names.\n\n    Returns:\n    dict: A dictionary where keys are column names and values are dictionaries \n        with unique values in the column as keys and their counts as values.\n\n    Requirements:\n    - collections\n    - csv\n    - os\n    - pandas\n\n    Example:\n    >>> count_column_values(('Country', 'Gender'))\n    \"\"\"\n    counter = {column: collections.Counter() for column in my_tuple}\n\n    for csv_file in CSV_FILES:\n        df = pd.read_csv(os.path.join(DIR_PATH, csv_file))\n\n        for column in my_tuple:\n            counter[column].update(df[column])\n\n    return counter\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, matplotlib, sklearn.cluster, random\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom random import randint\n\n# Constants\nN_CLUSTER = 3\n\ndef simple_clustering(my_tuple):\n    \"\"\"\n    Perform K-means clustering on a 2D data set generated based on the input tuple.\n    The tuple is used to generate centers of the data clusters.\n\n    Parameters:\n    my_tuple (tuple): The tuple of 2D points (tuples) that represent centers of the data clusters.\n\n    Returns:\n    ndarray, ndarray: The cluster centers and the cluster indices of each data point.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.cluster\n    - random\n\n    Example:\n    >>> simple_clustering(((1, 1), (2, 2), (3, 3)))\n    \"\"\"\n    data = []\n\n    for center in my_tuple:\n        for _ in range(100):\n            data.append((center[0] + randint(-10, 10), center[1] + randint(-10, 10)))\n\n    data = np.array(data)\n\n    kmeans = KMeans(n_clusters=N_CLUSTER)\n    kmeans.fit(data)\n\n    plt.scatter(data[:, 0], data[:, 1], c=kmeans.labels_)\n    plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red')\n\n    return kmeans.cluster_centers_, kmeans.labels_\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, scipy.stats, matplotlib\n```python\nimport numpy as np\nfrom scipy.stats import kde\nimport matplotlib.pyplot as plt\n\n# Constants\nN_POINTS = 1000\n\ndef density_estimation(my_tuple):\n    \"\"\"\n    Perform kernel density estimation based on the input tuple and plot the result.\n\n    Parameters:\n    my_tuple (tuple): The tuple of 1D arrays representing different variables.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Example:\n    >>> density_estimation((np.random.normal(size=N_POINTS, loc=0, scale=1), \n                            np.random.normal(size=N_POINTS, loc=5, scale=2)))\n    \"\"\"\n    density = kde.gaussian_kde(my_tuple)\n    x = np.linspace(min(min(var) for var in my_tuple), \n                    max(max(var) for var in my_tuple), N_POINTS)\n    plt.plot(x, density(x))\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, matplotlib, sklearn.linear_model, random\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom random import random\n\n# Constants\nX_RANGE = (0, 10)\nN_POINTS = 100\n\ndef simple_linear_regression(my_tuple):\n    \"\"\"\n    Perform simple linear regression based on the input tuple that represents the slope \n    and the intercept of the line. Also, plot the line and the data points.\n\n    Parameters:\n    my_tuple (tuple): The tuple with a slope and an intercept of the line.\n\n    Returns:\n    LinearRegression: The fitted LinearRegression model.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.linear_model\n    - random\n\n    Example:\n    >>> simple_linear_regression((2, 3))\n    \"\"\"\n    x = np.linspace(*X_RANGE, N_POINTS).reshape(-1, 1)\n    y = my_tuple[0] * x + my_tuple[1] + np.random.normal(scale=0.5, size=(N_POINTS, 1))\n\n    model = LinearRegression()\n    model.fit(x, y)\n\n    plt.scatter(x, y, color='blue')\n    plt.plot(x, model.predict(x), color='red')\n\n    return model\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, matplotlib, sklearn.preprocessing, random\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom random import random\n\n# Constants\nX_RANGE = (-10, 10)\nN_POINTS = 100\nDEGREE = 2\n\ndef polynomial_regression(my_tuple):\n    \"\"\"\n    Perform polynomial regression based on the input tuple that represents the coefficients of the polynomial.\n    Also, plot the polynomial and the data points.\n\n    Parameters:\n    my_tuple (tuple): The tuple of coefficients of the polynomial.\n\n    Returns:\n    LinearRegression: The fitted LinearRegression model.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.preprocessing\n    - sklearn.linear_model\n    - random\n\n    Example:\n    >>> polynomial_regression((1, 0, -1))  # y = x^2 - 1\n    \"\"\"\n    x = np.linspace(*X_RANGE, N_POINTS).reshape(-1, 1)\n    y = sum(coef * x ** i for i, coef in enumerate(my_tuple)) + np.random.normal(scale=2, size=(N_POINTS, 1))\n\n    poly_features = PolynomialFeatures(degree=DEGREE, include_bias=False)\n    x_poly = poly_features.fit_transform(x)\n\n    model = LinearRegression()\n    model.fit(x_poly, y)\n\n    x_new = np.linspace(*X_RANGE, 500).reshape(-1, 1)\n    x_new_poly = poly_features.transform(x_new)\n    y_new = model.predict(x_new_poly)\n\n    plt.scatter(x, y, color='blue')\n    plt.plot(x_new, y_new, color='red')\n\n    return model\n```"}
{"seed": "{\"task_id\": 518021, \"prompt\": \"def f_518021(my_string):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"len(my_string)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(\\\"sedfgbdjofgljnh\\\") == 15\\n\", \"\\n    assert candidate(\\\"             \\\") == 13\\n\", \"\\n    assert candidate(\\\"vsdh4'cdf'\\\") == 10\\n\"], \"entry_point\": \"f_518021\", \"intent\": \"Getting the length of `my_string`\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, random, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import choice\nimport matplotlib.pyplot as plt\n\n# Constants\nLETTERS = 'abcdefghijklmnopqrstuvwxyz'\nNAMES = ['Alice', 'Bob', 'Charlie', 'David', 'Eve']\n\ndef generate_data_frame(n):\n    \"\"\"\n    Generate a pandas DataFrame of random string data and lengths. \n    The DataFrame should have columns for the name, string, and length of the string.\n\n    Parameters:\n    n (int): The number of rows in the DataFrame.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the names, strings, and their lengths.\n\n    Requirements:\n    - pandas\n    - numpy\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = generate_data_frame(10)\n    >>> print(df)\n    >>> df['Length'].plot(kind='hist')\n    \"\"\"\n    data = []\n\n    for _ in range(n):\n        name = choice(NAMES)\n        string = ''.join(choice(LETTERS) for _ in range(np.random.poisson(10)))\n        length = len(string)\n        data.append([name, string, length])\n\n    df = pd.DataFrame(data, columns=['Name', 'String', 'Length'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \ncollections, random, string\n```python\nfrom collections import Counter\nfrom random import choice\nimport string\n\n# Constants\nLETTERS = string.ascii_letters\n\ndef count_letter_frequencies(n):\n    \"\"\"\n    Generate a string of n random letters and return a dictionary with the frequency of each letter.\n\n    Parameters:\n    n (int): The length of the string.\n\n    Returns:\n    dict: A dictionary with the frequency of each letter.\n\n    Requirements:\n    - collections\n    - random\n    - string\n\n    Example:\n    >>> frequencies = count_letter_frequencies(1000)\n    >>> print(frequencies)\n    \"\"\"\n    random_string = ''.join(choice(LETTERS) for _ in range(n))\n    frequencies = dict(Counter(random_string))\n\n    return frequencies\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nre, string, random\n```python\nimport re\nimport string\nfrom random import choice\n\n# Constants\nLETTERS = string.ascii_letters\nSPECIAL_CHARS = '!@#$%^&*()-_=+[]{}|;:,.<>/?'\n\ndef check_password_strength(password):\n    \"\"\"\n    Check the strength of a password. The password is strong if it contains at least one \n    uppercase letter, one lowercase letter, one digit, one special character, and is at least 8 characters long.\n\n    Parameters:\n    password (str): The password to check.\n\n    Returns:\n    str: A string indicating the strength of the password (\"weak\" or \"strong\").\n\n    Requirements:\n    - re\n    - string\n    - random\n\n    Example:\n    >>> check_password_strength('Password123!')\n    'strong'\n    \"\"\"\n    if (len(password) < 8 or not re.search('[a-z]', password) or \n        not re.search('[A-Z]', password) or not re.search('[0-9]', password) or \n        not re.search('[' + SPECIAL_CHARS + ']', password)):\n        return 'weak'\n    \n    return 'strong'\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nrandom, itertools, string\n```python\nfrom random import choice, randint\nimport itertools\nimport string\n\n# Constants\nLETTERS = string.ascii_letters\n\ndef find_substring(n, m):\n    \"\"\"\n    Generate a random string of length n. Generate all substrings of length m and find the one that appears the most.\n\n    Parameters:\n    n (int): The length of the string.\n    m (int): The length of the substrings.\n\n    Returns:\n    str: The substring that appears the most. If there are multiple, return one of them at random.\n\n    Requirements:\n    - random\n    - itertools\n    - string\n\n    Example:\n    >>> find_substring(100, 2)\n    \"\"\"\n    random_string = ''.join(choice(LETTERS) for _ in range(n))\n    substrings = [''.join(c) for c in itertools.combinations(random_string, m)]\n    counts = {substring: substrings.count(substring) for substring in set(substrings)}\n    max_count = max(counts.values())\n    most_frequent_substrings = [k for k, v in counts.items() if v == max_count]\n    return choice(most_frequent_substrings)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nrandom, string, collections\n```python\nfrom random import choice\nimport string\nfrom collections import defaultdict\n\n# Constants\nLETTERS = string.ascii_letters\n\ndef count_first_letters(n):\n    \"\"\"\n    Generate a list of n random strings and return a dictionary with the count of the first letter of each string.\n\n    Parameters:\n    n (int): The number of strings.\n\n    Returns:\n    dict: A dictionary with the count of the first letter of each string.\n\n    Requirements:\n    - random\n    - string\n    - collections\n\n    Example:\n    >>> counts = count_first_letters(1000)\n    >>> print(counts)\n    \"\"\"\n    strings = [''.join(choice(LETTERS) for _ in range(choice(range(1, 11)))) for _ in range(n)]\n    counts = defaultdict(int)\n\n    for string in strings:\n        counts[string[0]] += 1\n\n    return dict(counts)\n```\n"}
{"seed": "{\"task_id\": 40452956, \"prompt\": \"def f_40452956():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"b'\\\\\\\\a'.decode('unicode-escape')\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate() == '\\\\x07'\\n\"], \"entry_point\": \"f_40452956\", \"intent\": \"remove escape character from string \\\"\\\\\\\\a\\\"\", \"library\": []}", "generation": "Scenario 1: \nos, sys, re, argparse\n```python\nimport os\nimport sys\nimport re\nimport argparse\n\nESCAPE_SEQUENCE_RE = re.compile(r'\\\\[0-7]{1,3}')  # Octal escape sequence\n\ndef remove_escape_sequences_in_file(file_path):\n    \"\"\"\n    Remove all octal escape sequences in a text file.\n\n    Parameters:\n    file_path (str): The path to the text file.\n\n    Returns:\n    str: The text of the file with all octal escape sequences removed.\n\n    Requirements:\n    - os\n    - sys\n    - re\n    - argparse\n\n    Example:\n    >>> remove_escape_sequences_in_file('sample.txt')\n    \"\"\"\n    try:\n        with open(file_path, 'r') as file:\n            text = file.read()\n            text = ESCAPE_SEQUENCE_RE.sub('', text)\n            return text\n    except Exception as e:\n        print(f\"Error: {e}\", file=sys.stderr)\n        return None\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nre, json, requests\n```python\nimport re\nimport json\nimport requests\n\n# Regular expression pattern for hexadecimal escape sequences\nHEX_ESCAPE_RE = re.compile(r'\\\\x[0-9a-fA-F]{2}')\n\ndef remove_hex_escapes_in_json(json_url):\n    \"\"\"\n    Load a JSON from a URL and remove all hexadecimal escape sequences in the JSON strings.\n\n    Parameters:\n    json_url (str): The URL of the json file.\n\n    Returns:\n    dict: The loaded JSON with all hexadecimal escape sequences removed from the strings.\n\n    Requirements:\n    - re\n    - json\n    - requests\n\n    Example:\n    >>> remove_hex_escapes_in_json('https://example.com/sample.json')\n    \"\"\"\n    response = requests.get(json_url)\n    json_data = json.loads(response.text)\n\n    def remove_hex_escapes_in_string(s):\n        return HEX_ESCAPE_RE.sub('', s)\n\n    def traverse(obj):\n        if isinstance(obj, dict):\n            return {k: traverse(v) for k, v in obj.items()}\n        elif isinstance(obj, list):\n            return [traverse(elem) for elem in obj]\n        elif isinstance(obj, str):\n            return remove_hex_escapes_in_string(obj)\n        else:\n            return obj\n\n    return traverse(json_data)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \npandas, numpy, re\n```python\nimport pandas as pd\nimport numpy as np\nimport re\n\n# Regular expression pattern for Unicode escape sequences\nUNICODE_ESCAPE_RE = re.compile(r'\\\\u[0-9a-fA-F]{4}')\n\ndef remove_unicode_escapes_in_dataframe(df):\n    \"\"\"\n    Remove all Unicode escape sequences in a pandas DataFrame.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n\n    Returns:\n    DataFrame: The DataFrame with all Unicode escape sequences removed.\n\n    Requirements:\n    - pandas\n    - numpy\n    - re\n\n    Example:\n    >>> df = pd.DataFrame({'text': ['Hello\\\\u0020World', 'Python\\\\u0020Rocks']})\n    >>> remove_unicode_escapes_in_dataframe(df)\n    \"\"\"\n    return df.replace(to_replace=UNICODE_ESCAPE_RE, value='', regex=True)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nos, sys, re, argparse\n```python\nimport os\nimport sys\nimport re\nimport argparse\n\n# Regular expression pattern for ASCII escape sequences\nASCII_ESCAPE_RE = re.compile(r'\\\\[abfnrtv]')\n\ndef remove_ascii_escapes_in_directory(directory_path):\n    \"\"\"\n    Remove all ASCII escape sequences in all text files in a directory.\n\n    Parameters:\n    directory_path (str): The path to the directory.\n\n    Returns:\n    dict: A dictionary where the keys are the file names and the values are the texts of the files with all ASCII escape sequences removed.\n\n    Requirements:\n    - os\n    - sys\n    - re\n    - argparse\n\n    Example:\n    >>> remove_ascii_escapes_in_directory('sample_directory')\n    \"\"\"\n    result = {}\n\n    try:\n        for filename in os.listdir(directory_path):\n            if filename.endswith(\".txt\"):\n                file_path = os.path.join(directory_path, filename)\n                with open(file_path, 'r') as file:\n                    text = file.read()\n                    text = ASCII_ESCAPE_RE.sub('', text)\n                    result[filename] = text\n\n        return result\n    except Exception as e:\n        print(f\"Error: {e}\", file=sys.stderr)\n        return None\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nos, re, argparse\n```python\nimport os\nimport re\nimport argparse\n\n# Regular expression pattern for escape sequences\nESCAPE_RE = re.compile(r'\\\\[0-7]{1,3}|\\\\x[0-9a-fA-F]{2}|\\\\u[0-9a-fA-F]{4}|\\\\[abfnrtv]')\n\ndef remove_all_escapes_in_string(s):\n    \"\"\"\n    Remove all escape sequences in a string.\n\n    Parameters:\n    s (str): The string.\n\n    Returns:\n    str: The string with all escape sequences removed.\n\n    Requirements:\n    - os\n    - re\n    - argparse\n\n    Example:\n    >>> remove_all_escapes_in_string('Hello\\\\u0020World')\n    \"\"\"\n    return ESCAPE_RE.sub('', s)\n```"}
{"seed": "{\"task_id\": 8687018, \"prompt\": \"def f_8687018():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"\\\"\\\"\\\"obama\\\"\\\"\\\".replace('a', '%temp%').replace('b', 'a').replace('%temp%', 'b')\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate() == 'oabmb'\\n\"], \"entry_point\": \"f_8687018\", \"intent\": \"replace each 'a' with 'b' and each 'b' with 'a' in the string 'obama' in a single pass.\", \"library\": []}", "generation": "Scenario 1:\ncollections, random, string\n```python\nimport collections\nimport random\nimport string\n\n# Constants\nLETTERS = string.ascii_lowercase\n\ndef random_letter_replacement(input_str):\n    \"\"\"\n    Replace each lowercase letter in the input string with a random unique \n    lowercase letter in a single pass.\n\n    Parameters:\n    input_str (str): The input string.\n\n    Returns:\n    str: The string after replacements.\n\n    Requirements:\n    - collections\n    - random\n    - string\n\n    Example:\n    >>> random_letter_replacement('obama')\n    \"\"\"\n    replacement_dict = collections.defaultdict(lambda: random.choice(LETTERS))\n    output_str = ''.join(replacement_dict[char] if char in LETTERS else char for char in input_str)\n\n    return output_str\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, sklearn.preprocessing\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\n\n# Constants\nCATEGORIES = ['a', 'b', 'c']\n\ndef encode_categories(df, column):\n    \"\"\"\n    Replace categorical values in a pandas DataFrame column with numerical \n    encodings using sklearn's LabelEncoder.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n    column (str): The column to encode.\n\n    Returns:\n    DataFrame: The DataFrame with the encoded column.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.preprocessing\n\n    Example:\n    >>> df = pd.DataFrame({'Category': ['a', 'b', 'b', 'a', 'c', 'b']})\n    >>> encode_categories(df, 'Category')\n    \"\"\"\n    le = LabelEncoder()\n    df[column] = le.fit_transform(df[column])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nre, random, string\n```python\nimport re\nimport random\nimport string\n\n# Constants\nPUNCTUATION = string.punctuation\n\ndef random_punctuation_replacement(input_str):\n    \"\"\"\n    Replace each punctuation symbol in the input string with a random unique \n    punctuation symbol in a single pass.\n\n    Parameters:\n    input_str (str): The input string.\n\n    Returns:\n    str: The string after replacements.\n\n    Requirements:\n    - re\n    - random\n    - string\n\n    Example:\n    >>> random_punctuation_replacement('Hello, world!')\n    \"\"\"\n    replacement_dict = {}\n    for char in set(re.findall(rf'[{PUNCTUATION}]', input_str)):\n        replacement_dict[char] = random.choice(PUNCTUATION)\n\n    output_str = ''.join(replacement_dict[char] if char in PUNCTUATION else char for char in input_str)\n\n    return output_str\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, sklearn.preprocessing\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelBinarizer\n\n# Constants\nCATEGORIES = ['a', 'b', 'c']\n\ndef one_hot_encode_categories(df, column):\n    \"\"\"\n    Replace categorical values in a pandas DataFrame column with one-hot \n    encodings using sklearn's LabelBinarizer.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n    column (str): The column to encode.\n\n    Returns:\n    DataFrame: The DataFrame with the one-hot encoded columns.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.preprocessing\n\n    Example:\n    >>> df = pd.DataFrame({'Category': ['a', 'b', 'b', 'a', 'c', 'b']})\n    >>> one_hot_encode_categories(df, 'Category')\n    \"\"\"\n    lb = LabelBinarizer()\n    one_hot = lb.fit_transform(df[column])\n    one_hot_df = pd.DataFrame(one_hot, columns=[f'{column}_{class_}' for class_ in lb.classes_])\n    df = pd.concat([df, one_hot_df], axis=1)\n    df.drop(column, axis=1, inplace=True)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\ncollections, random, string\n```python\nimport collections\nimport random\nimport string\n\n# Constants\nLETTERS = string.ascii_letters\n\ndef random_case_sensitive_letter_replacement(input_str):\n    \"\"\"\n    Replace each case-sensitive letter in the input string with a random unique \n    case-sensitive letter in a single pass.\n\n    Parameters:\n    input_str (str): The input string.\n\n    Returns:\n    str: The string after replacements.\n\n    Requirements:\n    - collections\n    - random\n    - string\n\n    Example:\n    >>> random_case_sensitive_letter_replacement('Obama')\n    \"\"\"\n    replacement_dict = collections.defaultdict(lambda: random.choice(LETTERS))\n    output_str = ''.join(replacement_dict[char] if char in LETTERS else char for char in input_str)\n\n    return output_str\n```"}
{"seed": "{\"task_id\": 303200, \"prompt\": \"def f_303200():\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"shutil.rmtree('/folder_name')\", \"test_start\": \"\\nimport os\\nimport shutil\\nfrom unittest.mock import Mock\\n\\ndef check(candidate):\", \"test\": [\"\\n    shutil.rmtree = Mock()\\n    os.walk = Mock(return_value = [])\\n    candidate()\\n    assert os.walk('/') == []\\n\"], \"entry_point\": \"f_303200\", \"intent\": \"remove directory tree '/folder_name'\", \"library\": [\"os\", \"shutil\"]}", "generation": "Scenario 1: \nos, shutil, glob, pathlib\n```python\nimport os\nimport shutil\nfrom glob import glob\nfrom pathlib import Path\n\n# Constants\nBASE_DIRECTORY = '/base_directory'\nFILE_EXTENSION = '.txt'\n\ndef remove_files_with_extension(directory, extension):\n    \"\"\"\n    Removes all files with a specific extension in a given directory and its subdirectories.\n\n    Parameters:\n    directory (str): The directory in which to search for the files.\n    extension (str): The extension of the files to remove.\n\n    Requirements:\n    - os\n    - shutil\n    - glob\n    - pathlib.Path\n\n    Example:\n    >>> remove_files_with_extension(BASE_DIRECTORY, FILE_EXTENSION)\n    \"\"\"\n    # Create a pattern for glob\n    pattern = f'**/*{extension}'\n\n    # Use glob to get all files with the extension in the directory and subdirectories\n    files = glob(os.path.join(directory, pattern), recursive=True)\n\n    # Remove each file\n    for file in files:\n        os.remove(file)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, shutil, time, datetime\n```python\nimport os\nimport shutil\nimport time\nfrom datetime import datetime, timedelta\n\n# Constants\nDIRECTORY = '/target_directory'\nDAYS = 7\n\ndef remove_files_older_than(directory, days):\n    \"\"\"\n    Removes all files in a given directory that were last modified more than a certain number of days ago.\n\n    Parameters:\n    directory (str): The directory in which to search for the files.\n    days (int): The number of days.\n\n    Requirements:\n    - os\n    - shutil\n    - time\n    - datetime.timedelta\n\n    Example:\n    >>> remove_files_older_than(DIRECTORY, DAYS)\n    \"\"\"\n    # Calculate the cutoff time\n    cutoff = datetime.now() - timedelta(days)\n\n    # Get all files in the directory\n    files = os.listdir(directory)\n\n    # Remove each file that was last modified before the cutoff\n    for file in files:\n        file_path = os.path.join(directory, file)\n        if datetime.fromtimestamp(os.path.getmtime(file_path)) < cutoff:\n            os.remove(file_path)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, shutil, fnmatch, io\n```python\nimport os\nimport shutil\nimport fnmatch\nimport io\n\n# Constants\nDIRECTORY = '/target_directory'\nPATTERN = '*.txt'\n\ndef copy_matching_files(source_directory, target_directory, pattern):\n    \"\"\"\n    Copies all files in a given directory that match a certain pattern to another directory.\n\n    Parameters:\n    source_directory (str): The directory in which to search for the files.\n    target_directory (str): The directory to which the files should be copied.\n    pattern (str): The pattern to match.\n\n    Requirements:\n    - os\n    - shutil\n    - fnmatch.fnmatch\n    - io\n\n    Example:\n    >>> copy_matching_files('/source_directory', '/target_directory', '*.txt')\n    \"\"\"\n    # Get all files in the source directory\n    files = os.listdir(source_directory)\n\n    # Copy each file that matches the pattern to the target directory\n    for file in files:\n        if fnmatch.fnmatch(file, pattern):\n            shutil.copy(os.path.join(source_directory, file), target_directory)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, shutil, re, sys\n```python\nimport os\nimport shutil\nimport re\nimport sys\n\n# Constants\nDIRECTORY = '/target_directory'\nREGEX = r'^\\d+\\.txt$'\n\ndef rename_matching_files(directory, regex):\n    \"\"\"\n    Renames all files in a given directory that match a certain regular expression by appending '_renamed' to the file name.\n\n    Parameters:\n    directory (str): The directory in which to search for the files.\n    regex (str): The regular expression to match.\n\n    Requirements:\n    - os\n    - shutil\n    - re\n    - sys\n\n    Example:\n    >>> rename_matching_files(DIRECTORY, REGEX)\n    \"\"\"\n    # Compile the regular expression\n    pattern = re.compile(regex)\n\n    # Get all files in the directory\n    files = os.listdir(directory)\n\n    # Rename each file that matches the pattern\n    for file in files:\n        if pattern.match(file):\n            base, ext = os.path.splitext(file)\n            new_name = f'{base}_renamed{ext}'\n            os.rename(os.path.join(directory, file), os.path.join(directory, new_name))\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, shutil, csv, collections\n```python\nimport os\nimport shutil\nimport csv\nfrom collections import defaultdict\n\n# Constants\nDIRECTORY = '/target_directory'\nEXTENSION = '.csv'\n\ndef merge_csv_files(directory, extension):\n    \"\"\"\n    Merges all CSV files in a given directory into a single CSV file.\n\n    Parameters:\n    directory (str): The directory in which to search for the files.\n    extension (str): The extension of the files to merge.\n\n    Requirements:\n    - os\n    - shutil\n    - csv\n    - collections.defaultdict\n\n    Example:\n    >>> merge_csv_files(DIRECTORY, EXTENSION)\n    \"\"\"\n    # Create a dictionary to hold the data\n    data = defaultdict(list)\n\n    # Get all files in the directory\n    files = os.listdir(directory)\n\n    # Read each CSV file and add its data to the dictionary\n    for file in files:\n        if file.endswith(extension):\n            with open(os.path.join(directory, file), 'r') as f:\n                reader = csv.reader(f)\n                for row in reader:\n                    data[row[0]].extend(row[1:])\n\n    # Write the merged data to a new CSV file\n    with open(os.path.join(directory, 'merged.csv'), 'w') as f:\n        writer = csv.writer(f)\n        for key, values in data.items():\n            writer.writerow([key] + values)\n```\n"}
{"seed": "{\"task_id\": 13740672, \"prompt\": \"def f_13740672(data):\\n\\t\", \"suffix\": \"\\n\\treturn data\", \"canonical_solution\": \"\\n    def weekday(i):\\n        if i >=1 and i <= 5: return True\\n        else: return False\\n    data['weekday'] = data['my_dt'].apply(lambda x: weekday(x))\\n\", \"test_start\": \"\\nimport pandas as pd\\n\\ndef check(candidate):\", \"test\": [\"\\n    data = pd.DataFrame([1, 2, 3, 4, 5, 6, 7], columns = ['my_dt'])\\n    data = candidate(data)\\n    assert data['weekday'][5] == False\\n    assert data['weekday'][6] == False\\n    for i in range (0, 5):\\n        assert data['weekday'][i]\\n\"], \"entry_point\": \"f_13740672\", \"intent\": \"create a new column `weekday` in pandas data frame `data` based on the values in column `my_dt`\", \"library\": [\"pandas\"]}", "generation": "Scenario 1: \npandas, pytz, datetime, numpy\n```python\nimport pandas as pd\nimport pytz\nfrom datetime import datetime\nimport numpy as np\n\n# Constants\nWEEKDAY_MAPPING = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n\ndef annotate_weekdays_and_hours(data, column, timezone):\n    \"\"\"\n    Annotate a pandas DataFrame with weekdays and hours based on the datetime values of a specific column \n    for a given timezone.\n    \n    Parameters:\n    data (DataFrame): The input pandas DataFrame.\n    column (str): The column with datetime values.\n    timezone (str): The timezone for the datetime values.\n    \n    Returns:\n    DataFrame: The pandas DataFrame with the new columns 'weekday' and 'hour'.\n    \n    Requirements:\n    - pandas\n    - pytz\n    - datetime\n    - numpy\n    \n    Example:\n    >>> data = pd.DataFrame({'datetime': pd.date_range(start='1/1/2021', end='1/7/2021')})\n    >>> annotated_data = annotate_weekdays_and_hours(data, 'datetime', 'UTC')\n    >>> print(annotated_data)\n    \"\"\"\n    data = data.copy()\n    tz = pytz.timezone(timezone)\n    data['weekday'] = data[column].apply(lambda x: WEEKDAY_MAPPING[x.astimezone(tz).weekday()])\n    data['hour'] = data[column].apply(lambda x: x.astimezone(tz).hour)\n    return data\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 2: \npandas, numpy, random\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import randint\n\n# Constants\nCATEGORIES = ['A', 'B', 'C', 'D', 'E']\nVALUES_RANGE = [0, 100]\n\ndef generate_random_dataframe(rows):\n    \"\"\"\n    Generate a random pandas DataFrame with a specific number of rows. \n    The DataFrame will have a categorical column 'Category' and a numerical column 'Value'.\n    \n    Parameters:\n    rows (int): The number of rows for the DataFrame.\n    \n    Returns:\n    DataFrame: The generated pandas DataFrame.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - random\n    \n    Example:\n    >>> df = generate_random_dataframe(10)\n    >>> print(df)\n    \"\"\"\n    category_data = np.random.choice(CATEGORIES, size=rows)\n    value_data = np.random.uniform(VALUES_RANGE[0], VALUES_RANGE[1], size=rows)\n    df = pd.DataFrame({'Category': category_data, 'Value': value_data})\n    return df\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, matplotlib, numpy\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_histogram(data, column, bins):\n    \"\"\"\n    Plot a histogram of a numerical column of a pandas DataFrame.\n\n    Parameters:\n    data (DataFrame): The pandas DataFrame.\n    column (str): The numerical column for the histogram.\n    bins (int): The number of bins for the histogram.\n\n    Returns:\n    None: This function does not return a value but plots a histogram.\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - numpy\n\n    Example:\n    >>> data = pd.DataFrame({'values': np.random.randn(1000)})\n    >>> plot_histogram(data, 'values', 50)\n    \"\"\"\n    plt.hist(data[column], bins=bins)\n    plt.title(f'Histogram of {column}')\n    plt.xlabel(column)\n    plt.ylabel('Frequency')\n    plt.show()\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, pandas, datetime\n```python\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\n# Constants\nDATE_START = datetime(2021, 1, 1)\nDATE_END = datetime(2021, 12, 31)\n\ndef generate_date_value_dataframe(start=DATE_START, end=DATE_END):\n    \"\"\"\n    Generate a pandas DataFrame with dates between a start and end date, and random values for each date.\n\n    Parameters:\n    start (datetime): The start date.\n    end (datetime): The end date.\n\n    Returns:\n    DataFrame: The generated pandas DataFrame with 'date' and 'value' columns.\n\n    Requirements:\n    - numpy\n    - pandas\n    - datetime\n\n    Example:\n    >>> df = generate_date_value_dataframe()\n    >>> print(df)\n    \"\"\"\n    dates = pd.date_range(start, end)\n    values = np.random.randn(len(dates))\n    df = pd.DataFrame({'date': dates, 'value': values})\n    return df\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnumpy, pandas, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef plot_scatter(data, x_column, y_column):\n    \"\"\"\n    Plot a scatter plot of two columns of a pandas DataFrame.\n\n    Parameters:\n    data (DataFrame): The pandas DataFrame.\n    x_column (str): The column for the x-axis.\n    y_column (str): The column for the y-axis.\n\n    Returns:\n    None: This function does not return a value but plots a scatter plot.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> data = pd.DataFrame({'x': np.random.randn(100), 'y': np.random.randn(100)})\n    >>> plot_scatter(data, 'x', 'y')\n    \"\"\"\n    plt.scatter(data[x_column], data[y_column])\n    plt.title(f'Scatter plot of {x_column} vs {y_column}')\n    plt.xlabel(x_column)\n    plt.ylabel(y_column)\n    plt.show()\n```"}
{"seed": "{\"task_id\": 20950650, \"prompt\": \"def f_20950650(x):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"sorted(x, key=x.get, reverse=True)\", \"test_start\": \"\\nfrom collections import Counter\\n\\ndef check(candidate):\", \"test\": [\"\\n    x = Counter({'blue': 1, 'red': 2, 'green': 3})\\n    assert candidate(x) == ['green', 'red', 'blue']\\n\", \"\\n    x = Counter({'blue': 1.234, 'red': 1.35, 'green': 1.789})\\n    assert candidate(x) == ['green', 'red', 'blue']\\n\", \"\\n    x = Counter({'blue': \\\"b\\\", 'red': \\\"r\\\", 'green': \\\"g\\\"})\\n    assert candidate(x) == ['red', 'green', 'blue']\\n\"], \"entry_point\": \"f_20950650\", \"intent\": \"reverse sort Counter `x` by values\", \"library\": [\"collections\"]}", "generation": "Scenario 1:\ncollections, datetime, pytz, numpy, matplotlib\n```python\nfrom collections import Counter\nimport datetime\nimport pytz\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCITIES = ['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']\n\n# Time zones for the cities\nTIMEZONES = {\n    'New York': 'America/New_York',\n    'London': 'Europe/London',\n    'Beijing': 'Asia/Shanghai',\n    'Tokyo': 'Asia/Tokyo',\n    'Sydney': 'Australia/Sydney'\n}\n\ndef plot_city_visits(visits):\n    \"\"\"\n    Plot a bar chart of the number of visits to different cities (in descending order).\n\n    Parameters:\n    visits (Counter): A Counter object with city names as keys and number of visits as values.\n\n    Returns:\n    None\n\n    Requirements:\n    - collections\n    - pytz\n    - datetime\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> visits = Counter({'New York': 5, 'London': 3, 'Beijing': 8, 'Tokyo': 2, 'Sydney': 6})\n    >>> plot_city_visits(visits)\n    \"\"\"\n    sorted_visits = sorted(visits.items(), key=lambda x: x[1], reverse=True)\n    cities, visit_counts = zip(*sorted_visits)\n\n    plt.bar(cities, visit_counts)\n    plt.xlabel('City')\n    plt.ylabel('Number of visits')\n    plt.title('Number of visits to different cities')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \ncollections, pandas, numpy, matplotlib\n```python\nfrom collections import Counter\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nITEMS = ['apple', 'banana', 'cherry', 'date', 'elderberry']\n\ndef plot_item_sales(sales):\n    \"\"\"\n    Plot a pie chart of sales of different items.\n\n    Parameters:\n    sales (Counter): A Counter object with item names as keys and sales quantities as values.\n\n    Returns:\n    None\n\n    Requirements:\n    - collections\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> sales = Counter({'apple': 10, 'banana': 15, 'cherry': 7, 'date': 13, 'elderberry': 5})\n    >>> plot_item_sales(sales)\n    \"\"\"\n    sorted_sales = sorted(sales.items(), key=lambda x: x[1], reverse=True)\n    items, quantities = zip(*sorted_sales)\n\n    plt.pie(quantities, labels=items, autopct='%1.1f%%')\n    plt.title('Sales of different items')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \ncollections, os, pandas, numpy\n```python\nfrom collections import Counter\nimport os\nimport pandas as pd\nimport numpy as np\n\n# Constants\nFILE_PATH = 'sample_data.csv'\n\ndef count_file_types(directory):\n    \"\"\"\n    Count the number of files of each type in a directory and write the counts to a CSV file.\n\n    Parameters:\n    directory (str): The path of the directory.\n\n    Returns:\n    None\n\n    Requirements:\n    - collections\n    - os\n    - pandas\n    - numpy\n\n    Example:\n    >>> count_file_types('/home/user/documents')\n    \"\"\"\n    file_types = Counter()\n\n    for filename in os.listdir(directory):\n        file_extension = os.path.splitext(filename)[-1]\n        file_types[file_extension] += 1\n\n    df = pd.DataFrame.from_dict(file_types, orient='index', columns=['Count'])\n    df.index.name = 'File Type'\n    df.to_csv(FILE_PATH)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \ncollections, random, string, pandas, numpy\n```python\nfrom collections import Counter\nimport random\nimport string\nimport pandas as pd\nimport numpy as np\n\n# Constants\nLETTERS = string.ascii_letters\n\ndef random_string_frequency(n, k):\n    \"\"\"\n    Generate n random strings of length k, count the frequency of each string, and return a sorted list of tuples.\n\n    Parameters:\n    n (int): The number of random strings to generate.\n    k (int): The length of each random string.\n\n    Returns:\n    List[Tuple[str, int]]: A list of tuples, where each tuple contains a string and its frequency.\n\n    Requirements:\n    - collections\n    - random\n    - string\n    - pandas\n    - numpy\n\n    Example:\n    >>> random_string_frequency(1000, 5)\n    \"\"\"\n    strings = [''.join(random.choices(LETTERS, k=k)) for _ in range(n)]\n    frequencies = Counter(strings)\n\n    sorted_frequencies = sorted(frequencies.items(), key=lambda x: x[1], reverse=True)\n\n    return sorted_frequencies\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \ncollections, datetime, random, pandas\n```python\nfrom collections import Counter\nimport datetime\nimport random\nimport pandas as pd\n\n# Constants\nITEMS = ['apple', 'banana', 'cherry', 'date', 'elderberry']\n\ndef generate_sales_report(n):\n    \"\"\"\n    Generate a sales report for n transactions, where each transaction is the sale of a random item, \n    and return a DataFrame containing the number of sales of each item.\n\n    Parameters:\n    n (int): The number of transactions.\n\n    Returns:\n    DataFrame: A DataFrame containing the item names and their sales quantities.\n\n    Requirements:\n    - collections\n    - datetime\n    - random\n    - pandas\n\n    Example:\n    >>> generate_sales_report(1000)\n    \"\"\"\n    transactions = [random.choice(ITEMS) for _ in range(n)]\n    sales = Counter(transactions)\n\n    df = pd.DataFrame.from_dict(sales, orient='index', columns=['Sales'])\n    df.index.name = 'Item'\n\n    return df\n```"}
{"seed": "{\"task_id\": 20950650, \"prompt\": \"def f_20950650(x):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"sorted(list(x.items()), key=lambda pair: pair[1], reverse=True)\", \"test_start\": \"\\nfrom collections import Counter\\n\\ndef check(candidate):\", \"test\": [\"\\n    x = Counter({'blue': 1, 'red': 2, 'green': 3})\\n    assert candidate(x) == [('green', 3), ('red', 2), ('blue', 1)]\\n\", \"\\n    x = Counter({'blue': 1.234, 'red': 1.35, 'green': 1.789})\\n    assert candidate(x) == [('green', 1.789), ('red', 1.35), ('blue', 1.234)]\\n\", \"\\n    x = Counter({'blue': \\\"b\\\", 'red': \\\"r\\\", 'green': \\\"g\\\"})\\n    assert candidate(x) == [('red', \\\"r\\\"), ('green', \\\"g\\\"), ('blue', \\\"b\\\")]\\n\"], \"entry_point\": \"f_20950650\", \"intent\": \"reverse sort counter `x` by value\", \"library\": [\"collections\"]}", "generation": "Scenario 1: \npandas, collections, matplotlib, numpy, string\n```python\nimport pandas as pd\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport string\n\n# Constants\nLETTERS = list(string.ascii_lowercase)\n\ndef analyze_text(text):\n    \"\"\"\n    Analyze the given text and return a DataFrame containing the frequency of each letter \n    in the text, sorted in descending order. Also plot a bar chart of the letter frequencies.\n\n    Parameters:\n    text (str): The text to analyze.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the frequency of each letter.\n\n    Requirements:\n    - pandas\n    - collections\n    - matplotlib.pyplot\n    - numpy\n    - string\n\n    Example:\n    >>> text = \"hello world\"\n    >>> df = analyze_text(text)\n    >>> print(df)\n    >>> df.plot(kind='bar', x='Letter', y='Frequency')\n    \"\"\"\n    text = text.lower()\n    counter = Counter(text)\n\n    data = []\n    for letter in LETTERS:\n        data.append([letter, counter[letter]])\n\n    df = pd.DataFrame(data, columns=['Letter', 'Frequency'])\n    df = df.sort_values(by='Frequency', ascending=False)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncollections, os, glob, pathlib\n```python\nfrom collections import Counter\nimport os\nimport glob\nfrom pathlib import Path\n\ndef count_file_types(directory):\n    \"\"\"\n    Count the number of each type of file in a directory, sorted in \n    descending order by count.\n\n    Parameters:\n    directory (str): The directory to analyze.\n\n    Returns:\n    list: A list of tuples, each containing a file extension and its count.\n\n    Requirements:\n    - collections\n    - os\n    - glob\n    - pathlib\n\n    Example:\n    >>> count_file_types('/path/to/directory')\n    \"\"\"\n    files = glob.glob(os.path.join(directory, \"*\"))\n    counter = Counter()\n\n    for file in files:\n        extension = Path(file).suffix\n        counter[extension] += 1\n\n    sorted_counter = sorted(counter.items(), key=lambda pair: pair[1], reverse=True)\n\n    return sorted_counter\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ncollections, matplotlib, numpy, random\n```python\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\n\n# Constants\nNUMBERS = list(range(1, 11))\n\ndef simulate_dice_rolls(n):\n    \"\"\"\n    Simulate rolling a 10-sided die n times, and plot a histogram of the results.\n\n    Parameters:\n    n (int): The number of times to roll the die.\n\n    Requirements:\n    - collections\n    - matplotlib.pyplot\n    - numpy\n    - random\n\n    Example:\n    >>> simulate_dice_rolls(1000)\n    \"\"\"\n    results = [random.choice(NUMBERS) for _ in range(n)]\n    counter = Counter(results)\n    frequencies = [counter[number] for number in NUMBERS]\n\n    plt.bar(NUMBERS, frequencies)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ncollections, json, requests, bs4\n```python\nfrom collections import Counter\nimport json\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef count_words_on_webpage(url):\n    \"\"\"\n    Count the frequency of each word on a webpage, sorted in descending order by count.\n\n    Parameters:\n    url (str): The URL of the webpage to analyze.\n\n    Returns:\n    list: A list of tuples, each containing a word and its count.\n\n    Requirements:\n    - collections\n    - json\n    - requests\n    - bs4\n\n    Example:\n    >>> count_words_on_webpage('https://www.example.com')\n    \"\"\"\n    response = requests.get(url)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    text = soup.get_text()\n    words = text.split()\n    counter = Counter(words)\n    sorted_counter = sorted(counter.items(), key=lambda pair: pair[1], reverse=True)\n\n    return sorted_counter\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\ncollections, pandas, matplotlib, os\n```python\nfrom collections import Counter\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\n\ndef analyze_file_sizes(directory):\n    \"\"\"\n    Analyze the sizes of files in a directory and return a DataFrame containing the \n    size of each file, sorted in descending order. Also plot a histogram of the file sizes.\n\n    Parameters:\n    directory (str): The directory to analyze.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the size of each file.\n\n    Requirements:\n    - collections\n    - pandas\n    - matplotlib.pyplot\n    - os\n\n    Example:\n    >>> df = analyze_file_sizes('/path/to/directory')\n    >>> print(df)\n    >>> df.plot(kind='hist', bins=30)\n    \"\"\"\n    sizes = []\n    for file in os.listdir(directory):\n        size = os.path.getsize(os.path.join(directory, file))\n        sizes.append([file, size])\n\n    df = pd.DataFrame(sizes, columns=['File', 'Size'])\n    df = df.sort_values(by='Size', ascending=False)\n\n    return df\n```"}
{"seed": "{\"task_id\": 9775297, \"prompt\": \"def f_9775297(a, b):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"np.vstack((a, b))\", \"test_start\": \"\\nimport numpy as np\\n\\ndef check(candidate):\", \"test\": [\"\\n    a = np.array([[1, 2, 3], [4, 5, 6]])\\n    b = np.array([[9, 8, 7], [6, 5, 4]])\\n    assert np.array_equal(candidate(a, b), np.array([[1, 2, 3], [4, 5, 6], [9, 8, 7], [6, 5, 4]]))\\n\", \"\\n    a = np.array([[1, 2.45, 3], [4, 0.55, 612]])\\n    b = np.array([[988, 8, 7], [6, 512, 4]])\\n    assert np.array_equal(candidate(a, b), np.array([[1, 2.45, 3], [4, 0.55, 612], [988, 8, 7], [6, 512, 4]]))\\n\"], \"entry_point\": \"f_9775297\", \"intent\": \"append a numpy array 'b' to a numpy array 'a'\", \"library\": [\"numpy\"]}", "generation": "Scenario 1: \npandas, numpy, sklearn.preprocessing, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\n# Constants\nFEATURES = ['Age', 'Height', 'Weight', 'BP', 'Cholesterol']\n\ndef scale_and_plot(df, features):\n    \"\"\"\n    Standardize the given features in a DataFrame and then plot the data for each feature.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame containing the data.\n    features (list): The list of features to standardize and plot.\n\n    Returns:\n    DataFrame: The DataFrame with standardized features.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.preprocessing.StandardScaler\n    - matplotlib.pyplot\n\n    Example:\n    >>> data = {'Age': [25, 30, 35, 40], 'Height': [170, 175, 180, 185], 'Weight': [70, 75, 80, 85], 'BP': [120, 125, 130, 135], 'Cholesterol': [200, 205, 210, 215]}\n    >>> df = pd.DataFrame(data)\n    >>> df = scale_and_plot(df, FEATURES)\n    >>> print(df)\n    \"\"\"\n    scaler = StandardScaler()\n\n    df[features] = scaler.fit_transform(df[features])\n\n    for feature in features:\n        plt.figure()\n        plt.plot(df[feature])\n        plt.title(f'{feature} Plot')\n        plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, scipy.spatial, matplotlib\n```python\nimport numpy as np\nfrom scipy.spatial import ConvexHull\nimport matplotlib.pyplot as plt\n\ndef plot_convex_hull(points):\n    \"\"\"\n    Compute the convex hull of a set of points in the plane, and plot it.\n\n    Parameters:\n    points (ndarray): An array of points in the plane.\n\n    Returns:\n    ConvexHull: The ConvexHull object representing the convex hull of the points.\n\n    Requirements:\n    - numpy\n    - scipy.spatial.ConvexHull\n    - matplotlib.pyplot\n\n    Example:\n    >>> points = np.random.rand(30, 2)   # 30 random points in 2-D\n    >>> hull = plot_convex_hull(points)\n    \"\"\"\n    hull = ConvexHull(points)\n\n    plt.plot(points[:,0], points[:,1], 'o')\n    for simplex in hull.simplices:\n        plt.plot(points[simplex, 0], points[simplex, 1], 'k-')\n\n    plt.show()\n\n    return hull\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, sklearn.decomposition, matplotlib\n```python\nimport numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\ndef perform_pca(X, n_components):\n    \"\"\"\n    Perform PCA on a given dataset and plot the explained variance ratio.\n\n    Parameters:\n    X (ndarray): The data to perform PCA on.\n    n_components (int): The number of components to keep.\n\n    Returns:\n    PCA: The PCA object with the result of the fit.\n\n    Requirements:\n    - numpy\n    - sklearn.decomposition.PCA\n    - matplotlib.pyplot\n\n    Example:\n    >>> X = np.random.rand(100, 10)   # 100 samples with 10 features each\n    >>> pca = perform_pca(X, 2)\n    \"\"\"\n    pca = PCA(n_components=n_components)\n    pca.fit(X)\n\n    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n    plt.xlabel('number of components')\n    plt.ylabel('cumulative explained variance')\n    plt.show()\n\n    return pca\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, scipy.stats, matplotlib\n```python\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef plot_normal_distribution(mean, std_dev):\n    \"\"\"\n    Plot a normal distribution with a given mean and standard deviation.\n\n    Parameters:\n    mean (float): The mean of the normal distribution.\n    std_dev (float): The standard deviation of the normal distribution.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_normal_distribution(0, 1)\n    \"\"\"\n    x = np.linspace(mean - 3*std_dev, mean + 3*std_dev, 100)\n    y = stats.norm.pdf(x, mean, std_dev)\n\n    plt.plot(x, y)\n\n    plt.title('Normal Distribution')\n    plt.xlabel('x')\n    plt.ylabel('pdf(x)')\n\n    plt.grid(True)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, scipy.optimize, matplotlib\n```python\nimport numpy as np\nfrom scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\n\n# Define the form of the function we want to fit\ndef func(x, a, b, c):\n    return a * np.exp(-b * x) + c\n\ndef fit_and_plot(xdata, ydata):\n    \"\"\"\n    Fit a curve to the given data and plot the data and the fitted curve.\n\n    Parameters:\n    xdata (ndarray): The x-values of the data.\n    ydata (ndarray): The y-values of the data.\n\n    Returns:\n    tuple: The optimized parameters.\n\n    Requirements:\n    - numpy\n    - scipy.optimize.curve_fit\n    - matplotlib.pyplot\n\n    Example:\n    >>> xdata = np.linspace(0, 4, 50)\n    >>> y = func(xdata, 2.5, 1.3, 0.5)\n    >>> ydata = y + 0.2 * np.random.normal(size=len(xdata))  # add noise\n    >>> popt = fit_and_plot(xdata, ydata)\n    \"\"\"\n    popt, pcov = curve_fit(func, xdata, ydata)\n\n    plt.figure()\n    plt.plot(xdata, ydata, 'b-', label='data')\n    plt.plot(xdata, func(xdata, *popt), 'r-', label='fit: a=%5.3f, b=%5.3f, c=%5.3f' % tuple(popt))\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.legend()\n    plt.show()\n\n    return popt\n```\n"}
{"seed": "{\"task_id\": 21887754, \"prompt\": \"def f_21887754(a, b):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"np.concatenate((a, b), axis=0)\", \"test_start\": \"\\nimport numpy as np\\n\\ndef check(candidate):\", \"test\": [\"\\n    a = np.array([[1, 5, 9], [2, 6, 10]])\\n    b = np.array([[3, 7, 11], [4, 8, 12]])\\n    assert np.array_equal(candidate(a, b), np.array([[1, 5, 9], [2, 6, 10], [3, 7, 11], [4, 8, 12]]))\\n\", \"\\n    a = np.array([[1, 2.45, 3], [4, 0.55, 612]])\\n    b = np.array([[988, 8, 7], [6, 512, 4]])\\n    assert np.array_equal(candidate(a, b), np.array([[1, 2.45, 3], [4, 0.55, 612], [988, 8, 7], [6, 512, 4]]))\\n\"], \"entry_point\": \"f_21887754\", \"intent\": \"numpy concatenate two arrays `a` and `b` along the first axis\", \"library\": [\"numpy\"]}", "generation": "Scenario 1: \nnumpy, pandas, sklearn.preprocessing, random\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom random import randint\n\n# Constants\nDATA = pd.DataFrame({'Age': [20, 25, 31, 35, 40], \n                     'Salary': [3000, 3500, 4000, 4200, 4500]})\n\ndef concatenate_and_standardize(a, b):\n    \"\"\"\n    Concatenate two numpy arrays along the first axis, standardize the resulting array using \n    sklearn's StandardScaler, and add a random column to the data.\n\n    Parameters:\n    a, b (numpy.ndarray): The input arrays to concatenate.\n\n    Returns:\n    DataFrame: A pandas DataFrame of the standardized array with an added random column.\n\n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.preprocessing\n    - random\n\n    Example:\n    >>> a = np.array([[1, 5], [2, 6]])\n    >>> b = np.array([[3, 7], [4, 8]])\n    >>> print(concatenate_and_standardize(a, b))\n    \"\"\"\n    concatenated_array = np.concatenate((a, b), axis=0)\n    scaler = StandardScaler()\n    standardized_array = scaler.fit_transform(concatenated_array)\n    random_column = np.array([randint(0, 10) for _ in range(standardized_array.shape[0])])\n    df = pd.DataFrame(np.column_stack((standardized_array, random_column)), columns=list(DATA.columns) + ['Random'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, matplotlib, seaborn, pandas\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n# Constants\nCOLUMNS = ['Score1', 'Score2']\n\ndef concatenate_and_visualize(a, b):\n    \"\"\"\n    Concatenate two numpy arrays along the first axis, convert the resulting array to a pandas DataFrame, \n    and visualize the distribution of the DataFrame's columns.\n\n    Parameters:\n    a, b (numpy.ndarray): The input arrays to concatenate.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - matplotlib\n    - seaborn\n    - pandas\n\n    Example:\n    >>> a = np.array([[1, 5], [2, 6]])\n    >>> b = np.array([[3, 7], [4, 8]])\n    >>> concatenate_and_visualize(a, b)\n    \"\"\"\n    concatenated_array = np.concatenate((a, b), axis=0)\n    df = pd.DataFrame(concatenated_array, columns=COLUMNS)\n\n    for column in df.columns:\n        plt.figure(figsize=(10, 5))\n        sns.histplot(df[column])\n        plt.title(f'Distribution of {column}')\n        plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, pandas, sklearn.decomposition, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['Feature1', 'Feature2', 'Feature3', 'Feature4']\n\ndef concatenate_and_pca(a, b):\n    \"\"\"\n    Concatenate two numpy arrays along the first axis, convert the resulting array to a pandas DataFrame, \n    perform PCA on the DataFrame, and plot the explained variance ratio.\n\n    Parameters:\n    a, b (numpy.ndarray): The input arrays to concatenate.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.decomposition\n    - matplotlib\n\n    Example:\n    >>> a = np.array([[1, 5, 9, 13], [2, 6, 10, 14]])\n    >>> b = np.array([[3, 7, 11, 15], [4, 8, 12, 16]])\n    >>> concatenate_and_pca(a, b)\n    \"\"\"\n    concatenated_array = np.concatenate((a, b), axis=0)\n    df = pd.DataFrame(concatenated_array, columns=COLUMNS)\n\n    pca = PCA()\n    pca.fit(df)\n\n    plt.figure(figsize=(10, 5))\n    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n    plt.xlabel('Number of Components')\n    plt.ylabel('Cumulative Explained Variance')\n    plt.title('Explained Variance by Components')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, pandas, sklearn.preprocessing, seaborn\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport seaborn as sns\n\n# Constants\nCOLUMNS = ['Age', 'Salary']\n\ndef concatenate_and_normalize(a, b):\n    \"\"\"\n    Concatenate two numpy arrays along the first axis, convert the resulting array to a pandas DataFrame, \n    normalize the DataFrame using sklearn's MinMaxScaler, and visualize the distribution of the columns.\n\n    Parameters:\n    a, b (numpy.ndarray): The input arrays to concatenate.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.preprocessing\n    - seaborn\n\n    Example:\n    >>> a = np.array([[20, 3000], [25, 3500]])\n    >>> b = np.array([[31, 4000], [35, 4200]])\n    >>> concatenate_and_normalize(a, b)\n    \"\"\"\n    concatenated_array = np.concatenate((a, b), axis=0)\n    df = pd.DataFrame(concatenated_array, columns=COLUMNS)\n\n    scaler = MinMaxScaler()\n    df[COLUMNS] = scaler.fit_transform(df[COLUMNS])\n\n    for column in df.columns:\n        sns.histplot(df[column], bins=10)\n        plt.title(f'Distribution of {column}')\n        plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, pandas, sklearn.cluster, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['X', 'Y']\n\ndef concatenate_and_cluster(a, b):\n    \"\"\"\n    Concatenate two numpy arrays along the first axis, convert the resulting array to a pandas DataFrame, \n    perform KMeans clustering on the DataFrame, and plot the data points with their cluster assignments.\n\n    Parameters:\n    a, b (numpy.ndarray): The input arrays to concatenate.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.cluster\n    - matplotlib\n\n    Example:\n    >>> a = np.array([[1, 5], [2, 6]])\n    >>> b = np.array([[3, 7], [4, 8]])\n    >>> concatenate_and_cluster(a, b)\n    \"\"\"\n    concatenated_array = np.concatenate((a, b), axis=0)\n    df = pd.DataFrame(concatenated_array, columns=COLUMNS)\n\n    kmeans = KMeans(n_clusters=2)\n    df['Cluster'] = kmeans.fit_predict(df)\n\n    plt.figure(figsize=(10, 5))\n    plt.scatter(df['X'], df['Y'], c=df['Cluster'])\n    plt.title('Cluster Assignments')\n    plt.show()\n```"}
{"seed": "{\"task_id\": 21887754, \"prompt\": \"def f_21887754(a, b):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"np.concatenate((a, b), axis=1)\", \"test_start\": \"\\nimport numpy as np\\n\\ndef check(candidate):\", \"test\": [\"\\n    a = np.array([[1, 5, 9], [2, 6, 10]])\\n    b = np.array([[3, 7, 11], [4, 8, 12]])\\n    assert np.array_equal(candidate(a, b), np.array([[1, 5, 9, 3, 7, 11], [2, 6, 10, 4, 8, 12]]))\\n\", \"\\n    a = np.array([[1, 2.45, 3], [4, 0.55, 612]])\\n    b = np.array([[988, 8, 7], [6, 512, 4]])\\n    assert np.array_equal(candidate(a, b), np.array([[1, 2.45, 3, 988, 8, 7], [4, 0.55, 612, 6, 512, 4]]))\\n\"], \"entry_point\": \"f_21887754\", \"intent\": \"numpy concatenate two arrays `a` and `b` along the second axis\", \"library\": [\"numpy\"]}", "generation": "Scenario 1:\nnumpy, scipy.stats, matplotlib, pandas\n```python\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants\nCOLORS = ['red', 'green', 'blue', 'yellow', 'purple']\n\ndef plot_bivariate_distribution(a, b, group_labels):\n    \"\"\"\n    Concatenate two numpy arrays along the second axis, calculate the bivariate \n    distribution, and plot it with different colors for different groups.\n    \n    Parameters:\n    a (numpy.ndarray): The first numpy array.\n    b (numpy.ndarray): The second numpy array.\n    group_labels (list): The labels for different groups.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n    - pandas\n    \n    Example:\n    >>> a = np.array([[1, 2], [3, 4], [5, 6]])\n    >>> b = np.array([[7, 8], [9, 10], [11, 12]])\n    >>> group_labels = ['Group 1', 'Group 2', 'Group 3']\n    >>> plot_bivariate_distribution(a, b, group_labels)\n    \"\"\"\n    # Concatenate the arrays\n    combined = np.concatenate((a, b), axis=1)\n    \n    # Create a DataFrame\n    df = pd.DataFrame(combined, columns=['x1', 'x2', 'y1', 'y2'])\n    \n    # Add group labels\n    df['Group'] = group_labels\n    \n    # Calculate bivariate distribution for each group and plot\n    for i, group in enumerate(df['Group'].unique()):\n        group_data = df[df['Group'] == group]\n        kde = stats.gaussian_kde(group_data[['x1', 'x2', 'y1', 'y2']].T)\n        x, y = np.mgrid[0:10:100j, 0:10:100j]\n        positions = np.vstack([x.ravel(), y.ravel()])\n        z = np.reshape(kde(positions).T, x.shape)\n        plt.contour(x, y, z, colors=COLORS[i])\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, pandas, sklearn.linear_model, sklearn.model_selection\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\ndef fit_linear_regression(a, b):\n    \"\"\"\n    Concatenate two numpy arrays along the second axis, split the data into \n    training and testing sets, and fit a linear regression model.\n    \n    Parameters:\n    a (numpy.ndarray): The first numpy array.\n    b (numpy.ndarray): The second numpy array.\n    \n    Returns:\n    float: The R-squared score of the model on the testing set.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.linear_model\n    - sklearn.model_selection\n    \n    Example:\n    >>> a = np.array([[1, 2], [3, 4], [5, 6]])\n    >>> b = np.array([[7], [9], [11]])\n    >>> score = fit_linear_regression(a, b)\n    \"\"\"\n    # Concatenate the arrays\n    combined = np.concatenate((a, b), axis=1)\n    \n    # Create a DataFrame\n    df = pd.DataFrame(combined, columns=['x1', 'x2', 'y'])\n    \n    # Split into features and target\n    X = df[['x1', 'x2']]\n    y = df['y']\n    \n    # Split into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    # Fit the model\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    \n    # Calculate R-squared score\n    score = model.score(X_test, y_test)\n    \n    return score\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, pandas, seaborn, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef create_heatmap(a, b):\n    \"\"\"\n    Concatenate two numpy arrays along the second axis and create a heatmap of \n    the correlation matrix.\n    \n    Parameters:\n    a (numpy.ndarray): The first numpy array.\n    b (numpy.ndarray): The second numpy array.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - numpy\n    - pandas\n    - seaborn\n    - matplotlib.pyplot\n    \n    Example:\n    >>> a = np.array([[1, 2], [3, 4], [5, 6]])\n    >>> b = np.array([[7, 8], [9, 10], [11, 12]])\n    >>> create_heatmap(a, b)\n    \"\"\"\n    # Concatenate the arrays\n    combined = np.concatenate((a, b), axis=1)\n    \n    # Create a DataFrame\n    df = pd.DataFrame(combined, columns=['x1', 'x2', 'y1', 'y2'])\n    \n    # Calculate the correlation matrix\n    corr = df.corr()\n    \n    # Create a heatmap\n    sns.heatmap(corr, annot=True, cmap='coolwarm')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, pandas, sklearn.preprocessing\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef min_max_scaling(a, b):\n    \"\"\"\n    Concatenate two numpy arrays along the second axis and perform min-max \n    scaling on the data.\n    \n    Parameters:\n    a (numpy.ndarray): The first numpy array.\n    b (numpy.ndarray): The second numpy array.\n    \n    Returns:\n    numpy.ndarray: The scaled data.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.preprocessing\n    \n    Example:\n    >>> a = np.array([[1, 2], [3, 4], [5, 6]])\n    >>> b = np.array([[7, 8], [9, 10], [11, 12]])\n    >>> scaled_data = min_max_scaling(a, b)\n    \"\"\"\n    # Concatenate the arrays\n    combined = np.concatenate((a, b), axis=1)\n    \n    # Create a DataFrame\n    df = pd.DataFrame(combined, columns=['x1', 'x2', 'y1', 'y2'])\n    \n    # Perform min-max scaling\n    scaler = MinMaxScaler()\n    scaled_data = scaler.fit_transform(df)\n    \n    return scaled_data\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, pandas, sklearn.decomposition, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\ndef perform_pca(a, b):\n    \"\"\"\n    Concatenate two numpy arrays along the second axis and perform PCA on the data.\n    \n    Parameters:\n    a (numpy.ndarray): The first numpy array.\n    b (numpy.ndarray): The second numpy array.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.decomposition\n    - matplotlib.pyplot\n    \n    Example:\n    >>> a = np.array([[1, 2], [3, 4], [5, 6]])\n    >>> b = np.array([[7, 8], [9, 10], [11, 12]])\n    >>> perform_pca(a, b)\n    \"\"\"\n    # Concatenate the arrays\n    combined = np.concatenate((a, b), axis=1)\n    \n    # Create a DataFrame\n    df = pd.DataFrame(combined, columns=['x1', 'x2', 'y1', 'y2'])\n    \n    # Perform PCA\n    pca = PCA(n_components=2)\n    pca_result = pca.fit_transform(df)\n    \n    # Plot the result\n    plt.scatter(pca_result[:, 0], pca_result[:, 1])\n    plt.show()\n```\n"}
{"seed": "{\"task_id\": 21887754, \"prompt\": \"def f_21887754(a, b):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"np.r_[(a[None, :], b[None, :])]\", \"test_start\": \"\\nimport numpy as np\\n\\ndef check(candidate):\", \"test\": [\"\\n    a = np.array([[1, 5, 9], [2, 6, 10]])\\n    b = np.array([[3, 7, 11], [4, 8, 12]])\\n    assert np.array_equal(candidate(a, b), np.array([[[1, 5, 9], [2, 6, 10]], [[3, 7, 11], [4, 8, 12]]]))\\n\", \"\\n    a = np.array([[1, 2.45, 3], [4, 0.55, 612]])\\n    b = np.array([[988, 8, 7], [6, 512, 4]])\\n    assert np.array_equal(candidate(a, b), np.array([[[1, 2.45, 3], [4, 0.55, 612]], [[988, 8 , 7], [6, 512, 4]]]))\\n\"], \"entry_point\": \"f_21887754\", \"intent\": \"numpy concatenate two arrays `a` and `b` along the first axis\", \"library\": [\"numpy\"]}", "generation": "Scenario 1: \nnumpy, pandas, sklearn.preprocessing, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\ndef visualize_standardized_data(df):\n    \"\"\"\n    Standardize the numerical columns of a DataFrame and create a boxplot for each column.\n\n    Parameters:\n    df (pandas.DataFrame): The DataFrame to be standardized.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.preprocessing.StandardScaler\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame({'A': [1, 2, 3, 4, 5], 'B': [10, 20, 30, 40, 50]})\n    >>> visualize_standardized_data(df)\n    \"\"\"\n    scaler = StandardScaler()\n    df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n\n    for column in df_scaled.columns:\n        plt.figure()\n        plt.boxplot(df_scaled[column])\n        plt.title(f'Boxplot for {column}')\n        plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nnumpy, pandas, sklearn.decomposition, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLORS = ['r', 'g', 'b', 'c', 'm', 'y', 'k']\n\ndef visualize_pca(df, target_column):\n    \"\"\"\n    Perform Principal Component Analysis (PCA) on a DataFrame and visualize the first two\n    principal components colored by the target column.\n\n    Parameters:\n    df (pandas.DataFrame): The DataFrame to be processed.\n    target_column (str): The column name of the target variable.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.decomposition.PCA\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame({'A': [1, 2, 3, 4, 5], 'B': [10, 20, 30, 40, 50], 'target': ['red', 'red', 'blue', 'blue', 'red']})\n    >>> visualize_pca(df, 'target')\n    \"\"\"\n    features = df.drop(target_column, axis=1)\n    target = df[target_column]\n\n    pca = PCA(n_components=2)\n    principal_components = pca.fit_transform(features)\n\n    plt.figure()\n    for i, target in enumerate(np.unique(target)):\n        plt.scatter(principal_components[target==target, 0],\n                    principal_components[target==target, 1],\n                    s=50, color=COLORS[i], label=target)\n    plt.legend()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnumpy, pandas, seaborn, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nHEATMAP_TITLE = 'Correlation Heatmap'\n\ndef plot_correlation_heatmap(df):\n    \"\"\"\n    Calculate the correlation matrix of a DataFrame and plot a heatmap.\n\n    Parameters:\n    df (pandas.DataFrame): The DataFrame to be processed.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - pandas\n    - seaborn\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame({'A': [1, 2, 3, 4, 5], 'B': [10, 20, 30, 40, 50], 'C': [5, 10, 15, 20, 25]})\n    >>> plot_correlation_heatmap(df)\n    \"\"\"\n    correlation_matrix = df.corr()\n\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm')\n    plt.title(HEATMAP_TITLE)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, scipy.stats, matplotlib\n```python\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\n# Constants\nHIST_BINS = 30\n\ndef plot_normal_distribution(mu, sigma):\n    \"\"\"\n    Plot a histogram of a normal distribution with given mean and standard deviation.\n\n    Parameters:\n    mu (float): The mean of the distribution.\n    sigma (float): The standard deviation of the distribution.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_normal_distribution(0, 1)\n    \"\"\"\n    x = np.random.normal(mu, sigma, 1000)\n    plt.hist(x, HIST_BINS, density=True)\n\n    x = np.linspace(mu - 3.5*sigma, mu + 3.5*sigma, 100)\n    y = stats.norm.pdf(x, mu, sigma)\n    plt.plot(x, y, color='red')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnumpy, pandas, sklearn.cluster, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\ndef perform_kmeans_clustering(df, n_clusters):\n    \"\"\"\n    Perform KMeans clustering on a DataFrame and plot the clusters.\n\n    Parameters:\n    df (pandas.DataFrame): The DataFrame to be processed.\n    n_clusters (int): The number of clusters.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.cluster.KMeans\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame({'A': [1, 2, 3, 4, 5], 'B': [10, 20, 30, 40, 50]})\n    >>> perform_kmeans_clustering(df, 2)\n    \"\"\"\n    kmeans = KMeans(n_clusters=n_clusters)\n    kmeans.fit(df)\n\n    plt.scatter(df.iloc[:, 0], df.iloc[:, 1], c=kmeans.labels_, cmap='viridis')\n    plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red')\n    plt.show()\n```"}
{"seed": "{\"task_id\": 21887754, \"prompt\": \"def f_21887754(a, b):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"np.array((a, b))\", \"test_start\": \"\\nimport numpy as np\\n\\ndef check(candidate):\", \"test\": [\"\\n    a = np.array([[1, 5, 9], [2, 6, 10]])\\n    b = np.array([[3, 7, 11], [4, 8, 12]])\\n    assert np.array_equal(candidate(a, b), np.array([[[1, 5, 9], [2, 6, 10]], [[3, 7, 11], [4, 8, 12]]]))\\n\", \"\\n    a = np.array([[1, 2.45, 3], [4, 0.55, 612]])\\n    b = np.array([[988, 8, 7], [6, 512, 4]])\\n    assert np.array_equal(candidate(a, b), np.array([[[1, 2.45, 3], [4, 0.55, 612]], [[988, 8 , 7], [6, 512, 4]]]))\\n\"], \"entry_point\": \"f_21887754\", \"intent\": \"numpy concatenate two arrays `a` and `b` along the first axis\", \"library\": [\"numpy\"]}", "generation": "Scenario 1:\nnumpy, pandas, matplotlib, random\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nSTOCKS = ['GOOG', 'AAPL', 'MSFT', 'TSLA', 'AMZN']\nDATES = pd.date_range(start='01-01-2020', end='12-31-2020')\n\ndef generate_stock_prices(stocks, dates):\n    \"\"\"\n    Generate a pandas DataFrame that contains random stock prices for a list of stocks \n    over the specified dates.\n    \n    Parameters:\n    stocks (list): The list of stock symbols.\n    dates (DatetimeIndex): The dates for which to generate stock prices.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the stock prices.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> prices = generate_stock_prices(STOCKS, DATES)\n    >>> print(prices)\n    >>> prices.plot()\n    \"\"\"\n    prices_data = []\n\n    for stock in stocks:\n        prices = np.random.randint(150, 1000, len(dates))\n        prices_data.append(prices)\n\n    prices_df = pd.DataFrame(np.array(prices_data).T, columns=stocks, index=dates)\n\n    return prices_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, pandas, matplotlib, sklearn\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n# Constants\nYEARS = np.array(range(2000, 2021))\n\ndef predict_future_population(years, population):\n    \"\"\"\n    Predict the future population based on past data using simple linear regression.\n    \n    Parameters:\n    years (ndarray): The years for which population data is available.\n    population (ndarray): The population data for the corresponding years.\n    \n    Returns:\n    float: The predicted population for the next year.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.linear_model\n    \n    Example:\n    >>> population = np.random.randint(7000000000, 8000000000, len(YEARS))\n    >>> predict_future_population(YEARS, population)\n    \"\"\"\n    years = years.reshape(-1, 1)\n    population = population.reshape(-1, 1)\n\n    model = LinearRegression()\n    model.fit(years, population)\n\n    next_year = np.array([years[-1][0] + 1]).reshape(-1, 1)\n    predicted_population = model.predict(next_year)\n\n    plt.scatter(years, population, color='blue')\n    plt.plot(years, model.predict(years), color='red')\n    plt.show()\n\n    return predicted_population[0][0]\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, matplotlib, scipy\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.fftpack import fft\n\n# Constants\nN = 600\nT = 1.0 / 800.0\nx = np.linspace(0.0, N*T, N)\n\ndef plot_fft(x, y):\n    \"\"\"\n    Plot the Fast Fourier Transform (FFT) of a given signal.\n    \n    Parameters:\n    x (ndarray): The x values of the signal.\n    y (ndarray): The y values of the signal.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - scipy.fftpack\n    \n    Example:\n    >>> y = np.sin(50.0 * 2.0*np.pi*x) + 0.5*np.sin(80.0 * 2.0*np.pi*x)\n    >>> plot_fft(x, y)\n    \"\"\"\n    yf = fft(y)\n    xf = np.linspace(0.0, 1.0/(2.0*T), N//2)\n    plt.plot(xf, 2.0/N * np.abs(yf[0:N//2]))\n    plt.grid()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, pandas, seaborn, sklearn\n```python\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\n\n# Constants\nIRIS = load_iris()\n\ndef plot_iris_dataset():\n    \"\"\"\n    Load the iris dataset, convert it to a pandas DataFrame, and create a pairplot.\n    \n    Parameters:\n    None\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the iris data.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - seaborn\n    - sklearn.datasets\n    \n    Example:\n    >>> df = plot_iris_dataset()\n    >>> print(df)\n    \"\"\"\n    iris_df = pd.DataFrame(data= np.c_[IRIS['data'], IRIS['target']],\n                           columns= IRIS['feature_names'] + ['target'])\n    sns.pairplot(iris_df, hue='target')\n    plt.show()\n\n    return iris_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, matplotlib, random, scipy\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\nfrom scipy.stats import norm\n\n# Constants\nSAMPLE_SIZE = 1000\n\ndef plot_normal_distribution(mean, std_dev):\n    \"\"\"\n    Generate a random sample from a normal distribution with the specified mean and standard deviation, \n    plot a histogram of the sample, and overlay a probability density function.\n    \n    Parameters:\n    mean (float): The mean of the normal distribution.\n    std_dev (float): The standard deviation of the normal distribution.\n    \n    Returns:\n    ndarray: The random sample.\n    \n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - random\n    - scipy.stats\n    \n    Example:\n    >>> sample = plot_normal_distribution(0, 1)\n    \"\"\"\n    sample = np.random.normal(mean, std_dev, SAMPLE_SIZE)\n    plt.hist(sample, bins=30, density=True, alpha=0.6, color='g')\n\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, mean, std_dev)\n    plt.plot(x, p, 'k', linewidth=2)\n    plt.show()\n\n    return sample\n```"}
{"seed": "{\"task_id\": 2805231, \"prompt\": \"def f_2805231():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"socket.getaddrinfo('google.com', 80)\", \"test_start\": \"\\nimport socket\\n\\ndef check(candidate):\", \"test\": [\"\\n    res = candidate()\\n    assert all([(add[4][1] == 80) for add in res])\\n\"], \"entry_point\": \"f_2805231\", \"intent\": \"fetch address information for host 'google.com' ion port 80\", \"library\": [\"socket\"]}", "generation": "Scenario 1: \nsocket, urllib, ssl, json\n```python\nimport socket\nimport urllib.request\nimport ssl\nimport json\n\n# Constants\nUSER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537'\nHEADERS = {'User-Agent': USER_AGENT}\n\ndef fetch_ssl_cert_info_and_http_headers(host, port):\n    \"\"\"\n    Fetch SSL certificate information and HTTP headers for a given host and port.\n\n    Parameters:\n    host (str): The hostname.\n    port (int): The port number.\n\n    Returns:\n    dict: A dictionary containing the SSL certificate information and HTTP headers.\n\n    Requirements:\n    - socket\n    - urllib.request\n    - ssl\n    - json\n\n    Example:\n    >>> fetch_ssl_cert_info_and_http_headers('google.com', 443)\n    \"\"\"\n    # Fetch SSL certificate\n    ctx = ssl.create_default_context()\n    with ctx.wrap_socket(socket.socket(), server_hostname=host) as s:\n        s.connect((host, port))\n        cert = s.getpeercert()\n\n    # Fetch HTTP headers\n    req = urllib.request.Request(f\"https://{host}:{port}\", headers=HEADERS)\n    response = urllib.request.urlopen(req)\n    headers = dict(response.getheaders())\n\n    return {\n        'ssl_certificate': cert,\n        'http_headers': headers\n    }\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nsocket, os, requests, bs4\n```python\nimport socket\nimport os\nimport requests\nfrom bs4 import BeautifulSoup\n\n# Constants\nDOWNLOAD_DIR = '/path/to/download/directory'\n\ndef download_all_images_from_website(host, port):\n    \"\"\"\n    Download all images from a website specified by a given host and port.\n\n    Parameters:\n    host (str): The hostname.\n    port (int): The port number.\n\n    Returns:\n    int: The number of images downloaded.\n\n    Requirements:\n    - socket\n    - os\n    - requests\n    - bs4\n\n    Example:\n    >>> download_all_images_from_website('example.com', 80)\n    \"\"\"\n    url = f\"http://{host}:{port}\"\n    response = requests.get(url)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    images = soup.find_all('img')\n\n    for i, img in enumerate(images):\n        img_url = img.get('src')\n        if not img_url.startswith('http'):\n            img_url = f\"{url}/{img_url}\"\n        r = requests.get(img_url, stream=True)\n        if r.status_code == 200:\n            with open(os.path.join(DOWNLOAD_DIR, f\"image{i}.jpg\"), 'wb') as f:\n                for chunk in r:\n                    f.write(chunk)\n\n    return len(images)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nsocket, urllib, ssl, cryptography\n```python\nimport socket\nimport urllib.request\nimport ssl\nfrom cryptography import x509\nfrom cryptography.hazmat.backends import default_backend\n\ndef verify_ssl_certificate(host, port):\n    \"\"\"\n    Verify the SSL certificate of a website specified by a given host and port.\n\n    Parameters:\n    host (str): The hostname.\n    port (int): The port number.\n\n    Returns:\n    bool: True if the certificate is valid, False otherwise.\n\n    Requirements:\n    - socket\n    - urllib.request\n    - ssl\n    - cryptography\n\n    Example:\n    >>> verify_ssl_certificate('google.com', 443)\n    \"\"\"\n    # Fetch SSL certificate\n    ctx = ssl.create_default_context()\n    with ctx.wrap_socket(socket.socket(), server_hostname=host) as s:\n        s.connect((host, port))\n        cert = s.getpeercert(binary_form=True)\n\n    # Verify certificate\n    x509_cert = x509.load_der_x509_certificate(cert, default_backend())\n    try:\n        host_names = x509_cert.extensions.get_extension_for_class(x509.SubjectAlternativeName).value.get_values_for_type(x509.DNSName)\n        return host in host_names\n    except x509.ExtensionNotFound:\n        return False\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nsocket, dns.resolver, requests, json\n```python\nimport socket\nimport dns.resolver\nimport requests\nimport json\n\ndef fetch_ip_address_and_geolocation(host):\n    \"\"\"\n    Fetch the IP address and geolocation information of a host.\n\n    Parameters:\n    host (str): The hostname.\n\n    Returns:\n    dict: A dictionary containing the IP address and geolocation information.\n\n    Requirements:\n    - socket\n    - dns.resolver\n    - requests\n    - json\n\n    Example:\n    >>> fetch_ip_address_and_geolocation('google.com')\n    \"\"\"\n    # Fetch IP address\n    ip_address = socket.gethostbyname(host)\n\n    # Fetch geolocation\n    response = requests.get(f\"https://ipinfo.io/{ip_address}\")\n    geolocation = json.loads(response.text)\n\n    return {\n        'ip_address': ip_address,\n        'geolocation': geolocation\n    }\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nsocket, requests, re, time\n```python\nimport socket\nimport requests\nimport re\nimport time\n\ndef measure_website_load_time(host, port):\n    \"\"\"\n    Measure the load time of a website specified by a given host and port.\n\n    Parameters:\n    host (str): The hostname.\n    port (int): The port number.\n\n    Returns:\n    float: The load time in seconds.\n\n    Requirements:\n    - socket\n    - requests\n    - re\n    - time\n\n    Example:\n    >>> measure_website_load_time('google.com', 80)\n    \"\"\"\n    url = f\"http://{host}:{port}\"\n    start_time = time.time()\n    response = requests.get(url)\n    end_time = time.time()\n    load_time = end_time - start_time\n\n    return load_time\n```\n"}
{"seed": "{\"task_id\": 17552997, \"prompt\": \"def f_17552997(df):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"df.xs('sat', level='day', drop_level=False)\", \"test_start\": \"\\nimport pandas as pd \\n\\ndef check(candidate):\", \"test\": [\"\\n    df = pd.DataFrame({'year':[2008,2008,2008,2008,2009,2009,2009,2009], \\n                      'flavour':['strawberry','strawberry','banana','banana',\\n                      'strawberry','strawberry','banana','banana'],\\n                      'day':['sat','sun','sat','sun','sat','sun','sat','sun'],\\n                      'sales':[10,12,22,23,11,13,23,24]})\\n    df = df.set_index(['year','flavour','day'])\\n    assert candidate(df).to_dict() == {'sales': {(2008, 'strawberry', 'sat'): 10, (2008, 'banana', 'sat'): 22, (2009, 'strawberry', 'sat'): 11, (2009, 'banana', 'sat'): 23}}\\n\"], \"entry_point\": \"f_17552997\", \"intent\": \"add a column 'day' with value 'sat' to dataframe `df`\", \"library\": [\"pandas\"]}", "generation": "Scenario 1: \npandas, numpy, matplotlib, seaborn, itertools\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport itertools\n\n# Constants\nFRUITS = ['Apple', 'Banana', 'Cherry', 'Date', 'Elderberry']\nDAYS = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n\ndef generate_sales_data_and_plot(df):\n    \"\"\"\n    Generate a report of fruit sales for a week with random sales values and plot a heatmap for the sales data.\n    \n    Parameters:\n    df (DataFrame): The initial dataframe.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with sales data.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - seaborn\n    - itertools\n    \n    Example:\n    >>> df = pd.DataFrame()\n    >>> df = generate_sales_data_and_plot(df)\n    >>> print(df)\n    \"\"\"\n    data = list(itertools.product(FRUITS, DAYS))\n    sales_data = pd.DataFrame(data, columns=['Fruit', 'Day'])\n    sales_data['Sales'] = np.random.randint(1, 50, size=len(data))\n\n    df = pd.concat([df, sales_data])\n    df_pivot = df.pivot(\"Fruit\", \"Day\", \"Sales\")\n    \n    plt.figure(figsize=(10, 6))\n    sns.heatmap(df_pivot, annot=True, fmt=\".1f\", linewidths=.5, square = True, cmap = 'Blues');\n    plt.ylabel('Fruits');\n    plt.xlabel('Day of week');\n    title = 'Heatmap of Fruit Sales'\n    plt.title(title, size = 15);\n    plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \npandas, numpy, matplotlib, sklearn.preprocessing\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Constants\nFRUITS = ['Apple', 'Banana', 'Cherry', 'Date', 'Elderberry']\nYEARS = [2018, 2019, 2020, 2021, 2022]\n\ndef generate_and_scale_sales_data(df):\n    \"\"\"\n    Generate a report of fruit sales for years with random sales values and scale the sales values to the range 0-1.\n    \n    Parameters:\n    df (DataFrame): The initial dataframe.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with scaled sales data.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.preprocessing.MinMaxScaler\n    \n    Example:\n    >>> df = pd.DataFrame()\n    >>> df = generate_and_scale_sales_data(df)\n    >>> print(df)\n    \"\"\"\n    data = list(itertools.product(FRUITS, YEARS))\n    sales_data = pd.DataFrame(data, columns=['Fruit', 'Year'])\n    sales_data['Sales'] = np.random.randint(1, 1000, size=len(data))\n\n    df = pd.concat([df, sales_data])\n\n    scaler = MinMaxScaler()\n    df[['Sales']] = scaler.fit_transform(df[['Sales']])\n\n    df.plot(x='Year', y='Sales', kind='bar')\n    plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \npandas, numpy, datetime, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nimport matplotlib.pyplot as plt\n\n# Constants\nFRUITS = ['Apple', 'Banana', 'Cherry', 'Date', 'Elderberry']\nDAYS = [datetime.today() - timedelta(days=x) for x in range(len(FRUITS))]\n\ndef generate_sales_data_and_plot(df):\n    \"\"\"\n    Generate a report of fruit sales for the last few days with random sales values and plot a line graph for the sales data.\n    \n    Parameters:\n    df (DataFrame): The initial dataframe.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with sales data.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - datetime\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = pd.DataFrame()\n    >>> df = generate_sales_data_and_plot(df)\n    >>> print(df)\n    \"\"\"\n    data = list(itertools.product(FRUITS, DAYS))\n    sales_data = pd.DataFrame(data, columns=['Fruit', 'Day'])\n    sales_data['Sales'] = np.random.randint(1, 50, size=len(data))\n\n    df = pd.concat([df, sales_data])\n    df.plot(x='Day', y='Sales', kind='line')\n    plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \npandas, numpy, datetime, matplotlib, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nFRUITS = ['Apple', 'Banana', 'Cherry', 'Date', 'Elderberry']\nDAYS = [datetime.today() - timedelta(days=x) for x in range(len(FRUITS))]\n\ndef generate_sales_data_and_plot(df):\n    \"\"\"\n    Generate a report of fruit sales for the last few days with random sales values and plot a box plot for the sales data.\n    \n    Parameters:\n    df (DataFrame): The initial dataframe.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with sales data.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - datetime\n    - matplotlib.pyplot\n    - seaborn\n    \n    Example:\n    >>> df = pd.DataFrame()\n    >>> df = generate_sales_data_and_plot(df)\n    >>> print(df)\n    \"\"\"\n    data = list(itertools.product(FRUITS, DAYS))\n    sales_data = pd.DataFrame(data, columns=['Fruit', 'Day'])\n    sales_data['Sales'] = np.random.randint(1, 50, size=len(data))\n\n    df = pd.concat([df, sales_data])\n    sns.boxplot(x='Fruit', y='Sales', data=df)\n    plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \npandas, numpy, datetime, matplotlib, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nFRUITS = ['Apple', 'Banana', 'Cherry', 'Date', 'Elderberry']\nDAYS = [datetime.today() - timedelta(days=x) for x in range(len(FRUITS))]\n\ndef generate_sales_data_and_plot(df):\n    \"\"\"\n    Generate a report of fruit sales for the last few days with random sales values and plot a violin plot for the sales data.\n    \n    Parameters:\n    df (DataFrame): The initial dataframe.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with sales data.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - datetime\n    - matplotlib.pyplot\n    - seaborn\n    \n    Example:\n    >>> df = pd.DataFrame()\n    >>> df = generate_sales_data_and_plot(df)\n    >>> print(df)\n    \"\"\"\n    data = list(itertools.product(FRUITS, DAYS))\n    sales_data = pd.DataFrame(data, columns=['Fruit', 'Day'])\n    sales_data['Sales'] = np.random.randint(1, 50, size=len(data))\n\n    df = pd.concat([df, sales_data])\n    sns.violinplot(x='Fruit', y='Sales', data=df)\n    plt.show()\n\n    return df\n```\n"}
{"seed": "{\"task_id\": 4356842, \"prompt\": \"def f_4356842():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"HttpResponse('Unauthorized', status=401)\", \"test_start\": \"\\nfrom django.http import HttpResponse\\nfrom django.conf import settings\\n\\nif not settings.configured:\\n    settings.configure(DEBUG=True)\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate().status_code == 401\\n\"], \"entry_point\": \"f_4356842\", \"intent\": \"return a 401 unauthorized in django\", \"library\": [\"django\"]}", "generation": "Scenario 1:\nDjango, os, time\n```python\nimport os\nimport time\nfrom django.http import JsonResponse\n\ndef get_system_info():\n    \"\"\"\n    Return a JsonResponse containing the basic system information including the current timestamp.\n\n    Returns:\n    JsonResponse: A Django JsonResponse with system info.\n\n    Requirements:\n    - django\n    - os\n    - time\n\n    Example:\n    >>> response = get_system_info()\n    >>> assert response.status_code == 200\n    \"\"\"\n    system_info = {\n        'os_name': os.name,\n        'timestamp': time.time(),\n        'platform': os.sys.platform,\n    }\n\n    return JsonResponse(system_info)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nDjango, random, string, time\n```python\nimport random\nimport string\nfrom django.http import HttpResponse\n\n# Constants\nSESSION_EXPIRE_TIME = 60  # in seconds\n\ndef generate_session_key(request):\n    \"\"\"\n    Generate a new session key and set it in the response with an expiry time.\n\n    Parameters:\n    request (HttpRequest): The incoming Django HttpRequest.\n\n    Returns:\n    HttpResponse: A Django HttpResponse with the session key set.\n\n    Requirements:\n    - django\n    - random\n    - string\n    - time\n\n    Example:\n    >>> request = HttpRequest()\n    >>> response = generate_session_key(request)\n    >>> assert response.status_code == 200\n    \"\"\"\n    session_key = ''.join(random.choices(string.ascii_letters + string.digits, k=20))\n    response = HttpResponse('Session key generated successfully.')\n\n    response.set_cookie('session_key', session_key, max_age=SESSION_EXPIRE_TIME)\n\n    return response\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nDjango, json, hashlib, base64\n```python\nimport json\nimport hashlib\nimport base64\nfrom django.http import HttpResponseBadRequest, HttpResponse\n\ndef handle_login_request(request):\n    \"\"\"\n    Handle a login request. The request body should be a JSON object with \"username\" and \"password\".\n    The password should be hashed using SHA256 and encoded with base64 before sending.\n\n    Parameters:\n    request (HttpRequest): The incoming Django HttpRequest.\n\n    Returns:\n    HttpResponse: A Django HttpResponse indicating the login result.\n\n    Requirements:\n    - django\n    - json\n    - hashlib\n    - base64\n\n    Example:\n    >>> request = HttpRequest()\n    >>> request.body = json.dumps({'username': 'admin', 'password': '...'})\n    >>> response = handle_login_request(request)\n    >>> assert response.status_code in [200, 400]\n    \"\"\"\n    try:\n        body = json.loads(request.body)\n        username = body['username']\n        password = base64.b64decode(body['password']).decode()\n    except (KeyError, json.JSONDecodeError, UnicodeDecodeError, binascii.Error):\n        return HttpResponseBadRequest('Bad Request')\n\n    hashed_password = hashlib.sha256(password.encode()).digest()\n\n    # Here you can check the hashed password against your user database\n    # For simplicity, we just check if the username is \"admin\" and the password is \"password\"\n    if username == 'admin' and hashed_password == hashlib.sha256('password'.encode()).digest():\n        return HttpResponse('Login successful.')\n    else:\n        return HttpResponse('Login failed.', status=401)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nDjango, csv, io\n```python\nimport csv\nimport io\nfrom django.http import FileResponse\n\n# Constant\nHEADER = ['id', 'name', 'email']\n\ndef generate_csv_response(request):\n    \"\"\"\n    Generate a CSV response containing some dummy data.\n\n    Parameters:\n    request (HttpRequest): The incoming Django HttpRequest.\n\n    Returns:\n    FileResponse: A Django FileResponse with the CSV data.\n\n    Requirements:\n    - django\n    - csv\n    - io\n\n    Example:\n    >>> request = HttpRequest()\n    >>> response = generate_csv_response(request)\n    >>> assert response.status_code == 200\n    \"\"\"\n    csv_data = [['1', 'John Doe', 'john@example.com'], ['2', 'Jane Doe', 'jane@example.com']]\n\n    csv_io = io.StringIO()\n    writer = csv.writer(csv_io)\n    writer.writerow(HEADER)\n    writer.writerows(csv_data)\n\n    response = FileResponse(csv_io, as_attachment=True, filename='data.csv')\n\n    return response\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nDjango, os, zipfile, io\n```python\nimport os\nimport zipfile\nimport io\nfrom django.http import FileResponse\n\ndef download_files_as_zip(request):\n    \"\"\"\n    Download multiple files as a zip file.\n\n    Parameters:\n    request (HttpRequest): The incoming Django HttpRequest.\n\n    Returns:\n    FileResponse: A Django FileResponse with the zip file.\n\n    Requirements:\n    - django\n    - os\n    - zipfile\n    - io\n\n    Example:\n    >>> request = HttpRequest()\n    >>> response = download_files_as_zip(request)\n    >>> assert response.status_code == 200\n    \"\"\"\n    file_paths = ['file1.txt', 'file2.txt']  # Replace with the actual file paths\n\n    zip_io = io.BytesIO()\n\n    with zipfile.ZipFile(zip_io, 'w') as zip_file:\n        for file_path in file_paths:\n            # Here we just write the file name into the zip file for simplicity\n            # In a real scenario, you should read from the actual file and write the content into the zip file\n            zip_file.writestr(file_path, 'This is the content of {}.'.format(file_path))\n\n    response = FileResponse(zip_io, as_attachment=True, filename='files.zip')\n\n    return response\n```"}
{"seed": "{\"task_id\": 13598363, \"prompt\": \"def f_13598363():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"Flask('test', template_folder='wherever')\", \"test_start\": \"\\nfrom flask import Flask\\n\\ndef check(candidate):\", \"test\": [\"\\n    __name__ == \\\"test\\\"\\n    assert candidate().template_folder == \\\"wherever\\\"\\n\"], \"entry_point\": \"f_13598363\", \"intent\": \"Flask set folder 'wherever' as the default template folder\", \"library\": [\"flask\"]}", "generation": "Scenario 1:\nflask, os, json, logging\n```python\nfrom flask import Flask, render_template, request\nimport os\nimport json\nimport logging\n\n# Constants\nTEMPLATE_FOLDER = 'templates'\nLOG_FILE = 'app.log'\n\n# Setup logging\nlogging.basicConfig(filename=LOG_FILE, level=logging.INFO)\n\ndef create_flask_app():\n    \"\"\"\n    Create a Flask app, set the template folder, and define a route that renders a template \n    and logs the request data to a file.\n\n    Requirements:\n    - flask\n    - os\n    - json\n    - logging\n\n    Example:\n    >>> app = create_flask_app()\n    >>> app.run(debug=True)\n    \"\"\"\n    app = Flask(__name__, template_folder=TEMPLATE_FOLDER)\n\n    @app.route('/', methods=['POST'])\n    def home():\n        data = request.json\n        logging.info(json.dumps(data))\n        return render_template('index.html', data=data)\n\n    return app\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nflask, flask_sqlalchemy, flask_migrate, os\n```python\nfrom flask import Flask\nfrom flask_sqlalchemy import SQLAlchemy\nfrom flask_migrate import Migrate\nimport os\n\n# Constants\nDATABASE_URI = 'sqlite:///test.db'\nMIGRATION_DIR = 'migrations'\n\ndef create_flask_app_with_db():\n    \"\"\"\n    Create a Flask app, set the database URI, initialize SQLAlchemy and Flask-Migrate.\n\n    Requirements:\n    - flask\n    - flask_sqlalchemy\n    - flask_migrate\n    - os\n\n    Example:\n    >>> app = create_flask_app_with_db()\n    >>> app.run(debug=True)\n    \"\"\"\n    app = Flask(__name__, template_folder='templates')\n    app.config['SQLALCHEMY_DATABASE_URI'] = DATABASE_URI\n\n    # Initialize extensions\n    db = SQLAlchemy(app)\n    migrate = Migrate(app, db, directory=MIGRATION_DIR)\n\n    return app\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nflask, flask_restful, requests, json\n```python\nfrom flask import Flask\nfrom flask_restful import Resource, Api\nimport requests\nimport json\n\n# Constants\nAPI_URL = 'https://api.example.com/data'\nTEMPLATE_FOLDER = 'templates'\n\ndef create_flask_restful_app():\n    \"\"\"\n    Create a Flask app, set the template folder, and define a RESTful endpoint that fetches \n    data from an external API.\n\n    Requirements:\n    - flask\n    - flask_restful\n    - requests\n    - json\n\n    Example:\n    >>> app = create_flask_restful_app()\n    >>> app.run(debug=True)\n    \"\"\"\n    app = Flask(__name__, template_folder=TEMPLATE_FOLDER)\n    api = Api(app)\n\n    class DataResource(Resource):\n        def get(self):\n            response = requests.get(API_URL)\n            data = response.json()\n            return data\n\n    api.add_resource(DataResource, '/data')\n\n    return app\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nflask, flask_login, flask_wtf, wtforms, werkzeug.security\n```python\nfrom flask import Flask, render_template, redirect, url_for\nfrom flask_login import LoginManager, UserMixin, login_user, login_required, logout_user\nfrom flask_wtf import FlaskForm\nfrom wtforms import StringField, PasswordField, SubmitField\nfrom wtforms.validators import DataRequired, Length\nfrom werkzeug.security import generate_password_hash, check_password_hash\n\n# Constants\nSECRET_KEY = 'mysecretkey'\nTEMPLATE_FOLDER = 'templates'\n\n# Setup login manager\nlogin_manager = LoginManager()\n\ndef create_flask_login_app():\n    \"\"\"\n    Create a Flask app, set the template folder, setup Flask-Login and define login and logout routes.\n\n    Requirements:\n    - flask\n    - flask_login\n    - flask_wtf\n    - wtforms\n    - werkzeug.security\n\n    Example:\n    >>> app = create_flask_login_app()\n    >>> app.run(debug=True)\n    \"\"\"\n    app = Flask(__name__, template_folder=TEMPLATE_FOLDER)\n    app.config['SECRET_KEY'] = SECRET_KEY\n\n    login_manager.init_app(app)\n\n    class User(UserMixin):\n        def __init__(self, username, password):\n            self.id = username\n            self.password_hash = generate_password_hash(password)\n\n        def check_password(self, password):\n            return check_password_hash(self.password_hash, password)\n\n    class LoginForm(FlaskForm):\n        username = StringField('Username', validators=[DataRequired(), Length(min=4, max=25)])\n        password = PasswordField('Password', validators=[DataRequired(), Length(min=8, max=80)])\n        submit = SubmitField('Log In')\n\n    @app.route('/login', methods=['GET', 'POST'])\n    def login():\n        form = LoginForm()\n        if form.validate_on_submit():\n            user = User(form.username.data, form.password.data)\n            login_user(user)\n            return redirect(url_for('protected'))\n\n        return render_template('login.html', form=form)\n\n    @app.route('/logout')\n    @login_required\n    def logout():\n        logout_user()\n        return redirect(url_for('login'))\n\n    @app.route('/protected')\n    @login_required\n    def protected():\n        return 'Logged in as: ' + current_user.id\n\n    return app\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nflask, flask_mail, smtplib, os\n```python\nfrom flask import Flask\nfrom flask_mail import Mail, Message\nimport smtplib\nimport os\n\n# Constants\nSMTP_SERVER = 'smtp.example.com'\nSMTP_PORT = 587\nSMTP_USER = 'user@example.com'\nSMTP_PASSWORD = 'password'\nTEMPLATE_FOLDER = 'templates'\n\n# Setup mail\nmail = Mail()\n\ndef create_flask_mail_app():\n    \"\"\"\n    Create a Flask app, set the template folder, and setup Flask-Mail for sending emails.\n\n    Requirements:\n    - flask\n    - flask_mail\n    - smtplib\n    - os\n\n    Example:\n    >>> app = create_flask_mail_app()\n    >>> app.run(debug=True)\n    \"\"\"\n    app = Flask(__name__, template_folder=TEMPLATE_FOLDER)\n    app.config['MAIL_SERVER'] = SMTP_SERVER\n    app.config['MAIL_PORT'] = SMTP_PORT\n    app.config['MAIL_USERNAME'] = SMTP_USER\n    app.config['MAIL_PASSWORD'] = SMTP_PASSWORD\n    app.config['MAIL_USE_TLS'] = True\n\n    mail.init_app(app)\n\n    @app.route('/send_mail')\n    def send_mail():\n        msg = Message('Hello', sender='from@example.com', recipients=['to@example.com'])\n        msg.body = 'Hello Flask message sent from Flask-Mail'\n        mail.send(msg)\n\n        return 'Mail sent!'\n\n    return app\n```\n"}
{"seed": "{\"task_id\": 3398589, \"prompt\": \"def f_3398589(c2):\\n\\t\", \"suffix\": \"\\n\\treturn c2\", \"canonical_solution\": \"c2.sort(key=lambda row: row[2])\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    c2 = [[14, 25, 46], [1, 22, 53], [7, 8, 9]]\\n    candidate(c2)\\n    assert c2[0] == [7,8,9]\\n\", \"\\n    c2 = [[14.343, 25.24, 46], [1, 22, 53.45], [7, 8.65, 9]]\\n    candidate(c2)\\n    assert c2[0] == [7,8.65,9]\\n\"], \"entry_point\": \"f_3398589\", \"intent\": \"sort a list of lists 'c2' such that third row comes first\", \"library\": []}", "generation": "Scenario 1:\nnumpy, pandas, matplotlib, random\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import choice\n\n# Constants\nCOLUMNS = [\"Product\", \"Sales\", \"Profit\"]\nPRODUCTS = [\"iPhone\", \"iPad\", \"Macbook\", \"Airpods\", \"Apple Watch\"]\n\ndef generate_sales_report():\n    \"\"\"\n    Generate a sales report for a list of products. The report contains sales \n    and profit data for each product which are randomly generated. \n    After generating the report, sort the data by profit in descending order \n    and plot the profit data.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with sales and profit data for the products.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - random.choice\n    \n    Example:\n    >>> report = generate_sales_report()\n    >>> print(report)\n    >>> report.plot(x='Product', y='Profit', kind='bar')\n    \"\"\"\n    data = []\n    \n    for _ in range(100):\n        product = choice(PRODUCTS)\n        sales = np.random.randint(50, 200)\n        profit = sales * np.random.uniform(0.1, 0.5)\n        data.append([product, sales, profit])\n\n    df = pd.DataFrame(data, columns=COLUMNS)\n    df = df.groupby(\"Product\", as_index=False).sum()\n    df.sort_values(\"Profit\", ascending=False, inplace=True)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, pandas, matplotlib, datetime\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom datetime import datetime, timedelta\n\n# Constants\nCOLUMNS = [\"Date\", \"Temperature\", \"Humidity\", \"Wind Speed\"]\n\ndef generate_weather_data(start_date, end_date):\n    \"\"\"\n    Generate a weather data for a range of dates. The data contains temperature,\n    humidity and wind speed which are randomly generated. \n    After generating the data, plot the temperature, humidity and wind speed over time.\n    \n    Parameters:\n    start_date (datetime): The start date.\n    end_date (datetime): The end date.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with weather data for the dates.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - datetime\n    \n    Example:\n    >>> start_date = datetime(2021, 1, 1)\n    >>> end_date = datetime(2021, 12, 31)\n    >>> data = generate_weather_data(start_date, end_date)\n    >>> print(data)\n    >>> data.plot(x='Date', y=['Temperature', 'Humidity', 'Wind Speed'])\n    \"\"\"\n    data = []\n    date = start_date\n\n    while date <= end_date:\n        temp = np.random.uniform(-10, 40)\n        humidity = np.random.uniform(20, 100)\n        wind_speed = np.random.uniform(0, 20)\n        data.append([date, temp, humidity, wind_speed])\n        date += timedelta(days=1)\n\n    df = pd.DataFrame(data, columns=COLUMNS)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, pandas, matplotlib, random\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import choice\n\n# Constants\nCOLUMNS = [\"Student\", \"Score\"]\nSTUDENTS = [\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\"]\n\ndef generate_student_scores():\n    \"\"\"\n    Generate scores for a list of students. The scores are randomly generated. \n    After generating the data, sort the data by score in ascending order \n    and plot the scores.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with students' scores.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - random.choice\n    \n    Example:\n    >>> scores = generate_student_scores()\n    >>> print(scores)\n    >>> scores.plot(x='Student', y='Score', kind='bar')\n    \"\"\"\n    scores = []\n\n    for student in STUDENTS:\n        score = np.random.randint(0, 100)\n        scores.append([student, score])\n\n    df = pd.DataFrame(scores, columns=COLUMNS)\n    df.sort_values(\"Score\", ascending=True, inplace=True)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, pandas, matplotlib, random\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import choices\n\n# Constants\nCOLUMNS = [\"Product\", \"Rating\"]\nPRODUCTS = [\"iPhone\", \"iPad\", \"Macbook\", \"Airpods\", \"Apple Watch\"]\nRATINGS = [1, 2, 3, 4, 5]\nWEIGHTS = [0.05, 0.1, 0.2, 0.3, 0.35]\n\ndef generate_product_ratings():\n    \"\"\"\n    Generate ratings for a list of products. The ratings are randomly generated \n    with a weighted probability. After generating the data, sort the data by \n    rating in descending order and plot the ratings.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with products' ratings.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - random.choices\n    \n    Example:\n    >>> ratings = generate_product_ratings()\n    >>> print(ratings)\n    >>> ratings.plot(x='Product', y='Rating', kind='bar')\n    \"\"\"\n    ratings = []\n\n    for product in PRODUCTS:\n        rating = choices(RATINGS, WEIGHTS, k=1)[0]\n        ratings.append([product, rating])\n\n    df = pd.DataFrame(ratings, columns=COLUMNS)\n    df.sort_values(\"Rating\", ascending=False, inplace=True)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, pandas, matplotlib, datetime\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom datetime import datetime, timedelta\n\n# Constants\nCOLUMNS = [\"Date\", \"Sales\"]\n\ndef generate_sales_data(start_date, end_date):\n    \"\"\"\n    Generate sales data for a range of dates. The sales are randomly generated. \n    After generating the data, plot the sales over time.\n    \n    Parameters:\n    start_date (datetime): The start date.\n    end_date (datetime): The end date.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with sales data for the dates.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - datetime\n    \n    Example:\n    >>> start_date = datetime(2021, 1, 1)\n    >>> end_date = datetime(2021, 12, 31)\n    >>> data = generate_sales_data(start_date, end_date)\n    >>> print(data)\n    >>> data.plot(x='Date', y='Sales')\n    \"\"\"\n    data = []\n    date = start_date\n\n    while date <= end_date:\n        sales = np.random.randint(0, 500)\n        data.append([date, sales])\n        date += timedelta(days=1)\n\n    df = pd.DataFrame(data, columns=COLUMNS)\n\n    return df\n```\n"}
{"seed": "{\"task_id\": 3398589, \"prompt\": \"def f_3398589(c2):\\n\\t\", \"suffix\": \"\\n\\treturn c2\", \"canonical_solution\": \"c2.sort(key=lambda row: (row[2], row[1], row[0]))\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    c2 = [[14, 25, 46], [1, 22, 53], [7, 8, 9]]\\n    candidate(c2)\\n    assert c2[0] == [7,8,9]\\n\", \"\\n    c2 = [[14.343, 25.24, 46], [1, 22, 53.45], [7, 8.65, 9]]\\n    candidate(c2)\\n    assert c2[0] == [7,8.65,9]\\n\"], \"entry_point\": \"f_3398589\", \"intent\": \"sort a list of lists 'c2' in reversed row order\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, math, collections\n```python\nimport pandas as pd\nimport numpy as np\nimport math\nfrom collections import Counter\n\n# Constants\nRADIUS_EARTH_KM = 6371.0\n\ndef calculate_nearest_neighbours(data, target, k):\n    \"\"\"\n    Given a dataset and a target data point, calculate the 'k' nearest neighbours based on \n    the geographical coordinates. The function should return a list of the 'k' nearest neighbours \n    sorted in ascending order of their distances from the target. \n\n    Parameters:\n    data (DataFrame): The dataset containing the geographical coordinates.\n    target (list): The target data point.\n    k (int): The number of nearest neighbours to return.\n\n    Returns:\n    list: The list of the 'k' nearest neighbours.\n\n    Requirements:\n    - pandas\n    - numpy\n    - math\n    - collections.Counter\n\n    Example:\n    >>> data = pd.DataFrame([[14, 25], [1, 22], [7, 8]], columns=['Latitude', 'Longitude'])\n    >>> target = [10, 15]\n    >>> k = 2\n    >>> calculate_nearest_neighbours(data, target, k)\n    \"\"\"\n    def calculate_distance(coord1, coord2):\n        lat1, lon1 = math.radians(coord1[0]), math.radians(coord1[1])\n        lat2, lon2 = math.radians(coord2[0]), math.radians(coord2[1])\n\n        dlat = lat2 - lat1\n        dlon = lon2 - lon1\n\n        a = math.sin(dlat/2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon/2)**2\n        c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n\n        return RADIUS_EARTH_KM * c\n\n    distances = np.array([calculate_distance(target, coord) for coord in data.to_numpy()])\n    nearest_indices = distances.argsort()[:k]\n    nearest_neighbours = data.iloc[nearest_indices].values.tolist()\n\n    return nearest_neighbours\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nnumpy, matplotlib, scipy, sklearn\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\n\n# Constants\nOUTLIER_Z_SCORE = 3.0\n\ndef remove_and_plot_outliers(data, column):\n    \"\"\"\n    Given a dataset, remove the outliers in a particular column based on the \n    Z-score and plot the data before and after the removal of outliers.\n\n    Parameters:\n    data (ndarray): The dataset.\n    column (int): The index of the column.\n\n    Returns:\n    tuple: The original data, the data without outliers, and the indices of the outliers.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - scipy.stats\n    - sklearn.preprocessing.StandardScaler\n\n    Example:\n    >>> data = np.array([[14, 25], [1, 22], [7, 8], [100, 200]])\n    >>> column = 1\n    >>> remove_and_plot_outliers(data, column)\n    \"\"\"\n    # Copy the data to avoid modifying the original array\n    data_copy = np.copy(data)\n    column_data = data_copy[:, column]\n\n    # Standardize the data to have a mean of 0 and a standard deviation of 1\n    scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(column_data.reshape(-1, 1))\n\n    # Calculate the Z-scores\n    z_scores = np.abs(stats.zscore(standardized_data))\n\n    # Identify the outliers\n    outliers = np.where(z_scores > OUTLIER_Z_SCORE)\n    data_without_outliers = np.delete(data_copy, outliers, axis=0)\n\n    # Plot the data before and after the removal of outliers\n    plt.figure(figsize=(10, 5))\n    \n    plt.subplot(1, 2, 1)\n    plt.scatter(data_copy[:, 0], data_copy[:, 1])\n    plt.title('Data with Outliers')\n    \n    plt.subplot(1, 2, 2)\n    plt.scatter(data_without_outliers[:, 0], data_without_outliers[:, 1])\n    plt.title('Data without Outliers')\n    \n    plt.show()\n\n    return data_copy, data_without_outliers, outliers\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnumpy, scipy, matplotlib, pandas\n```python\nimport numpy as np\nfrom scipy.stats import linregress\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef perform_linear_regression(data, column1, column2):\n    \"\"\"\n    Perform linear regression on two columns of a dataset and plot the result.\n\n    Parameters:\n    data (DataFrame): The dataset.\n    column1 (str): The name of the first column.\n    column2 (str): The name of the second column.\n\n    Returns:\n    tuple: The slope, intercept, r-value, p-value, and standard error of the regression.\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n    - pandas\n\n    Example:\n    >>> data = pd.DataFrame([[14, 25], [1, 22], [7, 8]], columns=['Column1', 'Column2'])\n    >>> perform_linear_regression(data, 'Column1', 'Column2')\n    \"\"\"\n    x = data[column1].values\n    y = data[column2].values\n\n    slope, intercept, r_value, p_value, std_err = linregress(x, y)\n\n    plt.plot(x, y, 'o', label='original data')\n    plt.plot(x, intercept + slope*x, 'r', label='fitted line')\n    plt.legend()\n    plt.show()\n\n    return slope, intercept, r_value, p_value, std_err\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, pandas, matplotlib, sklearn\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\n\n# Constants\nN_CLUSTERS = 3\n\ndef perform_kmeans_clustering(data):\n    \"\"\"\n    Perform K-means clustering on a dataset and plot the result.\n\n    Parameters:\n    data (DataFrame): The dataset.\n\n    Returns:\n    ndarray: The cluster labels.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - sklearn.cluster\n\n    Example:\n    >>> data = pd.DataFrame([[14, 25], [1, 22], [7, 8]], columns=['Column1', 'Column2'])\n    >>> perform_kmeans_clustering(data)\n    \"\"\"\n    kmeans = KMeans(n_clusters=N_CLUSTERS)\n    kmeans.fit(data)\n\n    labels = kmeans.labels_\n    centroids = kmeans.cluster_centers_\n\n    plt.scatter(data['Column1'], data['Column2'], c=labels)\n    plt.scatter(centroids[:, 0], centroids[:, 1], marker='X', s=200, c='red')\n    plt.show()\n\n    return labels\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnumpy, pandas, matplotlib, sklearn\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\n# Constants\nN_COMPONENTS = 2\n\ndef perform_pca(data):\n    \"\"\"\n    Perform Principal Component Analysis (PCA) on a dataset and plot the result.\n\n    Parameters:\n    data (DataFrame): The dataset.\n\n    Returns:\n    DataFrame: The transformed data.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - sklearn.decomposition\n\n    Example:\n    >>> data = pd.DataFrame([[14, 25], [1, 22], [7, 8]], columns=['Column1', 'Column2'])\n    >>> perform_pca(data)\n    \"\"\"\n    pca = PCA(n_components=N_COMPONENTS)\n    transformed_data = pca.fit_transform(data)\n\n    plt.scatter(transformed_data[:, 0], transformed_data[:, 1])\n    plt.show()\n\n    return pd.DataFrame(transformed_data, columns=['PC1', 'PC2'])\n```\n"}
{"seed": "{\"task_id\": 3398589, \"prompt\": \"def f_3398589(c2):\\n\\t\", \"suffix\": \"\\n\\treturn c2\", \"canonical_solution\": \"c2.sort(key=lambda row: (row[2], row[1]))\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    c2 = [[14, 25, 46], [1, 22, 53], [7, 8, 9]]\\n    candidate(c2)\\n    assert c2[0] == [7,8,9]\\n\", \"\\n    c2 = [[14.343, 25.24, 46], [1, 22, 53.45], [7, 8.65, 9]]\\n    candidate(c2)\\n    assert c2[0] == [7,8.65,9]\\n\"], \"entry_point\": \"f_3398589\", \"intent\": \"Sorting a list of lists `c2`, each by the third and second row\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, matplotlib, random\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint, uniform\n\n# Constants\nCATEGORIES = ['Electronics', 'Clothing', 'Home & Kitchen', 'Books', 'Beauty & Personal Care']\nMONTHS = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n\ndef generate_sales_report():\n    \"\"\"\n    Generate a sales report for different categories of products for each month and plot \n    the sales trends.\n\n    Returns:\n    DataFrame: A pandas DataFrame with sales data for the categories.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - random\n    \n    Example:\n    >>> report = generate_sales_report()\n    >>> print(report)\n    >>> report.plot(x='Month', y=CATEGORIES)\n    \"\"\"\n    sales_data = []\n\n    for month in MONTHS:\n        for category in CATEGORIES:\n            sales = randint(100, 500) + uniform(0, 1)\n            sales_data.append([month, category, sales])\n\n    sales_df = pd.DataFrame(sales_data, columns=['Month', 'Category', 'Sales'])\n\n    return sales_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, scipy.stats, matplotlib\n```python\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\n# Constants\nMEAN = 0\nSTD_DEV = 1\nNUM_SAMPLES = 1000\n\ndef plot_normal_distribution():\n    \"\"\"\n    Generate a normal distribution with a given mean and standard deviation and \n    plot its histogram along with the probability density function.\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_normal_distribution()\n    \"\"\"\n    samples = np.random.normal(MEAN, STD_DEV, NUM_SAMPLES)\n    plt.hist(samples, bins=30, density=True, alpha=0.6, color='g')\n\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, MEAN, STD_DEV)\n    plt.plot(x, p, 'k', linewidth=2)\n    title = \"Fit results: mean = %.2f,  std = %.2f\" % (MEAN, STD_DEV)\n    plt.title(title)\n\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ncsv, collections, operator\n```python\nimport csv\nfrom collections import Counter\nimport operator\n\n# Constants\nCSV_FILE = 'data.csv'\nCSV_DELIMITER = ','\n\ndef count_most_common_words():\n    \"\"\"\n    Read a CSV file and count the most common words in the file.\n\n    Returns:\n    list: A list of tuples with the most common words and their counts.\n    \n    Requirements:\n    - csv\n    - collections\n    - operator\n    \n    Example:\n    >>> count_most_common_words()\n    \"\"\"\n    words = []\n\n    with open(CSV_FILE, 'r') as f:\n        reader = csv.reader(f, delimiter=CSV_DELIMITER)\n        for row in reader:\n            words.extend(row)\n\n    word_counter = Counter(words)\n    most_common_words = sorted(word_counter.items(), key=operator.itemgetter(1), reverse=True)\n\n    return most_common_words\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nmath, itertools, functools\n```python\nimport math\nimport itertools\nfrom functools import reduce\n\n# Constants\nNUMBERS = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29]\n\ndef calculate_product_of_combinations():\n    \"\"\"\n    Calculate the product of all combinations of a list of numbers and \n    find the sum of the logarithms of the products.\n\n    Returns:\n    float: The sum of the logarithms of the products.\n    \n    Requirements:\n    - math\n    - itertools\n    - functools\n    \n    Example:\n    >>> calculate_product_of_combinations()\n    \"\"\"\n    sum_log_products = 0\n\n    for r in range(1, len(NUMBERS) + 1):\n        combinations = itertools.combinations(NUMBERS, r)\n        for combination in combinations:\n            product = reduce(lambda x, y: x * y, combination)\n            sum_log_products += math.log(product)\n\n    return sum_log_products\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nrandom, string, collections\n```python\nimport random\nimport string\nfrom collections import Counter\n\n# Constants\nNUM_STRINGS = 1000\nSTRING_LENGTH = 5\n\ndef count_most_common_characters():\n    \"\"\"\n    Generate a list of random strings and count the most common characters in the strings.\n\n    Returns:\n    list: A list of tuples with the most common characters and their counts.\n    \n    Requirements:\n    - random\n    - string\n    - collections\n    \n    Example:\n    >>> count_most_common_characters()\n    \"\"\"\n    strings = [''.join(random.choices(string.ascii_lowercase, k=STRING_LENGTH)) for _ in range(NUM_STRINGS)]\n    characters = ''.join(strings)\n    character_counter = Counter(characters)\n    most_common_characters = character_counter.most_common()\n\n    return most_common_characters\n```"}
{"seed": "{\"task_id\": 10960463, \"prompt\": \"def f_10960463():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"matplotlib.rc('font', **{'sans-serif': 'Arial', 'family': 'sans-serif'})\", \"test_start\": \"\\nimport matplotlib\\n\\ndef check(candidate):\", \"test\": [\"\\n    try:\\n        candidate()\\n    except:\\n        assert False\\n\"], \"entry_point\": \"f_10960463\", \"intent\": \"set font `Arial` to display non-ascii characters in matplotlib\", \"library\": [\"matplotlib\"]}", "generation": "Scenario 1:\nmatplotlib, numpy, pandas, seaborn, sklearn\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\n\n# Constants\nFONT = {'sans-serif': 'Arial', 'family': 'sans-serif'}\nIRIS = load_iris()\n\ndef plot_iris_data():\n    \"\"\"\n    Plot the iris dataset using seaborn's pairplot function after setting the font to Arial\n    to properly display non-ascii characters.\n\n    Returns:\n    Figure: A matplotlib Figure instance.\n\n    Requirements:\n    - matplotlib\n    - numpy\n    - pandas\n    - seaborn\n    - sklearn.datasets\n\n    Example:\n    >>> plot_iris_data()\n    \"\"\"\n    plt.rc('font', **FONT)\n    iris_df = pd.DataFrame(data=IRIS.data, columns=IRIS.feature_names)\n    iris_df['target'] = IRIS.target\n    pair_plot = sns.pairplot(iris_df, hue='target')\n    return pair_plot.fig\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nmatplotlib, pandas, random, datetime\n```python\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nfrom datetime import datetime, timedelta\n\n# Constants\nFONT = {'sans-serif': 'Arial', 'family': 'sans-serif'}\n\ndef generate_random_time_series():\n    \"\"\"\n    Generate a random time series data for the past 30 days and plot it using matplotlib \n    after setting the font to Arial to properly display non-ascii characters.\n\n    Returns:\n    Figure: A matplotlib Figure instance.\n\n    Requirements:\n    - matplotlib\n    - pandas\n    - random\n    - datetime\n\n    Example:\n    >>> generate_random_time_series()\n    \"\"\"\n    plt.rc('font', **FONT)\n    \n    dates = pd.date_range(end = datetime.now(), periods = 30).to_pydatetime().tolist()\n    values = [random.randint(0, 100) for _ in range(30)]\n    \n    plt.figure(figsize=(10,6))\n    plt.plot(dates, values)\n    plt.xlabel('Date')\n    plt.ylabel('Value')\n    plt.title('Random Time Series Data')\n    return plt.gcf()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nmatplotlib, os, pandas, seaborn, sklearn\n```python\nimport matplotlib.pyplot as plt\nimport os\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import load_boston\n\n# Constants\nFONT = {'sans-serif': 'Arial', 'family': 'sans-serif'}\nBOSTON = load_boston()\n\ndef save_boston_data_heatmap():\n    \"\"\"\n    Plot the correlation heatmap of the Boston housing dataset using seaborn and \n    save it to a file after setting the font to Arial to properly display non-ascii characters.\n\n    Returns:\n    str: The file path of the saved image.\n\n    Requirements:\n    - matplotlib\n    - os\n    - pandas\n    - seaborn\n    - sklearn.datasets\n\n    Example:\n    >>> save_boston_data_heatmap()\n    \"\"\"\n    plt.rc('font', **FONT)\n    boston_df = pd.DataFrame(data=BOSTON.data, columns=BOSTON.feature_names)\n    corr = boston_df.corr()\n    heatmap = sns.heatmap(corr)\n    \n    file_path = os.path.join(os.getcwd(), 'boston_heatmap.png')\n    plt.savefig(file_path)\n    return file_path\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nmatplotlib, pandas, pytz, datetime\n```python\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom pytz import timezone\nfrom datetime import datetime\n\n# Constants\nFONT = {'sans-serif': 'Arial', 'family': 'sans-serif'}\nNY_TIMEZONE = timezone('America/New_York')\n\ndef plot_daily_temperatures(temperatures):\n    \"\"\"\n    Plot the daily temperatures for New York over a period of time after setting the font \n    to Arial to properly display non-ascii characters.\n\n    Parameters:\n    temperatures (DataFrame): The temperatures data as a pandas DataFrame with a DateTimeIndex \n                              in the 'America/New_York' timezone and a 'temperature' column.\n\n    Returns:\n    Figure: A matplotlib Figure instance.\n\n    Requirements:\n    - matplotlib\n    - pandas\n    - pytz\n    - datetime\n\n    Example:\n    >>> temperatures = pd.DataFrame({\n    ...     'temperature': [random.randint(-10, 30) for _ in range(365)],\n    ...     'date': pd.date_range(start='01-01-2023', periods=365, tz='America/New_York')\n    ... }).set_index('date')\n    >>> plot_daily_temperatures(temperatures)\n    \"\"\"\n    plt.rc('font', **FONT)\n    \n    plt.figure(figsize=(10,6))\n    plt.plot(temperatures.index, temperatures['temperature'])\n    plt.xlabel('Date')\n    plt.ylabel('Temperature (\u00b0C)')\n    plt.title('Daily Temperatures in New York')\n    return plt.gcf()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nmatplotlib, numpy, sklearn, seaborn\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.datasets import load_diabetes\n\n# Constants\nFONT = {'sans-serif': 'Arial', 'family': 'sans-serif'}\nDIABETES = load_diabetes()\n\ndef plot_diabetes_data():\n    \"\"\"\n    Plot the diabetes dataset using seaborn's pairplot function after setting the font to Arial\n    to properly display non-ascii characters.\n\n    Returns:\n    Figure: A matplotlib Figure instance.\n\n    Requirements:\n    - matplotlib\n    - numpy\n    - seaborn\n    - sklearn.datasets\n\n    Example:\n    >>> plot_diabetes_data()\n    \"\"\"\n    plt.rc('font', **FONT)\n    diabetes_df = pd.DataFrame(data=DIABETES.data, columns=DIABETES.feature_names)\n    pair_plot = sns.pairplot(diabetes_df)\n    return pair_plot.fig\n```"}
{"seed": "{\"task_id\": 20576618, \"prompt\": \"def f_20576618(df):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"df['date'].apply(lambda x: x.toordinal())\", \"test_start\": \"\\nimport pandas as pd\\n\\ndef check(candidate):\", \"test\": [\"\\n    df = pd.DataFrame(\\n        {\\n            \\\"group\\\": [\\\"A\\\", \\\"A\\\", \\\"A\\\", \\\"A\\\", \\\"A\\\"],\\n            \\\"date\\\": pd.to_datetime([\\\"2020-01-02\\\", \\\"2020-01-13\\\", \\\"2020-02-01\\\", \\\"2020-02-23\\\", \\\"2020-03-05\\\"]),\\n            \\\"value\\\": [10, 20, 16, 31, 56],\\n        })    \\n    data_series = candidate(df).tolist()\\n    assert data_series[1] == 737437\\n\", \"\\n    df = pd.DataFrame(\\n        {\\n            \\\"group\\\": [\\\"A\\\", \\\"A\\\", \\\"A\\\", \\\"A\\\", \\\"A\\\"],\\n            \\\"date\\\": pd.to_datetime([\\\"2020-01-02\\\", \\\"2020-01-13\\\", \\\"2020-02-01\\\", \\\"2020-02-23\\\", \\\"2020-03-05\\\"]),\\n            \\\"value\\\": [10, 20, 16, 31, 56],\\n        })    \\n    data_series = candidate(df).tolist()\\n    assert data_series[1] == 737437\\n\"], \"entry_point\": \"f_20576618\", \"intent\": \"Convert  DateTime column 'date' of pandas dataframe 'df' to ordinal\", \"library\": [\"pandas\"]}", "generation": "Scenario 1:\npandas, numpy, matplotlib, itertools\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom itertools import cycle\n\n# Constants\nGROUPS = ['A', 'B', 'C', 'D', 'E']\n\ndef analyze_groups(df):\n    \"\"\"\n    Analyze the groups in a dataframe by plotting a scatterplot of the ordinal dates \n    against the values for each group. \n    \n    Parameters:\n    df (DataFrame): The dataframe with columns 'group', 'date', and 'value'.\n    \n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - itertools\n\n    Example:\n    >>> df = pd.DataFrame(\n    ...     {\n    ...     \"group\": [\"A\", \"A\", \"A\", \"B\", \"B\"],\n    ...     \"date\": pd.to_datetime([\"2022-01-02\", \"2022-01-13\", \"2022-02-01\", \"2022-02-23\", \"2022-03-05\"]),\n    ...     \"value\": [10, 20, 16, 31, 56],\n    ...     })\n    >>> analyze_groups(df)\n    \"\"\"\n    color_cycle = cycle('bgrcmk')\n\n    plt.figure(figsize=(10, 6))\n\n    for group in GROUPS:\n        group_df = df[df['group'] == group]\n        group_df['date'] = group_df['date'].apply(lambda x: x.toordinal())\n        plt.scatter(group_df['date'], group_df['value'], color=next(color_cycle))\n\n    plt.xlabel('Date (ordinal)')\n    plt.ylabel('Value')\n    plt.title('Scatterplot of Values for Each Group Over Time')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, matplotlib, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef perform_eda(df):\n    \"\"\"\n    Perform exploratory data analysis on a dataframe by converting the 'date' column to ordinal, \n    then generate a correlation matrix and a pairplot of the dataframe. \n    \n    Parameters:\n    df (DataFrame): The dataframe with columns 'group', 'date', and 'value'.\n    \n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> df = pd.DataFrame(\n    ...     {\n    ...     \"group\": [\"A\", \"A\", \"A\", \"B\", \"B\"],\n    ...     \"date\": pd.to_datetime([\"2022-01-02\", \"2022-01-13\", \"2022-02-01\", \"2022-02-23\", \"2022-03-05\"]),\n    ...     \"value\": [10, 20, 16, 31, 56],\n    ...     })\n    >>> perform_eda(df)\n    \"\"\"\n    df['date'] = df['date'].apply(lambda x: x.toordinal())\n    correlation_matrix = df.corr()\n\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n    plt.title('Correlation Matrix')\n    plt.show()\n\n    sns.pairplot(df)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, sklearn, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\n\ndef predict_values(df):\n    \"\"\"\n    Convert the 'date' column of a dataframe to ordinal, then create a linear regression model \n    to predict 'value' based on 'date'. Plot the original and predicted values.\n\n    Parameters:\n    df (DataFrame): The dataframe with columns 'group', 'date', and 'value'.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.linear_model\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame(\n    ...     {\n    ...     \"group\": [\"A\", \"A\", \"A\", \"B\", \"B\"],\n    ...     \"date\": pd.to_datetime([\"2022-01-02\", \"2022-01-13\", \"2022-02-01\", \"2022-02-23\", \"2022-03-05\"]),\n    ...     \"value\": [10, 20, 16, 31, 56],\n    ...     })\n    >>> predict_values(df)\n    \"\"\"\n    df['date'] = df['date'].apply(lambda x: x.toordinal())\n\n    X = df[['date']]\n    y = df['value']\n\n    model = LinearRegression()\n    model.fit(X, y)\n    y_pred = model.predict(X)\n\n    plt.scatter(X, y, color = 'red')\n    plt.plot(X, y_pred, color = 'blue')\n    plt.title('Value vs Date (Linear Regression Prediction)')\n    plt.xlabel('Date (ordinal)')\n    plt.ylabel('Value')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, sklearn, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\ndef cluster_values(df):\n    \"\"\"\n    Convert the 'date' column of a dataframe to ordinal, then perform KMeans clustering \n    on the 'date' and 'value' columns. Plot the clusters.\n\n    Parameters:\n    df (DataFrame): The dataframe with columns 'group', 'date', and 'value'.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.cluster\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame(\n    ...     {\n    ...     \"group\": [\"A\", \"A\", \"A\", \"B\", \"B\"],\n    ...     \"date\": pd.to_datetime([\"2022-01-02\", \"2022-01-13\", \"2022-02-01\", \"2022-02-23\", \"2022-03-05\"]),\n    ...     \"value\": [10, 20, 16, 31, 56],\n    ...     })\n    >>> cluster_values(df)\n    \"\"\"\n    df['date'] = df['date'].apply(lambda x: x.toordinal())\n\n    X = df[['date', 'value']]\n\n    kmeans = KMeans(n_clusters=3, random_state=0)\n    kmeans.fit(X)\n    y_kmeans = kmeans.predict(X)\n\n    plt.scatter(X['date'], X['value'], c=y_kmeans, cmap='viridis')\n    plt.title('KMeans Clustering of Value vs Date')\n    plt.xlabel('Date (ordinal)')\n    plt.ylabel('Value')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, matplotlib, statsmodels\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\ndef decompose_time_series(df):\n    \"\"\"\n    Convert the 'date' column of a dataframe to ordinal, then perform a time series decomposition \n    on the 'value' column. Plot the original data, trend, seasonality, and residuals.\n\n    Parameters:\n    df (DataFrame): The dataframe with columns 'group', 'date', and 'value'.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - statsmodels.tsa.seasonal\n\n    Example:\n    >>> df = pd.DataFrame(\n    ...     {\n    ...     \"group\": [\"A\", \"A\", \"A\", \"B\", \"B\"],\n    ...     \"date\": pd.to_datetime([\"2022-01-02\", \"2022-01-13\", \"2022-02-01\", \"2022-02-23\", \"2022-03-05\"]),\n    ...     \"value\": [10, 20, 16, 31, 56],\n    ...     })\n    >>> decompose_time_series(df)\n    \"\"\"\n    df.set_index('date', inplace=True)\n    df['value'] = df['value'].astype(float)\n\n    result = seasonal_decompose(df['value'], model='multiplicative')\n\n    result.plot()\n    plt.show()\n```\n"}
{"seed": "{\"task_id\": 31793195, \"prompt\": \"def f_31793195(df):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"df.index.get_loc('bob')\", \"test_start\": \"\\nimport pandas as pd\\nimport numpy as np\\n\\ndef check(candidate):\", \"test\": [\"\\n    df = pd.DataFrame(data=np.asarray([[1,2,3],[4,5,6],[7,8,9]]), index=['alice', 'bob', 'charlie'])\\n    index = candidate(df)\\n    assert index == 1\\n\"], \"entry_point\": \"f_31793195\", \"intent\": \"Get the integer location of a key `bob` in a pandas data frame `df`\", \"library\": [\"numpy\", \"pandas\"]}", "generation": "Scenario 1:\npandas, numpy, random, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import choice\nimport matplotlib.pyplot as plt\n\n# Constants\nITEMS = ['apple', 'banana', 'grape', 'orange', 'pineapple']\nLOCATIONS = ['store1', 'store2', 'store3', 'store4', 'store5']\n\ndef create_item_location_report(df):\n    \"\"\"\n    Create a bar chart of item distribution across various locations based on a pandas DataFrame.\n    \n    Parameters:\n    df (DataFrame): A pandas DataFrame with 'Item' and 'Location' columns.\n    \n    Returns:\n    None: The function will plot a bar chart.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = pd.DataFrame(data={'Item': [choice(ITEMS) for _ in range(100)], 'Location': [choice(LOCATIONS) for _ in range(100)]})\n    >>> create_item_location_report(df)\n    \"\"\"\n    item_count_df = df.groupby(['Location', 'Item']).size().unstack().fillna(0)\n    item_count_df.plot(kind='bar', stacked=True)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, datetime, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\n\ndef plot_daily_sales(df):\n    \"\"\"\n    Plot daily sales from a pandas DataFrame with 'Date' and 'Sales' columns.\n    \n    Parameters:\n    df (DataFrame): A pandas DataFrame with 'Date' and 'Sales' columns.\n    \n    Returns:\n    None: The function will plot a line chart.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - datetime\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = pd.DataFrame(data={'Date': pd.date_range(start='1/1/2021', end='12/31/2021'), 'Sales': np.random.randint(100,2000,size=365)})\n    >>> plot_daily_sales(df)\n    \"\"\"\n    df['Date'] = pd.to_datetime(df['Date'])\n    df = df.set_index('Date')\n    df.resample('D').sum()['Sales'].plot()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, sklearn, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\n\ndef predict_sales(df):\n    \"\"\"\n    Predict future sales based on historical data using a simple linear regression model.\n    \n    Parameters:\n    df (DataFrame): A pandas DataFrame with 'Sales' and 'Marketing Spend' columns.\n    \n    Returns:\n    None: The function will plot the regression line and the actual data points.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.model_selection\n    - sklearn.linear_model\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = pd.DataFrame(data={'Sales': np.random.randint(100,2000,size=365), 'Marketing Spend': np.random.randint(1000,5000,size=365)})\n    >>> predict_sales(df)\n    \"\"\"\n    X = df[['Marketing Spend']]\n    y = df['Sales']\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    plt.scatter(X, y)\n    plt.plot(X, model.predict(X), color='red')\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, datetime, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nimport seaborn as sns\n\ndef plot_temperature_heatmap(df):\n    \"\"\"\n    Plot a heatmap of temperature data from a pandas DataFrame with 'Date', 'Time' and 'Temperature' columns.\n    \n    Parameters:\n    df (DataFrame): A pandas DataFrame with 'Date', 'Time' and 'Temperature' columns.\n    \n    Returns:\n    None: The function will plot a heatmap.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - datetime\n    - seaborn\n    \n    Example:\n    >>> df = pd.DataFrame(data={'Date': pd.date_range(start='1/1/2021', end='12/31/2021'), 'Time': ['12:00']*365, 'Temperature': np.random.randint(-10,35,size=365)})\n    >>> plot_temperature_heatmap(df)\n    \"\"\"\n    df['Date'] = pd.to_datetime(df['Date'])\n    df['Month'] = df['Date'].dt.month\n    df['Day'] = df['Date'].dt.day\n\n    df_pivot = df.pivot(\"Month\", \"Day\", \"Temperature\")\n    sns.heatmap(df_pivot)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, random, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import choice\nimport matplotlib.pyplot as plt\n\n# Constants\nSTATUSES = ['Pending', 'In Progress', 'Completed', 'Cancelled']\n\ndef plot_status_distribution(df):\n    \"\"\"\n    Plot a pie chart of status distribution from a pandas DataFrame with 'Status' column.\n    \n    Parameters:\n    df (DataFrame): A pandas DataFrame with 'Status' column.\n    \n    Returns:\n    None: The function will plot a pie chart.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = pd.DataFrame(data={'Status': [choice(STATUSES) for _ in range(100)]})\n    >>> plot_status_distribution(df)\n    \"\"\"\n    status_counts = df['Status'].value_counts()\n    plt.pie(status_counts, labels = status_counts.index, autopct='%1.1f%%')\n```"}
{"seed": "{\"task_id\": 10487278, \"prompt\": \"def f_10487278(my_dict):\\n\\t\", \"suffix\": \"\\n\\treturn my_dict\", \"canonical_solution\": \"my_dict.update({'third_key': 1})\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    my_dict = {'a':1, 'b':2}\\n    assert candidate(my_dict) == {'a':1, 'b':2, 'third_key': 1}\\n\", \"\\n    my_dict = {'c':1, 'd':2}\\n    assert candidate(my_dict) == {'c':1, 'd':2, 'third_key': 1}\\n\"], \"entry_point\": \"f_10487278\", \"intent\": \"add an item with key 'third_key' and value 1 to an dictionary `my_dict`\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, re, collections\n```python\nimport pandas as pd\nimport numpy as np\nimport re\nfrom collections import Counter\n\n# Constants\nSPECIAL_CHARACTERS = '!@#$%^&*()_-+={}[]|\\\\:;\"\\'<>,.?/~`'\n\ndef analyze_text_data(my_dict):\n    \"\"\"\n    Analyze a dictionary of text data and add new items with keys 'word_count', \n    'special_char_count', 'numeric_count', 'most_common_word', and 'average_word_length'.\n    \n    Parameters:\n    my_dict (dict): The dictionary which contains a 'text' key with a string value.\n    \n    Returns:\n    dict: The updated dictionary.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - re\n    - collections.Counter\n    \n    Example:\n    >>> my_dict = {'text': 'Hello, world! 123'}\n    >>> analyze_text_data(my_dict)\n    {'text': 'Hello, world! 123', 'word_count': 3, 'special_char_count': 2, 'numeric_count': 3, \n     'most_common_word': 'Hello,', 'average_word_length': 5.0}\n    \"\"\"\n    text = my_dict['text']\n    words = re.findall(r'\\b\\w+\\b', text)\n    special_chars = [char for char in text if char in SPECIAL_CHARACTERS]\n    numerics = [char for char in text if char.isdigit()]\n\n    my_dict.update({\n        'word_count': len(words),\n        'special_char_count': len(special_chars),\n        'numeric_count': len(numerics),\n        'most_common_word': Counter(words).most_common(1)[0][0],\n        'average_word_length': np.mean([len(word) for word in words])\n    })\n\n    return my_dict\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ndatetime, pytz, hashlib, base64\n```python\nfrom datetime import datetime\nimport pytz\nimport hashlib\nimport base64\n\n# Constants\nUTC_TZ = pytz.timezone('UTC')\n\ndef add_timestamp_and_hash(my_dict):\n    \"\"\"\n    Add a UTC timestamp and a SHA256 hash of all values in the dictionary to the dictionary.\n    \n    Parameters:\n    my_dict (dict): The dictionary to update.\n    \n    Returns:\n    dict: The updated dictionary.\n    \n    Requirements:\n    - datetime\n    - pytz\n    - hashlib\n    - base64\n\n    Example:\n    >>> my_dict = {'a': 'Hello', 'b': 'World'}\n    >>> add_timestamp_and_hash(my_dict)\n    {'a': 'Hello', 'b': 'World', 'timestamp': '2022-12-01T12:34:56+00:00', 'hash': 'sB3SCjeZgZjwVg3LHhJxEBh2Ahl9hQhbKJ3GKkzWJHc='}\n    \"\"\"\n    timestamp = datetime.now(UTC_TZ).isoformat()\n    hash_str = ''.join(str(v) for v in my_dict.values())\n    hash_bytes = hashlib.sha256(hash_str.encode()).digest()\n    hash_base64 = base64.b64encode(hash_bytes).decode()\n\n    my_dict.update({\n        'timestamp': timestamp,\n        'hash': hash_base64\n    })\n\n    return my_dict\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\njson, collections, random, string\n```python\nimport json\nfrom collections import Counter\nimport random\nimport string\n\n# Constants\nKEYS = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']\n\ndef add_random_items(my_dict):\n    \"\"\"\n    Add 10 random items to the dictionary. The keys are letters from a to j \n    and the values are random integers from 1 to 100.\n    \n    Parameters:\n    my_dict (dict): The dictionary to update.\n    \n    Returns:\n    dict: The updated dictionary.\n    \n    Requirements:\n    - json\n    - collections.Counter\n    - random\n    - string\n\n    Example:\n    >>> my_dict = {'first_key': 1, 'second_key': 2}\n    >>> add_random_items(my_dict)\n    \"\"\"\n    for key in KEYS:\n        my_dict[key] = random.randint(1, 100)\n\n    return my_dict\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, pandas, sklearn.preprocessing\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Constants\nSCALER = MinMaxScaler()\n\ndef add_normalized_values(my_dict):\n    \"\"\"\n    Add normalized values of a numpy array in the dictionary to the dictionary.\n    \n    Parameters:\n    my_dict (dict): The dictionary which contains an 'array' key with a numpy array value.\n    \n    Returns:\n    dict: The updated dictionary.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.preprocessing.MinMaxScaler\n\n    Example:\n    >>> my_dict = {'array': np.array([1, 2, 3, 4, 5])}\n    >>> add_normalized_values(my_dict)\n    {'array': array([1, 2, 3, 4, 5]), 'normalized_array': array([0.  , 0.25, 0.5 , 0.75, 1.  ])}\n    \"\"\"\n    array = my_dict['array'].reshape(-1, 1)\n    normalized_array = SCALER.fit_transform(array).reshape(-1)\n    \n    my_dict['normalized_array'] = normalized_array\n\n    return my_dict\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, scipy.stats\n```python\nimport numpy as np\nfrom scipy.stats import mode\n\n# Constants\n\ndef add_mode(my_dict):\n    \"\"\"\n    Add the mode of a numpy array in the dictionary to the dictionary.\n    \n    Parameters:\n    my_dict (dict): The dictionary which contains an 'array' key with a numpy array value.\n    \n    Returns:\n    dict: The updated dictionary.\n    \n    Requirements:\n    - numpy\n    - scipy.stats.mode\n\n    Example:\n    >>> my_dict = {'array': np.array([1, 2, 2, 3, 3, 3])}\n    >>> add_mode(my_dict)\n    {'array': array([1, 2, 2, 3, 3, 3]), 'mode': 3}\n    \"\"\"\n    mode_value = mode(my_dict['array']).mode[0]\n    \n    my_dict['mode'] = mode_value\n\n    return my_dict\n```"}
{"seed": "{\"task_id\": 10487278, \"prompt\": \"def f_10487278():\\n\\t\", \"suffix\": \"\\n\\treturn my_list\", \"canonical_solution\": \"my_list = []\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate() == []\\n\"], \"entry_point\": \"f_10487278\", \"intent\": \"declare an array `my_list`\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, random, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import choice\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['Name', 'Age', 'Gender', 'Score']\n\n# Sample data\nNAME_LIST = ['John', 'Mike', 'Sara', 'Emma', 'Nick']\nGENDER_LIST = ['Male', 'Female']\n\ndef generate_student_data(num_of_students):\n    \"\"\"\n    Generate a pandas DataFrame of student data with random values.\n    \n    Parameters:\n    num_of_students (int): The number of students to generate data for.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with student data.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - random\n    \n    Example:\n    >>> student_data = generate_student_data(5)\n    >>> print(student_data)\n    >>> student_data['Score'].hist()\n    \"\"\"\n    data = []\n    for _ in range(num_of_students):\n        name = choice(NAME_LIST)\n        age = np.random.randint(15, 20)\n        gender = choice(GENDER_LIST)\n        score = np.random.randint(50, 100)\n        data.append([name, age, gender, score])\n    \n    df = pd.DataFrame(data, columns=COLUMNS)\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, scipy.stats, matplotlib\n```python\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\n# Constants\nSAMPLE_SIZE = 1000\n\ndef generate_normal_distribution(mu, sigma):\n    \"\"\"\n    Generate a normal distribution with a given mean and standard deviation \n    and plot its histogram and probability density function.\n    \n    Parameters:\n    mu (float): The mean of the distribution.\n    sigma (float): The standard deviation of the distribution.\n    \n    Returns:\n    ndarray: A numpy array sampled from the normal distribution.\n    \n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n    \n    Example:\n    >>> samples = generate_normal_distribution(0, 1)\n    >>> print(samples)\n    >>> plt.hist(samples, bins=30, density=True)\n    >>> xmin, xmax = plt.xlim()\n    >>> x = np.linspace(xmin, xmax, 100)\n    >>> p = stats.norm.pdf(x, mu, sigma)\n    >>> plt.plot(x, p, 'r', linewidth=2)\n    \"\"\"\n    samples = np.random.normal(mu, sigma, SAMPLE_SIZE)\n    return samples\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\njson, os, shutil\n```python\nimport json\nimport os\nimport shutil\n\n# Constants\nDIRECTORY = 'dir'\nBACKUP_DIRECTORY = 'backup_dir'\n\ndef backup_json_files():\n    \"\"\"\n    Scan a directory for JSON files and copy them to a backup directory.\n    \n    Returns:\n    list: A list of copied files paths.\n    \n    Requirements:\n    - json\n    - os\n    - shutil\n    \n    Example:\n    >>> backup_files = backup_json_files()\n    >>> print(backup_files)\n    \"\"\"\n    copied_files = []\n    \n    if not os.path.exists(BACKUP_DIRECTORY):\n        os.makedirs(BACKUP_DIRECTORY)\n        \n    for filename in os.listdir(DIRECTORY):\n        if filename.endswith('.json'):\n            src = os.path.join(DIRECTORY, filename)\n            dst = os.path.join(BACKUP_DIRECTORY, filename)\n            shutil.copy(src, dst)\n            copied_files.append(dst)\n            \n    return copied_files\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, datetime, random\n```python\nimport pandas as pd\nfrom datetime import datetime, timedelta\nfrom random import randint\n\n# Constants\nSTART_DATE = datetime(2020, 1, 1)\nEND_DATE = datetime(2020, 12, 31)\n\ndef generate_date_series(start_date=START_DATE, end_date=END_DATE):\n    \"\"\"\n    Generate a pandas Series of random dates within a given range.\n    \n    Parameters:\n    start_date (datetime): The start of the date range.\n    end_date (datetime): The end of the date range.\n    \n    Returns:\n    Series: A pandas Series of random dates.\n    \n    Requirements:\n    - pandas\n    - datetime\n    - random\n    \n    Example:\n    >>> dates = generate_date_series()\n    >>> print(dates)\n    \"\"\"\n    num_days = (end_date - start_date).days\n    dates = pd.Series([start_date + timedelta(days=randint(0, num_days)) for _ in range(num_days)])\n    return dates\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, matplotlib, math\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\n\n# Constants\nX = np.linspace(-10, 10, 400)\nY = X**2\n\ndef plot_parabola():\n    \"\"\"\n    Generate a plot of a parabola (y = x^2).\n    \n    Returns:\n    None.\n    \n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - math\n    \n    Example:\n    >>> plot_parabola()\n    \"\"\"\n    plt.figure()\n    plt.plot(X, Y)\n    plt.title('y = x^2')\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.grid(True)\n    plt.show()\n```\nAbove are the illustrations."}
{"seed": "{\"task_id\": 10487278, \"prompt\": \"def f_10487278(my_list):\\n\\t\", \"suffix\": \"\\n\\treturn my_list\", \"canonical_solution\": \"my_list.append(12)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([1,2]) == [1, 2, 12] \\n\", \"\\n    assert candidate([5,6]) == [5, 6, 12]\\n\"], \"entry_point\": \"f_10487278\", \"intent\": \"Insert item `12` to a list `my_list`\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, random, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nCATEGORIES = ['Electronics', 'Fashion', 'Home & Kitchen', 'Automotive', 'Sports']\n\ndef simulate_sales_data(my_list):\n    \"\"\"\n    Insert an item '12' to a list `my_list` and simulate sales data for different\n    categories. The sales data is a DataFrame with random sales numbers for each category.\n\n    Parameters:\n    my_list (list): The input list.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with simulated sales data.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> my_list = [1, 2, 3]\n    >>> data = simulate_sales_data(my_list)\n    >>> print(data)\n    >>> data.plot(kind='bar')\n    \"\"\"\n    my_list.append(12)\n\n    sales_data = []\n    for category in CATEGORIES:\n        sales = my_list[randint(0, len(my_list)-1)] * np.random.randint(100, 1000)\n        sales_data.append([category, sales])\n\n    sales_df = pd.DataFrame(sales_data, columns=['Category', 'Sales'])\n\n    return sales_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncollections, random, re\n```python\nfrom collections import Counter\nfrom random import choice\nimport re\n\n# Constants\nWORDS = ['red', 'blue', 'green', 'yellow', 'purple', 'orange']\n\ndef add_word_and_analyze_text(my_list):\n    \"\"\"\n    Insert item '12' to a list `my_list`, add a random color word to a text string,\n    and analyze the frequency of each word in the text.\n    \n    Parameters:\n    my_list (list): The input list.\n    \n    Returns:\n    Counter: A Counter object with the frequency of each word in the text.\n    \n    Requirements:\n    - collections\n    - random\n    - re\n    \n    Example:\n    >>> my_list = ['apple', 'banana', 'cherry']\n    >>> counter = add_word_and_analyze_text(my_list)\n    >>> print(counter)\n    \"\"\"\n    my_list.append(12)\n    text = ' '.join(my_list) + ' ' + choice(WORDS)\n\n    words = re.findall(r'\\w+', text)\n    word_counts = Counter(words)\n\n    return word_counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, random, matplotlib\n```python\nimport numpy as np\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nSIZE = 100\n\ndef add_number_and_generate_array(my_list):\n    \"\"\"\n    Insert item '12' to a list `my_list` and generate a numpy array of random numbers \n    with the size of the sum of the numbers in the list.\n    \n    Parameters:\n    my_list (list): The input list.\n    \n    Returns:\n    numpy.ndarray: A numpy array of random numbers.\n    \n    Requirements:\n    - numpy\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> my_list = [2, 3, 5]\n    >>> array = add_number_and_generate_array(my_list)\n    >>> print(array)\n    >>> plt.hist(array, bins=20)\n    \"\"\"\n    my_list.append(12)\n\n    size = sum(my_list)\n    random_array = np.random.rand(size)\n\n    return random_array\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, os, glob\n```python\nimport pandas as pd\nimport os\nimport glob\n\n# Constants\nFILE_DIR = './data_files/'\nFILE_EXT = '.csv'\n\ndef add_number_and_concat_files(my_list):\n    \"\"\"\n    Insert item '12' to a list `my_list` and concatenate all csv files in a directory \n    into a single DataFrame. The number of files to concatenate is determined by the sum \n    of the numbers in the list.\n    \n    Parameters:\n    my_list (list): The input list.\n    \n    Returns:\n    DataFrame: A pandas DataFrame that concatenates all csv files.\n    \n    Requirements:\n    - pandas\n    - os\n    - glob\n    \n    Example:\n    >>> my_list = [1, 2, 3]\n    >>> df = add_number_and_concat_files(my_list)\n    >>> print(df)\n    \"\"\"\n    my_list.append(12)\n\n    num_files = sum(my_list)\n    files = glob.glob(os.path.join(FILE_DIR, '*' + FILE_EXT))[:num_files]\n\n    data_frames = []\n    for file in files:\n        df = pd.read_csv(file)\n        data_frames.append(df)\n\n    concatenated_df = pd.concat(data_frames, ignore_index=True)\n\n    return concatenated_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nrandom, time, matplotlib\n```python\nfrom random import randint\nimport time\nimport matplotlib.pyplot as plt\n\n# Constants\nSIZE = 100\n\ndef add_number_and_measure_time(my_list):\n    \"\"\"\n    Insert item '12' to a list `my_list` and measure the time it takes to generate \n    a list of random numbers. The size of the list is determined by the sum of the numbers in the list.\n    \n    Parameters:\n    my_list (list): The input list.\n    \n    Returns:\n    float: The time it takes to generate the list.\n    \n    Requirements:\n    - random\n    - time\n    - matplotlib.pyplot\n    \n    Example:\n    >>> my_list = [2, 3, 5]\n    >>> time_taken = add_number_and_measure_time(my_list)\n    >>> print(time_taken)\n    \"\"\"\n    my_list.append(12)\n\n    size = sum(my_list)\n\n    start_time = time.time()\n    random_list = [randint(1, 100) for _ in range(size)]\n    end_time = time.time()\n\n    plt.hist(random_list, bins=20)\n\n    return end_time - start_time\n```"}
{"seed": "{\"task_id\": 10155684, \"prompt\": \"def f_10155684(myList):\\n\\t\", \"suffix\": \"\\n\\treturn myList\", \"canonical_solution\": \"myList.insert(0, 'wuggah')\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([1,2]) == ['wuggah', 1, 2]\\n\", \"\\n    assert candidate([]) == ['wuggah'] \\n\"], \"entry_point\": \"f_10155684\", \"intent\": \"add an entry 'wuggah' at the beginning of list `myList`\", \"library\": []}", "generation": "Scenario 1: \npandas, random, matplotlib, statistics, numpy\n```python\nimport pandas as pd\nfrom random import randint, choice\nimport matplotlib.pyplot as plt\nimport statistics\nimport numpy as np\n\n# Constants\nANIMALS = ['Lion', 'Elephant', 'Tiger', 'Giraffe', 'Panda']\n\ndef generate_zoo_report():\n    \"\"\"\n    Generate a report of animal counts in a zoo. For each animal, generate a random count,\n    calculate mean, median, mode and standard deviation of the counts and plot a bar graph.\n\n    Returns:\n    DataFrame: A pandas DataFrame with animal counts and their statistics.\n\n    Requirements:\n    - pandas\n    - random\n    - matplotlib.pyplot\n    - statistics\n    - numpy\n\n    Example:\n    >>> report = generate_zoo_report()\n    >>> print(report)\n    >>> report['Count'].plot(kind='bar')\n    \"\"\"\n    report_data = []\n\n    for animal in ANIMALS:\n        counts = [randint(1, 100) for _ in range(10)]\n        mean = statistics.mean(counts)\n        median = statistics.median(counts)\n        mode = statistics.mode(counts)\n        std_dev = np.std(counts)\n        report_data.append([animal, mean, median, mode, std_dev])\n    \n    report_df = pd.DataFrame(report_data, columns=['Animal', 'Mean', 'Median', 'Mode', 'Standard Deviation'])\n\n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncollections, itertools, json, random\n```python\nfrom collections import defaultdict\nimport itertools\nimport json\nfrom random import randint, choice\n\n# Constants\nLETTERS = ['a', 'b', 'c', 'd', 'e']\n\ndef generate_letter_combinations(n):\n    \"\"\"\n    Generate all combinations of a given set of letters of length 'n', count the occurrences of each letter in \n    the combinations, and save the result to a JSON file.\n\n    Parameters:\n    n (int): The length of the combinations.\n\n    Returns:\n    str: The name of the JSON file.\n\n    Requirements:\n    - collections\n    - itertools\n    - json\n    - random\n\n    Example:\n    >>> generate_letter_combinations(3)\n    \"\"\"\n    combinations = list(itertools.combinations(LETTERS, n))\n    letter_counts = defaultdict(int)\n\n    for combination in combinations:\n        for letter in combination:\n            letter_counts[letter] += 1\n\n    filename = f'letter_combinations_{randint(1, 100)}.json'\n    with open(filename, 'w') as f:\n        json.dump(letter_counts, f)\n\n    return filename\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, shutil, glob, hashlib\n```python\nimport os\nimport shutil\nimport glob\nimport hashlib\n\n# Constants\nROOT_DIR = '/path/to/root/dir'\nDEST_DIR = '/path/to/dest/dir'\n\ndef move_files_with_same_hash():\n    \"\"\"\n    Move all files with the same hash from a root directory to a destination directory.\n\n    Returns:\n    int: The number of files moved.\n\n    Requirements:\n    - os\n    - shutil\n    - glob\n    - hashlib\n\n    Example:\n    >>> move_files_with_same_hash()\n    \"\"\"\n    files_moved = 0\n\n    os.makedirs(DEST_DIR, exist_ok=True)\n\n    for filename in glob.glob(os.path.join(ROOT_DIR, '*')):\n        with open(filename, 'rb') as f:\n            file_hash = hashlib.md5(f.read()).hexdigest()\n\n        if file_hash == 'some hash value':\n            shutil.move(filename, DEST_DIR)\n            files_moved += 1\n\n    return files_moved\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ndatetime, requests, bs4, pandas\n```python\nfrom datetime import datetime\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\n# Constants\nURL = 'http://example.com'\n\ndef scrape_website():\n    \"\"\"\n    Scrape a website and extract data into a pandas DataFrame.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the scraped data.\n\n    Requirements:\n    - datetime\n    - requests\n    - bs4\n    - pandas\n\n    Example:\n    >>> df = scrape_website()\n    >>> print(df)\n    \"\"\"\n    response = requests.get(URL)\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    data = []\n    for row in soup.find_all('tr'):\n        cols = row.find_all('td')\n        cols = [ele.text.strip() for ele in cols]\n        data.append([ele for ele in cols if ele])\n\n    df = pd.DataFrame(data)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, matplotlib, random, math\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\nimport math\n\n# Constants\nPOINTS = 100\n\ndef plot_random_walk():\n    \"\"\"\n    Simulate a random walk and plot the path.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - random\n    - math\n\n    Example:\n    >>> plot_random_walk()\n    \"\"\"\n    x = np.zeros(POINTS)\n    y = np.zeros(POINTS)\n\n    for i in range(1, POINTS):\n        val = randint(0, 1)\n        if val == 1:\n            x[i] = x[i - 1] + math.cos(2 * math.pi * val)\n            y[i] = y[i - 1] + math.sin(2 * math.pi * val)\n        else:\n            x[i] = x[i - 1] - math.cos(2 * math.pi * val)\n            y[i] = y[i - 1] - math.sin(2 * math.pi * val)\n\n    plt.plot(x, y)\n    plt.show()\n```"}
{"seed": "{\"task_id\": 3519125, \"prompt\": \"def f_3519125(hex_str):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"bytes.fromhex(hex_str.replace('\\\\\\\\x', ''))\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(\\\"\\\\\\\\xF3\\\\\\\\xBE\\\\\\\\x80\\\\\\\\x80\\\") == b'\\\\xf3\\\\xbe\\\\x80\\\\x80'\\n\"], \"entry_point\": \"f_3519125\", \"intent\": \"convert a hex-string representation `hex_str` to actual bytes\", \"library\": []}", "generation": "Scenario 1: \nbase64, binascii, os, hashlib\n```python\nimport base64\nimport binascii\nimport os\nimport hashlib\n\n# Constants\nSALT_SIZE = 16\n\ndef secure_hash(hex_str):\n    \"\"\"\n    Convert a hex-string representation `hex_str` to actual bytes, salt it with a \n    random value, and compute its SHA256 hash.\n    \n    Parameters:\n    hex_str (str): The hex string to be hashed.\n    \n    Returns:\n    tuple: A tuple containing the salt and the hash.\n    \n    Requirements:\n    - base64\n    - binascii\n    - os\n    - hashlib\n    \n    Example:\n    >>> secure_hash(\"\\\\xF3\\\\xBE\\\\x80\\\\x80\")\n    \"\"\"\n    salt = os.urandom(SALT_SIZE)\n    data = binascii.unhexlify(hex_str.replace('\\\\x', ''))\n    salted_data = salt + data\n    hash_value = hashlib.sha256(salted_data).hexdigest()\n    \n    return (base64.b64encode(salt).decode('utf-8'), hash_value)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nhashlib, binascii, hmac, time\n```python\nimport hashlib\nimport binascii\nimport hmac\nimport time\n\nKEY = 'secret_key'\n\ndef compute_hmac(hex_str):\n    \"\"\"\n    Convert a hex-string representation `hex_str` to actual bytes and compute \n    its HMAC-SHA256 using a secret key.\n    \n    Parameters:\n    hex_str (str): The hex string.\n    \n    Returns:\n    str: The HMAC-SHA256 value.\n    \n    Requirements:\n    - hashlib\n    - binascii\n    - hmac\n    - time\n    \n    Example:\n    >>> compute_hmac(\"\\\\xF3\\\\xBE\\\\x80\\\\x80\")\n    \"\"\"\n    data = binascii.unhexlify(hex_str.replace('\\\\x', ''))\n    hmac_value = hmac.new(KEY.encode('utf-8'), data, hashlib.sha256).hexdigest()\n    \n    return hmac_value\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nbinascii, zlib, base64, random\n```python\nimport binascii\nimport zlib\nimport base64\nimport random\n\ndef compress_and_encode(hex_str):\n    \"\"\"\n    Convert a hex-string representation `hex_str` to actual bytes, compress it \n    using zlib, and then base64 encode it.\n    \n    Parameters:\n    hex_str (str): The hex string.\n    \n    Returns:\n    str: The base64 encoded compressed data.\n    \n    Requirements:\n    - binascii\n    - zlib\n    - base64\n    - random\n    \n    Example:\n    >>> compress_and_encode(\"\\\\xF3\\\\xBE\\\\x80\\\\x80\")\n    \"\"\"\n    data = binascii.unhexlify(hex_str.replace('\\\\x', ''))\n    compressed_data = zlib.compress(data)\n    encoded_data = base64.b64encode(compressed_data).decode('utf-8')\n    \n    return encoded_data\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nbinascii, hashlib, sqlite3, os\n```python\nimport binascii\nimport hashlib\nimport sqlite3\nimport os\n\nDB_PATH = 'hex_data.db'\n\ndef store_hash_in_db(hex_str):\n    \"\"\"\n    Convert a hex-string representation `hex_str` to actual bytes and store its \n    SHA256 hash in a SQLite database.\n    \n    Parameters:\n    hex_str (str): The hex string.\n    \n    Returns:\n    str: The SHA256 hash.\n    \n    Requirements:\n    - binascii\n    - hashlib\n    - sqlite3\n    - os\n    \n    Example:\n    >>> store_hash_in_db(\"\\\\xF3\\\\xBE\\\\x80\\\\x80\")\n    \"\"\"\n    data = binascii.unhexlify(hex_str.replace('\\\\x', ''))\n    hash_value = hashlib.sha256(data).hexdigest()\n    \n    conn = sqlite3.connect(DB_PATH)\n    c = conn.cursor()\n    c.execute(\"CREATE TABLE IF NOT EXISTS hashes (hash TEXT)\")\n    c.execute(\"INSERT INTO hashes VALUES (?)\", (hash_value,))\n    conn.commit()\n    conn.close()\n    \n    return hash_value\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nbinascii, numpy, matplotlib.pyplot, pandas\n```python\nimport binascii\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef plot_byte_values(hex_str):\n    \"\"\"\n    Convert a hex-string representation `hex_str` to actual bytes and plot \n    the frequency of each byte value.\n    \n    Parameters:\n    hex_str (str): The hex string.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - binascii\n    - numpy\n    - matplotlib.pyplot\n    - pandas\n    \n    Example:\n    >>> plot_byte_values(\"\\\\xF3\\\\xBE\\\\x80\\\\x80\")\n    \"\"\"\n    data = binascii.unhexlify(hex_str.replace('\\\\x', ''))\n    byte_values, counts = np.unique(list(data), return_counts=True)\n    \n    df = pd.DataFrame({\n        'Byte Value': byte_values,\n        'Frequency': counts\n    })\n    \n    df.plot(kind='bar', x='Byte Value', y='Frequency', title='Frequency of Byte Values')\n    plt.show()\n```"}
{"seed": "{\"task_id\": 40144769, \"prompt\": \"def f_40144769(df):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"df[df.columns[-1]]\", \"test_start\": \"\\nimport pandas as pd \\n\\ndef check(candidate):\", \"test\": [\"\\n    df = pd.DataFrame([[1, 2, 3],[4,5,6]], columns=[\\\"a\\\", \\\"b\\\", \\\"c\\\"])\\n    assert candidate(df).tolist() == [3,6]\\n\", \"\\n    df = pd.DataFrame([[\\\"Hello\\\", \\\"world!\\\"],[\\\"Hi\\\", \\\"world!\\\"]], columns=[\\\"a\\\", \\\"b\\\"])\\n    assert candidate(df).tolist() == [\\\"world!\\\", \\\"world!\\\"]\\n\"], \"entry_point\": \"f_40144769\", \"intent\": \"select the last column of dataframe `df`\", \"library\": [\"pandas\"]}", "generation": "Scenario 1:\nnumpy, pandas, matplotlib, sklearn\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\n\ndef normalize_and_plot(df):\n    \"\"\"\n    Normalize the last column of the dataframe using sklearn's MinMaxScaler and then plot it.\n    \n    Parameters:\n    df (DataFrame): The input dataframe.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the normalized last column.\n    Matplotlib figure: A plot of the normalized data.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - sklearn.preprocessing\n    \n    Example:\n    >>> df = pd.DataFrame(np.random.randint(0,100,size=(100, 4)), columns=list('ABCD'))\n    >>> normalized_df, fig = normalize_and_plot(df)\n    >>> plt.show()\n    \"\"\"\n    last_col = df.columns[-1]\n    data = df[last_col].values.reshape(-1, 1)\n    min_max_scaler = preprocessing.MinMaxScaler()\n    df_normalized = min_max_scaler.fit_transform(data)\n    df[last_col] = df_normalized\n\n    fig, ax = plt.subplots()\n    ax.plot(df_normalized)\n    ax.set_title('Normalized Data')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n\n    return df, fig\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, matplotlib, numpy\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_histogram(df):\n    \"\"\"\n    Plot a histogram of the last column of the dataframe.\n    \n    Parameters:\n    df (DataFrame): The input dataframe.\n    \n    Returns:\n    Matplotlib figure: A histogram of the last column of the dataframe.\n    \n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - numpy\n    \n    Example:\n    >>> df = pd.DataFrame(np.random.randint(0,100,size=(100, 4)), columns=list('ABCD'))\n    >>> fig = plot_histogram(df)\n    >>> plt.show()\n    \"\"\"\n    last_col = df.columns[-1]\n    fig, ax = plt.subplots()\n    ax.hist(df[last_col], bins=20)\n    ax.set_title('Histogram of last column')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    return fig\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, pandas, sklearn, seaborn\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.impute import SimpleImputer\nimport seaborn as sns\n\ndef impute_and_visualize(df):\n    \"\"\"\n    Impute missing values in the last column of the dataframe using mean imputation, \n    then plot a boxplot to visualize the distribution of data in the last column.\n    \n    Parameters:\n    df (DataFrame): The input dataframe.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the imputed last column.\n    Seaborn boxplot: A boxplot of the last column of the dataframe.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.impute\n    - seaborn\n    \n    Example:\n    >>> df = pd.DataFrame(np.random.randint(0,100,size=(100, 4)), columns=list('ABCD'))\n    >>> df.iloc[::3, -1] = np.nan  # Insert some NaN values\n    >>> imputed_df, boxplot = impute_and_visualize(df)\n    \"\"\"\n    last_col = df.columns[-1]\n    imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n    df[last_col] = imp_mean.fit_transform(df[last_col].values.reshape(-1, 1))\n\n    boxplot = sns.boxplot(x=df[last_col])\n\n    return df, boxplot\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, pandas, sklearn, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\ndef pca_and_plot(df):\n    \"\"\"\n    Perform Principal Component Analysis (PCA) on the dataframe and plot the two principal components.\n    \n    Parameters:\n    df (DataFrame): The input dataframe.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the principal components.\n    Matplotlib figure: A scatter plot of the two principal components.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.decomposition\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = pd.DataFrame(np.random.randint(0,100,size=(100, 4)), columns=list('ABCD'))\n    >>> pca_df, fig = pca_and_plot(df)\n    >>> plt.show()\n    \"\"\"\n    pca = PCA(n_components=2)\n    principal_components = pca.fit_transform(df)\n    \n    pca_df = pd.DataFrame(data = principal_components, columns = ['Principal Component 1', 'Principal Component 2'])\n\n    fig, ax = plt.subplots()\n    ax.scatter(pca_df['Principal Component 1'], pca_df['Principal Component 2'])\n    ax.set_title('2 Component PCA')\n    ax.set_xlabel('Principal Component 1')\n    ax.set_ylabel('Principal Component 2')\n\n    return pca_df, fig\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, pandas, scipy\n```python\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import skew\n\ndef calculate_skewness(df):\n    \"\"\"\n    Calculate the skewness of the last column of the dataframe.\n    \n    Parameters:\n    df (DataFrame): The input dataframe.\n    \n    Returns:\n    float: The skewness of the last column of the dataframe.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - scipy.stats\n    \n    Example:\n    >>> df = pd.DataFrame(np.random.randint(0,100,size=(100, 4)), columns=list('ABCD'))\n    >>> skewness = calculate_skewness(df)\n    \"\"\"\n    last_col = df.columns[-1]\n    skewness = skew(df[last_col])\n\n    return skewness\n```"}
{"seed": "{\"task_id\": 30787901, \"prompt\": \"def f_30787901(df):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"df.loc[df['Letters'] == 'C', 'Letters'].values[0]\", \"test_start\": \"\\nimport pandas as pd \\n\\ndef check(candidate):\", \"test\": [\"\\n    df = pd.DataFrame([[\\\"a\\\", 1],[\\\"C\\\", 6]], columns=[\\\"Letters\\\", \\\"Numbers\\\"])\\n    assert candidate(df) == 'C'\\n\", \"\\n    df = pd.DataFrame([[None, 1],[\\\"C\\\", 789]], columns=[\\\"Letters\\\", \\\"Names\\\"])\\n    assert candidate(df) == 'C'\\n\"], \"entry_point\": \"f_30787901\", \"intent\": \"get the first value from dataframe `df` where column 'Letters' is equal to 'C'\", \"library\": [\"pandas\"]}", "generation": "Scenario 1: \npandas, numpy, random, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\n\n# Constants\nLETTERS = list('abcdefghijklmnopqrstuvwxyz'.upper())\nNUMBERS = list(range(1, 27))\n\ndef generate_letter_frequency(df):\n    \"\"\"\n    Generate a bar plot of letter frequencies in a dataframe where column 'Letters' \n    contains uppercase English alphabets.\n    \n    Parameters:\n    df (DataFrame): The dataframe with a 'Letters' column.\n    \n    Returns:\n    None: Plots a bar graph of letter frequency.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = pd.DataFrame({'Letters': random.choices(LETTERS, k=100)})\n    >>> generate_letter_frequency(df)\n    \"\"\"\n    letter_frequency = df['Letters'].value_counts()\n    letter_frequency.plot(kind='bar')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_distributions(df):\n    \"\"\"\n    Plot the distributions of numeric columns in a dataframe.\n    \n    Parameters:\n    df (DataFrame): The dataframe.\n    \n    Returns:\n    None: Plots a histogram for each numeric column.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = pd.DataFrame({'A': np.random.normal(0, 1, 1000), 'B': np.random.exponential(1, 1000)})\n    >>> plot_distributions(df)\n    \"\"\"\n    numeric_cols = df.select_dtypes(include=np.number).columns\n    for col in numeric_cols:\n        df[col].plot(kind='hist', title=col)\n        plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, sklearn.preprocessing\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\n\ndef standardize_columns(df, cols):\n    \"\"\"\n    Standardize specified numeric columns in a dataframe.\n    \n    Parameters:\n    df (DataFrame): The dataframe.\n    cols (list): The columns to standardize.\n    \n    Returns:\n    DataFrame: The dataframe with standardized columns.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.preprocessing.StandardScaler\n    \n    Example:\n    >>> df = pd.DataFrame({'A': np.random.normal(0, 1, 1000), 'B': np.random.exponential(1, 1000)})\n    >>> df = standardize_columns(df, ['A', 'B'])\n    >>> print(df.describe())\n    \"\"\"\n    scaler = StandardScaler()\n    df[cols] = scaler.fit_transform(df[cols])\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, matplotlib, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef plot_correlation_matrix(df):\n    \"\"\"\n    Plot the correlation matrix of a dataframe's numeric columns.\n    \n    Parameters:\n    df (DataFrame): The dataframe.\n    \n    Returns:\n    None: Plots a heatmap of the correlation matrix.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - seaborn\n    \n    Example:\n    >>> df = pd.DataFrame({'A': np.random.normal(0, 1, 1000), 'B': np.random.exponential(1, 1000), 'C': np.random.uniform(-1, 1, 1000)})\n    >>> plot_correlation_matrix(df)\n    \"\"\"\n    corr_matrix = df.corr()\n    sns.heatmap(corr_matrix, annot=True)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, sklearn.model_selection, sklearn.linear_model\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\ndef train_linear_regression(df, target):\n    \"\"\"\n    Train a linear regression model on a dataframe with a specified target column.\n    \n    Parameters:\n    df (DataFrame): The dataframe.\n    target (str): The target column.\n    \n    Returns:\n    LinearRegression: The trained Linear Regression model.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.model_selection.train_test_split\n    - sklearn.linear_model.LinearRegression\n    \n    Example:\n    >>> df = pd.DataFrame({'A': np.random.normal(0, 1, 1000), 'B': np.random.exponential(1, 1000), 'C': np.random.uniform(-1, 1, 1000)})\n    >>> model = train_linear_regression(df, 'C')\n    \"\"\"\n    X = df.drop(target, axis=1)\n    y = df[target]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    model = LinearRegression().fit(X_train, y_train)\n    return model\n```"}
{"seed": "{\"task_id\": 18730044, \"prompt\": \"def f_18730044():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"np.column_stack(([1, 2, 3], [4, 5, 6]))\", \"test_start\": \"\\nimport numpy as np \\n\\ndef check(candidate):\", \"test\": [\"\\n    assert np.all(candidate() == np.array([[1, 4], [2, 5], [3, 6]]))\\n\"], \"entry_point\": \"f_18730044\", \"intent\": \"converting two lists `[1, 2, 3]` and `[4, 5, 6]` into a matrix\", \"library\": [\"numpy\"]}", "generation": "Scenario 1: \nnumpy, pandas, statistics, random\n```python\nimport numpy as np\nimport pandas as pd\nimport statistics\nfrom random import choice\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E', 'F']\n\ndef generate_random_dataframe(rows):\n    \"\"\"\n    Generate a pandas DataFrame with a given number of rows and six columns (A-F),\n    each filled with random numbers between 1 and 100. Also, calculate the mean\n    and median for each column.\n\n    Parameters:\n    rows (int): The number of rows in the DataFrame.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the generated data.\n    dict: A dictionary with the mean and median for each column.\n\n    Requirements:\n    - numpy\n    - pandas\n    - statistics\n    - random\n\n    Example:\n    >>> df, stats = generate_random_dataframe(10)\n    >>> print(df)\n    >>> print(stats)\n    \"\"\"\n    data = np.random.randint(1, 100, size=(rows, len(COLUMNS)))\n    df = pd.DataFrame(data, columns=COLUMNS)\n    \n    stats = {}\n    for col in COLUMNS:\n        stats[col] = {\n            'mean': statistics.mean(df[col]),\n            'median': statistics.median(df[col])\n        }\n    \n    return df, stats\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, matplotlib, math\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\n\n# Constants\nX_VALUES = np.linspace(0, 2 * np.pi, 400)\n\ndef plot_sin_cos():\n    \"\"\"\n    Plot two subplots. The first subplot is the sine function from 0 to 2pi. \n    The second subplot is the cosine function from 0 to 2pi.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - math\n\n    Example:\n    >>> plot_sin_cos()\n    \"\"\"\n    fig, axs = plt.subplots(2)\n    \n    axs[0].plot(X_VALUES, np.sin(X_VALUES))\n    axs[0].set_title('Sine function')\n    \n    axs[1].plot(X_VALUES, np.cos(X_VALUES))\n    axs[1].set_title('Cosine function')\n    \n    plt.tight_layout()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnumpy, scipy, matplotlib\n```python\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\n# Constants\nDATA = np.random.normal(loc=10, scale=5, size=100)\n\ndef plot_normal_distribution():\n    \"\"\"\n    Fit a normal distribution to the data and plot the PDF over the data. \n\n    Requirements:\n    - numpy\n    - scipy\n    - matplotlib.pyplot \n\n    Example:\n    >>> plot_normal_distribution()\n    \"\"\"\n    mu, sigma = stats.norm.fit(DATA)\n    x = np.linspace(min(DATA), max(DATA), 100)\n    pdf = stats.norm.pdf(x, mu, sigma)\n\n    plt.hist(DATA, density=True, alpha=0.6, color='g')\n    plt.plot(x, pdf, 'k', linewidth=2)\n    title = \"Fit results: mu = %.2f,  std = %.2f\" % (mu, sigma)\n    plt.title(title)\n    \n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, sympy, matplotlib\n```python\nimport numpy as np\nfrom sympy import symbols, Eq, solve\nimport matplotlib.pyplot as plt\n\n# Constants\nX = np.linspace(-10, 10, 400)\n\ndef plot_equation_solution():\n    \"\"\"\n    Plot the equation y = 2x + 1 and its solution for x = 2.\n\n    Requirements:\n    - numpy\n    - sympy\n    - matplotlib.pyplot \n\n    Example:\n    >>> plot_equation_solution()\n    \"\"\"\n    y = 2 * X + 1\n\n    plt.plot(X, y, '-r', label='y=2x+1')\n    \n    x, y = symbols('x y')\n    equation = Eq(2*x + 1, y)\n    solution = solve(equation.subs(x, 2))\n    \n    plt.plot(2, solution[0], 'go')\n    plt.title('Solution of the equation at x=2')\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.legend(loc='best')\n    plt.grid()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, pandas, sklearn, seaborn\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import load_iris\nimport seaborn as sns\n\n# Load iris dataset\niris = load_iris()\niris_df = pd.DataFrame(data=np.c_[iris['data'], iris['target']],\n                       columns=iris['feature_names'] + ['target'])\n\ndef visualize_iris_dataset():\n    \"\"\"\n    Load the iris dataset, convert it to a pandas DataFrame and visualize \n    pairplot with seaborn.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the iris dataset.\n\n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.datasets\n    - seaborn\n\n    Example:\n    >>> df = visualize_iris_dataset()\n    >>> print(df)\n    \"\"\"\n    \n    sns.pairplot(iris_df, hue=\"target\")\n    plt.show()\n\n    return iris_df\n```"}
{"seed": "{\"task_id\": 402504, \"prompt\": \"def f_402504(i):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"type(i)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(\\\"hello\\\") is str\\n\", \"\\n    assert candidate(123) is int\\n\", \"\\n    assert candidate(\\\"123\\\") is str\\n\", \"\\n    assert candidate(123.4) is float\\n\"], \"entry_point\": \"f_402504\", \"intent\": \"get the type of `i`\", \"library\": []}", "generation": "Scenario 1:\npandas, numpy, matplotlib, math\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\n\ndef stats_analysis(data):\n    \"\"\"\n    Perform statistical analysis on a given list of numerical data, \n    including mean, median, mode, standard deviation, and visualization of data distribution.\n    \n    Parameters:\n    data (list): The list of numerical data.\n    \n    Returns:\n    dict: A dictionary containing the mean, median, mode and standard deviation.\n    matplotlib object: A histogram visualizing the data distribution.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - math\n    \n    Example:\n    >>> data = [2, 3, 4, 2, 2, 3, 5, 6, 2, 3, 4, 5, 6, 7, 8, 9, 6, 5, 4, 3]\n    >>> stats_analysis(data)\n    \"\"\"\n    df = pd.DataFrame(data, columns=['Values'])\n    mean = df['Values'].mean()\n    median = df['Values'].median()\n    mode = df['Values'].mode().values[0]\n    std_dev = math.sqrt(df['Values'].var())\n    \n    result = {\n        \"Mean\": mean,\n        \"Median\": median,\n        \"Mode\": mode,\n        \"Standard Deviation\": std_dev\n    }\n    \n    plt.hist(df['Values'], bins=np.arange(min(data)-0.5, max(data)+1, 1), edgecolor='black')\n    plt.title('Data Distribution')\n    plt.xlabel('Values')\n    plt.ylabel('Frequency')\n    \n    plt.show()\n    \n    return result\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, sympy, matplotlib\n```python\nimport numpy as np\nfrom sympy import symbols, Eq, solve\nimport matplotlib.pyplot as plt\n\ndef quadratic_solver_visualizer(a, b, c):\n    \"\"\"\n    Solve a quadratic equation of the form ax^2 + bx + c = 0, and visualize the function and its roots.\n    \n    Parameters:\n    a (int): The coefficient of x^2.\n    b (int): The coefficient of x.\n    c (int): The constant term.\n    \n    Returns:\n    list: The roots of the quadratic equation.\n    matplotlib object: A plot of the quadratic function and its roots.\n    \n    Requirements:\n    - numpy\n    - sympy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> quadratic_solver_visualizer(1, -3, 2)\n    \"\"\"\n    x = symbols('x')\n    eq = Eq(a*x**2 + b*x + c, 0)\n    roots = solve(eq)\n    \n    x_values = np.linspace(-10, 10, 400)\n    y_values = a*x_values**2 + b*x_values + c\n    \n    plt.plot(x_values, y_values, label='y = {}x^2 + {}x + {}'.format(a, b, c))\n    plt.plot([float(root) for root in roots], [0]*len(roots), 'ro', label='Roots')\n    plt.title('Quadratic Function and its Roots')\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.legend()\n    \n    plt.show()\n    \n    return [float(root) for root in roots]\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, scipy.stats\n```python\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import pearsonr\n\ndef correlation_analysis(data1, data2):\n    \"\"\"\n    Perform correlation analysis between two given lists of numerical data using Pearson's correlation coefficient.\n    \n    Parameters:\n    data1 (list): The first list of numerical data.\n    data2 (list): The second list of numerical data.\n    \n    Returns:\n    float: The Pearson's correlation coefficient.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - scipy.stats.pearsonr\n    \n    Example:\n    >>> data1 = [1, 2, 3, 4, 5]\n    >>> data2 = [2, 4, 6, 8, 10]\n    >>> correlation_analysis(data1, data2)\n    \"\"\"\n    df = pd.DataFrame({'Data1': data1, 'Data2': data2})\n    correlation, _ = pearsonr(df['Data1'], df['Data2'])\n    \n    return correlation\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, shutil, glob\n```python\nimport os\nimport shutil\nimport glob\n\ndef copy_specific_files(source_dir, destination_dir, file_extension):\n    \"\"\"\n    Copy all files with a certain extension from one directory to another.\n    \n    Parameters:\n    source_dir (str): The source directory.\n    destination_dir (str): The destination directory.\n    file_extension (str): The file extension.\n    \n    Returns:\n    int: The number of files copied.\n    \n    Requirements:\n    - os\n    - shutil\n    - glob\n    \n    Example:\n    >>> copy_specific_files('./source', './destination', '.txt')\n    \"\"\"\n    files = glob.glob(os.path.join(source_dir, '*{}'.format(file_extension)))\n    \n    for file in files:\n        shutil.copy(file, destination_dir)\n        \n    return len(files)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, seaborn, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef heatmap_correlation_matrix(df):\n    \"\"\"\n    Calculate the correlation matrix of a DataFrame and visualize it using a heatmap.\n    \n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n    \n    Returns:\n    DataFrame: The correlation matrix of the DataFrame.\n    seaborn object: A heatmap of the correlation matrix.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - seaborn\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = pd.DataFrame({'A': [1, 2, 3, 4, 5], 'B': [1, 3, 5, 7, 9], 'C': [2, 4, 6, 8, 10]})\n    >>> heatmap_correlation_matrix(df)\n    \"\"\"\n    correlation_matrix = df.corr()\n    \n    plt.figure(figsize=(8, 6))\n    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n    plt.title('Correlation Matrix Heatmap')\n    \n    plt.show()\n    \n    return correlation_matrix\n```"}
{"seed": "{\"task_id\": 402504, \"prompt\": \"def f_402504(v):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"type(v)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(\\\"hello\\\") is str\\n\", \"\\n    assert candidate(123) is int\\n\", \"\\n    assert candidate(\\\"123\\\") is str\\n\", \"\\n    assert candidate(123.4) is float\\n\"], \"entry_point\": \"f_402504\", \"intent\": \"determine the type of variable `v`\", \"library\": []}", "generation": "Scenario 1:\nnumpy, pandas, matplotlib, scipy\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n# Constants\nALPHABET = 'abcdefghijklmnopqrstuvwxyz'\nLETTER_DISTRIBUTION = [8.167, 1.492, 2.782, 4.253, 12.702, 2.228, 2.015, 6.094, 6.966, \n                       0.153, 0.772, 4.025, 2.406, 6.749, 7.507, 1.929, 0.095, 5.987, \n                       6.327, 9.056, 2.758, 0.978, 2.360, 0.150, 1.974, 0.074] # English letter frequencies\n\ndef text_analysis(text):\n    \"\"\"\n    Analyze a text string: calculate the frequencies of each letter in the alphabet, \n    compare it to the standard English letter distribution, and plot the distributions.\n    \n    Parameters:\n    text (str): The text to analyze.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the letter frequencies in the text.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - scipy.stats\n    \n    Example:\n    >>> text = \"Hello, World!\"\n    >>> df = text_analysis(text)\n    >>> print(df)\n    >>> df[['Actual Frequency', 'Expected Frequency']].plot(kind='bar')\n    \"\"\"\n    text = text.lower()\n    letter_counts = [text.count(letter) for letter in ALPHABET]\n    total_letters = sum(letter_counts)\n    actual_distribution = [count / total_letters for count in letter_counts]\n    \n    expected_distribution = np.array(LETTER_DISTRIBUTION) / 100\n    \n    chi2, p = stats.chisquare(actual_distribution, expected_distribution)\n    \n    df = pd.DataFrame({\n        'Letter': list(ALPHABET),\n        'Actual Frequency': actual_distribution,\n        'Expected Frequency': expected_distribution,\n        'Chi-Square': chi2,\n        'P-value': p\n    })\n    \n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, pandas, scipy, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\n\ndef variable_analysis(df, variable):\n    \"\"\"\n    Analyze a variable in a pandas DataFrame: calculate the mean, median, \n    mode, standard deviation, skewness, and kurtosis, and plot a histogram \n    and boxplot.\n    \n    Parameters:\n    df (DataFrame): The pandas DataFrame containing the data.\n    variable (str): The name of the variable to analyze.\n    \n    Returns:\n    dict: A dictionary with the calculated statistics.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - scipy.stats\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = pd.DataFrame({'age': [25, 26, 25, 24, 30, 29, 23, 26, 27, 28]})\n    >>> stats = variable_analysis(df, 'age')\n    >>> print(stats)\n    >>> df['age'].plot(kind='hist')\n    >>> df['age'].plot(kind='box')\n    \"\"\"\n    data = df[variable]\n    mean = np.mean(data)\n    median = np.median(data)\n    mode = stats.mode(data)[0][0]\n    std_dev = np.std(data)\n    skewness = stats.skew(data)\n    kurtosis = stats.kurtosis(data)\n    \n    stats_dict = {\n        'Mean': mean,\n        'Median': median,\n        'Mode': mode,\n        'Standard Deviation': std_dev,\n        'Skewness': skewness,\n        'Kurtosis': kurtosis\n    }\n    \n    return stats_dict\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnumpy, scipy, matplotlib, pandas\n```python\nimport numpy as np\nfrom scipy.fft import fft\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef analyze_signal(time, signal):\n    \"\"\"\n    Analyze a time-domain signal: calculate the Fourier Transform and \n    plot the signal in the time and frequency domains.\n    \n    Parameters:\n    time (numpy array): The time points of the signal.\n    signal (numpy array): The signal values at the time points.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the frequencies and corresponding amplitudes.\n    \n    Requirements:\n    - numpy\n    - scipy.fft\n    - matplotlib.pyplot\n    - pandas\n    \n    Example:\n    >>> t = np.linspace(0, 1, 500, False)  # 1 second\n    >>> s = np.sin(2 * np.pi * 10 * t) + np.sin(2 * np.pi * 20 * t)\n    >>> df = analyze_signal(t, s)\n    >>> print(df)\n    >>> plt.figure()\n    >>> plt.plot(t, s)\n    >>> plt.figure()\n    >>> plt.plot(df['Frequency'], df['Amplitude'])\n    \"\"\"\n    N = signal.size\n    T = time[1] - time[0]\n    x = np.linspace(0.0, 1.0/(2.0*T), int(N/2))\n    yf = fft(signal)\n    y = 2.0/N * np.abs(yf[0:int(N/2)])\n    \n    df = pd.DataFrame({\n        'Frequency': x,\n        'Amplitude': y\n    })\n    \n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, pandas, matplotlib, sklearn.preprocessing\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef analyze_variable(df, variable):\n    \"\"\"\n    Analyze a variable in a pandas DataFrame: calculate the mean, standard deviation, \n    plot a histogram, perform min-max normalization, and plot the normalized histogram.\n    \n    Parameters:\n    df (DataFrame): The pandas DataFrame containing the data.\n    variable (str): The name of the variable to analyze.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the original and normalized variable.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - sklearn.preprocessing.MinMaxScaler\n    \n    Example:\n    >>> df = pd.DataFrame({'age': [25, 26, 25, 24, 30, 29, 23, 26, 27, 28]})\n    >>> df_normalized = analyze_variable(df, 'age')\n    >>> print(df_normalized)\n    >>> df['age'].plot(kind='hist')\n    >>> df_normalized['age_normalized'].plot(kind='hist')\n    \"\"\"\n    data = df[variable]\n    mean = np.mean(data)\n    std_dev = np.std(data)\n    \n    scaler = MinMaxScaler()\n    data_normalized = scaler.fit_transform(data.values.reshape(-1, 1))\n    \n    df[variable + '_normalized'] = data_normalized\n    \n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnumpy, pandas, matplotlib, scipy\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\ndef variable_comparison(df, variable1, variable2):\n    \"\"\"\n    Compare two variables in a pandas DataFrame: calculate the correlation coefficient, \n    perform a t-test, and plot a scatter plot.\n    \n    Parameters:\n    df (DataFrame): The pandas DataFrame containing the data.\n    variable1 (str): The name of the first variable.\n    variable2 (str): The name of the second variable.\n    \n    Returns:\n    dict: A dictionary with the correlation coefficient and t-test results.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - scipy.stats\n    \n    Example:\n    >>> df = pd.DataFrame({'age': [25, 26, 25, 24, 30, 29, 23, 26, 27, 28], 'income': [50000, 60000, 70000, 80000, 90000, 100000, 110000, 120000, 130000, 140000]})\n    >>> results = variable_comparison(df, 'age', 'income')\n    >>> print(results)\n    >>> df.plot(x='age', y='income', kind='scatter')\n    \"\"\"\n    data1 = df[variable1]\n    data2 = df[variable2]\n    \n    correlation = np.corrcoef(data1, data2)[0, 1]\n    \n    t_statistic, p_value = stats.ttest_ind(data1, data2)\n    \n    results = {\n        'Correlation': correlation,\n        'T-Statistic': t_statistic,\n        'P-Value': p_value\n    }\n    \n    return results\n```\n"}
{"seed": "{\"task_id\": 402504, \"prompt\": \"def f_402504(v):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"type(v)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(\\\"hello\\\") is str\\n\", \"\\n    assert candidate(123) is int\\n\", \"\\n    assert candidate(\\\"123\\\") is str\\n\", \"\\n    assert candidate(123.4) is float\\n\"], \"entry_point\": \"f_402504\", \"intent\": \"determine the type of variable `v`\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, sklearn.preprocessing, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\n# Define data\n# df = pd.DataFrame({'A': np.random.rand(100), 'B': np.random.rand(100), 'C': np.random.rand(100)})\n\ndef data_preprocessing(df):\n    \"\"\"\n    Performs data preprocessing on a given pandas DataFrame. \n    It standardizes all the numeric columns and plots the histogram of each column.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame to be processed.\n\n    Returns:\n    DataFrame: The processed pandas DataFrame.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.preprocessing\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame({'A': np.random.rand(100), 'B': np.random.rand(100), 'C': np.random.rand(100)})\n    >>> processed_df = data_preprocessing(df)\n    >>> print(processed_df)\n    \"\"\"\n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n\n    scaler = StandardScaler()\n    df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n\n    for column in df.columns:\n        plt.hist(df[column], bins=30)\n        plt.title(f'Histogram of {column}')\n        plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, scipy.optimize, matplotlib\n```python\nimport numpy as np\nfrom scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\n\n# Constants\nFUNC = lambda x, a, b: a*np.exp(b*x)\n\ndef fit_function(x_data, y_data):\n    \"\"\"\n    Fits a function to a set of data and plots the original data and the fitted function.\n\n    Parameters:\n    x_data (array-like): The x-coordinates of the data.\n    y_data (array-like): The y-coordinates of the data.\n\n    Returns:\n    array: The optimal values for the parameters of the function.\n\n    Requirements:\n    - numpy\n    - scipy.optimize\n    - matplotlib.pyplot\n\n    Example:\n    >>> x_data = np.linspace(0, 4, 50)\n    >>> y_data = FUNC(x_data, 2.5, 1.3) + np.random.normal(0, 0.2, 50)\n    >>> params = fit_function(x_data, y_data)\n    \"\"\"\n    params, _ = curve_fit(FUNC, x_data, y_data)\n\n    plt.scatter(x_data, y_data, label='Data')\n    plt.plot(x_data, FUNC(x_data, *params), label='Fitted function', color='r')\n    plt.legend()\n    plt.show()\n\n    return params\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, pandas, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nCATEGORIES = ['A', 'B', 'C', 'D', 'E']\n\ndef create_categorical_distribution():\n    \"\"\"\n    Creates a distribution of categorical data, stores it in a pandas DataFrame, and plots the distribution as a bar plot.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the distribution of the categories.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = create_categorical_distribution()\n    >>> print(df)\n    \"\"\"\n    data = np.random.choice(CATEGORIES, 1000)\n    df = pd.DataFrame(data, columns=['Category'])\n    df['Category'].value_counts().plot(kind='bar')\n    plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, pandas, sklearn.model_selection, sklearn.linear_model, sklearn.metrics\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n# Define data\n# X = np.random.rand(1000, 5)\n# y = 2*X[:,0] + 3*X[:,1] - 4*X[:,2] + 5*X[:,3] - 6*X[:,4] + np.random.normal(0, 0.1, 1000)\n\ndef perform_linear_regression(X, y):\n    \"\"\"\n    Performs a linear regression on a given set of data and returns the mean squared error of the model.\n\n    Parameters:\n    X (array-like): The input data.\n    y (array-like): The output data.\n\n    Returns:\n    float: The mean squared error of the model.\n\n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.model_selection\n    - sklearn.linear_model\n    - sklearn.metrics\n\n    Example:\n    >>> X = np.random.rand(1000, 5)\n    >>> y = 2*X[:,0] + 3*X[:,1] - 4*X[:,2] + 5*X[:,3] - 6*X[:,4] + np.random.normal(0, 0.1, 1000)\n    >>> mse = perform_linear_regression(X, y)\n    \"\"\"\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    y_pred = model.predict(X_test)\n\n    mse = mean_squared_error(y_test, y_pred)\n\n    return mse\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, pandas, scipy.stats, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import chi2_contingency\nimport matplotlib.pyplot as plt\n\n# Constants\nCATEGORIES = ['A', 'B', 'C', 'D', 'E']\n\ndef perform_chi_square_test():\n    \"\"\"\n    Performs a chi-square test on a contingency table created from a distribution of categorical data.\n\n    Returns:\n    float: The p-value of the chi-square test.\n\n    Requirements:\n    - numpy\n    - pandas\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Example:\n    >>> p_value = perform_chi_square_test()\n    \"\"\"\n    data = np.random.choice(CATEGORIES, 1000)\n    df = pd.DataFrame(data, columns=['Category'])\n    contingency_table = pd.crosstab(index=df['Category'], columns='count')\n    chi2, p_value, _, _ = chi2_contingency(contingency_table)\n\n    contingency_table.plot(kind='bar')\n    plt.show()\n\n    return p_value\n```"}
{"seed": "{\"task_id\": 402504, \"prompt\": \"def f_402504(variable_name):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"type(variable_name)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(\\\"hello\\\") is str\\n\", \"\\n    assert candidate(123) is int\\n\", \"\\n    assert candidate(\\\"123\\\") is str\\n\", \"\\n    assert candidate(123.4) is float\\n\"], \"entry_point\": \"f_402504\", \"intent\": \"get the type of variable `variable_name`\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, matplotlib, random\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import sample\n\n# Constants\nCATEGORIES = ['Fruits', 'Vegetables', 'Dairy', 'Meat', 'Grains, Cereal', 'Sweets']\n\ndef generate_sales_report(variable_name):\n    \"\"\"\n    Generate a random sales report for a supermarket based on the type of the variable `variable_name`.\n    If `variable_name` is a string, the report will only contain the category specified by `variable_name`.\n    If `variable_name` is an integer, the report will contain `variable_name` number of categories randomly selected.\n    If `variable_name` is a list, the report will contain the categories specified in `variable_name`.\n    If `variable_name` is none of the above types, an error message is returned.\n\n    Parameters:\n    variable_name: The variable to determine the categories in the report.\n\n    Returns:\n    DataFrame/str: A pandas DataFrame with the sales report or an error message.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> report = generate_sales_report('Fruits')\n    >>> print(report)\n    >>> report.plot(kind='bar')\n    \"\"\"\n    if isinstance(variable_name, str):\n        categories = [variable_name]\n    elif isinstance(variable_name, int):\n        categories = sample(CATEGORIES, variable_name)\n    elif isinstance(variable_name, list):\n        categories = variable_name\n    else:\n        return 'Error: variable_name should be a string, integer, or list.'\n\n    data = {'Category': categories, 'Sales': np.random.randint(1000, 5000, len(categories))}\n    report = pd.DataFrame(data)\n\n    return report\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nnumpy, sklearn.preprocessing, matplotlib, seaborn\n```python\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nFEATURES = ['Feature1', 'Feature2', 'Feature3', 'Feature4', 'Feature5']\n\ndef standardize_and_visualize(variable_name):\n    \"\"\"\n    Given a variable `variable_name` that specifies the number of observations, \n    generate a random dataset with 5 features and `variable_name` number of observations.\n    Standardize the dataset and visualize the distribution of the features before and after standardization.\n\n    Parameters:\n    variable_name (int): The number of observations.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - sklearn.preprocessing\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> standardize_and_visualize(1000)\n    \"\"\"\n    if not isinstance(variable_name, int):\n        return 'Error: variable_name should be an integer.'\n\n    data = np.random.randn(variable_name, len(FEATURES))\n    df = pd.DataFrame(data, columns=FEATURES)\n\n    # Visualize before standardization\n    plt.figure(figsize=(10, 5))\n    plt.suptitle('Before Standardization')\n    for i, col in enumerate(df.columns):\n        plt.subplot(2, 3, i+1)\n        sns.histplot(df[col], kde=True, color='blue')\n\n    # Standardize\n    scaler = StandardScaler()\n    df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n\n    # Visualize after standardization\n    plt.figure(figsize=(10, 5))\n    plt.suptitle('After Standardization')\n    for i, col in enumerate(df_scaled.columns):\n        plt.subplot(2, 3, i+1)\n        sns.histplot(df_scaled[col], kde=True, color='red')\n\n    plt.tight_layout()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnumpy, scipy.stats, matplotlib\n```python\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\n# Constants\nN = 1000\n\ndef simulate_and_test(variable_name):\n    \"\"\"\n    Given a variable `variable_name` that specifies the number of simulations, \n    generate `variable_name` samples from a standard normal distribution, each with `N` observations.\n    Perform a Shapiro-Wilk Test to test the null hypothesis that the sample comes from a normal distribution.\n    Plot the p-values from the tests.\n\n    Parameters:\n    variable_name (int): The number of simulations.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Example:\n    >>> simulate_and_test(500)\n    \"\"\"\n    if not isinstance(variable_name, int):\n        return 'Error: variable_name should be an integer.'\n\n    p_values = []\n    for _ in range(variable_name):\n        sample = np.random.randn(N)\n        _, p = stats.shapiro(sample)\n        p_values.append(p)\n\n    plt.hist(p_values, bins=50, edgecolor='black')\n    plt.title('P-values from Shapiro-Wilk Test')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, pandas, sklearn.model_selection, sklearn.ensemble, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nimport matplotlib.pyplot as plt\n\n# Constants\nFEATURES = ['Feature1', 'Feature2', 'Feature3', 'Feature4', 'Feature5']\nN = 1000\n\ndef random_forest_classification(variable_name):\n    \"\"\"\n    Given a variable `variable_name` that specifies the number of features, \n    generate a random dataset with `variable_name` features and `N` observations.\n    Perform a Random Forest classification and plot the feature importance.\n\n    Parameters:\n    variable_name (int): The number of features.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.model_selection\n    - sklearn.ensemble\n    - matplotlib.pyplot\n\n    Example:\n    >>> random_forest_classification(5)\n    \"\"\"\n    if not isinstance(variable_name, int):\n        return 'Error: variable_name should be an integer.'\n\n    X = np.random.randn(N, variable_name)\n    y = np.random.randint(0, 2, N)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    clf = RandomForestClassifier(random_state=42)\n    clf.fit(X_train, y_train)\n\n    importance = clf.feature_importances_\n    plt.bar([f'feature {i}' for i in range(variable_name)], importance)\n    plt.title('Feature Importance')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnumpy, pandas, seaborn, sklearn.cluster\n```python\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.cluster import KMeans\n\n# Constants\nFEATURES = ['Feature1', 'Feature2', 'Feature3', 'Feature4', 'Feature5']\nN = 1000\n\ndef kmeans_clustering(variable_name):\n    \"\"\"\n    Given a variable `variable_name` that specifies the number of clusters, \n    generate a random dataset with 5 features and `N` observations.\n    Perform K-means clustering with `variable_name` clusters and visualize the clusters using a pairplot.\n\n    Parameters:\n    variable_name (int): The number of clusters.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - pandas\n    - seaborn\n    - sklearn.cluster\n\n    Example:\n    >>> kmeans_clustering(3)\n    \"\"\"\n    if not isinstance(variable_name, int):\n        return 'Error: variable_name should be an integer.'\n\n    X = np.random.randn(N, len(FEATURES))\n    df = pd.DataFrame(X, columns=FEATURES)\n\n    kmeans = KMeans(n_clusters=variable_name, random_state=0).fit(X)\n    df['Cluster'] = kmeans.labels_\n\n    sns.pairplot(df, hue='Cluster')\n    plt.show()\n```\n"}
{"seed": "{\"task_id\": 2300756, \"prompt\": \"def f_2300756(g):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"next(itertools.islice(g, 5, 5 + 1))\", \"test_start\": \"\\nimport itertools\\n\\ndef check(candidate):\", \"test\": [\"\\n    test = [1, 2, 3, 4, 5, 6, 7]\\n    assert(candidate(test) == 6)\\n\"], \"entry_point\": \"f_2300756\", \"intent\": \"get the 5th item of a generator `g`\", \"library\": [\"itertools\"]}", "generation": "Scenario 1:\nnumpy, itertools, random, matplotlib\n```python\nimport numpy as np\nimport itertools\nimport random\nimport matplotlib.pyplot as plt\n\n# Constants\nITEMS_COUNT = 1000\nRANGE_START = 0\nRANGE_END = 100\n\ndef plot_random_walk_from_generator(g):\n    \"\"\"\n    Generate a random walk with \"n\" steps from a generator \"g\", extract the 5th element,\n    and plot the resulting random walk.\n\n    Parameters:\n    g (generator): A generator of random integers.\n\n    Returns:\n    int: The 5th element in the random walk.\n\n    Requirements:\n    - numpy\n    - itertools\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> g = (random.randint(RANGE_START, RANGE_END) for _ in range(ITEMS_COUNT))\n    >>> fifth_element = plot_random_walk_from_generator(g)\n    >>> print(fifth_element)\n    \"\"\"\n    random_walk = list(itertools.islice(g, ITEMS_COUNT))\n    fifth_element = random_walk[4] \n    plt.plot(np.cumsum(random_walk))\n    plt.show()\n\n    return fifth_element\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nitertools, collections, string, re\n```python\nimport itertools\nimport collections\nimport string\nimport re\n\n# Constants\nMIN_WORD_LENGTH = 5\n\ndef find_most_common_word(g):\n    \"\"\"\n    From a generator \"g\" of strings, get the 5th word that has a length greater than or equal to 5 \n    and find the most common word in the text.\n\n    Parameters:\n    g (generator): A generator of strings.\n\n    Returns:\n    tuple: The most common word and its frequency.\n\n    Requirements:\n    - itertools\n    - collections\n    - string\n    - re\n\n    Example:\n    >>> g = (\"Random string with different words\" for _ in range(1000))\n    >>> most_common_word = find_most_common_word(g)\n    >>> print(most_common_word)\n    \"\"\"\n    words = (word.lower() for line in g for word in re.findall(r'\\b\\w+\\b', line)\n             if len(word) >= MIN_WORD_LENGTH)\n    fifth_word = next(itertools.islice(words, 4, 5))\n    word_counts = collections.Counter(words)\n    most_common_word = word_counts.most_common(1)[0]\n\n    return most_common_word\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nitertools, numpy, scipy.stats, matplotlib\n```python\nimport itertools\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\n# Constants\nSAMPLE_SIZE = 1000\n\ndef analyze_data_distribution(g):\n    \"\"\"\n    From a generator \"g\" of numbers, get the 5th number, calculate the mean, \n    standard deviation, and plot a histogram and a QQ-plot of the data.\n\n    Parameters:\n    g (generator): A generator of numbers.\n\n    Returns:\n    float: The 5th number in the dataset.\n\n    Requirements:\n    - itertools\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Example:\n    >>> g = (np.random.normal(0, 1) for _ in range(SAMPLE_SIZE))\n    >>> fifth_number = analyze_data_distribution(g)\n    >>> print(fifth_number)\n    \"\"\"\n    data = list(itertools.islice(g, SAMPLE_SIZE))\n    fifth_number = data[4]\n\n    mean = np.mean(data)\n    std_dev = np.std(data)\n    print(f'Mean: {mean}, Standard Deviation: {std_dev}')\n\n    plt.hist(data, bins='auto')\n    plt.show()\n\n    stats.probplot(data, plot=plt)\n    plt.show()\n\n    return fifth_number\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nitertools, pandas, seaborn, sklearn.cluster\n```python\nimport itertools\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.cluster import KMeans\n\n# Constants\nFEATURES_COUNT = 5\nCLUSTERS_COUNT = 3\n\ndef cluster_data_from_generator(g):\n    \"\"\"\n    From a generator \"g\" of tuples representing data points, get the 5th data point,\n    perform k-means clustering on the data, and plot a heatmap of the data.\n\n    Parameters:\n    g (generator): A generator of tuples.\n\n    Returns:\n    tuple: The 5th data point in the dataset.\n\n    Requirements:\n    - itertools\n    - pandas\n    - seaborn\n    - sklearn.cluster\n\n    Example:\n    >>> g = ((random.random() for _ in range(FEATURES_COUNT)) for _ in range(ITEMS_COUNT))\n    >>> fifth_datapoint = cluster_data_from_generator(g)\n    >>> print(fifth_datapoint)\n    \"\"\"\n    data = list(itertools.islice(g, ITEMS_COUNT))\n    fifth_datapoint = data[4]\n\n    df = pd.DataFrame(data)\n    kmeans = KMeans(n_clusters=CLUSTERS_COUNT)\n    df['Cluster'] = kmeans.fit_predict(df)\n\n    sns.heatmap(df.corr())\n    plt.show()\n\n    return fifth_datapoint\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nitertools, numpy, scipy.optimize\n```python\nimport itertools\nimport numpy as np\nfrom scipy.optimize import curve_fit\n\n# Constants\nSAMPLE_SIZE = 1000\n\ndef fit_curve_from_generator(g):\n    \"\"\"\n    From a generator \"g\" of tuples representing (x, y) data points, get the 5th data point,\n    fit a curve to the data using a polynomial function, and return the coefficients.\n\n    Parameters:\n    g (generator): A generator of tuples.\n\n    Returns:\n    tuple: The coefficients of the fitted polynomial.\n\n    Requirements:\n    - itertools\n    - numpy\n    - scipy.optimize\n\n    Example:\n    >>> g = ((x, x**2 + np.random.normal(0, 1)) for x in np.linspace(-10, 10, SAMPLE_SIZE))\n    >>> coefficients = fit_curve_from_generator(g)\n    >>> print(coefficients)\n    \"\"\"\n    data = list(itertools.islice(g, SAMPLE_SIZE))\n    fifth_datapoint = data[4]\n\n    x, y = zip(*data)\n    popt, pcov = curve_fit(lambda x, a, b, c: a*x**2 + b*x + c, x, y)\n\n    return popt\n```"}
{"seed": "{\"task_id\": 20056548, \"prompt\": \"def f_20056548(word):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"'\\\"{}\\\"'.format(word)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('Some Random Word') == '\\\"Some Random Word\\\"'\\n\"], \"entry_point\": \"f_20056548\", \"intent\": \"return a string `word` with string format\", \"library\": []}", "generation": "Scenario 1:\npandas, json, csv, os\n```python\nimport pandas as pd\nimport json\nimport csv\nimport os\n\n# Constants\nCSV_FILE = 'data.csv'\nJSON_FILE = 'data.json'\n\ndef csv_to_json(csv_file, json_file):\n    \"\"\"\n    Convert a CSV file into a JSON file.\n\n    Parameters:\n    csv_file (str): The path to the CSV file.\n    json_file (str): The path to the JSON file.\n\n    Returns:\n    str: The confirmation message if the file is successfully converted.\n\n    Requirements:\n    - pandas\n    - json\n    - csv\n    - os\n\n    Example:\n    >>> csv_to_json('data.csv', 'data.json')\n    'CSV file has been successfully converted to JSON.'\n    \"\"\"\n    if not os.path.isfile(csv_file):\n        return \"The CSV file does not exist.\"\n\n    df = pd.read_csv(csv_file)\n    df.to_json(json_file, orient='records')\n\n    return 'CSV file has been successfully converted to JSON.'\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, matplotlib, random, math\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\nimport math\n\n# Constants\nRADIUS = 1\nPOINTS = 10000\n\ndef estimate_pi(points):\n    \"\"\"\n    Estimate the value of pi using the Monte Carlo method.\n\n    Parameters:\n    points (int): The number of random points to generate.\n\n    Returns:\n    float: The estimated value of pi.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - random\n    - math\n\n    Example:\n    >>> estimate_pi(10000)\n    \"\"\"\n    inside_circle = 0\n\n    for _ in range(points):\n        x = np.random.uniform(-RADIUS, RADIUS)\n        y = np.random.uniform(-RADIUS, RADIUS)\n        distance = math.sqrt(x**2 + y**2)\n\n        if distance <= RADIUS:\n            inside_circle += 1\n\n    return 4 * inside_circle / points\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nrequests, bs4, re, os\n```python\nimport requests\nfrom bs4 import BeautifulSoup\nimport re\nimport os\n\n# Constants\nURL = 'https://www.example.com'\n\ndef scrape_webpage(url):\n    \"\"\"\n    Scrape a webpage and save its content to a file.\n\n    Parameters:\n    url (str): The URL of the webpage.\n\n    Returns:\n    str: The confirmation message if the web page is successfully scraped.\n\n    Requirements:\n    - requests\n    - bs4.BeautifulSoup\n    - re\n    - os\n\n    Example:\n    >>> scrape_webpage('https://www.example.com')\n    'Web page has been successfully scraped.'\n    \"\"\"\n    response = requests.get(url)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    \n    filename = re.sub(r'\\W+', '_', url) + '.txt'\n    with open(filename, 'w') as file:\n        file.write(soup.prettify())\n\n    return 'Web page has been successfully scraped.'\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, matplotlib, seaborn, numpy\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Constants\nDATA_FILE = 'data.csv'\nCOLUMN = 'Some Column'\n\ndef plot_histogram(data_file, column):\n    \"\"\"\n    Plot a histogram of a column in a CSV file.\n\n    Parameters:\n    data_file (str): The path to the CSV file.\n    column (str): The name of the column.\n\n    Returns:\n    str: The confirmation message if the histogram is successfully plotted.\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - seaborn\n    - numpy\n\n    Example:\n    >>> plot_histogram('data.csv', 'Some Column')\n    'Histogram has been successfully plotted.'\n    \"\"\"\n    df = pd.read_csv(data_file)\n    plt.hist(df[column], bins=np.sqrt(len(df[column])), edgecolor='black')\n    plt.show()\n\n    return 'Histogram has been successfully plotted.'\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, itertools, math\n```python\nimport numpy as np\nfrom itertools import combinations\nimport math\n\n# Constants\nARRAY = np.array([1, 2, 3, 4, 5])\nCOMBINATION_SIZE = 3\n\ndef find_combinations(array, combination_size):\n    \"\"\"\n    Find all combinations of a certain size in a numpy array.\n\n    Parameters:\n    array (numpy.array): The array.\n    combination_size (int): The size of the combinations.\n\n    Returns:\n    list: A list of combinations.\n\n    Requirements:\n    - numpy\n    - itertools.combinations\n    - math\n\n    Example:\n    >>> find_combinations(np.array([1, 2, 3, 4, 5]), 3)\n    \"\"\"\n    array = array.tolist()\n    combinations = list(combinations(array, combination_size))\n\n    return combinations\n```"}
{"seed": "{\"task_id\": 8546245, \"prompt\": \"def f_8546245(list):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"\\\"\\\"\\\" \\\"\\\"\\\".join(list)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    test = ['hello', 'good', 'morning']\\n    assert candidate(test) == \\\"hello good morning\\\"\\n\"], \"entry_point\": \"f_8546245\", \"intent\": \"join a list of strings `list` using a space ' '\", \"library\": []}", "generation": "Scenario 1:\nnumpy, random, matplotlib, pandas\n```python\nimport numpy as np\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants\nWORDS = ['hello', 'good', 'morning', 'python', 'world', 'programming', 'code', 'computer', 'language', 'developer']\n\ndef generate_random_sentences(n_sentences, n_words):\n    \"\"\"\n    Generate a DataFrame of n_sentences random sentences, each containing n_words randomly picked from WORDS list.\n\n    Parameters:\n    n_sentences (int): The number of sentences to generate.\n    n_words (int): The number of words in each sentence.\n\n    Returns:\n    DataFrame: A pandas DataFrame with generated sentences and their lengths.\n\n    Requirements:\n    - numpy\n    - random\n    - matplotlib.pyplot\n    - pandas\n\n    Example:\n    >>> sentences = generate_random_sentences(10, 5)\n    >>> print(sentences)\n    >>> sentences['Length'].plot(kind='bar')\n    \"\"\"\n    sentence_data = []\n\n    for _ in range(n_sentences):\n        sentence = ' '.join(np.random.choice(WORDS, n_words))\n        length = len(sentence)\n        sentence_data.append([sentence, length])\n\n    sentences_df = pd.DataFrame(sentence_data, columns=['Sentence', 'Length'])\n\n    return sentences_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, fnmatch, zipfile, datetime\n```python\nimport os\nimport fnmatch\nimport zipfile\nfrom datetime import datetime\n\n# Constants\nDIRECTORY = './'\nPATTERN = '*.txt'\nZIP_NAME = 'my_archive'\n\ndef archive_matching_files(directory=DIRECTORY, pattern=PATTERN, zip_name=ZIP_NAME):\n    \"\"\"\n    Archive all files in the specified directory that match the provided pattern into a zip file.\n\n    Parameters:\n    directory (str): The directory to search for files.\n    pattern (str): The pattern to match filenames against.\n    zip_name (str): The name of the zip file to create.\n\n    Returns:\n    str: The name of the zip file.\n\n    Requirements:\n    - os\n    - fnmatch\n    - zipfile\n    - datetime\n\n    Example:\n    >>> archive_matching_files('./', '*.py', 'my_python_files')\n    \"\"\"\n    matches = []\n\n    for root, dirs, files in os.walk(directory):\n        for filename in fnmatch.filter(files, pattern):\n            matches.append(os.path.join(root, filename))\n\n    zip_name += '_' + datetime.now().strftime('%Y%m%d%H%M%S') + '.zip'\n\n    with zipfile.ZipFile(zip_name, 'w') as my_zip:\n        for file in matches:\n            my_zip.write(file)\n\n    return zip_name\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, pandas, matplotlib, random\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import choice\n\n# Constants\nLETTERS = list('abcdefghijklmnopqrstuvwxyz')\n\ndef generate_random_data(n_rows, n_columns):\n    \"\"\"\n    Generate a DataFrame of random data with n_rows and n_columns. Column names are random letters.\n\n    Parameters:\n    n_rows (int): The number of rows in the DataFrame.\n    n_columns (int): The number of columns in the DataFrame.\n\n    Returns:\n    DataFrame: A pandas DataFrame with random data.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> df = generate_random_data(100, 5)\n    >>> print(df)\n    >>> df.plot(kind='box')\n    \"\"\"\n    column_names = [choice(LETTERS) for _ in range(n_columns)]\n    data = np.random.randn(n_rows, n_columns)\n    df = pd.DataFrame(data, columns=column_names)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, pandas, matplotlib, glob\n```python\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport glob\n\n# Constants\nDIRECTORY = './'\nPATTERN = '*.csv'\n\ndef merge_csv_files(directory=DIRECTORY, pattern=PATTERN):\n    \"\"\"\n    Merge all CSV files in the specified directory into a single DataFrame and plot the distribution of each column.\n\n    Parameters:\n    directory (str): The directory to search for CSV files.\n    pattern (str): The pattern to match filenames against.\n\n    Returns:\n    DataFrame: A pandas DataFrame with data merged from all matching CSV files.\n\n    Requirements:\n    - os\n    - pandas\n    - matplotlib.pyplot\n    - glob\n\n    Example:\n    >>> df = merge_csv_files('./', '*.csv')\n    >>> print(df)\n    >>> df.hist(bins=50, figsize=(20,15))\n    \"\"\"\n    all_files = glob.glob(os.path.join(directory, pattern))\n\n    all_dfs = (pd.read_csv(f) for f in all_files)\n    df_merged = pd.concat(all_dfs, ignore_index=True)\n\n    return df_merged\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, itertools, pandas, matplotlib\n```python\nimport numpy as np\nimport itertools\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nWORDS = ['hello', 'good', 'morning', 'python', 'world']\n\ndef generate_permutation_sentences(n_words):\n    \"\"\"\n    Generate a DataFrame of all possible sentences of length n_words using permutations of WORDS list.\n\n    Parameters:\n    n_words (int): The number of words in each sentence.\n\n    Returns:\n    DataFrame: A pandas DataFrame with permutation sentences and their lengths.\n\n    Requirements:\n    - numpy\n    - itertools\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> sentences = generate_permutation_sentences(3)\n    >>> print(sentences)\n    >>> sentences['Length'].hist(bins=50)\n    \"\"\"\n    sentence_data = []\n\n    for words in itertools.permutations(WORDS, n_words):\n        sentence = ' '.join(words)\n        length = len(sentence)\n        sentence_data.append([sentence, length])\n\n    sentences_df = pd.DataFrame(sentence_data, columns=['Sentence', 'Length'])\n\n    return sentences_df\n```"}
{"seed": "{\"task_id\": 2276416, \"prompt\": \"def f_2276416():\\n\\t\", \"suffix\": \"\\n\\treturn y\", \"canonical_solution\": \"y = [[] for n in range(2)]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert(candidate() == [[], []])\\n\"], \"entry_point\": \"f_2276416\", \"intent\": \"create list `y` containing two empty lists\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, random\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import sample\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\nROWS = 10\n\ndef create_random_dataframe():\n    \"\"\"\n    Create a pandas DataFrame y containing data for 5 columns and 10 rows. \n    The data is random integers between 1 and 100.\n\n    Returns:\n    DataFrame: A pandas DataFrame with random integers.\n\n    Requirements:\n    - pandas\n    - numpy\n    - random\n\n    Example:\n    >>> df = create_random_dataframe()\n    >>> print(df)\n    \"\"\"\n    y = pd.DataFrame(np.random.randint(1, 101, size=(ROWS, len(COLUMNS))), columns=COLUMNS)\n\n    return y\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nnumpy, matplotlib\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nROWS = 2\nCOLS = 100\n\ndef plot_random_walks():\n    \"\"\"\n    Create two random walks and plot them.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_random_walks()\n    \"\"\"\n    y = np.random.randn(ROWS, COLS).cumsum(axis=1)\n\n    plt.plot(y.transpose())\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnumpy, scipy.stats, matplotlib\n```python\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\n# Constants\nSIZE = 1000\nBINS = 30\n\ndef plot_normal_distribution():\n    \"\"\"\n    Generate a normal distribution with 1000 data points and plot a histogram.\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_normal_distribution()\n    \"\"\"\n    y = np.random.normal(size=SIZE)\n\n    plt.hist(y, bins=BINS, density=True, alpha=0.6, color='g')\n\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, 0, 1)\n    plt.plot(x, p, 'k', linewidth=2)\n\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \npandas, numpy, sklearn.preprocessing\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\nROWS = 10\n\ndef standardize_dataframe():\n    \"\"\"\n    Create a pandas DataFrame y containing data for 5 columns and 10 rows. \n    The data is random integers between 1 and 100. Then standardize the values.\n\n    Returns:\n    DataFrame: A pandas DataFrame with standardized values.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.preprocessing\n\n    Example:\n    >>> df = standardize_dataframe()\n    >>> print(df)\n    \"\"\"\n    y = pd.DataFrame(np.random.randint(1, 101, size=(ROWS, len(COLUMNS))), columns=COLUMNS)\n\n    scaler = StandardScaler()\n    y_scaled = pd.DataFrame(scaler.fit_transform(y), columns=y.columns)\n\n    return y_scaled\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnumpy, matplotlib, sklearn.datasets\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\n\n# Constants\nN_SAMPLES = 1000\nN_FEATURES = 2\nCENTERS = 3\n\ndef generate_and_plot_clusters():\n    \"\"\"\n    Generate a 2D dataset of samples using sklearn's make_blobs function and plot the clusters.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.datasets\n\n    Example:\n    >>> generate_and_plot_clusters()\n    \"\"\"\n    y, labels = make_blobs(n_samples=N_SAMPLES, n_features=N_FEATURES, centers=CENTERS)\n\n    plt.scatter(y[:, 0], y[:, 1], c=labels)\n    plt.show()\n```\n`GPT_ODEX_BREAK`"}
{"seed": "{\"task_id\": 3925614, \"prompt\": \"def f_3925614(filename):\\n\\t\", \"suffix\": \"\\n\\treturn data\", \"canonical_solution\": \"data = [line.strip() for line in open(filename, 'r')]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    file1 = open(\\\"myfile.txt\\\", \\\"w\\\")\\n    L = [\\\"This is Delhi \\\\n\\\", \\\"This is Paris \\\\n\\\", \\\"This is London \\\\n\\\"]\\n    file1.writelines(L)\\n    file1.close()\\n    assert candidate('myfile.txt') == ['This is Delhi', 'This is Paris', 'This is London']\\n\"], \"entry_point\": \"f_3925614\", \"intent\": \"read a file `filename` into a list `data`\", \"library\": []}", "generation": "Scenario 1:\npandas, os, json, matplotlib\n```python\nimport pandas as pd\nimport os\nimport json\nimport matplotlib.pyplot as plt\n\ndef analyze_json_data(filename):\n    \"\"\"\n    Analyze the JSON data from a file `filename`. The data should be a list of dictionaries where each dictionary \n    represents a record with keys 'name', 'age', 'country' and 'salary'. Calculate the mean, max, min and median of ages \n    and salaries, count the number of records by country, and plot the histogram of ages.\n\n    Requirements:\n    - pandas\n    - os\n    - json\n    - matplotlib.pyplot\n\n    Example:\n    >>> analyze_json_data('data.json')\n    \"\"\"\n    if not os.path.isfile(filename):\n        raise FileNotFoundError(f'{filename} does not exist.')\n\n    with open(filename, 'r') as file:\n        data = json.load(file)\n\n    df = pd.DataFrame(data)\n\n    stats = {\n        'Age': {\n            'Mean': df['age'].mean(),\n            'Max': df['age'].max(),\n            'Min': df['age'].min(),\n            'Median': df['age'].median()\n        },\n        'Salary': {\n            'Mean': df['salary'].mean(),\n            'Max': df['salary'].max(),\n            'Min': df['salary'].min(),\n            'Median': df['salary'].median()\n        },\n        'Country': df['country'].value_counts().to_dict()\n    }\n\n    plt.hist(df['age'], bins=10, edgecolor='black')\n    plt.title('Age Distribution')\n    plt.xlabel('Age')\n    plt.ylabel('Count')\n    plt.show()\n\n    return stats\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncsv, collections, string, random\n```python\nimport csv\nfrom collections import Counter\nimport string\nimport random\n\ndef random_word_frequency(filename, num_words=10):\n    \"\"\"\n    Read a CSV file `filename` with a field 'text', which contains sentences. Generate a list of `num_words` random words \n    and count their frequency in the text.\n\n    Requirements:\n    - csv\n    - collections.Counter\n    - string\n    - random\n\n    Example:\n    >>> random_word_frequency('data.csv', 5)\n    \"\"\"\n    with open(filename, 'r') as file:\n        reader = csv.DictReader(file)\n        text = ' '.join(row['text'] for row in reader)\n\n    words = text.split()\n    random_words = random.sample(words, num_words)\n    word_count = Counter(random_words)\n\n    return dict(word_count)\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, sklearn.model_selection, sklearn.linear_model, sklearn.metrics\n```python\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\ndef predict_sales(filename):\n    \"\"\"\n    Read a CSV file `filename` with fields 'Advertisement Spend', 'Sales'. Split the data into a training set and a test\n    set, train a linear regression model on the training set, make predictions on the test set, and calculate the mean \n    squared error of the predictions.\n\n    Requirements:\n    - pandas\n    - sklearn.model_selection.train_test_split\n    - sklearn.linear_model.LinearRegression\n    - sklearn.metrics.mean_squared_error\n\n    Example:\n    >>> predict_sales('ad_spend_sales.csv')\n    \"\"\"\n    df = pd.read_csv(filename)\n    X = df[['Advertisement Spend']]\n    y = df['Sales']\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    predictions = model.predict(X_test)\n    mse = mean_squared_error(y_test, predictions)\n\n    return mse\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, shutil, PIL.Image\n```python\nimport os\nimport shutil\nfrom PIL import Image\n\ndef organize_images_by_size(filename, output_dir='output'):\n    \"\"\"\n    Read a directory `filename` of images, calculate the size (width x height) of each image, and move the images into \n    subdirectories in `output_dir` named by the image sizes.\n\n    Requirements:\n    - os\n    - shutil\n    - PIL.Image\n\n    Example:\n    >>> organize_images_by_size('images')\n    \"\"\"\n    if not os.path.isdir(filename):\n        raise NotADirectoryError(f'{filename} is not a directory.')\n\n    os.makedirs(output_dir, exist_ok=True)\n\n    for file in os.listdir(filename):\n        if file.lower().endswith(('.png', '.jpg', '.jpeg', '.gif')):\n            image = Image.open(os.path.join(filename, file))\n            width, height = image.size\n            size_dir = os.path.join(output_dir, f'{width}x{height}')\n            os.makedirs(size_dir, exist_ok=True)\n            shutil.move(os.path.join(filename, file), os.path.join(size_dir, file))\n\n    return output_dir\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, nltk, sklearn.feature_extraction.text\n```python\nimport pandas as pd\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndef analyze_text_data(filename):\n    \"\"\"\n    Analyze the text data from a CSV file `filename` with a field 'text'. Remove the stopwords from the text, convert \n    the text to a bag-of-words representation, and return the most common 10 words.\n\n    Requirements:\n    - pandas\n    - nltk.corpus.stopwords\n    - sklearn.feature_extraction.text.CountVectorizer\n\n    Example:\n    >>> analyze_text_data('texts.csv')\n    \"\"\"\n    df = pd.read_csv(filename)\n    stop_words = set(stopwords.words('english'))\n    df['text'] = df['text'].apply(lambda x: ' '.join(word for word in x.split() if word not in stop_words))\n    vectorizer = CountVectorizer(max_features=10)\n    bag_of_words = vectorizer.fit_transform(df['text'])\n    common_words = vectorizer.get_feature_names_out()\n\n    return list(common_words)\n```"}
{"seed": "{\"task_id\": 22187233, \"prompt\": \"def f_22187233():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"\\\"\\\"\\\"\\\"\\\"\\\".join([char for char in 'it is icy' if char != 'i'])\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate() == 't s cy'\\n\"], \"entry_point\": \"f_22187233\", \"intent\": \"delete all occurrences of character 'i' in string 'it is icy'\", \"library\": []}", "generation": "Scenario 1: \npandas, os, re, collections\n```python\nimport pandas as pd\nimport os\nimport re\nfrom collections import Counter\n\n# Constants\nCHARACTER = 'i'\nFILE_PATH = '/path/to/file.txt'\n\ndef count_char_excluding_files(char, file_path):\n    \"\"\"\n    Count the occurrence of all characters in a file, excluding a specific character.\n    \n    Parameters:\n    char (str): The character to exclude.\n    file_path (str): The path to the file.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with character count, excluding the specified character.\n    \n    Requirements:\n    - pandas\n    - os\n    - re\n    - collections.Counter\n    \n    Example:\n    >>> count = count_char_excluding_files('i', '/path/to/file.txt')\n    >>> print(count)\n    \"\"\"\n    if not os.path.isfile(file_path):\n        raise FileNotFoundError(f\"No such file: '{file_path}'\")\n\n    with open(file_path, 'r') as file:\n        text = file.read()\n        text = re.sub(char, '', text, flags=re.IGNORECASE)\n        char_counts = Counter(text)\n\n    char_df = pd.DataFrame.from_dict(char_counts, orient='index', columns=['Count'])\n    char_df.index.name = 'Character'\n\n    return char_df\n\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncsv, re, collections\n```python\nimport csv\nimport re\nfrom collections import Counter\n\n# Constants\nCHARACTER = 'i'\nCSV_FILE_PATH = '/path/to/file.csv'\nCOLUMN_NAME = 'Text'\n\ndef count_char_excluding_csv_column(char, csv_file_path, column_name):\n    \"\"\"\n    Count the occurrence of all characters in a specific column of a CSV file, excluding a specific character.\n    \n    Parameters:\n    char (str): The character to exclude.\n    csv_file_path (str): The path to the CSV file.\n    column_name (str): The name of the column.\n    \n    Returns:\n    dict: A dictionary with character count, excluding the specified character.\n    \n    Requirements:\n    - csv\n    - re\n    - collections.Counter\n    \n    Example:\n    >>> count = count_char_excluding_csv_column('i', '/path/to/file.csv', 'Text')\n    >>> print(count)\n    \"\"\"\n    char_counts = Counter()\n\n    with open(csv_file_path, 'r') as file:\n        reader = csv.DictReader(file)\n        if column_name not in reader.fieldnames:\n            raise KeyError(f\"No such column: '{column_name}'\")\n        for row in reader:\n            text = row[column_name]\n            text = re.sub(char, '', text, flags=re.IGNORECASE)\n            char_counts.update(text)\n\n    return dict(char_counts)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, re, collections, matplotlib\n```python\nimport pandas as pd\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\n# Constants\nCHARACTER = 'i'\nDATAFRAME = pd.DataFrame({'Text': ['it is icy', 'inside the igloo', 'imagine if it snows']})\n\ndef count_char_excluding_df_column(char, df, column_name):\n    \"\"\"\n    Count the occurrence of all characters in a specific column of a pandas DataFrame, excluding a specific character.\n    Plot a bar chart of the character counts.\n    \n    Parameters:\n    char (str): The character to exclude.\n    df (DataFrame): The pandas DataFrame.\n    column_name (str): The name of the column.\n    \n    Returns:\n    Series: A pandas Series with character count, excluding the specified character.\n    \n    Requirements:\n    - pandas\n    - re\n    - collections.Counter\n    - matplotlib.pyplot\n    \n    Example:\n    >>> count = count_char_excluding_df_column('i', DATAFRAME, 'Text')\n    >>> print(count)\n    >>> count.plot(kind='bar')\n    \"\"\"\n    if column_name not in df.columns:\n        raise KeyError(f\"No such column: '{column_name}'\")\n\n    text = ' '.join(df[column_name])\n    text = re.sub(char, '', text, flags=re.IGNORECASE)\n    char_counts = Counter(text)\n\n    char_series = pd.Series(char_counts)\n\n    return char_series\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, re, collections, shutil\n```python\nimport os\nimport re\nfrom collections import Counter\nimport shutil\n\n# Constants\nCHARACTER = 'i'\nDIRECTORY_PATH = '/path/to/directory/'\n\ndef count_char_excluding_directory(char, directory_path):\n    \"\"\"\n    Count the occurrence of all characters in all text files in a directory, excluding a specific character.\n    \n    Parameters:\n    char (str): The character to exclude.\n    directory_path (str): The path to the directory.\n    \n    Returns:\n    dict: A dictionary with character count, excluding the specified character.\n    \n    Requirements:\n    - os\n    - re\n    - collections.Counter\n    - shutil\n    \n    Example:\n    >>> count = count_char_excluding_directory('i', '/path/to/directory/')\n    >>> print(count)\n    \"\"\"\n    if not os.path.isdir(directory_path):\n        raise FileNotFoundError(f\"No such directory: '{directory_path}'\")\n\n    char_counts = Counter()\n\n    for file_name in os.listdir(directory_path):\n        if not file_name.endswith('.txt'):\n            continue\n        file_path = os.path.join(directory_path, file_name)\n        with open(file_path, 'r') as file:\n            text = file.read()\n            text = re.sub(char, '', text, flags=re.IGNORECASE)\n            char_counts.update(text)\n\n    return dict(char_counts)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, re, collections, sqlite3\n```python\nimport pandas as pd\nimport re\nfrom collections import Counter\nimport sqlite3\n\n# Constants\nCHARACTER = 'i'\nDB_PATH = '/path/to/database.db'\nTABLE_NAME = 'TextTable'\nCOLUMN_NAME = 'Text'\n\ndef count_char_excluding_db_column(char, db_path, table_name, column_name):\n    \"\"\"\n    Count the occurrence of all characters in a specific column of a table in a SQLite database, excluding a specific character.\n    \n    Parameters:\n    char (str): The character to exclude.\n    db_path (str): The path to the SQLite database.\n    table_name (str): The name of the table.\n    column_name (str): The name of the column.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with character count, excluding the specified character.\n    \n    Requirements:\n    - pandas\n    - re\n    - collections.Counter\n    - sqlite3\n    \n    Example:\n    >>> count = count_char_excluding_db_column('i', '/path/to/database.db', 'TextTable', 'Text')\n    >>> print(count)\n    \"\"\"\n    conn = sqlite3.connect(db_path)\n    df = pd.read_sql_query(f\"SELECT {column_name} FROM {table_name}\", conn)\n\n    text = ' '.join(df[column_name])\n    text = re.sub(char, '', text, flags=re.IGNORECASE)\n    char_counts = Counter(text)\n\n    char_df = pd.DataFrame.from_dict(char_counts, orient='index', columns=['Count'])\n    char_df.index.name = 'Character'\n\n    return char_df\n```"}
{"seed": "{\"task_id\": 22187233, \"prompt\": \"def f_22187233():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"re.sub('i', '', 'it is icy')\", \"test_start\": \"\\nimport re \\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate() == 't s cy'\\n\"], \"entry_point\": \"f_22187233\", \"intent\": \"delete all instances of a character 'i' in a string 'it is icy'\", \"library\": [\"re\"]}", "generation": "Scenario 1: \nre, string, random, pandas, matplotlib\n```python\nimport re\nimport string\nimport random\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nLETTERS = string.ascii_lowercase\nTEXT = 'it is a truth universally acknowledged that a single man in possession of a good fortune must be in want of a wife'\n\ndef analyze_letter_frequency(text):\n    \"\"\"\n    Analyze the frequency of each letter in a text, plot the frequency distribution,\n    and return a DataFrame with the frequency information.\n\n    Parameters:\n    text (str): The input text to analyze.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the letters and their frequencies.\n\n    Requirements:\n    - re\n    - string\n    - random\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = analyze_letter_frequency(TEXT)\n    >>> print(df)\n    >>> df.plot(x='Letter', y='Frequency', kind='bar')\n    \"\"\"\n    letter_frequencies = {letter: len(re.findall(letter, text)) for letter in LETTERS}\n    \n    df = pd.DataFrame.from_dict(letter_frequencies, orient='index', columns=['Frequency']).reset_index()\n    df.rename(columns={'index': 'Letter'}, inplace=True)\n\n    df.sort_values(by='Frequency', ascending=False, inplace=True)\n    df.reset_index(drop=True, inplace=True)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nre, datetime, pytz, dateutil\n```python\nimport re\nfrom datetime import datetime\nimport pytz\nfrom dateutil.parser import parse\n\n# Constants\nDATE_PATTERN = r'\\b(\\d{4}-\\d{2}-\\d{2})\\b'\n\ndef find_and_convert_dates(text, from_tz, to_tz):\n    \"\"\"\n    Find all dates in a text, convert them from one timezone to another, \n    and return a list of original and converted dates.\n\n    Parameters:\n    text (str): The input text.\n    from_tz (str): The original timezone of the dates.\n    to_tz (str): The timezone to which the dates should be converted.\n\n    Returns:\n    list: A list of tuples (original date, converted date).\n\n    Requirements:\n    - re\n    - datetime\n    - pytz\n    - dateutil.parser\n\n    Example:\n    >>> find_and_convert_dates('Meeting dates: 2023-06-15, 2023-07-20', 'UTC', 'America/New_York')\n    \"\"\"\n    from_tz = pytz.timezone(from_tz)\n    to_tz = pytz.timezone(to_tz)\n    \n    dates = re.findall(DATE_PATTERN, text)\n    converted_dates = []\n\n    for date_str in dates:\n        original_date = parse(date_str).replace(tzinfo=from_tz)\n        converted_date = original_date.astimezone(to_tz)\n        converted_dates.append((original_date, converted_date))\n\n    return converted_dates\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nre, json, os, pandas\n```python\nimport re\nimport json\nimport os\nimport pandas as pd\n\n# Constants\nJSON_FILES_DIR = 'json_files'\nJSON_CONTENT_PATTERN = r'\"content\":\\s*\"([^\"]*)\"'\n\ndef extract_json_content_to_df():\n    \"\"\"\n    Extract the 'content' field from all JSON files in a directory, \n    and return a DataFrame with the file names and the extracted content.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the file names and the extracted content.\n\n    Requirements:\n    - re\n    - json\n    - os\n    - pandas\n\n    Example:\n    >>> df = extract_json_content_to_df()\n    >>> print(df)\n    \"\"\"\n    data = []\n    \n    for file_name in os.listdir(JSON_FILES_DIR):\n        if file_name.endswith('.json'):\n            with open(os.path.join(JSON_FILES_DIR, file_name), 'r') as file:\n                json_str = file.read()\n                content = re.search(JSON_CONTENT_PATTERN, json_str).group(1)\n                data.append((file_name, content))\n\n    df = pd.DataFrame(data, columns=['File Name', 'Content'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nre, urllib.request, bs4, csv\n```python\nimport re\nimport urllib.request\nfrom bs4 import BeautifulSoup\nimport csv\n\n# Constants\nURL = 'https://example.com'\nHTML_TAG_PATTERN = r'<([a-z][a-z0-9]*)\\b[^>]*>.*?</\\1>'\nCSV_FILE = 'output.csv'\n\ndef scrape_and_save_html_tags():\n    \"\"\"\n    Scrape a webpage, extract all HTML tags from the page, and save them into a CSV file.\n\n    Requirements:\n    - re\n    - urllib.request\n    - bs4 (BeautifulSoup)\n    - csv\n\n    Example:\n    >>> scrape_and_save_html_tags()\n    \"\"\"\n    with urllib.request.urlopen(URL) as response:\n        html = response.read().decode()\n\n    soup = BeautifulSoup(html, 'html.parser')\n    tags = re.findall(HTML_TAG_PATTERN, str(soup))\n\n    with open(CSV_FILE, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['HTML Tag'])\n        for tag in tags:\n            writer.writerow([tag])\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nre, pandas, numpy, matplotlib\n```python\nimport re\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nTEXT = 'it is a truth universally acknowledged that a single man in possession of a good fortune must be in want of a wife'\nWORD_PATTERN = r'\\b(\\w+)\\b'\n\ndef analyze_word_length_distribution(text):\n    \"\"\"\n    Analyze the distribution of word lengths in a text, plot the distribution,\n    and return a DataFrame with the word lengths and their frequencies.\n\n    Parameters:\n    text (str): The input text to analyze.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the word lengths and their frequencies.\n\n    Requirements:\n    - re\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = analyze_word_length_distribution(TEXT)\n    >>> print(df)\n    >>> df.plot(x='Word Length', y='Frequency', kind='bar')\n    \"\"\"\n    words = re.findall(WORD_PATTERN, text)\n    word_lengths = [len(word) for word in words]\n\n    df = pd.DataFrame(word_lengths, columns=['Word Length'])\n    frequency = df['Word Length'].value_counts().reset_index()\n    frequency.columns = ['Word Length', 'Frequency']\n\n    return frequency\n```\n"}
{"seed": "{\"task_id\": 22187233, \"prompt\": \"def f_22187233():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"\\\"\\\"\\\"it is icy\\\"\\\"\\\".replace('i', '')\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate() == 't s cy'\\n\"], \"entry_point\": \"f_22187233\", \"intent\": \"delete all characters \\\"i\\\" in string \\\"it is icy\\\"\", \"library\": []}", "generation": "Scenario 1: \npandas, re, os, random\n```python\nimport pandas as pd\nimport re\nimport os\nfrom random import randint\n\n# Constants\nFILES = ['file1.txt', 'file2.txt', 'file3.txt', 'file4.txt', 'file5.txt']\nCHARS_TO_REMOVE = ['a', 'e', 'i', 'o', 'u']\n\ndef remove_chars_from_files():\n    \"\"\"\n    Randomly pick a character from a predefined list and remove all occurrences \n    of this character in a list of text files.\n    \n    Returns:\n    str: The name of the modified file.\n    \n    Requirements:\n    - pandas\n    - re\n    - os\n    - random\n    \n    Example:\n    >>> remove_chars_from_files()\n    \"\"\"\n    file = FILES[randint(0, len(FILES)-1)]\n    char = CHARS_TO_REMOVE[randint(0, len(CHARS_TO_REMOVE)-1)]\n\n    with open(file, 'r') as f:\n        text = f.read()\n\n    text = re.sub(char, '', text)\n\n    with open(file, 'w') as f:\n        f.write(text)\n\n    return file\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nnumpy, os, random\n```python\nimport numpy as np\nimport os\nfrom random import randint\n\n# Constants\nMATRIX_SIZE = 5\nDIRS = ['up', 'down', 'left', 'right']\n\ndef move_in_matrix():\n    \"\"\"\n    Given a 5x5 matrix filled with zeros and a single one in a random position,\n    move the one in one of the directions (up, down, left, right) and return the new matrix.\n    \n    Returns:\n    ndarray: The new matrix after the move.\n    \n    Requirements:\n    - numpy\n    - random\n    \n    Example:\n    >>> move_in_matrix()\n    \"\"\"\n    matrix = np.zeros((MATRIX_SIZE, MATRIX_SIZE))\n    x, y = randint(0, MATRIX_SIZE-1), randint(0, MATRIX_SIZE-1)\n    matrix[x, y] = 1\n\n    dir = DIRS[randint(0, len(DIRS)-1)]\n\n    if dir == 'up' and x > 0:\n        matrix[x, y], matrix[x-1, y] = matrix[x-1, y], matrix[x, y]\n    elif dir == 'down' and x < MATRIX_SIZE-1:\n        matrix[x, y], matrix[x+1, y] = matrix[x+1, y], matrix[x, y]\n    elif dir == 'left' and y > 0:\n        matrix[x, y], matrix[x, y-1] = matrix[x, y-1], matrix[x, y]\n    elif dir == 'right' and y < MATRIX_SIZE-1:\n        matrix[x, y], matrix[x, y+1] = matrix[x, y+1], matrix[x, y]\n\n    return matrix\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \npandas, os, random\n```python\nimport pandas as pd\nimport os\nfrom random import randint\n\n# Constants\nFILES = ['data1.csv', 'data2.csv', 'data3.csv', 'data4.csv', 'data5.csv']\nCOLS_TO_REMOVE = ['col1', 'col2', 'col3', 'col4', 'col5']\n\ndef remove_cols_from_csv():\n    \"\"\"\n    Randomly pick a column from a predefined list and remove this column in a list of CSV files.\n    \n    Returns:\n    str: The name of the modified file.\n    \n    Requirements:\n    - pandas\n    - os\n    - random\n    \n    Example:\n    >>> remove_cols_from_csv()\n    \"\"\"\n    file = FILES[randint(0, len(FILES)-1)]\n    col = COLS_TO_REMOVE[randint(0, len(COLS_TO_REMOVE)-1)]\n\n    df = pd.read_csv(file)\n    if col in df.columns:\n        df = df.drop(columns=[col])\n        df.to_csv(file, index=False)\n\n    return file\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, random\n```python\nimport numpy as np\nfrom random import randint\n\n# Constants\nLIST_SIZE = 100\nINDEXES_TO_REMOVE = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n\ndef remove_from_list():\n    \"\"\"\n    Given a list of 100 random integers, remove the elements at the positions in a predefined list.\n    \n    Returns:\n    list: The new list after the removals.\n    \n    Requirements:\n    - numpy\n    - random\n    \n    Example:\n    >>> remove_from_list()\n    \"\"\"\n    list = np.random.randint(1, 1000, LIST_SIZE).tolist()\n    for index in sorted(INDEXES_TO_REMOVE, reverse=True):\n        if index < len(list):\n            del list[index]\n\n    return list\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \npandas, random\n```python\nimport pandas as pd\nfrom random import randint\n\n# Constants\nDATA = pd.DataFrame({\n    'col1': list(range(10)),\n    'col2': list(range(10, 20)),\n    'col3': list(range(20, 30)),\n    'col4': list(range(30, 40)),\n    'col5': list(range(40, 50))\n})\nROWS_TO_REMOVE = [1, 2, 3, 4, 5]\n\ndef remove_rows_from_df():\n    \"\"\"\n    Given a DataFrame, remove the rows at the positions in a predefined list.\n    \n    Returns:\n    DataFrame: The new DataFrame after the removals.\n    \n    Requirements:\n    - pandas\n    - random\n    \n    Example:\n    >>> remove_rows_from_df()\n    \"\"\"\n    for row in sorted(ROWS_TO_REMOVE, reverse=True):\n        if row < len(DATA):\n            DATA.drop(row, inplace=True)\n\n    return DATA\n```"}
{"seed": "{\"task_id\": 13413590, \"prompt\": \"def f_13413590(df):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"df.dropna(subset=[1])\", \"test_start\": \"\\nimport numpy as np\\nimport pandas as pd\\n\\ndef check(candidate):\", \"test\": [\"\\n    data = {0:[3.0, 4.0, 2.0], 1:[2.0, 3.0, np.nan], 2:[np.nan, 3.0, np.nan]}\\n    df = pd.DataFrame(data)\\n    d = {0:[3.0, 4.0], 1:[2.0, 3.0], 2:[np.nan, 3.0]}\\n    res = pd.DataFrame(d)\\n    assert candidate(df).equals(res)\\n\"], \"entry_point\": \"f_13413590\", \"intent\": \"Drop rows of pandas dataframe `df` having NaN in column at index \\\"1\\\"\", \"library\": [\"numpy\", \"pandas\"]}", "generation": "Scenario 1:\npandas, numpy, random, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nCATEGORIES = ['Electronics', 'Clothing', 'Home & Kitchen', 'Books', 'Automotive']\n\ndef process_sales_data(df):\n    \"\"\"\n    Process a sales dataframe by replacing NaN values with the median of the respective category, \n    removing rows with NaN in 'Product Category' column, and plotting a pie chart of sales by category.\n    \n    Parameters:\n    df (DataFrame): The sales data dataframe.\n\n    Returns:\n    DataFrame: The processed sales data dataframe.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - random\n    - matplotlib\n    \n    Example:\n    >>> data = {0:['Electronics', 'Clothing', np.nan, 'Books', 'Automotive'], \n                1:[100.0, 200.0, np.nan, 150.0, np.nan],\n                2:[np.nan, 3.0, np.nan]}\n    >>> df = pd.DataFrame(data)\n    >>> df = process_sales_data(df)\n    >>> print(df)\n    \"\"\"\n    # Replace NaNs with median values\n    for category in CATEGORIES:\n        median_value = df[df[0] == category][1].median()\n        df.loc[df[0] == category, 1] = df.loc[df[0] == category, 1].fillna(median_value)\n        \n    # Drop rows with NaN in 'Product Category' column\n    df = df.dropna(subset=[0])\n    \n    # Plot pie chart\n    sales_by_category = df.groupby(0)[1].sum()\n    plt.pie(sales_by_category, labels=sales_by_category.index, autopct='%1.1f%%')\n    plt.title('Sales by Category')\n    plt.show()\n    \n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, pandas, matplotlib, seaborn\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nLABELS = ['Length', 'Width', 'Height']\n\ndef analyze_dimensions(df):\n    \"\"\"\n    Analyze a dataframe of dimensions by replacing NaN values with the mean,\n    removing rows with all NaN values, and plotting a pairplot of the dimensions.\n    \n    Parameters:\n    df (DataFrame): The dimensions dataframe.\n\n    Returns:\n    DataFrame: The processed dimensions dataframe.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib\n    - seaborn\n    \n    Example:\n    >>> data = {0:[3.0, 4.0, np.nan], 1:[2.0, 3.0, np.nan], 2:[np.nan, 3.0, np.nan]}\n    >>> df = pd.DataFrame(data)\n    >>> df = analyze_dimensions(df)\n    >>> print(df)\n    \"\"\"\n    # Replace NaNs with mean values\n    for i in range(df.shape[1]):\n        mean_value = df[i].mean()\n        df[i] = df[i].fillna(mean_value)\n        \n    # Drop rows with all NaN values\n    df = df.dropna(how='all')\n    \n    # Plot pairplot\n    df.columns = LABELS\n    sns.pairplot(df)\n    plt.show()\n    \n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, pandas, sklearn.preprocessing\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\n# Constants\nFEATURES = ['Feature1', 'Feature2', 'Feature3']\n\ndef preprocess_data(df):\n    \"\"\"\n    Preprocess a dataframe by dropping rows with NaN in 'Feature1' column,\n    replacing NaN values in other columns with their respective means,\n    and standardizing the features.\n\n    Parameters:\n    df (DataFrame): The data dataframe.\n\n    Returns:\n    DataFrame: The preprocessed data dataframe.\n\n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.preprocessing.StandardScaler\n\n    Example:\n    >>> data = {0:[3.0, 4.0, np.nan], 1:[2.0, 3.0, np.nan], 2:[np.nan, 3.0, np.nan]}\n    >>> df = pd.DataFrame(data)\n    >>> df = preprocess_data(df)\n    >>> print(df)\n    \"\"\"\n    # Drop rows with NaN in 'Feature1' column\n    df = df.dropna(subset=[0])\n    \n    # Replace NaNs with mean values\n    for i in range(1, df.shape[1]):\n        mean_value = df[i].mean()\n        df[i] = df[i].fillna(mean_value)\n        \n    # Standardize the features\n    scaler = StandardScaler()\n    df = pd.DataFrame(scaler.fit_transform(df), columns=FEATURES)\n    \n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, pandas, matplotlib, statsmodels.api\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n\n# Constants\nCOLUMNS = ['X', 'Y']\n\ndef analyze_regression(df):\n    \"\"\"\n    Analyze a dataframe by dropping rows with NaN in 'X' column,\n    replacing NaN values in 'Y' column with its mean,\n    and performing a simple linear regression of 'Y' on 'X'.\n    \n    Parameters:\n    df (DataFrame): The data dataframe.\n\n    Returns:\n    dict: The regression results (intercept, slope, R-squared).\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib\n    - statsmodels.api\n\n    Example:\n    >>> data = {0:[3.0, 4.0, np.nan], 1:[2.0, 3.0, np.nan]}\n    >>> df = pd.DataFrame(data)\n    >>> results = analyze_regression(df)\n    >>> print(results)\n    \"\"\"\n    # Drop rows with NaN in 'X' column\n    df = df.dropna(subset=[0])\n    \n    # Replace NaNs in 'Y' with mean value\n    mean_value = df[1].mean()\n    df[1] = df[1].fillna(mean_value)\n    \n    # Perform regression\n    df.columns = COLUMNS\n    X = sm.add_constant(df['X'])\n    model = sm.OLS(df['Y'], X)\n    results = model.fit()\n    regression_results = {'intercept': results.params['const'],\n                          'slope': results.params['X'], 'R-squared': results.rsquared}\n    \n    return regression_results\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, pandas, sklearn.cluster\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import KMeans\n\n# Constants\nN_CLUSTERS = 3\n\ndef cluster_data(df):\n    \"\"\"\n    Cluster a dataframe by dropping rows with NaN in column 0,\n    replacing NaN values in other columns with their respective medians,\n    and performing K-means clustering.\n    \n    Parameters:\n    df (DataFrame): The data dataframe.\n\n    Returns:\n    DataFrame: The dataframe with an additional 'Cluster' column.\n\n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.cluster.KMeans\n\n    Example:\n    >>> data = {0:[3.0, 4.0, np.nan], 1:[2.0, 3.0, np.nan], 2:[np.nan, 3.0, np.nan]}\n    >>> df = pd.DataFrame(data)\n    >>> df = cluster_data(df)\n    >>> print(df)\n    \"\"\"\n    # Drop rows with NaN in column 0\n    df = df.dropna(subset=[0])\n    \n    # Replace NaNs with median values\n    for i in range(1, df.shape[1]):\n        median_value = df[i].median()\n        df[i] = df[i].fillna(median_value)\n        \n    # Perform clustering\n    kmeans = KMeans(n_clusters=N_CLUSTERS)\n    df['Cluster'] = kmeans.fit_predict(df)\n    \n    return df\n```\n"}
{"seed": "{\"task_id\": 598398, \"prompt\": \"def f_598398(myList):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"[x for x in myList if x.n == 30]\", \"test_start\": \"\\nimport numpy as np\\nimport pandas as pd\\n\\ndef check(candidate):\", \"test\": [\"\\n    class Data: \\n        def __init__(self, a, n): \\n            self.a = a\\n            self.n = n\\n    \\n    myList = [Data(i, 10*(i%4)) for i in range(20)]\\n    assert candidate(myList) == [myList[i] for i in [3, 7, 11, 15, 19]]\\n\"], \"entry_point\": \"f_598398\", \"intent\": \"get elements from list `myList`, that have a field `n` value 30\", \"library\": [\"numpy\", \"pandas\"]}", "generation": "Scenario 1: \npandas, numpy, random\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import randint\n\n# Constants\nNUM_PEOPLE = 50\nNAMES = ['Alice', 'Bob', 'Charlie', 'David', 'Eve', 'Frank', 'Grace', 'Heidi', 'Ivan', 'Judy', 'Mallory', 'Nia', 'Oliver', 'Pat', 'Quincy', 'Randy', 'Sybil', 'Trudy', 'Victor', 'Wendy', 'Xander', 'Yara', 'Zoe']\nAGES = range(18, 60)\n\ndef create_people_df():\n    \"\"\"\n    Generate a pandas DataFrame with randomly assigned names and ages for a \n    specified number of people. Then filter the DataFrame to include only people \n    with ages from 30 to 40.\n\n    Returns:\n    DataFrame: A pandas DataFrame with filtered people data.\n\n    Requirements:\n    - pandas\n    - numpy\n    - random\n\n    Example:\n    >>> people_df = create_people_df()\n    >>> print(people_df)\n    \"\"\"\n    names = [NAMES[randint(0, len(NAMES)-1)] for _ in range(NUM_PEOPLE)]\n    ages = [AGES[randint(0, len(AGES)-1)] for _ in range(NUM_PEOPLE)]\n\n    people_df = pd.DataFrame({'Name': names, 'Age': ages})\n\n    filtered_df = people_df[(people_df['Age'] >= 30) & (people_df['Age'] <= 40)]\n\n    return filtered_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, random, matplotlib\n```python\nimport numpy as np\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nNUM_ELEMENTS = 100\nVALUES = range(1, 100)\n\ndef plot_histogram():\n    \"\"\"\n    Generate a list of random values and plot a histogram of the values.\n\n    Requirements:\n    - numpy\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_histogram()\n    \"\"\"\n    values = [VALUES[randint(0, len(VALUES)-1)] for _ in range(NUM_ELEMENTS)]\n    plt.hist(values, bins=10)\n    plt.title('Histogram of Random Values')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.show()\n\n    return values\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnumpy, matplotlib, scipy.stats\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n# Constants\nMEAN = 0\nSTD_DEV = 1\nNUM_SAMPLES = 10000\n\ndef plot_normal_distribution():\n    \"\"\"\n    Generate a list of samples from a normal distribution and plot a histogram \n    of the samples with a fitted normal distribution curve.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - scipy.stats.norm\n\n    Example:\n    >>> plot_normal_distribution()\n    \"\"\"\n    samples = np.random.normal(MEAN, STD_DEV, NUM_SAMPLES)\n    plt.hist(samples, bins=30, density=True, alpha=0.6, color='g')\n\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, MEAN, STD_DEV)\n    plt.plot(x, p, 'k', linewidth=2)\n    title = \"Fit results: mu = %.2f,  std = %.2f\" % (MEAN, STD_DEV)\n    plt.title(title)\n\n    plt.show()\n\n    return samples\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, pandas, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nDATES = pd.date_range('20220101', periods=100)\nNUM_COLS = 4\n\ndef plot_time_series():\n    \"\"\"\n    Generate a DataFrame with 4 time series of random values and plot them.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_time_series()\n    \"\"\"\n    df = pd.DataFrame(np.random.randn(len(DATES), NUM_COLS), index=DATES, columns=list('ABCD'))\n    df = df.cumsum()\n\n    plt.figure()\n    df.plot()\n    plt.legend(loc='best')\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnumpy, pandas, random\n```python\nimport numpy as np\nimport pandas as pd\nfrom random import randint\n\n# Constants\nNUM_ROWS = 50\nVALUES = range(1, 100)\n\ndef get_rows_with_max_value_in_column():\n    \"\"\"\n    Generate a DataFrame with random values and return the rows with the maximum \n    value in a specified column.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the rows with the maximum value.\n\n    Requirements:\n    - numpy\n    - pandas\n    - random\n\n    Example:\n    >>> max_rows_df = get_rows_with_max_value_in_column()\n    >>> print(max_rows_df)\n    \"\"\"\n    df = pd.DataFrame({'A': [VALUES[randint(0, len(VALUES)-1)] for _ in range(NUM_ROWS)],\n                       'B': [VALUES[randint(0, len(VALUES)-1)] for _ in range(NUM_ROWS)],\n                       'C': [VALUES[randint(0, len(VALUES)-1)] for _ in range(NUM_ROWS)],\n                       'D': [VALUES[randint(0, len(VALUES)-1)] for _ in range(NUM_ROWS)]})\n\n    max_rows_df = df[df['A'] == df['A'].max()]\n\n    return max_rows_df\n```\n"}
{"seed": "{\"task_id\": 10351772, \"prompt\": \"def f_10351772(intstringlist):\\n\\t\", \"suffix\": \"\\n\\treturn nums\", \"canonical_solution\": \"nums = [int(x) for x in intstringlist]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(['1', '2', '3', '4', '5']) == [1, 2, 3, 4, 5]\\n\", \"\\n    assert candidate(['001', '200', '3', '4', '5']) == [1, 200, 3, 4, 5]\\n\"], \"entry_point\": \"f_10351772\", \"intent\": \"converting list of strings `intstringlist` to list of integer `nums`\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, matplotlib, random\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import choice, randint\n\n# Constants\nLETTERS = list('abcdefghijklmnopqrstuvwxyz')\n\ndef generate_random_dataframe(nrows, ncols):\n    \"\"\"\n    Generate a pandas DataFrame with nrows and ncols filled with random integer \n    values and random column names.\n    \n    Parameters:\n    nrows (int): The number of rows in the DataFrame.\n    ncols (int): The number of columns in the DataFrame.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with random integers and column names.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - random\n    \n    Example:\n    >>> df = generate_random_dataframe(5, 3)\n    >>> print(df)\n    >>> df.describe().plot(kind='bar')\n    \"\"\"\n    colnames = [''.join([choice(LETTERS) for _ in range(randint(5, 10))]) for _ in range(ncols)]\n    df = pd.DataFrame(np.random.randint(0,100,size=(nrows, ncols)), columns=colnames)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ndatetime, pytz, pandas, numpy\n```python\nimport pandas as pd\nfrom datetime import datetime, timedelta\nimport pytz\nimport numpy as np\n\n# Constants\nTIMEZONES = ['America/Los_Angeles', 'Asia/Kolkata', 'Europe/London', 'Australia/Sydney', 'Africa/Johannesburg']\n\ndef generate_time_series(start_date, end_date, freq, tz):\n    \"\"\"\n    Generate a pandas Series of datetime objects beginning at start_date and ending \n    at end_date at a given frequency and timezone.\n    \n    Parameters:\n    start_date (str): The start date in \"yyyy-mm-dd\" format.\n    end_date (str): The end date in \"yyyy-mm-dd\" format.\n    freq (str): The frequency of the time series. Accepts any pandas frequency string.\n    tz (str): The timezone of the time series. Accepts any pytz timezone string.\n    \n    Returns:\n    Series: A pandas Series of datetime objects.\n    \n    Requirements:\n    - pandas\n    - datetime\n    - pytz\n    - numpy\n    \n    Example:\n    >>> series = generate_time_series('2021-01-01', '2021-12-31', 'D', 'America/Los_Angeles')\n    >>> print(series)\n    \"\"\"\n    tz = pytz.timezone(tz)\n    start_date = datetime.strptime(start_date, '%Y-%m-%d')\n    end_date = datetime.strptime(end_date, '%Y-%m-%d')\n    dates = pd.date_range(start_date, end_date, freq=freq)\n    dates_tz = dates.tz_localize(tz)\n\n    return dates_tz\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nrequests, json, pandas, io\n```python\nimport requests\nimport json\nimport pandas as pd\nfrom io import StringIO\n\ndef get_data_from_api(url, params):\n    \"\"\"\n    Make a GET request to a RESTful API, convert the response to JSON, and parse \n    the JSON into a pandas DataFrame.\n    \n    Parameters:\n    url (str): The URL of the API.\n    params (dict): A dictionary of parameters to pass to the GET request.\n    \n    Returns:\n    DataFrame: A pandas DataFrame of the JSON response.\n    \n    Requirements:\n    - requests\n    - json\n    - pandas\n    - io.StringIO\n    \n    Example:\n    >>> url = 'https://api.github.com/users'\n    >>> params = {'since': 135}\n    >>> df = get_data_from_api(url, params)\n    >>> print(df)\n    \"\"\"\n    response = requests.get(url, params=params)\n    data = json.loads(response.text)\n    df = pd.read_json(StringIO(json.dumps(data)))\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, math, matplotlib\n```python\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\n\n# Constants\nX_RANGE = np.linspace(-10, 10, 400)\nY_RANGE = np.linspace(-10, 10, 400)\n\ndef plot_complex_function(func):\n    \"\"\"\n    Plot the real and imaginary parts of a complex-valued function over a 2D \n    domain.\n    \n    Parameters:\n    func (function): A complex-valued function of a single variable.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - numpy\n    - math\n    - matplotlib.pyplot\n    \n    Example:\n    >>> plot_complex_function(math.sin)\n    \"\"\"\n    X, Y = np.meshgrid(X_RANGE, Y_RANGE)\n    Z = func(X + 1j * Y)\n    plt.figure()\n    plt.imshow(np.angle(Z), extent=(X.min(), X.max(), Y.min(), Y.max()))\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, scipy.optimize, matplotlib\n```python\nimport numpy as np\nfrom scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\n\n# Constants\nX = np.linspace(0, 10, 100)\n\ndef fit_and_plot_function(func, ydata, p0=None):\n    \"\"\"\n    Fit a function to data using non-linear least squares, plot the data and the \n    fit, and return the optimized parameters.\n    \n    Parameters:\n    func (function): The function to fit.\n    ydata (array_like): The y-data to fit.\n    p0 (array_like, optional): Initial guess for the parameters. If None, use \n    ones.\n    \n    Returns:\n    array: Optimized parameters.\n    \n    Requirements:\n    - numpy\n    - scipy.optimize\n    - matplotlib.pyplot\n    \n    Example:\n    >>> y = 3.0 * np.sin(2.0 * X + 0.5) + 0.5 * np.random.normal(size=X.shape)\n    >>> params = fit_and_plot_function(np.sin, y, p0=[1.0, 1.0, 0.0])\n    >>> print(params)\n    \"\"\"\n    if p0 is None:\n        p0 = np.ones(func.__code__.co_argcount)\n\n    popt, pcov = curve_fit(func, X, ydata, p0)\n\n    plt.figure()\n    plt.plot(X, ydata, 'b-', label='data')\n    plt.plot(X, func(X, *popt), 'r-', label='fit: %s' % ' '.join(['%.3f' % p for p in popt]))\n    plt.legend()\n    plt.show()\n\n    return popt\n```"}
{"seed": "{\"task_id\": 493386, \"prompt\": \"def f_493386():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"sys.stdout.write('.')\", \"test_start\": \"\\nimport sys\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate() == 1\\n\"], \"entry_point\": \"f_493386\", \"intent\": \"print \\\".\\\" without newline\", \"library\": [\"sys\"]}", "generation": "Scenario 1: \nos, sys, time, threading\n```python\nimport os\nimport sys\nimport time\nimport threading\n\n# Constants\nPROGRESS_BAR_WIDTH = 50\n\ndef progress_bar(duration):\n    \"\"\"\n    Display a progress bar in the console for a specified duration.\n\n    Parameters:\n    duration (int): The duration for which the progress bar should run (in seconds).\n\n    Requirements:\n    - os\n    - sys\n    - time\n    - threading\n\n    Example:\n    >>> progress_bar(5)\n    \"\"\"\n    for i in range(duration):\n        percent = (i+1)/duration\n        sys.stdout.write('\\r')\n        sys.stdout.write(\"[%-50s] %d%%\" % ('='*int(percent*PROGRESS_BAR_WIDTH), int(percent*100)))\n        sys.stdout.flush()\n        time.sleep(1)\n    print()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, sys, shutil, pathlib\n```python\nimport os\nimport sys\nimport shutil\nfrom pathlib import Path\n\ndef copy_directory(src_dir, dest_dir):\n    \"\"\"\n    Copy all files from a source directory to a destination directory.\n\n    Parameters:\n    src_dir (str): The source directory.\n    dest_dir (str): The destination directory.\n\n    Requirements:\n    - os\n    - sys\n    - shutil\n    - pathlib.Path\n\n    Example:\n    >>> copy_directory('/path/to/src', '/path/to/dest')\n    \"\"\"\n    src_path = Path(src_dir)\n    dest_path = Path(dest_dir)\n\n    if not src_path.is_dir() or not dest_path.is_dir():\n        sys.stderr.write(\"Either source or destination is not a directory.\\n\")\n        return False\n\n    for item in src_path.iterdir():\n        if item.is_file():\n            shutil.copy(str(item), str(dest_path))\n\n    return True\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nos, sys, glob, csv\n```python\nimport os\nimport sys\nimport glob\nimport csv\n\ndef parse_logs(directory, output_file):\n    \"\"\"\n    Parse all log files in a directory and write the results to a CSV file.\n\n    Parameters:\n    directory (str): The directory containing the log files.\n    output_file (str): The CSV file to write the results to.\n\n    Requirements:\n    - os\n    - sys\n    - glob\n    - csv\n\n    Example:\n    >>> parse_logs('/path/to/logs', 'output.csv')\n    \"\"\"\n    log_files = glob.glob(os.path.join(directory, \"*.log\"))\n\n    with open(output_file, 'w') as f:\n        writer = csv.writer(f)\n        writer.writerow([\"filename\", \"line_number\", \"content\"])\n\n        for log_file in log_files:\n            with open(log_file, 'r') as log:\n                for i, line in enumerate(log, start=1):\n                    writer.writerow([os.path.basename(log_file), i, line.strip()])\n\n    return True\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nsubprocess, sys, time, threading\n```python\nimport subprocess\nimport sys\nimport time\nimport threading\n\ndef run_command_with_timeout(command, timeout):\n    \"\"\"\n    Run a shell command with a timeout.\n\n    Parameters:\n    command (str): The command to run.\n    timeout (int): The timeout in seconds.\n\n    Requirements:\n    - subprocess\n    - sys\n    - time\n    - threading\n\n    Example:\n    >>> run_command_with_timeout('ls -l', 5)\n    \"\"\"\n    process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    timer = threading.Timer(timeout, process.kill)\n    timer.start()\n\n    stdout, stderr = process.communicate()\n\n    if timer.is_alive():\n        timer.cancel()\n        return stdout.decode('utf-8'), stderr.decode('utf-8'), process.returncode\n    else:\n        sys.stderr.write(\"Command timed out.\\n\")\n        return None, None, -1\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, sys, hashlib, pathlib\n```python\nimport os\nimport sys\nimport hashlib\nfrom pathlib import Path\n\ndef calculate_file_hash(file_path, algorithm='sha256'):\n    \"\"\"\n    Calculate the hash of a file using a specified algorithm.\n\n    Parameters:\n    file_path (str): The path to the file.\n    algorithm (str): The hash algorithm to use. Default is 'sha256'.\n\n    Requirements:\n    - os\n    - sys\n    - hashlib\n    - pathlib.Path\n\n    Example:\n    >>> calculate_file_hash('/path/to/file.txt', 'md5')\n    \"\"\"\n    file_path = Path(file_path)\n\n    if not file_path.is_file():\n        sys.stderr.write(\"The specified path is not a file.\\n\")\n        return None\n\n    hash_func = getattr(hashlib, algorithm, None)\n\n    if not hash_func:\n        sys.stderr.write(f\"Unsupported hash algorithm: {algorithm}\\n\")\n        return None\n\n    with open(file_path, 'rb') as f:\n        data = f.read()\n        file_hash = hash_func(data).hexdigest()\n\n    return file_hash\n```"}
{"seed": "{\"task_id\": 6569528, \"prompt\": \"def f_6569528():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"int(round(2.52 * 100))\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate() == 252\\n\"], \"entry_point\": \"f_6569528\", \"intent\": \"round off the float that is the product of `2.52 * 100` and convert it to an int\", \"library\": []}", "generation": "Scenario 1: \nnumpy, random, math, matplotlib\n```python\nimport numpy as np\nimport random\nimport math\nimport matplotlib.pyplot as plt\n\n# Constants\nNUM_SAMPLES = 10000\nMU = 0\nSIGMA = 2.52\n\ndef simulate_normal_distribution():\n    \"\"\"\n    Simulate a normal distribution with a given mu and sigma, round off the generated \n    float numbers to the nearest integer and plot the histogram of the distribution.\n    \n    Requirements:\n    - numpy\n    - random\n    - math\n    - matplotlib.pyplot\n    \n    Example:\n    >>> simulate_normal_distribution()\n    \"\"\"\n    samples = np.random.normal(MU, SIGMA, NUM_SAMPLES)\n    samples_rounded = [int(round(sample)) for sample in samples]\n\n    plt.hist(samples_rounded, bins=50, density=True)\n    plt.show()\n\n    return samples_rounded\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, random, math, statistics\n```python\nimport numpy as np\nimport random\nimport math\nimport statistics\n\n# Constants\nNUM_SAMPLES = 10000\nMU = 0\nSIGMA = 2.52\n\ndef calculate_mode_normal_distribution():\n    \"\"\"\n    Simulate a normal distribution with a given mu and sigma, round off the generated \n    float numbers to the nearest integer and calculate the mode of the distribution.\n    \n    Returns:\n    int: The mode of the distribution.\n\n    Requirements:\n    - numpy\n    - random\n    - math\n    - statistics\n    \n    Example:\n    >>> calculate_mode_normal_distribution()\n    \"\"\"\n    samples = np.random.normal(MU, SIGMA, NUM_SAMPLES)\n    samples_rounded = [int(round(sample)) for sample in samples]\n    mode = statistics.mode(samples_rounded)\n\n    return mode\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, random, math, pandas\n```python\nimport numpy as np\nimport random\nimport math\nimport pandas as pd\n\n# Constants\nNUM_SAMPLES = 10000\nMU = 0\nSIGMA = 2.52\n\ndef calculate_percentiles_normal_distribution():\n    \"\"\"\n    Simulate a normal distribution with a given mu and sigma, round off the generated \n    float numbers to the nearest integer and calculate the 25th, 50th, and 75th \n    percentiles of the distribution.\n    \n    Returns:\n    dict: A dictionary with the percentiles.\n\n    Requirements:\n    - numpy\n    - random\n    - math\n    - pandas\n    \n    Example:\n    >>> calculate_percentiles_normal_distribution()\n    \"\"\"\n    samples = np.random.normal(MU, SIGMA, NUM_SAMPLES)\n    samples_rounded = [int(round(sample)) for sample in samples]\n    df = pd.DataFrame(samples_rounded)\n\n    percentiles = df.quantile([.25, .5, .75]).to_dict()[0]\n\n    return percentiles\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, random, math, scipy.stats\n```python\nimport numpy as np\nimport random\nimport math\nfrom scipy import stats\n\n# Constants\nNUM_SAMPLES = 10000\nMU = 0\nSIGMA = 2.52\n\ndef calculate_kurtosis_normal_distribution():\n    \"\"\"\n    Simulate a normal distribution with a given mu and sigma, round off the generated \n    float numbers to the nearest integer and calculate the kurtosis of the distribution.\n    \n    Returns:\n    float: The kurtosis of the distribution.\n\n    Requirements:\n    - numpy\n    - random\n    - math\n    - scipy.stats\n    \n    Example:\n    >>> calculate_kurtosis_normal_distribution()\n    \"\"\"\n    samples = np.random.normal(MU, SIGMA, NUM_SAMPLES)\n    samples_rounded = [int(round(sample)) for sample in samples]\n    kurtosis = stats.kurtosis(samples_rounded)\n\n    return kurtosis\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, random, math, seaborn\n```python\nimport numpy as np\nimport random\nimport math\nimport seaborn as sns\n\n# Constants\nNUM_SAMPLES = 10000\nMU = 0\nSIGMA = 2.52\n\ndef plot_kde_normal_distribution():\n    \"\"\"\n    Simulate a normal distribution with a given mu and sigma, round off the generated \n    float numbers to the nearest integer and plot the kernel density estimation (KDE) \n    of the distribution.\n    \n    Requirements:\n    - numpy\n    - random\n    - math\n    - seaborn\n    \n    Example:\n    >>> plot_kde_normal_distribution()\n    \"\"\"\n    samples = np.random.normal(MU, SIGMA, NUM_SAMPLES)\n    samples_rounded = [int(round(sample)) for sample in samples]\n\n    sns.kdeplot(samples_rounded)\n    plt.show()\n```\n"}
{"seed": "{\"task_id\": 3964681, \"prompt\": \"def f_3964681():\\n\\t\", \"suffix\": \"\\n\\treturn files\", \"canonical_solution\": \"\\n\\tos.chdir('/mydir')\\n\\tfiles = [] \\n\\tfor file in glob.glob('*.txt'):\\n\\t\\tfiles.append(file)\\n\", \"test_start\": \"\\nimport os\\nimport glob\\nfrom unittest.mock import Mock\\n\\ndef check(candidate):\", \"test\": [\"\\n    samples = ['abc.txt']\\n    os.chdir = Mock()\\n    glob.glob = Mock(return_value = samples)\\n    assert candidate() == samples\\n\"], \"entry_point\": \"f_3964681\", \"intent\": \"Find all files `files` in directory '/mydir' with extension '.txt'\", \"library\": [\"glob\", \"os\"]}", "generation": "Scenario 1:\nos, glob, pandas, matplotlib.pyplot\n```python\nimport os\nimport glob\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nDIRECTORY = '/mydir'\nEXTENSION = '.txt'\n\ndef analyze_file_sizes():\n    \"\"\"\n    Analyze the sizes of all files in a specific directory with a given extension and \n    visualize the distribution of file sizes using a histogram.\n    \n    Returns:\n    list: A list of file sizes in bytes.\n    \n    Requirements:\n    - os\n    - glob\n    - pandas\n    - matplotlib.pyplot\n    \n    Example:\n    >>> analyze_file_sizes()\n    \"\"\"\n    os.chdir(DIRECTORY)\n    file_sizes = []\n\n    for file in glob.glob(f'*{EXTENSION}'):\n        file_size = os.path.getsize(file)\n        file_sizes.append(file_size)\n\n    # Visualize the file sizes\n    pd.Series(file_sizes).plot(kind='hist', edgecolor='black')\n\n    plt.title('Distribution of File Sizes')\n    plt.xlabel('File Size (Bytes)')\n    plt.ylabel('Frequency')\n    plt.show()\n\n    return file_sizes\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, glob, csv, collections\n```python\nimport os\nimport glob\nimport csv\nfrom collections import Counter\n\n# Constants\nDIRECTORY = '/mydir'\nEXTENSION = '.csv'\n\ndef count_column_values(column_name):\n    \"\"\"\n    Count the occurrence of each unique value in a specific column across all csv files \n    in a specific directory.\n    \n    Parameters:\n    column_name (str): The name of the column to count values for.\n    \n    Returns:\n    Counter: A collections.Counter object containing the counts of each unique value.\n    \n    Requirements:\n    - os\n    - glob\n    - csv\n    - collections\n    \n    Example:\n    >>> count_column_values('Name')\n    \"\"\"\n    os.chdir(DIRECTORY)\n    value_counts = Counter()\n\n    for file in glob.glob(f'*{EXTENSION}'):\n        with open(file, 'r') as f:\n            reader = csv.DictReader(f)\n            for row in reader:\n                value_counts[row[column_name]] += 1\n\n    return value_counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, glob, shutil, zipfile\n```python\nimport os\nimport glob\nimport shutil\nimport zipfile\n\n# Constants\nDIRECTORY = '/mydir'\nEXTENSION = '.txt'\nARCHIVE_NAME = 'archive.zip'\n\ndef archive_files():\n    \"\"\"\n    Archive all files in a specific directory with a given extension into a zip file.\n    \n    Returns:\n    str: The path to the created zip file.\n    \n    Requirements:\n    - os\n    - glob\n    - shutil\n    - zipfile\n    \n    Example:\n    >>> archive_files()\n    \"\"\"\n    os.chdir(DIRECTORY)\n    with zipfile.ZipFile(ARCHIVE_NAME, 'w') as zipf:\n        for file in glob.glob(f'*{EXTENSION}'):\n            zipf.write(file)\n\n    return os.path.join(DIRECTORY, ARCHIVE_NAME)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, glob, hashlib\n```python\nimport os\nimport glob\nimport hashlib\n\n# Constants\nDIRECTORY = '/mydir'\nEXTENSION = '.txt'\n\ndef calculate_file_hashes():\n    \"\"\"\n    Calculate the MD5 hashes for all files in a specific directory with a given extension.\n    \n    Returns:\n    dict: A dictionary where keys are file names and values are their corresponding MD5 hashes.\n    \n    Requirements:\n    - os\n    - glob\n    - hashlib\n    \n    Example:\n    >>> calculate_file_hashes()\n    \"\"\"\n    os.chdir(DIRECTORY)\n    file_hashes = {}\n\n    for file in glob.glob(f'*{EXTENSION}'):\n        with open(file, 'rb') as f:\n            file_hash = hashlib.md5(f.read()).hexdigest()\n            file_hashes[file] = file_hash\n\n    return file_hashes\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, glob, pandas\n```python\nimport os\nimport glob\nimport pandas as pd\n\n# Constants\nDIRECTORY = '/mydir'\nEXTENSION = '.csv'\n\ndef merge_csv_files():\n    \"\"\"\n    Merge all csv files in a specific directory into a single pandas DataFrame.\n    \n    Returns:\n    DataFrame: The merged pandas DataFrame.\n    \n    Requirements:\n    - os\n    - glob\n    - pandas\n    \n    Example:\n    >>> merge_csv_files()\n    \"\"\"\n    os.chdir(DIRECTORY)\n    dfs = []\n\n    for file in glob.glob(f'*{EXTENSION}'):\n        df = pd.read_csv(file)\n        dfs.append(df)\n\n    merged_df = pd.concat(dfs, ignore_index=True)\n\n    return merged_df\n```\n"}
{"seed": "{\"task_id\": 3964681, \"prompt\": \"def f_3964681():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"[file for file in os.listdir('/mydir') if file.endswith('.txt')]\", \"test_start\": \"\\nimport os\\nfrom unittest.mock import Mock\\n\\ndef check(candidate):\", \"test\": [\"\\n    samples = ['abc.txt', 'f.csv']\\n    os.listdir = Mock(return_value = samples)\\n    assert candidate() == ['abc.txt']\\n\"], \"entry_point\": \"f_3964681\", \"intent\": \"Find all files in directory \\\"/mydir\\\" with extension \\\".txt\\\"\", \"library\": [\"os\"]}", "generation": "Scenario 1:\npandas, os, re, numpy, shutil\n```python\nimport pandas as pd\nimport os\nimport re\nimport numpy as np\nimport shutil\n\n# Constants\nDIR_NAME = '/mydir'\nFILE_EXT = '.txt'\nCSV_EXT = '.csv'\n\ndef move_txt_files():\n    \"\"\"\n    Find all files in directory \"/mydir\" with extension \".txt\". Then, randomly split these files\n    into two sets with ratio 7:3. Move the 70% files to a directory named \"/mydir/train\" and the rest \n    to \"/mydir/test\". Finally, create a DataFrame summarizing the file counts in each directory and \n    save it to \"/mydir/summary.csv\".\n    \n    Requirements:\n    - pandas\n    - os\n    - re\n    - numpy\n    - shutil\n\n    Example:\n    >>> move_txt_files()\n    \"\"\"\n    txt_files = [file for file in os.listdir(DIR_NAME) if file.endswith(FILE_EXT)]\n    np.random.shuffle(txt_files)\n    train_files = txt_files[:int(len(txt_files)*0.7)]\n    test_files = txt_files[int(len(txt_files)*0.7):]\n\n    train_dir = os.path.join(DIR_NAME, 'train')\n    test_dir = os.path.join(DIR_NAME, 'test')\n\n    os.makedirs(train_dir, exist_ok=True)\n    os.makedirs(test_dir, exist_ok=True)\n\n    for file in train_files:\n        shutil.move(os.path.join(DIR_NAME, file), os.path.join(train_dir, file))\n\n    for file in test_files:\n        shutil.move(os.path.join(DIR_NAME, file), os.path.join(test_dir, file))\n\n    summary = pd.DataFrame({\n        'Directory': ['train', 'test'],\n        'File Count': [len(train_files), len(test_files)]\n    })\n\n    summary.to_csv(os.path.join(DIR_NAME, 'summary'+CSV_EXT), index=False)\n\n    return summary\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, re, json, collections, time\n```python\nimport os\nimport re\nimport json\nfrom collections import Counter\nimport time\n\n# Constants\nLOG_DIR = '/logdir'\nLOG_EXT = '.log'\nRESULT_FILE = 'result.json'\n\ndef analyze_log_files():\n    \"\"\"\n    Find all log files in directory \"/logdir\" with extension \".log\". Then, calculate the occurrences \n    of each log level (ERROR, WARN, INFO, DEBUG) in all files and save the result to a JSON file \n    named \"result.json\".\n    \n    Requirements:\n    - os\n    - re\n    - json\n    - collections.Counter\n    - time\n\n    Example:\n    >>> analyze_log_files()\n    \"\"\"\n    log_files = [file for file in os.listdir(LOG_DIR) if file.endswith(LOG_EXT)]\n    log_levels = ['ERROR', 'WARN', 'INFO', 'DEBUG']\n    counts = Counter()\n\n    for file in log_files:\n        with open(os.path.join(LOG_DIR, file), 'r') as f:\n            for line in f:\n                for level in log_levels:\n                    if level in line:\n                        counts[level] += 1\n\n    with open(RESULT_FILE, 'w') as f:\n        json.dump(counts, f)\n\n    return counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, re, glob, zipfile, tarfile\n```python\nimport os\nimport re\nimport glob\nimport zipfile\nimport tarfile\n\n# Constants\nARCHIVE_DIR = '/archiveDir'\nZIP_EXT = '.zip'\nTAR_GZ_EXT = '.tar.gz'\n\ndef extract_archives():\n    \"\"\"\n    Find all .zip and .tar.gz files in directory \"/archiveDir\". Extract all these files in the \n    same directory.\n    \n    Requirements:\n    - os\n    - re\n    - glob\n    - zipfile\n    - tarfile\n\n    Example:\n    >>> extract_archives()\n    \"\"\"\n    zip_files = glob.glob(os.path.join(ARCHIVE_DIR, '*'+ZIP_EXT))\n    tar_gz_files = glob.glob(os.path.join(ARCHIVE_DIR, '*'+TAR_GZ_EXT))\n\n    for zip_file in zip_files:\n        with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n            zip_ref.extractall(ARCHIVE_DIR)\n\n    for tar_gz_file in tar_gz_files:\n        with tarfile.open(tar_gz_file, 'r:gz') as tar_ref:\n            tar_ref.extractall(ARCHIVE_DIR)\n\n    return len(zip_files) + len(tar_gz_files)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, re, shutil, pathlib\n```python\nimport os\nimport re\nimport shutil\nfrom pathlib import Path\n\n# Constants\nSOURCE_DIR = '/sourceDir'\nTARGET_DIR = '/targetDir'\n\ndef move_jpg_files():\n    \"\"\"\n    Find all JPEG files in directory \"/sourceDir\". Move all these files to directory \"/targetDir\".\n    \n    Requirements:\n    - os\n    - re\n    - shutil\n    - pathlib.Path\n\n    Example:\n    >>> move_jpg_files()\n    \"\"\"\n    source_dir = Path(SOURCE_DIR)\n    target_dir = Path(TARGET_DIR)\n    target_dir.mkdir(parents=True, exist_ok=True)\n\n    jpg_files = list(source_dir.glob('**/*.jpg'))\n\n    for jpg_file in jpg_files:\n        shutil.move(str(jpg_file), str(target_dir / jpg_file.name))\n\n    return len(jpg_files)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, csv, pathlib, collections\n```python\nimport os\nimport csv\nfrom pathlib import Path\nfrom collections import defaultdict\n\n# Constants\nDATA_DIR = '/dataDir'\nCSV_EXT = '.csv'\n\ndef analyze_csv_files():\n    \"\"\"\n    Find all CSV files in directory \"/dataDir\". Calculate the total number of rows and columns for \n    each file and return the result as a dictionary.\n    \n    Requirements:\n    - os\n    - csv\n    - pathlib.Path\n    - collections.defaultdict\n\n    Example:\n    >>> analyze_csv_files()\n    \"\"\"\n    data_dir = Path(DATA_DIR)\n    csv_files = list(data_dir.glob('*'+CSV_EXT))\n\n    result = defaultdict(dict)\n\n    for csv_file in csv_files:\n        with open(csv_file, 'r') as f:\n            reader = csv.reader(f)\n            rows = list(reader)\n            result[csv_file.name]['rows'] = len(rows)\n            result[csv_file.name]['columns'] = len(rows[0]) if rows else 0\n\n    return result\n```"}
{"seed": "{\"task_id\": 3964681, \"prompt\": \"def f_3964681():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"[file for (root, dirs, files) in os.walk('/mydir') for file in files if file.endswith('.txt')]\", \"test_start\": \"\\nimport os\\nfrom unittest.mock import Mock\\n\\ndef check(candidate):\", \"test\": [\"\\n    name = '/mydir'\\n    samples = [(name, [], ['abc.txt', 'f.csv'])]\\n    os.walk = Mock(return_value = samples)\\n    assert candidate() == ['abc.txt']\\n\"], \"entry_point\": \"f_3964681\", \"intent\": \"Find all files in directory \\\"/mydir\\\" with extension \\\".txt\\\"\", \"library\": [\"os\"]}", "generation": "Scenario 1: \nos, pandas, csv, string, random\n```python\nimport os\nimport pandas as pd\nimport csv\nimport string\nimport random\n\n# Constants\nDIR_PATH = '/mydir'\nEXTENSION = '.csv'\nCSV_HEADERS = ['First Name', 'Last Name', 'Email', 'Phone Number']\n\ndef generate_random_csv_files(n):\n    \"\"\"\n    Generate n number of random .csv files in a specific directory. Each file contains\n    random contact information (first name, last name, email, phone number).\n\n    Parameters:\n    n (int): The number of .csv files to generate.\n\n    Returns:\n    list: A list of file paths for the generated .csv files.\n\n    Requirements:\n    - os\n    - pandas\n    - csv\n    - string\n    - random\n\n    Example:\n    >>> generate_random_csv_files(5)\n    \"\"\"\n    file_paths = []\n    for i in range(n):\n        filename = \"\".join(random.choices(string.ascii_letters, k=10)) + EXTENSION\n        file_path = os.path.join(DIR_PATH, filename)\n\n        data = {\n            'First Name': [\"\".join(random.choices(string.ascii_letters, k=5)) for _ in range(10)],\n            'Last Name': [\"\".join(random.choices(string.ascii_letters, k=5)) for _ in range(10)],\n            'Email': [\"\".join(random.choices(string.ascii_letters, k=5)) + '@gmail.com' for _ in range(10)],\n            'Phone Number': [''.join([\"{}\".format(random.randint(0, 9)) for num in range(0, 10)]) for _ in range(10)]\n        }\n\n        df = pd.DataFrame(data)\n        df.to_csv(file_path, index=False)\n\n        file_paths.append(file_path)\n        \n    return file_paths\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nos, pandas, csv, re, datetime\n```python\nimport os\nimport pandas as pd\nimport csv\nimport re\nfrom datetime import datetime\n\n# Constants\nDIR_PATH = '/mydir'\nEXTENSION = '.csv'\nCSV_HEADERS = ['First Name', 'Last Name', 'Email', 'Phone Number']\n\ndef find_files_with_date_pattern():\n    \"\"\"\n    Find all .csv files in a specific directory that have a date pattern (YYYY-MM-DD) in their names.\n\n    Returns:\n    list: A list of file paths for the .csv files that match the date pattern.\n\n    Requirements:\n    - os\n    - pandas\n    - csv\n    - re\n    - datetime\n\n    Example:\n    >>> find_files_with_date_pattern()\n    \"\"\"\n    file_paths = []\n    for root, dirs, files in os.walk(DIR_PATH):\n        for file in files:\n            if file.endswith(EXTENSION) and re.search(r'\\d{4}-\\d{2}-\\d{2}', file):\n                file_path = os.path.join(root, file)\n                file_paths.append(file_path)\n                \n    return file_paths\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, json, urllib.request, time\n```python\nimport os\nimport json\nimport urllib.request\nimport time\n\n# Constants\nDIR_PATH = '/mydir'\nEXTENSION = '.json'\nURL = 'https://jsonplaceholder.typicode.com/posts'\n\ndef download_json_files(n):\n    \"\"\"\n    Download n number of .json files from a specific URL and save them in a specific directory.\n\n    Parameters:\n    n (int): The number of .json files to download.\n\n    Returns:\n    list: A list of file paths for the downloaded .json files.\n\n    Requirements:\n    - os\n    - json\n    - urllib.request\n    - time\n\n    Example:\n    >>> download_json_files(5)\n    \"\"\"\n    file_paths = []\n    with urllib.request.urlopen(URL) as url:\n        data = json.loads(url.read().decode())\n        for i in range(n):\n            filename = str(time.time()) + EXTENSION\n            file_path = os.path.join(DIR_PATH, filename)\n            with open(file_path, 'w') as f:\n                json.dump(data[i], f)\n            file_paths.append(file_path)\n\n    return file_paths\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nos, shutil, fnmatch, pathlib\n```python\nimport os\nimport shutil\nimport fnmatch\nimport pathlib\n\n# Constants\nSOURCE_DIR = '/mydir/source'\nTARGET_DIR = '/mydir/target'\nEXTENSION = '.txt'\n\ndef copy_files_to_target_directory():\n    \"\"\"\n    Copy all .txt files from a source directory to a target directory. \n    If the target directory does not exist, create it.\n\n    Returns:\n    list: A list of file paths for the copied .txt files in the target directory.\n\n    Requirements:\n    - os\n    - shutil\n    - fnmatch\n    - pathlib\n\n    Example:\n    >>> copy_files_to_target_directory()\n    \"\"\"\n    pathlib.Path(TARGET_DIR).mkdir(parents=True, exist_ok=True)\n    \n    file_paths = []\n    for root, dirs, files in os.walk(SOURCE_DIR):\n        for file in fnmatch.filter(files, '*' + EXTENSION):\n            shutil.copy(os.path.join(root, file), TARGET_DIR)\n            file_paths.append(os.path.join(TARGET_DIR, file))\n\n    return file_paths\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, zipfile, fnmatch\n```python\nimport os\nimport zipfile\nimport fnmatch\n\n# Constants\nDIR_PATH = '/mydir'\nEXTENSION = '.txt'\nZIP_PATH = '/mydir/archive.zip'\n\ndef compress_files_to_zip():\n    \"\"\"\n    Compress all .txt files in a specific directory to a .zip file.\n\n    Returns:\n    str: The file path for the .zip file.\n\n    Requirements:\n    - os\n    - zipfile\n    - fnmatch\n\n    Example:\n    >>> compress_files_to_zip()\n    \"\"\"\n    with zipfile.ZipFile(ZIP_PATH, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        for root, dirs, files in os.walk(DIR_PATH):\n            for file in fnmatch.filter(files, '*' + EXTENSION):\n                zipf.write(os.path.join(root, file), arcname=file)\n\n    return ZIP_PATH\n```"}
{"seed": "{\"task_id\": 20865487, \"prompt\": \"def f_20865487(df):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"df.plot(legend=False)\", \"test_start\": \"\\nimport os \\nimport pandas as pd\\n\\ndef check(candidate):\", \"test\": [\"\\n    df = pd.DataFrame([1, 2, 3, 4, 5], columns = ['Vals'])\\n    res = candidate(df)\\n    assert 'AxesSubplot' in str(type(res))\\n    assert res.legend_ is None\\n\"], \"entry_point\": \"f_20865487\", \"intent\": \"plot dataframe `df` without a legend\", \"library\": [\"os\", \"pandas\"]}", "generation": "Scenario 1:\npandas, matplotlib.pyplot, seaborn, numpy\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Constants\nCATEGORIES = ['A', 'B', 'C', 'D', 'E']\nCOLORS = ['red', 'blue', 'green', 'purple', 'yellow']\n\ndef plot_categories_distribution(df, column):\n    \"\"\"\n    Plot the distribution of a categorical column in the dataframe. Each category is \n    represented with a different color. The plot doesn't have a legend.\n\n    Parameters:\n    df (DataFrame): The DataFrame to plot.\n    column (str): The name of the column to plot.\n\n    Returns:\n    AxesSubplot: The plot's AxesSubplot.\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - seaborn\n    - numpy\n    \n    Example:\n    >>> df = pd.DataFrame({'Category': np.random.choice(CATEGORIES, 1000)})\n    >>> plot = plot_categories_distribution(df, 'Category')\n    >>> plt.show()\n    \"\"\"\n    plt.figure(figsize=(10, 6))\n    plot = sns.countplot(x=column, data=df, palette=COLORS, legend=False)\n    return plot\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, matplotlib.pyplot, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nCATEGORIES = ['A', 'B', 'C', 'D', 'E']\nCOLORS = ['red', 'blue', 'green', 'purple', 'yellow']\n\ndef plot_categorical_vs_numerical(df, cat_col, num_col):\n    \"\"\"\n    Plot a boxplot to visualize the distribution of a numerical column grouped by \n    a categorical column. Each category is represented with a different color. \n    The plot doesn't have a legend.\n\n    Parameters:\n    df (DataFrame): The DataFrame to plot.\n    cat_col (str): The name of the categorical column.\n    num_col (str): The name of the numerical column.\n\n    Returns:\n    AxesSubplot: The plot's AxesSubplot.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - seaborn\n    \n    Example:\n    >>> df = pd.DataFrame({\n    ...     'Category': np.random.choice(CATEGORIES, 1000),\n    ...     'Value': np.random.randn(1000)\n    ... })\n    >>> plot = plot_categorical_vs_numerical(df, 'Category', 'Value')\n    >>> plt.show()\n    \"\"\"\n    plt.figure(figsize=(10, 6))\n    plot = sns.boxplot(x=cat_col, y=num_col, data=df, palette=COLORS)\n    plot.legend_ = None\n    return plot\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, matplotlib.pyplot, seaborn, numpy\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Constants\nCATEGORIES = ['A', 'B', 'C', 'D', 'E']\nCOLORS = ['red', 'blue', 'green', 'purple', 'yellow']\n\ndef plot_pairplot(df):\n    \"\"\"\n    Plot a pairplot to visualize the relationships between numerical columns in the \n    dataframe. Each category of the 'Category' column is represented with a different \n    color. The plot doesn't have a legend.\n\n    Parameters:\n    df (DataFrame): The DataFrame to plot.\n\n    Returns:\n    PairGrid: The plot's PairGrid.\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - seaborn\n    - numpy\n\n    Example:\n    >>> df = pd.DataFrame({\n    ...     'Category': np.random.choice(CATEGORIES, 1000),\n    ...     'Value1': np.random.randn(1000),\n    ...     'Value2': np.random.randn(1000),\n    ...     'Value3': np.random.randn(1000)\n    ... })\n    >>> plot = plot_pairplot(df)\n    >>> plt.show()\n    \"\"\"\n    plot = sns.pairplot(df, hue='Category', palette=COLORS)\n    plot._legend.remove()\n    return plot\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, matplotlib.pyplot, seaborn, numpy\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Constants\nCATEGORIES = ['A', 'B', 'C', 'D', 'E']\nCOLORS = ['red', 'blue', 'green', 'purple', 'yellow']\n\ndef plot_correlation_matrix(df):\n    \"\"\"\n    Plot a heatmap to visualize the correlation matrix of the dataframe. The plot \n    doesn't have a legend.\n\n    Parameters:\n    df (DataFrame): The DataFrame to plot.\n\n    Returns:\n    AxesSubplot: The plot's AxesSubplot.\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - seaborn\n    - numpy\n\n    Example:\n    >>> df = pd.DataFrame({\n    ...     'Value1': np.random.randn(1000),\n    ...     'Value2': np.random.randn(1000) + np.random.randn(1000),\n    ...     'Value3': np.random.randn(1000) - np.random.randn(1000),\n    ...     'Value4': np.random.randn(1000) * np.random.randn(1000)\n    ... })\n    >>> plot = plot_correlation_matrix(df)\n    >>> plt.show()\n    \"\"\"\n    corr = df.corr()\n    plt.figure(figsize=(10, 6))\n    plot = sns.heatmap(corr, annot=True, cmap='coolwarm')\n    return plot\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, matplotlib.pyplot, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nCATEGORIES = ['A', 'B', 'C', 'D', 'E']\nCOLORS = ['red', 'blue', 'green', 'purple', 'yellow']\n\ndef plot_distribution(df, column):\n    \"\"\"\n    Plot a histogram and a KDE plot to visualize the distribution of a numerical column \n    in the dataframe. The KDE plot is represented with a different color. The plot \n    doesn't have a legend.\n\n    Parameters:\n    df (DataFrame): The DataFrame to plot.\n    column (str): The name of the column to plot.\n\n    Returns:\n    AxesSubplot: The plot's AxesSubplot.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> df = pd.DataFrame({'Value': np.random.randn(1000)})\n    >>> plot = plot_distribution(df, 'Value')\n    >>> plt.show()\n    \"\"\"\n    plt.figure(figsize=(10, 6))\n    plot = sns.histplot(df[column], kde=False, color=COLORS[0])\n    sns.kdeplot(df[column], color=COLORS[1])\n    plot.legend_ = None\n    return plot\n```\n"}
{"seed": "{\"task_id\": 13368659, \"prompt\": \"def f_13368659():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"['192.168.%d.%d'%(i, j) for i in range(256) for j in range(256)]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    addrs = candidate()\\n    assert len(addrs) == 256*256\\n    assert addrs == [f'192.168.{i}.{j}' for i in range(256) for j in range(256)]\\n\"], \"entry_point\": \"f_13368659\", \"intent\": \"loop through the IP address range \\\"192.168.x.x\\\"\", \"library\": []}", "generation": "Scenario 3:\nitertools, socket, sys\n```python\nimport socket\nimport sys\nfrom itertools import product\n\n# Constants\nIP_PREFIX = '192.168.'\nPORTS = list(range(20, 30))  # Sample ports\n\ndef scan_ip_addresses():\n    \"\"\"\n    Loop through the IP address range \"192.168.x.x\" and for each IP, \n    loop through a range of ports (20-30) and check if the port is open.\n    \n    Returns:\n    dict: A dictionary with IP addresses as keys and a list of open ports as values.\n\n    Requirements:\n    - itertools\n    - socket\n    - sys\n\n    Example:\n    >>> open_ports = scan_ip_addresses()\n    >>> print(open_ports)\n    \"\"\"\n    open_ports = {}\n\n    for ip_suffix in product(range(256), repeat=2):\n        ip = IP_PREFIX + '.'.join(map(str, ip_suffix))\n\n        for port in PORTS:\n            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n            sock.settimeout(1)\n\n            try:\n                sock.connect((ip, port))\n            except socket.error:\n                pass\n            else:\n                if ip not in open_ports:\n                    open_ports[ip] = []\n                open_ports[ip].append(port)\n\n            sock.close()\n\n    return open_ports\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nipaddress, requests, random\n```python\nimport random\nfrom ipaddress import IPv4Network\nimport requests\n\n# Constants\nIP_RANGE = '192.168.0.0/16'\nTIMEOUT = 5  # seconds\n\ndef find_active_webservers():\n    \"\"\"\n    Loop through the IP address range \"192.168.x.x\" and send a HTTP GET request \n    to each IP to check if it's an active web server.\n    \n    Returns:\n    list: A list of active web servers.\n\n    Requirements:\n    - ipaddress\n    - requests\n    - random\n\n    Example:\n    >>> active_webservers = find_active_webservers()\n    >>> print(active_webservers)\n    \"\"\"\n    active_webservers = []\n\n    for ip in IPv4Network(IP_RANGE):\n        try:\n            response = requests.get(f'http://{ip}', timeout=TIMEOUT)\n        except (requests.exceptions.Timeout, requests.exceptions.ConnectionError):\n            continue\n\n        if response.status_code == 200:\n            active_webservers.append(str(ip))\n\n    return active_webservers\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\ncsv, ipaddress, random\n```python\nimport csv\nfrom ipaddress import IPv4Network\nimport random\n\n# Constants\nIP_RANGE = '192.168.0.0/16'\nCSV_PATH = '/path/to/csv/file.csv'\n\ndef generate_ip_csv():\n    \"\"\"\n    Loop through the IP address range \"192.168.x.x\" and write each IP to a CSV file.\n    \n    Returns:\n    str: The path to the CSV file.\n\n    Requirements:\n    - csv\n    - ipaddress\n    - random\n\n    Example:\n    >>> csv_path = generate_ip_csv()\n    >>> print(csv_path)\n    \"\"\"\n    with open(CSV_PATH, 'w', newline='') as csvfile:\n        fieldnames = ['IP Address']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n\n        writer.writeheader()\n\n        for ip in IPv4Network(IP_RANGE):\n            writer.writerow({'IP Address': str(ip)})\n\n    return CSV_PATH\n```\n`GPT_ODEX_BREAK`\n\nScenario 6:\nipaddress, subprocess, sys\n```python\nimport subprocess\nfrom ipaddress import IPv4Network\n\n# Constants\nIP_RANGE = '192.168.0.0/16'\n\ndef ping_ips():\n    \"\"\"\n    Loop through the IP address range \"192.168.x.x\" and ping each IP to check if it's active.\n    \n    Returns:\n    dict: A dictionary with IP addresses as keys and their active status as values.\n\n    Requirements:\n    - ipaddress\n    - subprocess\n    - sys\n\n    Example:\n    >>> active_ips = ping_ips()\n    >>> print(active_ips)\n    \"\"\"\n    active_ips = {}\n\n    for ip in IPv4Network(IP_RANGE):\n        try:\n            subprocess.check_output(f'ping -c 1 {ip}', shell=True)\n            active_ips[str(ip)] = True\n        except subprocess.CalledProcessError:\n            active_ips[str(ip)] = False\n\n    return active_ips\n```\n`GPT_ODEX_BREAK`\n\nScenario 7:\nsocket, ipaddress, threading\n```python\nimport socket\nfrom ipaddress import IPv4Network\nfrom threading import Thread\n\n# Constants\nIP_RANGE = '192.168.0.0/16'\nPORT = 80\n\ndef check_open_ports():\n    \"\"\"\n    Loop through the IP address range \"192.168.x.x\" and check if port 80 is open on each IP.\n    \n    Returns:\n    dict: A dictionary with IP addresses as keys and their port status as values.\n\n    Requirements:\n    - socket\n    - ipaddress\n    - threading\n\n    Example:\n    >>> open_ports = check_open_ports()\n    >>> print(open_ports)\n    \"\"\"\n    open_ports = {}\n\n    def check_port(ip):\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        sock.settimeout(1)\n        try:\n            sock.connect((str(ip), PORT))\n            open_ports[str(ip)] = True\n        except socket.error:\n            open_ports[str(ip)] = False\n\n        sock.close()\n\n    threads = []\n\n    for ip in IPv4Network(IP_RANGE):\n        thread = Thread(target=check_port, args=(ip,))\n        thread.start()\n        threads.append(thread)\n\n    for thread in threads:\n        thread.join()\n\n    return open_ports\n```"}
{"seed": "{\"task_id\": 4065737, \"prompt\": \"def f_4065737(x):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"sum(1 << i for i, b in enumerate(x) if b)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([1,2,3]) == 7\\n\", \"\\n    assert candidate([1,2,None,3,None]) == 11\\n\"], \"entry_point\": \"f_4065737\", \"intent\": \"Sum the corresponding decimal values for binary values of each boolean element in list `x`\", \"library\": []}", "generation": "Scenario 1:\nnumpy, itertools, random\n```python\nimport numpy as np\nimport itertools\nimport random\n\n# Constants\nBINARY = [0, 1]\n\ndef generate_binary_matrix(n, m):\n    \"\"\"\n    Generate a n*m binary matrix and for each row sum the corresponding decimal values for binary values.\n    \n    Parameters:\n    n (int): The number of rows in the matrix.\n    m (int): The number of columns in the matrix.\n    \n    Returns:\n    np.array: The generated binary matrix.\n    list: The list of sums for each row.\n\n    Requirements:\n    - numpy\n    - itertools\n    - random\n\n    Example:\n    >>> matrix, sums = generate_binary_matrix(3, 3)\n    >>> print(matrix)\n    >>> print(sums)\n    \"\"\"\n    matrix = np.random.choice(BINARY, size=(n, m))\n    sums = [sum(1 << i for i, b in enumerate(row) if b) for row in matrix]\n\n    return matrix, sums\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nBINARY = [0, 1]\n\ndef plot_binary_dataframe(n, m):\n    \"\"\"\n    Generate a n*m binary DataFrame and plot a histogram of the decimal sums of each row.\n    \n    Parameters:\n    n (int): The number of rows in the DataFrame.\n    m (int): The number of columns in the DataFrame.\n    \n    Returns:\n    DataFrame: The generated binary DataFrame.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = plot_binary_dataframe(100, 10)\n    >>> print(df)\n    >>> df.sum(axis=1).apply(lambda x: sum(1 << i for i, b in enumerate(x) if b)).plot(kind='hist')\n    \"\"\"\n    df = pd.DataFrame(np.random.choice(BINARY, size=(n, m)))\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, random\n```python\nimport numpy as np\nimport random\n\n# Constants\nBINARY = [0, 1]\n\ndef generate_binary_array(n, m):\n    \"\"\"\n    Generate a n*m binary array and for each row sum the corresponding decimal values for binary values.\n    \n    Parameters:\n    n (int): The number of rows in the array.\n    m (int): The number of columns in the array.\n    \n    Returns:\n    np.array: The generated binary array.\n    list: The list of sums for each row.\n\n    Requirements:\n    - numpy\n    - random\n\n    Example:\n    >>> array, sums = generate_binary_array(3, 3)\n    >>> print(array)\n    >>> print(sums)\n    \"\"\"\n    array = np.random.choice(BINARY, size=(n, m))\n    sums = [sum(1 << i for i, b in enumerate(row) if b) for row in array]\n\n    return array, sums\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n\n# Constants\nBINARY = [0, 1]\n\ndef plot_binary_dataframe_heatmap(n, m):\n    \"\"\"\n    Generate a n*m binary DataFrame and plot a heatmap of the DataFrame.\n    \n    Parameters:\n    n (int): The number of rows in the DataFrame.\n    m (int): The number of columns in the DataFrame.\n    \n    Returns:\n    DataFrame: The generated binary DataFrame.\n\n    Requirements:\n    - pandas\n    - numpy\n    - seaborn\n\n    Example:\n    >>> df = plot_binary_dataframe_heatmap(100, 10)\n    >>> print(df)\n    >>> sns.heatmap(df)\n    \"\"\"\n    df = pd.DataFrame(np.random.choice(BINARY, size=(n, m)))\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, random, matplotlib\n```python\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\n\n# Constants\nBINARY = [0, 1]\n\ndef plot_binary_matrix_histogram(n, m):\n    \"\"\"\n    Generate a n*m binary matrix and plot a histogram of the decimal sums of each row.\n    \n    Parameters:\n    n (int): The number of rows in the matrix.\n    m (int): The number of columns in the matrix.\n    \n    Returns:\n    np.array: The generated binary matrix.\n\n    Requirements:\n    - numpy\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> matrix = plot_binary_matrix_histogram(100, 10)\n    >>> print(matrix)\n    >>> plt.hist([sum(1 << i for i, b in enumerate(row) if b) for row in matrix], bins=10)\n    \"\"\"\n    matrix = np.random.choice(BINARY, size=(n, m))\n\n    return matrix\n```"}
{"seed": "{\"task_id\": 8691311, \"prompt\": \"def f_8691311(line1, line2, line3, target):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"target.write('%r\\\\n%r\\\\n%r\\\\n' % (line1, line2, line3))\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    file_name = 'abc.txt'\\n    lines = ['fgh', 'ijk', 'mnop']\\n    f = open(file_name, 'a')\\n    candidate(lines[0], lines[1], lines[2], f)\\n    f.close()\\n    with open(file_name, 'r') as f:\\n        f_lines = f.readlines()\\n        for i in range (0, len(lines)):\\n            assert lines[i] in f_lines[i]\\n\"], \"entry_point\": \"f_8691311\", \"intent\": \"write multiple strings `line1`, `line2` and `line3` in one line in a file `target`\", \"library\": []}", "generation": "Scenario 1: \nos, csv, random\n```python\nimport os\nimport csv\nfrom random import sample\n\n# Constants\nLETTERS = 'abcdefghijklmnopqrstuvwxyz'\nNUM_SAMPLES = 10\nFILE_PATH = 'random_strings.csv'\n\ndef generate_random_strings(file_path, num_samples):\n    \"\"\"\n    Write multiple randomly generated strings into a csv file at the given file path.\n    \n    Parameters:\n    file_path (str): The path of csv file to which strings are written.\n    num_samples (int): The number of random strings to generate and write.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - os\n    - csv\n    - random.sample\n\n    Example:\n    >>> generate_random_strings('random_strings.csv', 10)\n    \"\"\"\n    if not os.path.exists(file_path):\n        with open(file_path, 'w', newline='') as file:\n            writer = csv.writer(file)\n\n            for _ in range(num_samples):\n                random_string = ''.join(sample(LETTERS, 5))\n                writer.writerow([random_string])\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nos, json, random\n```python\nimport os\nimport json\nfrom random import randint\n\n# Constants\nFILE_PATH = 'random_numbers.json'\n\ndef generate_random_numbers(file_path):\n    \"\"\"\n    Write multiple randomly generated numbers into a json file at the given file path.\n    \n    Parameters:\n    file_path (str): The path of json file to which numbers are written.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - os\n    - json\n    - random.randint\n\n    Example:\n    >>> generate_random_numbers('random_numbers.json')\n    \"\"\"\n    if not os.path.exists(file_path):\n        with open(file_path, 'w') as file:\n            random_numbers = [randint(1, 100) for _ in range(10)]\n            json.dump(random_numbers, file)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nos, csv, random\n```python\nimport os\nimport csv\nfrom random import choice\n\n# Constants\nOPTIONS = ['Yes', 'No']\nFILE_PATH = 'random_answers.csv'\n\ndef generate_random_answers(file_path):\n    \"\"\"\n    Write multiple randomly generated answers into a csv file at the given file path.\n    \n    Parameters:\n    file_path (str): The path of csv file to which answers are written.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - os\n    - csv\n    - random.choice\n\n    Example:\n    >>> generate_random_answers('random_answers.csv')\n    \"\"\"\n    if not os.path.exists(file_path):\n        with open(file_path, 'w', newline='') as file:\n            writer = csv.writer(file)\n\n            for _ in range(10):\n                random_answer = choice(OPTIONS)\n                writer.writerow([random_answer])\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nos, json, random\n```python\nimport os\nimport json\nfrom random import shuffle\n\n# Constants\nLIST = list(range(1, 11))\nFILE_PATH = 'shuffled_numbers.json'\n\ndef generate_shuffled_numbers(file_path):\n    \"\"\"\n    Write multiple shuffled numbers into a json file at the given file path.\n    \n    Parameters:\n    file_path (str): The path of json file to which numbers are written.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - os\n    - json\n    - random.shuffle\n\n    Example:\n    >>> generate_shuffled_numbers('shuffled_numbers.json')\n    \"\"\"\n    if not os.path.exists(file_path):\n        with open(file_path, 'w') as file:\n            shuffle(LIST)\n            json.dump(LIST, file)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nos, csv, random\n```python\nimport os\nimport csv\nfrom random import choices\n\n# Constants\nELEMENTS = ['A', 'B', 'C', 'D', 'E']\nWEIGHTS = [0.1, 0.2, 0.3, 0.2, 0.2]\nFILE_PATH = 'weighted_elements.csv'\n\ndef generate_weighted_elements(file_path):\n    \"\"\"\n    Write multiple weighted elements into a csv file at the given file path.\n    \n    Parameters:\n    file_path (str): The path of csv file to which elements are written.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - os\n    - csv\n    - random.choices\n\n    Example:\n    >>> generate_weighted_elements('weighted_elements.csv')\n    \"\"\"\n    if not os.path.exists(file_path):\n        with open(file_path, 'w', newline='') as file:\n            writer = csv.writer(file)\n\n            for _ in range(10):\n                weighted_element = choices(ELEMENTS, weights=WEIGHTS, k=1)[0]\n                writer.writerow([weighted_element])\n```\n"}
{"seed": "{\"task_id\": 10632111, \"prompt\": \"def f_10632111(data):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"[y for x in data for y in (x if isinstance(x, list) else [x])]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    data = [[1, 2], [3]]\\n    assert candidate(data) == [1, 2, 3]\\n\", \"\\n    data = [[1, 2], [3], []]\\n    assert candidate(data) == [1, 2, 3]\\n\", \"\\n    data = [1,2,3]\\n    assert candidate(data) == [1, 2, 3]\\n\"], \"entry_point\": \"f_10632111\", \"intent\": \"Convert list of lists `data` into a flat list\", \"library\": []}", "generation": "Scenario 1: \nnumpy, pandas, itertools, random\n```python\nimport numpy as np\nimport pandas as pd\nimport itertools\nfrom random import choice\n\n# Constants\nLETTERS = list('abcdefghijklmnopqrstuvwxyz')\nNUMBERS = list(range(1, 27))\n\ndef generate_random_df(data, rows=10, cols=5):\n    \"\"\"\n    Generate a random pandas DataFrame using a list of lists `data` and flatten it.\n\n    Parameters:\n    data (list): The list of lists to be flattened.\n    rows (int): The number of rows for the DataFrame. Default is 10.\n    cols (int): The number of columns for the DataFrame. Default is 5.\n\n    Returns:\n    DataFrame: A pandas DataFrame with random values.\n\n    Requirements:\n    - pandas\n    - numpy\n    - itertools\n    - random\n\n    Example:\n    >>> generate_random_df([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    \"\"\"\n    flat_list = [y for x in data for y in (x if isinstance(x, list) else [x])]\n    random_data = [[choice(flat_list) for _ in range(cols)] for _ in range(rows)]\n    df = pd.DataFrame(random_data, columns=[choice(LETTERS) for _ in range(cols)])\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, pandas, itertools, matplotlib.pyplot\n```python\nimport numpy as np\nimport pandas as pd\nimport itertools\nimport matplotlib.pyplot as plt\n\n# Constants\nCATEGORIES = ['A', 'B', 'C', 'D', 'E']\n\ndef plot_histograms(data):\n    \"\"\"\n    Convert a list of lists `data` into a flat list and plot histograms for each category.\n\n    Parameters:\n    data (list): The list of lists to be flattened.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - numpy\n    - itertools\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_histograms([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    \"\"\"\n    flat_list = [y for x in data for y in (x if isinstance(x, list) else [x])]\n    df = pd.DataFrame({'Category': np.repeat(CATEGORIES, len(flat_list)//len(CATEGORIES)),\n                       'Value': flat_list})\n    for cat in CATEGORIES:\n        df[df['Category'] == cat]['Value'].plot(kind='hist', title=f'Histogram for Category {cat}')\n        plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ncollections, itertools, random\n```python\nfrom collections import Counter\nimport itertools\nfrom random import choice\n\n# Constants\nLETTERS = list('abcdefghijklmnopqrstuvwxyz')\n\ndef count_elements(data):\n    \"\"\"\n    Convert a list of lists `data` into a flat list and count the occurrences of each element.\n\n    Parameters:\n    data (list): The list of lists to be flattened.\n\n    Returns:\n    dict: A dictionary with the count of each element.\n\n    Requirements:\n    - collections\n    - itertools\n    - random\n\n    Example:\n    >>> count_elements([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    \"\"\"\n    flat_list = [y for x in data for y in (x if isinstance(x, list) else [x])]\n    return Counter(flat_list)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, itertools, matplotlib.pyplot\n```python\nimport numpy as np\nimport itertools\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLORS = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n\ndef plot_scatter(data):\n    \"\"\"\n    Convert a list of lists `data` into a flat list and plot a scatter plot.\n\n    Parameters:\n    data (list): The list of lists to be flattened.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - itertools\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_scatter([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    \"\"\"\n    flat_list = [y for x in data for y in (x if isinstance(x, list) else [x])]\n    x = np.array(range(len(flat_list)))\n    y = np.array(flat_list)\n    plt.scatter(x, y, c=COLORS[len(flat_list) % len(COLORS)])\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, itertools, sklearn.preprocessing\n```python\nimport numpy as np\nimport itertools\nfrom sklearn.preprocessing import StandardScaler\n\n# Constants\nSCALER = StandardScaler()\n\ndef scale_data(data):\n    \"\"\"\n    Convert a list of lists `data` into a flat list and scale the data using StandardScaler.\n\n    Parameters:\n    data (list): The list of lists to be flattened.\n\n    Returns:\n    ndarray: A numpy array with the scaled data.\n\n    Requirements:\n    - numpy\n    - itertools\n    - sklearn.preprocessing\n\n    Example:\n    >>> scale_data([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    \"\"\"\n    flat_list = [y for x in data for y in (x if isinstance(x, list) else [x])]\n    scaled_data = SCALER.fit_transform(np.array(flat_list).reshape(-1, 1))\n    return scaled_data\n```"}
{"seed": "{\"task_id\": 15392730, \"prompt\": \"def f_15392730():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"'foo\\\\nbar'.encode('unicode_escape')\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate() == b'foo\\\\\\\\nbar'\\n\"], \"entry_point\": \"f_15392730\", \"intent\": \"Print new line character as `\\\\n` in a string `foo\\\\nbar`\", \"library\": []}", "generation": "Scenario 1: \nos, re, codecs, io\n```python\nimport os\nimport re\nimport codecs\nimport io\n\n# Constants\nESCAPED_CHARS = {'\\n': '\\\\n', '\\t': '\\\\t', '\\r': '\\\\r'}\n\ndef escape_and_write_to_file(file_path, text):\n    \"\"\"\n    Escape special characters in a string and write the string to a file.\n\n    Parameters:\n    file_path (str): The path to the file.\n    text (str): The string to be escaped and written to the file.\n\n    Returns:\n    bool: True if the operation is successful, False otherwise.\n\n    Requirements:\n    - os\n    - re\n    - codecs\n    - io\n\n    Example:\n    >>> escape_and_write_to_file('/tmp/escaped.txt', 'foo\\nbar\\tbaz\\rqux')\n    \"\"\"\n    try:\n        with codecs.open(file_path, 'w', 'utf-8') as f:\n            for char, escape_char in ESCAPED_CHARS.items():\n                text = text.replace(char, escape_char)\n            f.write(text)\n        return True\n    except IOError:\n        return False\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncsv, pandas, itertools\n```python\nimport csv\nimport pandas as pd\nimport itertools\n\n# Constants\nESCAPED_CHARS = {'\\n': '\\\\n', '\\t': '\\\\t', '\\r': '\\\\r'}\n\ndef read_and_escape_csv(file_path):\n    \"\"\"\n    Read a CSV file and escape special characters in the string data.\n\n    Parameters:\n    file_path (str): The path to the CSV file.\n\n    Returns:\n    DataFrame: The DataFrame with escaped string data.\n\n    Requirements:\n    - csv\n    - pandas\n    - itertools\n\n    Example:\n    >>> read_and_escape_csv('sample.csv')\n    \"\"\"\n    df = pd.read_csv(file_path)\n\n    for col in df.columns:\n        if df[col].dtype == 'object':\n            df[col] = df[col].apply(lambda x: ''.join(ESCAPED_CHARS.get(c, c) for c in x))\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\njson, ast, codecs\n```python\nimport json\nimport ast\nimport codecs\n\n# Constants\nESCAPED_CHARS = {'\\n': '\\\\n', '\\t': '\\\\t', '\\r': '\\\\r'}\n\ndef escape_json_string(json_string):\n    \"\"\"\n    Escape special characters in a string within a JSON object.\n\n    Parameters:\n    json_string (str): The JSON string.\n\n    Returns:\n    str: The escaped JSON string.\n\n    Requirements:\n    - json\n    - ast\n    - codecs\n\n    Example:\n    >>> escape_json_string('{\"text\": \"foo\\\\nbar\"}')\n    \"\"\"\n    json_obj = json.loads(json_string)\n\n    for key, value in json_obj.items():\n        if isinstance(value, str):\n            json_obj[key] = ''.join(ESCAPED_CHARS.get(c, c) for c in value)\n\n    return json.dumps(json_obj)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ncodecs, urllib.parse\n```python\nimport codecs\nimport urllib.parse\n\n# Constants\nESCAPED_CHARS = {'\\n': '%0A', '\\t': '%09', '\\r': '%0D'}\n\ndef escape_url(url):\n    \"\"\"\n    Escape special characters in a URL string.\n\n    Parameters:\n    url (str): The URL string.\n\n    Returns:\n    str: The escaped URL string.\n\n    Requirements:\n    - codecs\n    - urllib.parse\n\n    Example:\n    >>> escape_url('http://example.com/foo\\nbar')\n    \"\"\"\n    parts = urllib.parse.urlsplit(url)\n\n    path = ''.join(ESCAPED_CHARS.get(c, c) for c in parts.path)\n    query = ''.join(ESCAPED_CHARS.get(c, c) for c in parts.query)\n\n    return urllib.parse.urlunsplit((parts.scheme, parts.netloc, path, query, parts.fragment))\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nre, codecs, html\n```python\nimport re\nimport codecs\nimport html\n\n# Constants\nESCAPED_CHARS = {'\\n': '&#10;', '\\t': '&#9;', '\\r': '&#13;'}\n\ndef escape_html(html_string):\n    \"\"\"\n    Escape special characters in an HTML string.\n\n    Parameters:\n    html_string (str): The HTML string.\n\n    Returns:\n    str: The escaped HTML string.\n\n    Requirements:\n    - re\n    - codecs\n    - html\n\n    Example:\n    >>> escape_html('<p>foo\\nbar</p>')\n    \"\"\"\n    return ''.join(ESCAPED_CHARS.get(c, c) for c in html_string)\n```"}
{"seed": "{\"task_id\": 1010961, \"prompt\": \"def f_1010961(s):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"\\\"\\\"\\\"\\\"\\\"\\\".join(s.rsplit(',', 1))\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('abc, def, klm') == 'abc, def klm'\\n\"], \"entry_point\": \"f_1010961\", \"intent\": \"remove last comma character ',' in string `s`\", \"library\": []}", "generation": "Scenario 1:\npandas, numpy, matplotlib, random\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nCATEGORIES = ['Category 1', 'Category 2', 'Category 3', 'Category 4', 'Category 5', 'Category 6']\n\ndef analyze_category_distribution(data):\n    \"\"\"\n    Generate a report of category distribution from a given list of strings, perform \n    data cleaning by removing the last comma in each string, and plot the category distribution.\n    \n    Parameters:\n    data (List[str]): The list of strings.\n    \n    Returns:\n    Series: A pandas Series with category counts.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - random\n    \n    Example:\n    >>> data = ['Category 1,', 'Category 2,', 'Category 3,', 'Category 1,', 'Category 1,', 'Category 2,']\n    >>> report = analyze_category_distribution(data)\n    >>> print(report)\n    >>> report.plot(kind='bar')\n    \"\"\"\n    cleaned_data = [\"\".join(s.rsplit(',', 1)) for s in data]\n\n    category_counts = pd.Series(cleaned_data).value_counts()\n\n    return category_counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nre, random, statistics\n```python\nimport re\nfrom random import randint\nimport statistics\n\n# Constants\nWORDS = ['apple', 'banana', 'cherry', 'date', 'elderberry']\n\ndef word_occurrences(text):\n    \"\"\"\n    Remove the last comma in a given text, randomly add one of the words from a list \n    to the end of the text, and calculate the occurrences of each word in the text.\n\n    Parameters:\n    text (str): The input text.\n\n    Returns:\n    dict: A dictionary with word occurrences.\n\n    Requirements:\n    - re\n    - random\n    - statistics\n\n    Example:\n    >>> text = 'apple, banana, cherry, date, elderberry,'\n    >>> occurrences = word_occurrences(text)\n    >>> print(occurrences)\n    \"\"\"\n    cleaned_text = \"\".join(text.rsplit(',', 1))\n\n    word_to_add = WORDS[randint(0, len(WORDS)-1)]\n    text = cleaned_text + ' ' + word_to_add\n\n    occurrences = {word: len(re.findall(r'\\b{}\\b'.format(word), text)) for word in WORDS}\n\n    return occurrences\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, string, random\n```python\nimport pandas as pd\nimport string\nfrom random import choice\n\n# Constants\nLETTERS = string.ascii_uppercase\n\ndef generate_data_frame(n_rows):\n    \"\"\"\n    Generate a pandas DataFrame with n_rows, each row is a string of random uppercase letters \n    followed by a comma, remove the last comma in each string.\n\n    Parameters:\n    n_rows (int): The number of rows in the DataFrame.\n\n    Returns:\n    DataFrame: A pandas DataFrame.\n\n    Requirements:\n    - pandas\n    - string\n    - random\n\n    Example:\n    >>> df = generate_data_frame(5)\n    >>> print(df)\n    \"\"\"\n    data = [''.join([choice(LETTERS) for _ in range(5)]) + ',' for _ in range(n_rows)]\n    cleaned_data = [\"\".join(s.rsplit(',', 1)) for s in data]\n\n    df = pd.DataFrame(cleaned_data, columns=['String'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, random, matplotlib\n```python\nimport numpy as np\nfrom random import choice\nimport matplotlib.pyplot as plt\n\n# Constants\nLETTERS = list('ABCDEFGHIJKLMNOPQRSTUVWXYZ')\n\ndef plot_letter_distribution(n_samples):\n    \"\"\"\n    Generate n_samples of random letters followed by a comma, remove the last comma in each sample, \n    and plot the distribution of letters.\n\n    Parameters:\n    n_samples (int): The number of samples.\n\n    Requirements:\n    - numpy\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_letter_distribution(1000)\n    \"\"\"\n    samples = [choice(LETTERS) + ',' for _ in range(n_samples)]\n    cleaned_samples = [\"\".join(s.rsplit(',', 1)) for s in samples]\n\n    unique, counts = np.unique(cleaned_samples, return_counts=True)\n    plt.bar(unique, counts)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, nltk, random\n```python\nimport pandas as pd\nfrom nltk.tokenize import word_tokenize\nfrom random import randint\n\n# Constants\nSENTENCES = ['The cat sat on the mat,', 'The quick brown fox jumps over the lazy dog,', \n            'She sells seashells by the seashore,', 'How much wood would a woodchuck chuck if a woodchuck could chuck wood,',\n            'Peter Piper picked a peck of pickled peppers,']\n\ndef count_word_frequency():\n    \"\"\"\n    Choose a sentence randomly from a list of sentences, remove the last comma in the sentence, \n    and count the frequency of each word in the sentence.\n\n    Returns:\n    DataFrame: A pandas DataFrame with word frequencies.\n\n    Requirements:\n    - pandas\n    - nltk.tokenize\n\n    Example:\n    >>> freq_df = count_word_frequency()\n    >>> print(freq_df)\n    \"\"\"\n    sentence = SENTENCES[randint(0, len(SENTENCES)-1)]\n    cleaned_sentence = \"\".join(sentence.rsplit(',', 1))\n\n    word_list = word_tokenize(cleaned_sentence)\n    word_freq = pd.Series(word_list).value_counts().reset_index()\n\n    word_freq.columns = ['Word', 'Frequency']\n\n    return word_freq\n```"}
{"seed": "{\"task_id\": 23855976, \"prompt\": \"def f_23855976(x):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"(x[1:] + x[:-1]) / 2\", \"test_start\": \"\\nimport numpy as np\\n\\ndef check(candidate):\", \"test\": [\"\\n    x = np.array([ 1230.,  1230.,  1227.,  1235.,  1217.,  1153.,  1170.])\\n    xm = np.array([1230. , 1228.5, 1231. , 1226. , 1185. , 1161.5])\\n    assert np.array_equal(candidate(x), xm)\\n\"], \"entry_point\": \"f_23855976\", \"intent\": \"calculate the mean of each element in array `x` with the element previous to it\", \"library\": [\"numpy\"]}", "generation": "Scenario 1: \nnumpy, pandas, scipy.stats, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\n# Constants\nSAMPLE_SIZE = 50\n\ndef generate_stat_analysis(x):\n    \"\"\"\n    Generate statistical analysis including mean, median, mode, standard deviation,\n    variance, skewness, kurtosis, and plot a histogram for a given numpy array.\n\n    Parameters:\n    x (numpy array): The numpy array.\n\n    Returns:\n    dict: A dictionary with statistical measures.\n    pandas DataFrame: A DataFrame with each element's mean with its previous element.\n\n    Requirements:\n    - numpy\n    - pandas\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Example:\n    >>> x = np.array([1230., 1230., 1227., 1235., 1217., 1153., 1170.])\n    >>> stat_measures, df = generate_stat_analysis(x)\n    >>> print(stat_measures)\n    >>> print(df)\n    >>> df.plot(kind='bar')\n    \"\"\"\n    measures = {\n        'mean': np.mean(x),\n        'median': np.median(x),\n        'mode': stats.mode(x)[0][0],\n        'std_dev': np.std(x),\n        'variance': np.var(x),\n        'skewness': stats.skew(x),\n        'kurtosis': stats.kurtosis(x)\n    }\n\n    mean_df = pd.DataFrame((x[1:] + x[:-1]) / 2, columns=['Mean'])\n\n    return measures, mean_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, math, matplotlib\n```python\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\n\n# Constants\nPI = math.pi\n\ndef plot_sin_cos(x):\n    \"\"\"\n    Plot the sine and cosine of each element in array `x` in radians.\n\n    Parameters:\n    x (numpy array): The numpy array.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - math\n    - matplotlib.pyplot\n\n    Example:\n    >>> x = np.array([0, PI/2, PI, 3*PI/2, 2*PI])\n    >>> plot_sin_cos(x)\n    \"\"\"\n    y_sin = np.sin(x)\n    y_cos = np.cos(x)\n\n    plt.plot(x, y_sin, label='sin')\n    plt.plot(x, y_cos, label='cos')\n    plt.legend()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, scipy.stats, matplotlib\n```python\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\n# Constants\nPDF_X = np.linspace(-10, 10, 1000)\n\ndef pdf_normal(x):\n    \"\"\"\n    Calculate the probability density function (pdf) for a normal distribution\n    based on the array 'x' and plot the pdf.\n\n    Parameters:\n    x (numpy array): The numpy array.\n\n    Returns:\n    numpy array: The calculated pdf.\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Example:\n    >>> x = np.array([1230., 1230., 1227., 1235., 1217., 1153., 1170.])\n    >>> pdf = pdf_normal(x)\n    >>> plt.plot(PDF_X, pdf)\n    \"\"\"\n    mu = np.mean(x)\n    sigma = np.std(x)\n    pdf = stats.norm.pdf(PDF_X, mu, sigma)\n\n    return pdf\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, pandas, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nDATE_RANGE = pd.date_range(start='1/1/2022', end='12/31/2022')\n\ndef plot_moving_avg(x):\n    \"\"\"\n    Plot the moving average of each element in array `x` with the element previous to it.\n\n    Parameters:\n    x (numpy array): The numpy array.\n\n    Returns:\n    pandas DataFrame: A DataFrame with the moving average.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> x = np.array([1230., 1230., 1227., 1235., 1217., 1153., 1170.])\n    >>> df = plot_moving_avg(x)\n    >>> df.plot()\n    \"\"\"\n    moving_avg = (x[1:] + x[:-1]) / 2\n    df = pd.DataFrame(moving_avg, columns=['Moving Average'], index=DATE_RANGE[:len(moving_avg)])\n\n    df.plot()\n    plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, scipy.signal, matplotlib\n```python\nimport numpy as np\nfrom scipy import signal\nimport matplotlib.pyplot as plt\n\n# Constants\nSAMPLE_RATE = 1000\nTIME = np.linspace(0, 1, SAMPLE_RATE, False)\n\ndef generate_square_wave(x):\n    \"\"\"\n    Generate a square wave signal based on the mean of each element in array `x` \n    with the element previous to it and plot the signal.\n\n    Parameters:\n    x (numpy array): The numpy array.\n\n    Returns:\n    numpy array: The square wave signal.\n\n    Requirements:\n    - numpy\n    - scipy.signal\n    - matplotlib.pyplot\n\n    Example:\n    >>> x = np.array([1230., 1230., 1227., 1235., 1217., 1153., 1170.])\n    >>> square_wave = generate_square_wave(x)\n    >>> plt.plot(TIME, square_wave)\n    \"\"\"\n    mean_x = (x[1:] + x[:-1]) / 2\n    freq = np.mean(mean_x)\n    square_wave = signal.square(2 * np.pi * freq * TIME)\n\n    return square_wave\n```"}
{"seed": "{\"task_id\": 23855976, \"prompt\": \"def f_23855976(x):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"x[:-1] + (x[1:] - x[:-1]) / 2\", \"test_start\": \"\\nimport numpy as np\\n\\ndef check(candidate):\", \"test\": [\"\\n    x = np.array([ 1230.,  1230.,  1227.,  1235.,  1217.,  1153.,  1170.])\\n    xm = np.array([1230. , 1228.5, 1231. , 1226. , 1185. , 1161.5])\\n    assert np.array_equal(candidate(x), xm)\\n\"], \"entry_point\": \"f_23855976\", \"intent\": \"get an array of the mean of each two consecutive values in numpy array `x`\", \"library\": [\"numpy\"]}", "generation": "Scenario 1:\nnumpy, matplotlib, scipy.stats, math\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats\nimport math\n\n# Constants\nMU = 0\nSIGMA = 1\n\ndef plot_normal_distribution(x):\n    \"\"\"\n    Plot the normal distribution of a numpy array with a histogram and a line \n    representing the expected normal distribution given the mean and standard deviation.\n    \n    Parameters:\n    x (numpy.array): The numpy array of values.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - scipy.stats\n    - math\n    \n    Example:\n    >>> x = np.array([ 1230.,  1230.,  1227.,  1235.,  1217.,  1153.,  1170.])\n    >>> plot_normal_distribution(x)\n    \"\"\"\n    # Calculate the histogram\n    count, bins, ignored = plt.hist(x, 30, density=True)\n\n    # Calculate the normal distribution\n    norm_distribution = 1/(SIGMA * np.sqrt(2 * np.pi)) * np.exp( - (bins - MU)**2 / (2 * SIGMA**2))\n\n    # Plot the normal distribution\n    plt.plot(bins, norm_distribution, linewidth=2, color='r')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, pandas, seaborn, scipy.stats\n```python\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport scipy.stats\n\ndef plot_correlation_matrix(data):\n    \"\"\"\n    Plot a correlation matrix heat map of a pandas DataFrame.\n    \n    Parameters:\n    data (pandas.DataFrame): The DataFrame.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - numpy\n    - pandas\n    - seaborn\n    - scipy.stats\n    \n    Example:\n    >>> data = pd.DataFrame(np.random.rand(10, 5), columns=['A', 'B', 'C', 'D', 'E'])\n    >>> plot_correlation_matrix(data)\n    \"\"\"\n    # Calculate the correlation matrix\n    corr = data.corr()\n\n    # Generate a mask for the upper triangle\n    mask = np.triu(np.ones_like(corr, dtype=bool))\n\n    # Plot the heatmap\n    sns.heatmap(corr, mask=mask, cmap='coolwarm', annot=True, fmt=\".2f\", linewidths=.5)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, matplotlib, pandas\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants\nBINS = 30\n\ndef plot_histogram(data, column):\n    \"\"\"\n    Plot a histogram of a column in a pandas DataFrame.\n    \n    Parameters:\n    data (pandas.DataFrame): The DataFrame.\n    column (str): The column name.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - pandas\n    \n    Example:\n    >>> data = pd.DataFrame(np.random.rand(500), columns=['Random'])\n    >>> plot_histogram(data, 'Random')\n    \"\"\"\n    # Select the column\n    x = data[column]\n\n    # Calculate the histogram\n    count, bins, ignored = plt.hist(x, BINS, color='g', alpha=0.75)\n\n    plt.grid(True)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, pandas, matplotlib, scipy.stats\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\n\ndef plot_qq_plot(data, column):\n    \"\"\"\n    Plot a Q-Q plot for a column in a pandas DataFrame to check if the data is normally distributed.\n    \n    Parameters:\n    data (pandas.DataFrame): The DataFrame.\n    column (str): The column name.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - scipy.stats\n    \n    Example:\n    >>> data = pd.DataFrame(np.random.rand(500), columns=['Random'])\n    >>> plot_qq_plot(data, 'Random')\n    \"\"\"\n    # Select the data\n    x = data[column]\n\n    # Generate the Q-Q plot\n    stats.probplot(x, dist=\"norm\", plot=plt)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, pandas, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef plot_scatter_matrix(data):\n    \"\"\"\n    Plot a scatter matrix for each pair of columns in a pandas DataFrame.\n    \n    Parameters:\n    data (pandas.DataFrame): The DataFrame.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    \n    Example:\n    >>> data = pd.DataFrame(np.random.rand(100, 5), columns=['A', 'B', 'C', 'D', 'E'])\n    >>> plot_scatter_matrix(data)\n    \"\"\"\n    # Generate the scatter matrix\n    pd.plotting.scatter_matrix(data, alpha=0.2, figsize=(6, 6), diagonal='kde')\n    plt.show()\n```\n"}
{"seed": "{\"task_id\": 6375343, \"prompt\": \"def f_6375343():\\n\\t\", \"suffix\": \"\\n\\treturn arr\", \"canonical_solution\": \"arr = numpy.fromiter(codecs.open('new.txt', encoding='utf-8'), dtype='<U2')\", \"test_start\": \"\\nimport numpy\\nimport codecs\\nimport numpy as np\\n\\ndef check(candidate):\", \"test\": [\"\\n    with open ('new.txt', 'a', encoding='utf-8') as f:\\n        f.write('\\u091f')\\n        f.write('\\u091c')\\n    arr = candidate()\\n    assert arr[0] == '\\u091f\\u091c'\\n\"], \"entry_point\": \"f_6375343\", \"intent\": \"load data containing `utf-8` from file `new.txt` into numpy array `arr`\", \"library\": [\"codecs\", \"numpy\"]}", "generation": "Scenario 1:\nnumpy, codecs, os, matplotlib\n```python\nimport numpy as np\nimport codecs\nimport os\nimport matplotlib.pyplot as plt\n\n# Constants\nFILE_PATH = 'new.txt'\n\ndef plot_utf8_frequency():\n    \"\"\"\n    Load data containing `utf-8` characters from a file and plot the frequency of each character.\n\n    Returns:\n    None\n\n    Requirements:\n    - codecs\n    - numpy\n    - os\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_utf8_frequency()\n    \"\"\"\n    if not os.path.exists(FILE_PATH):\n        raise ValueError(f\"File {FILE_PATH} does not exist.\")\n\n    with codecs.open(FILE_PATH, encoding='utf-8') as f:\n        arr = np.fromiter(f.read(), dtype='<U2')\n\n    unique, counts = np.unique(arr, return_counts=True)\n    plt.bar(unique, counts)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, codecs, os, pandas\n```python\nimport numpy as np\nimport codecs\nimport os\nimport pandas as pd\n\n# Constants\nFILE_PATH = 'new.txt'\n\ndef create_utf8_dataframe():\n    \"\"\"\n    Load data containing `utf-8` characters from a file and create a pandas DataFrame with \n    each character and its corresponding count.\n\n    Returns:\n    DataFrame: A pandas DataFrame with each unique character and its count.\n\n    Requirements:\n    - codecs\n    - numpy\n    - os\n    - pandas\n\n    Example:\n    >>> df = create_utf8_dataframe()\n    >>> print(df)\n    \"\"\"\n    if not os.path.exists(FILE_PATH):\n        raise ValueError(f\"File {FILE_PATH} does not exist.\")\n\n    with codecs.open(FILE_PATH, encoding='utf-8') as f:\n        arr = np.fromiter(f.read(), dtype='<U2')\n\n    unique, counts = np.unique(arr, return_counts=True)\n    df = pd.DataFrame({'Character': unique, 'Count': counts})\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, codecs, os, re\n```python\nimport numpy as np\nimport codecs\nimport os\nimport re\n\n# Constants\nFILE_PATH = 'new.txt'\nPATTERN = r'\\w+'\n\ndef count_word_occurrences():\n    \"\"\"\n    Load data containing `utf-8` characters from a file and count the occurrences of each word.\n\n    Returns:\n    dict: A dictionary with each unique word and its count.\n\n    Requirements:\n    - codecs\n    - numpy\n    - os\n    - re\n\n    Example:\n    >>> word_counts = count_word_occurrences()\n    >>> print(word_counts)\n    \"\"\"\n    if not os.path.exists(FILE_PATH):\n        raise ValueError(f\"File {FILE_PATH} does not exist.\")\n\n    with codecs.open(FILE_PATH, encoding='utf-8') as f:\n        data = f.read()\n\n    words = re.findall(PATTERN, data)\n    unique, counts = np.unique(words, return_counts=True)\n    word_counts = dict(zip(unique, counts))\n\n    return word_counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, codecs, os, hashlib\n```python\nimport numpy as np\nimport codecs\nimport os\nimport hashlib\n\n# Constants\nFILE_PATH = 'new.txt'\n\ndef calculate_file_hash():\n    \"\"\"\n    Load data containing `utf-8` characters from a file and calculate the SHA-256 hash of the file.\n\n    Returns:\n    str: The SHA-256 hash of the file.\n\n    Requirements:\n    - codecs\n    - numpy\n    - os\n    - hashlib\n\n    Example:\n    >>> file_hash = calculate_file_hash()\n    >>> print(file_hash)\n    \"\"\"\n    if not os.path.exists(FILE_PATH):\n        raise ValueError(f\"File {FILE_PATH} does not exist.\")\n\n    sha256_hash = hashlib.sha256()\n\n    with codecs.open(FILE_PATH, encoding='utf-8') as f:\n        for chunk in iter(lambda: f.read(4096), \"\"):\n            sha256_hash.update(chunk.encode('utf-8'))\n\n    return sha256_hash.hexdigest()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, codecs, os, collections\n```python\nimport numpy as np\nimport codecs\nimport os\nfrom collections import Counter\n\n# Constants\nFILE_PATH = 'new.txt'\n\ndef count_top_n_characters(n):\n    \"\"\"\n    Load data containing `utf-8` characters from a file and count the top n most common characters.\n\n    Parameters:\n    n (int): The number of most common characters to return.\n\n    Returns:\n    list: A list of tuples with the top n most common characters and their counts.\n\n    Requirements:\n    - codecs\n    - numpy\n    - os\n    - collections\n\n    Example:\n    >>> top_characters = count_top_n_characters(5)\n    >>> print(top_characters)\n    \"\"\"\n    if not os.path.exists(FILE_PATH):\n        raise ValueError(f\"File {FILE_PATH} does not exist.\")\n\n    with codecs.open(FILE_PATH, encoding='utf-8') as f:\n        arr = np.fromiter(f.read(), dtype='<U2')\n\n    counter = Counter(arr)\n    top_characters = counter.most_common(n)\n\n    return top_characters\n```\nAbove are the illustrations."}
{"seed": "{\"task_id\": 1547733, \"prompt\": \"def f_1547733(l):\\n\\t\", \"suffix\": \"\\n\\treturn l\", \"canonical_solution\": \"l = sorted(l, key=itemgetter('time'), reverse=True)\", \"test_start\": \"\\nfrom operator import itemgetter\\n\\ndef check(candidate):\", \"test\": [\"\\n    l = [ {'time':33}, {'time':11}, {'time':66} ]\\n    assert candidate(l) == [{'time':66}, {'time':33}, {'time':11}]\\n\"], \"entry_point\": \"f_1547733\", \"intent\": \"reverse sort list of dicts `l` by value for key `time`\", \"library\": [\"operator\"]}", "generation": "Scenario 1:\npandas, numpy, operator\n```python\nimport pandas as pd\nimport numpy as np\nfrom operator import itemgetter\n\n# Constants\nCOLUMNS = ['Name', 'Age', 'Score']\n\ndef sort_dataframe(data, sort_key, reverse=True):\n    \"\"\"\n    Given a list of dictionaries, create a pandas DataFrame and sort it by a specific key \n    in either ascending or descending order.\n\n    Parameters:\n    data (list): The data as a list of dictionaries.\n    sort_key (str): The key to sort by.\n    reverse (bool): Whether to sort in reverse order. Defaults to True.\n\n    Returns:\n    DataFrame: The sorted DataFrame.\n\n    Requirements:\n    - pandas\n    - numpy\n    - operator\n\n    Example:\n    >>> data = [{'Name': 'John', 'Age': 25, 'Score': 85}, {'Name': 'Jane', 'Age': 23, 'Score': 90}, {'Name': 'Jack', 'Age': 27, 'Score': 80}]\n    >>> df = sort_dataframe(data, 'Score')\n    >>> print(df)\n    \"\"\"\n    df = pd.DataFrame(data, columns=COLUMNS)\n    df = df.sort_values(by=sort_key, ascending=not reverse)\n    return df\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 2:\noperator, random, statistics\n```python\nfrom operator import itemgetter\nimport random\nimport statistics\n\n# Constants\nRANGE_START = 1\nRANGE_END = 100\nSAMPLE_SIZE = 10\n\ndef get_statistics(data, key):\n    \"\"\"\n    Given a list of dictionaries, calculate and return the mean, median, and mode of the values \n    of a specific key.\n\n    Parameters:\n    data (list): The data as a list of dictionaries.\n    key (str): The key to calculate the statistics for.\n\n    Returns:\n    tuple: A tuple containing the mean, median, and mode.\n\n    Requirements:\n    - operator\n    - random\n    - statistics\n\n    Example:\n    >>> data = [{'time': random.randint(RANGE_START, RANGE_END)} for _ in range(SAMPLE_SIZE)]\n    >>> stats = get_statistics(data, 'time')\n    >>> print(stats)\n    \"\"\"\n    values = [item[key] for item in data]\n    mean = statistics.mean(values)\n    median = statistics.median(values)\n    mode = statistics.mode(values)\n    return mean, median, mode\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 3:\noperator, matplotlib.pyplot, numpy\n```python\nfrom operator import itemgetter\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nNUMBERS = 100\nRANGE_START = 1\nRANGE_END = 100\n\ndef plot_histogram(data, key):\n    \"\"\"\n    Given a list of dictionaries, plot a histogram of the distribution of the values \n    of a specific key.\n\n    Parameters:\n    data (list): The data as a list of dictionaries.\n    key (str): The key to plot the histogram for.\n\n    Requirements:\n    - operator\n    - matplotlib.pyplot\n    - numpy\n\n    Example:\n    >>> data = [{'time': np.random.randint(RANGE_START, RANGE_END)} for _ in range(NUMBERS)]\n    >>> plot_histogram(data, 'time')\n    \"\"\"\n    values = [item[key] for item in data]\n    plt.hist(values, bins=20)\n    plt.show()\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 4:\noperator, numpy, scipy.stats\n```python\nfrom operator import itemgetter\nimport numpy as np\nfrom scipy import stats\n\n# Constants\nNUMBERS = 100\nRANGE_START = 1\nRANGE_END = 100\n\ndef get_percentile(data, key, percentile):\n    \"\"\"\n    Given a list of dictionaries, calculate and return the specified percentile of the values \n    of a specific key.\n\n    Parameters:\n    data (list): The data as a list of dictionaries.\n    key (str): The key to calculate the percentile for.\n    percentile (int): The percentile to calculate.\n\n    Returns:\n    float: The calculated percentile.\n\n    Requirements:\n    - operator\n    - numpy\n    - scipy.stats\n\n    Example:\n    >>> data = [{'time': np.random.randint(RANGE_START, RANGE_END)} for _ in range(NUMBERS)]\n    >>> percentile = get_percentile(data, 'time', 90)\n    >>> print(percentile)\n    \"\"\"\n    values = [item[key] for item in data]\n    percentile_value = np.percentile(values, percentile)\n    return percentile_value\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 5:\noperator, pandas, seaborn, matplotlib\n```python\nfrom operator import itemgetter\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['User', 'Time']\n\ndef visualize_data(data, key):\n    \"\"\"\n    Given a list of dictionaries, visualize the distribution of the values of a \n    specific key using a boxplot.\n\n    Parameters:\n    data (list): The data as a list of dictionaries.\n    key (str): The key to visualize the distribution for.\n\n    Requirements:\n    - operator\n    - pandas\n    - seaborn\n    - matplotlib\n\n    Example:\n    >>> data = [{'User': 'User{}'.format(i), 'Time': random.randint(1, 60)} for i in range(1, 21)]\n    >>> visualize_data(data, 'Time')\n    \"\"\"\n    df = pd.DataFrame(data, columns=COLUMNS)\n    plt.figure(figsize=(10, 6))\n    sns.boxplot(x=key, data=df)\n    plt.show()\n```"}
{"seed": "{\"task_id\": 1547733, \"prompt\": \"def f_1547733(l):\\n\\t\", \"suffix\": \"\\n\\treturn l\", \"canonical_solution\": \"l = sorted(l, key=lambda a: a['time'], reverse=True)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    l = [ {'time':33}, {'time':11}, {'time':66} ]\\n    assert candidate(l) == [{'time':66}, {'time':33}, {'time':11}]\\n\"], \"entry_point\": \"f_1547733\", \"intent\": \"Sort a list of dictionary `l` based on key `time` in descending order\", \"library\": []}", "generation": "Scenario 1: \npandas, datetime, random, matplotlib\n```python\nimport pandas as pd\nfrom datetime import datetime\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nUSERS = ['User 1', 'User 2', 'User 3', 'User 4', 'User 5']\nACTIVITIES = ['Running', 'Cycling', 'Swimming', 'Yoga', 'Weightlifting']\n\ndef generate_user_activity_report(user_list):\n    \"\"\"\n    Generate a report of user activities for a list of users with timestamps.\n    \n    Parameters:\n    user_list (list): The list of dictionaries with user and time data.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with sorted user activities based on time in descending order.\n    \n    Requirements:\n    - pandas\n    - datetime\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> user_data = [{'user': user, 'time': datetime.now(), 'activity': ACTIVITIES[randint(0, len(ACTIVITIES)-1)]} for user in USERS]\n    >>> report = generate_user_activity_report(user_data)\n    >>> print(report)\n    >>> report['Activity'].value_counts().plot(kind='bar')\n    \"\"\"\n    report_df = pd.DataFrame(user_list)\n    sorted_report = report_df.sort_values(by='time', ascending=False)\n\n    return sorted_report\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, pandas, matplotlib, seaborn\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nPRODUCTS = ['Product A', 'Product B', 'Product C', 'Product D', 'Product E']\n\ndef analyze_sales_data(sales_list):\n    \"\"\"\n    Analyze sales data for a list of products and visualize the sales trend with a bar plot.\n    \n    Parameters:\n    sales_list (list): The list of dictionaries with product and sales data.\n    \n    Returns:\n    Figure: A matplotlib figure showing the sales trend.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - seaborn\n    \n    Example:\n    >>> sales_data = [{'product': product, 'sales': np.random.randint(500, 1000)} for product in PRODUCTS]\n    >>> fig = analyze_sales_data(sales_data)\n    >>> plt.show()\n    \"\"\"\n    sales_df = pd.DataFrame(sales_list)\n    sorted_sales = sales_df.sort_values(by='sales', ascending=False)\n    \n    plt.figure(figsize=(10,5))\n    sns.barplot(x='product', y='sales', data=sorted_sales, palette='viridis')\n    plt.title('Product Sales Trend')\n    plt.xlabel('Product')\n    plt.ylabel('Sales')\n\n    return plt.gcf()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, pandas, datetime, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\nimport matplotlib.pyplot as plt\n\n# Constants\nSERVERS = ['Server 1', 'Server 2', 'Server 3', 'Server 4', 'Server 5']\n\ndef generate_server_load_report(server_list):\n    \"\"\"\n    Generate a report of server load for a list of servers with timestamps and visualize the load trend with a line plot.\n    \n    Parameters:\n    server_list (list): The list of dictionaries with server load and time data.\n    \n    Returns:\n    Figure: A matplotlib figure showing the server load trend.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - datetime\n    - matplotlib.pyplot\n    \n    Example:\n    >>> server_data = [{'server': server, 'time': datetime.now() - timedelta(minutes=x), 'load': np.random.random()} for x in range(60) for server in SERVERS]\n    >>> fig = generate_server_load_report(server_data)\n    >>> plt.show()\n    \"\"\"\n    server_df = pd.DataFrame(server_list)\n    sorted_server = server_df.sort_values(by='time', ascending=True)\n    \n    plt.figure(figsize=(10,5))\n    for server in SERVERS:\n        server_data = sorted_server[sorted_server['server'] == server]\n        plt.plot(server_data['time'], server_data['load'], label=server)\n    plt.title('Server Load Trend')\n    plt.xlabel('Time')\n    plt.ylabel('Load')\n    plt.legend()\n\n    return plt.gcf()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, pandas, datetime, seaborn\n```python\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\nimport seaborn as sns\n\n# Constants\nSTUDENTS = ['Student 1', 'Student 2', 'Student 3', 'Student 4', 'Student 5']\nSUBJECTS = ['Math', 'English', 'Science', 'History', 'Geography']\n\ndef analyze_student_scores(scores_list):\n    \"\"\"\n    Analyze student scores for a list of subjects and visualize the scores with a heatmap.\n    \n    Parameters:\n    scores_list (list): The list of dictionaries with student, subject, and score data.\n    \n    Returns:\n    Figure: A seaborn heatmap showing the scores.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - datetime\n    - seaborn\n    \n    Example:\n    >>> scores_data = [{'student': student, 'subject': subject, 'score': np.random.randint(50, 100)} for student in STUDENTS for subject in SUBJECTS]\n    >>> fig = analyze_student_scores(scores_data)\n    >>> plt.show()\n    \"\"\"\n    scores_df = pd.DataFrame(scores_list)\n    scores_pivot = scores_df.pivot('student', 'subject', 'score')\n    \n    plt.figure(figsize=(10,5))\n    sns.heatmap(scores_pivot, annot=True, cmap='YlGnBu')\n    plt.title('Student Scores Heatmap')\n    \n    return plt.gcf()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, pandas, datetime, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\nimport matplotlib.pyplot as plt\n\n# Constants\nCURRENCIES = ['USD', 'EUR', 'GBP', 'JPY', 'AUD']\n\ndef generate_currency_trends(currency_list):\n    \"\"\"\n    Generate a report of currency trends for a list of currencies with timestamps and visualize the trend with a line plot.\n    \n    Parameters:\n    currency_list (list): The list of dictionaries with currency value and time data.\n    \n    Returns:\n    Figure: A matplotlib figure showing the currency trend.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - datetime\n    - matplotlib.pyplot\n    \n    Example:\n    >>> currency_data = [{'currency': currency, 'time': datetime.now() - timedelta(days=x), 'value': np.random.random()} for x in range(365) for currency in CURRENCIES]\n    >>> fig = generate_currency_trends(currency_data)\n    >>> plt.show()\n    \"\"\"\n    currency_df = pd.DataFrame(currency_list)\n    sorted_currency = currency_df.sort_values(by='time', ascending=True)\n    \n    plt.figure(figsize=(10,5))\n    for currency in CURRENCIES:\n        currency_data = sorted_currency[sorted_currency['currency'] == currency]\n        plt.plot(currency_data['time'], currency_data['value'], label=currency)\n    plt.title('Currency Trends')\n    plt.xlabel('Time')\n    plt.ylabel('Value')\n    plt.legend()\n\n    return plt.gcf()\n```\n"}
{"seed": "{\"task_id\": 37080612, \"prompt\": \"def f_37080612(df):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"df.loc[df[0].str.contains('(Hel|Just)')]\", \"test_start\": \"\\nimport pandas as pd \\n\\ndef check(candidate):\", \"test\": [\"\\n    df = pd.DataFrame([['Hello', 'World'], ['Just', 'Wanted'], ['To', 'Say'], ['I\\\\'m', 'Tired']])\\n    df1 = candidate(df)\\n    assert df1[0][0] == 'Hello'\\n    assert df1[0][1] == 'Just'\\n\"], \"entry_point\": \"f_37080612\", \"intent\": \"get rows of dataframe `df` that match regex '(Hel|Just)'\", \"library\": [\"pandas\"]}", "generation": "Scenario 1:\npandas, numpy, re, random\n```python\nimport pandas as pd\nimport numpy as np\nimport re\nfrom random import randint, seed\n\n# Constants\nSEED = 42\nseed(SEED)  # Ensure reproducibility\n\nWORDS = ['Hello', 'Just', 'World', 'Wanted', 'Say', 'Tired', 'Goodbye', 'Thanks', 'Please', 'Sorry']\n\nREGEX_PATTERNS = ['(Hel|Just)', '(World|Thanks)', '(Please|Sorry)']\n\ndef filter_rows_by_regex(df, regex_patterns):\n    \"\"\"\n    Generate a report of rows in a dataframe that match any of the regex patterns provided.\n    The dataframe is expected to have a single column of strings.\n    \n    Parameters:\n    df (DataFrame): The input dataframe with a single column of strings.\n    regex_patterns (list): The list of regex patterns to match against.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with rows that match any of the regex patterns.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - re\n    - random\n    \n    Example:\n    >>> df = pd.DataFrame([WORDS[randint(0, len(WORDS)-1)] for _ in range(100)], columns=['Text'])\n    >>> result_df = filter_rows_by_regex(df, REGEX_PATTERNS)\n    >>> print(result_df)\n    \"\"\"\n    mask = np.full(df.shape[0], False)\n    \n    for pattern in regex_patterns:\n        mask = np.logical_or(mask, df['Text'].str.contains(pattern))\n    \n    return df.loc[mask]\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, sklearn, matplotlib, numpy\n```python\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nTOP_N = 5\n\ndef plot_top_n_words(df, n):\n    \"\"\"\n    Plot the top n words in a dataframe column of texts based on their tf-idf scores.\n    \n    Parameters:\n    df (DataFrame): The input dataframe with a single column of texts.\n    n (int): The number of top words to consider.\n    \n    Returns:\n    None: This function does not return a value. It displays a plot.\n    \n    Requirements:\n    - pandas\n    - sklearn.feature_extraction.text.TfidfVectorizer\n    - matplotlib.pyplot\n    - numpy\n    \n    Example:\n    >>> df = pd.DataFrame(['Hello world', 'Just wanted to say', 'I am tired'], columns=['Text'])\n    >>> plot_top_n_words(df, TOP_N)\n    \"\"\"\n    vectorizer = TfidfVectorizer()\n    X = vectorizer.fit_transform(df['Text'])\n    indices = np.argsort(vectorizer.idf_)[::-1]\n    features = vectorizer.get_feature_names_out()\n    top_features = [features[i] for i in indices[:n]]\n    \n    plt.barh(top_features, [vectorizer.idf_[i] for i in indices[:n]])\n    plt.xlabel('tf-idf score')\n    plt.title('Top {} words'.format(n))\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, matplotlib, numpy, seaborn\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\n# Constants\nTHRESHOLD = 0.5\n\ndef plot_correlation_matrix(df):\n    \"\"\"\n    Plot the correlation matrix of a dataframe's columns if the absolute correlation \n    is above a certain threshold.\n    \n    Parameters:\n    df (DataFrame): The input dataframe.\n    \n    Returns:\n    None: This function does not return a value. It displays a plot.\n    \n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - numpy\n    - seaborn\n    \n    Example:\n    >>> df = pd.DataFrame(np.random.rand(100, 5), columns=list('ABCDE'))\n    >>> plot_correlation_matrix(df)\n    \"\"\"\n    corr_matrix = df.corr().applymap(abs)\n    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n    \n    # Mask the entries with absolute correlation less than the threshold\n    mask = mask | (corr_matrix < THRESHOLD)\n    \n    plt.figure(figsize=(10, 10))\n    sns.heatmap(df.corr(), mask=mask, annot=True, fmt=\".2f\", cmap='coolwarm')\n    plt.title('Correlation Matrix')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, sklearn, numpy, seaborn\n```python\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\nimport seaborn as sns\n\ndef normalize_and_plot(df):\n    \"\"\"\n    Normalize the columns of a dataframe and plot a pairplot of the normalized dataframe.\n    \n    Parameters:\n    df (DataFrame): The input dataframe.\n    \n    Returns:\n    DataFrame: The normalized dataframe.\n    \n    Requirements:\n    - pandas\n    - sklearn.preprocessing.StandardScaler\n    - numpy\n    - seaborn\n    \n    Example:\n    >>> df = pd.DataFrame(np.random.rand(100, 5), columns=list('ABCDE'))\n    >>> normalized_df = normalize_and_plot(df)\n    >>> print(normalized_df)\n    \"\"\"\n    scaler = StandardScaler()\n    normalized_df = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n    \n    sns.pairplot(normalized_df)\n    plt.show()\n    \n    return normalized_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, datetime, re, seaborn\n```python\nimport pandas as pd\nfrom datetime import datetime\nimport re\nimport seaborn as sns\n\n# Constants\nDATE_FORMAT = \"%Y-%m-%d %H:%M:%S\"\n\ndef filter_and_plot(df, regex_pattern, date_format=DATE_FORMAT):\n    \"\"\"\n    Filter the rows of a dataframe with a column of date strings that match a regex pattern,\n    and plot a histogram of the distribution of dates.\n    \n    Parameters:\n    df (DataFrame): The input dataframe.\n    regex_pattern (str): The regex pattern to match against the date strings.\n    date_format (str): The format of the date strings.\n    \n    Returns:\n    DataFrame: The filtered dataframe.\n    \n    Requirements:\n    - pandas\n    - datetime\n    - re\n    - seaborn\n    \n    Example:\n    >>> df = pd.DataFrame({'date': pd.date_range(start='1/1/2021', periods=200).astype(str)})\n    >>> filtered_df = filter_and_plot(df, r'2021-03.*')\n    >>> print(filtered_df)\n    \"\"\"\n    df['date'] = pd.to_datetime(df['date'], format=date_format)\n    mask = df['date'].astype(str).str.contains(regex_pattern)\n    filtered_df = df.loc[mask]\n    \n    sns.histplot(filtered_df['date'], kde=True)\n    plt.show()\n    \n    return filtered_df\n```\n"}
{"seed": "{\"task_id\": 14716342, \"prompt\": \"def f_14716342(your_string):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"re.search('\\\\\\\\[(.*)\\\\\\\\]', your_string).group(1)\", \"test_start\": \"\\nimport re\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('[uranus]') == 'uranus'\\n\", \"\\n    assert candidate('hello[world] !') == 'world'\\n\"], \"entry_point\": \"f_14716342\", \"intent\": \"find the string in `your_string` between two special characters \\\"[\\\" and \\\"]\\\"\", \"library\": [\"re\"]}", "generation": "Scenario 1:\nre, os, json\n```python\nimport re\nimport os\nimport json\n\n# Constants\nDIRECTORY_PATH = '/path/to/your/directory/'\n\ndef extract_json_files_info(your_string):\n    \"\"\"\n    Find all json files in a given directory that contain a string between two special characters \n    \"[\" and \"]\" in their content and return their names and sizes.\n\n    Parameters:\n    your_string (str): The string to search in the file content.\n\n    Returns:\n    list: A list of dictionaries with the filename and size of each file found.\n\n    Requirements:\n    - re\n    - os\n    - json\n\n    Example:\n    >>> extract_json_files_info('[target]')\n    \"\"\"\n    regex_pattern = re.compile('\\[' + re.escape(your_string) + '\\]')\n    json_files_info = []\n\n    for filename in os.listdir(DIRECTORY_PATH):\n        if filename.endswith('.json'):\n            with open(os.path.join(DIRECTORY_PATH, filename), 'r') as file:\n                content = json.load(file)\n                if regex_pattern.search(str(content)):\n                    file_info = {'filename': filename, 'size': os.path.getsize(os.path.join(DIRECTORY_PATH, filename))}\n                    json_files_info.append(file_info)\n\n    return json_files_info\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nre, pandas, matplotlib.pyplot\n```python\nimport re\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nCSV_PATH = '/path/to/your/csv/file.csv'\n\ndef count_occurrences_and_plot(your_string):\n    \"\"\"\n    Count the occurrences of a string between two special characters \"[\" and \"]\" in a CSV file \n    and plot the counts.\n\n    Parameters:\n    your_string (str): The string to search in the CSV file.\n\n    Returns:\n    dict: A dictionary with the counts of the string in each column.\n\n    Requirements:\n    - re\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> count_occurrences_and_plot('[target]')\n    \"\"\"\n    df = pd.read_csv(CSV_PATH)\n    count_dict = {}\n\n    for column in df.columns:\n        count = 0\n        for cell in df[column]:\n            if re.search('\\[' + re.escape(your_string) + '\\]', str(cell)):\n                count += 1\n        count_dict[column] = count\n\n    pd.Series(count_dict).plot(kind='bar')\n\n    return count_dict\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nre, numpy, matplotlib.pyplot\n```python\nimport re\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nARRAY_LENGTH = 1000\n\ndef find_and_plot(your_string):\n    \"\"\"\n    Generate a random array of strings, find the indices of strings that contain a string \n    between two special characters \"[\" and \"]\" in their content, and plot the indices.\n\n    Parameters:\n    your_string (str): The string to search in the array.\n\n    Returns:\n    list: A list with the indices of the strings found.\n\n    Requirements:\n    - re\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> find_and_plot('[target]')\n    \"\"\"\n    array = np.random.choice(['[target]', '[nottarget]', 'target', 'nottarget'], ARRAY_LENGTH)\n    indices = []\n\n    for i, string in enumerate(array):\n        if re.search('\\[' + re.escape(your_string) + '\\]', string):\n            indices.append(i)\n\n    plt.plot(indices, 'ro')\n\n    return indices\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nre, time, datetime, matplotlib.pyplot\n```python\nimport re\nimport time\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\n\n# Constants\nLOG_FILE_PATH = '/path/to/your/log/file.log'\n\ndef find_and_time(your_string):\n    \"\"\"\n    Parse a log file and find the timestamp of lines that contain a string between two \n    special characters \"[\" and \"]\" in their content.\n\n    Parameters:\n    your_string (str): The string to search in the log file.\n\n    Returns:\n    list: A list with the timestamps of the lines found.\n\n    Requirements:\n    - re\n    - time\n    - datetime\n    - matplotlib.pyplot\n\n    Example:\n    >>> find_and_time('[target]')\n    \"\"\"\n    timestamps = []\n\n    with open(LOG_FILE_PATH, 'r') as file:\n        for line in file:\n            if re.search('\\[' + re.escape(your_string) + '\\]', line):\n                timestamp = datetime.strptime(line.split()[0], '%Y-%m-%d %H:%M:%S')\n                timestamps.append(time.mktime(timestamp.timetuple()))\n\n    plt.plot(timestamps, 'ro')\n\n    return timestamps\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nre, requests, bs4\n```python\nimport re\nimport requests\nfrom bs4 import BeautifulSoup\n\n# Constants\nURL = 'https://www.example.com/'\n\ndef find_in_webpage(your_string):\n    \"\"\"\n    Fetch a webpage and find all tags that contain a string between two special characters \n    \"[\" and \"]\" in their content.\n\n    Parameters:\n    your_string (str): The string to search in the webpage.\n\n    Returns:\n    list: A list with the tags found.\n\n    Requirements:\n    - re\n    - requests\n    - bs4\n\n    Example:\n    >>> find_in_webpage('[target]')\n    \"\"\"\n    response = requests.get(URL)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    tags = []\n\n    for tag in soup.find_all():\n        if re.search('\\[' + re.escape(your_string) + '\\]', tag.get_text()):\n            tags.append(tag.name)\n\n    return tags\n```\n"}
{"seed": "{\"task_id\": 18684076, \"prompt\": \"def f_18684076():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"[d.strftime('%Y%m%d') for d in pandas.date_range('20130226', '20130302')]\", \"test_start\": \"\\nimport pandas \\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate() == ['20130226', '20130227', '20130228', '20130301', '20130302']\\n\"], \"entry_point\": \"f_18684076\", \"intent\": \"create a list of date string in 'yyyymmdd' format with Python Pandas from '20130226' to '20130302'\", \"library\": [\"pandas\"]}", "generation": "Scenario 1: \npandas, numpy, datetime, matplotlib, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nfrom datetime import timedelta\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nSTART_DATE = '2020-01-01'\nEND_DATE = '2020-12-31'\nDATE_FORMAT = '%Y%m%d'\n\ndef generate_sales_data():\n    \"\"\"\n    Generate a DataFrame of sales data with daily sales amounts from '2020-01-01' to '2020-12-31'. \n    Plot a seaborn line plot of the sales amounts over time.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with Date and Sales columns.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - datetime\n    - matplotlib.pyplot\n    - seaborn\n    \n    Example:\n    >>> sales_data = generate_sales_data()\n    >>> print(sales_data.head())\n    >>> sns.lineplot(data=sales_data, x=\"Date\", y=\"Sales\")\n    \"\"\"\n    date_range = pd.date_range(START_DATE, END_DATE)\n    sales_amounts = np.random.randint(100, 500, len(date_range))\n\n    sales_data = pd.DataFrame({\n        'Date': [d.strftime(DATE_FORMAT) for d in date_range],\n        'Sales': sales_amounts\n    })\n\n    return sales_data\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \npandas, numpy, datetime\n```python\nimport pandas as pd\nimport numpy as np\n\n# Constants\nSTART_DATE = '2020-01-01'\nEND_DATE = '2020-12-31'\nDATE_FORMAT = '%Y%m%d'\n\ndef generate_temperature_data():\n    \"\"\"\n    Generate a DataFrame of temperature data with daily average temperatures from '2020-01-01' to '2020-12-31'.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with Date and Temperature columns.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - datetime\n    \n    Example:\n    >>> temp_data = generate_temperature_data()\n    >>> print(temp_data.head())\n    \"\"\"\n    date_range = pd.date_range(START_DATE, END_DATE)\n    temperatures = np.random.uniform(20, 30, len(date_range))\n\n    temp_data = pd.DataFrame({\n        'Date': [d.strftime(DATE_FORMAT) for d in date_range],\n        'Temperature': temperatures\n    })\n\n    return temp_data\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \npandas, numpy, datetime, time, os\n```python\nimport pandas as pd\nimport numpy as np\nfrom datetime import timedelta\nimport time\nimport os\n\n# Constants\nSTART_DATE = '2020-01-01'\nEND_DATE = '2020-12-31'\nDATE_FORMAT = '%Y%m%d'\nFILE_NAME = 'sales_data.csv'\n\ndef generate_and_save_sales_data():\n    \"\"\"\n    Generate a DataFrame of sales data with daily sales amounts from '2020-01-01' to '2020-12-31' \n    and save it to a CSV file in the current directory.\n    \n    Returns:\n    str: The path of the saved CSV file.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - datetime\n    - time\n    - os\n    \n    Example:\n    >>> csv_path = generate_and_save_sales_data()\n    >>> print(csv_path)\n    \"\"\"\n    date_range = pd.date_range(START_DATE, END_DATE)\n    sales_amounts = np.random.randint(100, 500, len(date_range))\n\n    sales_data = pd.DataFrame({\n        'Date': [d.strftime(DATE_FORMAT) for d in date_range],\n        'Sales': sales_amounts\n    })\n\n    file_path = os.path.join(os.getcwd(), FILE_NAME)\n    sales_data.to_csv(file_path, index=False)\n\n    return file_path\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \npandas, matplotlib, datetime, seaborn\n```python\nimport pandas as pd\nfrom datetime import timedelta\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nSTART_DATE = '2020-01-01'\nEND_DATE = '2020-12-31'\nDATE_FORMAT = '%Y%m%d'\n\ndef plot_temperature_data(temp_data):\n    \"\"\"\n    Plot a seaborn line plot of temperature data over time.\n    \n    Parameters:\n    temp_data (pandas.DataFrame): The temperature data with 'Date' and 'Temperature' columns.\n    \n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - seaborn\n    - datetime\n    \n    Example:\n    >>> temp_data = pd.DataFrame({\n    ...     'Date': pd.date_range(START_DATE, END_DATE).strftime(DATE_FORMAT),\n    ...     'Temperature': np.random.uniform(20, 30, 365)\n    ... })\n    >>> plot_temperature_data(temp_data)\n    \"\"\"\n    temp_data['Date'] = pd.to_datetime(temp_data['Date'], format=DATE_FORMAT)\n    plt.figure(figsize=(10, 6))\n    sns.lineplot(data=temp_data, x='Date', y='Temperature')\n    plt.title('Temperature over Time')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \npandas, numpy, datetime, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nfrom datetime import timedelta\nimport seaborn as sns\n\n# Constants\nSTART_DATE = '2020-01-01'\nEND_DATE = '2020-12-31'\nDATE_FORMAT = '%Y%m%d'\n\ndef plot_sales_data(sales_data):\n    \"\"\"\n    Plot a seaborn line plot of sales data over time.\n    \n    Parameters:\n    sales_data (pandas.DataFrame): The sales data with 'Date' and 'Sales' columns.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - datetime\n    - seaborn\n    \n    Example:\n    >>> sales_data = pd.DataFrame({\n    ...     'Date': pd.date_range(START_DATE, END_DATE).strftime(DATE_FORMAT),\n    ...     'Sales': np.random.randint(100, 500, 365)\n    ... })\n    >>> plot_sales_data(sales_data)\n    \"\"\"\n    sales_data['Date'] = pd.to_datetime(sales_data['Date'], format=DATE_FORMAT)\n    plt.figure(figsize=(10, 6))\n    sns.lineplot(data=sales_data, x='Date', y='Sales')\n    plt.title('Sales over Time')\n    plt.show()\n```"}
{"seed": "{\"task_id\": 1666700, \"prompt\": \"def f_1666700():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"\\\"\\\"\\\"The big brown fox is brown\\\"\\\"\\\".count('brown')\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate() == 2\\n\"], \"entry_point\": \"f_1666700\", \"intent\": \"count number of times string 'brown' occurred in string 'The big brown fox is brown'\", \"library\": []}", "generation": "Scenario 1: \ncollections, re, string\n```python\nfrom collections import Counter\nimport re\nimport string\n\n# Constants\nTEXT = \"The big brown fox is brown. Brown foxes are common in North America.\"\n\ndef count_word_frequency(text=TEXT, word='brown'):\n    \"\"\"\n    Count the frequency of a word in a given text string, ignoring case and punctuation.\n    \n    Parameters:\n    text (str): The text string to search. Default is a predefined string.\n    word (str): The word to count. Default is 'brown'.\n    \n    Returns:\n    int: The count of the word in the text.\n    \n    Requirements:\n    - collections.Counter\n    - re\n    - string\n\n    Example:\n    >>> count_word_frequency('The big brown fox is brown. Brown foxes are common in North America.', 'brown')\n    \"\"\"\n    text = text.lower()\n    word = word.lower()\n    text = re.sub('['+string.punctuation+']', '', text)\n    word_counts = Counter(text.split())\n    \n    return word_counts[word]\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \ncollections, re, string\n```python\nfrom collections import Counter\nimport re\nimport string\n\n# Constants\nTEXT = \"This is a sample text. It is meant to illustrate the counting of words.\"\n\ndef count_all_words(text=TEXT):\n    \"\"\"\n    Count the frequency of all words in a given text string, ignoring case and punctuation.\n    \n    Parameters:\n    text (str): The text string to search. Default is a predefined string.\n    \n    Returns:\n    dict: A dictionary with words as keys and their counts as values.\n    \n    Requirements:\n    - collections.Counter\n    - re\n    - string\n\n    Example:\n    >>> count_all_words('This is a sample text. It is meant to illustrate the counting of words.')\n    \"\"\"\n    text = text.lower()\n    text = re.sub('['+string.punctuation+']', '', text)\n    word_counts = Counter(text.split())\n    \n    return dict(word_counts)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \ncollections, re, string\n```python\nfrom collections import Counter\nimport re\nimport string\n\n# Constants\nTEXT = \"This is a sample text. It is meant to illustrate the counting of words.\"\n\ndef find_most_common_word(text=TEXT):\n    \"\"\"\n    Find the most common word in a given text string, ignoring case and punctuation.\n    \n    Parameters:\n    text (str): The text string to search. Default is a predefined string.\n    \n    Returns:\n    tuple: A tuple with the most common word and its count.\n    \n    Requirements:\n    - collections.Counter\n    - re\n    - string\n\n    Example:\n    >>> find_most_common_word('This is a sample text. It is meant to illustrate the counting of words.')\n    \"\"\"\n    text = text.lower()\n    text = re.sub('['+string.punctuation+']', '', text)\n    word_counts = Counter(text.split())\n    \n    return word_counts.most_common(1)[0]\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \ncollections, re, string\n```python\nfrom collections import Counter\nimport re\nimport string\n\n# Constants\nTEXT = \"This is a sample text. It is meant to illustrate the counting of words.\"\n\ndef find_least_common_words(text=TEXT):\n    \"\"\"\n    Find the least common words in a given text string, ignoring case and punctuation.\n    \n    Parameters:\n    text (str): The text string to search. Default is a predefined string.\n    \n    Returns:\n    list: A list of tuples with the least common words and their counts.\n    \n    Requirements:\n    - collections.Counter\n    - re\n    - string\n\n    Example:\n    >>> find_least_common_words('This is a sample text. It is meant to illustrate the counting of words.')\n    \"\"\"\n    text = text.lower()\n    text = re.sub('['+string.punctuation+']', '', text)\n    word_counts = Counter(text.split())\n    \n    least_common = word_counts.most_common()[:-4:-1]\n    \n    return least_common\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \ncollections, re, string\n```python\nfrom collections import Counter\nimport re\nimport string\n\n# Constants\nTEXT = \"This is a sample text. It is meant to illustrate the counting of words.\"\n\ndef count_word_frequency_list(text=TEXT, words=['this', 'is', 'a']):\n    \"\"\"\n    Count the frequency of a list of words in a given text string, ignoring case and punctuation.\n    \n    Parameters:\n    text (str): The text string to search. Default is a predefined string.\n    words (list): The list of words to count. Default is ['this', 'is', 'a'].\n    \n    Returns:\n    dict: A dictionary with words as keys and their counts as values.\n    \n    Requirements:\n    - collections.Counter\n    - re\n    - string\n\n    Example:\n    >>> count_word_frequency_list('This is a sample text. It is meant to illustrate the counting of words.', ['this', 'is', 'a'])\n    \"\"\"\n    text = text.lower()\n    text = re.sub('['+string.punctuation+']', '', text)\n    word_counts = Counter(text.split())\n    \n    return {word: word_counts[word] for word in words}\n```\n"}
{"seed": "{\"task_id\": 18979111, \"prompt\": \"def f_18979111(request_body):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"json.loads(request_body)\", \"test_start\": \"\\nimport json \\n\\ndef check(candidate):\", \"test\": [\"\\n    x = \\\"\\\"\\\"{\\n    \\\"Name\\\": \\\"Jennifer Smith\\\",\\n    \\\"Contact Number\\\": 7867567898,\\n    \\\"Email\\\": \\\"jen123@gmail.com\\\",\\n    \\\"Hobbies\\\":[\\\"Reading\\\", \\\"Sketching\\\", \\\"Horse Riding\\\"]\\n    }\\\"\\\"\\\"\\n    assert candidate(x) == {'Hobbies': ['Reading', 'Sketching', 'Horse Riding'], 'Name': 'Jennifer Smith', 'Email': 'jen123@gmail.com', 'Contact Number': 7867567898}\\n\"], \"entry_point\": \"f_18979111\", \"intent\": \"decode json string `request_body` to python dict\", \"library\": [\"json\"]}", "generation": "Scenario 1: \njson, pandas, numpy, seaborn, matplotlib\n```python\nimport json\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['Name', 'Contact Number', 'Email', 'Hobbies']\n\ndef analyze_hobbies(json_string):\n    \"\"\"\n    Decode a json string to python dict, convert it to pandas DataFrame,\n    count the occurrence of each hobby, and plot the result.\n\n    Parameters:\n    json_string (str): The json string to be decoded.\n\n    Returns:\n    DataFrame, Bar plot: A pandas DataFrame with count of each hobby, and a bar plot.\n\n    Requirements:\n    - json\n    - pandas\n    - numpy\n    - seaborn\n    - matplotlib.pyplot\n\n    Example:\n    >>> json_str = '{\"Name\": \"Jennifer Smith\", \"Contact Number\": 7867567898, \"Email\": \"jen123@gmail.com\", \"Hobbies\":[\"Reading\", \"Sketching\", \"Horse Riding\"]}'\n    >>> df, plot = analyze_hobbies(json_str)\n    >>> print(df)\n    >>> plt.show(plot)\n    \"\"\"\n    data_dict = json.loads(json_string)\n    df = pd.DataFrame([data_dict], columns=COLUMNS)\n    hobbies = np.array(df['Hobbies'].explode())\n    hobbies_df = pd.DataFrame(hobbies, columns=['Hobbies'])\n    plot = sns.countplot(data=hobbies_df, x='Hobbies')\n\n    return hobbies_df, plot\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\njson, urllib, socket, time, hashlib\n```python\nimport json\nimport urllib.parse\nimport socket\nimport time\nimport hashlib\n\n# Constants\nBUFFER_SIZE = 1024\n\ndef send_json_over_socket(json_string, host, port):\n    \"\"\"\n    Decode a json string to python dict, encode it back to json string with sorted keys,\n    create hash of the json string, send json string and it's hash over a socket.\n\n    Parameters:\n    json_string (str): The json string to be decoded.\n    host (str): The host of the server to connect to.\n    port (int): The port of the server to connect to.\n\n    Returns:\n    None: This function does not return anything.\n\n    Requirements:\n    - json\n    - urllib.parse\n    - socket\n    - time\n    - hashlib\n\n    Example:\n    >>> json_str = '{\"Name\": \"Jennifer Smith\", \"Contact Number\": 7867567898, \"Email\": \"jen123@gmail.com\", \"Hobbies\":[\"Reading\", \"Sketching\", \"Horse Riding\"]}'\n    >>> send_json_over_socket(json_str, 'localhost', 12345)\n    \"\"\"\n    data_dict = json.loads(json_string)\n    sorted_json_str = json.dumps(data_dict, sort_keys=True)\n    hash_object = hashlib.md5(sorted_json_str.encode())\n    hashed_json_str = hash_object.hexdigest()\n\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n        s.connect((host, port))\n        s.sendall(urllib.parse.quote_plus(sorted_json_str + hashed_json_str).encode('utf-8'))\n\n    time.sleep(1)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\njson, os, zipfile, datetime\n```python\nimport json\nimport os\nimport zipfile\nfrom datetime import datetime\n\n# Constants\nFILE_NAME = 'data.json'\n\ndef save_json_to_zip(json_string, zip_name):\n    \"\"\"\n    Decode a json string to python dict, save it to a json file,\n    compress the file into a zip with the current datetime as the file name.\n\n    Parameters:\n    json_string (str): The json string to be decoded.\n    zip_name (str): The name of the zip file.\n\n    Returns:\n    str: The path of the zip file.\n\n    Requirements:\n    - json\n    - os\n    - zipfile\n    - datetime\n\n    Example:\n    >>> json_str = '{\"Name\": \"Jennifer Smith\", \"Contact Number\": 7867567898, \"Email\": \"jen123@gmail.com\", \"Hobbies\":[\"Reading\", \"Sketching\", \"Horse Riding\"]}'\n    >>> save_json_to_zip(json_str, 'data.zip')\n    \"\"\"\n    data_dict = json.loads(json_string)\n\n    with open(FILE_NAME, 'w') as f:\n        json.dump(data_dict, f)\n\n    zip_file_name = f\"{zip_name}_{datetime.now().strftime('%Y%m%d%H%M%S')}.zip\"\n    with zipfile.ZipFile(zip_file_name, 'w') as z:\n        z.write(FILE_NAME)\n\n    os.remove(FILE_NAME)\n\n    return zip_file_name\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\njson, pandas, sklearn.linear_model, numpy\n```python\nimport json\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\n\n# Constants\nFEATURES = ['Feature1', 'Feature2', 'Feature3']\nTARGET = 'Target'\n\ndef train_linear_regression(json_string):\n    \"\"\"\n    Decode a json string to python dict, convert it to pandas DataFrame,\n    train a linear regression model using the data in the DataFrame.\n\n    Parameters:\n    json_string (str): The json string to be decoded.\n\n    Returns:\n    LinearRegression: The trained linear regression model.\n\n    Requirements:\n    - json\n    - pandas\n    - sklearn.linear_model\n    - numpy\n\n    Example:\n    >>> json_str = '[{\"Feature1\": 1, \"Feature2\": 2, \"Feature3\": 3, \"Target\": 6}, {\"Feature1\": 4, \"Feature2\": 5, \"Feature3\": 6, \"Target\": 15}]'\n    >>> model = train_linear_regression(json_str)\n    >>> print(model.predict(np.array([7, 8, 9]).reshape(1, -1)))\n    \"\"\"\n    data_list = json.loads(json_string)\n    df = pd.DataFrame(data_list)\n    X = df[FEATURES]\n    y = df[TARGET]\n\n    model = LinearRegression()\n    model.fit(X, y)\n\n    return model\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\njson, matplotlib, pandas, seaborn\n```python\nimport json\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n# Constants\nCOLUMNS = ['Name', 'Contact Number', 'Email', 'Hobbies']\n\ndef plot_hobby_distribution(json_string):\n    \"\"\"\n    Decode a json string to python dict, convert it to pandas DataFrame,\n    explode the 'Hobbies' column and plot the distribution of hobbies.\n\n    Parameters:\n    json_string (str): The json string to be decoded.\n\n    Returns:\n    DataFrame, Bar plot: A pandas DataFrame with exploded 'Hobbies' column, and a bar plot.\n\n    Requirements:\n    - json\n    - matplotlib\n    - pandas\n    - seaborn\n\n    Example:\n    >>> json_str = '{\"Name\": \"Jennifer Smith\", \"Contact Number\": 7867567898, \"Email\": \"jen123@gmail.com\", \"Hobbies\":[\"Reading\", \"Sketching\", \"Horse Riding\"]}'\n    >>> df, plot = plot_hobby_distribution(json_str)\n    >>> print(df)\n    >>> plt.show(plot)\n    \"\"\"\n    data_dict = json.loads(json_string)\n    df = pd.DataFrame([data_dict], columns=COLUMNS)\n    df = df.explode('Hobbies')\n    plot = sns.countplot(data=df, x='Hobbies')\n\n    return df, plot\n```"}
{"seed": "{\"task_id\": 7243750, \"prompt\": \"def f_7243750(url, file_name):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"urllib.request.urlretrieve(url, file_name)\", \"test_start\": \"\\nimport urllib \\n\\ndef check(candidate):\", \"test\": [\"\\n    file_name = 'g.html'\\n    candidate('https://asia.nikkei.com/Business/Tech/Semiconductors/U.S.-chip-tool-maker-Synopsys-expands-in-Vietnam-amid-China-tech-war', file_name)\\n    with open (file_name, 'r') as f:\\n        lines = f.readlines()\\n        if len(lines) == 0: assert False\\n        else: assert True\\n\"], \"entry_point\": \"f_7243750\", \"intent\": \"download the file from url `url` and save it under file `file_name`\", \"library\": [\"urllib\"]}", "generation": "Scenario 1: \nurllib, bs4, re, pandas\n```python\nimport urllib.request\nfrom bs4 import BeautifulSoup\nimport re\nimport pandas as pd\n\ndef extract_data_from_url_and_save_to_csv(url, file_name):\n    \"\"\"\n    Download a webpage from a specified url, extract data from the table in that webpage,\n    and save the data to a csv file under a specified file name.\n    \n    Parameters:\n    url (str): The url of the webpage.\n    file_name (str): The name of the csv file to save the data to.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - urllib\n    - bs4\n    - re\n    - pandas\n    \n    Example:\n    >>> extract_data_from_url_and_save_to_csv('https://www.example.com', 'data.csv')\n    \"\"\"\n    html = urllib.request.urlopen(url).read()\n    soup = BeautifulSoup(html, 'html.parser')\n    table = soup.find('table')\n    data = []\n    headers = []\n\n    for row in table.findAll('tr'):\n        cells = row.findAll('td')\n        if len(cells) > 0:\n            data.append([cell.text for cell in cells])\n        else:\n            headers = [header.text for header in row.findAll('th')]\n\n    df = pd.DataFrame(data, columns=headers)\n    df.to_csv(file_name, index=False)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nurllib, PIL, os, hashlib\n```python\nimport urllib.request\nfrom PIL import Image\nimport os\nimport hashlib\n\ndef download_image_resize_and_save(url, file_name, size=(200, 200)):\n    \"\"\"\n    Download an image from a specified url, resize it to a specified size, \n    and save it under a specified file name with the hash value of the image content \n    appended to the original file name.\n    \n    Parameters:\n    url (str): The url of the image.\n    file_name (str): The original file name.\n    size (tuple): The size to resize the image to. Default is (200, 200).\n    \n    Returns:\n    str: The name of the saved file.\n    \n    Requirements:\n    - urllib\n    - PIL\n    - os\n    - hashlib\n    \n    Example:\n    >>> download_image_resize_and_save('https://www.example.com/image.jpg', 'image.jpg')\n    \"\"\"\n    urllib.request.urlretrieve(url, file_name)\n    \n    with open(file_name, 'rb') as f:\n        content = f.read()\n        hash_value = hashlib.sha256(content).hexdigest()\n\n    new_file_name = os.path.splitext(file_name)[0] + '_' + hash_value + os.path.splitext(file_name)[1]\n\n    image = Image.open(file_name)\n    image = image.resize(size)\n    image.save(new_file_name)\n    \n    os.remove(file_name)\n\n    return new_file_name\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nurllib, pdfminer, io, re\n```python\nimport urllib.request\nfrom pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\nfrom pdfminer.converter import TextConverter\nfrom pdfminer.layout import LAParams\nfrom pdfminer.pdfpage import PDFPage\nimport io\nimport re\n\ndef download_pdf_and_extract_text(url, file_name, pages=None):\n    \"\"\"\n    Download a pdf from a specified url, extract the text from the pdf, \n    and save it under a specified file name.\n    \n    Parameters:\n    url (str): The url of the pdf.\n    file_name (str): The name of the file to save the text to.\n    pages (list): The list of pages to extract the text from. Default is None, which means all pages.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - urllib\n    - pdfminer\n    - io\n    - re\n    \n    Example:\n    >>> download_pdf_and_extract_text('https://www.example.com/document.pdf', 'document.txt')\n    \"\"\"\n    urllib.request.urlretrieve(url, 'temp.pdf')\n    \n    manager = PDFResourceManager()\n    file = io.StringIO()\n    converter = TextConverter(manager, file, laparams=LAParams())\n    interpreter = PDFPageInterpreter(manager, converter)\n\n    with open('temp.pdf', 'rb') as f:\n        for page in PDFPage.get_pages(f, pages):\n            interpreter.process_page(page)\n        text = file.getvalue()\n\n    with open(file_name, 'w') as f:\n        f.write(text)\n\n    converter.close()\n    file.close()\n    os.remove('temp.pdf')\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nurllib, zipfile, os\n```python\nimport urllib.request\nimport zipfile\nimport os\n\ndef download_and_extract_zip(url, destination_folder):\n    \"\"\"\n    Download a zip file from a specified url and extract it to a specified destination folder.\n    \n    Parameters:\n    url (str): The url of the zip file.\n    destination_folder (str): The destination folder to extract the zip file to.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - urllib\n    - zipfile\n    - os\n    \n    Example:\n    >>> download_and_extract_zip('https://www.example.com/archive.zip', 'data')\n    \"\"\"\n    urllib.request.urlretrieve(url, 'temp.zip')\n    \n    with zipfile.ZipFile('temp.zip', 'r') as zip_ref:\n        zip_ref.extractall(destination_folder)\n\n    os.remove('temp.zip')\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nurllib, lxml, pandas\n```python\nimport urllib.request\nfrom lxml import etree\nimport pandas as pd\n\ndef download_xml_and_convert_to_dataframe(url, file_name):\n    \"\"\"\n    Download an xml file from a specified url, parse it, \n    and convert it to a pandas DataFrame saved under a specified file name.\n    \n    Parameters:\n    url (str): The url of the xml file.\n    file_name (str): The name of the file to save the DataFrame to.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - urllib\n    - lxml\n    - pandas\n    \n    Example:\n    >>> download_xml_and_convert_to_dataframe('https://www.example.com/data.xml', 'data.csv')\n    \"\"\"\n    urllib.request.urlretrieve(url, 'temp.xml')\n    \n    tree = etree.parse('temp.xml')\n    root = tree.getroot()\n\n    data = []\n    for element in root:\n        data.append({child.tag: child.text for child in element})\n\n    df = pd.DataFrame(data)\n    df.to_csv(file_name, index=False)\n\n    os.remove('temp.xml')\n```"}
{"seed": "{\"task_id\": 743806, \"prompt\": \"def f_743806(text):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"text.split()\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('The quick brown fox') == ['The', 'quick', 'brown', 'fox']\\n\", \"\\n    assert candidate('hello!') == ['hello!']\\n\", \"\\n    assert candidate('hello world !') == ['hello', 'world', '!']\\n\"], \"entry_point\": \"f_743806\", \"intent\": \"split string `text` by space\", \"library\": []}", "generation": "Scenario 1: \nnltk, string, re, collections\n```python\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom string import punctuation\nimport re\nfrom collections import Counter\n\n# Constants\nSTOPWORDS = set(stopwords.words('english'))\n\ndef count_word_frequency(text, n_most_common):\n    \"\"\"\n    Count the frequency of each word in a text string, excluding stopwords and punctuation, \n    and return the n most common words and their frequencies.\n\n    Parameters:\n    text (str): The text string.\n    n_most_common (int): The number of most common words to return.\n\n    Returns:\n    list: A list of tuples with the most common words and their frequencies.\n\n    Requirements:\n    - nltk\n    - string\n    - re\n    - collections\n\n    Example:\n    >>> count_word_frequency('The quick brown fox jumps over the quick dog', 2)\n    [('quick', 2), ('brown', 1)]\n    \"\"\"\n    words = word_tokenize(text.lower())\n    words = [word for word in words if word not in STOPWORDS and word not in punctuation]\n    word_counts = Counter(words)\n\n    return word_counts.most_common(n_most_common)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nnltk, string, re, collections\n```python\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Constants\nSTOPWORDS = set(stopwords.words('english'))\n\ndef calculate_tfidf(text_list):\n    \"\"\"\n    Calculate the TF-IDF (Term Frequency-Inverse Document Frequency) score for each word \n    in a list of text strings.\n\n    Parameters:\n    text_list (list): The list of text strings.\n\n    Returns:\n    numpy.ndarray: A 2D array with the TF-IDF scores for each word in each document.\n\n    Requirements:\n    - numpy\n    - sklearn.feature_extraction.text\n\n    Example:\n    >>> calculate_tfidf(['The quick brown fox', 'jumps over the quick dog'])\n    array([[0.57735027, 0.57735027, 0.57735027, 0.        , 0.        , 0.        ],\n           [0.        , 0.57735027, 0.        , 0.57735027, 0.57735027, 0.57735027]])\n    \"\"\"\n    vectorizer = TfidfVectorizer(stop_words=STOPWORDS)\n    tfidf_matrix = vectorizer.fit_transform(text_list)\n\n    return tfidf_matrix.toarray()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnltk, string, re, collections\n```python\nimport nltk\nfrom nltk.tokenize import sent_tokenize\n\ndef count_sentences(text):\n    \"\"\"\n    Count the number of sentences in a text string.\n\n    Parameters:\n    text (str): The text string.\n\n    Returns:\n    int: The number of sentences.\n\n    Requirements:\n    - nltk\n\n    Example:\n    >>> count_sentences('The quick brown fox. Jumps over the quick dog.')\n    2\n    \"\"\"\n    sentences = sent_tokenize(text)\n\n    return len(sentences)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnltk, string, re, collections\n```python\nimport nltk\nfrom nltk.corpus import wordnet\n\ndef get_synonyms(word):\n    \"\"\"\n    Get a list of synonyms for a given word using the WordNet lexical database.\n\n    Parameters:\n    word (str): The word.\n\n    Returns:\n    list: The list of synonyms.\n\n    Requirements:\n    - nltk\n    - nltk.corpus.wordnet\n\n    Example:\n    >>> get_synonyms('quick')\n    ['speedy', 'fast', 'rapid', 'quick']\n    \"\"\"\n    synonyms = []\n    for synset in wordnet.synsets(word):\n        for lemma in synset.lemmas():\n            synonyms.append(lemma.name())\n\n    return synonyms\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnltk, string, re, collections\n```python\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\n\ndef stem_words(text):\n    \"\"\"\n    Stem each word in a text string using the Porter stemming algorithm.\n\n    Parameters:\n    text (str): The text string.\n\n    Returns:\n    list: The list of stemmed words.\n\n    Requirements:\n    - nltk\n    - nltk.tokenize\n    - nltk.stem\n\n    Example:\n    >>> stem_words('The quick brown fox jumps over the quick dog')\n    ['The', 'quick', 'brown', 'fox', 'jump', 'over', 'the', 'quick', 'dog']\n    \"\"\"\n    words = word_tokenize(text)\n    stemmer = PorterStemmer()\n    stemmed_words = [stemmer.stem(word) for word in words]\n\n    return stemmed_words\n```"}
{"seed": "{\"task_id\": 743806, \"prompt\": \"def f_743806(text):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"text.split(',')\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('The quick brown fox') == ['The quick brown fox']\\n\", \"\\n    assert candidate('The,quick,brown,fox') == ['The', 'quick', 'brown', 'fox']\\n\"], \"entry_point\": \"f_743806\", \"intent\": \"split string `text` by \\\",\\\"\", \"library\": []}", "generation": "Scenario 1:\npandas, numpy, random, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import choice\nimport matplotlib.pyplot as plt\n\n# Constants\nCATEGORIES = ['Electronics', 'Fashion', 'Home & Kitchen', 'Sports', 'Health & Personal Care']\nSTATUSES = ['Low Stock', 'In Stock', 'Out of Stock']\n\ndef generate_inventory_report(n_products):\n    \"\"\"\n    Generate a report of inventory status for a list of products across various \n    categories.\n    \n    Parameters:\n    n_products (int): The number of products.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with inventory status for the products.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> report = generate_inventory_report(10)\n    >>> print(report)\n    >>> report['Status'].value_counts().plot(kind='bar')\n    \"\"\"\n    report_data = []\n\n    for _ in range(n_products):\n        category = choice(CATEGORIES)\n        status = choice(STATUSES)\n        report_data.append([category, status])\n\n    report_df = pd.DataFrame(report_data, columns=['Category', 'Status'])\n\n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnltk, string, collections\n```python\nimport nltk\nfrom string import punctuation\nfrom collections import Counter\n\ndef count_words(text):\n    \"\"\"\n    Count the frequency of each word in the given text.\n\n    Parameters:\n    text (str): The text to analyze.\n\n    Returns:\n    dict: A dictionary with each unique word as the key and its frequency as the value.\n\n    Requirements:\n    - nltk\n    - string\n    - collections\n\n    Example:\n    >>> count_words('The quick brown fox jumps over the lazy dog.')\n    \"\"\"\n    words = nltk.word_tokenize(text)\n    words = [word.lower() for word in words if word.isalpha()]\n    word_counts = Counter(words)\n\n    return dict(word_counts)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nrandom, string\n```python\nimport random\nimport string\n\ndef generate_random_strings(n, length):\n    \"\"\"\n    Generate n random strings of a given length.\n\n    Parameters:\n    n (int): The number of strings to generate.\n    length (int): The length of each string.\n\n    Returns:\n    list: A list of random strings.\n\n    Requirements:\n    - random\n    - string\n\n    Example:\n    >>> generate_random_strings(5, 10)\n    \"\"\"\n    strings = []\n    for _ in range(n):\n        str_ = ''.join(random.choices(string.ascii_letters + string.digits, k=length))\n        strings.append(str_)\n\n    return strings\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, scipy.stats\n```python\nimport numpy as np\nfrom scipy.stats import pearsonr\n\ndef calculate_correlation(array1, array2):\n    \"\"\"\n    Calculate the Pearson correlation coefficient between two arrays.\n\n    Parameters:\n    array1, array2 (numpy arrays): The arrays to calculate correlation.\n\n    Returns:\n    float: The correlation coefficient.\n\n    Requirements:\n    - numpy\n    - scipy.stats\n\n    Example:\n    >>> calculate_correlation(np.array([1, 2, 3]), np.array([1, 2, 3]))\n    \"\"\"\n    correlation, _ = pearsonr(array1, array2)\n\n    return correlation\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nmatplotlib.pyplot, numpy\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_sin_and_cos():\n    \"\"\"\n    Plot sine and cosine functions on the same plot.\n\n    Requirements:\n    - matplotlib.pyplot\n    - numpy\n\n    Example:\n    >>> plot_sin_and_cos()\n    \"\"\"\n    x = np.linspace(-np.pi, np.pi, 200)\n    plt.plot(x, np.sin(x), label='sin(x)')\n    plt.plot(x, np.cos(x), label='cos(x)')\n    plt.legend()\n    plt.show()\n```\n"}
{"seed": "{\"task_id\": 743806, \"prompt\": \"def f_743806(line):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"line.split()\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('The  quick brown  fox') == ['The', 'quick', 'brown', 'fox']\\n\"], \"entry_point\": \"f_743806\", \"intent\": \"Split string `line` into a list by whitespace\", \"library\": []}", "generation": "Scenario 1:\ncollections, re, nltk\n```python\nfrom collections import Counter\nimport re\nfrom nltk.corpus import stopwords\n\n# Constants\nSTOPWORDS = set(stopwords.words('english'))\n\ndef count_non_stopwords(text):\n    \"\"\"\n    Count the non-stopword words in a text string.\n\n    Parameters:\n    text (str): The text string.\n\n    Returns:\n    dict: A dictionary with word counts.\n\n    Requirements:\n    - collections\n    - re\n    - nltk\n\n    Example:\n    >>> count_non_stopwords('The quick brown fox jumps over the lazy dog')\n    \"\"\"\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    non_stopwords = [word for word in words if word not in STOPWORDS]\n    word_counts = dict(Counter(non_stopwords))\n\n    return word_counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, scipy, sklearn\n```python\nimport numpy as np\nfrom scipy import stats\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndef get_tf_idf_scores(texts):\n    \"\"\"\n    Calculate the TF-IDF scores of words in a list of texts.\n\n    Parameters:\n    texts (list): A list of text strings.\n\n    Returns:\n    array: A 2D numpy array with TF-IDF scores.\n\n    Requirements:\n    - numpy\n    - scipy\n    - sklearn.feature_extraction.text\n\n    Example:\n    >>> get_tf_idf_scores(['The quick brown fox', 'jumps over the lazy dog'])\n    \"\"\"\n    vectorizer = CountVectorizer()\n    word_counts = vectorizer.fit_transform(texts)\n    tf_idf_scores = word_counts.toarray()\n\n    return tf_idf_scores\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnltk, string, collections\n```python\nfrom nltk import word_tokenize\nfrom string import punctuation\nfrom collections import defaultdict\n\ndef count_punctuations(text):\n    \"\"\"\n    Count the punctuations in a text string.\n\n    Parameters:\n    text (str): The text string.\n\n    Returns:\n    dict: A dictionary with punctuation counts.\n\n    Requirements:\n    - nltk\n    - string\n    - collections\n\n    Example:\n    >>> count_punctuations('Hello, world! How are you doing today?')\n    \"\"\"\n    tokens = word_tokenize(text)\n    punc_counts = defaultdict(int)\n\n    for token in tokens:\n        if token in punctuation:\n            punc_counts[token] += 1\n\n    return dict(punc_counts)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, glob, pandas\n```python\nimport os\nimport glob\nimport pandas as pd\n\ndef combine_csv_files(directory):\n    \"\"\"\n    Combine all CSV files in a directory into a single pandas DataFrame.\n\n    Parameters:\n    directory (str): The directory path.\n\n    Returns:\n    DataFrame: A pandas DataFrame with all data.\n\n    Requirements:\n    - os\n    - glob\n    - pandas\n\n    Example:\n    >>> combine_csv_files('/path/to/directory')\n    \"\"\"\n    all_files = glob.glob(os.path.join(directory, \"*.csv\"))\n    all_dfs = [pd.read_csv(file) for file in all_files]\n    combined_df = pd.concat(all_dfs, ignore_index=True)\n\n    return combined_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nrequests, BeautifulSoup, pandas\n```python\nfrom requests import get\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\ndef scrape_html_table(url, table_number=0):\n    \"\"\"\n    Scrape an HTML table from a web page into a pandas DataFrame.\n\n    Parameters:\n    url (str): The URL of the web page.\n    table_number (int): The number of the table on the page (default is 0 for the first table).\n\n    Returns:\n    DataFrame: A pandas DataFrame with the table data.\n\n    Requirements:\n    - requests\n    - BeautifulSoup\n    - pandas\n\n    Example:\n    >>> scrape_html_table('https://www.example.com')\n    \"\"\"\n    response = get(url)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    table = soup.find_all('table')[table_number]\n    data = [[cell.text for cell in row.find_all('td')] for row in table.find_all('tr')]\n    df = pd.DataFrame(data)\n\n    return df\n```"}
{"seed": "{\"task_id\": 35044115, \"prompt\": \"def f_35044115(s):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"[re.sub('(?<!\\\\\\\\d)\\\\\\\\.(?!\\\\\\\\d)', ' ', i) for i in s]\", \"test_start\": \"\\nimport re \\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('h.j.k') == ['h', ' ', 'j', ' ', 'k']\\n\"], \"entry_point\": \"f_35044115\", \"intent\": \"replace dot characters  '.' associated with ascii letters in list `s` with space ' '\", \"library\": [\"re\"]}", "generation": "Scenario 1:\npandas, re, datetime, random\n```python\nimport pandas as pd\nimport re\nfrom datetime import datetime\nfrom random import randint\n\n# Constants\nNAMES = ['John.Doe', 'Jane.Doe', 'John.Smith', 'Jane.Smith', 'James.Bond']\nDATES = pd.date_range(start='1/1/2022', end='31/12/2022')\n\ndef generate_attendance_report(names= NAMES, dates=DATES):\n    \"\"\"\n    Generate a report of attendance for a list of names (with '.' character) \n    over a given date range. The '.' character in names will be replaced with ' '.\n    \n    Parameters:\n    names (list): List of names. Default is NAMES list.\n    dates (DatetimeIndex): Pandas DatetimeIndex. Default is DATES list.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with attendance report for the names.\n    \n    Requirements:\n    - pandas\n    - re\n    - datetime\n    - random\n    \n    Example:\n    >>> report = generate_attendance_report()\n    >>> print(report)\n    \"\"\"\n    report_data = []\n\n    for date in dates:\n        for name in names:\n            name = re.sub('(?<!\\\\\\\\d)\\\\\\\\.(?!\\\\\\\\d)', ' ', name)\n            attendance = bool(randint(0, 1))\n            report_data.append([date, name, attendance])\n\n    report_df = pd.DataFrame(report_data, columns=['Date', 'Name', 'Attendance'])\n\n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nre, os, csv\n```python\nimport re\nimport os\nimport csv\n\n# Constants\nDIRECTORY = 'path_to_your_directory'\n\ndef search_and_replace_in_files(directory=DIRECTORY):\n    \"\"\"\n    Search and replace '.' characters associated with ascii letters in all files \n    in a given directory with ' '.\n    \n    Parameters:\n    directory (str): Path to the directory. Default is DIRECTORY constant.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - re\n    - os\n    - csv\n    \n    Example:\n    >>> search_and_replace_in_files()\n    \"\"\"\n    for filename in os.listdir(directory):\n        if filename.endswith('.csv'):\n            with open(os.path.join(directory, filename), 'r') as file:\n                reader = csv.reader(file)\n                data = [row for row in reader]\n\n            for i, row in enumerate(data):\n                data[i] = [re.sub('(?<!\\\\\\\\d)\\\\\\\\.(?!\\\\\\\\d)', ' ', cell) for cell in row]\n\n            with open(os.path.join(directory, filename), 'w') as file:\n                writer = csv.writer(file)\n                writer.writerows(data)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, re, matplotlib\n```python\nimport pandas as pd\nimport re\nimport matplotlib.pyplot as plt\n\n# Constants\nDATA = {'Names': ['John.Doe', 'Jane.Doe', 'John.Smith', 'Jane.Smith', 'James.Bond'],\n        'Scores': [85, 90, 95, 88, 92]}\n\ndef plot_scores(data=DATA):\n    \"\"\"\n    Plot a bar chart of scores for a list of names (with '.' character). \n    The '.' character in names will be replaced with ' '.\n    \n    Parameters:\n    data (dict): Dictionary with 'Names' and 'Scores'. Default is DATA constant.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - pandas\n    - re\n    - matplotlib\n    \n    Example:\n    >>> plot_scores()\n    \"\"\"\n    df = pd.DataFrame(data)\n    df['Names'] = df['Names'].apply(lambda x: re.sub('(?<!\\\\\\\\d)\\\\\\\\.(?!\\\\\\\\d)', ' ', x))\n\n    df.plot(x='Names', y='Scores', kind='bar', legend=False)\n    plt.ylabel('Scores')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nre, os, glob, shutil\n```python\nimport re\nimport os\nimport glob\nimport shutil\n\n# Constants\nSOURCE_DIRECTORY = 'source_directory'\nTARGET_DIRECTORY = 'target_directory'\n\ndef copy_files_with_replacement(source_directory=SOURCE_DIRECTORY, target_directory=TARGET_DIRECTORY):\n    \"\"\"\n    Copy all files from a source directory to a target directory, replacing '.' \n    characters associated with ascii letters in the filenames with ' '.\n    \n    Parameters:\n    source_directory (str): Path to the source directory. Default is SOURCE_DIRECTORY constant.\n    target_directory (str): Path to the target directory. Default is TARGET_DIRECTORY constant.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - re\n    - os\n    - glob\n    - shutil\n    \n    Example:\n    >>> copy_files_with_replacement()\n    \"\"\"\n    for file in glob.glob(os.path.join(source_directory, '*')):\n        filename = os.path.basename(file)\n        filename = re.sub('(?<!\\\\\\\\d)\\\\\\\\.(?!\\\\\\\\d)', ' ', filename)\n        shutil.copy(file, os.path.join(target_directory, filename))\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nre, pandas, json\n```python\nimport re\nimport pandas as pd\nimport json\n\n# Constants\nJSON_FILE = 'path_to_your_json_file'\n\ndef read_json_and_replace(json_file=JSON_FILE):\n    \"\"\"\n    Read a JSON file into pandas DataFrame and replace '.' characters associated \n    with ascii letters in all string values with ' '.\n    \n    Parameters:\n    json_file (str): Path to the JSON file. Default is JSON_FILE constant.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with replaced string values.\n    \n    Requirements:\n    - re\n    - pandas\n    - json\n    \n    Example:\n    >>> df = read_json_and_replace()\n    >>> print(df)\n    \"\"\"\n    with open(json_file, 'r') as file:\n        data = json.load(file)\n\n    df = pd.json_normalize(data)\n\n    for column in df.columns:\n        if df[column].dtype == 'object':\n            df[column] = df[column].apply(lambda x: re.sub('(?<!\\\\\\\\d)\\\\\\\\.(?!\\\\\\\\d)', ' ', x) if isinstance(x, str) else x)\n\n    return df\n```"}
{"seed": "{\"task_id\": 38388799, \"prompt\": \"def f_38388799(list_of_strings):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"sorted(list_of_strings, key=lambda s: s.split(',')[1])\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(['parrot, medicine', 'abott, kangaroo', 'sriracha, coriander', 'phone, bottle']) == ['phone, bottle', 'sriracha, coriander', 'abott, kangaroo', 'parrot, medicine']\\n\", \"\\n    assert candidate(['abott, kangaroo', 'parrot, medicine', 'sriracha, coriander', 'phone, bottle']) == ['phone, bottle', 'sriracha, coriander', 'abott, kangaroo', 'parrot, medicine']\\n\"], \"entry_point\": \"f_38388799\", \"intent\": \"sort list `list_of_strings` based on second index of each string `s`\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, csv, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport csv\nimport matplotlib.pyplot as plt\n\n# Constants\nCSV_FILE = 'data.csv'\n\ndef sort_and_plot_csv_column(csv_file=CSV_FILE):\n    \"\"\"\n    Read a CSV file into a pandas DataFrame, sort based on the second column,\n    and plot the distribution of the sorted second column.\n    \n    Parameters:\n    csv_file (str): The path to the CSV file.\n    \n    Returns:\n    DataFrame: A pandas DataFrame sorted based on the second column.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - csv\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = sort_and_plot_csv_column('data.csv')\n    >>> print(df)\n    >>> df.iloc[:, 1].hist()\n    \"\"\"\n    df = pd.read_csv(csv_file)\n    sorted_df = df.sort_values(df.columns[1])\n    sorted_df.iloc[:, 1].hist()\n    \n    return sorted_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \npandas, numpy, csv, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Constants\nTEXT_DOCUMENTS = ['doc1.txt', 'doc2.txt', 'doc3.txt', 'doc4.txt']\n\ndef sort_and_compare_documents(documents=TEXT_DOCUMENTS):\n    \"\"\"\n    Read text documents into a list, create a TF-IDF matrix, \n    sort the documents based on the second term in the matrix, \n    and compute the pairwise cosine similarity.\n    \n    Parameters:\n    documents (list): The list of text documents.\n    \n    Returns:\n    array: The cosine similarity matrix.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.feature_extraction.text.TfidfVectorizer\n    - sklearn.metrics.pairwise.cosine_similarity\n    \n    Example:\n    >>> similarity_matrix = sort_and_compare_documents(['doc1.txt', 'doc2.txt', 'doc3.txt', 'doc4.txt'])\n    >>> print(similarity_matrix)\n    \"\"\"\n    corpus = []\n\n    for doc in documents:\n        with open(doc, 'r') as file:\n            corpus.append(file.read().replace('\\n', ''))\n            \n    vectorizer = TfidfVectorizer()\n    X = vectorizer.fit_transform(corpus)\n    terms = vectorizer.get_feature_names_out()\n    sorted_indices = np.argsort(terms)\n    sorted_X = X[:, sorted_indices]\n    \n    similarity_matrix = cosine_similarity(sorted_X)\n    \n    return similarity_matrix\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \ncollections, heapq, operator, csv\n```python\nimport collections\nimport heapq\nimport operator\nimport csv\n\n# Constants\nCSV_FILE = 'data.csv'\n\ndef sort_and_top_k_csv_column(csv_file=CSV_FILE, k=5):\n    \"\"\"\n    Read a CSV file into a dictionary, sort based on the second value of each item,\n    and get the top k items.\n    \n    Parameters:\n    csv_file (str): The path to the CSV file.\n    k (int): The number of top items to return.\n    \n    Returns:\n    list: A list of tuples representing the top k items.\n    \n    Requirements:\n    - collections\n    - heapq\n    - operator\n    - csv\n    \n    Example:\n    >>> top_items = sort_and_top_k_csv_column('data.csv', 5)\n    >>> print(top_items)\n    \"\"\"\n    with open(csv_file, mode ='r')as file:\n        csvFile = csv.reader(file)\n        my_dict = {rows[0]:rows[1] for rows in csvFile}\n        \n    sorted_dict = sorted(my_dict.items(), key=operator.itemgetter(1))\n    \n    top_items = heapq.nlargest(k, sorted_dict, key=operator.itemgetter(1))\n    \n    return top_items\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nos, hashlib, operator, csv\n```python\nimport os\nimport hashlib\nimport operator\nimport csv\n\n# Constants\nDIRECTORY = 'documents'\n\ndef sort_and_hash_files(directory=DIRECTORY):\n    \"\"\"\n    Read files in a directory, create a hash for each file, \n    and sort the files based on the second character of the hash.\n    \n    Parameters:\n    directory (str): The directory containing the files.\n    \n    Returns:\n    list: A list of tuples representing the sorted files and their hashes.\n    \n    Requirements:\n    - os\n    - hashlib\n    - operator\n    - csv\n    \n    Example:\n    >>> sorted_files = sort_and_hash_files('documents')\n    >>> print(sorted_files)\n    \"\"\"\n    files = os.listdir(directory)\n    \n    file_hashes = []\n    for file in files:\n        with open(os.path.join(directory, file), 'rb') as f:\n            bytes = f.read()\n            readable_hash = hashlib.sha256(bytes).hexdigest()\n            file_hashes.append((file, readable_hash))\n            \n    sorted_files = sorted(file_hashes, key=lambda x: x[1][1])\n    \n    return sorted_files\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \npandas, numpy, csv, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport csv\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Constants\nCSV_FILE = 'data.csv'\n\ndef sort_and_scale_csv_column(csv_file=CSV_FILE):\n    \"\"\"\n    Read a CSV file into a pandas DataFrame, sort based on the second column,\n    and scale the sorted second column to the range [0, 1].\n    \n    Parameters:\n    csv_file (str): The path to the CSV file.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the sorted and scaled second column.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - csv\n    - sklearn.preprocessing.MinMaxScaler\n    \n    Example:\n    >>> df = sort_and_scale_csv_column('data.csv')\n    >>> print(df)\n    \"\"\"\n    df = pd.read_csv(csv_file)\n    sorted_df = df.sort_values(df.columns[1])\n    \n    scaler = MinMaxScaler()\n    sorted_df[df.columns[1]] = scaler.fit_transform(sorted_df[df.columns[1]].values.reshape(-1,1))\n    \n    return sorted_df\n```\n"}
{"seed": "{\"task_id\": 37004138, \"prompt\": \"def f_37004138(lst):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"[element for element in lst if isinstance(element, int)]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    lst = [1, \\\"hello\\\", \\\"string\\\", 2, 4.46]\\n    assert candidate(lst) == [1, 2]\\n\", \"\\n    lst = [\\\"hello\\\", \\\"string\\\"]\\n    assert candidate(lst) == []\\n\"], \"entry_point\": \"f_37004138\", \"intent\": \"eliminate non-integer items from list `lst`\", \"library\": []}", "generation": "Scenario 1: \npandas, matplotlib, seaborn, numpy\n\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Constants\nDATA_COLUMNS = ['Age', 'Height', 'Weight', 'BMI', 'Blood Pressure']\n\ndef analyze_health_data(health_data):\n    \"\"\"\n    Analyze health data by eliminating non-numeric values, computing BMI,\n    and generating density plots for each health parameter.\n    \n    Parameters:\n    health_data (List[List[Union[int, str]]): The health data.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with cleaned health data.\n    \n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - seaborn\n    - numpy\n    \n    Example:\n    >>> data = [[25, '165', '65', '23.8', '120/80'], [30, 'NA', '70', 'NA', '130/85'], ['NA', '170', '75', '25.5', '140/90']]\n    >>> analyze_health_data(data)\n    \"\"\"\n    df = pd.DataFrame(health_data, columns=DATA_COLUMNS)\n    df = df.apply(pd.to_numeric, errors='coerce')  # Convert to numeric, set invalid parsing to NaN\n    df = df.dropna()  # Drop rows with NaN values\n    \n    # Compute BMI if not available\n    df.loc[pd.isnull(df['BMI']), 'BMI'] = df['Weight'] / np.square(df['Height']/100)\n    \n    # Generate density plots\n    for column in DATA_COLUMNS:\n        plt.figure(figsize=(8, 6))\n        sns.kdeplot(df[column], fill=True)\n        plt.title(f'Density Plot of {column}')\n        plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, sklearn.preprocessing, numpy\n\n```python\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nimport numpy as np\n\n# Constants\nCATEGORIES = ['fruit', 'color', 'size']\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess data by eliminating non-string items, encoding categorical features,\n    and replacing missing values with the mode (most frequent value).\n    \n    Parameters:\n    data (List[List[Union[int, str]]): The data to be preprocessed.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with preprocessed data.\n    \n    Requirements:\n    - pandas\n    - sklearn.preprocessing.LabelEncoder\n    - numpy\n    \n    Example:\n    >>> data = [['apple', 'red', 'small'], ['banana', 'yellow', 5], ['cherry', 'red', np.nan]]\n    >>> preprocess_data(data)\n    \"\"\"\n    df = pd.DataFrame(data, columns=CATEGORIES)\n    df = df.applymap(lambda x: x if isinstance(x, str) else np.nan)  # Eliminate non-string items\n\n    # Encode categorical features\n    le = LabelEncoder()\n    for column in CATEGORIES:\n        df[column] = le.fit_transform(df[column].astype(str))\n\n    # Replace missing values with the mode\n    for column in CATEGORIES:\n        df[column].fillna(df[column].mode()[0], inplace=True)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nmath, numpy, matplotlib\n\n```python\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nX_VALUES = np.linspace(-10, 10, 400)\n\ndef plot_math_functions(functions_list):\n    \"\"\"\n    Plot specified math functions on the same graph, excluding non-callable items.\n    \n    Parameters:\n    functions_list (List[Callable]): List of math function to plot.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - math\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> plot_math_functions([math.sin, math.cos, 'invalid'])\n    \"\"\"\n    for func in functions_list:\n        if callable(func):\n            y_values = [func(x) for x in X_VALUES]\n            plt.plot(X_VALUES, y_values, label=func.__name__)\n    \n    plt.legend()\n    plt.grid(True)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, scipy.stats\n\n```python\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\n\n# Constants\nDATA_COLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef process_statistics(data):\n    \"\"\"\n    Process data by eliminating non-numeric items, and then computing the mean,\n    median, mode, and standard deviation for each column.\n    \n    Parameters:\n    data (List[List[Union[int, str]]): The data to process.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with statistical data.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - scipy.stats\n    \n    Example:\n    >>> data = [[1, '2', 3, 4, 5], [6, 7, '8', 9, 10], [11, 12, 13, '14', 15]]\n    >>> process_statistics(data)\n    \"\"\"\n    df = pd.DataFrame(data, columns=DATA_COLUMNS)\n    df = df.apply(pd.to_numeric, errors='coerce')  # Convert to numeric, set invalid parsing to NaN\n    df = df.dropna()  # Drop rows with NaN values\n\n    stats_data = {\n        'Mean': df.mean(),\n        'Median': df.median(),\n        'Mode': df.mode().loc[0],\n        'Std Dev': df.std()\n    }\n    \n    stats_df = pd.DataFrame(stats_data)\n\n    return stats_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, pandas, numpy, matplotlib\n\n```python\nimport os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nDIRECTORY_PATH = './data_files/'\n\ndef process_directory_files():\n    \"\"\"\n    Process all CSV files in a specific directory by eliminating non-numeric items,\n    computing the mean of each column, and plotting a bar chart of the results.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - os\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> process_directory_files()\n    \"\"\"\n    file_names = os.listdir(DIRECTORY_PATH)\n\n    for file_name in file_names:\n        if file_name.endswith('.csv'):\n            df = pd.read_csv(os.path.join(DIRECTORY_PATH, file_name))\n            df = df.apply(pd.to_numeric, errors='coerce')  # Convert to numeric, set invalid parsing to NaN\n            df = df.dropna()  # Drop rows with NaN values\n            \n            means = df.mean()\n            means.plot(kind='bar', title=file_name)\n            plt.show()\n```\n"}
{"seed": "{\"task_id\": 37004138, \"prompt\": \"def f_37004138(lst):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"[element for element in lst if not isinstance(element, str)]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    lst = [1, \\\"hello\\\", \\\"string\\\", 2, 4.46]\\n    assert candidate(lst) == [1, 2, 4.46]\\n\", \"\\n    lst = [\\\"hello\\\", \\\"string\\\"]\\n    assert candidate(lst) == []\\n\"], \"entry_point\": \"f_37004138\", \"intent\": \"get all the elements except strings from the list 'lst'.\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, random, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import choice\nimport matplotlib.pyplot as plt\n\n# Constants\nCATEGORIES = ['Electronics', 'Fashion', 'Home & Kitchen', 'Books', 'Sports & Outdoors']\nCOUNTRIES = ['USA', 'UK', 'Germany', 'France', 'Canada']\n\ndef generate_sales_report():\n    \"\"\"\n    Generate a random sales report for different categories of products in various countries.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with sales data for the categories.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> report = generate_sales_report()\n    >>> print(report)\n    >>> report.plot(kind='bar', stacked=True)\n    \"\"\"\n    sales_data = []\n\n    for country in COUNTRIES:\n        for category in CATEGORIES:\n            sales = np.random.randint(low=100, high=1000)\n            sales_data.append([country, category, sales])\n\n    sales_df = pd.DataFrame(sales_data, columns=['Country', 'Category', 'Sales'])\n\n    return sales_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncollections, random\n```python\nfrom collections import Counter\nimport random\n\n# Constants\nLETTERS = 'abcdefghijklmnopqrstuvwxyz'\n\ndef generate_letter_counts(n):\n    \"\"\"\n    Generate a dictionary with counts of randomly generated letters.\n    \n    Parameters:\n    n (int): The number of random letters to generate.\n\n    Returns:\n    dict: A dictionary with letter counts.\n\n    Requirements:\n    - collections.Counter\n    - random\n\n    Example:\n    >>> counts = generate_letter_counts(1000)\n    >>> print(counts)\n    \"\"\"\n    letters = [random.choice(LETTERS) for _ in range(n)]\n    letter_counts = dict(Counter(letters))\n\n    return letter_counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, scipy.stats\n```python\nimport numpy as np\nfrom scipy.stats import ttest_ind\n\n# Constants\nGROUP_A = np.random.normal(loc=5.0, scale=1.0, size=100)\nGROUP_B = np.random.normal(loc=5.3, scale=1.0, size=100)\n\ndef perform_ttest():\n    \"\"\"\n    Perform a t-test on two randomly generated groups of data.\n\n    Returns:\n    float: The t-statistic and the p-value.\n\n    Requirements:\n    - numpy\n    - scipy.stats.ttest_ind\n\n    Example:\n    >>> t_stat, p_val = perform_ttest()\n    >>> print(f't-statistic: {t_stat}, p-value: {p_val}')\n    \"\"\"\n    t_stat, p_val = ttest_ind(GROUP_A, GROUP_B)\n\n    return t_stat, p_val\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, sklearn.preprocessing\n```python\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\n\n# Constants\nDATA = np.random.randint(low=0, high=100, size=(100, 5))\n\ndef standardize_data():\n    \"\"\"\n    Standardize a randomly generated numpy array using sklearn's StandardScaler.\n\n    Returns:\n    ndarray: The standardized numpy array.\n\n    Requirements:\n    - numpy\n    - sklearn.preprocessing.StandardScaler\n\n    Example:\n    >>> standardized_data = standardize_data()\n    >>> print(standardized_data)\n    \"\"\"\n    scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(DATA)\n\n    return standardized_data\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nmath, random\n```python\nimport math\nimport random\n\n# Constants\nRADIUS = 10\n\ndef estimate_pi(n):\n    \"\"\"\n    Estimate the value of pi using the Monte Carlo method.\n\n    Parameters:\n    n (int): The number of random points to generate.\n\n    Returns:\n    float: The estimated value of pi.\n\n    Requirements:\n    - math\n    - random\n\n    Example:\n    >>> pi_est = estimate_pi(1000000)\n    >>> print(pi_est)\n    \"\"\"\n    inside_circle = 0\n\n    for _ in range(n):\n        x = random.uniform(-RADIUS, RADIUS)\n        y = random.uniform(-RADIUS, RADIUS)\n\n        if math.sqrt(x**2 + y**2) <= RADIUS:\n            inside_circle += 1\n\n    pi_est = 4 * inside_circle / n\n\n    return pi_est\n```"}
{"seed": "{\"task_id\": 72899, \"prompt\": \"def f_72899(list_to_be_sorted):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"sorted(list_to_be_sorted, key=lambda k: k['name'])\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    list_to_be_sorted = [{'name': 'Homer', 'age': 39}, {'name': 'Bart', 'age': 10}]\\n    assert candidate(list_to_be_sorted) == [{'name': 'Bart', 'age': 10}, {'name': 'Homer', 'age': 39}]\\n\", \"\\n    list_to_be_sorted = [{'name': 'ABCD'}, {'name': 'AABCD'}]\\n    assert candidate(list_to_be_sorted) == [{'name': 'AABCD'}, {'name': 'ABCD'}]\\n\"], \"entry_point\": \"f_72899\", \"intent\": \"Sort a list of dictionaries `list_to_be_sorted` by the value of the dictionary key `name`\", \"library\": []}", "generation": "Scenario 1:\nnumpy, pandas, statistics, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nimport statistics\nimport matplotlib.pyplot as plt\n\n# Constants\nKEYS = ['name', 'age', 'country', 'score']\nCOUNTRIES = ['USA', 'Canada', 'UK', 'Australia', 'Germany']\n\ndef analyze_and_plot(list_of_dicts):\n    \"\"\"\n    Analyze a list of dictionaries and plot the distribution of scores by country.\n\n    Parameters:\n    list_of_dicts (list): A list of dictionaries.\n\n    Returns:\n    str: The country with the highest average score.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - statistics\n    - matplotlib.pyplot\n\n    Example:\n    >>> list_of_dicts = [{'name': 'Homer', 'age': 39, 'country': 'USA', 'score': 85},\n    ...                  {'name': 'Bart', 'age': 10, 'country': 'Canada', 'score': 90},\n    ...                  {'name': 'Lisa', 'age': 8, 'country': 'UK', 'score': 95}]\n    >>> analyze_and_plot(list_of_dicts)\n    'UK'\n    \"\"\"\n    df = pd.DataFrame(list_of_dicts)\n    df = df[df['country'].isin(COUNTRIES)]\n    scores = df.groupby('country')['score'].apply(list).to_dict()\n    avg_scores = {k: statistics.mean(v) for k, v in scores.items()}\n\n    plt.boxplot(scores.values(), labels=scores.keys())\n    plt.title('Score Distribution by Country')\n    plt.show()\n\n    highest_avg_country = max(avg_scores, key=avg_scores.get)\n\n    return highest_avg_country\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, random, matplotlib, statistics\n```python\nimport pandas as pd\nimport random\nimport matplotlib.pyplot as plt\nimport statistics\n\n# Constants\nKEYS = ['name', 'age', 'country', 'score']\nCOUNTRIES = ['USA', 'Canada', 'UK', 'Australia', 'Germany']\n\ndef find_median_and_plot_histogram(list_of_dicts, country):\n    \"\"\"\n    Find the median age in a specific country from a list of dictionaries and plot a histogram of the ages.\n\n    Parameters:\n    list_of_dicts (list): A list of dictionaries.\n    country (str): The country to analyze.\n\n    Returns:\n    float: The median age in the specified country.\n\n    Requirements:\n    - pandas\n    - random\n    - matplotlib.pyplot\n    - statistics\n\n    Example:\n    >>> list_of_dicts = [{'name': 'Homer', 'age': 39, 'country': 'USA', 'score': 85},\n    ...                  {'name': 'Bart', 'age': 10, 'country': 'Canada', 'score': 90},\n    ...                  {'name': 'Lisa', 'age': 8, 'country': 'UK', 'score': 95}]\n    >>> find_median_and_plot_histogram(list_of_dicts, 'USA')\n    39.0\n    \"\"\"\n    df = pd.DataFrame(list_of_dicts)\n    ages = df[df['country'] == country]['age'].tolist()\n\n    plt.hist(ages, bins=10, color='blue')\n    plt.title(f'Age Distribution in {country}')\n    plt.xlabel('Age')\n    plt.ylabel('Frequency')\n    plt.show()\n\n    median_age = statistics.median(ages)\n\n    return median_age\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, random, matplotlib, statistics, numpy\n```python\nimport pandas as pd\nimport random\nimport matplotlib.pyplot as plt\nimport statistics\nimport numpy as np\n\n# Constants\nKEYS = ['name', 'age', 'country', 'score']\nCOUNTRIES = ['USA', 'Canada', 'UK', 'Australia', 'Germany']\n\ndef find_std_and_plot_scatter(list_of_dicts, country):\n    \"\"\"\n    Find the standard deviation of scores in a specific country from a list of dictionaries \n    and plot a scatter plot of the ages against scores.\n\n    Parameters:\n    list_of_dicts (list): A list of dictionaries.\n    country (str): The country to analyze.\n\n    Returns:\n    float: The standard deviation of the scores in the specified country.\n\n    Requirements:\n    - pandas\n    - random\n    - matplotlib.pyplot\n    - statistics\n    - numpy\n\n    Example:\n    >>> list_of_dicts = [{'name': 'Homer', 'age': 39, 'country': 'USA', 'score': 85},\n    ...                  {'name': 'Bart', 'age': 10, 'country': 'Canada', 'score': 90},\n    ...                  {'name': 'Lisa', 'age': 8, 'country': 'UK', 'score': 95}]\n    >>> find_std_and_plot_scatter(list_of_dicts, 'USA')\n    nan\n    \"\"\"\n    df = pd.DataFrame(list_of_dicts)\n    data = df[df['country'] == country][['age', 'score']]\n\n    plt.scatter(data['age'], data['score'])\n    plt.title(f'Age vs Score in {country}')\n    plt.xlabel('Age')\n    plt.ylabel('Score')\n    plt.show()\n\n    std_dev = np.std(data['score'])\n\n    return std_dev\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, random, matplotlib, statistics\n```python\nimport pandas as pd\nimport random\nimport matplotlib.pyplot as plt\nimport statistics\n\n# Constants\nKEYS = ['name', 'age', 'country', 'score']\nCOUNTRIES = ['USA', 'Canada', 'UK', 'Australia', 'Germany']\n\ndef find_mode_and_plot_pie(list_of_dicts):\n    \"\"\"\n    Find the mode of the countries in a list of dictionaries \n    and plot a pie chart of the country distribution.\n\n    Parameters:\n    list_of_dicts (list): A list of dictionaries.\n\n    Returns:\n    str: The mode of the countries.\n\n    Requirements:\n    - pandas\n    - random\n    - matplotlib.pyplot\n    - statistics\n\n    Example:\n    >>> list_of_dicts = [{'name': 'Homer', 'age': 39, 'country': 'USA', 'score': 85},\n    ...                  {'name': 'Bart', 'age': 10, 'country': 'Canada', 'score': 90},\n    ...                  {'name': 'Lisa', 'age': 8, 'country': 'UK', 'score': 95}]\n    >>> find_mode_and_plot_pie(list_of_dicts)\n    'USA'\n    \"\"\"\n    df = pd.DataFrame(list_of_dicts)\n    countries = df['country'].tolist()\n\n    plt.pie([countries.count(c) for c in COUNTRIES], labels=COUNTRIES, autopct='%1.1f%%')\n    plt.title('Country Distribution')\n    plt.show()\n\n    mode_country = statistics.mode(countries)\n\n    return mode_country\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, random, matplotlib, statistics\n```python\nimport pandas as pd\nimport random\nimport matplotlib.pyplot as plt\nimport statistics\n\n# Constants\nKEYS = ['name', 'age', 'country', 'score']\nCOUNTRIES = ['USA', 'Canada', 'UK', 'Australia', 'Germany']\n\ndef find_range_and_plot_line(list_of_dicts, country):\n    \"\"\"\n    Find the range of ages in a specific country from a list of dictionaries \n    and plot a line graph of the ages.\n\n    Parameters:\n    list_of_dicts (list): A list of dictionaries.\n    country (str): The country to analyze.\n\n    Returns:\n    int: The range of ages in the specified country.\n\n    Requirements:\n    - pandas\n    - random\n    - matplotlib.pyplot\n    - statistics\n\n    Example:\n    >>> list_of_dicts = [{'name': 'Homer', 'age': 39, 'country': 'USA', 'score': 85},\n    ...                  {'name': 'Bart', 'age': 10, 'country': 'Canada', 'score': 90},\n    ...                  {'name': 'Lisa', 'age': 8, 'country': 'UK', 'score': 95}]\n    >>> find_range_and_plot_line(list_of_dicts, 'USA')\n    0\n    \"\"\"\n    df = pd.DataFrame(list_of_dicts)\n    ages = df[df['country'] == country]['age'].tolist()\n\n    plt.plot(sorted(ages))\n    plt.title(f'Age Distribution in {country}')\n    plt.xlabel('Index')\n    plt.ylabel('Age')\n    plt.show()\n\n    range_age = max(ages) - min(ages)\n\n    return range_age\n```\n"}
{"seed": "{\"task_id\": 72899, \"prompt\": \"def f_72899(l):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"sorted(l, key=itemgetter('name'), reverse=True)\", \"test_start\": \"\\nfrom operator import itemgetter\\n\\ndef check(candidate):\", \"test\": [\"\\n    list_to_be_sorted = [{'name': 'Homer', 'age': 39}, {'name': 'Bart', 'age': 10}]\\n    assert candidate(list_to_be_sorted) == [{'name': 'Homer', 'age': 39}, {'name': 'Bart', 'age': 10}]\\n\", \"\\n    list_to_be_sorted = [{'name': 'ABCD'}, {'name': 'AABCD'}]\\n    assert candidate(list_to_be_sorted) == [{'name': 'ABCD'}, {'name': 'AABCD'}]\\n\"], \"entry_point\": \"f_72899\", \"intent\": \"sort a list of dictionaries `l` by values in key `name` in descending order\", \"library\": [\"operator\"]}", "generation": "Scenario 1: \npandas, random, numpy, matplotlib, datetime\n```python\nimport pandas as pd\nfrom random import randint\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom datetime import datetime, timedelta\n\n# Constants\nNAMES = ['Homer', 'Bart', 'Lisa', 'Marge', 'Maggie']\nAGES = list(range(1, 100))\n\ndef generate_demographics(num_entries):\n    \"\"\"\n    Generate a dataset of demographics with random names and ages, and sort \n    them by age in descending order. Also, plot a histogram of the ages.\n\n    Parameters:\n    num_entries (int): The number of entries to generate.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the sorted demographics data.\n    \n    Requirements:\n    - pandas\n    - random\n    - numpy\n    - matplotlib.pyplot\n    - datetime\n    \n    Example:\n    >>> demographics = generate_demographics(100)\n    >>> print(demographics)\n    \"\"\"\n    demographics_data = []\n\n    for _ in range(num_entries):\n        name = NAMES[randint(0, len(NAMES)-1)]\n        age = AGES[randint(0, len(AGES)-1)]\n        demographics_data.append({'name': name, 'age': age})\n\n    demographics_df = pd.DataFrame(demographics_data)\n    sorted_demographics_df = demographics_df.sort_values('age', ascending=False)\n    \n    plt.hist(sorted_demographics_df['age'], bins=20, edgecolor='black')\n    plt.title('Age distribution')\n    plt.xlabel('Age')\n    plt.ylabel('Frequency')\n    plt.show()\n\n    return sorted_demographics_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, pandas, matplotlib, random\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nLETTERS = list('ABCDEFGHIJKLMNOPQRSTUVWXYZ')\nNUM_VALUES = list(range(1, 100))\n\ndef generate_and_sort_data(num_entries):\n    \"\"\"\n    Generate a dataset using random letters and numbers, sort them by the numeric \n    values in descending order, and plot a bar chart of the top 10 values.\n\n    Parameters:\n    num_entries (int): The number of entries to generate.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the sorted data.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - random\n    \n    Example:\n    >>> data = generate_and_sort_data(100)\n    >>> print(data)\n    \"\"\"\n    data = []\n\n    for _ in range(num_entries):\n        letter = LETTERS[randint(0, len(LETTERS)-1)]\n        num_value = NUM_VALUES[randint(0, len(NUM_VALUES)-1)]\n        data.append({'Letter': letter, 'Number': num_value})\n\n    data_df = pd.DataFrame(data)\n    sorted_data_df = data_df.sort_values('Number', ascending=False)\n\n    sorted_data_df.head(10).plot(kind='bar', x='Letter', y='Number', legend=False)\n    plt.title('Top 10 Values')\n    plt.ylabel('Number')\n    plt.tight_layout()\n    plt.show()\n\n    return sorted_data_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, seaborn, matplotlib, datetime, random\n```python\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom datetime import datetime, timedelta\nfrom random import randint\n\n# Constants\nPRODUCTS = ['TV', 'Laptop', 'Mobile', 'Headphone', 'Speaker']\nDATES = [datetime.today() - timedelta(days=i) for i in range(365)]\n\ndef generate_sales_report(num_entries):\n    \"\"\"\n    Generate a sales report with random products and dates, and sort them by \n    date in descending order. Also, plot a countplot of the sales per product.\n\n    Parameters:\n    num_entries (int): The number of entries to generate.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the sorted sales report.\n    \n    Requirements:\n    - pandas\n    - seaborn\n    - matplotlib.pyplot\n    - datetime\n    - random\n    \n    Example:\n    >>> sales_report = generate_sales_report(1000)\n    >>> print(sales_report)\n    \"\"\"\n    sales_data = []\n\n    for _ in range(num_entries):\n        product = PRODUCTS[randint(0, len(PRODUCTS)-1)]\n        date = DATES[randint(0, len(DATES)-1)]\n        sales_data.append({'Product': product, 'Date': date})\n\n    sales_df = pd.DataFrame(sales_data)\n    sorted_sales_df = sales_df.sort_values('Date', ascending=False)\n\n    plt.figure(figsize=(10,6))\n    sns.countplot(data=sorted_sales_df, x='Product', order=sorted_sales_df['Product'].value_counts().index)\n    plt.title('Product sales count')\n    plt.show()\n\n    return sorted_sales_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nrandom, pandas, matplotlib, numpy\n```python\nimport random\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nNAMES = ['John', 'Sue', 'Bob', 'Alice', 'Charlie', 'Eve']\n\ndef generate_and_plot_scores(num_entries):\n    \"\"\"\n    Generate a dataset of names and scores, sort them by score in descending \n    order, and plot a histogram of the scores.\n\n    Parameters:\n    num_entries (int): The number of entries to generate.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the sorted scores data.\n    \n    Requirements:\n    - random\n    - pandas\n    - matplotlib.pyplot\n    - numpy\n    \n    Example:\n    >>> scores = generate_and_plot_scores(100)\n    >>> print(scores)\n    \"\"\"\n    scores_data = []\n\n    for _ in range(num_entries):\n        name = random.choice(NAMES)\n        score = np.random.normal(70, 15)\n        scores_data.append({'Name': name, 'Score': score})\n\n    scores_df = pd.DataFrame(scores_data)\n    sorted_scores_df = scores_df.sort_values('Score', ascending=False)\n\n    plt.hist(sorted_scores_df['Score'], bins=20, edgecolor='black')\n    plt.title('Score distribution')\n    plt.xlabel('Score')\n    plt.ylabel('Frequency')\n    plt.show()\n\n    return sorted_scores_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, random, matplotlib, datetime\n```python\nimport pandas as pd\nfrom random import choice\nimport matplotlib.pyplot as plt\nfrom datetime import datetime, timedelta\n\n# Constants\nCOUNTRIES = ['USA', 'China', 'India', 'Russia', 'Germany', 'France']\nDATES = [datetime.today() - timedelta(days=i) for i in range(365)]\n\ndef generate_and_sort_data(num_entries):\n    \"\"\"\n    Generate a dataset of countries and dates, sort them by date in descending \n    order, and plot a pie chart of the count of entries per country.\n\n    Parameters:\n    num_entries (int): The number of entries to generate.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the sorted data.\n    \n    Requirements:\n    - pandas\n    - random\n    - matplotlib.pyplot\n    - datetime\n    \n    Example:\n    >>> data = generate_and_sort_data(1000)\n    >>> print(data)\n    \"\"\"\n    data = []\n\n    for _ in range(num_entries):\n        country = choice(COUNTRIES)\n        date = choice(DATES)\n        data.append({'Country': country, 'Date': date})\n\n    data_df = pd.DataFrame(data)\n    sorted_data_df = data_df.sort_values('Date', ascending=False)\n\n    sorted_data_df['Country'].value_counts().plot(kind='pie', autopct='%1.1f%%')\n    plt.title('Country distribution')\n    plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n    plt.show()\n\n    return sorted_data_df\n```\n"}
{"seed": "{\"task_id\": 72899, \"prompt\": \"def f_72899(list_of_dicts):\\n\\t\", \"suffix\": \"\\n\\treturn list_of_dicts\", \"canonical_solution\": \"list_of_dicts.sort(key=operator.itemgetter('name'))\", \"test_start\": \"\\nimport operator\\n\\ndef check(candidate):\", \"test\": [\"\\n    list_to_be_sorted = [{'name': 'Homer', 'age': 39}, {'name': 'Bart', 'age': 10}]\\n    assert candidate(list_to_be_sorted) == [{'name': 'Bart', 'age': 10}, {'name': 'Homer', 'age': 39}]\\n\", \"\\n    list_to_be_sorted = [{'name': 'ABCD'}, {'name': 'AABCD'}]\\n    assert candidate(list_to_be_sorted) == [{'name': 'AABCD'}, {'name': 'ABCD'}]\\n\"], \"entry_point\": \"f_72899\", \"intent\": \"sort a list of dictionaries `list_of_dicts` by `name` values of the dictionary\", \"library\": [\"operator\"]}", "generation": "Scenario 1:\noperator, datetime, random, pandas\n```python\nimport operator\nfrom datetime import datetime, timedelta\nfrom random import randint\nimport pandas as pd\n\n# Constants\nNAMES = [\"Homer\", \"Bart\", \"Marge\", \"Lisa\", \"Maggie\"]\nAGES = range(1, 101)\n\ndef generate_sort_family():\n    \"\"\"\n    Generate a list of dictionaries each containing family member's name, age and birthdate \n    and then sort the list by name and birthdate.\n\n    Returns:\n    DataFrame: A pandas DataFrame sorted by 'Name' and 'Birthdate'.\n\n    Requirements:\n    - operator\n    - datetime\n    - random\n    - pandas\n\n    Example:\n    >>> family_df = generate_sort_family()\n    >>> print(family_df)\n    \"\"\"\n    list_of_dicts = []\n\n    for name in NAMES:\n        age = randint(AGES[0], AGES[-1])\n        birthdate = datetime.now() - timedelta(days=365*age)\n        list_of_dicts.append({'Name': name, 'Age': age, 'Birthdate': birthdate})\n\n    list_of_dicts.sort(key=operator.itemgetter('Name', 'Birthdate'))\n    family_df = pd.DataFrame(list_of_dicts)\n\n    return family_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\noperator, datetime, random, pandas\n```python\nimport operator\nfrom datetime import datetime, timedelta\nfrom random import randint\nimport pandas as pd\n\n# Constants\nPRODUCTS = [\"Apple\", \"Banana\", \"Cherry\", \"Date\", \"Elderberry\"]\nQUANTITIES = range(1, 101)\n\ndef generate_sort_inventory():\n    \"\"\"\n    Generate a list of dictionaries each containing product name, quantity and restocking date\n    and then sort the list by name and restocking date.\n\n    Returns:\n    DataFrame: A pandas DataFrame sorted by 'Product' and 'Restocking Date'.\n\n    Requirements:\n    - operator\n    - datetime\n    - random\n    - pandas\n\n    Example:\n    >>> inventory_df = generate_sort_inventory()\n    >>> print(inventory_df)\n    \"\"\"\n    list_of_dicts = []\n\n    for product in PRODUCTS:\n        quantity = randint(QUANTITIES[0], QUANTITIES[-1])\n        restocking_date = datetime.now() + timedelta(days=randint(1, 30))\n        list_of_dicts.append({'Product': product, 'Quantity': quantity, 'Restocking Date': restocking_date})\n\n    list_of_dicts.sort(key=operator.itemgetter('Product', 'Restocking Date'))\n    inventory_df = pd.DataFrame(list_of_dicts)\n\n    return inventory_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\noperator, random, pandas\n```python\nimport operator\nfrom random import choice\nimport pandas as pd\n\n# Constants\nCITIES = [\"New York\", \"London\", \"Paris\", \"Tokyo\", \"Sydney\"]\nWEATHER = [\"Sunny\", \"Cloudy\", \"Rainy\", \"Snowy\", \"Stormy\"]\n\ndef generate_sort_weather():\n    \"\"\"\n    Generate a list of dictionaries each containing city name and weather condition\n    and then sort the list by city name.\n\n    Returns:\n    DataFrame: A pandas DataFrame sorted by 'City'.\n\n    Requirements:\n    - operator\n    - random\n    - pandas\n\n    Example:\n    >>> weather_df = generate_sort_weather()\n    >>> print(weather_df)\n    \"\"\"\n    list_of_dicts = []\n\n    for city in CITIES:\n        weather_condition = choice(WEATHER)\n        list_of_dicts.append({'City': city, 'Weather': weather_condition})\n\n    list_of_dicts.sort(key=operator.itemgetter('City'))\n    weather_df = pd.DataFrame(list_of_dicts)\n\n    return weather_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\noperator, random, pandas\n```python\nimport operator\nfrom random import choice\nimport pandas as pd\n\n# Constants\nSTUDENTS = [\"Adam\", \"Barbara\", \"Charlie\", \"Diana\", \"Ethan\"]\nGRADES = [\"A\", \"B\", \"C\", \"D\", \"F\"]\n\ndef generate_sort_grades():\n    \"\"\"\n    Generate a list of dictionaries each containing student name and grade\n    and then sort the list by student name.\n\n    Returns:\n    DataFrame: A pandas DataFrame sorted by 'Student'.\n\n    Requirements:\n    - operator\n    - random\n    - pandas\n\n    Example:\n    >>> grades_df = generate_sort_grades()\n    >>> print(grades_df)\n    \"\"\"\n    list_of_dicts = []\n\n    for student in STUDENTS:\n        grade = choice(GRADES)\n        list_of_dicts.append({'Student': student, 'Grade': grade})\n\n    list_of_dicts.sort(key=operator.itemgetter('Student'))\n    grades_df = pd.DataFrame(list_of_dicts)\n\n    return grades_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\noperator, random, pandas\n```python\nimport operator\nfrom random import choice\nimport pandas as pd\n\n# Constants\nPLAYERS = [\"Alice\", \"Bob\", \"Charlie\", \"Dave\", \"Eve\"]\nSCORES = range(0, 101)\n\ndef generate_sort_scores():\n    \"\"\"\n    Generate a list of dictionaries each containing player name and score\n    and then sort the list by player name.\n\n    Returns:\n    DataFrame: A pandas DataFrame sorted by 'Player'.\n\n    Requirements:\n    - operator\n    - random\n    - pandas\n\n    Example:\n    >>> scores_df = generate_sort_scores()\n    >>> print(scores_df)\n    \"\"\"\n    list_of_dicts = []\n\n    for player in PLAYERS:\n        score = choice(SCORES)\n        list_of_dicts.append({'Player': player, 'Score': score})\n\n    list_of_dicts.sort(key=operator.itemgetter('Player'))\n    scores_df = pd.DataFrame(list_of_dicts)\n\n    return scores_df\n```"}
{"seed": "{\"task_id\": 72899, \"prompt\": \"def f_72899(list_of_dicts):\\n\\t\", \"suffix\": \"\\n\\treturn list_of_dicts\", \"canonical_solution\": \"list_of_dicts.sort(key=operator.itemgetter('age'))\", \"test_start\": \"\\nimport operator\\n\\ndef check(candidate):\", \"test\": [\"\\n    list_to_be_sorted = [{'name': 'Homer', 'age': 39}, {'name': 'Bart', 'age': 10}]\\n    assert candidate(list_to_be_sorted) == [{'name': 'Bart', 'age': 10}, {'name': 'Homer', 'age': 39}]\\n\", \"\\n    list_to_be_sorted = [{'name': 'ABCD', 'age': 10}, {'name': 'AABCD', 'age': 9}]\\n    assert candidate(list_to_be_sorted) == [{'name': 'AABCD', 'age': 9}, {'name': 'ABCD', 'age': 10}]\\n\"], \"entry_point\": \"f_72899\", \"intent\": \"sort a list of dictionaries `list_of_dicts` by `age` values of the dictionary\", \"library\": [\"operator\"]}", "generation": "Scenario 1: \nnumpy, scipy.stats, operator, pandas\n```python\nimport numpy as np\nfrom scipy import stats\nimport operator\nimport pandas as pd\n\n# Constants\nFIELDS = ['Name', 'Age', 'Height', 'Weight']\n\ndef calculate_body_mass_index(list_of_dicts):\n    \"\"\"\n    Calculate the Body Mass Index (BMI) for a list of individuals and return the list sorted by BMI.\n    The input is a list of dictionaries where each dictionary represents an individual with the fields: \n    'Name', 'Age', 'Height' (in meters), 'Weight' (in kilograms). The function adds a new field 'BMI' \n    to each dictionary and sorts the list by this field in ascending order.\n\n    Parameters:\n    list_of_dicts (list): A list of dictionaries representing individuals.\n\n    Returns:\n    list: A list of dictionaries sorted by 'BMI'.\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - operator\n    - pandas\n\n    Example:\n    >>> list_of_people = [{'Name': 'John', 'Age': 25, 'Height': 1.75, 'Weight': 70},\n                          {'Name': 'Jane', 'Age': 28, 'Height': 1.62, 'Weight': 60}]\n    >>> calculate_body_mass_index(list_of_people)\n    \"\"\"\n    for person in list_of_dicts:\n        person['BMI'] = person['Weight'] / (person['Height'] ** 2)\n\n    list_of_dicts.sort(key=operator.itemgetter('BMI'))\n\n    return list_of_dicts\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\noperator, pandas, numpy, seaborn, matplotlib.pyplot\n```python\nimport operator\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nFIELDS = ['Name', 'Age', 'Score']\n\ndef analyze_scores(list_of_dicts):\n    \"\"\"\n    Analyze the scores of a list of students and create a histogram of the scores.\n    The input is a list of dictionaries where each dictionary represents a student with the fields: 'Name', 'Age', 'Score'.\n\n    Parameters:\n    list_of_dicts (list): A list of dictionaries representing students.\n\n    Returns:\n    None\n\n    Requirements:\n    - operator\n    - pandas\n    - numpy\n    - seaborn\n    - matplotlib.pyplot\n\n    Example:\n    >>> list_of_students = [{'Name': 'John', 'Age': 18, 'Score': 85},\n                            {'Name': 'Jane', 'Age': 19, 'Score': 90}]\n    >>> analyze_scores(list_of_students)\n    \"\"\"\n    # Convert the list of dictionaries to a pandas DataFrame\n    df = pd.DataFrame(list_of_dicts)\n\n    # Plot a histogram of the scores\n    sns.histplot(df['Score'], bins=10, kde=False)\n    plt.title('Distribution of Scores')\n    plt.xlabel('Score')\n    plt.ylabel('Frequency')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\noperator, pandas, numpy, datetime\n```python\nimport operator\nimport pandas as pd\nimport numpy as np\nfrom datetime import date\n\n# Constants\nFIELDS = ['Name', 'DOB', 'Score']\n\ndef calculate_age_and_average_score(list_of_dicts):\n    \"\"\"\n    Calculate the current age and average score for a list of students.\n    The input is a list of dictionaries where each dictionary represents a student with the fields: 'Name', 'DOB', 'Score'.\n    The function adds a new field 'Age' to each dictionary and calculates the average score.\n\n    Parameters:\n    list_of_dicts (list): A list of dictionaries representing students.\n\n    Returns:\n    tuple: A tuple containing the updated list of dictionaries and the average score.\n\n    Requirements:\n    - operator\n    - pandas\n    - numpy\n    - datetime\n\n    Example:\n    >>> list_of_students = [{'Name': 'John', 'DOB': date(2003, 5, 20), 'Score': 85},\n                            {'Name': 'Jane', 'DOB': date(2004, 7, 15), 'Score': 90}]\n    >>> calculate_age_and_average_score(list_of_students)\n    \"\"\"\n    today = date.today()\n\n    for person in list_of_dicts:\n        person['Age'] = today.year - person['DOB'].year - (\n            (today.month, today.day) < (person['DOB'].month, person['DOB'].day))\n\n    average_score = np.mean([person['Score'] for person in list_of_dicts])\n\n    return list_of_dicts, average_score\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\noperator, pandas, numpy, matplotlib.pyplot\n```python\nimport operator\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nFIELDS = ['Name', 'Salary', 'Position']\n\ndef plot_salary_by_position(list_of_dicts):\n    \"\"\"\n    Plot a bar chart of average salary by position for a list of employees.\n    The input is a list of dictionaries where each dictionary represents an employee with the fields: 'Name', 'Salary', 'Position'.\n\n    Parameters:\n    list_of_dicts (list): A list of dictionaries representing employees.\n\n    Returns:\n    None\n\n    Requirements:\n    - operator\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> list_of_employees = [{'Name': 'John', 'Salary': 50000, 'Position': 'Manager'},\n                             {'Name': 'Jane', 'Salary': 60000, 'Position': 'Engineer'}]\n    >>> plot_salary_by_position(list_of_employees)\n    \"\"\"\n    # Convert the list of dictionaries to a pandas DataFrame\n    df = pd.DataFrame(list_of_dicts)\n\n    # Calculate average salary by position\n    avg_salary_by_position = df.groupby('Position')['Salary'].mean()\n\n    # Plot a bar chart\n    avg_salary_by_position.plot(kind='bar')\n    plt.title('Average Salary by Position')\n    plt.xlabel('Position')\n    plt.ylabel('Average Salary')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\noperator, pandas, numpy, random\n```python\nimport operator\nimport pandas as pd\nimport numpy as np\nimport random\n\n# Constants\nFIELDS = ['Name', 'Age', 'Score']\n\ndef add_random_scores(list_of_dicts, n):\n    \"\"\"\n    Add n random scores to each student in a list of dictionaries representing students.\n    The input is a list of dictionaries where each dictionary represents a student with the fields: 'Name', 'Age', 'Score'.\n\n    Parameters:\n    list_of_dicts (list): A list of dictionaries representing students.\n    n (int): The number of random scores to add.\n\n    Returns:\n    list: A list of dictionaries with added scores.\n\n    Requirements:\n    - operator\n    - pandas\n    - numpy\n    - random\n\n    Example:\n    >>> list_of_students = [{'Name': 'John', 'Age': 18, 'Score': 85},\n                            {'Name': 'Jane', 'Age': 19, 'Score': 90}]\n    >>> add_random_scores(list_of_students, 2)\n    \"\"\"\n    for person in list_of_dicts:\n        for i in range(n):\n            person[f'Score_{i+1}'] = random.randint(0, 100)\n\n    return list_of_dicts\n```\n"}
{"seed": "{\"task_id\": 36402748, \"prompt\": \"def f_36402748(df):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"df.groupby('prots').sum().sort_values('scores', ascending=False)\", \"test_start\": \"\\nimport pandas as pd \\n\\ndef check(candidate):\", \"test\": [\"\\n    COLUMN_NAMES = [\\\"chemicals\\\", \\\"prots\\\", \\\"scores\\\"]\\n    data = [[\\\"chemical1\\\", \\\"prot1\\\", 100],[\\\"chemical2\\\", \\\"prot2\\\", 50],[\\\"chemical3\\\", \\\"prot1\\\", 120]]\\n    df = pd.DataFrame(data, columns = COLUMN_NAMES)\\n    assert candidate(df).to_dict() == {'scores': {'prot1': 220, 'prot2': 50}}\\n\"], \"entry_point\": \"f_36402748\", \"intent\": \"sort a Dataframe `df` by the total ocurrences in a column 'scores' group by 'prots'\", \"library\": [\"pandas\"]}", "generation": "Scenario 1:\npandas, matplotlib, random\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\n\n# Constants\nCHEMICALS = ['Chemical1', 'Chemical2', 'Chemical3', 'Chemical4', 'Chemical5']\nPROTS = ['Prot1', 'Prot2', 'Prot3', 'Prot4', 'Prot5']\n\ndef generate_and_plot_data(df):\n    \"\"\"\n    Generate a DataFrame with random data, group by 'prots', sum the 'scores', sort the results, \n    and plot the data.\n    \n    Parameters:\n    df (pandas.DataFrame): The DataFrame to which the generated data should be appended.\n    \n    Returns:\n    pandas.DataFrame: The DataFrame with the generated data.\n    \n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - random\n    \n    Example:\n    >>> df = pd.DataFrame(columns=['chemicals', 'prots', 'scores'])\n    >>> df = generate_and_plot_data(df)\n    >>> print(df)\n    \"\"\"\n    # Generate data\n    for _ in range(100):\n        chemical = random.choice(CHEMICALS)\n        prot = random.choice(PROTS)\n        score = random.randint(0, 100)\n        df = df.append({'chemicals': chemical, 'prots': prot, 'scores': score}, ignore_index=True)\n\n    # Group, sum, and sort\n    df = df.groupby('prots').sum().sort_values('scores', ascending=False)\n\n    # Plot\n    df.plot(kind='bar', y='scores')\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n\ndef find_and_plot_correlation(df):\n    \"\"\"\n    Find the correlation between the 'scores' of different 'prots', and plot a heatmap of the correlation matrix.\n    \n    Parameters:\n    df (pandas.DataFrame): The DataFrame with the data.\n    \n    Returns:\n    pandas.DataFrame: The DataFrame with the correlation matrix.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - seaborn\n    \n    Example:\n    >>> df = pd.DataFrame({\n    ...     'chemicals': ['Chemical1', 'Chemical2', 'Chemical3', 'Chemical4', 'Chemical5'],\n    ...     'prots': ['Prot1', 'Prot2', 'Prot3', 'Prot4', 'Prot5'],\n    ...     'scores': [100, 200, 150, 50, 75]\n    ... })\n    >>> correlation_matrix = find_and_plot_correlation(df)\n    >>> print(correlation_matrix)\n    \"\"\"\n    # Pivot the DataFrame\n    df_pivot = df.pivot(index='chemicals', columns='prots', values='scores')\n\n    # Replace NaN values with 0\n    df_pivot = df_pivot.replace(np.nan, 0)\n\n    # Find the correlation\n    correlation_matrix = df_pivot.corr()\n\n    # Plot the heatmap\n    sns.heatmap(correlation_matrix)\n\n    return correlation_matrix\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, statistics, matplotlib\n```python\nimport pandas as pd\nimport statistics\nimport matplotlib.pyplot as plt\n\ndef calculate_and_plot_statistics(df):\n    \"\"\"\n    Calculate the mean and standard deviation of the 'scores' for each 'prot', and plot the results.\n    \n    Parameters:\n    df (pandas.DataFrame): The DataFrame with the data.\n    \n    Returns:\n    pandas.DataFrame: The DataFrame with the mean and standard deviation.\n    \n    Requirements:\n    - pandas\n    - statistics\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = pd.DataFrame({\n    ...     'chemicals': ['Chemical1', 'Chemical2', 'Chemical3', 'Chemical4', 'Chemical5'],\n    ...     'prots': ['Prot1', 'Prot2', 'Prot3', 'Prot4', 'Prot5'],\n    ...     'scores': [100, 200, 150, 50, 75]\n    ... })\n    >>> stats_df = calculate_and_plot_statistics(df)\n    >>> print(stats_df)\n    \"\"\"\n    mean_scores = df.groupby('prots')['scores'].mean()\n    std_dev_scores = df.groupby('prots')['scores'].apply(statistics.stdev)\n\n    stats_df = pd.DataFrame({\n        'Mean Scores': mean_scores,\n        'Standard Deviation Scores': std_dev_scores\n    })\n\n    stats_df.plot(kind='bar')\n\n    return stats_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, seaborn, sklearn.preprocessing\n```python\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef normalize_and_plot_distribution(df):\n    \"\"\"\n    Normalize the 'scores' column using Min-Max scaling, and plot the distribution of the normalized scores.\n    \n    Parameters:\n    df (pandas.DataFrame): The DataFrame with the data.\n    \n    Returns:\n    pandas.DataFrame: The DataFrame with the normalized scores.\n    \n    Requirements:\n    - pandas\n    - seaborn\n    - sklearn.preprocessing.MinMaxScaler\n    \n    Example:\n    >>> df = pd.DataFrame({\n    ...     'chemicals': ['Chemical1', 'Chemical2', 'Chemical3', 'Chemical4', 'Chemical5'],\n    ...     'prots': ['Prot1', 'Prot2', 'Prot3', 'Prot4', 'Prot5'],\n    ...     'scores': [100, 200, 150, 50, 75]\n    ... })\n    >>> normalized_df = normalize_and_plot_distribution(df)\n    >>> print(normalized_df)\n    \"\"\"\n    scaler = MinMaxScaler()\n\n    df['Normalized Scores'] = scaler.fit_transform(df[['scores']])\n\n    sns.distplot(df['Normalized Scores'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, sklearn.cluster, matplotlib\n```python\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\ndef cluster_and_plot(df):\n    \"\"\"\n    Cluster the 'prots' based on their 'scores' using K-means clustering, and plot the results.\n    \n    Parameters:\n    df (pandas.DataFrame): The DataFrame with the data.\n    \n    Returns:\n    pandas.DataFrame: The DataFrame with the cluster labels.\n    \n    Requirements:\n    - pandas\n    - sklearn.cluster.KMeans\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = pd.DataFrame({\n    ...     'chemicals': ['Chemical1', 'Chemical2', 'Chemical3', 'Chemical4', 'Chemical5'],\n    ...     'prots': ['Prot1', 'Prot2', 'Prot3', 'Prot4', 'Prot5'],\n    ...     'scores': [100, 200, 150, 50, 75]\n    ... })\n    >>> clustered_df = cluster_and_plot(df)\n    >>> print(clustered_df)\n    \"\"\"\n    kmeans = KMeans(n_clusters=3)\n\n    df['Cluster'] = kmeans.fit_predict(df[['scores']])\n\n    df.plot(kind='scatter', x='prots', y='scores', c='Cluster', colormap='viridis')\n\n    return df\n```\n"}
{"seed": "{\"task_id\": 29881993, \"prompt\": \"def f_29881993(trans):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"\\\"\\\"\\\",\\\"\\\"\\\".join(trans['category'])\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    trans = {'category':[\\\"hello\\\", \\\"world\\\",\\\"test\\\"], 'dummy_key':[\\\"dummy_val\\\"]}\\n    assert candidate(trans) == \\\"hello,world,test\\\"\\n\"], \"entry_point\": \"f_29881993\", \"intent\": \"join together with \\\",\\\" elements inside a list indexed with 'category' within a dictionary `trans`\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, sklearn, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nimport matplotlib.pyplot as plt\n\n# Constants\nCATEGORIES = ['fruit', 'vegetable', 'meat', 'dairy', 'grain']\n\ndef analyze_categories(trans):\n    \"\"\"\n    Analyze a dictionary of transactions, encoding the category labels, calculating the \n    frequency of each category, and plotting the distribution.\n\n    Parameters:\n    trans (dict): The transaction dictionary with 'category' key.\n\n    Returns:\n    DataFrame: A pandas DataFrame with encoded category labels and their frequency.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.preprocessing\n    - matplotlib.pyplot\n\n    Example:\n    >>> trans = {'category': ['fruit', 'vegetable', 'meat', 'fruit', 'dairy', 'grain', 'fruit']}\n    >>> df = analyze_categories(trans)\n    >>> print(df)\n    >>> df['Frequency'].plot(kind='bar')\n    \"\"\"\n    df = pd.DataFrame(trans)\n    le = LabelEncoder()\n    df['Encoded Category'] = le.fit_transform(df['category'])\n    category_frequency = df['Encoded Category'].value_counts().reset_index()\n    category_frequency.columns = ['Encoded Category', 'Frequency']\n    return category_frequency\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, seaborn, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nPRODUCTS = ['Apple', 'Banana', 'Carrot', 'Chicken', 'Milk', 'Bread']\n\ndef product_sales(trans):\n    \"\"\"\n    Analyze a dictionary of transactions, calculate the sales of each product, \n    and plot the distribution.\n\n    Parameters:\n    trans (dict): The transaction dictionary with 'product' and 'sales' keys.\n\n    Returns:\n    DataFrame: A pandas DataFrame with product sales.\n\n    Requirements:\n    - pandas\n    - numpy\n    - seaborn\n    - matplotlib.pyplot\n\n    Example:\n    >>> trans = {'product': ['Apple', 'Banana', 'Carrot', 'Chicken', 'Milk', 'Bread'], 'sales': [10, 15, 7, 12, 18, 20]}\n    >>> df = product_sales(trans)\n    >>> print(df)\n    >>> sns.barplot(x='Product', y='Sales', data=df)\n    \"\"\"\n    df = pd.DataFrame(trans)\n    df.columns = ['Product', 'Sales']\n    df.sort_values(by='Sales', inplace=True)\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, sklearn, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['category', 'price', 'quantity']\n\ndef analyze_transactions(trans):\n    \"\"\"\n    Analyze a dictionary of transactions, standardize the price and quantity features, \n    and plot the distribution.\n\n    Parameters:\n    trans (dict): The transaction dictionary with 'category', 'price', and 'quantity' keys.\n\n    Returns:\n    DataFrame: A pandas DataFrame with standardized features.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.preprocessing\n    - matplotlib.pyplot\n\n    Example:\n    >>> trans = {'category': ['fruit', 'vegetable', 'meat'], 'price': [1.5, 2.0, 3.0], 'quantity': [10, 15, 20]}\n    >>> df = analyze_transactions(trans)\n    >>> print(df)\n    >>> df[['Price', 'Quantity']].plot(kind='bar')\n    \"\"\"\n    df = pd.DataFrame(trans)\n    scaler = StandardScaler()\n    df[['price', 'quantity']] = scaler.fit_transform(df[['price', 'quantity']])\n    df.columns = ['Category', 'Price', 'Quantity']\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, seaborn, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nCATEGORIES = ['fruit', 'vegetable', 'meat', 'dairy', 'grain']\n\ndef analyze_category_sales(trans):\n    \"\"\"\n    Analyze a dictionary of transactions, calculate the total sales per category, \n    and plot the distribution.\n\n    Parameters:\n    trans (dict): The transaction dictionary with 'category' and 'sales' keys.\n\n    Returns:\n    DataFrame: A pandas DataFrame with category sales.\n\n    Requirements:\n    - pandas\n    - numpy\n    - seaborn\n    - matplotlib.pyplot\n\n    Example:\n    >>> trans = {'category': ['fruit', 'vegetable', 'meat', 'fruit', 'dairy', 'grain'], 'sales': [10, 15, 7, 12, 18, 20]}\n    >>> df = analyze_category_sales(trans)\n    >>> print(df)\n    >>> sns.barplot(x='Category', y='Sales', data=df)\n    \"\"\"\n    df = pd.DataFrame(trans)\n    df.columns = ['Category', 'Sales']\n    category_sales = df.groupby('Category')['Sales'].sum().reset_index()\n    return category_sales\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, scipy, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import zscore\nimport matplotlib.pyplot as plt\n\n# Constants\nCATEGORIES = ['fruit', 'vegetable', 'meat', 'dairy', 'grain']\n\ndef detect_outliers(trans):\n    \"\"\"\n    Analyze a dictionary of transactions, calculate the z-score for sales, \n    and detect any outliers.\n\n    Parameters:\n    trans (dict): The transaction dictionary with 'category' and 'sales' keys.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the z-score for sales.\n\n    Requirements:\n    - pandas\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Example:\n    >>> trans = {'category': ['fruit', 'vegetable', 'meat', 'fruit', 'dairy', 'grain'], 'sales': [10, 15, 7, 50, 18, 20]}\n    >>> df = detect_outliers(trans)\n    >>> print(df)\n    >>> df['Z-Score'].plot(kind='bar')\n    \"\"\"\n    df = pd.DataFrame(trans)\n    df.columns = ['Category', 'Sales']\n    df['Z-Score'] = zscore(df['Sales'])\n    return df\n```\n"}
{"seed": "{\"task_id\": 34158494, \"prompt\": \"def f_34158494():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"\\\"\\\"\\\"\\\"\\\"\\\".join(['A', 'B', 'C', 'D'])\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate() == 'ABCD'\\n\"], \"entry_point\": \"f_34158494\", \"intent\": \"concatenate array of strings `['A', 'B', 'C', 'D']` into a string\", \"library\": []}", "generation": "Scenario 1:\nos, sys, string, random\n```python\nimport os\nimport sys\nimport string\nimport random\n\n# Constants\nLETTERS = list(string.ascii_uppercase)\nFILE_NAME = 'file.txt'\n\ndef generate_and_read_file(letter_count):\n    \"\"\"\n    Generate a file with random letters and then read the file to return the \n    concatenated string of letters.\n\n    Parameters:\n    letter_count (int): The number of random letters to generate.\n\n    Returns:\n    str: The concatenated string of letters.\n\n    Requirements:\n    - os\n    - sys\n    - string\n    - random\n\n    Example:\n    >>> generate_and_read_file(10)\n    \"\"\"\n    with open(FILE_NAME, 'w') as f:\n        for _ in range(letter_count):\n            f.write(random.choice(LETTERS))\n    \n    with open(FILE_NAME, 'r') as f:\n        letters = f.read()\n\n    return letters\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, string\n```python\nimport numpy as np\nimport string\n\n# Constants\nLETTERS = list(string.ascii_uppercase)\n\ndef concatenate_letters(letter_count):\n    \"\"\"\n    Generate a numpy array of random letters and concatenate them into a \n    string.\n\n    Parameters:\n    letter_count (int): The number of random letters to generate.\n\n    Returns:\n    str: The concatenated string of letters.\n\n    Requirements:\n    - numpy\n    - string\n\n    Example:\n    >>> concatenate_letters(10)\n    \"\"\"\n    letters_array = np.random.choice(LETTERS, size=letter_count)\n    letters_str = \"\".join(letters_array)\n\n    return letters_str\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nrandom, string, collections\n```python\nimport random\nimport string\nfrom collections import Counter\n\n# Constants\nLETTERS = list(string.ascii_uppercase)\n\ndef generate_letter_counts(letter_count):\n    \"\"\"\n    Generate a dictionary of letter counts from a list of random letters.\n\n    Parameters:\n    letter_count (int): The number of random letters to generate.\n\n    Returns:\n    dict: A dictionary of letter counts.\n\n    Requirements:\n    - random\n    - string\n    - collections\n\n    Example:\n    >>> generate_letter_counts(10)\n    \"\"\"\n    letters = [random.choice(LETTERS) for _ in range(letter_count)]\n    letter_counts = Counter(letters)\n\n    return dict(letter_counts)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nrandom, string, pandas\n```python\nimport random\nimport string\nimport pandas as pd\n\n# Constants\nLETTERS = list(string.ascii_uppercase)\n\ndef generate_letter_dataframe(letter_count):\n    \"\"\"\n    Generate a pandas DataFrame with random letters and their ASCII values.\n\n    Parameters:\n    letter_count (int): The number of random letters to generate.\n\n    Returns:\n    DataFrame: A pandas DataFrame with random letters and their ASCII values.\n\n    Requirements:\n    - random\n    - string\n    - pandas\n\n    Example:\n    >>> generate_letter_dataframe(10)\n    \"\"\"\n    letters = [random.choice(LETTERS) for _ in range(letter_count)]\n    ascii_values = [ord(letter) for letter in letters]\n\n    df = pd.DataFrame({\"Letter\": letters, \"ASCII Value\": ascii_values})\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nrandom, string, collections, pandas\n```python\nimport random\nimport string\nfrom collections import Counter\nimport pandas as pd\n\n# Constants\nLETTERS = list(string.ascii_uppercase)\n\ndef generate_letter_count_dataframe(letter_count):\n    \"\"\"\n    Generate a pandas DataFrame with random letters and their counts.\n\n    Parameters:\n    letter_count (int): The number of random letters to generate.\n\n    Returns:\n    DataFrame: A pandas DataFrame with random letters and their counts.\n\n    Requirements:\n    - random\n    - string\n    - collections\n    - pandas\n\n    Example:\n    >>> generate_letter_count_dataframe(10)\n    \"\"\"\n    letters = [random.choice(LETTERS) for _ in range(letter_count)]\n    letter_counts = Counter(letters)\n\n    df = pd.DataFrame.from_dict(letter_counts, orient='index').reset_index()\n    df.columns = ['Letter', 'Count']\n\n    return df\n```"}
{"seed": "{\"task_id\": 12666897, \"prompt\": \"def f_12666897(sents):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"[x for x in sents if not x.startswith('@$\\\\t') and not x.startswith('#')]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    sents = [\\\"@$\\tabcd\\\", \\\"#453923\\\", \\\"abcd\\\", \\\"hello\\\", \\\"1\\\"]\\n    assert candidate(sents) == [\\\"abcd\\\", \\\"hello\\\", \\\"1\\\"]\\n\", \"\\n    sents = [\\\"@$\\tabcd\\\", \\\"@$t453923\\\", \\\"abcd\\\", \\\"hello\\\", \\\"1\\\"]\\n    assert candidate(sents) == [\\\"@$t453923\\\", \\\"abcd\\\", \\\"hello\\\", \\\"1\\\"]\\n\", \"\\n    sents = [\\\"#tabcd\\\", \\\"##453923\\\", \\\"abcd\\\", \\\"hello\\\", \\\"1\\\"]\\n    assert candidate(sents) == [\\\"abcd\\\", \\\"hello\\\", \\\"1\\\"] \\n\"], \"entry_point\": \"f_12666897\", \"intent\": \"Remove all strings from a list a strings `sents` where the values starts with `@$\\\\t` or `#`\", \"library\": []}", "generation": "Scenario 1:\npandas, re, matplotlib.pyplot, numpy, csv\n```python\nimport pandas as pd\nimport re\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport csv\n\n# Constants\nINVALID_START_CHARACTERS = [\"@$\\\\t\", \"#\"]\n\ndef clean_and_visualize_csv_data(csv_file):\n    \"\"\"\n    Remove all strings from a csv file where the values of a specific column start\n    with certain characters. Then, visualize the cleaned data.\n\n    Parameters:\n    csv_file (str): The path to the csv file.\n\n    Returns:\n    DataFrame: The cleaned pandas DataFrame.\n\n    Requirements:\n    - pandas\n    - re\n    - matplotlib.pyplot\n    - numpy\n    - csv\n\n    Example:\n    >>> clean_and_visualize_csv_data('data.csv')\n    \"\"\"\n    df = pd.read_csv(csv_file)\n\n    for col in df.columns:\n        df = df[~df[col].astype(str).str.startswith(tuple(INVALID_START_CHARACTERS))]\n\n    df.hist(figsize=(10,10))\n    plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnltk, re, sklearn, matplotlib.pyplot\n```python\nfrom nltk.corpus import stopwords\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\n\nSTOPWORDS = set(stopwords.words('english'))\nINVALID_START_CHARACTERS = [\"@$\\\\t\", \"#\"]\n\ndef clean_and_visualize_text_data(texts):\n    \"\"\"\n    Remove all strings from a list of texts where the words start\n    with certain characters. Then, visualize the cleaned data.\n\n    Parameters:\n    texts (list): The list of texts.\n\n    Returns:\n    list: The cleaned list of texts.\n\n    Requirements:\n    - nltk\n    - re\n    - sklearn\n    - matplotlib.pyplot\n\n    Example:\n    >>> clean_and_visualize_text_data(['This is a test', '#Not a test', '@$\\\\tAnother test'])\n    \"\"\"\n    cleaned_texts = []\n    for text in texts:\n        words = text.split()\n        cleaned_words = [word for word in words if not word.startswith(tuple(INVALID_START_CHARACTERS)) and word not in STOPWORDS]\n        cleaned_texts.append(' '.join(cleaned_words))\n        \n    cv = CountVectorizer()\n    word_count_vector = cv.fit_transform(cleaned_texts)\n    \n    sum_words = word_count_vector.sum(axis=0)\n    words_freq = [(word, sum_words[0, idx]) for word, idx in cv.vocabulary_.items()]\n    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n    \n    plt.bar(*zip(*words_freq[:10]))\n    plt.show()\n    \n    return cleaned_texts\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnltk, re, wordcloud\n```python\nfrom nltk.corpus import stopwords\nimport re\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\nSTOPWORDS = set(stopwords.words('english'))\nINVALID_START_CHARACTERS = [\"@$\\\\t\", \"#\"]\n\ndef clean_text_and_generate_wordcloud(texts):\n    \"\"\"\n    Remove all strings from a list of texts where the words start\n    with certain characters. Then, generate a wordcloud of the cleaned data.\n\n    Parameters:\n    texts (list): The list of texts.\n\n    Returns:\n    None\n\n    Requirements:\n    - nltk\n    - re\n    - wordcloud\n\n    Example:\n    >>> clean_text_and_generate_wordcloud(['This is a test', '#Not a test', '@$\\\\tAnother test'])\n    \"\"\"\n    cleaned_texts = []\n    for text in texts:\n        words = text.split()\n        cleaned_words = [word for word in words if not word.startswith(tuple(INVALID_START_CHARACTERS)) and word not in STOPWORDS]\n        cleaned_texts.append(' '.join(cleaned_words))\n\n    wordcloud = WordCloud().generate(' '.join(cleaned_texts))\n\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis(\"off\")\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, re, matplotlib.pyplot\n```python\nimport pandas as pd\nimport re\nimport matplotlib.pyplot as plt\n\nINVALID_START_CHARACTERS = [\"@$\\\\t\", \"#\"]\n\ndef clean_and_visualize_dataframe(df):\n    \"\"\"\n    Remove all strings from a DataFrame where the values start\n    with certain characters. Then, visualize the cleaned data.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n\n    Returns:\n    DataFrame: The cleaned pandas DataFrame.\n\n    Requirements:\n    - pandas\n    - re\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame({'A': ['test', '#not a test', '@$\\\\tAnother test'], 'B': ['test', 'not a test', 'Another test']})\n    >>> clean_and_visualize_dataframe(df)\n    \"\"\"\n    for col in df.columns:\n        df = df[~df[col].astype(str).str.startswith(tuple(INVALID_START_CHARACTERS))]\n\n    df.hist(figsize=(10,10))\n    plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnltk, re, sklearn.feature_extraction.text\n```python\nfrom nltk.corpus import stopwords\nimport re\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nSTOPWORDS = set(stopwords.words('english'))\nINVALID_START_CHARACTERS = [\"@$\\\\t\", \"#\"]\n\ndef clean_and_vectorize_text_data(texts):\n    \"\"\"\n    Remove all strings from a list of texts where the words start\n    with certain characters. Then, vectorize the cleaned data.\n\n    Parameters:\n    texts (list): The list of texts.\n\n    Returns:\n    array: The TF-IDF vectorized representation of the cleaned texts.\n\n    Requirements:\n    - nltk\n    - re\n    - sklearn.feature_extraction.text\n\n    Example:\n    >>> clean_and_vectorize_text_data(['This is a test', '#Not a test', '@$\\\\tAnother test'])\n    \"\"\"\n    cleaned_texts = []\n    for text in texts:\n        words = text.split()\n        cleaned_words = [word for word in words if not word.startswith(tuple(INVALID_START_CHARACTERS)) and word not in STOPWORDS]\n        cleaned_texts.append(' '.join(cleaned_words))\n        \n    tfidf = TfidfVectorizer()\n    vectors = tfidf.fit_transform(cleaned_texts).toarray()\n    \n    return vectors\n```"}
{"seed": "{\"task_id\": 5944630, \"prompt\": \"def f_5944630(list):\\n\\t\", \"suffix\": \"\\n\\treturn list\", \"canonical_solution\": \"list.sort(key=lambda item: (item['points'], item['time']))\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    list = [\\n        {'name':'JOHN', 'points' : 30, 'time' : '0:02:2'},\\n        {'name':'KARL','points':50,'time': '0:03:00'},\\n        {'name':'TEST','points':20,'time': '0:03:00'}\\n    ]\\n    assert candidate(list) == [\\n        {'name':'TEST','points':20,'time': '0:03:00'}, \\n        {'name':'JOHN', 'points' : 30, 'time' : '0:02:2'},\\n        {'name':'KARL','points':50,'time': '0:03:00'}\\n    ]\\n\", \"\\n    list = [\\n        {'name':'JOHN', 'points' : 30, 'time' : '0:02:2'},\\n        {'name':'KARL','points':30,'time': '0:03:00'},\\n        {'name':'TEST','points':30,'time': '0:01:01'}\\n    ]\\n    assert candidate(list) == [\\n        {'name':'TEST','points':30,'time': '0:01:01'},\\n        {'name':'JOHN', 'points' : 30, 'time' : '0:02:2'},\\n        {'name':'KARL','points':30,'time': '0:03:00'}\\n    ]\\n\"], \"entry_point\": \"f_5944630\", \"intent\": \"sort a list of dictionary `list` first by key `points` and then by `time`\", \"library\": []}", "generation": "Scenario 1:\npandas, datetime, time, operator\n```python\nimport pandas as pd\nfrom datetime import datetime\nimport time\nimport operator\n\n# Constants\nFORMAT = '%H:%M:%S'\n\ndef sort_players_by_points_and_time(players):\n    \"\"\"\n    Sort a list of player dictionaries first by points (in descending order) and then by time (in ascending order).\n    Time is converted from a string to seconds for accurate comparison.\n    \n    Parameters:\n    players (list): A list of dictionaries where each dictionary represents a player with keys 'name', 'points', and 'time'.\n    \n    Returns:\n    list: A sorted list of player dictionaries.\n    \n    Requirements:\n    - pandas\n    - datetime\n    - time\n    - operator\n    \n    Example:\n    >>> players = [{'name':'John', 'points' : 30, 'time' : '0:02:2'},\n                   {'name':'Karl','points':50,'time': '0:03:00'},\n                   {'name':'Test','points':20,'time': '0:03:00'}]\n    >>> sort_players_by_points_and_time(players)\n    \"\"\"\n    for player in players:\n        t = time.strptime(player['time'], FORMAT)\n        player['time'] = datetime.fromtimestamp(time.mktime(t))\n\n    players.sort(key=operator.itemgetter('points', 'time'), reverse=True)\n\n    for player in players:\n        player['time'] = time.strftime(FORMAT, time.gmtime(player['time'].second))\n\n    return players\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, datetime, time, operator\n```python\nimport pandas as pd\nfrom datetime import datetime, timedelta\nimport time\nimport operator\n\n# Constants\nFORMAT = '%H:%M:%S'\n\ndef calculate_and_sort_by_total_time(players):\n    \"\"\"\n    Add a 'total_time' key to each player dictionary which represents the total time elapsed \n    since the player started playing (based on a fixed start time) and sort the list by this new key.\n    \n    Parameters:\n    players (list): A list of dictionaries where each dictionary represents a player with keys 'name', 'points', and 'time'.\n    \n    Returns:\n    list: A sorted list of player dictionaries.\n    \n    Requirements:\n    - pandas\n    - datetime\n    - time\n    - operator\n    \n    Example:\n    >>> players = [{'name':'John', 'points' : 30, 'time' : '0:02:2'},\n                   {'name':'Karl','points':50,'time': '0:03:00'},\n                   {'name':'Test','points':20,'time': '0:03:00'}]\n    >>> calculate_and_sort_by_total_time(players)\n    \"\"\"\n    start_time = datetime.now()\n\n    for player in players:\n        t = time.strptime(player['time'], FORMAT)\n        player['time'] = datetime.fromtimestamp(time.mktime(t))\n        player['total_time'] = (start_time - player['time']).total_seconds()\n\n    players.sort(key=operator.itemgetter('total_time'))\n\n    for player in players:\n        player['time'] = time.strftime(FORMAT, time.gmtime(player['time'].second))\n\n    return players\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ndatetime, time, operator, numpy\n```python\nimport numpy as np\nfrom datetime import datetime\nimport time\nimport operator\n\n# Constants\nFORMAT = '%H:%M:%S'\nPOINTS_STD = 10\n\ndef normalize_and_sort_players_by_points(players):\n    \"\"\"\n    Normalize the 'points' key in each player dictionary using a standard deviation and sort the list by this new key\n    \n    Parameters:\n    players (list): A list of dictionaries where each dictionary represents a player with keys 'name', 'points', and 'time'.\n    \n    Returns:\n    list: A sorted list of player dictionaries.\n    \n    Requirements:\n    - datetime\n    - time\n    - operator\n    - numpy\n    \n    Example:\n    >>> players = [{'name':'John', 'points' : 30, 'time' : '0:02:2'},\n                   {'name':'Karl','points':50,'time': '0:03:00'},\n                   {'name':'Test','points':20,'time': '0:03:00'}]\n    >>> normalize_and_sort_players_by_points(players)\n    \"\"\"\n    points = [player['points'] for player in players]\n    mean_points = np.mean(points)\n    std_points = np.std(points)\n    normalized_points = (points - mean_points) / std_points\n\n    for i, player in enumerate(players):\n        player['points'] = normalized_points[i]\n\n    players.sort(key=operator.itemgetter('points'))\n\n    return players\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ndatetime, time, operator, numpy, collections\n```python\nimport numpy as np\nfrom datetime import datetime\nimport time\nimport operator\nfrom collections import Counter\n\n# Constants\nFORMAT = '%H:%M:%S'\nPOINTS_STD = 10\n\ndef calculate_and_sort_players_by_score_frequency(players):\n    \"\"\"\n    Add a 'frequency' key to each player dictionary which represents the frequency of the player's score in the list \n    and sort the list by this new key in descending order.\n    \n    Parameters:\n    players (list): A list of dictionaries where each dictionary represents a player with keys 'name', 'points', and 'time'.\n    \n    Returns:\n    list: A sorted list of player dictionaries.\n    \n    Requirements:\n    - datetime\n    - time\n    - operator\n    - numpy\n    - collections\n    \n    Example:\n    >>> players = [{'name':'John', 'points' : 30, 'time' : '0:02:2'},\n                   {'name':'Karl','points':30,'time': '0:03:00'},\n                   {'name':'Test','points':20,'time': '0:03:00'}]\n    >>> calculate_and_sort_players_by_score_frequency(players)\n    \"\"\"\n    points = [player['points'] for player in players]\n    points_frequency = Counter(points)\n\n    for player in players:\n        player['frequency'] = points_frequency[player['points']]\n\n    players.sort(key=operator.itemgetter('frequency'), reverse=True)\n\n    return players\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\ndatetime, time, operator, numpy, collections\n```python\nimport numpy as np\nfrom datetime import datetime\nimport time\nimport operator\nfrom collections import Counter\n\n# Constants\nFORMAT = '%H:%M:%S'\nPOINTS_STD = 10\n\ndef calculate_and_sort_players_by_time_and_points(players):\n    \"\"\"\n    Add a 'score' key to each player dictionary which represents the sum of the player's points and the total time elapsed \n    since the player started playing (based on a fixed start time) and sort the list by this new key.\n    \n    Parameters:\n    players (list): A list of dictionaries where each dictionary represents a player with keys 'name', 'points', and 'time'.\n    \n    Returns:\n    list: A sorted list of player dictionaries.\n    \n    Requirements:\n    - datetime\n    - time\n    - operator\n    - numpy\n    - collections\n    \n    Example:\n    >>> players = [{'name':'John', 'points' : 30, 'time' : '0:02:2'},\n                   {'name':'Karl','points':50,'time': '0:03:00'},\n                   {'name':'Test','points':20,'time': '0:03:00'}]\n    >>> calculate_and_sort_players_by_time_and_points(players)\n    \"\"\"\n    start_time = datetime.now()\n\n    for player in players:\n        t = time.strptime(player['time'], FORMAT)\n        player['time'] = datetime.fromtimestamp(time.mktime(t))\n        player['score'] = player['points'] + (start_time - player['time']).total_seconds()\n\n    players.sort(key=operator.itemgetter('score'))\n\n    return players\n```\n`GPT_ODEX_BREAK`"}
{"seed": "{\"task_id\": 7852855, \"prompt\": \"def f_7852855():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"datetime.datetime(1970, 1, 1).second\", \"test_start\": \"\\nimport time\\nimport datetime\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate() == 0\\n\"], \"entry_point\": \"f_7852855\", \"intent\": \"convert datetime object `(1970, 1, 1)` to seconds\", \"library\": [\"datetime\", \"time\"]}", "generation": "Scenario 1: \npandas, datetime, random, matplotlib.pyplot, seaborn\n```python\nimport pandas as pd\nfrom datetime import datetime\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nPRODUCT_CATEGORIES = ['Electronics', 'Fashion', 'Home & Kitchen', 'Books', 'Toys & Games']\nMONTHS = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n\ndef generate_sales_report(year):\n    \"\"\"\n    Generate a report of sales for various product categories over a year.\n    \n    Parameters:\n    year (int): The year for which the report is to be generated.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with sales data for the product categories.\n    \n    Requirements:\n    - pandas\n    - datetime\n    - random\n    - matplotlib.pyplot\n    - seaborn\n    \n    Example:\n    >>> report = generate_sales_report(2020)\n    >>> print(report)\n    >>> sns.lineplot(data=report, x=\"Month\", y=\"Sales\", hue=\"Product Category\")\n    \"\"\"\n    sales_data = []\n\n    for month in MONTHS:\n        for category in PRODUCT_CATEGORIES:\n            sales = randint(1000, 5000)\n            sales_data.append([datetime(year, MONTHS.index(month)+1, 1), category, sales])\n\n    sales_df = pd.DataFrame(sales_data, columns=['Date', 'Product Category', 'Sales'])\n\n    return sales_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ndatetime, numpy, matplotlib.pyplot\n```python\nfrom datetime import datetime, timedelta\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nSTART_DATE = datetime(2020, 1, 1)\nEND_DATE = datetime(2020, 12, 31)\nAVERAGE_TEMPERATURE = 25\nTEMPERATURE_VARIATION = 10\n\ndef plot_temperature_trend():\n    \"\"\"\n    Generate and plot a random temperature trend for a year.\n\n    Requirements:\n    - datetime\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_temperature_trend()\n    \"\"\"\n    total_days = (END_DATE - START_DATE).days + 1\n    dates = np.array([START_DATE + timedelta(days=i) for i in range(total_days)])\n    temperatures = np.random.normal(AVERAGE_TEMPERATURE, TEMPERATURE_VARIATION, total_days)\n\n    plt.figure(figsize=(10, 5))\n    plt.plot(dates, temperatures)\n    plt.xlabel('Date')\n    plt.ylabel('Temperature (\u00b0C)')\n    plt.title('Temperature Trend')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ndatetime, numpy, pandas, matplotlib.pyplot\n```python\nfrom datetime import datetime\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nPERIODS = 100\nFREQUENCY = 'D'\n\ndef simulate_stock_price():\n    \"\"\"\n    Simulate and plot a stock price using a simple random walk.\n\n    Requirements:\n    - datetime\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> simulate_stock_price()\n    \"\"\"\n    dates = pd.date_range(datetime.today().strftime('%Y-%m-%d'), periods=PERIODS, freq=FREQUENCY)\n    returns = np.random.normal(0, 1, PERIODS)\n    price = pd.Series(returns).cumsum()\n    price.plot()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ndatetime, numpy, pandas, seaborn\n```python\nfrom datetime import datetime, timedelta\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n# Constants\nSTART_DATE = datetime(2020, 1, 1)\nEND_DATE = datetime(2020, 12, 31)\n\ndef generate_and_plot_daily_data():\n    \"\"\"\n    Generate and plot daily data for a year.\n\n    Requirements:\n    - datetime\n    - numpy\n    - pandas\n    - seaborn\n\n    Example:\n    >>> generate_and_plot_daily_data()\n    \"\"\"\n    total_days = (END_DATE - START_DATE).days + 1\n    dates = pd.date_range(START_DATE, periods=total_days)\n    data = np.random.randint(0, 100, total_days)\n\n    df = pd.DataFrame({'Date': dates, 'Data': data})\n    plt.figure(figsize=(10, 5))\n    sns.lineplot(data=df, x=\"Date\", y=\"Data\")\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\ndatetime, pytz, time, random\n```python\nfrom datetime import datetime\nimport pytz\nimport time\nimport random\n\n# Constants\nTIMEZONES = pytz.all_timezones\n\ndef get_random_timezone_datetime():\n    \"\"\"\n    Get the current datetime in a randomly selected timezone.\n\n    Returns:\n    datetime: The current datetime in the selected timezone.\n\n    Requirements:\n    - datetime\n    - pytz\n    - time\n    - random\n\n    Example:\n    >>> print(get_random_timezone_datetime())\n    \"\"\"\n    selected_timezone = random.choice(TIMEZONES)\n    tz = pytz.timezone(selected_timezone)\n    tz_datetime = datetime.now(tz)\n\n    return tz_datetime\n```"}
{"seed": "{\"task_id\": 2763750, \"prompt\": \"def f_2763750():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"re.sub('(\\\\\\\\_a)?\\\\\\\\.([^\\\\\\\\.]*)$', '_suff.\\\\\\\\2', 'long.file.name.jpg')\", \"test_start\": \"\\nimport re \\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate() == 'long.file.name_suff.jpg'\\n\"], \"entry_point\": \"f_2763750\", \"intent\": \"insert `_suff` before the file extension in `long.file.name.jpg` or replace `_a` with `suff` if it precedes the extension.\", \"library\": [\"re\"]}", "generation": "Scenario 1: \nos, re, glob, pathlib\n```python\nimport os\nimport re\nimport glob\nfrom pathlib import Path\n\n# Constants\nFILE_PATTERN = '*.jpg'\nSUFFIX = '_suff'\n\ndef rename_files_in_directory(directory):\n    \"\"\"\n    Rename all files in a directory that match a pattern by inserting a suffix before the file extension, \n    or replacing '_a' with the suffix if it precedes the extension.\n\n    Parameters:\n    directory (str): The path to the directory.\n\n    Returns:\n    list: List of new file names.\n\n    Requirements:\n    - os\n    - re\n    - glob\n    - pathlib.Path\n\n    Example:\n    >>> rename_files_in_directory('/path/to/directory')\n    \"\"\"\n    os.chdir(directory)\n    files = glob.glob(FILE_PATTERN)\n    new_files = []\n\n    for file in files:\n        new_file = re.sub('(\\\\_a)?\\\\.([^\\\\.]*)$', '_suff.\\\\2', file)\n        os.rename(file, new_file)\n        new_files.append(new_file)\n\n    return new_files\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nre, os, glob, shutil\n```python\nimport re\nimport os\nimport glob\nimport shutil\n\n# Constants\nFILE_PATTERN = '*.jpg'\nSUFFIX = '_suff'\n\ndef move_files_with_suffix(directory_from, directory_to):\n    \"\"\"\n    Move all files with a certain suffix from one directory to another.\n\n    Parameters:\n    directory_from (str): The path to the source directory.\n    directory_to (str): The path to the target directory.\n\n    Returns:\n    list: List of moved file names.\n\n    Requirements:\n    - re\n    - os\n    - glob\n    - shutil\n\n    Example:\n    >>> move_files_with_suffix('/path/from', '/path/to')\n    \"\"\"\n    os.chdir(directory_from)\n    files = glob.glob(FILE_PATTERN)\n    moved_files = []\n\n    for file in files:\n        if re.search(SUFFIX, file):\n            shutil.move(file, directory_to)\n            moved_files.append(file)\n\n    return moved_files\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nre, os, glob, mimetypes\n```python\nimport re\nimport os\nimport glob\nimport mimetypes\n\n# Constants\nFILE_PATTERN = '*'\nSUFFIX = '_suff'\n\ndef check_file_types(directory):\n    \"\"\"\n    Check the file types of all files with a certain suffix in a directory.\n\n    Parameters:\n    directory (str): The path to the directory.\n\n    Returns:\n    dict: Dictionary with file names as keys and file types as values.\n\n    Requirements:\n    - re\n    - os\n    - glob\n    - mimetypes\n\n    Example:\n    >>> check_file_types('/path/to/directory')\n    \"\"\"\n    os.chdir(directory)\n    files = glob.glob(FILE_PATTERN)\n    file_types = {}\n\n    for file in files:\n        if re.search(SUFFIX, file):\n            file_type = mimetypes.guess_type(file)[0]\n            file_types[file] = file_type\n\n    return file_types\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nre, os, glob, hashlib\n```python\nimport re\nimport os\nimport glob\nimport hashlib\n\n# Constants\nFILE_PATTERN = '*'\nSUFFIX = '_suff'\n\ndef calculate_file_hashes(directory):\n    \"\"\"\n    Calculate the MD5 hashes of all files with a certain suffix in a directory.\n\n    Parameters:\n    directory (str): The path to the directory.\n\n    Returns:\n    dict: Dictionary with file names as keys and file hashes as values.\n\n    Requirements:\n    - re\n    - os\n    - glob\n    - hashlib\n\n    Example:\n    >>> calculate_file_hashes('/path/to/directory')\n    \"\"\"\n    os.chdir(directory)\n    files = glob.glob(FILE_PATTERN)\n    file_hashes = {}\n\n    for file in files:\n        if re.search(SUFFIX, file):\n            with open(file, 'rb') as f:\n                bytes = f.read()\n                file_hash = hashlib.md5(bytes).hexdigest()\n                file_hashes[file] = file_hash\n\n    return file_hashes\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nre, os, glob, zipfile\n```python\nimport re\nimport os\nimport glob\nimport zipfile\n\n# Constants\nFILE_PATTERN = '*'\nSUFFIX = '_suff'\n\ndef compress_files_with_suffix(directory):\n    \"\"\"\n    Compress all files with a certain suffix in a directory into a zip file.\n\n    Parameters:\n    directory (str): The path to the directory.\n\n    Returns:\n    str: The name of the zip file.\n\n    Requirements:\n    - re\n    - os\n    - glob\n    - zipfile\n\n    Example:\n    >>> compress_files_with_suffix('/path/to/directory')\n    \"\"\"\n    os.chdir(directory)\n    files = glob.glob(FILE_PATTERN)\n    zip_file = f'{directory}.zip'\n\n    with zipfile.ZipFile(zip_file, 'w') as zipf:\n        for file in files:\n            if re.search(SUFFIX, file):\n                zipf.write(file)\n\n    return zip_file\n```\n"}
{"seed": "{\"task_id\": 6420361, \"prompt\": \"def f_6420361(module):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"imp.reload(module)\", \"test_start\": \"\\nimport imp\\nfrom unittest.mock import Mock\\n\\ndef check(candidate):\", \"test\": [\"\\n    imp.reload = Mock()\\n    try:\\n        candidate('ads')\\n        assert True\\n    except:\\n        assert False\\n\"], \"entry_point\": \"f_6420361\", \"intent\": \"reload a module `module`\", \"library\": [\"imp\"]}", "generation": "Scenario 1:\nimp, os, sys, subprocess\n```python\nimport imp\nimport os\nimport sys\nimport subprocess\n\ndef reload_and_run(module_name):\n    \"\"\"\n    Reload a module and run a function `main` from the module if it exists.\n    \n    Parameters:\n    module_name (str): The name of the module.\n    \n    Returns:\n    str: The output of the `main` function if it exists, otherwise an empty string.\n    \n    Requirements:\n    - imp\n    - os\n    - sys\n    - subprocess\n    \n    Example:\n    >>> reload_and_run('my_module')\n    \"\"\"\n    # Reload the module\n    module = sys.modules.get(module_name)\n    if module is not None:\n        imp.reload(module)\n\n    # Check if the main function exists\n    main_func = getattr(module, 'main', None)\n    if main_func is not None:\n        return subprocess.check_output([sys.executable, '-c', 'import {}; {}.main()'.format(module_name, module_name)])\n    \n    return ''\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nimp, inspect, argparse, shlex\n```python\nimport imp\nimport inspect\nimport argparse\nimport shlex\n\ndef reload_and_inspect(module_name, function_name):\n    \"\"\"\n    Reload a module and inspect a function from the module.\n    \n    Parameters:\n    module_name (str): The name of the module.\n    function_name (str): The name of the function to inspect.\n    \n    Returns:\n    dict: A dictionary containing the function's args and defaults.\n    \n    Requirements:\n    - imp\n    - inspect\n    - argparse\n    - shlex\n    \n    Example:\n    >>> reload_and_inspect('my_module', 'my_function')\n    \"\"\"\n    # Reload the module\n    module = imp.reload(sys.modules[module_name])\n    \n    # Get the function\n    function = getattr(module, function_name)\n\n    # Get the function's args and defaults\n    args, _, _, defaults = inspect.getargspec(function)\n    defaults = defaults if defaults else []\n    \n    return {\n        'args': args[:-len(defaults)] if defaults else args,\n        'default_args': dict(zip(args[-len(defaults):], defaults)) if defaults else {}\n    }\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nimp, pkgutil, os, sys\n```python\nimport imp\nimport pkgutil\nimport os\nimport sys\n\ndef reload_all_modules_in_package(package_name):\n    \"\"\"\n    Reload all modules in a package.\n    \n    Parameters:\n    package_name (str): The name of the package.\n    \n    Returns:\n    None\n\n    Requirements:\n    - imp\n    - pkgutil\n    - os\n    - sys\n    \n    Example:\n    >>> reload_all_modules_in_package('my_package')\n    \"\"\"\n    package = sys.modules[package_name]\n    for _, module_name, _ in pkgutil.iter_modules(package.__path__):\n        module = sys.modules.get('{}.{}'.format(package_name, module_name))\n        if module is not None:\n            imp.reload(module)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nimp, types, sys\n```python\nimport imp\nimport types\nimport sys\n\ndef reload_and_create_instance(module_name, class_name, *args, **kwargs):\n    \"\"\"\n    Reload a module and create an instance of a class from the module.\n    \n    Parameters:\n    module_name (str): The name of the module.\n    class_name (str): The name of the class.\n    *args: The arguments to pass to the class constructor.\n    **kwargs: The keyword arguments to pass to the class constructor.\n    \n    Returns:\n    object: An instance of the class.\n    \n    Requirements:\n    - imp\n    - types\n    - sys\n    \n    Example:\n    >>> reload_and_create_instance('my_module', 'MyClass', 'arg1', 'arg2', keyword='value')\n    \"\"\"\n    # Reload the module\n    module = imp.reload(sys.modules[module_name])\n    \n    # Get the class\n    cls = getattr(module, class_name)\n\n    # Create an instance of the class\n    instance = cls(*args, **kwargs)\n    \n    return instance\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nimp, sys, inspect\n```python\nimport imp\nimport sys\nimport inspect\n\ndef reload_and_get_all_functions(module_name):\n    \"\"\"\n    Reload a module and get all function names from the module.\n    \n    Parameters:\n    module_name (str): The name of the module.\n    \n    Returns:\n    list: A list of function names.\n    \n    Requirements:\n    - imp\n    - sys\n    - inspect\n    \n    Example:\n    >>> reload_and_get_all_functions('my_module')\n    \"\"\"\n    # Reload the module\n    module = imp.reload(sys.modules[module_name])\n    \n    # Get all function names\n    function_names = [name for name, obj in inspect.getmembers(module) if inspect.isfunction(obj)]\n    \n    return function_names\n```\n"}
{"seed": "{\"task_id\": 19546911, \"prompt\": \"def f_19546911(number):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"struct.unpack('H', struct.pack('h', number))\", \"test_start\": \"\\nimport struct \\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(3) == (3,)\\n\"], \"entry_point\": \"f_19546911\", \"intent\": \"Convert integer `number` into an unassigned integer\", \"library\": [\"struct\"]}", "generation": "Scenario 1:\nstruct, numpy, matplotlib, math\n```python\nimport struct\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\n\n# Constants\nAMPLITUDE = 32767\nFREQUENCY = 440\nSAMPLE_RATE = 44100\nDURATION = 5\n\ndef generate_sine_wave():\n    \"\"\"\n    Generate a sine wave with a specific frequency and duration, convert the wave to \n    16-bit PCM and then plot the wave.\n\n    Returns:\n    numpy.array: The generated sine wave.\n\n    Requirements:\n    - struct\n    - numpy\n    - matplotlib.pyplot\n    - math\n\n    Example:\n    >>> wave = generate_sine_wave()\n    >>> plt.plot(wave[:1000])\n    \"\"\"\n    samples = np.arange(DURATION * SAMPLE_RATE)\n    wave = AMPLITUDE * np.sin(2 * np.pi * FREQUENCY * samples / SAMPLE_RATE)\n    wave = wave.astype(np.int16)\n\n    # Convert to 16-bit PCM\n    wave = [struct.pack('h', sample) for sample in wave]\n    wave = np.frombuffer(b''.join(wave), np.int16)\n\n    return wave\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nstruct, os, json, hashlib\n```python\nimport struct\nimport os\nimport json\nimport hashlib\n\n# Constants\nBUFFER_SIZE = 65536\n\ndef calculate_file_hash(file_path):\n    \"\"\"\n    Calculate the SHA-256 hash of a file with a specific path, and store the \n    hash and file size in a JSON file.\n\n    Parameters:\n    file_path (str): The path to the file.\n\n    Returns:\n    dict: The file size and hash.\n\n    Requirements:\n    - struct\n    - os\n    - json\n    - hashlib\n\n    Example:\n    >>> calculate_file_hash('example.txt')\n    {'size': 1234, 'hash': 'abc123...'}\n    \"\"\"\n    sha256 = hashlib.sha256()\n\n    with open(file_path, 'rb') as f:\n        while True:\n            data = f.read(BUFFER_SIZE)\n            if not data:\n                break\n            sha256.update(data)\n\n    file_size = os.path.getsize(file_path)\n    file_hash = sha256.hexdigest()\n\n    file_info = {\n        'size': file_size,\n        'hash': file_hash\n    }\n\n    with open(file_path + '.json', 'w') as f:\n        json.dump(file_info, f)\n\n    return file_info\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nstruct, scipy, numpy, matplotlib\n```python\nimport struct\nimport scipy.signal as signal\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nSAMPLE_RATE = 44100\nFREQUENCY = 1000\nDURATION = 1\nAMPLITUDE = 10000\n\ndef create_and_plot_signal():\n    \"\"\"\n    Create a signal of a specific frequency and amplitude, add white noise to the signal, \n    and plot the resulting signal.\n\n    Returns:\n    numpy.array: The generated signal with white noise.\n\n    Requirements:\n    - struct\n    - scipy\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> noisy_signal = create_and_plot_signal()\n    >>> plt.plot(noisy_signal[:1000])\n    \"\"\"\n    time = np.arange(0, DURATION, 1/SAMPLE_RATE)\n    signal = AMPLITUDE * np.sin(2 * np.pi * FREQUENCY * time)\n\n    # Add white noise\n    noise = np.random.normal(0, 1, signal.shape)\n    noisy_signal = signal + noise\n\n    plt.plot(time[:1000], noisy_signal[:1000])\n    plt.show()\n\n    return noisy_signal\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nstruct, datetime, pytz, os\n```python\nimport struct\nimport datetime\nimport pytz\nimport os\n\n# Constants\nDIRECTORY_PATH = '/path/to/directory'\n\ndef get_files_modified_after_date(date_str, tz_str):\n    \"\"\"\n    Get a list of files in a specific directory that were modified after a \n    specific date in a specific timezone.\n\n    Parameters:\n    date_str (str): The date string in \"yyyy-mm-dd\" format.\n    tz_str (str): The timezone.\n\n    Returns:\n    list: The list of files.\n\n    Requirements:\n    - struct\n    - datetime\n    - pytz\n    - os\n\n    Example:\n    >>> get_files_modified_after_date('2022-01-01', 'America/Los_Angeles')\n    ['file1.txt', 'file2.txt', ...]\n    \"\"\"\n    tz = pytz.timezone(tz_str)\n    date = datetime.datetime.strptime(date_str, '%Y-%m-%d').replace(tzinfo=tz)\n\n    files = []\n    for filename in os.listdir(DIRECTORY_PATH):\n        file_path = os.path.join(DIRECTORY_PATH, filename)\n        modified_time = datetime.datetime.fromtimestamp(os.path.getmtime(file_path), tz)\n        if modified_time > date:\n            files.append(filename)\n\n    return files\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nstruct, numpy, pandas, matplotlib.pyplot, sklearn\n```python\nimport struct\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n# Constants\nDATA_SIZE = 100\n\ndef create_and_analyze_data():\n    \"\"\"\n    Create a dataset with a linear relationship, split the dataset into a training set \n    and a test set, train a linear regression model on the training set, and evaluate \n    the model on the test set.\n\n    Returns:\n    float: The mean squared error of the model on the test set.\n\n    Requirements:\n    - struct\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - sklearn\n\n    Example:\n    >>> create_and_analyze_data()\n    0.123456789\n    \"\"\"\n    # Create a dataset with a linear relationship\n    x = np.linspace(0, 10, DATA_SIZE)\n    y = 3 * x + 2 + np.random.normal(0, 1, DATA_SIZE)\n\n    df = pd.DataFrame({'x': x, 'y': y})\n\n    # Split the dataset into a training set and a test set\n    train_df, test_df = train_test_split(df, test_size=0.2)\n\n    # Train a linear regression model on the training set\n    model = LinearRegression()\n    model.fit(train_df[['x']], train_df['y'])\n\n    # Evaluate the model on the test set\n    predictions = model.predict(test_df[['x']])\n    mse = mean_squared_error(test_df['y'], predictions)\n\n    return mse\n```"}
{"seed": "{\"task_id\": 9746522, \"prompt\": \"def f_9746522(numlist):\\n\\t\", \"suffix\": \"\\n\\treturn numlist\", \"canonical_solution\": \"numlist = [float(x) for x in numlist]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([3, 4]) == [3.0, 4.0]\\n\"], \"entry_point\": \"f_9746522\", \"intent\": \"convert int values in list `numlist` to float\", \"library\": []}", "generation": "Scenario 1:\npandas, numpy, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef analyze_numlist(numlist):\n    \"\"\"\n    Analyze a list of numbers: convert each to float, calculate basic statistics (mean, median, mode, \n    standard deviation), and create a histogram of the values.\n\n    Parameters:\n    numlist (list): The list of numbers.\n\n    Returns:\n    dict: A dictionary with analysis results.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> analyze_numlist([3, 4, 5, 5, 6])\n    \"\"\"\n    numlist = [float(x) for x in numlist]\n    num_series = pd.Series(numlist)\n\n    mean = num_series.mean()\n    median = num_series.median()\n    mode = num_series.mode().tolist()\n    std_dev = num_series.std()\n\n    plt.hist(numlist, bins='auto')\n    plt.title(\"Histogram of numlist\")\n    plt.show()\n\n    return {\"mean\": mean, \"median\": median, \"mode\": mode, \"std_dev\": std_dev}\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\njson, statistics, matplotlib\n```python\nimport json\nimport statistics\nimport matplotlib.pyplot as plt\n\ndef analyze_json_numbers(json_string):\n    \"\"\"\n    Analyze a JSON string that contains a list of numbers: convert each to float, \n    calculate basic statistics (mean, median, mode, standard deviation), and \n    create a histogram of the values.\n\n    Parameters:\n    json_string (str): The JSON string.\n\n    Returns:\n    dict: A dictionary with analysis results.\n\n    Requirements:\n    - json\n    - statistics\n    - matplotlib.pyplot\n\n    Example:\n    >>> analyze_json_numbers('{\"numbers\": [3, 4, 5, 5, 6]}')\n    \"\"\"\n    data = json.loads(json_string)\n    numlist = [float(x) for x in data[\"numbers\"]]\n\n    mean = statistics.mean(numlist)\n    median = statistics.median(numlist)\n    mode = statistics.mode(numlist)\n    std_dev = statistics.stdev(numlist)\n\n    plt.hist(numlist, bins='auto')\n    plt.title(\"Histogram of numlist\")\n    plt.show()\n\n    return {\"mean\": mean, \"median\": median, \"mode\": mode, \"std_dev\": std_dev}\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, scipy.stats, matplotlib\n```python\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef analyze_np_array(array):\n    \"\"\"\n    Analyze a numpy array of numbers: convert each to float, calculate basic statistics \n    (mean, median, mode, standard deviation), and create a histogram of the values.\n\n    Parameters:\n    array (numpy.array): The numpy array.\n\n    Returns:\n    dict: A dictionary with analysis results.\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Example:\n    >>> analyze_np_array(np.array([3, 4, 5, 5, 6]))\n    \"\"\"\n    array = np.array([float(x) for x in array])\n\n    mean = np.mean(array)\n    median = np.median(array)\n    mode = stats.mode(array)[0][0]\n    std_dev = np.std(array)\n\n    plt.hist(array, bins='auto')\n    plt.title(\"Histogram of array\")\n    plt.show()\n\n    return {\"mean\": mean, \"median\": median, \"mode\": mode, \"std_dev\": std_dev}\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, sklearn.preprocessing, matplotlib\n```python\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\ndef analyze_and_scale_numlist(numlist):\n    \"\"\"\n    Analyze a list of numbers and scale them: convert each to float, calculate basic statistics \n    (mean, median, mode, standard deviation), scale the numbers to have mean 0 and variance 1, \n    and create a histogram of the original and scaled values.\n\n    Parameters:\n    numlist (list): The list of numbers.\n\n    Returns:\n    dict: A dictionary with analysis results and the scaled list.\n\n    Requirements:\n    - pandas\n    - sklearn.preprocessing\n    - matplotlib.pyplot\n\n    Example:\n    >>> analyze_and_scale_numlist([3, 4, 5, 5, 6])\n    \"\"\"\n    numlist = [float(x) for x in numlist]\n    num_series = pd.Series(numlist)\n\n    mean = num_series.mean()\n    median = num_series.median()\n    mode = num_series.mode().tolist()\n    std_dev = num_series.std()\n\n    scaler = StandardScaler()\n    scaled_numlist = scaler.fit_transform(np.array(numlist).reshape(-1, 1)).flatten().tolist()\n\n    plt.hist(numlist, bins='auto', alpha=0.5, label='Original')\n    plt.hist(scaled_numlist, bins='auto', alpha=0.5, label='Scaled')\n    plt.title(\"Histogram of numlist and scaled numlist\")\n    plt.legend(loc='upper right')\n    plt.show()\n\n    return {\"mean\": mean, \"median\": median, \"mode\": mode, \"std_dev\": std_dev, \"scaled\": scaled_numlist}\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, scipy.stats, seaborn\n```python\nimport numpy as np\nfrom scipy import stats\nimport seaborn as sns\n\ndef analyze_np_array_with_plot(array):\n    \"\"\"\n    Analyze a numpy array of numbers and create a KDE plot: convert each to float, \n    calculate basic statistics (mean, median, mode, standard deviation), and create \n    a Kernel Density Estimation (KDE) plot of the values.\n\n    Parameters:\n    array (numpy.array): The numpy array.\n\n    Returns:\n    dict: A dictionary with analysis results.\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - seaborn\n\n    Example:\n    >>> analyze_np_array_with_plot(np.array([3, 4, 5, 5, 6]))\n    \"\"\"\n    array = np.array([float(x) for x in array])\n\n    mean = np.mean(array)\n    median = np.median(array)\n    mode = stats.mode(array)[0][0]\n    std_dev = np.std(array)\n\n    sns.kdeplot(array)\n    plt.title(\"KDE plot of array\")\n    plt.show()\n\n    return {\"mean\": mean, \"median\": median, \"mode\": mode, \"std_dev\": std_dev}\n```"}
{"seed": "{\"task_id\": 20107570, \"prompt\": \"def f_20107570(df, filename):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"df.to_csv(filename, index=False)\", \"test_start\": \"\\nimport pandas as pd\\n\\ndef check(candidate):\", \"test\": [\"\\n    file_name = 'a.csv'\\n    df = pd.DataFrame([1, 2, 3], columns = ['Vals'])\\n    candidate(df, file_name)\\n    with open (file_name, 'r') as f:\\n        lines = f.readlines()\\n    assert len(lines) == 4\\n\"], \"entry_point\": \"f_20107570\", \"intent\": \"write dataframe `df`, excluding index, to a csv file `filename`\", \"library\": [\"pandas\"]}", "generation": "Scenario 1: \npandas, numpy, matplotlib.pyplot, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nCOLUMNS = ['Temperature', 'Humidity', 'Wind Speed', 'Rainfall']\n\ndef analyze_weather_data(df, filename):\n    \"\"\"\n    Analyze a dataframe of weather data, generate various statistics and visualizations, \n    and write the processed dataframe to a CSV file.\n\n    Parameters:\n    df (DataFrame): The input dataframe.\n    filename (str): The output filename.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> df = pd.DataFrame(np.random.rand(100, 4), columns=COLUMNS)\n    >>> analyze_weather_data(df, 'processed_weather_data.csv')\n    \"\"\"\n    # Calculate basic statistics\n    statistics = df.describe()\n\n    # Generate a correlation matrix\n    corr_matrix = df.corr()\n\n    # Plot a heatmap of the correlation matrix\n    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n\n    # Show the plot\n    plt.show()\n\n    # Add statistics to the dataframe\n    df = pd.concat([df, statistics])\n\n    # Write the dataframe to a CSV file\n    df.to_csv(filename, index=False)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, os, glob, pathlib\n```python\nimport pandas as pd\nimport os\nimport glob\nfrom pathlib import Path\n\n# Constants\nFILE_EXTENSION = '.csv'\n\ndef merge_csv_files(directory, filename):\n    \"\"\"\n    Merge multiple CSV files in a directory into a single CSV file.\n\n    Parameters:\n    directory (str): The directory containing the CSV files.\n    filename (str): The output filename.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - os\n    - glob\n    - pathlib\n\n    Example:\n    >>> merge_csv_files('/path/to/csv/files', 'merged.csv')\n    \"\"\"\n    # Get the list of CSV files in the directory\n    csv_files = glob.glob(os.path.join(directory, '*' + FILE_EXTENSION))\n\n    # Initialize an empty dataframe\n    df = pd.DataFrame()\n\n    # Iterate over the CSV files and append them to the dataframe\n    for file in csv_files:\n        df = df.append(pd.read_csv(file), ignore_index=True)\n\n    # Write the dataframe to a CSV file\n    df.to_csv(filename, index=False)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, sklearn.preprocessing, matplotlib.pyplot, seaborn\n```python\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nCOLUMNS = ['Feature1', 'Feature2', 'Feature3', 'Feature4']\n\ndef preprocess_and_visualize(df, filename):\n    \"\"\"\n    Preprocess a dataframe by scaling its features to the range [0, 1] using MinMaxScaler, \n    visualize the distribution of the features after scaling, and write the processed dataframe \n    to a CSV file.\n\n    Parameters:\n    df (DataFrame): The input dataframe.\n    filename (str): The output filename.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - sklearn.preprocessing\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> df = pd.DataFrame(np.random.rand(100, 4), columns=COLUMNS)\n    >>> preprocess_and_visualize(df, 'processed_data.csv')\n    \"\"\"\n    # Initialize a MinMaxScaler\n    scaler = MinMaxScaler()\n\n    # Scale the features\n    df[COLUMNS] = scaler.fit_transform(df[COLUMNS])\n\n    # Plot the distribution of the features\n    df[COLUMNS].hist(bins=30, figsize=(10, 10))\n    plt.show()\n\n    # Write the dataframe to a CSV file\n    df.to_csv(filename, index=False)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, sklearn.model_selection, sklearn.linear_model\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\n# Constants\nFEATURES = ['Feature1', 'Feature2', 'Feature3', 'Feature4']\nTARGET = 'Target'\n\ndef train_linear_regression(df, filename):\n    \"\"\"\n    Train a linear regression model on a dataframe, save the trained model \n    to a file, and write the dataframe to a CSV file.\n\n    Parameters:\n    df (DataFrame): The input dataframe.\n    filename (str): The output filename.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.model_selection\n    - sklearn.linear_model\n\n    Example:\n    >>> df = pd.DataFrame(np.random.rand(100, 5), columns=FEATURES + [TARGET])\n    >>> train_linear_regression(df, 'trained_model.pkl')\n    \"\"\"\n    # Split the dataframe into features and target\n    X = df[FEATURES]\n    y = df[TARGET]\n\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    # Initialize a LinearRegression model\n    model = LinearRegression()\n\n    # Train the model\n    model.fit(X_train, y_train)\n\n    # Save the trained model to a file\n    with open(filename, 'wb') as f:\n        pickle.dump(model, f)\n\n    # Write the dataframe to a CSV file\n    df.to_csv(filename, index=False)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, sklearn.cluster, matplotlib.pyplot, seaborn\n```python\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nFEATURES = ['Feature1', 'Feature2', 'Feature3', 'Feature4']\nN_CLUSTERS = 3\n\ndef perform_kmeans_clustering(df, filename):\n    \"\"\"\n    Perform K-means clustering on a dataframe, visualize the clusters, \n    and write the dataframe with cluster labels to a CSV file.\n\n    Parameters:\n    df (DataFrame): The input dataframe.\n    filename (str): The output filename.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - sklearn.cluster\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> df = pd.DataFrame(np.random.rand(100, 4), columns=FEATURES)\n    >>> perform_kmeans_clustering(df, 'data_with_clusters.csv')\n    \"\"\"\n    # Perform K-means clustering\n    kmeans = KMeans(n_clusters=N_CLUSTERS, random_state=0).fit(df[FEATURES])\n\n    # Add cluster labels to the dataframe\n    df['Cluster'] = kmeans.labels_\n\n    # Plot the clusters\n    sns.scatterplot(data=df, x='Feature1', y='Feature2', hue='Cluster')\n    plt.show()\n\n    # Write the dataframe to a CSV file\n    df.to_csv(filename, index=False)\n```"}
{"seed": "{\"task_id\": 8740353, \"prompt\": \"def f_8740353(unescaped):\\n\\t\", \"suffix\": \"\\n\\treturn json_data\", \"canonical_solution\": \"json_data = json.loads(unescaped)\", \"test_start\": \"\\nimport json \\n\\ndef check(candidate):\", \"test\": [\"\\n    x = \\\"\\\"\\\"{\\n    \\\"Name\\\": \\\"Jennifer Smith\\\",\\n    \\\"Contact Number\\\": 7867567898,\\n    \\\"Email\\\": \\\"jen123@gmail.com\\\",\\n    \\\"Hobbies\\\":[\\\"Reading\\\", \\\"Sketching\\\", \\\"Horse Riding\\\"]\\n    }\\\"\\\"\\\"\\n    assert candidate(x) == {'Hobbies': ['Reading', 'Sketching', 'Horse Riding'], 'Name': 'Jennifer Smith', 'Email': 'jen123@gmail.com', 'Contact Number': 7867567898}\\n\"], \"entry_point\": \"f_8740353\", \"intent\": \"convert a urllib unquoted string `unescaped` to a json data `json_data`\", \"library\": [\"json\"]}", "generation": "Scenario 1: \njson, pandas, urllib.parse, numpy\n```python\nimport json\nimport pandas as pd\nimport urllib.parse\nimport numpy as np\n\n# Constants\nDATA_URL = \"https://raw.githubusercontent.com/...\"\n\ndef load_data_from_url(url_str):\n    \"\"\"\n    Load data from a given URL, convert it from a urllib unquoted string to JSON, \n    and then convert it to a pandas DataFrame. \n\n    Parameters:\n    url_str (str): The URL string containing the data.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the loaded data.\n    \n    Requirements:\n    - json\n    - pandas\n    - urllib.parse\n    - numpy\n\n    Example:\n    >>> df = load_data_from_url(DATA_URL)\n    >>> print(df.head())\n    \"\"\"\n    response = urllib.request.urlopen(url_str)\n    unquoted_str = urllib.parse.unquote(response.read().decode())\n    json_data = json.loads(unquoted_str)\n    df = pd.DataFrame(json_data)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \njson, os, shutil, urllib.parse\n```python\nimport json\nimport os\nimport shutil\nimport urllib.parse\n\n# Constants\nDOWNLOAD_DIRECTORY = \"/path/to/download_directory\"\nTEMP_DIRECTORY = \"/path/to/temp_directory\"\n\ndef download_and_save_json(url_str, filename):\n    \"\"\"\n    Convert a urllib unquoted string to JSON data and save it to a file.\n\n    Parameters:\n    url_str (str): The URL string containing the JSON data.\n    filename (str): The name of the file where the JSON data should be saved.\n\n    Returns:\n    str: The full path of the saved file.\n    \n    Requirements:\n    - json\n    - os\n    - shutil\n    - urllib.parse\n    \"\"\"\n    response = urllib.request.urlopen(url_str)\n    unquoted_str = urllib.parse.unquote(response.read().decode())\n    json_data = json.loads(unquoted_str)\n\n    temp_filepath = os.path.join(TEMP_DIRECTORY, filename)\n    with open(temp_filepath, 'w') as json_file:\n        json.dump(json_data, json_file)\n    \n    final_filepath = os.path.join(DOWNLOAD_DIRECTORY, filename)\n    shutil.move(temp_filepath, final_filepath)\n\n    return final_filepath\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \njson, urllib.parse, hashlib, base64\n```python\nimport json\nimport urllib.parse\nimport hashlib\nimport base64\n\ndef generate_hash_from_json(url_str):\n    \"\"\"\n    Convert a urllib unquoted string to JSON data and generate a hash from it.\n\n    Parameters:\n    url_str (str): The URL string containing the JSON data.\n\n    Returns:\n    str: The base64 encoded hash of the JSON data.\n    \n    Requirements:\n    - json\n    - urllib.parse\n    - hashlib\n    - base64\n    \"\"\"\n    response = urllib.request.urlopen(url_str)\n    unquoted_str = urllib.parse.unquote(response.read().decode())\n    json_data = json.loads(unquoted_str)\n\n    hash_object = hashlib.sha256(json.dumps(json_data).encode())\n    base64_hash = base64.b64encode(hash_object.digest()).decode()\n\n    return base64_hash\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \njson, urllib.parse, requests, time\n```python\nimport json\nimport urllib.parse\nimport requests\nimport time\n\n# Constants\nAPI_ENDPOINT = \"https://api.someservice.com/data\"\n\ndef post_json_to_api(url_str):\n    \"\"\"\n    Convert a urllib unquoted string to JSON data and post it to an API endpoint.\n\n    Parameters:\n    url_str (str): The URL string containing the JSON data.\n\n    Returns:\n    Response: The Response object from the API request.\n    \n    Requirements:\n    - json\n    - urllib.parse\n    - requests\n    - time\n    \"\"\"\n    response = urllib.request.urlopen(url_str)\n    unquoted_str = urllib.parse.unquote(response.read().decode())\n    json_data = json.loads(unquoted_str)\n\n    time.sleep(5)  # delay to avoid overloading the API\n\n    api_response = requests.post(API_ENDPOINT, json=json_data)\n\n    return api_response\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \njson, urllib.parse, gzip, io\n```python\nimport json\nimport urllib.parse\nimport gzip\nimport io\n\n# Constants\nFILE_PATH = \"/path/to/file.json.gz\"\n\ndef compress_json(url_str):\n    \"\"\"\n    Convert a urllib unquoted string to JSON data and compress it to a .gz file.\n\n    Parameters:\n    url_str (str): The URL string containing the JSON data.\n\n    Returns:\n    str: The path to the compressed file.\n    \n    Requirements:\n    - json\n    - urllib.parse\n    - gzip\n    - io\n    \"\"\"\n    response = urllib.request.urlopen(url_str)\n    unquoted_str = urllib.parse.unquote(response.read().decode())\n    json_data = json.loads(unquoted_str)\n\n    with gzip.open(FILE_PATH, 'wb') as f_out:\n        f_out.write(json.dumps(json_data).encode())\n\n    return FILE_PATH\n```"}
{"seed": "{\"task_id\": 5891453, \"prompt\": \"def f_5891453():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"[chr(i) for i in range(127)]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    chars = candidate()\\n    assert len(chars) == 127\\n    assert chars == [chr(i) for i in range(127)]\\n\"], \"entry_point\": \"f_5891453\", \"intent\": \"Create a list containing all ascii characters as its elements\", \"library\": []}", "generation": "Scenario 1:\nnumpy, itertools, string\n```python\nimport numpy as np\nimport itertools\nimport string\n\n# Constants\nASCII_CHARS = [chr(i) for i in range(127)]\nCHAR_COMB_LENGTH = 4\n\ndef generate_random_string_array(n, m):\n    \"\"\"\n    Generate an n by m numpy array filled with random 4-character combinations \n    of ASCII characters.\n\n    Parameters:\n    n (int): The number of rows of the array.\n    m (int): The number of columns of the array.\n\n    Returns:\n    numpy.ndarray: A numpy array filled with random 4-character combinations of ASCII characters.\n\n    Requirements:\n    - numpy\n    - itertools\n    - string\n\n    Example:\n    >>> np.random.seed(0)\n    >>> generate_random_string_array(2, 3)\n    array([['a3B!', '1Xc*', '!2A@'],\n           ['4B@!', '3#d$', '!Z7*']], dtype='<U4')\n    \"\"\"\n    np.random.seed(0)\n    char_combinations = [''.join(comb) for comb in itertools.combinations(ASCII_CHARS, CHAR_COMB_LENGTH)]\n    random_strings = np.random.choice(char_combinations, size=(n, m))\n\n    return random_strings\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nrandom, string\n```python\nimport random\nimport string\n\n# Constants\nASCII_CHARS = [chr(i) for i in range(127)]\nCHAR_COMB_LENGTH = 4\n\ndef generate_random_string_dict(n):\n    \"\"\"\n    Generate a dictionary with n items where the keys are random 4-character combinations\n    of ASCII characters and the values are random integers.\n\n    Parameters:\n    n (int): The number of items in the dictionary.\n\n    Returns:\n    dict: A dictionary with n items.\n\n    Requirements:\n    - random\n    - string\n\n    Example:\n    >>> random.seed(0)\n    >>> generate_random_string_dict(3)\n    {'81@#': 49, '14!$': 97, '0$^&': 53}\n    \"\"\"\n    random.seed(0)\n    random_strings = [''.join(random.choices(ASCII_CHARS, k=CHAR_COMB_LENGTH)) for _ in range(n)]\n    random_ints = [random.randint(0, 100) for _ in range(n)]\n    random_dict = dict(zip(random_strings, random_ints))\n\n    return random_dict\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nrandom, string, pandas\n```python\nimport random\nimport string\nimport pandas as pd\n\n# Constants\nASCII_CHARS = [chr(i) for i in range(127)]\nCHAR_COMB_LENGTH = 4\nCOLUMN_NAMES = ['Column1', 'Column2', 'Column3']\n\ndef generate_random_string_dataframe(n):\n    \"\"\"\n    Generate a DataFrame with n rows and 3 columns where each cell contains \n    a random 4-character combination of ASCII characters.\n\n    Parameters:\n    n (int): The number of rows in the DataFrame.\n\n    Returns:\n    DataFrame: A pandas DataFrame with n rows and 3 columns.\n\n    Requirements:\n    - random\n    - string\n    - pandas\n\n    Example:\n    >>> random.seed(0)\n    >>> generate_random_string_dataframe(2)\n       Column1 Column2 Column3\n    0    81@#    14!$    0$^&\n    1    4&*%    5(+)    6_=-\n    \"\"\"\n    random.seed(0)\n    random_strings = [[''.join(random.choices(ASCII_CHARS, k=CHAR_COMB_LENGTH)) for _ in range(len(COLUMN_NAMES))] for _ in range(n)]\n    random_df = pd.DataFrame(random_strings, columns=COLUMN_NAMES)\n\n    return random_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nrandom, string, numpy\n```python\nimport random\nimport string\nimport numpy as np\n\n# Constants\nASCII_CHARS = [chr(i) for i in range(127)]\nCHAR_COMB_LENGTH = 4\n\ndef generate_random_string_matrix(n, m):\n    \"\"\"\n    Generate an n by m matrix filled with random 4-character combinations of ASCII characters.\n\n    Parameters:\n    n (int): The number of rows of the matrix.\n    m (int): The number of columns of the matrix.\n\n    Returns:\n    numpy.ndarray: A numpy array filled with random 4-character combinations of ASCII characters.\n\n    Requirements:\n    - random\n    - string\n    - numpy\n\n    Example:\n    >>> random.seed(0)\n    >>> generate_random_string_matrix(2, 3)\n    array([['81@#', '14!$', '0$^&'],\n           ['4&*%', '5(+)', '6_=-']], dtype='<U4')\n    \"\"\"\n    random.seed(0)\n    random_strings = [[''.join(random.choices(ASCII_CHARS, k=CHAR_COMB_LENGTH)) for _ in range(m)] for _ in range(n)]\n    random_matrix = np.array(random_strings)\n\n    return random_matrix\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nrandom, string, matplotlib\n```python\nimport random\nimport string\nimport matplotlib.pyplot as plt\n\n# Constants\nASCII_CHARS = [chr(i) for i in range(127)]\nCHAR_COMB_LENGTH = 4\nN_STRINGS = 1000\n\ndef plot_random_string_distribution():\n    \"\"\"\n    Generate N_STRINGS number of random 4-character combinations of ASCII characters \n    and plot the distribution of the first characters in the combinations.\n\n    Requirements:\n    - random\n    - string\n    - matplotlib.pyplot\n\n    Example:\n    >>> random.seed(0)\n    >>> plot_random_string_distribution()\n    \"\"\"\n    random.seed(0)\n    random_strings = [''.join(random.choices(ASCII_CHARS, k=CHAR_COMB_LENGTH)) for _ in range(N_STRINGS)]\n    first_chars = [s[0] for s in random_strings]\n\n    plt.hist(first_chars, bins=range(127), alpha=0.5, edgecolor='black')\n    plt.title('Distribution of First Characters in Random Strings')\n    plt.xlabel('ASCII Value')\n    plt.ylabel('Frequency')\n\n    plt.show()\n```\nAs per the requirement of the question, the function only plots a graph and does not return any value for correctness assertion."}
{"seed": "{\"task_id\": 18367007, \"prompt\": \"def f_18367007(newFileBytes, newFile):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"newFile.write(struct.pack('5B', *newFileBytes))\", \"test_start\": \"\\nimport struct \\n\\ndef check(candidate):\", \"test\": [\"\\n    newFileBytes = [123, 3, 123, 100, 99]\\n    file_name = 'f.txt'\\n    newFile = open(file_name, 'wb')\\n    candidate(newFileBytes, newFile)\\n    newFile.close()\\n    with open (file_name, 'rb') as f:\\n        lines = f.readlines()\\n        assert lines == [b'{\\u0003{dc']\\n\"], \"entry_point\": \"f_18367007\", \"intent\": \"write `newFileBytes` to a binary file `newFile`\", \"library\": [\"struct\"]}", "generation": "Scenario 1:\nnumpy, struct, os, zlib\n```python\nimport numpy as np\nimport struct\nimport os\nimport zlib\n\n# Constants\nFILE_NAME = 'f.npy'\n\ndef save_array_compressed(newArray):\n    \"\"\"\n    Save a numpy array to a binary file in a compressed format.\n\n    Parameters:\n    newArray (numpy.array): The numpy array to be saved.\n\n    Returns:\n    str: The name of the file where the array is saved.\n\n    Requirements:\n    - numpy\n    - struct\n    - os\n    - zlib\n\n    Example:\n    >>> array = np.array([1, 2, 3, 4, 5])\n    >>> save_array_compressed(array)\n    \"\"\"\n    compressed_data = zlib.compress(newArray.tobytes())\n\n    with open(FILE_NAME, 'wb') as newFile:\n        newFile.write(struct.pack('B'*len(compressed_data), *compressed_data))\n\n    return FILE_NAME\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nstruct, io, gzip, numpy\n```python\nimport struct\nimport io\nimport gzip\nimport numpy as np\n\ndef save_to_gzip(newArray):\n    \"\"\"\n    Write a numpy array to a GZIP file using struct and gzip.\n\n    Parameters:\n    newArray (numpy.array): The numpy array to be saved.\n\n    Returns:\n    bytes: The gzipped data.\n\n    Requirements:\n    - struct\n    - io\n    - gzip\n    - numpy\n\n    Example:\n    >>> array = np.array([1, 2, 3, 4, 5])\n    >>> save_to_gzip(array)\n    \"\"\"\n    buffer = io.BytesIO()\n\n    with gzip.GzipFile(fileobj=buffer, mode='w') as f:\n        f.write(struct.pack('d'*newArray.size, *newArray))\n\n    return buffer.getvalue()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nstruct, os, zlib, hashlib\n```python\nimport struct\nimport os\nimport zlib\nimport hashlib\n\n# Constants\nCHUNK_SIZE = 1024\n\ndef validate_file_integrity(filename):\n    \"\"\"\n    Validate the integrity of a file by comparing a calculated hash with a stored hash.\n\n    Parameters:\n    filename (str): The name of the file to be validated.\n\n    Returns:\n    bool: True if the file is intact, False otherwise.\n\n    Requirements:\n    - struct\n    - os\n    - zlib\n    - hashlib\n\n    Example:\n    >>> validate_file_integrity('test.txt')\n    \"\"\"\n    with open(filename, 'rb') as f:\n        stored_hash = struct.unpack('20s', f.read(20))[0]\n        calculated_hash = hashlib.sha1()\n\n        while True:\n            data = f.read(CHUNK_SIZE)\n            if not data:\n                break\n            calculated_hash.update(data)\n\n        return stored_hash == calculated_hash.digest()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nstruct, random, zlib, os\n```python\nimport struct\nimport random\nimport zlib\nimport os\n\ndef create_random_binary_file(filename):\n    \"\"\"\n    Create a random binary file with a header that stores the length of the file.\n\n    Parameters:\n    filename (str): The name of the file to be created.\n\n    Returns:\n    None\n\n    Requirements:\n    - struct\n    - random\n    - zlib\n    - os\n\n    Example:\n    >>> create_random_binary_file('random.bin')\n    \"\"\"\n    with open(filename, 'wb') as f:\n        for _ in range(1000):\n            random_bytes = bytes(random.getrandbits(8) for _ in range(100))\n            compressed_bytes = zlib.compress(random_bytes)\n            f.write(struct.pack('I', len(compressed_bytes)))\n            f.write(compressed_bytes)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nstruct, numpy, os, zlib\n```python\nimport struct\nimport numpy as np\nimport os\nimport zlib\n\ndef save_ndarray_to_binary_file(ndarray, filename):\n    \"\"\"\n    Save a numpy ndarray to a binary file with zlib compression.\n\n    Parameters:\n    ndarray (numpy.ndarray): The numpy ndarray to be saved.\n    filename (str): The name of the file where the ndarray will be saved.\n\n    Returns:\n    str: The name of the file where the ndarray is saved.\n\n    Requirements:\n    - struct\n    - numpy\n    - os\n    - zlib\n\n    Example:\n    >>> ndarray = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    >>> save_ndarray_to_binary_file(ndarray, 'ndarray.bin')\n    \"\"\"\n    compressed_data = zlib.compress(ndarray.tobytes())\n\n    with open(filename, 'wb') as f:\n        f.write(struct.pack('I', ndarray.shape[0]))\n        f.write(struct.pack('I', ndarray.shape[1]))\n        f.write(compressed_data)\n\n    return filename\n```"}
{"seed": "{\"task_id\": 21805490, \"prompt\": \"def f_21805490(string):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"re.sub('^[A-Z0-9]*(?![a-z])', '', string)\", \"test_start\": \"\\nimport re \\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(\\\"AASKH317298DIUANFProgramming is fun\\\") == \\\"Programming is fun\\\"\\n\"], \"entry_point\": \"f_21805490\", \"intent\": \"python regex - check for a capital letter with a following lowercase in string `string`\", \"library\": [\"re\"]}", "generation": "Scenario 1:\npandas, re, os, random\n```python\nimport pandas as pd\nimport re\nimport os\nfrom random import randint\n\n# Constants\nPATTERNS = [\"^[A-Z0-9]*(?![a-z])\", \"^[0-9]*\", \"^[A-Z]*\", \"^[a-z]*\", \"^[a-zA-Z0-9]*\"]\n\ndef clean_directory_filenames(pattern_id):\n    \"\"\"\n    Clean filenames in the current directory based on a specific regex pattern.\n\n    Parameters:\n    pattern_id (int): The id of the regex pattern to be used.\n\n    Returns:\n    dict: A dictionary with the original filenames as keys and cleaned filenames as values.\n\n    Requirements:\n    - pandas\n    - re\n    - os\n    - random\n\n    Example:\n    >>> clean_directory_filenames(0)\n    \"\"\"\n    file_dict = {}\n    pattern = PATTERNS[pattern_id]\n    \n    for filename in os.listdir():\n        cleaned_name = re.sub(pattern, '', filename)\n        file_dict[filename] = cleaned_name\n\n    return file_dict\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nre, random, string, datetime\n```python\nimport re\nimport random\nimport string\nfrom datetime import datetime\n\n# Constants\nSTRING_LENGTH = 100\nALPHANUMERIC_CHARS = string.ascii_letters + string.digits\n\ndef generate_random_string_and_find(pattern):\n    \"\"\"\n    Generate a random alphanumeric string of a given length and find all matches of a regex pattern.\n\n    Parameters:\n    pattern (str): The regex pattern to be used.\n\n    Returns:\n    list: A list of all matches found in the string.\n\n    Requirements:\n    - re\n    - random\n    - string\n    - datetime\n\n    Example:\n    >>> generate_random_string_and_find('[A-Z][0-9]')\n    \"\"\"\n    random_string = ''.join(random.choices(ALPHANUMERIC_CHARS, k=STRING_LENGTH))\n\n    matches = re.findall(pattern, random_string)\n\n    return matches\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nre, pandas, csv, string\n```python\nimport re\nimport pandas as pd\nimport csv\nimport string\n\n# Constants\nCSV_FILE = 'data.csv'\nCOLUMN_NAME = 'Text'\n\ndef extract_emails_from_csv():\n    \"\"\"\n    Extract all email addresses from a specific column in a CSV file.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the extracted emails.\n\n    Requirements:\n    - re\n    - pandas\n    - csv\n    - string\n\n    Example:\n    >>> extract_emails_from_csv()\n    \"\"\"\n    df = pd.read_csv(CSV_FILE)\n\n    emails = df[COLUMN_NAME].str.extractall(r'([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)')\n\n    return emails\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nre, os, shutil, sys\n```python\nimport re\nimport os\nimport shutil\nimport sys\n\n# Constants\nDIRECTORY = 'path/to/directory'\n\ndef rename_files_with_pattern(pattern, replacement):\n    \"\"\"\n    Rename all files in a directory that match a regex pattern.\n\n    Parameters:\n    pattern (str): The regex pattern to be used.\n    replacement (str): The string to replace the pattern with.\n\n    Returns:\n    list: A list of new filenames.\n\n    Requirements:\n    - re\n    - os\n    - shutil\n    - sys\n\n    Example:\n    >>> rename_files_with_pattern('file', 'document')\n    \"\"\"\n    new_filenames = []\n\n    for filename in os.listdir(DIRECTORY):\n        new_filename = re.sub(pattern, replacement, filename)\n        try:\n            shutil.move(os.path.join(DIRECTORY, filename), os.path.join(DIRECTORY, new_filename))\n            new_filenames.append(new_filename)\n        except Exception as e:\n            print(f\"Error renaming file {filename}: {e}\", file=sys.stderr)\n\n    return new_filenames\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, re, matplotlib, numpy\n```python\nimport pandas as pd\nimport re\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nDATASET = 'dataset.csv'\nCOLUMN_NAME = 'Text'\nPATTERN = '[A-Z][a-z]*'\n\ndef plot_word_length_distribution():\n    \"\"\"\n    Plot a histogram of word lengths from a specific column in a CSV dataset, where words are defined by a regex pattern.\n\n    Returns:\n    list: A list of word lengths.\n\n    Requirements:\n    - pandas\n    - re\n    - matplotlib\n    - numpy\n\n    Example:\n    >>> plot_word_length_distribution()\n    \"\"\"\n    df = pd.read_csv(DATASET)\n\n    words = df[COLUMN_NAME].str.extractall(PATTERN)[0]\n\n    word_lengths = words.str.len().value_counts().sort_index()\n\n    plt.bar(word_lengths.index, word_lengths.values)\n    plt.xlabel('Word Length')\n    plt.ylabel('Count')\n    plt.title('Word Length Distribution')\n    plt.show()\n\n    return word_lengths.tolist()\n```"}
{"seed": "{\"task_id\": 16125229, \"prompt\": \"def f_16125229(dict):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"list(dict.keys())[-1]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate({'t': 1, 'r': 2}) == 'r'\\n\", \"\\n    assert candidate({'c': 1, 'b': 2, 'a': 1}) == 'a'\\n\"], \"entry_point\": \"f_16125229\", \"intent\": \"get the last key of dictionary `dict`\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, random\n```python\nimport pandas as pd\nimport numpy as np\nimport random\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef generate_dataframe_and_get_last_column(data_dict):\n    \"\"\"\n    Convert a dictionary into a pandas DataFrame and return the last column label.\n    \n    Parameters:\n    data_dict (dict): The dictionary with keys as column labels and values as lists of column values.\n    \n    Returns:\n    str: The last column label.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - random\n    \n    Example:\n    >>> data = {column: random.sample(range(1, 101), 10) for column in COLUMNS}\n    >>> last_column = generate_dataframe_and_get_last_column(data)\n    >>> print(last_column)\n    \"\"\"\n    df = pd.DataFrame(data_dict)\n\n    last_column = df.columns[-1]\n\n    return last_column\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \ncollections, json, re\n```python\nimport collections\nimport json\nimport re\n\ndef get_last_key_in_nested_dict(nested_dict):\n    \"\"\"\n    Parse a nested dictionary and return the last key in the depth-first search order.\n    \n    Parameters:\n    nested_dict (dict): The nested dictionary.\n    \n    Returns:\n    str: The last key in the depth-first search order.\n    \n    Requirements:\n    - collections\n    - json\n    - re\n    \n    Example:\n    >>> nested_dict = {'a': {'b': {'c': 3, 'd': 4}, 'e': 5}, 'f': 6}\n    >>> last_key = get_last_key_in_nested_dict(nested_dict)\n    >>> print(last_key)\n    \"\"\"\n    nested_dict_str = json.dumps(nested_dict)\n    keys = re.findall(r'\\\"(.*?)\\\"\\:', nested_dict_str)\n    last_key = keys[-1] if keys else None\n\n    return last_key\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \npandas, numpy, matplotlib.pyplot\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_last_column_hist(data_dict):\n    \"\"\"\n    Convert a dictionary into a pandas DataFrame, and plot a histogram of the last column.\n    \n    Parameters:\n    data_dict (dict): The dictionary with keys as column labels and values as lists of column values.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> data = {'A': np.random.randn(100), 'B': np.random.randn(100)}\n    >>> plot_last_column_hist(data)\n    \"\"\"\n    df = pd.DataFrame(data_dict)\n\n    last_column = df.columns[-1]\n    plt.hist(df[last_column])\n    plt.title(f'Histogram of {last_column}')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \ncollections, itertools\n```python\nimport collections\nimport itertools\n\ndef get_last_key_of_most_common(dict):\n    \"\"\"\n    Get the last key of the most common values in a dictionary.\n    \n    Parameters:\n    dict (dict): The dictionary.\n    \n    Returns:\n    str: The last key of the most common values.\n    \n    Requirements:\n    - collections\n    - itertools\n    \n    Example:\n    >>> dict = {'a': 1, 'b': 2, 'c': 2, 'd': 1, 'e': 3, 'f': 2, 'g': 1}\n    >>> last_key = get_last_key_of_most_common(dict)\n    >>> print(last_key)\n    \"\"\"\n    counter = collections.Counter(dict.values())\n    most_common_value = counter.most_common(1)[0][0]\n\n    keys_of_most_common = [k for k, v in dict.items() if v == most_common_value]\n    last_key = keys_of_most_common[-1] if keys_of_most_common else None\n\n    return last_key\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnumpy, matplotlib.pyplot\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_dict_and_get_last_key(dict):\n    \"\"\"\n    Plot the values in a dictionary and return the last key.\n    \n    Parameters:\n    dict (dict): The dictionary with numeric values.\n    \n    Returns:\n    str: The last key.\n    \n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> dict = {'A': np.random.randn(100), 'B': np.random.randn(100)}\n    >>> last_key = plot_dict_and_get_last_key(dict)\n    >>> print(last_key)\n    \"\"\"\n    for key, values in dict.items():\n        plt.plot(values, label=key)\n    \n    plt.legend()\n    plt.show()\n\n    last_key = list(dict.keys())[-1]\n\n    return last_key\n```"}
{"seed": "{\"task_id\": 6159900, \"prompt\": \"def f_6159900(f):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"print('hi there', file=f)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    file_name = 'a.txt'\\n    f = open(file_name, 'w')\\n    candidate(f)\\n    f.close()\\n    with open (file_name, 'r') as f:\\n        lines = f.readlines()\\n        assert lines[0] == 'hi there\\\\n'\\n\"], \"entry_point\": \"f_6159900\", \"intent\": \"write line \\\"hi there\\\" to file `f`\", \"library\": []}", "generation": "Scenario 1: \nos, sys, csv, datetime\n```python\nimport os\nimport sys\nimport csv\nfrom datetime import datetime\n\n# Constants\nLOG_DIR = '/path/to/log/directory'\nFILE_NAME = 'system_logs.csv'\nFILE_PATH = os.path.join(LOG_DIR, FILE_NAME)\n\ndef write_system_log(system_event):\n    \"\"\"\n    Write a line with system event details and the current datetime to a CSV log file.\n\n    Parameters:\n    system_event (str): The details of the system event.\n\n    Returns:\n    None\n\n    Requirements:\n    - os\n    - sys\n    - csv\n    - datetime\n\n    Example:\n    >>> write_system_log('System rebooted')\n    \"\"\"\n    with open(FILE_PATH, 'a', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow([datetime.now(), system_event])\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, pandas, json, datetime\n```python\nimport numpy as np\nimport pandas as pd\nimport json\nfrom datetime import datetime\n\n# Constants\nDATA_PATH = '/path/to/data.csv'\nJSON_PATH = '/path/to/data.json'\n\ndef convert_csv_to_json():\n    \"\"\"\n    Read data from a CSV file, perform some calculations on the data,\n    and then write the processed data to a JSON file.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - pandas\n    - json\n    - datetime\n\n    Example:\n    >>> convert_csv_to_json()\n    \"\"\"\n    df = pd.read_csv(DATA_PATH)\n    df['processed_data'] = np.sqrt(df['raw_data'])\n    df['timestamp'] = datetime.now()\n\n    json_data = df.to_dict(orient='records')\n\n    with open(JSON_PATH, 'w') as file:\n        json.dump(json_data, file)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, zipfile, datetime, pathlib\n```python\nimport os\nimport zipfile\nfrom datetime import datetime\nfrom pathlib import Path\n\n# Constants\nSOURCE_DIR = '/path/to/source/directory'\nDEST_DIR = '/path/to/destination/directory'\nZIP_NAME = f'backup_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.zip'\n\ndef create_backup():\n    \"\"\"\n    Create a zip file containing all files in the source directory and save it to the destination directory.\n\n    Returns:\n    None\n\n    Requirements:\n    - os\n    - zipfile\n    - datetime\n    - pathlib.Path\n\n    Example:\n    >>> create_backup()\n    \"\"\"\n    with zipfile.ZipFile(os.path.join(DEST_DIR, ZIP_NAME), 'w') as zipf:\n        for file in Path(SOURCE_DIR).rglob('*'):\n            zipf.write(file, arcname=os.path.relpath(file, SOURCE_DIR))\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nrandom, json, datetime, os\n```python\nimport random\nimport json\nfrom datetime import datetime\nimport os\n\n# Constants\nDATA_DIR = '/path/to/data/directory'\nFILE_NAME = f'data_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.json'\nFILE_PATH = os.path.join(DATA_DIR, FILE_NAME)\n\ndef generate_random_data(n):\n    \"\"\"\n    Generate n random integers between 1 and 100, write them to a JSON file.\n\n    Parameters:\n    n (int): The number of random integers to generate.\n\n    Returns:\n    None\n\n    Requirements:\n    - random\n    - json\n    - datetime\n    - os\n\n    Example:\n    >>> generate_random_data(10)\n    \"\"\"\n    data = [random.randint(1, 100) for _ in range(n)]\n\n    with open(FILE_PATH, 'w') as file:\n        json.dump(data, file)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, shutil, datetime, pathlib\n```python\nimport os\nimport shutil\nfrom datetime import datetime\nfrom pathlib import Path\n\n# Constants\nSOURCE_DIR = '/path/to/source/directory'\nDEST_DIR = '/path/to/destination/directory'\nDATE_STR = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n\ndef move_files():\n    \"\"\"\n    Move all files from the source directory to the destination directory and append the current datetime to each filename.\n\n    Returns:\n    None\n\n    Requirements:\n    - os\n    - shutil\n    - datetime\n    - pathlib.Path\n\n    Example:\n    >>> move_files()\n    \"\"\"\n    for file in Path(SOURCE_DIR).rglob('*'):\n        new_name = f'{os.path.splitext(file.name)[0]}_{DATE_STR}{os.path.splitext(file.name)[1]}'\n        shutil.move(str(file), os.path.join(DEST_DIR, new_name))\n```"}
{"seed": "{\"task_id\": 6159900, \"prompt\": \"def f_6159900(myfile):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"\\n\\tf = open(myfile, 'w')\\n\\tf.write(\\\"hi there\\\\n\\\")\\n\\tf.close()\\n\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    file_name = 'myfile'\\n    candidate(file_name)\\n    with open (file_name, 'r') as f:\\n        lines = f.readlines()\\n        assert lines[0] == 'hi there\\\\n'\\n\"], \"entry_point\": \"f_6159900\", \"intent\": \"write line \\\"hi there\\\" to file `myfile`\", \"library\": []}", "generation": "Scenario 1:\ncsv, os, random\n```python\nimport csv\nimport os\nimport random\n\n# Constants\nDATA = [[random.randint(0, 100) for _ in range(5)] for _ in range(10)]\n\ndef write_random_data_to_csv(filepath):\n    \"\"\"\n    Write a list of random numbers to a CSV file.\n\n    Parameters:\n    filepath (str): The file path of the CSV file.\n\n    Returns:\n    None\n\n    Requirements:\n    - csv\n    - os\n    - random\n\n    Example:\n    >>> write_random_data_to_csv('myfile.csv')\n    >>> with open('myfile.csv', 'r') as f:\n    ...     reader = csv.reader(f)\n    ...     for row in reader:\n    ...         print(row)\n    \"\"\"\n    with open(filepath, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerows(DATA)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\njson, os, random\n```python\nimport json\nimport os\nimport random\n\n# Constants\nDATA = {f'key_{i}': random.randint(0, 100) for i in range(10)}\n\ndef write_random_data_to_json(filepath):\n    \"\"\"\n    Write a dictionary of random numbers to a JSON file.\n\n    Parameters:\n    filepath (str): The file path of the JSON file.\n\n    Returns:\n    None\n\n    Requirements:\n    - json\n    - os\n    - random\n\n    Example:\n    >>> write_random_data_to_json('myfile.json')\n    >>> with open('myfile.json', 'r') as f:\n    ...     data = json.load(f)\n    ...     print(data)\n    \"\"\"\n    with open(filepath, 'w') as file:\n        json.dump(DATA, file)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npickle, os, random\n```python\nimport pickle\nimport os\nimport random\n\n# Constants\nDATA = [random.randint(0, 100) for _ in range(10)]\n\ndef write_random_data_to_pickle(filepath):\n    \"\"\"\n    Write a list of random numbers to a pickle file.\n\n    Parameters:\n    filepath (str): The file path of the pickle file.\n\n    Returns:\n    None\n\n    Requirements:\n    - pickle\n    - os\n    - random\n\n    Example:\n    >>> write_random_data_to_pickle('myfile.pickle')\n    >>> with open('myfile.pickle', 'rb') as f:\n    ...     data = pickle.load(f)\n    ...     print(data)\n    \"\"\"\n    with open(filepath, 'wb') as file:\n        pickle.dump(DATA, file)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, os, random\n```python\nimport pandas as pd\nimport os\nimport random\n\n# Constants\nDATA = pd.DataFrame({\n    'col1': [random.randint(0, 100) for _ in range(10)],\n    'col2': [random.randint(0, 100) for _ in range(10)]\n})\n\ndef write_random_data_to_excel(filepath):\n    \"\"\"\n    Write a DataFrame of random numbers to an Excel file.\n\n    Parameters:\n    filepath (str): The file path of the Excel file.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - os\n    - random\n\n    Example:\n    >>> write_random_data_to_excel('myfile.xlsx')\n    >>> df = pd.read_excel('myfile.xlsx')\n    >>> print(df)\n    \"\"\"\n    DATA.to_excel(filepath, index=False)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nmatplotlib, os, random\n```python\nimport matplotlib.pyplot as plt\nimport os\nimport random\n\n# Constants\nDATA = [random.randint(0, 100) for _ in range(10)]\n\ndef write_random_data_to_png(filepath):\n    \"\"\"\n    Generate a bar plot of a list of random numbers and save it to a PNG file.\n\n    Parameters:\n    filepath (str): The file path of the PNG file.\n\n    Returns:\n    None\n\n    Requirements:\n    - matplotlib.pyplot\n    - os\n    - random\n\n    Example:\n    >>> write_random_data_to_png('myfile.png')\n    >>> plt.imshow(plt.imread('myfile.png'))\n    \"\"\"\n    plt.bar(range(len(DATA)), DATA)\n    plt.savefig(filepath)\n    plt.close()\n```\n"}
{"seed": "{\"task_id\": 6159900, \"prompt\": \"def f_6159900():\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"\\n\\twith open('somefile.txt', 'a') as the_file: \\n\\t\\tthe_file.write('Hello\\\\n')\\n\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    file_name = 'somefile.txt'\\n    candidate()\\n    with open (file_name, 'r') as f:\\n        lines = f.readlines()\\n        assert lines[0] == 'Hello\\\\n'\\n\"], \"entry_point\": \"f_6159900\", \"intent\": \"write line \\\"Hello\\\" to file `somefile.txt`\", \"library\": []}", "generation": "Scenario 1: \nos, pathlib, random, string\n```python\nimport os\nfrom pathlib import Path\nimport random\nimport string\n\n# Constants\nFILE_NAME = 'random_words.txt'\nWORD_COUNT = 100\nWORD_LENGTH = 10\n\ndef generate_random_words_file():\n    \"\"\"\n    Generate a file with a list of random words, each word in a new line.\n\n    Requirements:\n    - os\n    - pathlib.Path\n    - random\n    - string\n\n    Example:\n    >>> generate_random_words_file()\n    \"\"\"\n    with open(FILE_NAME, 'w') as f:\n        for _ in range(WORD_COUNT):\n            word = ''.join(random.choices(string.ascii_lowercase, k=WORD_LENGTH))\n            f.write(f'{word}\\n')\n\n    return Path(FILE_NAME).exists()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nglob, os, shutil\n```python\nimport glob\nimport os\nimport shutil\n\n# Constants\nSOURCE_DIR = './source/'\nTARGET_DIR = './target/'\nEXTENSION = '.txt'\n\ndef move_files_with_extension():\n    \"\"\"\n    Move all the files with a specific extension from a source directory to a target directory.\n\n    Requirements:\n    - glob\n    - os\n    - shutil\n\n    Example:\n    >>> move_files_with_extension()\n    \"\"\"\n    files = glob.glob(f'{SOURCE_DIR}*{EXTENSION}')\n\n    for file in files:\n        shutil.move(file, TARGET_DIR)\n\n    return len(glob.glob(f'{TARGET_DIR}*{EXTENSION}')) == len(files)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \ncsv, random, string\n```python\nimport csv\nimport random\nimport string\n\n# Constants\nCSV_FILE = 'random_data.csv'\nROW_COUNT = 100\nID_LENGTH = 10\nRANGE_START = 0\nRANGE_END = 10000\n\ndef generate_random_csv():\n    \"\"\"\n    Generate a CSV file with random data. Each row contains an ID (random string) and a random integer.\n\n    Requirements:\n    - csv\n    - random\n    - string\n\n    Example:\n    >>> generate_random_csv()\n    \"\"\"\n    with open(CSV_FILE, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow([\"ID\", \"Value\"])\n\n        for _ in range(ROW_COUNT):\n            id = ''.join(random.choices(string.ascii_lowercase, k=ID_LENGTH))\n            value = random.randint(RANGE_START, RANGE_END)\n            writer.writerow([id, value])\n\n    return os.path.exists(CSV_FILE)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nos, zipfile\n```python\nimport os\nfrom zipfile import ZipFile\n\n# Constants\nDIRECTORY_TO_ZIP = './directory_to_zip/'\nZIP_FILE_NAME = 'zipped_directory.zip'\n\ndef zip_directory():\n    \"\"\"\n    Zip a directory and its content into a .zip file.\n\n    Requirements:\n    - os\n    - zipfile.ZipFile\n\n    Example:\n    >>> zip_directory()\n    \"\"\"\n    with ZipFile(ZIP_FILE_NAME, 'w') as zipf:\n        for root, dirs, files in os.walk(DIRECTORY_TO_ZIP):\n            for file in files:\n                zipf.write(os.path.join(root, file))\n\n    return os.path.exists(ZIP_FILE_NAME)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nos, hashlib\n```python\nimport os\nimport hashlib\n\n# Constants\nFILE_PATH = 'file_to_hash.txt'\n\ndef hash_file():\n    \"\"\"\n    Calculate the SHA-256 hash of a file.\n\n    Requirements:\n    - os\n    - hashlib\n\n    Example:\n    >>> hash_file()\n    \"\"\"\n    with open(FILE_PATH, 'rb') as f:\n        bytes = f.read()\n        readable_hash = hashlib.sha256(bytes).hexdigest()\n\n    return readable_hash\n```\n"}
{"seed": "{\"task_id\": 19527279, \"prompt\": \"def f_19527279(s):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"s.encode('iso-8859-15')\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('table') == b'table'\\n\", \"\\n    assert candidate('hello world!') == b'hello world!'\\n\"], \"entry_point\": \"f_19527279\", \"intent\": \"convert unicode string `s` to ascii\", \"library\": []}", "generation": "Scenario 1: \nhashlib, base64, binascii\n```python\nimport hashlib\nimport base64\nimport binascii\n\n# Constants\nHASH_TYPE = 'sha256'\n\ndef hash_and_encode_string(string, encoding='iso-8859-15'):\n    \"\"\"\n    Hash a string using SHA-256, then encode the hash to base64.\n\n    Parameters:\n    string (str): The string to be hashed and encoded.\n    encoding (str): The encoding to use.\n\n    Returns:\n    str: The base64-encoded hash of the string.\n\n    Requirements:\n    - hashlib\n    - base64\n    - binascii\n\n    Example:\n    >>> hash_and_encode_string('hello world!')\n    \"\"\"\n    encoded_string = string.encode(encoding)\n    hashed_string = hashlib.new(HASH_TYPE, encoded_string).digest()\n    base64_encoded_hash = base64.b64encode(hashed_string)\n\n    return base64_encoded_hash.decode()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, codecs, chardet\n```python\nimport os\nimport codecs\nimport chardet\n\ndef convert_file_encoding(file_path, target_encoding='iso-8859-15'):\n    \"\"\"\n    Convert the encoding of a file to a target encoding.\n\n    Parameters:\n    file_path (str): The path of the file to convert.\n    target_encoding (str): The target encoding.\n\n    Returns:\n    str: The path of the converted file.\n\n    Requirements:\n    - os\n    - codecs\n    - chardet\n\n    Example:\n    >>> convert_file_encoding('/path/to/file.txt')\n    \"\"\"\n    with open(file_path, 'rb') as file:\n        rawdata = file.read()\n        result = chardet.detect(rawdata)\n        original_encoding = result['encoding']\n\n    # Read the file again and convert it to target encoding\n    with codecs.open(file_path, 'r', original_encoding) as file:\n        content = file.read()\n    \n    new_file_path = os.path.splitext(file_path)[0] + f'_{target_encoding}' + os.path.splitext(file_path)[1]\n    \n    with codecs.open(new_file_path, 'w', target_encoding) as file:\n        file.write(content)\n\n    return new_file_path\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, matplotlib, string\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport string\n\ndef plot_character_frequencies(text, encoding='iso-8859-15'):\n    \"\"\"\n    Plot the frequencies of characters in a text.\n\n    Parameters:\n    text (str): The text to analyze.\n    encoding (str): The encoding of the text.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - string\n\n    Example:\n    >>> plot_character_frequencies('hello world!')\n    \"\"\"\n    # Encode the text\n    encoded_text = text.encode(encoding)\n    \n    # Initialize an array to store the frequencies\n    frequencies = np.zeros(len(string.printable))\n    \n    # Count the frequencies of each character\n    for character in encoded_text:\n        if chr(character) in string.printable:\n            frequencies[string.printable.index(chr(character))] += 1\n    \n    # Plot the frequencies\n    plt.bar(string.printable, frequencies)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, nltk, seaborn\n```python\nimport pandas as pd\nfrom nltk.corpus import stopwords\nimport seaborn as sns\n\n# Constants\nSTOPWORDS = set(stopwords.words('english'))\n\ndef plot_word_frequencies(text, encoding='iso-8859-15'):\n    \"\"\"\n    Plot the frequencies of non-stopwords in a text.\n\n    Parameters:\n    text (str): The text to analyze.\n    encoding (str): The encoding of the text.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - nltk.corpus\n    - seaborn\n\n    Example:\n    >>> plot_word_frequencies('hello world!')\n    \"\"\"\n    # Encode the text\n    encoded_text = text.encode(encoding).decode()\n    \n    # Tokenize the text and remove stopwords\n    words = [word for word in encoded_text.split() if word not in STOPWORDS]\n    \n    # Count the frequencies of each word\n    word_counts = pd.Series(words).value_counts()\n    \n    # Plot the frequencies\n    sns.barplot(word_counts.index, word_counts.values)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, scipy.spatial, matplotlib\n```python\nimport numpy as np\nfrom scipy.spatial import distance\nimport matplotlib.pyplot as plt\n\ndef calculate_and_plot_distances(points, encoding='iso-8859-15'):\n    \"\"\"\n    Calculate and plot the pairwise distances between a list of points.\n\n    Parameters:\n    points (str): A string of coordinates, each separated by a semicolon.\n    encoding (str): The encoding of the string.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - scipy.spatial\n    - matplotlib\n\n    Example:\n    >>> calculate_and_plot_distances('0,0;1,1;2,2')\n    \"\"\"\n    # Encode the string and split it into points\n    points = points.encode(encoding).decode().split(';')\n    points = np.array([list(map(float, point.split(','))) for point in points])\n    \n    # Calculate the pairwise distances\n    distances = distance.pdist(points)\n    \n    # Plot the distances\n    plt.hist(distances)\n    plt.show()\n```\nAbove is the illustration."}
{"seed": "{\"task_id\": 356483, \"prompt\": \"def f_356483(text):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"re.findall('Test([0-9.]*[0-9]+)', text)\", \"test_start\": \"\\nimport re \\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('Test0.9ssd') == ['0.9']\\n\", \"\\n    assert candidate('Test0.0 ..2ssd') == ['0.0']\\n\"], \"entry_point\": \"f_356483\", \"intent\": \"Find all numbers and dots from a string `text` using regex\", \"library\": [\"re\"]}", "generation": "Scenario 1:\nre, json, urllib.request, bs4\n```python\nimport re\nimport json\nimport urllib.request\nfrom bs4 import BeautifulSoup\n\n# Constants\nURL = \"https://www.python.org/downloads/\"\n\ndef get_python_versions_from_website():\n    \"\"\"\n    Fetch the Python website, extract the version numbers of all Python releases \n    from the html using regex and return them as a list.\n    \n    Returns:\n    list: The list of Python version numbers.\n\n    Requirements:\n    - re\n    - json\n    - urllib.request\n    - bs4.BeautifulSoup\n    \n    Example:\n    >>> print(get_python_versions_from_website())\n    \"\"\"\n    with urllib.request.urlopen(URL) as response:\n        html = response.read()\n        soup = BeautifulSoup(html, 'html.parser')\n        version_text = soup.find('ol', {'class': 'list-row-container menu'}).get_text()\n        versions = re.findall('Python ([0-9.]*[0-9]+)', version_text)\n        \n    return versions\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nre, pandas, matplotlib.pyplot, numpy\n```python\nimport re\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nDATA = [\n    'Product1 23.45',\n    'Product2 34.56',\n    'Product3 45.67',\n    'Product4 56.78',\n    'Product5 67.89'\n]\n\ndef plot_product_prices():\n    \"\"\"\n    Extract the product names and prices from a list of strings using regex, \n    create a pandas DataFrame from the data, and plot the prices.\n\n    Requirements:\n    - re\n    - pandas\n    - matplotlib.pyplot\n    - numpy\n    \n    Example:\n    >>> plot_product_prices()\n    \"\"\"\n    products = []\n    prices = []\n\n    for item in DATA:\n        match = re.search('(\\w+) ([0-9.]*[0-9]+)', item)\n        if match:\n            products.append(match.group(1))\n            prices.append(float(match.group(2)))\n\n    df = pd.DataFrame({'Product': products, 'Price': prices})\n    df.plot(kind='bar', x='Product', y='Price', rot=0)\n    plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nre, datetime, pytz, dateutil.parser\n```python\nimport re\nfrom datetime import datetime\nimport pytz\nfrom dateutil.parser import parse\n\n# Constants\nDATE_PATTERN = '(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})'\nTIMEZONE = 'America/New_York'\n\ndef get_earliest_date_in_new_york_timezone(text):\n    \"\"\"\n    Find all datetime strings in a text using regex, convert them to datetime objects, \n    change their timezone to New York, and return the earliest one.\n\n    Parameters:\n    text (str): The input text.\n\n    Returns:\n    datetime: The earliest datetime in New York timezone.\n\n    Requirements:\n    - re\n    - datetime\n    - pytz\n    - dateutil.parser\n    \n    Example:\n    >>> get_earliest_date_in_new_york_timezone('The first event is at 2023-06-15 12:00:00 and the second one is at 2023-06-16 12:00:00')\n    \"\"\"\n    dates = re.findall(DATE_PATTERN, text)\n    dates = [parse(date).replace(tzinfo=pytz.UTC).astimezone(pytz.timezone(TIMEZONE)) for date in dates]\n\n    return min(dates)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nre, pandas, matplotlib.pyplot, random\n```python\nimport re\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nDATA = [\n    'Player1 23',\n    'Player2 34',\n    'Player3 45',\n    'Player4 56',\n    'Player5 67'\n]\nSCORE_PATTERN = '(\\w+) (\\d+)'\n\ndef plot_random_scores():\n    \"\"\"\n    Extract the player names and scores from a list of strings using regex, \n    add random scores to the existing ones, create a pandas DataFrame from the data, \n    and plot the total scores.\n\n    Requirements:\n    - re\n    - pandas\n    - matplotlib.pyplot\n    - random\n    \n    Example:\n    >>> plot_random_scores()\n    \"\"\"\n    players = []\n    scores = []\n\n    for item in DATA:\n        match = re.search(SCORE_PATTERN, item)\n        if match:\n            players.append(match.group(1))\n            scores.append(int(match.group(2)) + randint(1, 10))\n\n    df = pd.DataFrame({'Player': players, 'Score': scores})\n    df.plot(kind='bar', x='Player', y='Score', rot=0)\n    plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nre, datetime, pytz, dateutil.parser\n```python\nimport re\nfrom datetime import datetime\nimport pytz\nfrom dateutil.parser import parse\n\n# Constants\nDATE_PATTERN = '(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})'\nTIMEZONES = ['America/New_York', 'Europe/London', 'Asia/Shanghai', 'Asia/Tokyo', 'Australia/Sydney']\n\ndef get_latest_date_in_each_timezone(text):\n    \"\"\"\n    Find all datetime strings in a text using regex, convert them to datetime objects, \n    change their timezone to each timezone in a list, and return the latest datetime in each timezone.\n\n    Parameters:\n    text (str): The input text.\n\n    Returns:\n    dict: The latest datetime in each timezone.\n\n    Requirements:\n    - re\n    - datetime\n    - pytz\n    - dateutil.parser\n    \n    Example:\n    >>> get_latest_date_in_each_timezone('The first event is at 2023-06-15 12:00:00 and the second one is at 2023-06-16 12:00:00')\n    \"\"\"\n    dates = re.findall(DATE_PATTERN, text)\n    dates = [parse(date) for date in dates]\n\n    latest_dates = {}\n\n    for timezone in TIMEZONES:\n        timezone_dates = [date.replace(tzinfo=pytz.UTC).astimezone(pytz.timezone(timezone)) for date in dates]\n        latest_dates[timezone] = max(timezone_dates)\n\n    return latest_dates\n```"}
{"seed": "{\"task_id\": 38081866, \"prompt\": \"def f_38081866():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"os.system('powershell.exe', 'script.ps1')\", \"test_start\": \"\\nimport os\\nfrom unittest.mock import Mock\\n\\ndef check(candidate):\", \"test\": [\"\\n    os.system = Mock()\\n    try:\\n        candidate()\\n        assert True\\n    except:\\n        assert False\\n\"], \"entry_point\": \"f_38081866\", \"intent\": \"execute script 'script.ps1' using 'powershell.exe' shell\", \"library\": [\"os\"]}", "generation": "Scenario 1:\nos, subprocess, sys, shutil\n```python\nimport os\nimport subprocess\nimport sys\nimport shutil\n\n# Constants\nSCRIPT_NAME = 'script.ps1'\nPYTHON_SCRIPT_NAME = 'script.py'\nSHELL_NAME = 'powershell.exe'\nPYTHON_SHELL_NAME = 'python.exe'\n\ndef run_scripts_with_different_shells():\n    \"\"\"\n    Execute two scripts ('script.ps1' and 'script.py') using two different shells ('powershell.exe' and 'python.exe') \n    and compare their output. If the outputs are not the same, then the function will raise an exception.\n\n    Requirements:\n    - os\n    - subprocess\n    - sys\n    - shutil\n\n    Example:\n    >>> run_scripts_with_different_shells()\n    \"\"\"\n    powershell_output = subprocess.check_output([SHELL_NAME, SCRIPT_NAME]).decode(sys.stdout.encoding)\n    python_output = subprocess.check_output([PYTHON_SHELL_NAME, PYTHON_SCRIPT_NAME]).decode(sys.stdout.encoding)\n\n    if powershell_output != python_output:\n        raise Exception('The outputs of the two scripts are not the same.')\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, subprocess, time, datetime\n```python\nimport os\nimport subprocess\nimport time\nfrom datetime import datetime, timedelta\n\n# Constants\nSCRIPT_NAME = 'script.ps1'\nSHELL_NAME = 'powershell.exe'\nTIMEOUT = 60  # 1 minute\n\ndef execute_script_with_timeout():\n    \"\"\"\n    Execute a script ('script.ps1') using a shell ('powershell.exe') and terminate it if it runs longer than a minute.\n\n    Requirements:\n    - os\n    - subprocess\n    - time\n    - datetime\n\n    Example:\n    >>> execute_script_with_timeout()\n    \"\"\"\n    start_time = datetime.now()\n    process = subprocess.Popen([SHELL_NAME, SCRIPT_NAME])\n\n    while True:\n        if process.poll() is not None:\n            break\n        elif datetime.now() - start_time > timedelta(seconds=TIMEOUT):\n            process.terminate()\n            break\n\n        time.sleep(1)  # check every second\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, subprocess, csv, pathlib\n```python\nimport os\nimport subprocess\nimport csv\nfrom pathlib import Path\n\n# Constants\nSCRIPT_NAME = 'script.ps1'\nSHELL_NAME = 'powershell.exe'\nCSV_FILE = 'scripts.csv'\n\ndef execute_scripts_from_csv():\n    \"\"\"\n    Execute a list of scripts from a CSV file using a shell ('powershell.exe'). The CSV file should \n    have a column named 'script' containing the script names.\n\n    Requirements:\n    - os\n    - subprocess\n    - csv\n    - pathlib\n\n    Example:\n    >>> execute_scripts_from_csv()\n    \"\"\"\n    with open(CSV_FILE, 'r') as file:\n        reader = csv.DictReader(file)\n        for row in reader:\n            script_path = Path(row['script'])\n            if script_path.exists():\n                subprocess.run([SHELL_NAME, str(script_path)])\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, subprocess, json, pathlib\n```python\nimport os\nimport subprocess\nimport json\nfrom pathlib import Path\n\n# Constants\nSCRIPT_NAME = 'script.ps1'\nSHELL_NAME = 'powershell.exe'\nJSON_FILE = 'scripts.json'\n\ndef execute_scripts_from_json():\n    \"\"\"\n    Execute a list of scripts from a JSON file using a shell ('powershell.exe'). The JSON file should \n    contain an array of script names.\n\n    Requirements:\n    - os\n    - subprocess\n    - json\n    - pathlib\n\n    Example:\n    >>> execute_scripts_from_json()\n    \"\"\"\n    with open(JSON_FILE, 'r') as file:\n        scripts = json.load(file)\n        for script in scripts:\n            script_path = Path(script)\n            if script_path.exists():\n                subprocess.run([SHELL_NAME, str(script_path)])\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, subprocess, glob, pathlib\n```python\nimport os\nimport subprocess\nimport glob\nfrom pathlib import Path\n\n# Constants\nSCRIPT_NAME = 'script.ps1'\nSHELL_NAME = 'powershell.exe'\nSCRIPT_EXTENSION = '.ps1'\n\ndef execute_all_scripts_in_directory():\n    \"\"\"\n    Execute all scripts with a certain extension in a directory using a shell ('powershell.exe').\n\n    Requirements:\n    - os\n    - subprocess\n    - glob\n    - pathlib\n\n    Example:\n    >>> execute_all_scripts_in_directory()\n    \"\"\"\n    scripts = glob.glob('*' + SCRIPT_EXTENSION)\n    for script in scripts:\n        script_path = Path(script)\n        if script_path.exists():\n            subprocess.run([SHELL_NAME, str(script_path)])\n```\nEach of the above scenarios brings more complexity to the original task by adding additional requirements and functionality. They also utilize multiple Python libraries to accomplish each task."}
{"seed": "{\"task_id\": 7349646, \"prompt\": \"def f_7349646(b):\\n\\t\", \"suffix\": \"\\n\\treturn b\", \"canonical_solution\": \"b.sort(key=lambda x: x[2])\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    b = [(1,2,3), (4,5,6), (7,8,0)]\\n    assert candidate(b) == [(7,8,0), (1,2,3), (4,5,6)]\\n\", \"\\n    b = [(1,2,'a'), (4,5,'c'), (7,8,'A')]\\n    assert candidate(b) == [(7,8,'A'), (1,2,'a'), (4,5,'c')]\\n\"], \"entry_point\": \"f_7349646\", \"intent\": \"Sort a list of tuples `b` by third item in the tuple\", \"library\": []}", "generation": "Scenario 1: \npandas, random, itertools, operator\n```python\nimport pandas as pd\nfrom random import sample\nfrom itertools import combinations\nfrom operator import itemgetter\n\n# Constants\nLETTERS = list('abcdefghijklmnopqrstuvwxyz')\n\ndef get_combinations_sorted(list_length, num_combinations):\n    \"\"\"\n    Generate a dataframe of random combinations of lowercase letters with the \n    length of the combinations specified. Sort the combinations by the third \n    letter of each combination.\n\n    Parameters:\n    list_length (int): The length of the combinations.\n    num_combinations (int): The number of combinations to generate.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the sorted combinations.\n\n    Requirements:\n    - pandas\n    - random.sample\n    - itertools.combinations\n    - operator.itemgetter\n    \n    Example:\n    >>> get_combinations_sorted(5, 10)\n    \"\"\"\n    combinations_list = []\n    \n    for _ in range(num_combinations):\n        combo = ''.join(sample(LETTERS, list_length))\n        combinations_list.append(combo)\n    \n    combinations_list.sort(key=itemgetter(2))\n    \n    df = pd.DataFrame(combinations_list, columns=['Combination'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, datetime, pandas\n```python\nimport numpy as np\nfrom datetime import datetime, timedelta\nimport pandas as pd\n\n# Constants\nSTART_DATE = datetime(2020, 1, 1)\n\ndef generate_date_dataframe(start_date, num_days, sort_by):\n    \"\"\"\n    Generate a dataframe with dates starting from a given date for a certain \n    number of days. Each row in the dataframe will have a date and a random \n    number. The dataframe will be sorted by a given column.\n\n    Parameters:\n    start_date (datetime): The start date.\n    num_days (int): The number of days.\n    sort_by (str): The column to sort by. Can be 'Date' or 'Number'.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the dates and random numbers.\n\n    Requirements:\n    - datetime, timedelta\n    - numpy.random.randint\n    - pandas\n    \n    Example:\n    >>> generate_date_dataframe(datetime(2020, 1, 1), 100, 'Number')\n    \"\"\"\n    dates = [start_date + timedelta(days=i) for i in range(num_days)]\n    numbers = np.random.randint(1, 100, size=num_days)\n    \n    df = pd.DataFrame({'Date': dates, 'Number': numbers})\n    \n    df.sort_values(by=sort_by, inplace=True)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nrandom, datetime, pandas\n```python\nimport random\nfrom datetime import datetime, timedelta\nimport pandas as pd\n\n# Constants\nPEOPLE = ['Alice', 'Bob', 'Charlie', 'David', 'Eve']\n\ndef generate_attendance_dataframe(start_date, num_days):\n    \"\"\"\n    Generate a dataframe with random attendance data for a list of people over \n    a certain number of days starting from a given date.\n\n    Parameters:\n    start_date (datetime): The start date.\n    num_days (int): The number of days.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the attendance data.\n\n    Requirements:\n    - datetime, timedelta\n    - random.choice\n    - pandas\n    \n    Example:\n    >>> generate_attendance_dataframe(datetime(2020, 1, 1), 100)\n    \"\"\"\n    dates = [start_date + timedelta(days=i) for i in range(num_days)]\n    attendance_data = []\n    \n    for date in dates:\n        for person in PEOPLE:\n            attendance = random.choice([True, False])\n            attendance_data.append([date, person, attendance])\n    \n    df = pd.DataFrame(attendance_data, columns=['Date', 'Person', 'Attendance'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nitertools, operator, string\n```python\nfrom itertools import permutations\nfrom operator import itemgetter\nimport string\n\n# Constants\nLETTERS = string.ascii_lowercase\n\ndef generate_permutations_sorted(list_length, num_permutations):\n    \"\"\"\n    Generate a list of random permutations of lowercase letters with the \n    length of the permutations specified. Sort the permutations by the third \n    letter of each permutation.\n\n    Parameters:\n    list_length (int): The length of the permutations.\n    num_permutations (int): The number of permutations to generate.\n\n    Returns:\n    list: A list of the sorted permutations.\n\n    Requirements:\n    - itertools.permutations\n    - operator.itemgetter\n    - string.ascii_lowercase\n    \n    Example:\n    >>> generate_permutations_sorted(5, 10)\n    \"\"\"\n    permutations_list = []\n    \n    for _ in range(num_permutations):\n        perm = ''.join(permutations(LETTERS, list_length))\n        permutations_list.append(perm)\n    \n    permutations_list.sort(key=itemgetter(2))\n\n    return permutations_list\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, datetime, pandas\n```python\nimport numpy as np\nfrom datetime import datetime, timedelta\nimport pandas as pd\n\n# Constants\nSTART_DATE = datetime(2020, 1, 1)\nNUM_DAYS = 365\n\ndef generate_temperature_dataframe(start_date, num_days):\n    \"\"\"\n    Generate a dataframe with random temperature data for a certain number of \n    days starting from a given date.\n\n    Parameters:\n    start_date (datetime): The start date.\n    num_days (int): The number of days.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the temperature data.\n\n    Requirements:\n    - datetime, timedelta\n    - numpy.random.normal\n    - pandas\n    \n    Example:\n    >>> generate_temperature_dataframe(datetime(2020, 1, 1), 365)\n    \"\"\"\n    dates = [start_date + timedelta(days=i) for i in range(num_days)]\n    temperatures = np.random.normal(20, 5, size=num_days)\n    \n    df = pd.DataFrame({'Date': dates, 'Temperature': temperatures})\n\n    return df\n```\n"}
{"seed": "{\"task_id\": 10607688, \"prompt\": \"def f_10607688():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"datetime.datetime.now()\", \"test_start\": \"\\nimport datetime\\n\\ndef check(candidate):\", \"test\": [\"\\n    y = candidate()\\n    assert y.year >= 2022\\n\"], \"entry_point\": \"f_10607688\", \"intent\": \"create a datetime with the current date & time\", \"library\": [\"datetime\"]}", "generation": "Scenario 1:\npandas, datetime, random, matplotlib, numpy\n```python\nimport pandas as pd\nfrom datetime import datetime\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nCATEGORIES = ['Electronics', 'Clothing', 'Home & Kitchen', 'Sports & Outdoors', 'Books']\nSTATES = ['CA', 'TX', 'FL', 'NY', 'PA']\n\ndef generate_sales_data(start_date, end_date):\n    \"\"\"\n    Generate a data frame of sales data for different categories of products in different states \n    over a given time period. Plot a bar chart of total sales by category.\n\n    Parameters:\n    start_date (datetime): The start date of the time period.\n    end_date (datetime): The end date of the time period.\n\n    Returns:\n    DataFrame: A pandas DataFrame with sales data.\n\n    Requirements:\n    - pandas\n    - datetime\n    - random\n    - matplotlib.pyplot\n    - numpy\n\n    Example:\n    >>> start = datetime(2023, 1, 1)\n    >>> end = datetime(2023, 12, 31)\n    >>> df = generate_sales_data(start, end)\n    >>> print(df)\n    >>> df.groupby('Category')['Sales'].sum().plot(kind='bar')\n    \"\"\"\n    dates = pd.date_range(start_date, end_date)\n    data = []\n\n    for date in dates:\n        for category in CATEGORIES:\n            for state in STATES:\n                sales = np.random.normal(5000, 2000)\n                data.append([date, category, state, sales])\n\n    df = pd.DataFrame(data, columns=['Date', 'Category', 'State', 'Sales'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ndatetime, pytz, dateutil, re, os\n```python\nimport os\nimport re\nfrom datetime import datetime\nimport pytz\nfrom dateutil.parser import parse\n\n# Constants\nLOG_DIR = '/var/log/my_app'\nLOG_REGEX = re.compile(r'(?P<timestamp>\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}) - (?P<level>\\w+) - (?P<message>.*)')\n\ndef extract_logs_after_date(date_str, level):\n    \"\"\"\n    Extract logs with a specified level that are timestamped after a given datetime from log files \n    in a specific directory.\n\n    Parameters:\n    date_str (str): The date string in \"yyyy-mm-dd hh:mm:ss\" format.\n    level (str): The log level to filter on.\n\n    Returns:\n    list: A list of log entries.\n\n    Requirements:\n    - datetime\n    - pytz\n    - dateutil.parser\n    - re\n    - os\n\n    Example:\n    >>> extract_logs_after_date('2023-01-01 00:00:00', 'ERROR')\n    \"\"\"\n    given_date = parse(date_str).replace(tzinfo=pytz.UTC)\n    logs = []\n\n    for filename in os.listdir(LOG_DIR):\n        with open(os.path.join(LOG_DIR, filename), 'r') as f:\n            for line in f:\n                match = LOG_REGEX.match(line)\n                if match:\n                    log_date = parse(match.group('timestamp')).replace(tzinfo=pytz.UTC)\n                    log_level = match.group('level')\n                    log_message = match.group('message')\n\n                    if log_date > given_date and log_level == level:\n                        logs.append((log_date, log_level, log_message))\n\n    return logs\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ndatetime, calendar, holidays, pandas\n```python\nimport pandas as pd\nfrom datetime import datetime\nimport calendar\nimport holidays\n\n# Constants\nSTART_DATE = datetime(2023, 1, 1)\nEND_DATE = datetime(2023, 12, 31)\nCOUNTRY_HOLIDAYS = holidays.US()\n\ndef get_business_days(start_date=START_DATE, end_date=END_DATE):\n    \"\"\"\n    Generate a list of business days between two dates, excluding public holidays.\n\n    Parameters:\n    start_date (datetime): The start date.\n    end_date (datetime): The end date.\n\n    Returns:\n    list: A list of business days.\n\n    Requirements:\n    - pandas\n    - datetime\n    - calendar\n    - holidays\n\n    Example:\n    >>> business_days = get_business_days()\n    >>> print(business_days)\n    \"\"\"\n    dates = pd.date_range(start_date, end_date)\n    business_days = [date for date in dates if date.weekday() < 5 and date not in COUNTRY_HOLIDAYS]\n\n    return business_days\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ndatetime, pytz, numpy, matplotlib\n```python\nimport numpy as np\nfrom datetime import datetime\nimport pytz\nimport matplotlib.pyplot as plt\n\n# Constants\nDATA_POINTS = 1000\nSTART_DATE = datetime(2023, 1, 1, tzinfo=pytz.UTC)\n\ndef plot_random_walk():\n    \"\"\"\n    Generate a random walk time series with a given number of data points starting from a specific date, \n    and plot the time series.\n\n    Requirements:\n    - numpy\n    - datetime\n    - pytz\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_random_walk()\n    \"\"\"\n    dates = pd.date_range(START_DATE, periods=DATA_POINTS)\n    steps = np.random.choice([-1, 1], DATA_POINTS)\n    walk = np.cumsum(steps)\n\n    plt.figure(figsize=(10, 5))\n    plt.plot(dates, walk)\n    plt.xlabel('Date')\n    plt.ylabel('Value')\n    plt.title('Random Walk Time Series')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\ndatetime, pytz, requests, bs4, pandas\n```python\nimport requests\nfrom bs4 import BeautifulSoup\nfrom datetime import datetime\nimport pytz\nimport pandas as pd\n\n# Constants\nURL = 'https://www.somesite.com'\nHEADERS = {'User-Agent': 'Mozilla/5.0'}\n\ndef scrape_data():\n    \"\"\"\n    Scrape data from a website and store the data along with the current UTC time in a pandas DataFrame.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the scraped data and timestamp.\n\n    Requirements:\n    - requests\n    - bs4.BeautifulSoup\n    - datetime\n    - pytz\n    - pandas\n\n    Example:\n    >>> df = scrape_data()\n    >>> print(df)\n    \"\"\"\n    response = requests.get(URL, headers=HEADERS)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    items = soup.find_all('div', class_='item')\n\n    data = []\n    for item in items:\n        name = item.find('h2').text\n        price = item.find('span', class_='price').text\n        timestamp = datetime.now().astimezone(pytz.UTC)\n        data.append([name, price, timestamp])\n\n    df = pd.DataFrame(data, columns=['Name', 'Price', 'Timestamp'])\n\n    return df\n```"}
{"seed": "{\"task_id\": 30843103, \"prompt\": \"def f_30843103(lst):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"next(i for i, x in enumerate(lst) if not isinstance(x, bool) and x == 1)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    lst = [True, False, 1, 3]\\n    assert candidate(lst) == 2\\n\"], \"entry_point\": \"f_30843103\", \"intent\": \"get the index of an integer `1` from a list `lst` if the list also contains boolean items\", \"library\": []}", "generation": "Scenario 1:\n```python\nimport numpy as np\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\n# Constants\nCATEGORICAL_COLUMNS = ['gender', 'region']\nNUMERIC_COLUMNS = ['age', 'income']\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess a dataset by filling missing values, encoding categorical variables \n    and normalizing numerical variables.\n\n    Parameters:\n    data (numpy array): The 2D array containing the raw data.\n\n    Returns:\n    numpy array: The preprocessed data.\n\n    Requirements:\n    - numpy\n    - sklearn.preprocessing.OneHotEncoder\n    - sklearn.impute.SimpleImputer\n    - sklearn.compose.ColumnTransformer\n    - sklearn.pipeline.Pipeline\n\n    Example:\n    >>> data = np.array([['Male', 'North', 25, 50000], \n    ...                  [np.nan, 'South', 35, np.nan], \n    ...                  ['Female', 'East', np.nan, 80000]])\n    >>> preprocess_data(data)\n    \"\"\"\n    # Define preprocessing pipelines\n    categorical_pipe = Pipeline(steps=[\n        ('imputer', SimpleImputer(strategy='most_frequent')),\n        ('encoder', OneHotEncoder(handle_unknown='ignore'))\n    ])\n    numeric_pipe = Pipeline(steps=[\n        ('imputer', SimpleImputer(strategy='median')),\n    ])\n\n    # Combine preprocessing pipelines\n    preprocessor = ColumnTransformer(transformers=[\n        ('num', numeric_pipe, NUMERIC_COLUMNS),\n        ('cat', categorical_pipe, CATEGORICAL_COLUMNS)\n    ])\n\n    return preprocessor.fit_transform(data)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\n# Constants\nCLUSTER_RANGE = range(2, 10)\n\ndef find_optimal_clusters(data):\n    \"\"\"\n    Find the optimal number of clusters for KMeans clustering using the silhouette score.\n\n    Parameters:\n    data (numpy array): The 2D array containing the data.\n\n    Returns:\n    int: The optimal number of clusters.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.cluster.KMeans\n    - sklearn.metrics.silhouette_score\n\n    Example:\n    >>> data = np.array([[1, 2], [1, 4], [1, 0], [4, 2], [4, 4], [4, 0]])\n    >>> find_optimal_clusters(data)\n    \"\"\"\n    scores = []\n    for n_clusters in CLUSTER_RANGE:\n        kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(data)\n        score = silhouette_score(data, kmeans.labels_)\n        scores.append(score)\n\n    optimal_clusters = CLUSTER_RANGE[np.argmax(scores)]\n\n    plt.plot(CLUSTER_RANGE, scores)\n    plt.xlabel('Number of clusters')\n    plt.ylabel('Silhouette Score')\n    plt.show()\n\n    return optimal_clusters\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\n```python\nimport numpy as np\nfrom scipy.stats import pearsonr\nfrom itertools import combinations\n\n# Constants\nTHRESHOLD = 0.8\n\ndef find_highly_correlated_pairs(data):\n    \"\"\"\n    Find pairs of columns in a dataset that have a high Pearson correlation coefficient.\n\n    Parameters:\n    data (numpy array): The 2D array containing the data.\n\n    Returns:\n    list of tuples: The pairs of column indices that are highly correlated.\n\n    Requirements:\n    - numpy\n    - scipy.stats.pearsonr\n    - itertools.combinations\n\n    Example:\n    >>> data = np.array([[1, 2, 3], [2, 4, 6], [3, 6, 9]])\n    >>> find_highly_correlated_pairs(data)\n    \"\"\"\n    n_cols = data.shape[1]\n    pairs = combinations(range(n_cols), 2)\n\n    high_corr_pairs = []\n    for pair in pairs:\n        corr, _ = pearsonr(data[:, pair[0]], data[:, pair[1]])\n        if np.abs(corr) >= THRESHOLD:\n            high_corr_pairs.append(pair)\n\n    return high_corr_pairs\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\n```python\nimport numpy as np\nfrom sklearn.decomposition import PCA\n\n# Constants\nN_COMPONENTS = 2\n\ndef apply_pca(data):\n    \"\"\"\n    Apply Principal Component Analysis (PCA) to a dataset to reduce its dimensionality.\n\n    Parameters:\n    data (numpy array): The 2D array containing the data.\n\n    Returns:\n    numpy array: The transformed data.\n\n    Requirements:\n    - numpy\n    - sklearn.decomposition.PCA\n\n    Example:\n    >>> data = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    >>> apply_pca(data)\n    \"\"\"\n    pca = PCA(n_components=N_COMPONENTS)\n    return pca.fit_transform(data)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\n```python\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\n# Constants\nCV = 5\n\ndef evaluate_classifier(X, y):\n    \"\"\"\n    Evaluate a Random Forest classifier using cross-validation.\n\n    Parameters:\n    X (numpy array): The 2D array containing the predictor variables.\n    y (numpy array): The 1D array containing the target variable.\n\n    Returns:\n    float: The average cross-validation score.\n\n    Requirements:\n    - numpy\n    - sklearn.ensemble.RandomForestClassifier\n    - sklearn.model_selection.cross_val_score\n\n    Example:\n    >>> X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n    >>> y = np.array([0, 1, 0, 1, 0])\n    >>> evaluate_classifier(X, y)\n    \"\"\"\n    clf = RandomForestClassifier(random_state=0)\n    scores = cross_val_score(clf, X, y, cv=CV)\n    return np.mean(scores)\n```\n`GPT_ODEX_BREAK`"}
{"seed": "{\"task_id\": 4918425, \"prompt\": \"def f_4918425(a):\\n\\t\", \"suffix\": \"\\n\\treturn a\", \"canonical_solution\": \"a[:] = [(x - 13) for x in a]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    a = [14, 15]\\n    candidate(a)\\n    assert a == [1, 2]\\n\", \"\\n    a = [float(x) for x in range(13, 20)]\\n    candidate(a)\\n    assert a == [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0]\\n\"], \"entry_point\": \"f_4918425\", \"intent\": \"subtract 13 from every number in a list `a`\", \"library\": []}", "generation": "Scenario 1: \nnumpy, pandas, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nSCALE_FACTOR = 13\n\ndef analyze_and_plot_data(data):\n    \"\"\"\n    Subtract a scale factor from every number in the given data (a numpy array), \n    convert it to a pandas DataFrame, calculate the statistical summary, and plot \n    the histogram of the data.\n\n    Parameters:\n    data (numpy array): The input numpy array.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the statistical summary of the data.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> data = np.array([14, 15, 16, 17, 18, 19])\n    >>> df = analyze_and_plot_data(data)\n    >>> print(df)\n    >>> plt.show()\n    \"\"\"\n    data = data - SCALE_FACTOR\n    df = pd.DataFrame(data, columns=['Data'])\n    summary = df.describe()\n    \n    df.hist(bins=10)\n    \n    return summary\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nnumpy, os, matplotlib\n```python\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\n\n# Constants\nSCALE_FACTOR = 13\n\ndef plot_scaled_directory_sizes(directory):\n    \"\"\"\n    Get the sizes of all files in a given directory, subtract a scale factor from \n    each size, and plot the resulting sizes.\n\n    Parameters:\n    directory (str): The directory path.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - os\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_scaled_directory_sizes('/path/to/directory')\n    >>> plt.show()\n    \"\"\"\n    sizes = np.array([os.path.getsize(os.path.join(directory, f)) for f in os.listdir(directory)])\n    sizes = sizes - SCALE_FACTOR\n\n    plt.hist(sizes, bins=10)\n    plt.title('File Sizes in Directory (Scaled)')\n    plt.xlabel('File Size')\n    plt.ylabel('Count')\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnumpy, pandas, sklearn.preprocessing\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Constants\nSCALE_FACTOR = 13\n\ndef scale_and_normalize_data(data):\n    \"\"\"\n    Subtract a scale factor from every number in the given data (a numpy array), \n    convert it to a pandas DataFrame, normalize the data using MinMaxScaler from \n    sklearn, and return the normalized data.\n\n    Parameters:\n    data (numpy array): The input numpy array.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the normalized data.\n\n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.preprocessing.MinMaxScaler\n\n    Example:\n    >>> data = np.array([14, 15, 16, 17, 18, 19])\n    >>> df = scale_and_normalize_data(data)\n    >>> print(df)\n    \"\"\"\n    data = data - SCALE_FACTOR\n    df = pd.DataFrame(data, columns=['Data'])\n\n    scaler = MinMaxScaler()\n    df['Data'] = scaler.fit_transform(df[['Data']])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, matplotlib, scipy.stats\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n# Constants\nSCALE_FACTOR = 13\n\ndef analyze_and_plot_ttest(data1, data2):\n    \"\"\"\n    Subtract a scale factor from every number in the given data sets (numpy arrays), \n    perform a t-test, plot the data sets, and return the t-statistic and p-value.\n\n    Parameters:\n    data1 (numpy array): The first input numpy array.\n    data2 (numpy array): The second input numpy array.\n\n    Returns:\n    tuple: The t-statistic and the p-value.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - scipy.stats\n\n    Example:\n    >>> data1 = np.array([14, 15, 16, 17, 18, 19])\n    >>> data2 = np.array([20, 21, 22, 23, 24, 25])\n    >>> t_statistic, p_value = analyze_and_plot_ttest(data1, data2)\n    >>> print(t_statistic, p_value)\n    >>> plt.show()\n    \"\"\"\n    data1 = data1 - SCALE_FACTOR\n    data2 = data2 - SCALE_FACTOR\n\n    t_statistic, p_value = stats.ttest_ind(data1, data2)\n\n    plt.hist(data1, bins=10, alpha=0.5, label='Data 1')\n    plt.hist(data2, bins=10, alpha=0.5, label='Data 2')\n    plt.legend(loc='upper right')\n\n    return t_statistic, p_value\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnumpy, pandas, sklearn.linear_model\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\n# Constants\nSCALE_FACTOR = 13\n\ndef fit_linear_regression(data_x, data_y):\n    \"\"\"\n    Subtract a scale factor from every number in the given data sets (numpy arrays), \n    fit a linear regression model using sklearn, and return the coefficient and \n    intercept of the model.\n\n    Parameters:\n    data_x (numpy array): The input numpy array for the x variable.\n    data_y (numpy array): The input numpy array for the y variable.\n\n    Returns:\n    tuple: The coefficient and the intercept.\n\n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.linear_model.LinearRegression\n\n    Example:\n    >>> data_x = np.array([14, 15, 16, 17, 18, 19])\n    >>> data_y = np.array([20, 21, 22, 23, 24, 25])\n    >>> coefficient, intercept = fit_linear_regression(data_x, data_y)\n    >>> print(coefficient, intercept)\n    \"\"\"\n    data_x = data_x - SCALE_FACTOR\n    data_y = data_y - SCALE_FACTOR\n\n    df = pd.DataFrame({'x': data_x, 'y': data_y})\n\n    model = LinearRegression()\n    model.fit(df[['x']], df['y'])\n\n    return model.coef_, model.intercept_\n```"}
{"seed": "{\"task_id\": 17794266, \"prompt\": \"def f_17794266(x):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"max(x.min(), x.max(), key=abs)\", \"test_start\": \"\\nimport numpy as np \\n\\ndef check(candidate):\", \"test\": [\"\\n    x = np.matrix([[1, 1], [2, -3]])\\n    assert candidate(x) == -3\\n\"], \"entry_point\": \"f_17794266\", \"intent\": \"get the highest element in absolute value in a numpy matrix `x`\", \"library\": [\"numpy\"]}", "generation": "Scenario 3:\nnumpy, math, random, matplotlib\n```python\nimport numpy as np\nimport math\nfrom random import uniform\nimport matplotlib.pyplot as plt\n\n# Constants\nMATRIX_SIZE = 5\nRANGE_START = -10\nRANGE_END = 10\n\ndef generate_and_visualize_matrix():\n    \"\"\"\n    Generate a random 5x5 matrix with values ranging from -10 to 10. \n    Then, visualize the matrix using a heatmap and return the maximum absolute value in the matrix.\n    \n    Returns:\n    int: The maximum absolute value in the matrix.\n    \n    Requirements:\n    - numpy\n    - math\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> max_abs_val = generate_and_visualize_matrix()\n    >>> print(max_abs_val)\n    \"\"\"\n    matrix = np.array([[uniform(RANGE_START, RANGE_END) for _ in range(MATRIX_SIZE)] for _ in range(MATRIX_SIZE)])\n    max_abs_val = max(matrix.min(), matrix.max(), key=abs)\n\n    # Plotting the heatmap\n    plt.imshow(matrix, cmap='hot', interpolation='nearest')\n    plt.colorbar(label='Value')\n    plt.title(f'Heatmap of Matrix (Max Abs Val: {max_abs_val})')\n    plt.show()\n    \n    return max_abs_val\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, scipy.stats, matplotlib\n```python\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\n# Constants\nSAMPLE_SIZE = 1000\nMU = 0\nSIGMA = 1\n\ndef visualize_normal_distribution():\n    \"\"\"\n    Generate a normal distribution with mean 0 and standard deviation 1 using `SAMPLE_SIZE` samples. \n    Then, visualize the distribution using a histogram and return the skewness and kurtosis of the distribution.\n    \n    Returns:\n    tuple: The skewness and kurtosis of the distribution.\n    \n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n    \n    Example:\n    >>> skewness, kurtosis = visualize_normal_distribution()\n    >>> print(skewness, kurtosis)\n    \"\"\"\n    # Generate the normal distribution\n    dist = np.random.normal(MU, SIGMA, SAMPLE_SIZE)\n\n    # Calculate skewness and kurtosis\n    skewness = stats.skew(dist)\n    kurtosis = stats.kurtosis(dist)\n\n    # Plotting the histogram\n    plt.hist(dist, bins=30, density=True, alpha=0.6, color='g')\n    plt.title(f'Normal Distribution (Skewness: {skewness:.2f}, Kurtosis: {kurtosis:.2f})')\n    plt.show()\n    \n    return skewness, kurtosis\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, pandas, sklearn.preprocessing, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\n# Constants\nDATA_SIZE = 100\n\ndef generate_and_visualize_data():\n    \"\"\"\n    Generate a pandas DataFrame with `DATA_SIZE` samples of data for three features ('A', 'B', 'C')\n    each generated from a normal distribution with mean 0 and standard deviation 1. \n    Then standardize the data using StandardScaler from sklearn and visualize the original and standardized data \n    using two side-by-side boxplots.\n    \n    Returns:\n    DataFrame: The standardized data.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.preprocessing\n    - matplotlib.pyplot\n    \n    Example:\n    >>> standardized_data = generate_and_visualize_data()\n    >>> print(standardized_data)\n    \"\"\"\n    # Generate the data\n    data = pd.DataFrame({\n        'A': np.random.normal(0, 1, DATA_SIZE),\n        'B': np.random.normal(0, 1, DATA_SIZE),\n        'C': np.random.normal(0, 1, DATA_SIZE)\n    })\n\n    # Standardize the data\n    scaler = StandardScaler()\n    standardized_data = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n\n    # Plotting the boxplots\n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n    data.boxplot(ax=axes[0])\n    axes[0].set_title('Original Data')\n    standardized_data.boxplot(ax=axes[1])\n    axes[1].set_title('Standardized Data')\n    plt.show()\n    \n    return standardized_data\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 6:\nnumpy, scipy.optimize, matplotlib\n```python\nimport numpy as np\nfrom scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\n\n# Constants\nSAMPLE_SIZE = 100\n\ndef fit_and_plot_curve():\n    \"\"\"\n    Generate `SAMPLE_SIZE` samples of data for x from 0 to 4pi and y = sin(x) with some noise. \n    Then fit a curve to the data using scipy's curve_fit and visualize the original data and the fitted curve.\n    \n    Returns:\n    list: The optimized parameters for the sin function.\n    \n    Requirements:\n    - numpy\n    - scipy.optimize\n    - matplotlib.pyplot\n    \n    Example:\n    >>> params = fit_and_plot_curve()\n    >>> print(params)\n    \"\"\"\n    # Generate the data\n    x = np.linspace(0, 4*np.pi, SAMPLE_SIZE)\n    y = np.sin(x) + np.random.normal(0, 0.2, SAMPLE_SIZE)\n\n    # Define the function to fit\n    def func(x, a, b, c):\n        return a * np.sin(b * x) + c\n\n    # Fit the curve\n    params, params_covariance = curve_fit(func, x, y, p0=[1, 1, 0])\n\n    # Plotting the data and the fitted curve\n    plt.scatter(x, y, label='Data')\n    plt.plot(x, func(x, params[0], params[1], params[2]), label='Fitted curve')\n    plt.legend(loc='best')\n    plt.show()\n    \n    return list(params)\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 7:\nnumpy, sklearn.decomposition, matplotlib\n```python\nimport numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\n# Constants\nDATA_SIZE = 200\n\ndef perform_and_visualize_pca():\n    \"\"\"\n    Generate a 2D dataset with `DATA_SIZE` samples. Then perform Principal Component Analysis (PCA) \n    to reduce the dimensionality to 1D and visualize the original data and the result of PCA using scatter plots.\n    \n    Returns:\n    numpy.array: The transformed data.\n    \n    Requirements:\n    - numpy\n    - sklearn.decomposition\n    - matplotlib.pyplot\n    \n    Example:\n    >>> transformed_data = perform_and_visualize_pca()\n    >>> print(transformed_data)\n    \"\"\"\n    # Generate the 2D data\n    X = np.random.rand(DATA_SIZE, 2)\n\n    # Perform PCA\n    pca = PCA(n_components=1)\n    X_transformed = pca.fit_transform(X)\n\n    # Plotting the original and transformed data\n    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n    axes[0].scatter(X[:, 0], X[:, 1])\n    axes[0].set_title('Original Data')\n    axes[1].scatter(X_transformed, [0] * len(X_transformed))\n    axes[1].set_title('Data after PCA')\n    plt.show()\n    \n    return X_transformed\n```"}
{"seed": "{\"task_id\": 30551576, \"prompt\": \"def f_30551576(s):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"re.findall(r'\\\"(http.*?)\\\"', s, re.MULTILINE | re.DOTALL)\", \"test_start\": \"\\nimport re\\n\\ndef check(candidate):\", \"test\": [\"\\n    s = (\\n      '     [irrelevant javascript code here]'\\n      '     sources:[{file:\\\"http://url.com/folder1/v.html\\\",label:\\\"label1\\\"},'\\n      '     {file:\\\"http://url.com/folder2/v.html\\\",label:\\\"label2\\\"},'\\n      '     {file:\\\"http://url.com/folder3/v.html\\\",label:\\\"label3\\\"}],'\\n      '     [irrelevant javascript code here]'\\n    )\\n    assert candidate(s) == ['http://url.com/folder1/v.html', 'http://url.com/folder2/v.html', 'http://url.com/folder3/v.html']\\n\", \"\\n    s = (\\n      '     [irrelevant javascript code here]'\\n      '     [irrelevant python code here]'\\n    )\\n    assert candidate(s) == []\\n\"], \"entry_point\": \"f_30551576\", \"intent\": \"Get all urls within text `s`\", \"library\": [\"re\"]}", "generation": "Scenario 1: \nre, os, glob, hashlib\n```python\nimport re\nimport os\nimport glob\nimport hashlib\n\n# Regular expression to match URLs\nURL_REGEX = r'\\\"(http.*?)\\\"'\n\ndef get_hash_of_urls_in_files(directory):\n    \"\"\"\n    Get a SHA256 hash of all URLs in all .txt files within the directory.\n    \n    Parameters:\n    directory (str): The directory in which to look for .txt files.\n    \n    Returns:\n    dict: A dictionary mapping filenames to their SHA256 hash.\n    \n    Requirements:\n    - re\n    - os\n    - glob\n    - hashlib\n    \n    Example:\n    >>> get_hash_of_urls_in_files('/path/to/directory')\n    \"\"\"\n    url_hashes = {}\n\n    for filename in glob.glob(os.path.join(directory, '*.txt')):\n        with open(filename, 'r') as file:\n            content = file.read()\n            urls = re.findall(URL_REGEX, content, re.MULTILINE | re.DOTALL)\n            url_string = ''.join(urls)\n            url_hash = hashlib.sha256(url_string.encode()).hexdigest()\n            url_hashes[filename] = url_hash\n\n    return url_hashes\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nre, json, urllib.request\n```python\nimport re\nimport json\nimport urllib.request\n\n# Regular expression to match URLs\nURL_REGEX = r'\\\"(http.*?)\\\"'\n\ndef count_occurrences_of_urls_in_webpage(url):\n    \"\"\"\n    Count the occurrences of each URL in the HTML content of a webpage.\n    \n    Parameters:\n    url (str): The URL of the webpage to analyze.\n    \n    Returns:\n    dict: A dictionary mapping URLs to their count.\n    \n    Requirements:\n    - re\n    - json\n    - urllib.request\n    \n    Example:\n    >>> count_occurrences_of_urls_in_webpage('https://www.example.com')\n    \"\"\"\n    with urllib.request.urlopen(url) as response:\n        html = response.read().decode()\n        urls = re.findall(URL_REGEX, html, re.MULTILINE | re.DOTALL)\n        url_counts = {url: urls.count(url) for url in set(urls)}\n\n    return url_counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nre, pandas, numpy\n```python\nimport re\nimport pandas as pd\nimport numpy as np\n\n# Regular expression to match URLs\nURL_REGEX = r'\\\"(http.*?)\\\"'\n\ndef analyze_urls_in_dataframe(df, column):\n    \"\"\"\n    Analyze the URLs in a specific column of a pandas DataFrame. The function \n    counts the occurrences of each URL and calculates the mean, median, and \n    standard deviation of the counts.\n    \n    Parameters:\n    df (DataFrame): The DataFrame to analyze.\n    column (str): The column containing the URLs.\n    \n    Returns:\n    dict: A dictionary with the mean, median, and standard deviation of the \n    URL counts.\n    \n    Requirements:\n    - re\n    - pandas\n    - numpy\n    \n    Example:\n    >>> df = pd.DataFrame({'text': ['text with \"http://url1\"', 'text with \"http://url2\"', 'text with \"http://url1\"']})\n    >>> analyze_urls_in_dataframe(df, 'text')\n    \"\"\"\n    urls = df[column].apply(lambda x: re.findall(URL_REGEX, x, re.MULTILINE | re.DOTALL))\n    flat_urls = [url for sublist in urls for url in sublist]\n    url_counts = pd.Series(flat_urls).value_counts().values\n\n    stats = {\n        'mean': np.mean(url_counts),\n        'median': np.median(url_counts),\n        'std': np.std(url_counts)\n    }\n\n    return stats\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nre, collections, matplotlib.pyplot\n```python\nimport re\nimport collections\nimport matplotlib.pyplot as plt\n\n# Regular expression to match URLs\nURL_REGEX = r'\\\"(http.*?)\\\"'\n\ndef plot_url_frequencies_in_text(text):\n    \"\"\"\n    Plot a bar chart of URL frequencies in a given text.\n    \n    Parameters:\n    text (str): The text to analyze.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - re\n    - collections\n    - matplotlib.pyplot\n    \n    Example:\n    >>> text = 'text with \"http://url1\" and \"http://url2\" and again \"http://url1\"'\n    >>> plot_url_frequencies_in_text(text)\n    \"\"\"\n    urls = re.findall(URL_REGEX, text, re.MULTILINE | re.DOTALL)\n    url_freq = collections.Counter(urls)\n    \n    plt.bar(url_freq.keys(), url_freq.values())\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nre, bs4, requests\n```python\nimport re\nfrom bs4 import BeautifulSoup\nimport requests\n\n# Regular expression to match URLs\nURL_REGEX = r'\\\"(http.*?)\\\"'\n\ndef extract_urls_from_webpage(url):\n    \"\"\"\n    Extract and return all URLs from a webpage.\n    \n    Parameters:\n    url (str): The URL of the webpage to analyze.\n    \n    Returns:\n    list: A list of URLs.\n    \n    Requirements:\n    - re\n    - bs4\n    - requests\n    \n    Example:\n    >>> extract_urls_from_webpage('https://www.example.com')\n    \"\"\"\n    response = requests.get(url)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    text = soup.get_text()\n    urls = re.findall(URL_REGEX, text, re.MULTILINE | re.DOTALL)\n\n    return urls\n```\n"}
{"seed": "{\"task_id\": 113534, \"prompt\": \"def f_113534(mystring):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"mystring.replace(' ', '! !').split('!')\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(\\\"This is the string I want to split\\\") ==       ['This',' ','is',' ','the',' ','string',' ','I',' ','want',' ','to',' ','split']\\n\"], \"entry_point\": \"f_113534\", \"intent\": \"split a string `mystring` considering the spaces ' '\", \"library\": []}", "generation": "Scenario 1: \nnumpy, pandas, matplotlib, random\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import choice\n\n# Constants\nWORDS = ['This', 'is', 'the', 'string', 'I', 'want', 'to', 'split']\nPUNCTUATIONS = ['.', ',', '!', '?', ';', ':']\n\ndef generate_random_sentences_and_plot_word_counts(n):\n    \"\"\"\n    Generate n random sentences using a predefined list of words and punctuations, \n    then plot a bar graph of the word counts.\n    \n    Parameters:\n    n (int): The number of sentences to generate.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the word counts.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> word_counts = generate_random_sentences_and_plot_word_counts(1000)\n    >>> print(word_counts)\n    >>> word_counts.plot(kind='bar')\n    \"\"\"\n    sentences = []\n\n    # Generate random sentences\n    for _ in range(n):\n        sentence_length = np.random.randint(1, len(WORDS) + 1)\n        sentence = ' '.join(choice(WORDS) for _ in range(sentence_length)) + choice(PUNCTUATIONS)\n        sentences.append(sentence)\n\n    # Split sentences into words and count word occurrences\n    words = ' '.join(sentences).split(' ')\n    word_counts = pd.Series(words).value_counts()\n\n    return word_counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, re, collections, matplotlib\n```python\nimport os\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\n# Constants\nFILE_DIR = './text_files'\nWORD_PATTERN = r'\\b\\w+\\b'\n\ndef count_words_in_files_and_plot():\n    \"\"\"\n    Count words in all text files in a directory and plot the top 10 most \n    common words in a bar chart.\n    \n    Returns:\n    Counter: A Counter object with the word counts.\n    \n    Requirements:\n    - os\n    - re\n    - collections\n    - matplotlib.pyplot\n\n    Example:\n    >>> word_counts = count_words_in_files_and_plot()\n    >>> print(word_counts.most_common(10))\n    >>> plt.bar(*zip(*word_counts.most_common(10)))\n    \"\"\"\n    word_counts = Counter()\n\n    for file_name in os.listdir(FILE_DIR):\n        if file_name.endswith('.txt'):\n            with open(os.path.join(FILE_DIR, file_name), 'r') as file:\n                file_content = file.read()\n                words = re.findall(WORD_PATTERN, file_content)\n                word_counts.update(words)\n    \n    return word_counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nre, nltk, matplotlib\n```python\nimport re\nfrom nltk import FreqDist\nimport matplotlib.pyplot as plt\n\n# Constants\nTEXT = \"This is the string I want to split. This is another string to split.\"\n\ndef plot_word_frequency_distribution():\n    \"\"\"\n    Plot the word frequency distribution in a text using NLTK's FreqDist.\n    \n    Requirements:\n    - re\n    - nltk\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_word_frequency_distribution()\n    \"\"\"\n    words = re.findall(r'\\b\\w+\\b', TEXT)\n    fdist = FreqDist(words)\n\n    fdist.plot(30, cumulative=False)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, matplotlib, skimage, scipy.ndimage\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom skimage import data\nfrom scipy.ndimage import gaussian_filter\n\ndef apply_gaussian_filter_and_plot():\n    \"\"\"\n    Apply a Gaussian filter to an image and plot the original and \n    filtered images side by side.\n    \n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - skimage\n    - scipy.ndimage\n\n    Example:\n    >>> apply_gaussian_filter_and_plot()\n    \"\"\"\n    image = data.coins()\n    filtered_image = gaussian_filter(image, sigma=2)\n\n    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n\n    ax[0].imshow(image, cmap=plt.cm.gray)\n    ax[0].set_title('Original')\n\n    ax[1].imshow(filtered_image, cmap=plt.cm.gray)\n    ax[1].set_title('Filtered')\n\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D']\nN_ROWS = 100\n\ndef create_dataframe_and_plot_histograms():\n    \"\"\"\n    Create a DataFrame with 4 columns of random numbers, then plot \n    histograms of each column.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the random numbers.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = create_dataframe_and_plot_histograms()\n    >>> print(df)\n    >>> df.hist(bins=30, alpha=0.5)\n    \"\"\"\n    df = pd.DataFrame(np.random.rand(N_ROWS, len(COLUMNS)), columns=COLUMNS)\n\n    df.hist(bins=30, alpha=0.5)\n    plt.show()\n\n    return df\n```"}
{"seed": "{\"task_id\": 5838735, \"prompt\": \"def f_5838735(path):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"open(path, 'r')\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    with open('tmp.txt', 'w') as fw: fw.write('hello world!')\\n    f = candidate('tmp.txt')\\n    assert f.name == 'tmp.txt'\\n    assert f.mode == 'r'\\n\"], \"entry_point\": \"f_5838735\", \"intent\": \"open file `path` with mode 'r'\", \"library\": []}", "generation": "Scenario 1: \nos, csv, json, datetime\n```python\nimport os\nimport csv\nimport json\nfrom datetime import datetime\n\n# Constants\nDATE_FORMAT = '%m/%d/%Y %H:%M:%S'\n\ndef convert_csv_to_json(csv_path, json_path):\n    \"\"\"\n    Convert a CSV file to a JSON file.\n\n    Parameters:\n    csv_path (str): The path of the CSV file.\n    json_path (str): The path of the JSON file.\n\n    Returns:\n    str: 'Conversion Successful' if conversion is successful.\n\n    Requirements:\n    - os\n    - csv\n    - json\n    - datetime\n\n    Example:\n    >>> convert_csv_to_json('test.csv', 'test.json')\n    \"\"\"\n    if not os.path.exists(csv_path):\n        return 'CSV file does not exist.'\n\n    data = []\n    with open(csv_path, 'r') as csv_file:\n        csv_reader = csv.DictReader(csv_file)\n        for row in csv_reader:\n            for key, value in row.items():\n                try:\n                    row[key] = datetime.strptime(value, DATE_FORMAT).isoformat()\n                except ValueError:\n                    pass\n            data.append(row)\n\n    with open(json_path, 'w') as json_file:\n        json.dump(data, json_file, indent=4)\n\n    return 'Conversion Successful'\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, pandas, matplotlib, seaborn\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef plot_correlation_matrix(file_path):\n    \"\"\"\n    Plot the correlation matrix for a given CSV file.\n\n    Parameters:\n    file_path (str): The path of the CSV file.\n\n    Returns:\n    str: 'Plot Successful' if plot is successful.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib\n    - seaborn\n\n    Example:\n    >>> plot_correlation_matrix('test.csv')\n    \"\"\"\n    df = pd.read_csv(file_path)\n    df = df[COLUMNS]\n\n    corr = df.corr()\n\n    sns.heatmap(corr, annot=True, cmap='coolwarm')\n\n    plt.title('Correlation Matrix')\n    plt.show()\n\n    return 'Plot Successful'\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, shutil, glob, zipfile\n```python\nimport os\nimport shutil\nimport glob\nimport zipfile\n\nFOLDER_PATH = './tmp'\n\ndef compress_files_with_extension(extension):\n    \"\"\"\n    Compress all files with a particular extension in a folder.\n\n    Parameters:\n    extension (str): The file extension.\n\n    Returns:\n    str: 'Compression Successful' if compression is successful.\n\n    Requirements:\n    - os\n    - shutil\n    - glob\n    - zipfile\n\n    Example:\n    >>> compress_files_with_extension('.txt')\n    \"\"\"\n    files = glob.glob(FOLDER_PATH + '/*' + extension)\n\n    with zipfile.ZipFile('compressed_files.zip', 'w') as zipf:\n        for file in files:\n            zipf.write(file)\n\n    return 'Compression Successful'\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npickle, os, random, time\n```python\nimport pickle\nimport os\nimport random\nimport time\n\n# Constants\nFILE_PATH = 'random_numbers.pkl'\n\ndef generate_and_save_random_numbers(n):\n    \"\"\"\n    Generate n random numbers, save them in a pickle file, and return the elapsed time.\n\n    Parameters:\n    n (int): The number of random numbers to generate.\n\n    Returns:\n    float: The elapsed time.\n\n    Requirements:\n    - pickle\n    - os\n    - random\n    - time\n\n    Example:\n    >>> generate_and_save_random_numbers(1000000)\n    \"\"\"\n    start_time = time.time()\n\n    random_numbers = [random.random() for _ in range(n)]\n\n    with open(FILE_PATH, 'wb') as f:\n        pickle.dump(random_numbers, f)\n\n    elapsed_time = time.time() - start_time\n\n    return elapsed_time\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, pandas, matplotlib, seaborn\n```python\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nCSV_PATH = 'data.csv'\n\ndef plot_distribution(file_path, column_name):\n    \"\"\"\n    Plot the distribution of a specific column in a CSV file.\n\n    Parameters:\n    file_path (str): The path of the CSV file.\n    column_name (str): The name of the column.\n\n    Returns:\n    str: 'Plot Successful' if plot is successful.\n\n    Requirements:\n    - os\n    - pandas\n    - matplotlib\n    - seaborn\n\n    Example:\n    >>> plot_distribution('test.csv', 'Age')\n    \"\"\"\n    if not os.path.exists(file_path):\n        return 'File does not exist.'\n\n    df = pd.read_csv(file_path)\n\n    sns.distplot(df[column_name], bins=30, kde=False)\n\n    plt.title(f'Distribution of {column_name}')\n    plt.show()\n\n    return 'Plot Successful'\n```"}
{"seed": "{\"task_id\": 36003967, \"prompt\": \"def f_36003967(data):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"[[sum(item) for item in zip(*items)] for items in zip(*data)]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    data = [[[5, 10, 30, 24, 100], [1, 9, 25, 49, 81]],\\n            [[15, 10, 10, 16, 70], [10, 1, 25, 11, 19]],\\n            [[34, 20, 10, 10, 30], [9, 20, 25, 30, 80]]]\\n    assert candidate(data) == [[54, 40, 50, 50, 200], [20, 30, 75, 90, 180]]\\n\"], \"entry_point\": \"f_36003967\", \"intent\": \"sum elements at the same index in list `data`\", \"library\": []}", "generation": "Scenario 1:\npandas, numpy, matplotlib, math\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\n\n# Constants\nDATA = np.array([[[5, 10, 30, 24, 100], [1, 9, 25, 49, 81]],\n                 [[15, 10, 10, 16, 70], [10, 1, 25, 11, 19]],\n                 [[34, 20, 10, 10, 30], [9, 20, 25, 30, 80]]])\n\ndef calculate_and_plot_stats(data):\n    \"\"\"\n    Calculate the mean and standard deviation of each list in 'data' and plot the results.\n    \n    Parameters:\n    data (numpy.array): The data to calculate and plot.\n    \n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - math\n\n    Example:\n    >>> calculate_and_plot_stats(DATA)\n    \"\"\"\n    means = [np.mean(item) for sublist in data for item in sublist]\n    std_devs = [np.std(item) for sublist in data for item in sublist]\n    labels = [f'List {i+1}' for i in range(len(means))]\n\n    df = pd.DataFrame({'Mean': means, 'Std Dev': std_devs}, index=labels)\n    df.plot(kind='bar', y='Mean', yerr='Std Dev', capsize=4)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, itertools, functools, operator\n```python\nimport numpy as np\nimport itertools\nimport functools\nimport operator\n\n# Constants\nDATA = np.array([[[5, 10, 30, 24, 100], [1, 9, 25, 49, 81]],\n                 [[15, 10, 10, 16, 70], [10, 1, 25, 11, 19]],\n                 [[34, 20, 10, 10, 30], [9, 20, 25, 30, 80]]])\n\ndef calculate_product_of_sums(data):\n    \"\"\"\n    Calculate the product of the sums of each list in 'data'.\n    \n    Parameters:\n    data (numpy.array): The data to calculate.\n    \n    Returns:\n    int: The product of the sums.\n\n    Requirements:\n    - numpy\n    - itertools\n    - functools\n    - operator\n\n    Example:\n    >>> calculate_product_of_sums(DATA)\n    \"\"\"\n    sums = [sum(item) for sublist in data for item in sublist]\n    product = functools.reduce(operator.mul, sums)\n\n    return product\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, scipy.stats, matplotlib, seaborn\n```python\nimport numpy as np\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nDATA = np.array([[[5, 10, 30, 24, 100], [1, 9, 25, 49, 81]],\n                 [[15, 10, 10, 16, 70], [10, 1, 25, 11, 19]],\n                 [[34, 20, 10, 10, 30], [9, 20, 25, 30, 80]]])\n\ndef plot_distribution(data):\n    \"\"\"\n    Flatten 'data' and plot the distribution of its elements.\n    \n    Parameters:\n    data (numpy.array): The data to plot.\n    \n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> plot_distribution(DATA)\n    \"\"\"\n    flattened_data = data.flatten()\n    sns.distplot(flattened_data, fit=stats.norm)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, sklearn.preprocessing, pandas, matplotlib\n```python\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nDATA = np.array([[[5, 10, 30, 24, 100], [1, 9, 25, 49, 81]],\n                 [[15, 10, 10, 16, 70], [10, 1, 25, 11, 19]],\n                 [[34, 20, 10, 10, 30], [9, 20, 25, 30, 80]]])\n\ndef scale_and_plot_data(data):\n    \"\"\"\n    Scale 'data' using standard scaling and plot the results.\n    \n    Parameters:\n    data (numpy.array): The data to scale and plot.\n    \n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - sklearn.preprocessing.StandardScaler\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> scale_and_plot_data(DATA)\n    \"\"\"\n    flattened_data = data.flatten().reshape(-1, 1)\n    scaler = StandardScaler()\n    scaled_data = scaler.fit_transform(flattened_data)\n\n    df = pd.DataFrame(scaled_data, columns=['Scaled Data'])\n    df.plot(kind='hist', rwidth=0.8)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, pandas, sklearn.linear_model, sklearn.metrics\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n# Constants\nDATA = np.array([[[5, 10, 30, 24, 100], [1, 9, 25, 49, 81]],\n                 [[15, 10, 10, 16, 70], [10, 1, 25, 11, 19]],\n                 [[34, 20, 10, 10, 30], [9, 20, 25, 30, 80]]])\n\ndef predict_values(data):\n    \"\"\"\n    Fit a linear regression model to 'data' and predict values for a given test set.\n    \n    Parameters:\n    data (numpy.array): The data to use for training.\n    \n    Returns:\n    float: The mean squared error of the predictions.\n\n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.linear_model.LinearRegression\n    - sklearn.metrics.mean_squared_error\n\n    Example:\n    >>> predict_values(DATA)\n    \"\"\"\n    flattened_data = data.flatten().reshape(-1, 1)\n    X_train = flattened_data[:-20]\n    X_test = flattened_data[-20:]\n    y_train = np.array([i for i in range(len(X_train))])\n    y_test = np.array([i for i in range(len(X_train), len(X_train)+20)])\n\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    predictions = model.predict(X_test)\n\n    mse = mean_squared_error(y_test, predictions)\n\n    return mse\n```"}
{"seed": "{\"task_id\": 7635237, \"prompt\": \"def f_7635237(a):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"a[:, (np.newaxis)]\", \"test_start\": \"\\nimport numpy as np \\n\\ndef check(candidate):\", \"test\": [\"\\n    data = np.array([[[5, 10, 30, 24, 100], [1, 9, 25, 49, 81]],\\n            [[15, 10, 10, 16, 70], [10, 1, 25, 11, 19]],\\n            [[34, 20, 10, 10, 30], [9, 20, 25, 30, 80]]])\\n    assert candidate(data).tolist() == [[[[  5,  10,  30,  24, 100],\\n         [  1,   9,  25,  49,  81]]],\\n       [[[ 15,  10,  10,  16,  70],\\n         [ 10,   1,  25,  11,  19]]],\\n       [[[ 34,  20,  10,  10,  30],\\n         [  9,  20,  25,  30,  80]]]]\\n\"], \"entry_point\": \"f_7635237\", \"intent\": \"add a new axis to array `a`\", \"library\": [\"numpy\"]}", "generation": "Scenario 1: \nnumpy, matplotlib, sklearn, pandas\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\n\ndef plot_standardized_data(data):\n    \"\"\"\n    Standardize the given data, add a new axis to the standardized data, and then plot the standardized data.\n\n    Parameters:\n    data (numpy.array): The input data.\n\n    Returns:\n    numpy.array: The standardized data with a new axis.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.preprocessing.StandardScaler\n    - pandas\n\n    Example:\n    >>> data = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    >>> standardized_data = plot_standardized_data(data)\n    >>> print(standardized_data)\n    \"\"\"\n    scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(data)\n    standardized_data = standardized_data[:, np.newaxis]\n    \n    pd.DataFrame(standardized_data[:, 0]).plot(kind='bar')\n\n    return standardized_data\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nnumpy, seaborn, sklearn, pandas\n```python\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\n\ndef plot_normalized_data(data):\n    \"\"\"\n    Normalize the given data, add a new axis to the normalized data, and then plot the normalized data.\n\n    Parameters:\n    data (numpy.array): The input data.\n\n    Returns:\n    numpy.array: The normalized data with a new axis.\n\n    Requirements:\n    - numpy\n    - seaborn\n    - sklearn.preprocessing.MinMaxScaler\n    - pandas\n\n    Example:\n    >>> data = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    >>> normalized_data = plot_normalized_data(data)\n    >>> print(normalized_data)\n    \"\"\"\n    scaler = MinMaxScaler()\n    normalized_data = scaler.fit_transform(data)\n    normalized_data = normalized_data[:, np.newaxis]\n    \n    sns.heatmap(normalized_data[:, 0])\n\n    return normalized_data\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnumpy, matplotlib, scipy, pandas\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.signal import savgol_filter\nimport pandas as pd\n\ndef plot_smoothed_data(data):\n    \"\"\"\n    Smooth the given data using Savitzky-Golay filter, add a new axis to the smoothed data, and then plot the smoothed data.\n\n    Parameters:\n    data (numpy.array): The input data.\n\n    Returns:\n    numpy.array: The smoothed data with a new axis.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - scipy.signal.savgol_filter\n    - pandas\n\n    Example:\n    >>> data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n    >>> smoothed_data = plot_smoothed_data(data)\n    >>> print(smoothed_data)\n    \"\"\"\n    smoothed_data = savgol_filter(data, 5, 2)\n    smoothed_data = smoothed_data[:, np.newaxis]\n\n    plt.plot(smoothed_data[:, 0])\n\n    return smoothed_data\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, sklearn, matplotlib, pandas\n```python\nimport numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef plot_pca_data(data):\n    \"\"\"\n    Apply Principal Component Analysis (PCA) to the data, add a new axis to the PCA-transformed data, \n    and then plot the PCA-transformed data.\n\n    Parameters:\n    data (numpy.array): The input data.\n\n    Returns:\n    numpy.array: The PCA-transformed data with a new axis.\n\n    Requirements:\n    - numpy\n    - sklearn.decomposition.PCA\n    - matplotlib.pyplot\n    - pandas\n\n    Example:\n    >>> data = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    >>> pca_data = plot_pca_data(data)\n    >>> print(pca_data)\n    \"\"\"\n    pca = PCA(n_components=2)\n    pca_data = pca.fit_transform(data)\n    pca_data = pca_data[:, np.newaxis]\n\n    pd.DataFrame(pca_data[:, 0]).plot(kind='bar')\n\n    return pca_data\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnumpy, seaborn, sklearn, pandas\n```python\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.manifold import TSNE\nimport pandas as pd\n\ndef plot_tsne_data(data):\n    \"\"\"\n    Apply t-distributed Stochastic Neighbor Embedding (t-SNE) to the data, \n    add a new axis to the t-SNE-transformed data, and then plot the t-SNE-transformed data.\n\n    Parameters:\n    data (numpy.array): The input data.\n\n    Returns:\n    numpy.array: The t-SNE-transformed data with a new axis.\n\n    Requirements:\n    - numpy\n    - seaborn\n    - sklearn.manifold.TSNE\n    - pandas\n\n    Example:\n    >>> data = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    >>> tsne_data = plot_tsne_data(data)\n    >>> print(tsne_data)\n    \"\"\"\n    tsne = TSNE(n_components=2)\n    tsne_data = tsne.fit_transform(data)\n    tsne_data = tsne_data[:, np.newaxis]\n\n    sns.heatmap(tsne_data[:, 0])\n\n    return tsne_data\n```\n"}
