{"seed": "{\"task_id\": 37146, \"prompt\": \"def f_37146(arr):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"arr[:, 0]\", \"test_start\": \"\\nimport numpy as np\\n\\ndef check(candidate):\", \"test\": [\"\\n    arr = np.array([[1,2],[3,4]])\\n    assert all(candidate(arr) == np.array([1,3]))\\n\", \"\\n    arr = np.array([[3,4,5]])\\n    assert all(candidate(arr) == np.array([3]))\\n\"], \"entry_point\": \"f_37146\", \"intent\": \"2\\u6b21\\u5143\\u914d\\u5217`arr`\\u306e\\u8981\\u7d20\\u3068\\u306a\\u3063\\u3066\\u3044\\u308b1\\u6b21\\u5143\\u914d\\u5217\\u304b\\u3089\\u5148\\u982d\\u306e\\u5024\\u306e\\u307f\\u3092\\u629c\\u304d\\u51fa\\u3059\", \"library\": [\"numpy\"]}", "generation": "Scenario 1: \nnumpy, scipy.spatial.distance, matplotlib.pyplot\n```python\nimport numpy as np\nfrom scipy.spatial.distance import cdist\nimport matplotlib.pyplot as plt\n\n# Constants\nCLUSTERS = 3\nCOLORS = ['r', 'g', 'b', 'y', 'm', 'c']\n\ndef plot_clusters_2d(points):\n    \"\"\"\n    Given a 2-dimensional numpy array representing points, calculate the Euclidean \n    distances between all points, cluster them using k-means clustering, \n    and plot the clusters in a 2D space.\n\n    Parameters:\n    points (numpy.ndarray): The 2-dimensional numpy array representing points.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - scipy.spatial.distance\n    - matplotlib.pyplot    \n\n    Example:\n    >>> points = np.array([[1,2],[3,4],[5,6],[7,8],[9,10]])\n    >>> plot_clusters_2d(points)\n    \"\"\"\n    from sklearn.cluster import KMeans\n\n    kmeans = KMeans(n_clusters=CLUSTERS)\n    kmeans.fit(points)\n    labels = kmeans.predict(points)\n    centroids = kmeans.cluster_centers_\n\n    for i in range(CLUSTERS):\n        ds = points[np.where(labels==i)]\n        plt.plot(ds[:,0], ds[:,1], 'o', color=COLORS[i])\n        lines = plt.plot(centroids[i,0], centroids[i,1], 'kx')\n        plt.setp(lines, ms=15.0)\n        plt.setp(lines, mew=2.0)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nnumpy, pandas, sklearn.preprocessing, sklearn.linear_model\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\n\ndef simple_linear_regression(x, y):\n    \"\"\"\n    Given 1-dimensional numpy arrays x and y, perform simple linear regression and return \n    the coefficients, intercept and R2 score.\n\n    Parameters:\n    x (numpy.ndarray): The 1-dimensional numpy array representing the predictors.\n    y (numpy.ndarray): The 1-dimensional numpy array representing the response variable.\n\n    Returns:\n    dict: A dictionary with 'coefficients', 'intercept' and 'r2_score' keys.\n\n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.preprocessing\n    - sklearn.linear_model   \n\n    Example:\n    >>> x = np.array([1, 2, 3, 4, 5])\n    >>> y = np.array([2, 4, 5, 4, 5])\n    >>> simple_linear_regression(x, y)\n    \"\"\"\n    x = x.reshape(-1, 1)\n    y = y.reshape(-1, 1)\n\n    scaler = StandardScaler()\n    x = scaler.fit_transform(x)\n    y = scaler.fit_transform(y)\n\n    model = LinearRegression()\n    model.fit(x, y)\n    \n    result = {\n        'coefficients': model.coef_[0][0],\n        'intercept': model.intercept_[0],\n        'r2_score': model.score(x, y)\n    }\n\n    return result\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnumpy, matplotlib.pyplot, scipy.signal\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.signal import find_peaks\n\n# Constants\nTHRESHOLD = 0.3\n\ndef plot_signal_peaks(signal):\n    \"\"\"\n    Given a 1-D numpy array representing a signal, find and plot all peaks in the signal \n    that are higher than a threshold.\n\n    Parameters:\n    signal (numpy.ndarray): The 1-dimensional numpy array representing the signal.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - scipy.signal   \n\n    Example:\n    >>> signal = np.array([1,2,3,2,0,2,3,2,1])\n    >>> plot_signal_peaks(signal)\n    \"\"\"\n    peaks, _ = find_peaks(signal, height=THRESHOLD)\n    \n    plt.plot(signal)\n    plt.plot(peaks, signal[peaks], \"x\")\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, sklearn.decomposition, matplotlib.pyplot\n```python\nimport numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\n# Constants\nN_COMPONENTS = 2\n\ndef pca_transform_plot(matrix):\n    \"\"\"\n    Given a 2-D numpy array representing a data matrix, perform PCA transformation \n    to reduce dimensionality and plot the transformed data.\n\n    Parameters:\n    matrix (numpy.ndarray): The 2-dimensional numpy array representing the data matrix.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - sklearn.decomposition\n    - matplotlib.pyplot   \n\n    Example:\n    >>> matrix = np.array([[1,2],[3,4],[5,6],[7,8],[9,10]])\n    >>> pca_transform_plot(matrix)\n    \"\"\"\n    pca = PCA(n_components=N_COMPONENTS)\n    transformed_data = pca.fit_transform(matrix)\n\n    plt.scatter(transformed_data[:, 0], transformed_data[:, 1])\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnumpy, scipy.stats, matplotlib.pyplot\n```python\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\ndef plot_normal_distribution(mean, std_dev):\n    \"\"\"\n    Given a mean and standard deviation, plot the corresponding normal distribution.\n\n    Parameters:\n    mean (float): The mean of the normal distribution.\n    std_dev (float): The standard deviation of the normal distribution.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot   \n\n    Example:\n    >>> plot_normal_distribution(0, 1)\n    \"\"\"\n    x = np.linspace(mean - 3*std_dev, mean + 3*std_dev, 100)\n    plt.plot(x, norm.pdf(x, mean, std_dev))\n    plt.show()\n```"}
{"seed": "{\"task_id\": 25263, \"prompt\": \"def f_25263(df):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"df.to_dict()\", \"test_start\": \"\\nimport pandas as pd\\n\\ndef check(candidate):\", \"test\": [\"\\n    df = pd.DataFrame([[1,2,3], [4,5,6], [6,5,4], [2,1,0]], columns=[\\\"AA\\\", \\\"b\\\", \\\"3\\\"])\\n    assert candidate(df) == {'AA': {0: 1, 1: 4, 2: 6, 3: 2},\\n                             'b': {0: 2, 1: 5, 2: 5, 3: 1},\\n                             '3': {0: 3, 1: 6, 2: 4, 3: 0}}\\n\", \"\\n    df = pd.DataFrame([[1,2,3], [4,5,6], [6,5,4], [2,1,0]])\\n    assert candidate(df) == {0: {0: 1, 1: 4, 2: 6, 3: 2},\\n                             1: {0: 2, 1: 5, 2: 5, 3: 1},\\n                             2: {0: 3, 1: 6, 2: 4, 3: 0}}\\n\"], \"entry_point\": \"f_25263\", \"intent\": \"\\u30c7\\u30fc\\u30bf\\u30d5\\u30ec\\u30fc\\u30e0\\u3092\\u8f9e\\u66f8\\u578b\\u30aa\\u30d6\\u30b8\\u30a7\\u30af\\u30c8\\u306b\\u5909\\u63db\\u3059\\u308b\", \"library\": [\"pandas\"]}", "generation": "Scenario 1:\npandas, numpy, matplotlib, random, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\nimport seaborn as sns\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D']\n\ndef generate_random_dataframe_and_visualize(n_rows):\n    \"\"\"\n    Generate a random DataFrame with n_rows and 4 columns ('A', 'B', 'C', 'D'). \n    Then, visualize the correlation matrix of the DataFrame using a heatmap.\n\n    Parameters:\n    n_rows (int): The number of rows for the DataFrame.\n\n    Returns:\n    DataFrame: The generated random DataFrame.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - random\n    - seaborn\n\n    Example:\n    >>> df = generate_random_dataframe_and_visualize(1000)\n    >>> print(df)\n    \"\"\"\n    df = pd.DataFrame(np.random.randint(0,100,size=(n_rows, 4)), columns=COLUMNS)\n\n    corr = df.corr()\n    sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, annot=True, cmap=sns.diverging_palette(220, 20, as_cmap=True))\n    plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, matplotlib, random, sklearn.linear_model\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef generate_dataframe_and_regression(n_rows):\n    \"\"\"\n    Generate a random DataFrame with n_rows and 5 columns ('A', 'B', 'C', 'D', 'E'), where 'E' is the dependent variable.\n    Then, perform a multiple linear regression with 'A', 'B', 'C', 'D' as independent variables and 'E' as the dependent variable.\n\n    Parameters:\n    n_rows (int): The number of rows for the DataFrame.\n\n    Returns:\n    list: The coefficients of the regression model.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.linear_model\n\n    Example:\n    >>> coefs = generate_dataframe_and_regression(1000)\n    >>> print(coefs)\n    \"\"\"\n    X = np.random.randint(0, 100, size=(n_rows, 4))\n    y = np.sum(X, axis=1) + np.random.normal(0, 10, size=n_rows)\n\n    df = pd.DataFrame(np.column_stack([X, y]), columns=COLUMNS)\n\n    model = LinearRegression()\n    model.fit(df[COLUMNS[:-1]], df[COLUMNS[-1]])\n\n    return model.coef_\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, matplotlib, random, sklearn.cluster\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\n\n# Constants\nCOLUMNS = ['A', 'B']\n\ndef generate_dataframe_and_clustering(n_rows, n_clusters):\n    \"\"\"\n    Generate a random DataFrame with n_rows and 2 columns ('A', 'B').\n    Then, perform a K-means clustering and visualize the clusters.\n\n    Parameters:\n    n_rows (int): The number of rows for the DataFrame.\n    n_clusters (int): The number of clusters for the K-means algorithm.\n\n    Returns:\n    numpy.ndarray: The labels of the clusters for each data point.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.cluster\n\n    Example:\n    >>> labels = generate_dataframe_and_clustering(1000, 3)\n    >>> print(labels)\n    \"\"\"\n    df = pd.DataFrame(np.random.randint(0,100,size=(n_rows, 2)), columns=COLUMNS)\n\n    kmeans = KMeans(n_clusters=n_clusters)\n    kmeans.fit(df)\n\n    labels = kmeans.labels_\n    df['Cluster'] = labels\n\n    plt.scatter(df['A'], df['B'], c=df['Cluster'])\n    plt.show()\n\n    return labels\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, matplotlib, random, sklearn.decomposition\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef generate_dataframe_and_pca(n_rows):\n    \"\"\"\n    Generate a random DataFrame with n_rows and 5 columns ('A', 'B', 'C', 'D', 'E').\n    Then, perform a PCA and visualize the explained variance ratio.\n\n    Parameters:\n    n_rows (int): The number of rows for the DataFrame.\n\n    Returns:\n    numpy.ndarray: The explained variance ratio of the PCA.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.decomposition\n\n    Example:\n    >>> explained_variance_ratio = generate_dataframe_and_pca(1000)\n    >>> print(explained_variance_ratio)\n    \"\"\"\n    df = pd.DataFrame(np.random.randint(0,100,size=(n_rows, 5)), columns=COLUMNS)\n\n    pca = PCA(n_components=2)\n    pca.fit(df)\n\n    explained_variance_ratio = pca.explained_variance_ratio_\n\n    plt.bar(range(2), explained_variance_ratio)\n    plt.ylabel('Explained Variance Ratio')\n    plt.xlabel('Principal Component')\n    plt.show()\n\n    return explained_variance_ratio\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, matplotlib, random, sklearn.ensemble\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\n\n# Constants\nFEATURE_COLUMNS = ['A', 'B', 'C', 'D']\nTARGET_COLUMN = 'E'\n\ndef generate_dataframe_and_random_forest(n_rows):\n    \"\"\"\n    Generate a random DataFrame with n_rows and 5 columns ('A', 'B', 'C', 'D', 'E'), where 'E' is the target variable.\n    Then, perform a Random Forest classification and visualize the confusion matrix.\n\n    Parameters:\n    n_rows (int): The number of rows for the DataFrame.\n\n    Returns:\n    numpy.ndarray: The confusion matrix of the classification.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.ensemble\n    - sklearn.metrics\n    - sklearn.model_selection\n\n    Example:\n    >>> confusion_mat = generate_dataframe_and_random_forest(1000)\n    >>> print(confusion_mat)\n    \"\"\"\n    X = np.random.randint(0, 100, size=(n_rows, 4))\n    y = np.random.randint(0, 2, size=n_rows)\n\n    df = pd.DataFrame(np.column_stack([X, y]), columns=FEATURE_COLUMNS + [TARGET_COLUMN])\n\n    X_train, X_test, y_train, y_test = train_test_split(df[FEATURE_COLUMNS], df[TARGET_COLUMN], test_size=0.2, random_state=42)\n\n    clf = RandomForestClassifier()\n    clf.fit(X_train, y_train)\n\n    y_pred = clf.predict(X_test)\n\n    confusion_mat = confusion_matrix(y_test, y_pred)\n\n    plt.matshow(confusion_mat, cmap=plt.cm.Blues)\n    plt.title('Confusion Matrix')\n    plt.colorbar()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()\n\n    return confusion_mat\n```"}
{"seed": "{\"task_id\": 28178, \"prompt\": \"def f_28178(soup):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"soup.find('tbody').find_all('tr')\", \"test_start\": \"\\nfrom bs4 import BeautifulSoup\\n\\ndef check(candidate):\", \"test\": [\"\\n    soup = BeautifulSoup(\\\"<td><b>Address:</b></td><tbody><tr>My home address</tr></tbody>\\\")\\n    result = candidate(soup)\\n    assert len(result) == 1\\n    assert result[0].contents == ['My home address']\\n\"], \"entry_point\": \"f_28178\", \"intent\": \"HTML\\u30c6\\u30fc\\u30d6\\u30eb\\u304b\\u3089\\u5404\\u884c\\u3092\\u53d6\\u5f97\\u3059\\u308b\", \"library\": [\"bs4\"]}", "generation": "Scenario 1:\nbs4, pandas, requests, re\n```python\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport requests\nimport re\n\ndef extract_table_data(url, table_id):\n    \"\"\"\n    Extract table data from the specified URL and convert it to pandas DataFrame.\n\n    Parameters:\n    url (str): The URL from which to scrape the data.\n    table_id (str): The id of the table to be scraped.\n\n    Returns:\n    DataFrame: A pandas DataFrame containing the scraped table data.\n\n    Requirements:\n    - bs4.BeautifulSoup\n    - pandas\n    - requests\n    - re\n\n    Example:\n    >>> url = 'https://www.example.com'\n    >>> table_id = 'table1'\n    >>> df = extract_table_data(url, table_id)\n    >>> print(df)\n    \"\"\"\n    response = requests.get(url)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    table = soup.find('table', {'id': table_id})\n    \n    header = []\n    for th in table.find_all('th'):\n        header.append(re.sub('\\n', '', th.text))\n\n    data = []\n    for row in table.find_all('tr'):\n        rowData = []\n        for td in row.find_all('td'):\n            rowData.append(re.sub('\\n', '', td.text))\n        data.append(rowData)\n\n    df = pd.DataFrame(data, columns=header)\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nbs4, requests, csv, os\n```python\nfrom bs4 import BeautifulSoup\nimport requests\nimport csv\nimport os\n\ndef download_images_from_webpage(url, output_directory):\n    \"\"\"\n    Downloads all images from a webpage to a specified directory.\n\n    Parameters:\n    url (str): The URL from which to download the images.\n    output_directory (str): The directory to which to save the images.\n\n    Returns:\n    int: The number of images downloaded.\n\n    Requirements:\n    - bs4.BeautifulSoup\n    - requests\n    - os\n\n    Example:\n    >>> url = 'https://www.example.com'\n    >>> output_directory = './images'\n    >>> count = download_images_from_webpage(url, output_directory)\n    >>> print(f'{count} images downloaded.')\n    \"\"\"\n    response = requests.get(url)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    images = soup.find_all('img')\n\n    if not os.path.exists(output_directory):\n        os.makedirs(output_directory)\n\n    downloaded = 0\n    for img in images:\n        img_url = img.get('src')\n        if img_url.startswith('http'):\n            img_data = requests.get(img_url).content\n            img_name = os.path.join(output_directory, img_url.split('/')[-1])\n            with open(img_name, 'wb') as handler:\n                handler.write(img_data)\n            downloaded += 1\n\n    return downloaded\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nbs4, requests, collections\n```python\nfrom bs4 import BeautifulSoup\nimport requests\nfrom collections import Counter\n\ndef count_tags_in_webpage(url):\n    \"\"\"\n    Counts the number of each type of HTML tag in a webpage.\n\n    Parameters:\n    url (str): The URL of the webpage to analyze.\n\n    Returns:\n    collections.Counter: A Counter object with each tag and its count.\n\n    Requirements:\n    - bs4.BeautifulSoup\n    - requests\n    - collections.Counter\n\n    Example:\n    >>> url = 'https://www.example.com'\n    >>> tag_count = count_tags_in_webpage(url)\n    >>> print(tag_count)\n    \"\"\"\n    response = requests.get(url)\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    tags = [tag.name for tag in soup.find_all()]\n\n    return Counter(tags)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nbs4, requests, json\n```python\nfrom bs4 import BeautifulSoup\nimport requests\nimport json\n\ndef extract_json_ld_data(url):\n    \"\"\"\n    Extracts JSON-LD data from a webpage.\n\n    Parameters:\n    url (str): The URL of the webpage to scrape.\n\n    Returns:\n    dict: A dictionary containing the JSON-LD data.\n\n    Requirements:\n    - bs4.BeautifulSoup\n    - requests\n    - json\n\n    Example:\n    >>> url = 'https://www.example.com'\n    >>> json_ld_data = extract_json_ld_data(url)\n    >>> print(json_ld_data)\n    \"\"\"\n    response = requests.get(url)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    json_ld = soup.find('script', type='application/ld+json')\n\n    return json.loads(json_ld.string)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nbs4, requests, lxml\n```python\nfrom bs4 import BeautifulSoup\nimport requests\nfrom lxml import html\n\ndef get_all_links(url):\n    \"\"\"\n    Extracts all hyperlinks from a webpage.\n\n    Parameters:\n    url (str): The URL of the webpage to scrape.\n\n    Returns:\n    list: A list of all hyperlinks on the webpage.\n\n    Requirements:\n    - bs4.BeautifulSoup\n    - requests\n    - lxml\n\n    Example:\n    >>> url = 'https://www.example.com'\n    >>> links = get_all_links(url)\n    >>> print(links)\n    \"\"\"\n    response = requests.get(url)\n    soup = BeautifulSoup(response.text, 'lxml')\n    links = [a['href'] for a in soup.find_all('a', href=True)]\n\n    return links\n```\n"}
{"seed": "{\"task_id\": 8656, \"prompt\": \"def f_8656():\\n\\t\", \"suffix\": \"\\n\\treturn handler\", \"canonical_solution\": \"class handler(http.server.BaseHTTPRequestHandler):\\n\\t    def do_POST(self):\\n\\t        os.environ['REQUEST_METHOD'] = 'POST'\\n\\t        form = cgi.FieldStorage(self.rfile, self.headers)\", \"test_start\": \"\\nimport cgi\\nimport http.server\\n\\ndef check(candidate):\", \"test\": [\"\\n    try:\\n        handler = candidate()\\n        srvr = http.server.HTTPServer(('127.0.0.1', 8889), handler)\\n    except:\\n        assert False\\n\"], \"entry_point\": \"f_8656\", \"intent\": \"POST\\u30c7\\u30fc\\u30bf\\u3092cgi.FieldStrage\\u3067\\u53d7\\u3051\\u53d6\\u308b\", \"library\": [\"cgi\", \"http\"]}", "generation": "Scenario 1:\ncgi, http, urllib, json, uuid\n```python\nimport cgi\nimport http.server\nimport urllib.parse\nimport json\nfrom uuid import uuid4\n\n# Constants\nSUCCESS_RESPONSE = {\n    'status': 'success',\n    'message': 'Data received successfully.'\n}\n\nERROR_RESPONSE = {\n    'status': 'error',\n    'message': 'Invalid data received.'\n}\n\ndef handle_post_request():\n    \"\"\"\n    Handle POST data, validate it and respond with a success or error message.\n    The data should be a JSON object with a 'data' key.\n\n    Returns:\n    function: A function that handles HTTP POST requests.\n\n    Requirements:\n    - cgi\n    - http.server\n    - urllib.parse\n    - json\n    - uuid\n\n    Example:\n    >>> handler = handle_post_request()\n    >>> server = http.server.HTTPServer(('127.0.0.1', 8080), handler)\n    >>> server.serve_forever()\n    \"\"\"\n    class PostRequestHandler(http.server.BaseHTTPRequestHandler):\n        def do_POST(self):\n            ctype, pdict = cgi.parse_header(self.headers.get('content-type'))\n            if ctype != 'application/json':\n                self.send_response(400)\n                self.end_headers()\n                response = ERROR_RESPONSE\n                response['message'] = 'Content-Type header is not application/json'\n                self.wfile.write(json.dumps(response).encode())\n                return\n\n            length = int(self.headers.get('content-length'))\n            message = json.loads(self.rfile.read(length))\n            if 'data' not in message:\n                self.send_response(400)\n                self.end_headers()\n                response = ERROR_RESPONSE\n                response['message'] = 'No data received'\n                self.wfile.write(json.dumps(response).encode())\n                return\n\n            self.send_response(200)\n            self.send_header('Content-type', 'application/json')\n            self.end_headers()\n            self.wfile.write(json.dumps(SUCCESS_RESPONSE).encode())\n\n    return PostRequestHandler\n\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncgi, http.server, base64, hashlib, binascii\n```python\nimport cgi\nimport http.server\nfrom base64 import b64decode\nfrom hashlib import pbkdf2_hmac\nfrom binascii import hexlify\n\n# Constants\nPASSWORD_SALT = b'some_random_salt'\nPASSWORD_ITERATIONS = 100000\n\ndef handle_auth_request():\n    \"\"\"\n    Handle a POST request with 'username' and 'password' fields.\n    The password is hashed and compared with a stored hash.\n    A success or failure message is returned.\n\n    Returns:\n    function: A function that handles HTTP POST requests.\n\n    Requirements:\n    - cgi\n    - http.server\n    - base64\n    - hashlib\n    - binascii\n\n    Example:\n    >>> handler = handle_auth_request()\n    >>> server = http.server.HTTPServer(('127.0.0.1', 8080), handler)\n    >>> server.serve_forever()\n    \"\"\"\n    class AuthRequestHandler(http.server.BaseHTTPRequestHandler):\n        def do_POST(self):\n            ctype, pdict = cgi.parse_header(self.headers.get('content-type'))\n            if ctype != 'multipart/form-data':\n                self.send_response(400)\n                self.end_headers()\n                return\n\n            form = cgi.FieldStorage(fp=self.rfile, headers=self.headers, environ={'REQUEST_METHOD':'POST'})\n\n            if 'username' not in form or 'password' not in form:\n                self.send_response(400)\n                self.end_headers()\n                return\n\n            username = form['username'].value\n            password = form['password'].value\n\n            password_hash = pbkdf2_hmac('sha256', password.encode(), PASSWORD_SALT, PASSWORD_ITERATIONS)\n            stored_hash = b64decode('stored_hash_for_the_username')\n\n            if hexlify(password_hash) != stored_hash:\n                self.send_response(401)\n                self.end_headers()\n                return\n\n            self.send_response(200)\n            self.end_headers()\n\n    return AuthRequestHandler\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ncgi, http.server, os, sqlite3\n```python\nimport cgi\nimport http.server\nimport os\nimport sqlite3\n\n# Constants\nDATABASE_NAME = 'database.db'\n\ndef handle_database_request():\n    \"\"\"\n    Handle a POST request to insert a record into the database.\n    The record should be a JSON object with 'name' and 'value' keys.\n\n    Returns:\n    function: A function that handles HTTP POST requests.\n\n    Requirements:\n    - cgi\n    - http.server\n    - os\n    - sqlite3\n\n    Example:\n    >>> handler = handle_database_request()\n    >>> server = http.server.HTTPServer(('127.0.0.1', 8080), handler)\n    >>> server.serve_forever()\n    \"\"\"\n    class DatabaseRequestHandler(http.server.BaseHTTPRequestHandler):\n        def do_POST(self):\n            ctype, pdict = cgi.parse_header(self.headers.get('content-type'))\n            if ctype != 'application/json':\n                self.send_response(400)\n                self.end_headers()\n                return\n\n            length = int(self.headers.get('content-length'))\n            record = json.loads(self.rfile.read(length))\n            if 'name' not in record or 'value' not in record:\n                self.send_response(400)\n                self.end_headers()\n                return\n\n            conn = sqlite3.connect(DATABASE_NAME)\n            cursor = conn.cursor()\n            cursor.execute(\"INSERT INTO records (name, value) VALUES (?, ?)\", (record['name'], record['value']))\n            conn.commit()\n            conn.close()\n\n            self.send_response(200)\n            self.end_headers()\n\n    return DatabaseRequestHandler\n\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ncgi, http.server, smtplib, email.mime.text\n```python\nimport cgi\nimport http.server\nimport smtplib\nfrom email.mime.text import MIMEText\n\n# Constants\nSMTP_SERVER = 'smtp.gmail.com'\nSMTP_PORT = 587\nSMTP_USERNAME = 'your_username@gmail.com'\nSMTP_PASSWORD = 'your_password'\n\ndef handle_email_request():\n    \"\"\"\n    Handle a POST request to send an email.\n    The email data should be a JSON object with 'subject', 'message', and 'to' keys.\n\n    Returns:\n    function: A function that handles HTTP POST requests.\n\n    Requirements:\n    - cgi\n    - http.server\n    - smtplib\n    - email.mime.text\n\n    Example:\n    >>> handler = handle_email_request()\n    >>> server = http.server.HTTPServer(('127.0.0.1', 8080), handler)\n    >>> server.serve_forever()\n    \"\"\"\n    class EmailRequestHandler(http.server.BaseHTTPRequestHandler):\n        def do_POST(self):\n            ctype, pdict = cgi.parse_header(self.headers.get('content-type'))\n            if ctype != 'application/json':\n                self.send_response(400)\n                self.end_headers()\n                return\n\n            length = int(self.headers.get('content-length'))\n            email_data = json.loads(self.rfile.read(length))\n            if 'subject' not in email_data or 'message' not in email_data or 'to' not in email_data:\n                self.send_response(400)\n                self.end_headers()\n                return\n\n            msg = MIMEText(email_data['message'])\n            msg['Subject'] = email_data['subject']\n            msg['From'] = SMTP_USERNAME\n            msg['To'] = email_data['to']\n\n            server = smtplib.SMTP(SMTP_SERVER, SMTP_PORT)\n            server.starttls()\n            server.login(SMTP_USERNAME, SMTP_PASSWORD)\n            server.sendmail(SMTP_USERNAME, [email_data['to']], msg.as_string())\n            server.quit()\n\n            self.send_response(200)\n            self.end_headers()\n\n    return EmailRequestHandler\n\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\ncgi, http.server, PIL.Image, os, io\n```python\nimport cgi\nimport http.server\nfrom PIL import Image\nimport os\nimport io\n\n# Constants\nIMAGE_DIRECTORY = 'images/'\n\ndef handle_image_request():\n    \"\"\"\n    Handle a POST request to save an image to disk.\n    The image data should be a part of multipart/form-data.\n\n    Returns:\n    function: A function that handles HTTP POST requests.\n\n    Requirements:\n    - cgi\n    - http.server\n    - PIL.Image\n    - os\n    - io\n\n    Example:\n    >>> handler = handle_image_request()\n    >>> server = http.server.HTTPServer(('127.0.0.1', 8080), handler)\n    >>> server.serve_forever()\n    \"\"\"\n    class ImageRequestHandler(http.server.BaseHTTPRequestHandler):\n        def do_POST(self):\n            ctype, pdict = cgi.parse_header(self.headers.get('content-type'))\n            if ctype != 'multipart/form-data':\n                self.send_response(400)\n                self.end_headers()\n                return\n\n            form = cgi.FieldStorage(fp=self.rfile, headers=self.headers, environ={'REQUEST_METHOD':'POST'})\n\n            if 'image' not in form:\n                self.send_response(400)\n                self.end_headers()\n                return\n\n            image_field = form['image']\n            image_data = image_field.file.read()\n            image = Image.open(io.BytesIO(image_data))\n\n            image_path = os.path.join(IMAGE_DIRECTORY, image_field.filename)\n            image.save(image_path)\n\n            self.send_response(200)\n            self.end_headers()\n\n    return ImageRequestHandler\n```"}
{"seed": "{\"task_id\": 9836, \"prompt\": \"def f_9836(li):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"random.choice(li)\", \"test_start\": \"\\nimport random\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([1,2,3]) in [1,2,3]\\n\"], \"entry_point\": \"f_9836\", \"intent\": \"\\u30ea\\u30b9\\u30c8`li`\\u306e\\u4e2d\\u304b\\u3089\\u30e9\\u30f3\\u30c0\\u30e0\\u306b\\u4e00\\u3064\\u306e\\u8981\\u7d20\\u3092\\u9078\\u629e\\u3059\\u308b\", \"library\": [\"random\"]}", "generation": "Scenario 1:\npandas, random, os, json\n```python\nimport pandas as pd\nimport random\nimport os\nimport json\n\n# Constants\nCITIES = ['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']\nCOUNTRIES = ['USA', 'UK', 'China', 'Japan', 'Australia']\n\ndef random_city_country_df_generator(n):\n    \"\"\"\n    Generate a pandas Dataframe with n rows, each row containing a random city and its corresponding \n    country from a predefined list of cities and countries.\n\n    Parameters:\n    n (int): The number of rows in the DataFrame.\n\n    Returns:\n    DataFrame: A pandas DataFrame with n rows of city and country pairs.\n\n    Requirements:\n    - pandas\n    - random\n    - os\n    - json\n\n    Example:\n    >>> df = random_city_country_df_generator(10)\n    >>> df.to_json(os.path.join('data', 'city_country.json'))\n    \"\"\"\n    data = []\n    \n    for _ in range(n):\n        city = random.choice(CITIES)\n        country = COUNTRIES[CITIES.index(city)]\n        data.append([city, country])\n        \n    df = pd.DataFrame(data, columns=['City', 'Country'])\n    \n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, random, matplotlib, math\n```python\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nimport math\n\ndef random_walk_plot(steps):\n    \"\"\"\n    Generate a plot of a random walk in a 2D plane for a given number of steps.\n\n    Parameters:\n    steps (int): The number of steps in the random walk.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - random\n    - matplotlib.pyplot\n    - math\n\n    Example:\n    >>> random_walk_plot(1000)\n    \"\"\"\n    x = np.zeros(steps)\n    y = np.zeros(steps)\n\n    for i in range(1, steps):\n        val = random.randint(1, 4)\n        if val == 1:\n            x[i] = x[i - 1] + 1\n            y[i] = y[i - 1]\n        elif val == 2:\n            x[i] = x[i - 1] - 1\n            y[i] = y[i - 1]\n        elif val == 3:\n            x[i] = x[i - 1]\n            y[i] = y[i - 1] + 1\n        else:\n            x[i] = x[i - 1]\n            y[i] = y[i - 1] - 1\n\n    plt.title(\"Random Walk ($n = \" + str(steps) + \"$ steps)\")\n    plt.plot(x, y)\n    plt.savefig(\"Random_Walk.png\")\n\n    return None\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nrandom, statistics, matplotlib\n```python\nimport random\nimport statistics\nimport matplotlib.pyplot as plt\n\ndef random_numbers_statistics_plot(n, m):\n    \"\"\"\n    Generate a bar chart showing the mean and standard deviation of n random numbers between 1 and m.\n\n    Parameters:\n    n (int): The number of random numbers to generate.\n    m (int): The upper limit for the random numbers.\n\n    Returns:\n    None\n\n    Requirements:\n    - random\n    - statistics\n    - matplotlib.pyplot\n\n    Example:\n    >>> random_numbers_statistics_plot(1000, 10)\n    \"\"\"\n    numbers = [random.randint(1, m) for _ in range(n)]\n    mean = statistics.mean(numbers)\n    stdev = statistics.stdev(numbers)\n\n    plt.bar(['Mean', 'Standard Deviation'], [mean, stdev])\n    plt.savefig(\"Random_Numbers_Statistics.png\")\n\n    return None\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nrandom, string, pandas\n```python\nimport random\nimport string\nimport pandas as pd\n\ndef random_string_dataframe(n, length):\n    \"\"\"\n    Generate a pandas DataFrame with n rows, each containing a random string of a given length.\n\n    Parameters:\n    n (int): The number of rows in the DataFrame.\n    length (int): The length of the random strings.\n\n    Returns:\n    DataFrame: A pandas DataFrame with n rows of random strings.\n\n    Requirements:\n    - random\n    - string\n    - pandas\n\n    Example:\n    >>> df = random_string_dataframe(10, 5)\n    >>> print(df)\n    \"\"\"\n    data = [''.join(random.choices(string.ascii_letters, k=length)) for _ in range(n)]\n    df = pd.DataFrame(data, columns=['Random String'])\n    \n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nrandom, itertools, numpy\n```python\nimport random\nimport itertools\nimport numpy as np\n\ndef random_combination_matrix(n, r):\n    \"\"\"\n    Generate a matrix where each row is a random combination of r elements from a list of n elements.\n\n    Parameters:\n    n (int): The number of elements in the list.\n    r (int): The number of elements in each combination.\n\n    Returns:\n    np.array: A numpy array with each row as a combination.\n\n    Requirements:\n    - random\n    - itertools\n    - numpy\n\n    Example:\n    >>> matrix = random_combination_matrix(10, 3)\n    >>> print(matrix)\n    \"\"\"\n    numbers = list(range(n))\n    combinations = list(itertools.combinations(numbers, r))\n    random.shuffle(combinations)\n    matrix = np.array(combinations)\n\n    return matrix\n```"}
{"seed": "{\"task_id\": 1589, \"prompt\": \"def f_1589(d):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"X = np.array(d, dtype='float32')\\n\\tX.tofile('binaryVec.bin')\", \"test_start\": \"\\nimport numpy as np\\n\\ndef check(candidate):\", \"test\": [\"\\n    f = open('binaryVec.bin', 'w')\\n    f.close()\\n    \\n    d = np.array([1., 2., 3.])\\n    candidate(d)\\n    d1 = np.fromfile('binaryVec.bin', dtype='float32')\\n    assert np.all(d == d1)\\n\"], \"entry_point\": \"f_1589\", \"intent\": \"\\u8981\\u7d20\\u304c\\u6570\\u5024\\u306e\\u30ea\\u30b9\\u30c8\\u578b\\u30c7\\u30fc\\u30bf`d`\\u3092\\u30d0\\u30a4\\u30ca\\u30ea\\u30c7\\u30fc\\u30bf`binaryVrc.bin`\\u3068\\u3057\\u3066\\u4fdd\\u5b58\\u3059\\u308b\", \"library\": [\"numpy\"]}", "generation": "Scenario 1: \nnumpy, os, random, datetime, struct\n```python\nimport numpy as np\nimport os\nimport random\nfrom datetime import datetime\nimport struct\n\n# Constants\nNUM_SAMPLES = 100\nNUM_FEATURES = 10\nFILENAME = 'binaryVec.bin'\n\ndef save_random_samples():\n    \"\"\"\n    Generate a 2D numpy array with NUM_SAMPLES samples and NUM_FEATURES features. \n    Each feature is a random float between 0 and 1. Save the array to a binary \n    file and return the size of the file in bytes.\n\n    Returns:\n    int: The size of the binary file in bytes.\n\n    Requirements:\n    - numpy\n    - os\n    - random\n    - datetime\n    - struct\n    \n    Example:\n    >>> save_random_samples()\n    \"\"\"\n    X = np.random.rand(NUM_SAMPLES, NUM_FEATURES).astype('float32')\n    X.tofile(FILENAME)\n\n    file_size = os.path.getsize(FILENAME)\n\n    return file_size\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nnumpy, pandas, matplotlib.pyplot, seaborn\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nFILENAME = 'binaryVec.bin'\n\ndef plot_binary_file():\n    \"\"\"\n    Read a binary file containing a float array, reshape it into a 2D array, \n    convert it into a pandas DataFrame, and plot a heatmap of the DataFrame.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - seaborn\n    \n    Example:\n    >>> plot_binary_file()\n    \"\"\"\n    X = np.fromfile(FILENAME, dtype='float32')\n    X = X.reshape(-1, int(np.sqrt(X.shape[0])))\n\n    df = pd.DataFrame(X)\n\n    plt.figure(figsize=(10, 10))\n    sns.heatmap(df, cmap='viridis')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnumpy, os, struct, datetime\n```python\nimport numpy as np\nimport os\nimport struct\nfrom datetime import datetime\n\n# Constants\nFILENAME = 'binaryVec.bin'\n\ndef calculate_file_checksum():\n    \"\"\"\n    Read a binary file, calculate the checksum of the file, and return \n    the checksum.\n\n    Returns:\n    int: The checksum of the file.\n\n    Requirements:\n    - numpy\n    - os\n    - struct\n    - datetime\n    \n    Example:\n    >>> calculate_file_checksum()\n    \"\"\"\n    with open(FILENAME, 'rb') as f:\n        data = f.read()\n\n    checksum = sum(struct.unpack('!{}f'.format(len(data)//4), data))\n\n    return checksum\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, scipy.fft, matplotlib.pyplot\n```python\nimport numpy as np\nfrom scipy.fft import fft\nimport matplotlib.pyplot as plt\n\n# Constants\nFILENAME = 'binaryVec.bin'\n\ndef compute_fft_and_plot():\n    \"\"\"\n    Read a binary file containing a float array, compute the fast Fourier \n    transform (FFT) of the array, and plot the absolute value of the FFT.\n\n    Requirements:\n    - numpy\n    - scipy.fft\n    - matplotlib.pyplot\n    \n    Example:\n    >>> compute_fft_and_plot()\n    \"\"\"\n    X = np.fromfile(FILENAME, dtype='float32')\n    X_fft = fft(X)\n\n    plt.figure(figsize=(10, 10))\n    plt.plot(np.abs(X_fft))\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnumpy, sklearn.preprocessing, pandas\n```python\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\n\n# Constants\nFILENAME = 'binaryVec.bin'\n\ndef standardize_and_save():\n    \"\"\"\n    Read a binary file containing a float array, standardize the features \n    by removing the mean and scaling to unit variance, save the result to \n    a new binary file, and return a pandas DataFrame of the standardized array.\n\n    Returns:\n    DataFrame: A pandas DataFrame of the standardized array.\n\n    Requirements:\n    - numpy\n    - sklearn.preprocessing\n    - pandas\n    \n    Example:\n    >>> standardize_and_save()\n    \"\"\"\n    X = np.fromfile(FILENAME, dtype='float32')\n    X = X.reshape(-1, int(np.sqrt(X.shape[0])))\n\n    scaler = StandardScaler()\n    X_std = scaler.fit_transform(X)\n\n    X_std.tofile(FILENAME.replace('.bin', '_std.bin'))\n    \n    df = pd.DataFrame(X_std)\n\n    return df\n```"}
{"seed": "{\"task_id\": 38532, \"prompt\": \"def f_38532(f):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"f.close()\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    f = open('tmp.txt', 'w')\\n    candidate(f)\\n    assert f.closed\\n\"], \"entry_point\": \"f_38532\", \"intent\": \"\\u958b\\u3044\\u3066\\u3044\\u308b\\u30d5\\u30a1\\u30a4\\u30eb'f'\\u3092\\u9589\\u3058\\u308b\", \"library\": []}", "generation": "Scenario 1: \nos, csv, json, datetime\n```python\nimport os\nimport csv\nimport json\nfrom datetime import datetime\n\n# Constants\nFILE_PATH = './'\n\ndef convert_csv_to_json(file_name):\n    \"\"\"\n    Convert a CSV file to a JSON file with the same name. The CSV file should \n    have a header row. The time of conversion is added as a key-value pair.\n\n    Parameters:\n    file_name (str): The name of the CSV file to convert.\n\n    Returns:\n    str: The name of the created JSON file.\n\n    Requirements:\n    - os\n    - csv\n    - json\n    - datetime\n\n    Example:\n    >>> convert_csv_to_json('sample.csv')\n    'sample.json'\n    \"\"\"\n    csv_file_path = os.path.join(FILE_PATH, file_name)\n    json_file_path = os.path.join(FILE_PATH, file_name.replace('.csv', '.json'))\n\n    data = []\n    with open(csv_file_path, 'r') as csv_file:\n        csv_reader = csv.DictReader(csv_file)\n        for row in csv_reader:\n            data.append(row)\n\n    data.append({'converted_on': str(datetime.now())})\n\n    with open(json_file_path, 'w') as json_file:\n        json.dump(data, json_file, indent=4)\n\n    return json_file.name\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nos, shutil, zipfile, datetime\n```python\nimport os\nimport shutil\nimport zipfile\nfrom datetime import datetime\n\n# Constants\nDIRECTORY_PATH = './'\n\ndef create_backup(directory_name):\n    \"\"\"\n    Create a zip backup of a directory with the current datetime appended to the\n    name of the backup file.\n\n    Parameters:\n    directory_name (str): The name of the directory to backup.\n\n    Returns:\n    str: The name of the backup file.\n\n    Requirements:\n    - os\n    - shutil\n    - zipfile\n    - datetime\n\n    Example:\n    >>> create_backup('sample_directory')\n    'sample_directory_backup_2023-08-15_12-30-00.zip'\n    \"\"\"\n    directory_path = os.path.join(DIRECTORY_PATH, directory_name)\n    backup_file_name = f'{directory_name}_backup_{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}.zip'\n    backup_file_path = os.path.join(DIRECTORY_PATH, backup_file_name)\n\n    with zipfile.ZipFile(backup_file_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        for root, dirs, files in os.walk(directory_path):\n            for file in files:\n                file_path = os.path.join(root, file)\n                zipf.write(file_path, file_path.replace(DIRECTORY_PATH, ''))\n\n    return backup_file_name\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nos, pandas, seaborn, matplotlib\n```python\nimport os\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nFILE_PATH = './'\n\ndef plot_data_distribution(file_name, column_name):\n    \"\"\"\n    Read a CSV file and plot the distribution of data in a specific column. Save \n    the plot as a PNG file with the same name as the CSV file.\n\n    Parameters:\n    file_name (str): The name of the CSV file.\n    column_name (str): The name of the column in the CSV file.\n\n    Returns:\n    str: The name of the created PNG file.\n\n    Requirements:\n    - os\n    - pandas\n    - seaborn\n    - matplotlib\n\n    Example:\n    >>> plot_data_distribution('sample.csv', 'age')\n    'sample.png'\n    \"\"\"\n    csv_file_path = os.path.join(FILE_PATH, file_name)\n    png_file_name = file_name.replace('.csv', '.png')\n    png_file_path = os.path.join(FILE_PATH, png_file_name)\n\n    df = pd.read_csv(csv_file_path)\n\n    plt.figure(figsize=(10, 6))\n    sns.distplot(df[column_name])\n    plt.savefig(png_file_path)\n\n    return png_file_name\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nos, pandas, numpy, datetime\n```python\nimport os\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\n\n# Constants\nFILE_PATH = './'\n\ndef add_random_column(file_name):\n    \"\"\"\n    Read a CSV file, add a column of random numbers to it, and save the modified \n    data back to the CSV file. The new column name should be the current datetime.\n\n    Parameters:\n    file_name (str): The name of the CSV file.\n\n    Returns:\n    str: The name of the modified CSV file.\n\n    Requirements:\n    - os\n    - pandas\n    - numpy\n    - datetime\n\n    Example:\n    >>> add_random_column('sample.csv')\n    'sample.csv'\n    \"\"\"\n    csv_file_path = os.path.join(FILE_PATH, file_name)\n\n    df = pd.read_csv(csv_file_path)\n\n    df[str(datetime.now())] = np.random.rand(len(df))\n\n    df.to_csv(csv_file_path, index=False)\n\n    return file_name\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nos, pathlib, shutil, datetime\n```python\nimport os\nimport pathlib\nimport shutil\nfrom datetime import datetime\n\n# Constants\nDIRECTORY_PATH = './'\n\ndef organize_files_by_extension(directory_name):\n    \"\"\"\n    Organize files in a directory by their file extensions. Each file type should \n    be moved to a subdirectory named after its file extension and created at the \n    current datetime.\n\n    Parameters:\n    directory_name (str): The name of the directory to organize.\n\n    Returns:\n    str: The name of the organized directory.\n\n    Requirements:\n    - os\n    - pathlib\n    - shutil\n    - datetime\n\n    Example:\n    >>> organize_files_by_extension('sample_directory')\n    'sample_directory'\n    \"\"\"\n    directory_path = os.path.join(DIRECTORY_PATH, directory_name)\n\n    for file in os.listdir(directory_path):\n        file_path = os.path.join(directory_path, file)\n        if os.path.isfile(file_path):\n            file_extension = pathlib.Path(file).suffix.lstrip('.')\n            if file_extension:\n                subdirectory_path = os.path.join(directory_path, f'{file_extension}_{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}')\n                os.makedirs(subdirectory_path, exist_ok=True)\n                shutil.move(file_path, subdirectory_path)\n\n    return directory_name\n```"}
{"seed": "{\"task_id\": 37696, \"prompt\": \"def f_37696(files, url, data):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"requests.post(url, files=files, data=data)\", \"test_start\": \"\\nimport requests\\nfrom unittest.mock import Mock\\n\\ndef check(candidate):\", \"test\": [\"\\n    r = requests.Response()\\n    r.status_code = 200\\n    requests.post = Mock(return_value = r)\\n    file_path = 'a.txt'\\n    with open (file_path, 'w') as f:\\n        f.write('abc')\\n    files = {'file': open(file_path, 'rb')}\\n    assert candidate(files, 'https://def.xyz', {'key':'value'}).status_code == 200\\n\"], \"entry_point\": \"f_37696\", \"intent\": \"multipart\\u306e\\u30ea\\u30af\\u30a8\\u30b9\\u30c8\\u3067\\u8907\\u6570\\u306e\\u30c7\\u30fc\\u30bf`files`, `data`\\u3092`url'\\u306bPOST\\u3059\\u308b\", \"library\": [\"requests\"]}", "generation": "Scenario 1: \nrequests, os, csv, time\n```python\nimport requests\nimport os\nimport csv\nimport time\n\n# Constants\nDATA_DIR = 'data_files'\nURL = 'https://def.xyz'\n\ndef post_files_and_data_to_url():\n    \"\"\"\n    Post all files in a directory and their corresponding data to a URL.\n    The data for each file is stored in a CSV file in the same directory.\n\n    Requirements:\n    - requests\n    - os\n    - csv\n    - time\n\n    Example:\n    >>> post_files_and_data_to_url()\n    \"\"\"\n    csv_file = os.path.join(DATA_DIR, 'data.csv')\n\n    with open(csv_file, 'r') as f:\n        reader = csv.reader(f)\n        data_list = list(reader)\n\n    for data in data_list:\n        file_name = data[0]\n        file_data = {'key': data[1]}\n        file_path = os.path.join(DATA_DIR, file_name)\n        with open(file_path, 'rb') as f:\n            files = {'file': f}\n            response = requests.post(URL, files=files, data=file_data)\n            print(f\"Posted {file_name} with status code {response.status_code}\")\n            time.sleep(2)  # sleep for 2 seconds to avoid flooding the server\n\n    return \"All files and data posted successfully\"\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nrequests, json, pandas, os\n```python\nimport requests\nimport json\nimport pandas as pd\nimport os\n\n# Constants\nAPI_KEY = 'your_api_key'\nURL = 'https://api.def.xyz'\nDATA_DIR = 'data_files'\n\ndef gather_and_post_data():\n    \"\"\"\n    Gather data from a JSON API, save the data to a CSV file, \n    then post the CSV file to another URL.\n\n    Requirements:\n    - requests\n    - json\n    - pandas\n    - os\n\n    Example:\n    >>> gather_and_post_data()\n    \"\"\"\n    headers = {'Authorization': f'Bearer {API_KEY}'}\n    response = requests.get(URL, headers=headers)\n    data = response.json()\n\n    df = pd.DataFrame(data)\n    csv_file_path = os.path.join(DATA_DIR, 'data.csv')\n    df.to_csv(csv_file_path, index=False)\n\n    with open(csv_file_path, 'rb') as f:\n        files = {'file': f}\n        response = requests.post(URL, files=files)\n        status = response.status_code\n\n    return status\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nrequests, bs4, os, time\n```python\nimport requests\nfrom bs4 import BeautifulSoup\nimport os\nimport time\n\n# Constants\nURL = 'https://website.com'\nDOWNLOAD_DIR = 'downloads'\n\ndef download_and_post_images():\n    \"\"\"\n    Download all images from a webpage and post them to another URL.\n\n    Requirements:\n    - requests\n    - bs4\n    - os\n    - time\n\n    Example:\n    >>> download_and_post_images()\n    \"\"\"\n    response = requests.get(URL)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    img_tags = soup.find_all('img')\n\n    for img in img_tags:\n        img_url = img['src']\n        img_name = os.path.basename(img_url)\n        img_path = os.path.join(DOWNLOAD_DIR, img_name)\n\n        with open(img_path, 'wb') as f:\n            response = requests.get(img_url)\n            f.write(response.content)\n\n        with open(img_path, 'rb') as f:\n            files = {'file': f}\n            response = requests.post(URL, files=files)\n            print(f\"Posted {img_name} with status code {response.status_code}\")\n            time.sleep(1)  # sleep for 1 second to avoid flooding the server\n\n    return \"All images downloaded and posted successfully\"\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nrequests, os, pathlib, shutil\n```python\nimport requests\nimport os\nfrom pathlib import Path\nimport shutil\n\n# Constants\nZIP_URL = 'https://website.com/data.zip'\nEXTRACT_DIR = 'extracted_data'\nPOST_URL = 'https://api.def.xyz'\n\ndef download_zip_and_post_files():\n    \"\"\"\n    Download a zip file from a URL, extract the files, \n    and post each file to another URL.\n\n    Requirements:\n    - requests\n    - os\n    - pathlib\n    - shutil\n\n    Example:\n    >>> download_zip_and_post_files()\n    \"\"\"\n    response = requests.get(ZIP_URL)\n    zip_file_path = os.path.join(EXTRACT_DIR, 'data.zip')\n\n    with open(zip_file_path, 'wb') as f:\n        f.write(response.content)\n\n    shutil.unpack_archive(zip_file_path, EXTRACT_DIR)\n\n    files_dir = Path(EXTRACT_DIR)\n    for file_path in files_dir.glob('*'):\n        if file_path.is_file():\n            with open(file_path, 'rb') as f:\n                files = {'file': f}\n                response = requests.post(POST_URL, files=files)\n                print(f\"Posted {file_path.name} with status code {response.status_code}\")\n\n    return \"All files from zip posted successfully\"\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nrequests, pandas, matplotlib.pyplot, os\n```python\nimport requests\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\n\n# Constants\nDATA_URL = 'https://api.def.xyz/data'\nCSV_FILE_PATH = 'data.csv'\nPOST_URL = 'https://api.def.xyz'\n\ndef fetch_process_and_post_data():\n    \"\"\"\n    Fetch data from a URL, process it into a CSV file, \n    create a plot from the data, save the plot to a PNG file, \n    and post both the CSV and PNG files to another URL.\n\n    Requirements:\n    - requests\n    - pandas\n    - matplotlib.pyplot\n    - os\n\n    Example:\n    >>> fetch_process_and_post_data()\n    \"\"\"\n    response = requests.get(DATA_URL)\n    data = response.json()\n\n    df = pd.DataFrame(data)\n    df.to_csv(CSV_FILE_PATH, index=False)\n\n    df.plot()\n    plt.savefig('plot.png')\n\n    with open(CSV_FILE_PATH, 'rb') as csv_file, open('plot.png', 'rb') as png_file:\n        files = {'csv': csv_file, 'png': png_file}\n        response = requests.post(POST_URL, files=files)\n        status = response.status_code\n\n    return status\n```\n"}
{"seed": "{\"task_id\": 29368, \"prompt\": \"def f_29368(X, y):\\n\\t\", \"suffix\": \"\\n\\treturn sss\", \"canonical_solution\": \"sss = StratifiedShuffleSplit()\\n\\tsss.get_n_splits(X, y)\", \"test_start\": \"\\nimport numpy as np\\nimport sklearn\\nfrom sklearn.model_selection import StratifiedShuffleSplit\\n\\ndef check(candidate):\", \"test\": [\"\\n    X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\\n    y = np.array([0, 0, 0, 1, 1, 1])\\n    assert candidate(X, y).__class__ == sklearn.model_selection._split.StratifiedShuffleSplit\\n\"], \"entry_point\": \"f_29368\", \"intent\": \"\\u30af\\u30e9\\u30b9\\u6570\\u306e\\u6bd4\\u7387\\u3092\\u4fdd\\u3063\\u305f\\u307e\\u307e\\u30c7\\u30fc\\u30bf\\u3092\\u5206\\u5272\\u3059\\u308b\", \"library\": [\"numpy\", \"sklearn\"]}", "generation": "Scenario 1: \nnumpy, pandas, sklearn, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\ndef PCA_visualization(X, y, test_size):\n    \"\"\"\n    Split the data into training set and test set with stratified sampling, standardize them, \n    and apply PCA and visualize the first two principal components.\n\n    Parameters:\n    X (numpy.ndarray): The feature matrix.\n    y (numpy.ndarray): The target vector.\n    test_size (float): The proportion of the dataset to include in the test split.\n\n    Returns:\n    PCA: The PCA object fitted on the training set.\n\n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.model_selection.train_test_split\n    - sklearn.preprocessing.StandardScaler\n    - sklearn.decomposition.PCA\n    - matplotlib.pyplot\n\n    Example:\n    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\n    >>> y = np.array([0, 0, 0, 1, 1, 1])\n    >>> PCA_visualization(X, y, 0.2)\n    \"\"\"\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, stratify=y)\n\n    sc = StandardScaler()\n    X_train_std = sc.fit_transform(X_train)\n    X_test_std = sc.transform(X_test)\n\n    pca = PCA()\n    X_train_pca = pca.fit_transform(X_train_std)\n    X_test_pca = pca.transform(X_test_std)\n\n    plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], color='red', alpha=0.5)\n    plt.scatter(X_test_pca[:, 0], X_test_pca[:, 1], color='blue', alpha=0.5)\n\n    return pca\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nnumpy, sklearn, matplotlib\n```python\nimport numpy as np\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\n\ndef logistic_regression_roc(X, y, test_size, random_state):\n    \"\"\"\n    Split the data into training set and test set with stratified sampling, fit a \n    logistic regression model and plot its ROC curve.\n\n    Parameters:\n    X (numpy.ndarray): The feature matrix.\n    y (numpy.ndarray): The target vector.\n    test_size (float): The proportion of the dataset to include in the test split.\n    random_state (int): The seed used by the random number generator.\n\n    Returns:\n    LogisticRegression: The logistic regression model fitted on the training set.\n\n    Requirements:\n    - numpy\n    - sklearn.model_selection.StratifiedShuffleSplit\n    - sklearn.linear_model.LogisticRegression\n    - sklearn.metrics.roc_curve, auc\n    - matplotlib.pyplot\n\n    Example:\n    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\n    >>> y = np.array([0, 0, 0, 1, 1, 1])\n    >>> logistic_regression_roc(X, y, 0.2, 42)\n    \"\"\"\n    sss = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n    for train_index, test_index in sss.split(X, y):\n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n\n    model = LogisticRegression()\n    model.fit(X_train, y_train)\n\n    y_score = model.predict_proba(X_test)[:,1]\n    fpr, tpr, _ = roc_curve(y_test, y_score)\n    roc_auc = auc(fpr, tpr)\n\n    plt.figure()\n    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return model\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnumpy, pandas, sklearn, seaborn\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\n\ndef random_forest_feature_importance(X, y, test_size, n_estimators, random_state):\n    \"\"\"\n    Split the data into training set and test set with stratified sampling, fit a \n    random forest classifier and plot the feature importances.\n\n    Parameters:\n    X (numpy.ndarray): The feature matrix.\n    y (numpy.ndarray): The target vector.\n    test_size (float): The proportion of the dataset to include in the test split.\n    n_estimators (int): The number of trees in the forest.\n    random_state (int): The seed used by the random number generator.\n\n    Returns:\n    RandomForestClassifier: The random forest classifier fitted on the training set.\n\n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.model_selection.StratifiedShuffleSplit\n    - sklearn.ensemble.RandomForestClassifier\n    - seaborn\n\n    Example:\n    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\n    >>> y = np.array([0, 0, 0, 1, 1, 1])\n    >>> random_forest_feature_importance(X, y, 0.2, 100, 42)\n    \"\"\"\n    sss = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n    for train_index, test_index in sss.split(X, y):\n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n\n    model = RandomForestClassifier(n_estimators=n_estimators, random_state=random_state)\n    model.fit(X_train, y_train)\n\n    feature_importances = model.feature_importances_\n    feature_importances = pd.DataFrame({'feature': range(X.shape[1]), 'importance': feature_importances})\n\n    sns.barplot(x='importance', y='feature', data=feature_importances.sort_values('importance', ascending=False))\n\n    return model\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, sklearn, matplotlib\n```python\nimport numpy as np\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\n\ndef svm_confusion_matrix(X, y, test_size, kernel, random_state):\n    \"\"\"\n    Split the data into training set and test set with stratified sampling, fit a \n    SVM classifier and plot the confusion matrix.\n\n    Parameters:\n    X (numpy.ndarray): The feature matrix.\n    y (numpy.ndarray): The target vector.\n    test_size (float): The proportion of the dataset to include in the test split.\n    kernel (str): Specifies the kernel type to be used in the algorithm.\n    random_state (int): The seed used by the random number generator.\n\n    Returns:\n    SVC: The SVM classifier fitted on the training set.\n\n    Requirements:\n    - numpy\n    - sklearn.model_selection.StratifiedShuffleSplit\n    - sklearn.svm.SVC\n    - sklearn.metrics.confusion_matrix\n    - matplotlib.pyplot\n\n    Example:\n    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\n    >>> y = np.array([0, 0, 0, 1, 1, 1])\n    >>> svm_confusion_matrix(X, y, 0.2, 'linear', 42)\n    \"\"\"\n    sss = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n    for train_index, test_index in sss.split(X, y):\n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n\n    model = SVC(kernel=kernel, random_state=random_state)\n    model.fit(X_train, y_train)\n\n    y_pred = model.predict(X_test)\n    cm = confusion_matrix(y_test, y_pred)\n\n    plt.matshow(cm, cmap=plt.cm.Blues)\n    plt.colorbar()\n    plt.show()\n\n    return model\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnumpy, sklearn, matplotlib\n```python\nimport numpy as np\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import precision_recall_curve\nimport matplotlib.pyplot as plt\n\ndef knn_precision_recall_curve(X, y, test_size, n_neighbors, random_state):\n    \"\"\"\n    Split the data into training set and test set with stratified sampling, fit a \n    k-nearest neighbors classifier and plot the precision-recall curve.\n\n    Parameters:\n    X (numpy.ndarray): The feature matrix.\n    y (numpy.ndarray): The target vector.\n    test_size (float): The proportion of the dataset to include in the test split.\n    n_neighbors (int): Number of neighbors to use.\n    random_state (int): The seed used by the random number generator.\n\n    Returns:\n    KNeighborsClassifier: The k-nearest neighbors classifier fitted on the training set.\n\n    Requirements:\n    - numpy\n    - sklearn.model_selection.StratifiedShuffleSplit\n    - sklearn.neighbors.KNeighborsClassifier\n    - sklearn.metrics.precision_recall_curve\n    - matplotlib.pyplot\n\n    Example:\n    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\n    >>> y = np.array([0, 0, 0, 1, 1, 1])\n    >>> knn_precision_recall_curve(X, y, 0.2, 3, 42)\n    \"\"\"\n    sss = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n    for train_index, test_index in sss.split(X, y):\n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n\n    model = KNeighborsClassifier(n_neighbors=n_neighbors)\n    model.fit(X_train, y_train)\n\n    y_score = model.predict_proba(X_test)[:,1]\n    precision, recall, _ = precision_recall_curve(y_test, y_score)\n\n    plt.figure()\n    plt.plot(recall, precision, marker='.')\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.show()\n\n    return model\n```"}
{"seed": "{\"task_id\": 40699, \"prompt\": \"def f_40699(low, high):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"plt.yticks(range(low,high))\", \"test_start\": \"\\nimport matplotlib.pyplot as plt\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert len(candidate(20, 50)[0]) == 30\\n\", \"\\n    assert len(candidate(0, 10)[0]) == 10\\n\"], \"entry_point\": \"f_40699\", \"intent\": \"y\\u8ef8\\u306e\\u30d7\\u30ed\\u30c3\\u30c8\\u306e\\u7bc4\\u56f2\\u3092\\u4e0b\\u9650`low`\\u3001\\u4e0a\\u9650`high`\\u306b\\u8a2d\\u5b9a\\u3059\\u308b\", \"library\": [\"matplotlib\"]}", "generation": "Scenario 1: \npandas, matplotlib, random, datetime, numpy\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import randint\nfrom datetime import datetime, timedelta\nimport numpy as np\n\n# Constants\nCITIES = ['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']\nWEATHER_CONDITIONS = ['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy']\n\ndef generate_and_plot_weather_report(low, high):\n    \"\"\"\n    Generate a report of weather conditions for a list of cities for the next 'high' days and \n    plot the weather conditions for the next 'high' days for the city with 'low' index in the CITIES list.\n\n    Parameters:\n    low (int): The index of the city in the CITIES list.\n    high (int): The number of days for the weather report.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with weather conditions for the cities.\n    \n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - random\n    - datetime\n    - numpy\n    \n    Example:\n    >>> report = generate_and_plot_weather_report(0, 7)\n    >>> print(report)\n    \"\"\"\n    report_data = []\n    \n    for i in range(high):\n        for city in CITIES:\n            date = datetime.now() + timedelta(days=i)\n            weather = WEATHER_CONDITIONS[randint(0, len(WEATHER_CONDITIONS)-1)]\n            report_data.append([city, date, weather])\n    \n    report_df = pd.DataFrame(report_data, columns=['City', 'Date', 'Weather'])\n    \n    city_report = report_df[report_df['City'] == CITIES[low]]\n    city_report['Weather'].value_counts().plot(kind='bar')\n    plt.yticks(range(0, high))\n    plt.show()\n    \n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, matplotlib, datetime, random, os\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom datetime import datetime, timedelta\nfrom random import randint\nimport os\n\n# Constants\nFILE_EXT = ['.jpg', '.png', '.txt', '.pdf', '.docx']\n\ndef generate_and_plot_files(low, high):\n    \"\"\"\n    Generate 'high' number of files with random extensions in a directory and plot a bar chart \n    showing the count of files per file extension. The y-axis range is set from 'low' to 'high'.\n\n    Parameters:\n    low (int): The lower limit for the y-axis.\n    high (int): The number of files to generate.\n\n    Returns:\n    None\n\n    Requirements:\n    - os\n    - numpy\n    - matplotlib.pyplot\n    - datetime\n    - random\n\n    Example:\n    >>> generate_and_plot_files(0, 10)\n    \"\"\"\n    os.mkdir('temp_files')\n    for i in range(high):\n        file_name = 'file' + str(i) + FILE_EXT[randint(0, len(FILE_EXT)-1)]\n        with open('temp_files/' + file_name, 'w') as f:\n            f.write('This is a random file.')\n\n    file_list = os.listdir('temp_files')\n    file_ext_counts = np.zeros(len(FILE_EXT), dtype=int)\n\n    for file in file_list:\n        ext = os.path.splitext(file)[1]\n        index = FILE_EXT.index(ext)\n        file_ext_counts[index] += 1\n\n    plt.bar(FILE_EXT, file_ext_counts)\n    plt.yticks(range(low, high))\n    plt.show()\n\n    # Clean up by deleting the files and directory\n    for file in file_list:\n        os.remove('temp_files/' + file)\n    os.rmdir('temp_files')\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \npandas, matplotlib, numpy, random, datetime\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom random import randint\nfrom datetime import datetime, timedelta\n\n# Constants\nPRODUCTS = ['Apples', 'Bananas', 'Oranges', 'Pears', 'Plums']\n\ndef generate_and_plot_sales_report(low, high):\n    \"\"\"\n    Generate a sales report for 'high' days for a list of products and plot the sales for the product \n    with 'low' index in the PRODUCTS list.\n\n    Parameters:\n    low (int): The index of the product in the PRODUCTS list.\n    high (int): The number of days for the sales report.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with sales for each product.\n    \n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - numpy\n    - random\n    - datetime\n    \n    Example:\n    >>> report = generate_and_plot_sales_report(0, 7)\n    >>> print(report)\n    \"\"\"\n    sales_data = []\n    \n    for i in range(high):\n        for product in PRODUCTS:\n            date = datetime.now() + timedelta(days=i)\n            sales = randint(10, 100)\n            sales_data.append([product, date, sales])\n    \n    sales_df = pd.DataFrame(sales_data, columns=['Product', 'Date', 'Sales'])\n    \n    product_sales = sales_df[sales_df['Product'] == PRODUCTS[low]]\n    product_sales.plot(x='Date', y='Sales', kind='bar')\n    plt.yticks(range(0, high))\n    plt.show()\n    \n    return sales_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, matplotlib, random, os, datetime\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\nimport os\nfrom datetime import datetime, timedelta\n\n# Constants\nDIRECTORIES = ['dir1', 'dir2', 'dir3', 'dir4', 'dir5']\n\ndef generate_and_plot_directories(low, high):\n    \"\"\"\n    Generate 'high' number of directories and plot a bar chart showing the creation time \n    of directories. The y-axis range is set from 'low' to 'high'.\n\n    Parameters:\n    low (int): The lower limit for the y-axis.\n    high (int): The number of directories to generate.\n\n    Returns:\n    None\n\n    Requirements:\n    - os\n    - numpy\n    - matplotlib.pyplot\n    - random\n    - datetime\n\n    Example:\n    >>> generate_and_plot_directories(0, 10)\n    \"\"\"\n    for i in range(high):\n        os.mkdir(DIRECTORIES[randint(0, len(DIRECTORIES)-1)] + str(i))\n\n    dir_list = [d for d in os.listdir() if os.path.isdir(d)]\n    dir_creation_times = []\n\n    for dir in dir_list:\n        creation_time = os.path.getmtime(dir)\n        dir_creation_times.append(creation_time)\n\n    plt.bar(dir_list, dir_creation_times)\n    plt.yticks(range(low, high))\n    plt.show()\n\n    # Clean up by deleting the directories\n    for dir in dir_list:\n        os.rmdir(dir)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \npandas, matplotlib, random, datetime, numpy\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import randint\nfrom datetime import datetime, timedelta\nimport numpy as np\n\n# Constants\nSPORTS = ['Basketball', 'Football', 'Baseball', 'Hockey', 'Soccer']\n\ndef generate_and_plot_sports_report(low, high):\n    \"\"\"\n    Generate a report of games played for a list of sports for the next 'high' days and \n    plot the games played for the next 'high' days for the sport with 'low' index in the SPORTS list.\n\n    Parameters:\n    low (int): The index of the sport in the SPORTS list.\n    high (int): The number of days for the sports report.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with games played for each sport.\n    \n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - random\n    - datetime\n    - numpy\n    \n    Example:\n    >>> report = generate_and_plot_sports_report(0, 7)\n    >>> print(report)\n    \"\"\"\n    report_data = []\n    \n    for i in range(high):\n        for sport in SPORTS:\n            date = datetime.now() + timedelta(days=i)\n            games = randint(1, 5)\n            report_data.append([sport, date, games])\n    \n    report_df = pd.DataFrame(report_data, columns=['Sport', 'Date', 'Games Played'])\n    \n    sport_report = report_df[report_df['Sport'] == SPORTS[low]]\n    sport_report['Games Played'].plot(kind='bar')\n    plt.yticks(range(0, high))\n    plt.show()\n    \n    return report_df\n```"}
{"seed": "{\"task_id\": 11011, \"prompt\": \"def f_11011(fig, onclick):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"fig.canvas.mpl_connect('pick_event', onclick)\", \"test_start\": \"\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom mpl_toolkits.mplot3d import Axes3D\\n\\ndef check(candidate):\", \"test\": [\"\\n    def onclick(event):\\n        pass\\n    fig = plt.figure()\\n    X = [[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5]]\\n    Y = [[1,1,1,1,1],[2,2,2,2,2],[3,3,3,3,3]]\\n    Z = [[10,11,13,14,16],[5,8,7,7,7,],[0,0,0,9,8]]\\n    ax = Axes3D(fig)\\n    ax.scatter3D(np.ravel(X),np.ravel(Y),np.ravel(Z))\\n    try:\\n        candidate(fig, onclick)\\n    except:\\n        assert False\\n\"], \"entry_point\": \"f_11011\", \"intent\": \"\\u30b0\\u30e9\\u30d5\\u4e0a\\u3067\\u9078\\u629e\\u3055\\u308c\\u305f\\u30c7\\u30fc\\u30bf\\u306e\\u5ea7\\u6a19\\u3092\\u8868\\u793a\\u3059\\u308b\", \"library\": [\"matplotlib\", \"mpl_toolkits\", \"numpy\"]}", "generation": "Scenario 1: \nmatplotlib, mpl_toolkits, numpy, pandas, os\n```python\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport numpy as np\nimport pandas as pd\nimport os\n\n# Constants\nDATA_DIR = 'data'\nFILE_NAME = 'sample.csv'\n\ndef plot_and_save_3d_scatter_from_csv(file_path, onpick):\n    \"\"\"\n    Plot a 3D scatter plot from a CSV file and save the figure to a directory.\n    The CSV file must have three columns: 'x', 'y', 'z'.\n    \n    Parameters:\n    file_path (str): The path to the CSV file.\n    onpick (function): The function to be called when a data point is picked.\n    \n    Requirements:\n    - matplotlib\n    - mpl_toolkits\n    - numpy\n    - pandas\n    - os\n    \n    Example:\n    >>> def onpick(event):\n    ...     ind = event.ind\n    ...     print(f'You picked data point(s) {ind}')\n    >>> plot_and_save_3d_scatter_from_csv('data/sample.csv', onpick)\n    \"\"\"\n    df = pd.read_csv(file_path)\n\n    fig = plt.figure()\n    ax = Axes3D(fig)\n    scatter = ax.scatter(df['x'], df['y'], df['z'], picker=True)\n    fig.canvas.mpl_connect('pick_event', onpick)\n\n    output_dir = os.path.join(DATA_DIR, 'output')\n    os.makedirs(output_dir, exist_ok=True)\n\n    fig_path = os.path.join(output_dir, 'scatter.png')\n    plt.savefig(fig_path)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nmatplotlib, mpl_toolkits, numpy, random, math\n```python\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport numpy as np\nfrom random import uniform\nimport math\n\n# Constants\nNUM_POINTS = 100\nRADIUS = 10\n\ndef plot_sphere_with_random_points(onpick):\n    \"\"\"\n    Plot a sphere with random points and call a function when a point is picked.\n    \n    Parameters:\n    onpick (function): The function to be called when a point is picked.\n    \n    Returns:\n    int: The number of points plotted.\n    \n    Requirements:\n    - matplotlib\n    - mpl_toolkits\n    - numpy\n    - random\n    - math\n    \n    Example:\n    >>> def onpick(event):\n    ...     ind = event.ind\n    ...     print(f'You picked data point(s) {ind}')\n    >>> num_points = plot_sphere_with_random_points(onpick)\n    >>> print(f'Plotted {num_points} points')\n    \"\"\"\n    fig = plt.figure()\n    ax = Axes3D(fig)\n\n    phi = np.linspace(0, np.pi, NUM_POINTS)\n    theta = np.linspace(0, 2 * np.pi, NUM_POINTS)\n    x = RADIUS * np.outer(np.sin(phi), np.sin(theta))\n    y = RADIUS * np.outer(np.sin(phi), np.cos(theta))\n    z = RADIUS * np.outer(np.cos(phi), np.ones_like(theta))\n\n    ax.plot_surface(x, y, z, color='b', alpha=0.1)\n\n    points = []\n    for _ in range(NUM_POINTS):\n        theta = uniform(0, 2 * math.pi)\n        phi = uniform(0, math.pi)\n        points.append([\n            RADIUS * math.sin(phi) * math.cos(theta),\n            RADIUS * math.sin(phi) * math.sin(theta),\n            RADIUS * math.cos(phi)\n        ])\n\n    points = np.array(points)\n    scatter = ax.scatter(points[:, 0], points[:, 1], points[:, 2], picker=True)\n    fig.canvas.mpl_connect('pick_event', onpick)\n\n    plt.show()\n\n    return len(points)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nmatplotlib, mpl_toolkits, numpy, cv2, os\n```python\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport numpy as np\nimport cv2\nimport os\n\n# Constants\nDATA_DIR = 'data'\nFILE_NAME = 'image.jpg'\n\ndef plot_image_histogram(file_path, onpick):\n    \"\"\"\n    Plot the color histogram of an image in 3D and call a function when a data point is picked.\n    \n    Parameters:\n    file_path (str): The path to the image file.\n    onpick (function): The function to be called when a data point is picked.\n    \n    Requirements:\n    - matplotlib\n    - mpl_toolkits\n    - numpy\n    - cv2\n    - os\n    \n    Example:\n    >>> def onpick(event):\n    ...     ind = event.ind\n    ...     print(f'You picked data point(s) {ind}')\n    >>> plot_image_histogram('data/image.jpg', onpick)\n    \"\"\"\n    img = cv2.imread(file_path)\n    color = ('b', 'g', 'r')\n    fig = plt.figure()\n    ax = Axes3D(fig)\n\n    for i, col in enumerate(color):\n        hist = cv2.calcHist([img], [i], None, [256], [0, 256])\n        ax.plot(hist, color=col)\n\n    fig.canvas.mpl_connect('pick_event', onpick)\n\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nmatplotlib, mpl_toolkits, numpy, sklearn, random\n```python\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport numpy as np\nfrom sklearn.datasets import make_blobs\nfrom random import randint\n\n# Constants\nNUM_SAMPLES = 100\nNUM_FEATURES = 3\nNUM_CLUSTERS = 4\n\ndef plot_3d_blobs(onpick):\n    \"\"\"\n    Generate a 3D scatter plot of blobs and call a function when a data point is picked.\n    \n    Parameters:\n    onpick (function): The function to be called when a data point is picked.\n    \n    Returns:\n    int: The number of blobs plotted.\n    \n    Requirements:\n    - matplotlib\n    - mpl_toolkits\n    - numpy\n    - sklearn.datasets\n    - random\n    \n    Example:\n    >>> def onpick(event):\n    ...     ind = event.ind\n    ...     print(f'You picked data point(s) {ind}')\n    >>> num_blobs = plot_3d_blobs(onpick)\n    >>> print(f'Plotted {num_blobs} blobs')\n    \"\"\"\n    fig = plt.figure()\n    ax = Axes3D(fig)\n\n    data, _ = make_blobs(n_samples=NUM_SAMPLES, n_features=NUM_FEATURES, centers=NUM_CLUSTERS)\n    scatter = ax.scatter(data[:, 0], data[:, 1], data[:, 2], c=[randint(0, NUM_CLUSTERS-1) for _ in range(NUM_SAMPLES)], picker=True)\n    fig.canvas.mpl_connect('pick_event', onpick)\n\n    plt.show()\n\n    return NUM_CLUSTERS\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nmatplotlib, mpl_toolkits, numpy, random, math\n```python\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport numpy as np\nfrom random import uniform\nimport math\n\n# Constants\nNUM_POINTS = 1000\nRADIUS = 10\n\ndef plot_random_spiral(onpick):\n    \"\"\"\n    Plot a spiral with random points in 3D and call a function when a point is picked.\n    \n    Parameters:\n    onpick (function): The function to be called when a point is picked.\n    \n    Returns:\n    int: The number of points plotted.\n    \n    Requirements:\n    - matplotlib\n    - mpl_toolkits\n    - numpy\n    - random\n    - math\n    \n    Example:\n    >>> def onpick(event):\n    ...     ind = event.ind\n    ...     print(f'You picked data point(s) {ind}')\n    >>> num_points = plot_random_spiral(onpick)\n    >>> print(f'Plotted {num_points} points')\n    \"\"\"\n    fig = plt.figure()\n    ax = Axes3D(fig)\n\n    points = []\n    for i in range(NUM_POINTS):\n        t = i / 100.0\n        x = (RADIUS + t) * math.cos(t)\n        y = (RADIUS + t) * math.sin(t)\n        z = t\n        points.append([x, y, z])\n\n    points = np.array(points)\n    scatter = ax.scatter(points[:, 0], points[:, 1], points[:, 2], picker=True)\n    fig.canvas.mpl_connect('pick_event', onpick)\n\n    plt.show()\n\n    return len(points)\n```\n"}
{"seed": "{\"task_id\": 42344, \"prompt\": \"def f_42344():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"re.compile('[\\u3041-\\u3093\\u30a1-\\u30f3\\u4e00-\\u9fa5]+')\", \"test_start\": \"\\nimport re\\n\\ndef check(candidate):\", \"test\": [\"\\n    pattern = candidate()\\n    words = ['\\u3042\\u3044\\u3046\\u3048\\u304a', '546', '\\u305f\\u306c\\u304d', '\\u9945\\u982d', 'abdf', '#%&', '    ']\\n    ja_words = [pattern.findall(w) for w in words]\\n    ja_words = [a for jw in ja_words for a in jw]\\n    assert ja_words == ['\\u3042\\u3044\\u3046\\u3048\\u304a', '\\u305f\\u306c\\u304d', '\\u9945\\u982d']\\n\"], \"entry_point\": \"f_42344\", \"intent\": \"\\u65e5\\u672c\\u8a9e\\uff08\\u3072\\u3089\\u304c\\u306a\\u3001\\u30ab\\u30bf\\u30ab\\u30ca\\u3001\\u6f22\\u5b57\\uff09\\u306e\\u5224\\u5225\\u3092\\u3059\\u308b\\u6b63\\u898f\\u8868\\u73fe\\u3092\\u5f97\\u308b\", \"library\": [\"re\"]}", "generation": "Scenario 3:\nre, random, string, pandas\n```python\nimport re\nimport random\nimport string\nimport pandas as pd\n\n# Constants\nPUNCTUATIONS = string.punctuation + ' '\nJAPANESE_REGEX = re.compile('[\\u3041-\\u3093\\u30a1-\\u30f3\\u4e00-\\u9fa5]+')\n\ndef generate_japanese_text_analysis(size=10):\n    \"\"\"\n    Generate a DataFrame containing random Japanese and non-Japanese words, \n    and a subsequent analysis of the count of Japanese and non-Japanese words.\n\n    Parameters:\n    size (int): The size of the DataFrame to be generated.\n\n    Returns:\n    DataFrame: A pandas DataFrame containing the text and their analysis.\n\n    Requirements:\n    - re\n    - random\n    - string\n    - pandas\n\n    Example:\n    >>> df = generate_japanese_text_analysis()\n    >>> print(df)\n    \"\"\"\n    data = []\n    for _ in range(size):\n        word = ''.join(random.choices(string.ascii_lowercase + string.digits + PUNCTUATIONS + \n                                      string.ascii_uppercase + '\\u3041-\\u3093\\u30a1-\\u30f3\\u4e00-\\u9fa5', k=random.randint(4, 10)))\n        japanese_words = JAPANESE_REGEX.findall(word)\n        non_japanese_words = re.sub(JAPANESE_REGEX, '', word)\n        data.append([word, len(japanese_words), len(non_japanese_words)])\n\n    df = pd.DataFrame(data, columns=['Text', 'Japanese Word Count', 'Non-Japanese Word Count'])\n\n    return df\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 4:\nre, pandas, os, glob\n```python\nimport re\nimport pandas as pd\nimport os\nimport glob\n\n# Constants\nJAPANESE_REGEX = re.compile('[\\u3041-\\u3093\\u30a1-\\u30f3\\u4e00-\\u9fa5]+')\n\ndef analyze_text_files(directory):\n    \"\"\"\n    Analyze text files in a given directory to count the number of Japanese and non-Japanese words.\n\n    Parameters:\n    directory (str): The directory containing the text files.\n\n    Returns:\n    DataFrame: A pandas DataFrame containing the analysis.\n\n    Requirements:\n    - re\n    - pandas\n    - os\n    - glob\n\n    Example:\n    >>> df = analyze_text_files('./text_files/')\n    >>> print(df)\n    \"\"\"\n    data = []\n    for file in glob.glob(os.path.join(directory, '*.txt')):\n        with open(file, 'r', encoding='utf8') as f:\n            text = f.read()\n            japanese_words = JAPANESE_REGEX.findall(text)\n            non_japanese_words = re.sub(JAPANESE_REGEX, '', text)\n            data.append([os.path.basename(file), len(japanese_words), len(non_japanese_words)])\n\n    df = pd.DataFrame(data, columns=['File Name', 'Japanese Word Count', 'Non-Japanese Word Count'])\n\n    return df\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 5:\nre, matplotlib, collections\n```python\nimport re\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n\n# Constants\nJAPANESE_REGEX = re.compile('[\\u3041-\\u3093\\u30a1-\\u30f3\\u4e00-\\u9fa5]+')\n\ndef plot_japanese_word_distribution(text):\n    \"\"\"\n    Plot the distribution of Japanese words in a given text.\n\n    Parameters:\n    text (str): The text to be analyzed.\n\n    Requirements:\n    - re\n    - matplotlib.pyplot\n    - collections\n\n    Example:\n    >>> plot_japanese_word_distribution('\u3053\u3093\u306b\u3061\u306f\u3001\u4e16\u754c\uff01')\n    \"\"\"\n    japanese_words = JAPANESE_REGEX.findall(text)\n    word_counts = Counter(japanese_words)\n\n    plt.bar(word_counts.keys(), word_counts.values())\n    plt.show()\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 6:\nre, nltk, matplotlib\n```python\nimport re\nimport nltk\nimport matplotlib.pyplot as plt\n\n# Constants\nJAPANESE_REGEX = re.compile('[\\u3041-\\u3093\\u30a1-\\u30f3\\u4e00-\\u9fa5]+')\n\ndef analyze_sentence_structure(text):\n    \"\"\"\n    Analyze the sentence structure of a text containing Japanese and non-Japanese words.\n\n    Parameters:\n    text (str): The text to be analyzed.\n\n    Returns:\n    list: A list of tuples containing the word and its type (Japanese or Non-Japanese).\n\n    Requirements:\n    - re\n    - nltk\n    - matplotlib.pyplot\n\n    Example:\n    >>> analyze_sentence_structure('\u3053\u3093\u306b\u3061\u306f\u3001\u4e16\u754c\uff01')\n    \"\"\"\n    words = nltk.word_tokenize(text)\n    structure = []\n    for word in words:\n        if JAPANESE_REGEX.match(word):\n            structure.append((word, 'Japanese'))\n        else:\n            structure.append((word, 'Non-Japanese'))\n\n    return structure\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 7:\nre, pandas, sklearn\n```python\nimport re\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Constants\nJAPANESE_REGEX = re.compile('[\\u3041-\\u3093\\u30a1-\\u30f3\\u4e00-\\u9fa5]+')\n\ndef create_japanese_word_features(texts):\n    \"\"\"\n    Create a feature matrix of Japanese word counts for a given list of texts.\n\n    Parameters:\n    texts (list): The list of texts to be analyzed.\n\n    Returns:\n    DataFrame: A pandas DataFrame containing the feature matrix.\n\n    Requirements:\n    - re\n    - pandas\n    - sklearn.feature_extraction.text.CountVectorizer\n\n    Example:\n    >>> texts = ['\u3053\u3093\u306b\u3061\u306f\u3001\u4e16\u754c\uff01', '\u4eca\u65e5\u306f\u3044\u3044\u5929\u6c17\u3067\u3059\u306d\u3002']\n    >>> features = create_japanese_word_features(texts)\n    >>> print(features)\n    \"\"\"\n    japanese_texts = [' '.join(JAPANESE_REGEX.findall(text)) for text in texts]\n    vectorizer = CountVectorizer(token_pattern=JAPANESE_REGEX)\n    features = vectorizer.fit_transform(japanese_texts)\n\n    return pd.DataFrame(features.toarray(), columns=vectorizer.get_feature_names_out())\n```"}
{"seed": "{\"task_id\": 17145, \"prompt\": \"def f_17145(br):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"br.submit().read()\", \"test_start\": \"\\nimport mechanize\\nimport urllib.request\\nfrom unittest.mock import Mock\\n\\ndef check(candidate):\", \"test\": [\"\\n    br = mechanize.Browser()\\n    x = urllib.request.urlopen('https://www.wikipedia.org')\\n    br.submit = Mock(return_value = x)\\n    assert b'Wikipedia' in candidate(br)\\n\"], \"entry_point\": \"f_17145\", \"intent\": \"\\u30d6\\u30e9\\u30a6\\u30b6\\u30aa\\u30d6\\u30b8\\u30a7\\u30af\\u30c8`br`\\u304b\\u3089submit\\u3057\\u305f\\u969b\\u306e\\u8fd4\\u308a\\u5024\\u3092\\u8aad\\u307f\\u3053\\u3080\", \"library\": [\"mechanize\", \"urllib\"]}", "generation": "Scenario 1: \nmechanize, urllib, bs4\n```python\nimport mechanize\nimport urllib.request\nfrom bs4 import BeautifulSoup\n\ndef extract_links(br, url):\n    \"\"\"\n    Extracts all the hyperlinks from the given url using the 'mechanize' browser object.\n\n    Parameters:\n    br (mechanize.Browser): The browser object.\n    url (str): The url from which hyperlinks are to be extracted.\n\n    Returns:\n    list: A list of hyperlinks.\n\n    Requirements:\n    - mechanize\n    - urllib.request\n    - bs4\n\n    Example:\n    >>> br = mechanize.Browser()\n    >>> links = extract_links(br, 'https://www.wikipedia.org')\n    >>> print(links)\n    \"\"\"\n    response = br.open(url)\n    soup = BeautifulSoup(response.read(), 'html.parser')\n    \n    links = [a['href'] for a in soup.find_all('a', href=True)]\n    \n    return links\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nmechanize, urllib, os, io, PIL\n```python\nimport mechanize\nimport urllib.request\nimport os\nimport io\nfrom PIL import Image\n\ndef save_image(br, url, file_path):\n    \"\"\"\n    Downloads and saves an image from the given url using the 'mechanize' browser object.\n\n    Parameters:\n    br (mechanize.Browser): The browser object.\n    url (str): The url of the image.\n    file_path (str): The path where the image should be saved.\n\n    Returns:\n    str: The path of the saved image.\n\n    Requirements:\n    - mechanize\n    - urllib.request\n    - os\n    - io\n    - PIL\n\n    Example:\n    >>> br = mechanize.Browser()\n    >>> save_image(br, 'https://www.wikipedia.org/static/images/project-logos/enwiki-2x.png', './wiki.png')\n    \"\"\"\n    response = br.open(url)\n    image = Image.open(io.BytesIO(response.read()))\n    image.save(file_path)\n    \n    return os.path.abspath(file_path)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nmechanize, urllib, re\n```python\nimport mechanize\nimport urllib.request\nimport re\n\ndef find_emails(br, url):\n    \"\"\"\n    Finds all the email addresses present on a webpage given its url using the 'mechanize' browser object.\n\n    Parameters:\n    br (mechanize.Browser): The browser object.\n    url (str): The url of the webpage.\n\n    Returns:\n    list: A list of email addresses.\n\n    Requirements:\n    - mechanize\n    - urllib.request\n    - re\n\n    Example:\n    >>> br = mechanize.Browser()\n    >>> emails = find_emails(br, 'https://www.example.com')\n    >>> print(emails)\n    \"\"\"\n    response = br.open(url)\n    content = response.read().decode()\n    \n    emails = re.findall(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', content)\n    \n    return emails\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nmechanize, urllib, lxml\n```python\nimport mechanize\nimport urllib.request\nfrom lxml import html\n\ndef extract_table_data(br, url, xpath):\n    \"\"\"\n    Extracts the data of a table from a webpage given its url and the xpath of the table using the 'mechanize' browser object.\n\n    Parameters:\n    br (mechanize.Browser): The browser object.\n    url (str): The url of the webpage.\n    xpath (str): The xpath of the table.\n\n    Returns:\n    list: A list of lists representing the rows and columns of the table.\n\n    Requirements:\n    - mechanize\n    - urllib.request\n    - lxml\n\n    Example:\n    >>> br = mechanize.Browser()\n    >>> table_data = extract_table_data(br, 'https://www.example.com', '//table')\n    >>> print(table_data)\n    \"\"\"\n    response = br.open(url)\n    tree = html.fromstring(response.read())\n    rows = tree.xpath(xpath + '/tr')\n\n    table_data = []\n    for row in rows:\n        cols = row.xpath('./td//text()')\n        table_data.append([col.strip() for col in cols])\n\n    return table_data\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nmechanize, urllib, requests, bs4\n```python\nimport mechanize\nimport urllib.request\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef submit_form_and_extract_title(br, url, form_id, data):\n    \"\"\"\n    Submits a form on a webpage with given data and extracts the title of the resulting page using the 'mechanize' browser object.\n\n    Parameters:\n    br (mechanize.Browser): The browser object.\n    url (str): The url of the webpage.\n    form_id (int): The id of the form.\n    data (dict): The data to be submitted in the form.\n\n    Returns:\n    str: The title of the resulting page.\n\n    Requirements:\n    - mechanize\n    - urllib.request\n    - requests\n    - bs4\n\n    Example:\n    >>> br = mechanize.Browser()\n    >>> data = {'username': 'admin', 'password': 'password'}\n    >>> title = submit_form_and_extract_title(br, 'https://www.example.com/login', 0, data)\n    >>> print(title)\n    \"\"\"\n    br.open(url)\n    br.select_form(nr=form_id)\n    \n    for key in data:\n        br[key] = data[key]\n    \n    response = br.submit()\n    \n    soup = BeautifulSoup(response.read(), 'html.parser')\n    title = soup.title.string\n    \n    return title\n```"}
{"seed": "{\"task_id\": 38824, \"prompt\": \"def f_38824(data):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"[print(*i) for i in data]\", \"test_start\": \"\\nimport sys\\nfrom io import StringIO\\n\\ndef check(candidate):\", \"test\": [\"\\n    stdout = sys.stdout\\n    s = StringIO()\\n    sys.stdout = s\\n    candidate([[1],[2],[3],[4],[5],[6]])\\n    sys.stdout = stdout    \\n    s.seek(0)\\n    assert len(s.read()) == 12\\n\"], \"entry_point\": \"f_38824\", \"intent\": \"\\u30bf\\u30d7\\u30eb`data`\\u3092\\u7a7a\\u767d\\u533a\\u5207\\u308a\\u3067\\u8868\\u793a\\u3059\\u308b\", \"library\": [\"io\", \"sys\"]}", "generation": "Scenario 3:\npandas, numpy, matplotlib, datetime, random\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom datetime import datetime, timedelta\nfrom random import choice\n\n# Constants\nPRODUCTS = ['Apple', 'Banana', 'Cherry', 'Date', 'Elderberry']\nSTART_DATE = datetime(2020, 1, 1)\nDAYS = 365\n\n# Sample Data\n# Generating random product sales data for the last year\ndata = [[(START_DATE + timedelta(days=i)).strftime('%Y-%m-%d'), choice(PRODUCTS), np.random.randint(1, 100)] for i in range(DAYS)]\n\ndef plot_sales(data):\n    \"\"\"\n    Plot a bar graph showing total sales of each product in the given data.\n    The data is a list of tuples where each tuple has three elements - date, product, and sales.\n\n    Parameters:\n    data (list): The sales data.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - datetime\n    - random\n\n    Example:\n    >>> plot_sales(data)\n    \"\"\"\n    df = pd.DataFrame(data, columns=['Date', 'Product', 'Sales'])\n    df['Date'] = pd.to_datetime(df['Date'])\n    df_grouped = df.groupby('Product').sum()\n    df_grouped.plot(kind='bar', legend=False)\n    plt.ylabel('Total Sales')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ncollections, json, os, datetime\n```python\nimport collections\nimport json\nimport os\nfrom datetime import datetime\n\n# Constants\nLOG_DIR = '/var/log/myapp/'\n\ndef analyse_logs():\n    \"\"\"\n    Analyse log files in a specific directory and return a summary.\n    The summary includes total number of logs and number of logs per type.\n\n    Requirements:\n    - collections\n    - json\n    - os\n    - datetime\n\n    Example:\n    >>> analyse_logs()\n    \"\"\"\n    log_summary = collections.defaultdict(int)\n\n    for filename in os.listdir(LOG_DIR):\n        if filename.endswith('.log'):\n            with open(os.path.join(LOG_DIR, filename), 'r') as f:\n                logs = json.load(f)\n                log_summary['total'] += len(logs)\n                for log in logs:\n                    log_summary[log['type']] += 1\n\n    return dict(log_summary)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, matplotlib, sklearn, numpy\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\n\n# Constants\nDATA_PATH = '/path/to/data.csv'\n\ndef predict_sales():\n    \"\"\"\n    Predict future sales based on historical data using linear regression model.\n    The data is loaded from a CSV file with columns 'Date' and 'Sales'.\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - sklearn.model_selection\n    - sklearn.linear_model\n    - numpy\n\n    Example:\n    >>> predict_sales()\n    \"\"\"\n    # Load data\n    df = pd.read_csv(DATA_PATH)\n    df['Date'] = pd.to_datetime(df['Date'])\n    df['Date'] = df['Date'].map(datetime.toordinal)\n\n    # Split data into training and test sets\n    X_train, X_test, y_train, y_test = train_test_split(df[['Date']], df['Sales'], test_size=0.2, random_state=42)\n\n    # Train the model\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    # Predict sales\n    y_pred = model.predict(X_test)\n\n    # Plot actual vs predicted sales\n    plt.scatter(X_test, y_test, color='blue', label='Actual')\n    plt.plot(X_test, y_pred, color='red', label='Predicted')\n    plt.xlabel('Date')\n    plt.ylabel('Sales')\n    plt.legend()\n    plt.show()\n\n    return np.round(y_pred, 2)\n```\n`GPT_ODEX_BREAK`\n\nScenario 6:\nrequests, bs4, os, re\n```python\nimport requests\nfrom bs4 import BeautifulSoup\nimport os\nimport re\n\n# Constants\nURL = 'https://example.com'\n\ndef download_images():\n    \"\"\"\n    Download all images from a webpage and save them into a local directory.\n\n    Requirements:\n    - requests\n    - bs4.BeautifulSoup\n    - os\n    - re\n\n    Example:\n    >>> download_images()\n    \"\"\"\n    # Make a request\n    response = requests.get(URL)\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    # Find all image tags\n    img_tags = soup.find_all('img')\n\n    # Extract URLs of images\n    img_urls = [img['src'] for img in img_tags]\n\n    # Create a directory for images\n    if not os.path.exists('images'):\n        os.makedirs('images')\n\n    # Download images\n    for url in img_urls:\n        response = requests.get(url)\n        filename = re.search(r'/([\\w_-]+[.](jpg|gif|png))$', url)\n        if filename is not None:\n            with open('images/' + filename.group(1), 'wb') as f:\n                f.write(response.content)\n\n    return len(img_urls)\n```\n`GPT_ODEX_BREAK`\n\nScenario 7:\npandas, os, zipfile\n```python\nimport pandas as pd\nimport os\nimport zipfile\n\n# Constants\nZIP_FILE = '/path/to/data.zip'\nEXTRACT_DIR = '/path/to/extract/'\n\ndef process_zip():\n    \"\"\"\n    Extract a zip file, load CSV files into pandas DataFrames, \n    and return a dictionary where keys are file names and values are DataFrames.\n\n    Requirements:\n    - pandas\n    - os\n    - zipfile\n\n    Example:\n    >>> dfs = process_zip()\n    >>> print(dfs.keys())\n    \"\"\"\n    with zipfile.ZipFile(ZIP_FILE, 'r') as zip_ref:\n        zip_ref.extractall(EXTRACT_DIR)\n\n    dfs = {}\n\n    for filename in os.listdir(EXTRACT_DIR):\n        if filename.endswith('.csv'):\n            df = pd.read_csv(os.path.join(EXTRACT_DIR, filename))\n            dfs[filename] = df\n\n    return dfs\n```"}
{"seed": "{\"task_id\": 38824, \"prompt\": \"def f_38824(data):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"for i in data:\\n\\t     print(' '.join(str(j) for j in i))\", \"test_start\": \"\\nimport sys\\nfrom io import StringIO\\n\\ndef check(candidate):\", \"test\": [\"\\n    stdout = sys.stdout\\n    s = StringIO()\\n    sys.stdout = s\\n    candidate([[1],[2],[3],[4],[5],[6]])\\n    sys.stdout = stdout    \\n    s.seek(0)\\n    assert len(s.read()) == 12\\n\"], \"entry_point\": \"f_38824\", \"intent\": \"\\u30bf\\u30d7\\u30eb`data`\\u3092\\u7a7a\\u767d\\u533a\\u5207\\u308a\\u3067\\u8868\\u793a\\u3059\\u308b\", \"library\": [\"io\", \"sys\"]}", "generation": "Scenario 1:\nnumpy, matplotlib.pyplot, math, random\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\nimport random\n\n# Constants\nMU, SIGMA = 0, 0.1 # mean and standard deviation\nNUM_SAMPLES = 1000\n\ndef generate_normal_distribution():\n    \"\"\"\n    Generate a normal distribution with a given mean and standard deviation and \n    plot the histogram of the generated data and a curve of the normal distribution.\n    \n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - math\n    - random\n\n    Example:\n    >>> generate_normal_distribution()\n    \"\"\"\n    s = np.random.normal(MU, SIGMA, NUM_SAMPLES)\n    count, bins, ignored = plt.hist(s, 30, density=True)\n    plt.plot(bins, 1/(SIGMA * np.sqrt(2 * np.pi)) * np.exp( - (bins - MU)**2 / (2 * SIGMA**2) ), linewidth=2, color='r')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, matplotlib.pyplot, random\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\n\n# Constants\nNUM_SAMPLES = 10000\nBIN_COUNT = 50\n\ndef plot_random_walk():\n    \"\"\"\n    Generate a random walk and plot the path of the walk.\n    \n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> plot_random_walk()\n    \"\"\"\n    steps = np.random.choice([-1, 1], size=NUM_SAMPLES)\n    walk = np.cumsum(steps)\n    plt.plot(walk)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, os, glob, re\n```python\nimport pandas as pd\nimport os\nimport glob\nimport re\n\n# Constants\nDATA_PATH = './data/*.csv'\n\ndef merge_csv_files():\n    \"\"\"\n    Merge all the csv files in a directory into a single DataFrame.\n\n    Requirements:\n    - pandas\n    - os\n    - glob\n    - re\n\n    Example:\n    >>> df = merge_csv_files()\n    >>> print(df.head())\n    \"\"\"\n    all_files = glob.glob(DATA_PATH)\n    all_df = []\n\n    for file in all_files:\n        df = pd.read_csv(file, sep=',')\n        all_df.append(df)\n      \n    merged_df = pd.concat(all_df, ignore_index=True)\n\n    return merged_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nrequests, BeautifulSoup, re\n```python\nimport requests\nfrom bs4 import BeautifulSoup\nimport re\n\n# Constants\nURL = 'https://www.python.org/'\n\ndef find_internal_links():\n    \"\"\"\n    Finds all the internal links (i.e., links to the python.org) in the HTML of the python.org homepage.\n\n    Requirements:\n    - requests\n    - BeautifulSoup\n    - re\n\n    Example:\n    >>> links = find_internal_links()\n    >>> print(links)\n    \"\"\"\n    page = requests.get(URL)\n    soup = BeautifulSoup(page.content, 'html.parser')\n\n    internal_links = []\n\n    for link in soup.find_all('a'):\n        url = link.get('href')\n        if url is not None and re.search('^(/.*|https://www\\.python\\.org/.*)$', url):\n            internal_links.append(url)\n\n    return internal_links\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, matplotlib.pyplot\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nNUM_SAMPLES = 1000\nBIN_COUNT = 50\n\ndef generate_and_plot_histogram():\n    \"\"\"\n    Generate a series of random numbers and plot a histogram of the data.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> generate_and_plot_histogram()\n    \"\"\"\n    data = pd.Series(np.random.randn(NUM_SAMPLES))\n    data.hist(bins=BIN_COUNT)\n    plt.show()\n```"}
{"seed": "{\"task_id\": 38824, \"prompt\": \"def f_38824(data):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"for i in data:\\n\\t     print(' '.join(map(str, i)))\", \"test_start\": \"\\nimport sys\\nfrom io import StringIO\\n\\ndef check(candidate):\", \"test\": [\"\\n    stdout = sys.stdout\\n    s = StringIO()\\n    sys.stdout = s\\n    candidate([[1],[2],[3],[4],[5],[6]])\\n    sys.stdout = stdout    \\n    s.seek(0)\\n    assert len(s.read()) == 12\\n\"], \"entry_point\": \"f_38824\", \"intent\": \"\\u30bf\\u30d7\\u30eb`data`\\u3092\\u7a7a\\u767d\\u533a\\u5207\\u308a\\u3067\\u8868\\u793a\\u3059\\u308b\", \"library\": [\"io\", \"sys\"]}", "generation": "Scenario 1:\npandas, numpy, matplotlib, random\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nCATEGORIES = ['A', 'B', 'C', 'D', 'E']\nNUMBERS = list(range(1, 101))\n\ndef generate_and_plot_random_data(data):\n    \"\"\"\n    Generate a DataFrame with random category and number data based on a list of tuples. \n    Then plot a histogram of the number distribution for each category.\n    \n    Parameters:\n    data (list): A list of tuples.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the generated random data.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - random\n    \n    Example:\n    >>> data = [(1, 2), (3, 4), (5, 6)]\n    >>> df = generate_and_plot_random_data(data)\n    >>> print(df)\n    >>> df['Number'].hist(by=df['Category'])\n    \"\"\"\n    random_data = []\n\n    for tup in data:\n        category = CATEGORIES[randint(0, len(CATEGORIES)-1)]\n        number = NUMBERS[randint(0, len(NUMBERS)-1)]\n        random_data.append((category, number))\n\n    df = pd.DataFrame(random_data, columns=['Category', 'Number'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, itertools, matplotlib\n```python\nimport numpy as np\nfrom itertools import combinations\nimport matplotlib.pyplot as plt\n\n# Constants\nPAIRS_NUM = 2\n\ndef plot_combinations(data):\n    \"\"\"\n    Generate all possible combinations of a certain size from a list of tuples and plot them.\n    \n    Parameters:\n    data (list): A list of tuples.\n    \n    Returns:\n    list: A list of all combinations.\n    \n    Requirements:\n    - numpy\n    - itertools\n    - matplotlib.pyplot\n    \n    Example:\n    >>> data = [(1, 2), (3, 4), (5, 6)]\n    >>> combinations = plot_combinations(data)\n    >>> print(combinations)\n    >>> plt.plot(combinations)\n    \"\"\"\n    combinations_list = list(combinations(data, PAIRS_NUM))\n\n    for c in combinations_list:\n        plt.figure()\n        plt.plot(c)\n\n    return combinations_list\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, sklearn\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\n# Constants\nTEST_SIZE = 0.2\nRANDOM_STATE = 42\n\ndef split_data(data):\n    \"\"\"\n    Split a list of tuples into training set and test set.\n    \n    Parameters:\n    data (list): A list of tuples.\n    \n    Returns:\n    list: Training set and test set.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.model_selection\n    \n    Example:\n    >>> data = [(1, 2), (3, 4), (5, 6)]\n    >>> train_set, test_set = split_data(data)\n    >>> print(train_set)\n    >>> print(test_set)\n    \"\"\"\n    df = pd.DataFrame(data, columns=['Feature', 'Target'])\n    X = df['Feature']\n    y = df['Target']\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n\n    return X_train.tolist(), X_test.tolist(), y_train.tolist(), y_test.tolist()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, matplotlib, random\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nNUMBERS = list(range(1, 101))\n\ndef plot_random_numbers(data):\n    \"\"\"\n    Generate a list of random numbers based on a list of tuples and plot them.\n    \n    Parameters:\n    data (list): A list of tuples.\n    \n    Returns:\n    list: A list of generated random numbers.\n    \n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - random\n    \n    Example:\n    >>> data = [(1, 2), (3, 4), (5, 6)]\n    >>> random_numbers = plot_random_numbers(data)\n    >>> print(random_numbers)\n    >>> plt.plot(random_numbers)\n    \"\"\"\n    random_numbers = []\n\n    for tup in data:\n        number = NUMBERS[randint(0, len(NUMBERS)-1)]\n        random_numbers.append(number)\n\n    plt.plot(random_numbers)\n\n    return random_numbers\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, sklearn.preprocessing\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\n\ndef normalize_data(data):\n    \"\"\"\n    Normalize a list of tuples using StandardScaler.\n    \n    Parameters:\n    data (list): A list of tuples.\n    \n    Returns:\n    numpy.ndarray: The normalized data.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.preprocessing\n    \n    Example:\n    >>> data = [(1, 2), (3, 4), (5, 6)]\n    >>> normalized_data = normalize_data(data)\n    >>> print(normalized_data)\n    \"\"\"\n    df = pd.DataFrame(data, columns=['Feature1', 'Feature2'])\n    scaler = StandardScaler()\n    normalized_data = scaler.fit_transform(df)\n\n    return normalized_data\n```"}
{"seed": "{\"task_id\": 35299, \"prompt\": \"def f_35299(n):\\n\\t\", \"suffix\": \"\\n\\treturn result\", \"canonical_solution\": \"\\n\\tt = 5\\n\\tz = tf.constant(0, shape=[n, n], dtype=tf.int32)\\n\\tabs = tf.maximum(t, z)\\n\\tresult = tf.reduce_sum(abs)\\n\", \"test_start\": \"\\nimport tensorflow as tf\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert str(type(candidate(4))).split(' ')[1] == \\\"'tensorflow.python.framework.ops.EagerTensor'>\\\"\\n\"], \"entry_point\": \"f_35299\", \"intent\": \"`n`\\u00d7`n`\\u306e\\u30c6\\u30f3\\u30bd\\u30eb\\u306e\\u8981\\u7d20\\u306e\\u3046\\u30610\\u4ee5\\u4e0a\\u306e\\u5024\\u306e\\u548c\\u3092\\u8a08\\u7b97\\u3059\\u308b\", \"library\": [\"tensorflow\"]}", "generation": "Scenario 1:\ntensorflow, numpy, matplotlib, sklearn.preprocessing\n```python\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Constants\nSCALE_RANGE = (0, 1)\n\ndef generate_normalized_tensor(n, plot=False):\n    \"\"\"\n    Generate a tensor of shape (n, n) with random values between 0 and 1, \n    normalize the tensor using MinMaxScaler and plot the tensor values if plot is True.\n\n    Parameters:\n    n (int): The shape of the tensor.\n    plot (bool): Whether to plot the tensor values.\n\n    Returns:\n    tf.Tensor: The normalized tensor.\n\n    Requirements:\n    - tensorflow\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.preprocessing.MinMaxScaler\n\n    Example:\n    >>> tensor = generate_normalized_tensor(4, True)\n    >>> print(tensor)\n    \"\"\"\n    tensor = tf.random.uniform(shape=(n, n), minval=0, maxval=1)\n    tensor_numpy = tensor.numpy()\n\n    scaler = MinMaxScaler(feature_range=SCALE_RANGE)\n    tensor_numpy = scaler.fit_transform(tensor_numpy.reshape(-1, 1)).reshape(n, n)\n\n    normalized_tensor = tf.convert_to_tensor(tensor_numpy)\n\n    if plot:\n        plt.imshow(normalized_tensor, cmap='hot', interpolation='nearest')\n        plt.show()\n\n    return normalized_tensor\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, tensorflow, sklearn.model_selection, sklearn.metrics\n```python\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Constants\nTEST_SIZE = 0.2\nRANDOM_STATE = 42\nEPOCHS = 10\nBATCH_SIZE = 32\n\n# Sample data\n# df = pd.read_csv('sample_data.csv')\n\ndef train_tf_model(df, target_col):\n    \"\"\"\n    Train a simple neural network using TensorFlow on a pandas DataFrame.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n    target_col (str): The target column.\n\n    Returns:\n    float: The accuracy of the model.\n\n    Requirements:\n    - pandas\n    - numpy\n    - tensorflow\n    - sklearn.model_selection.train_test_split\n    - sklearn.metrics.accuracy_score\n\n    Example:\n    >>> df = pd.read_csv('sample_data.csv')\n    >>> accuracy = train_tf_model(df, 'target')\n    >>> print(accuracy)\n    \"\"\"\n    X = df.drop(target_col, axis=1).values\n    y = df[target_col].values\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n\n    model = tf.keras.Sequential([\n        tf.keras.layers.Dense(64, activation='relu'),\n        tf.keras.layers.Dense(1, activation='sigmoid')\n    ])\n\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n    model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=0)\n\n    y_pred = model.predict(X_test).round()\n\n    return accuracy_score(y_test, y_pred)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ntensorflow, numpy, matplotlib, sklearn.preprocessing\n```python\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\n# Constants\nN_FEATURES = 3\n\ndef generate_standardized_tensor(n, plot=False):\n    \"\"\"\n    Generate a tensor of shape (n, N_FEATURES) with random values, \n    standardize the tensor using StandardScaler and plot the tensor values if plot is True.\n\n    Parameters:\n    n (int): The number of samples.\n    plot (bool): Whether to plot the tensor values.\n\n    Returns:\n    tf.Tensor: The standardized tensor.\n\n    Requirements:\n    - tensorflow\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.preprocessing.StandardScaler\n\n    Example:\n    >>> tensor = generate_standardized_tensor(100, True)\n    >>> print(tensor)\n    \"\"\"\n    tensor = tf.random.uniform(shape=(n, N_FEATURES), minval=-10, maxval=10)\n    tensor_numpy = tensor.numpy()\n\n    scaler = StandardScaler()\n    tensor_numpy = scaler.fit_transform(tensor_numpy)\n\n    standardized_tensor = tf.convert_to_tensor(tensor_numpy)\n\n    if plot:\n        plt.scatter(standardized_tensor[:, 0], standardized_tensor[:, 1])\n        plt.show()\n\n    return standardized_tensor\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ntensorflow, numpy, sklearn.model_selection, sklearn.preprocessing\n```python\nimport tensorflow as tf\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Constants\nN_SPLITS = 5\nBATCH_SIZE = 32\nEPOCHS = 10\n\ndef cross_validate_tf_model(X, y):\n    \"\"\"\n    Perform k-fold cross-validation on a simple neural network using TensorFlow.\n\n    Parameters:\n    X (numpy.array): The input data.\n    y (numpy.array): The target data. \n\n    Returns:\n    list: The history of the model for each fold.\n\n    Requirements:\n    - tensorflow\n    - numpy\n    - sklearn.model_selection.KFold\n    - sklearn.preprocessing.MinMaxScaler\n\n    Example:\n    >>> X = np.random.rand(100, 10)\n    >>> y = np.random.randint(0, 2, 100)\n    >>> history = cross_validate_tf_model(X, y)\n    >>> print(history)\n    \"\"\"\n    scaler = MinMaxScaler()\n    X = scaler.fit_transform(X)\n\n    kf = KFold(n_splits=N_SPLITS)\n    history = []\n\n    for train_index, test_index in kf.split(X):\n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n\n        model = tf.keras.models.Sequential([\n            tf.keras.layers.Dense(50, activation='relu'),\n            tf.keras.layers.Dense(1, activation='sigmoid')\n        ])\n\n        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n        hist = model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=0)\n        history.append(hist.history)\n\n    return history\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\ntensorflow, numpy, matplotlib.pyplot, sklearn.preprocessing\n```python\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Constants\nSCALE_RANGE = (0, 1)\nTENSOR_SHAPE = (50, 50)\n\ndef generate_and_plot_tensor():\n    \"\"\"\n    Generate a tensor of shape TENSOR_SHAPE with values between 0 and 1, and plot it as an image.\n\n    Returns:\n    tf.Tensor: The generated tensor.\n\n    Requirements:\n    - tensorflow\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.preprocessing.MinMaxScaler\n\n    Example:\n    >>> tensor = generate_and_plot_tensor()\n    >>> print(tensor)\n    \"\"\"\n    tensor = tf.random.uniform(shape=TENSOR_SHAPE, minval=-10, maxval=10)\n    tensor_numpy = tensor.numpy()\n\n    scaler = MinMaxScaler(feature_range=SCALE_RANGE)\n    tensor_numpy = scaler.fit_transform(tensor_numpy.reshape(-1, 1)).reshape(TENSOR_SHAPE)\n\n    tensor_scaled = tf.convert_to_tensor(tensor_numpy)\n\n    plt.imshow(tensor_scaled, cmap='hot', interpolation='nearest')\n    plt.show()\n\n    return tensor_scaled\n```\n"}
{"seed": "{\"task_id\": 38328, \"prompt\": \"def f_38328(df, pat):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"df.x.str.extract(pat)\", \"test_start\": \"\\nimport pandas as pd\\n\\ndef check(candidate):\", \"test\": [\"\\n    df = pd.DataFrame({'x': ['\\u8eca5(0.8km)', '5', '\\u8eca27(8.6km)']}, index=[1, 2, 3])\\n    pat = r'\\u8eca(\\\\d*)'\\n    ref = df.x.str.extract(pat)\\n    assert candidate(df, pat).count()[0] == 2\\n\"], \"entry_point\": \"f_38328\", \"intent\": \"\\u30c7\\u30fc\\u30bf\\u30d5\\u30ec\\u30fc\\u30e0`df`\\u306e\\u5217\\u30e9\\u30d9\\u30eb`x`\\u306e\\u5404\\u884c\\u306e\\u30c7\\u30fc\\u30bf\\u306b\\u5bfe\\u3057\\u3066\\u6b63\\u898f\\u8868\\u73fe`pat`\\u3092\\u9069\\u7528\\u3059\\u308b\", \"library\": [\"pandas\"]}", "generation": "Scenario 3:\npandas, numpy, matplotlib, seaborn, re\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\n\n# Constants\nPAT = r'(\\d+)'\n\ndef visualize_data_distribution(df, column):\n    \"\"\"\n    Extract numeric data from a given column of a pandas DataFrame using a regex pattern, \n    then visualize the distribution of the extracted data using a histogram.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n    column (str): The column from which to extract the data.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib\n    - seaborn\n    - re\n\n    Example:\n    >>> df = pd.DataFrame({'x': ['5(0.8km)', '5', '27(8.6km)']})\n    >>> visualize_data_distribution(df, 'x')\n    \"\"\"\n    data = df[column].apply(lambda x: re.findall(PAT, x))\n    data = data[data.apply(lambda x: len(x) > 0)].apply(lambda x: int(x[0]))\n    \n    plt.figure(figsize=(8, 6))\n    sns.histplot(data=data, bins=30, kde=True)\n    plt.title('Data Distribution')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, sklearn, numpy, matplotlib, re\n```python\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re\n\n# Constants\nPAT = r'(\\d+)'\n\ndef predict_using_linear_regression(df, column, target):\n    \"\"\"\n    Extract numeric data from a given column of a pandas DataFrame using a regex pattern, \n    then use linear regression to predict the target variable.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n    column (str): The column from which to extract the data.\n    target (str): The target variable to predict.\n\n    Returns:\n    LinearRegression: The trained linear regression model.\n\n    Requirements:\n    - pandas\n    - sklearn.model_selection\n    - sklearn.linear_model\n    - numpy\n    - matplotlib\n    - re\n\n    Example:\n    >>> df = pd.DataFrame({'x': ['5(0.8km)', '5', '27(8.6km)'], 'y': [1, 2, 3]})\n    >>> model = predict_using_linear_regression(df, 'x', 'y')\n    \"\"\"\n    data = df[column].apply(lambda x: re.findall(PAT, x))\n    data = data[data.apply(lambda x: len(x) > 0)].apply(lambda x: int(x[0]))\n\n    X = data.values.reshape(-1, 1)\n    y = df[target].values\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    y_pred = model.predict(X_test)\n    plt.scatter(X_test, y_test, color='black')\n    plt.plot(X_test, y_pred, color='blue', linewidth=3)\n    plt.xticks(())\n    plt.yticks(())\n    plt.show()\n\n    return model\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, sklearn, re\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cluster import KMeans\nimport re\n\n# Constants\nPAT = r'(\\d+)'\n\ndef cluster_data(df, column, n_clusters):\n    \"\"\"\n    Extract numeric data from a given column of a pandas DataFrame using a regex pattern, \n    then cluster the data using KMeans algorithm.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n    column (str): The column from which to extract the data.\n    n_clusters (int): The number of clusters.\n\n    Returns:\n    ndarray: The cluster labels for each data point.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.cluster\n    - re\n\n    Example:\n    >>> df = pd.DataFrame({'x': ['5(0.8km)', '5', '27(8.6km)']})\n    >>> labels = cluster_data(df, 'x', 2)\n    \"\"\"\n    data = df[column].apply(lambda x: re.findall(PAT, x))\n    data = data[data.apply(lambda x: len(x) > 0)].apply(lambda x: int(x[0]))\n\n    X = data.values.reshape(-1, 1)\n\n    kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(X)\n\n    return kmeans.labels_\n```\n`GPT_ODEX_BREAK`\n\nScenario 6:\npandas, numpy, scipy, matplotlib, re\n```python\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport re\n\n# Constants\nPAT = r'(\\d+)'\n\ndef analyze_data(df, column):\n    \"\"\"\n    Extract numeric data from a given column of a pandas DataFrame using a regex pattern, \n    then perform basic statistical analysis and plot the data.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n    column (str): The column from which to extract the data.\n\n    Returns:\n    tuple: The mean, median, mode, and standard deviation of the data.\n\n    Requirements:\n    - pandas\n    - numpy\n    - scipy\n    - matplotlib\n    - re\n\n    Example:\n    >>> df = pd.DataFrame({'x': ['5(0.8km)', '5', '27(8.6km)']})\n    >>> stats = analyze_data(df, 'x')\n    \"\"\"\n    data = df[column].apply(lambda x: re.findall(PAT, x))\n    data = data[data.apply(lambda x: len(x) > 0)].apply(lambda x: int(x[0]))\n\n    mean = np.mean(data)\n    median = np.median(data)\n    mode = stats.mode(data)[0][0]\n    std_dev = np.std(data)\n\n    plt.figure(figsize=(8, 6))\n    plt.hist(data, bins=30, alpha=0.5, color='g')\n    plt.axvline(mean, color='red', linestyle='dashed', linewidth=2)\n    plt.axvline(median, color='yellow', linestyle='dashed', linewidth=2)\n    plt.axvline(mode, color='blue', linestyle='dashed', linewidth=2)\n    plt.title('Data Analysis')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.legend({'Mean':mean,'Median':median,'Mode':mode})\n    plt.show()\n\n    return mean, median, mode, std_dev\n```\n`GPT_ODEX_BREAK`\n\nScenario 7:\npandas, numpy, sklearn, seaborn, re\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport seaborn as sns\nimport re\n\n# Constants\nPAT = r'(\\d+)'\n\ndef normalize_and_plot(df, column):\n    \"\"\"\n    Extract numeric data from a given column of a pandas DataFrame using a regex pattern, \n    normalize the data using MinMaxScaler, and then plot the distribution of the normalized data.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n    column (str): The column from which to extract the data.\n\n    Returns:\n    ndarray: The normalized data.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.preprocessing\n    - seaborn\n    - re\n\n    Example:\n    >>> df = pd.DataFrame({'x': ['5(0.8km)', '5', '27(8.6km)']})\n    >>> normalized_data = normalize_and_plot(df, 'x')\n    \"\"\"\n    data = df[column].apply(lambda x: re.findall(PAT, x))\n    data = data[data.apply(lambda x: len(x) > 0)].apply(lambda x: int(x[0]))\n\n    X = data.values.reshape(-1, 1)\n    scaler = MinMaxScaler()\n    X_scaled = scaler.fit_transform(X)\n\n    sns.distplot(X_scaled, hist=False, rug=True)\n    plt.title('Distribution of Normalized Data')\n    plt.xlabel('Value')\n    plt.ylabel('Density')\n    plt.show()\n\n    return X_scaled\n```"}
{"seed": "{\"task_id\": 37418, \"prompt\": \"def f_37418(file):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"open(file, 'w')\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    f = candidate('test.txt')\\n    assert f.name == 'test.txt'\\n    assert f.mode == 'w'\\n\"], \"entry_point\": \"f_37418\", \"intent\": \"\\u30d5\\u30a1\\u30a4\\u30eb`file`\\u3092\\u4e0a\\u66f8\\u304d\\u30e2\\u30fc\\u30c9\\u3067\\u958b\\u304f\", \"library\": []}", "generation": "Scenario 1:\npandas, os, matplotlib.pyplot, csv\n```python\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport csv\n\n# Constants\nFILE_PATH = 'data.csv'\n\ndef visualize_data(filename):\n    \"\"\"\n    Reads a csv file, creates a pandas DataFrame, and plots a bar graph of the data.\n\n    Parameters:\n    filename (str): The name of the csv file.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - os\n    - matplotlib.pyplot\n    - csv\n\n    Example:\n    >>> visualize_data('data.csv')\n    \"\"\"\n    if not os.path.isfile(filename):\n        print(f\"File {filename} does not exist.\")\n        return\n\n    data = pd.read_csv(filename)\n    data.plot(kind='bar')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, pandas, matplotlib.pyplot, random\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nDATA_SIZE = 100\n\ndef generate_and_plot_data():\n    \"\"\"\n    Generates random data and creates a scatter plot.\n\n    Parameters:\n    None\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> generate_and_plot_data()\n    \"\"\"\n    data = np.random.rand(DATA_SIZE, 2)\n    df = pd.DataFrame(data, columns=['x', 'y'])\n    plt.scatter(df['x'], df['y'])\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, zipfile, shutil\n```python\nimport os\nimport zipfile\nimport shutil\n\n# Constants\nDIRECTORY = 'files'\nARCHIVE_NAME = 'archive.zip'\n\ndef archive_directory(directory, archive_name):\n    \"\"\"\n    Archives a directory into a zip file.\n\n    Parameters:\n    directory (str): The directory to archive.\n    archive_name (str): The name of the zip file.\n\n    Returns:\n    None\n\n    Requirements:\n    - os\n    - zipfile\n    - shutil\n\n    Example:\n    >>> archive_directory('files', 'archive.zip')\n    \"\"\"\n    if not os.path.isdir(directory):\n        print(f\"Directory {directory} does not exist.\")\n        return\n\n    with zipfile.ZipFile(archive_name, 'w') as zipf:\n        for root, dirs, files in os.walk(directory):\n            for file in files:\n                zipf.write(os.path.join(root, file))\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, matplotlib.pyplot, sklearn.cluster\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\n\n# Constants\nCLUSTERS = 3\n\ndef perform_kmeans_clustering(data):\n    \"\"\"\n    Performs K-means clustering on a pandas DataFrame and plots the results.\n\n    Parameters:\n    data (DataFrame): The pandas DataFrame to cluster.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - sklearn.cluster\n\n    Example:\n    >>> data = pd.DataFrame({'x': [1, 2, 3, 4, 5], 'y': [2, 3, 4, 5, 6]})\n    >>> perform_kmeans_clustering(data)\n    \"\"\"\n    kmeans = KMeans(n_clusters=CLUSTERS)\n    kmeans.fit(data)\n\n    plt.scatter(data['x'], data['y'], c=kmeans.labels_)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, matplotlib.pyplot, math\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\n\n# Constants\nPI = math.pi\nFREQUENCY = 2\nAMPLITUDE = 1\nSAMPLE_RATE = 1000\nDURATION = 1\n\ndef generate_and_plot_sine_wave(frequency, amplitude, sample_rate, duration):\n    \"\"\"\n    Generates a sine wave and plots it.\n\n    Parameters:\n    frequency (int): The frequency of the sine wave.\n    amplitude (int): The amplitude of the sine wave.\n    sample_rate (int): The sample rate.\n    duration (int): The duration of the sine wave.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - math\n\n    Example:\n    >>> generate_and_plot_sine_wave(2, 1, 1000, 1)\n    \"\"\"\n    t = np.linspace(0, duration, sample_rate * duration, False)\n    note = np.sin(frequency * t * 2 * PI) * amplitude\n\n    plt.plot(t, note)\n    plt.show()\n```"}
{"seed": "{\"task_id\": 41200, \"prompt\": \"def f_41200(x_list, y_list):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"plt.plot(x_list, y_list)\", \"test_start\": \"\\nimport matplotlib.pyplot as plt\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert isinstance(candidate([1, 3, 5], [2, 4, 6]), list)\\n\"], \"entry_point\": \"f_41200\", \"intent\": \"\\u30c7\\u30fc\\u30bf`x_list`\\u3001`y_list`\\u304b\\u3089\\u306a\\u308b\\u30b0\\u30e9\\u30d5\\u3092\\u63cf\\u753b\\u3059\\u308b\\u6307\\u5b9a\\u3059\\u308b\", \"library\": [\"matplotlib\"]}", "generation": "Scenario 1:\npandas, matplotlib, numpy, scipy.stats, seaborn\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy import stats\nimport seaborn as sns\n\ndef plot_regression(x_list, y_list):\n    \"\"\"\n    Plot a scatterplot of the provided x and y data and fit a linear regression line \n    with a 95% confidence interval. Also show the Pearson correlation coefficient and \n    the p-value for testing non-correlation.\n\n    Parameters:\n    x_list (list): list of x-coordinates.\n    y_list (list): list of y-coordinates.\n\n    Returns:\n    Figure: A matplotlib Figure instance.\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - numpy\n    - scipy.stats\n    - seaborn\n\n    Example:\n    >>> plot_regression([1, 2, 3, 4, 5], [2, 4, 6, 8, 10])\n    \"\"\"\n    data = pd.DataFrame({\n        'X': x_list,\n        'Y': y_list\n    })\n\n    correlation_coef, p_value = stats.pearsonr(data['X'], data['Y'])\n\n    plt.figure(figsize=(10, 6))\n    sns.regplot(x='X', y='Y', data=data, ci=95)\n\n    plt.title('Linear Regression Plot')\n    plt.text(0.1, 0.9, f'Correlation coefficient: {correlation_coef:.2f}', transform=plt.gca().transAxes)\n    plt.text(0.1, 0.85, f'P-value: {p_value:.2f}', transform=plt.gca().transAxes)\n\n    return plt.gcf()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nmatplotlib, numpy, scipy.integrate, sympy\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy import integrate\nfrom sympy import symbols, diff\n\nX = symbols('X')\nFUNC = 3 * X ** 2 + 2 * X - 1  # The function to be integrated\n\ndef plot_integral(x_list, y_list):\n    \"\"\"\n    Plot the provided x and y data as a line graph, calculate the integral \n    of the function represented by the data, and fill the area under the curve.\n\n    Parameters:\n    x_list (list): list of x-coordinates.\n    y_list (list): list of y-coordinates.\n\n    Returns:\n    Figure: A matplotlib Figure instance.\n\n    Requirements:\n    - matplotlib.pyplot\n    - numpy\n    - scipy.integrate\n    - sympy\n\n    Example:\n    >>> plot_integral([1, 2, 3], [2, 4, 9])\n    \"\"\"\n    plt.figure(figsize=(10, 6))\n\n    # Plot the function\n    plt.plot(x_list, y_list, label='f(x)')\n\n    # Calculate the integral of the function\n    integral = integrate.quad(lambda x: eval(str(diff(FUNC, X))), min(x_list), max(x_list))\n\n    # Calculate the y-values for the integral function\n    y_integral = [integral[0] * x + integral[1] for x in x_list]\n\n    # Plot the integral function and fill the area under the curve\n    plt.plot(x_list, y_integral, 'r', label='F(x)')\n    plt.fill_between(x_list, y_integral, alpha=0.2)\n\n    plt.legend()\n    plt.grid(True)\n    plt.title('Integral of a Function')\n\n    return plt.gcf()\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 3:\nmatplotlib, numpy, pandas, sklearn.linear_model, sklearn.model_selection\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\ndef evaluate_linear_model(x_list, y_list):\n    \"\"\"\n    Evaluate the performance of a linear regression model using the provided x and y data.\n    The data is split into training and testing sets. The model is trained on the training \n    set and evaluated on the testing set. The function returns the model's score and plots \n    the model's predictions on the testing set.\n\n    Parameters:\n    x_list (list): list of x-coordinates.\n    y_list (list): list of y-coordinates.\n\n    Returns:\n    float: The model's score on the testing set.\n\n    Requirements:\n    - matplotlib.pyplot\n    - numpy\n    - pandas\n    - sklearn.linear_model\n    - sklearn.model_selection\n\n    Example:\n    >>> evaluate_linear_model([1, 2, 3, 4, 5], [2, 4, 6, 8, 10])\n    \"\"\"\n    data = pd.DataFrame({\n        'X': x_list,\n        'Y': y_list\n    })\n\n    X_train, X_test, y_train, y_test = train_test_split(data[['X']], data['Y'], test_size=0.2, random_state=42)\n\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    score = model.score(X_test, y_test)\n\n    plt.figure(figsize=(10, 6))\n    plt.scatter(X_test, y_test, color='blue')\n    plt.plot(X_test, model.predict(X_test), color='red')\n    plt.title('Linear Regression Model')\n    plt.xlabel('X')\n    plt.ylabel('Y')\n\n    return score\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nmatplotlib, numpy, pandas, seaborn, statsmodels.api\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport statsmodels.api as sm\n\ndef plot_distribution(x_list, y_list):\n    \"\"\"\n    Plot a scatterplot of the provided x and y data, fit a linear regression line, and \n    plot the residuals distribution and a Q-Q plot to check for normality.\n\n    Parameters:\n    x_list (list): list of x-coordinates.\n    y_list (list): list of y-coordinates.\n\n    Returns:\n    Figure: A matplotlib Figure instance.\n\n    Requirements:\n    - matplotlib.pyplot\n    - numpy\n    - pandas\n    - seaborn\n    - statsmodels.api\n\n    Example:\n    >>> plot_distribution([1, 2, 3, 4, 5], [2, 4, 6, 8, 10])\n    \"\"\"\n    data = pd.DataFrame({\n        'X': x_list,\n        'Y': y_list\n    })\n\n    model = sm.OLS(data['Y'], sm.add_constant(data['X'])).fit()\n    residuals = model.resid\n\n    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n\n    # Scatterplot and regression line\n    sns.regplot(x='X', y='Y', data=data, ax=axes[0])\n    axes[0].set_title('Scatterplot and Regression Line')\n\n    # Histogram and density plot of residuals\n    sns.histplot(residuals, kde=True, ax=axes[1])\n    axes[1].set_title('Residuals Distribution')\n\n    # Q-Q plot of residuals\n    sm.qqplot(residuals, line='s', ax=axes[2])\n    axes[2].set_title('Q-Q Plot')\n\n    plt.tight_layout()\n\n    return fig\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nmatplotlib, numpy, pandas, sklearn.decomposition, sklearn.preprocessing\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\ndef perform_pca(x_list, y_list):\n    \"\"\"\n    Perform Principal Component Analysis (PCA) on the provided x and y data after standardizing \n    it, and plot the first two principal components.\n\n    Parameters:\n    x_list (list): list of x-coordinates.\n    y_list (list): list of y-coordinates.\n\n    Returns:\n    Figure: A matplotlib Figure instance.\n\n    Requirements:\n    - matplotlib.pyplot\n    - numpy\n    - pandas\n    - sklearn.decomposition\n    - sklearn.preprocessing\n\n    Example:\n    >>> perform_pca([1, 2, 3, 4, 5], [2, 4, 6, 8, 10])\n    \"\"\"\n    data = pd.DataFrame({\n        'X': x_list,\n        'Y': y_list\n    })\n\n    scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(data)\n\n    pca = PCA(n_components=2)\n    principal_components = pca.fit_transform(standardized_data)\n\n    plt.figure(figsize=(10, 6))\n    plt.scatter(principal_components[:, 0], principal_components[:, 1])\n    plt.title('First Two Principal Components')\n    plt.xlabel('Principal Component 1')\n    plt.ylabel('Principal Component 2')\n\n    return plt.gcf()\n```"}
{"seed": "{\"task_id\": 43369, \"prompt\": \"def f_43369(a, b):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"pd.DataFrame([a, b])\", \"test_start\": \"\\nimport numpy as np\\nimport pandas as pd\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([1,1,1], [2,2,2]).equals(pd.DataFrame([[1,1,1], [2,2,2]]))\\n\", \"\\n    assert candidate([1,2,1], [2,3,4]).equals(pd.DataFrame([[1,2,1], [2,3,4]]))\\n\", \"\\n    assert candidate([0], [1]).equals(pd.DataFrame([[0], [1]]))\\n\"], \"entry_point\": \"f_43369\", \"intent\": \"2\\u3064\\u306e\\u30c7\\u30fc\\u30bf\\u30d5\\u30ec\\u30fc\\u30e0`a`\\u3068`b`\\u3092\\u884c\\u65b9\\u5411\\u306b\\u7d50\\u5408\\u3059\\u308b\", \"library\": [\"numpy\", \"pandas\"]}", "generation": "Scenario 1:\npandas, numpy, random, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nROWS = 5\nCOLS = 3\n\ndef random_df_statistics(a, b):\n    \"\"\"\n    Generate two random dataframes with given dimensions, calculate their statistics (mean, min, max, std)\n    and plot a bar chart comparing these statistics.\n\n    Parameters:\n    a (int): Number of rows for the dataframes.\n    b (int): Number of columns for the dataframes.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the statistics.\n\n    Requirements:\n    - pandas\n    - numpy\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> stats = random_df_statistics(5, 3)\n    >>> print(stats)\n    >>> stats.plot(kind='bar')\n    \"\"\"\n    df1 = pd.DataFrame(np.random.randint(0,100,size=(a, b)))\n    df2 = pd.DataFrame(np.random.randint(0,100,size=(a, b)))\n\n    stats1 = df1.describe().loc[['mean', 'min', 'max', 'std']]\n    stats2 = df2.describe().loc[['mean', 'min', 'max', 'std']]\n\n    stats = pd.concat([stats1, stats2], axis=1, keys=['DF1', 'DF2'])\n\n    return stats\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, seaborn, sklearn.preprocessing\n```python\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\n\n# Constants\nROWS = 5\nCOLS = 3\n\ndef standardize_and_plot_dataframes(a, b):\n    \"\"\"\n    Generate two random dataframes with given dimensions, standardize them using sklearn's StandardScaler,\n    and plot a boxplot for each dataframe.\n\n    Parameters:\n    a (int): Number of rows for the dataframes.\n    b (int): Number of columns for the dataframes.\n\n    Returns:\n    DataFrame: Two pandas DataFrames with standardized values.\n\n    Requirements:\n    - pandas\n    - numpy\n    - seaborn\n    - sklearn.preprocessing.StandardScaler\n    \n    Example:\n    >>> df1, df2 = standardize_and_plot_dataframes(5, 3)\n    >>> print(df1)\n    >>> print(df2)\n    >>> sns.boxplot(data=df1)\n    >>> sns.boxplot(data=df2)\n    \"\"\"\n    df1 = pd.DataFrame(np.random.randint(0,100,size=(a, b)))\n    df2 = pd.DataFrame(np.random.randint(0,100,size=(a, b)))\n\n    scaler = StandardScaler()\n\n    df1_standardized = pd.DataFrame(scaler.fit_transform(df1), columns=df1.columns)\n    df2_standardized = pd.DataFrame(scaler.fit_transform(df2), columns=df2.columns)\n\n    return df1_standardized, df2_standardized\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, matplotlib, sklearn.linear_model\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n# Constants\nROWS = 5\nCOLS = 3\n\ndef linear_regression_on_dataframes(a, b):\n    \"\"\"\n    Generate two random dataframes with given dimensions, perform linear regression using sklearn's LinearRegression,\n    and plot a scatter plot with the regression line.\n\n    Parameters:\n    a (int): Number of rows for the dataframes.\n    b (int): Number of columns for the dataframes.\n\n    Returns:\n    LinearRegression: The LinearRegression model.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.linear_model.LinearRegression\n    \n    Example:\n    >>> model = linear_regression_on_dataframes(5, 3)\n    >>> print(model.coef_)\n    \"\"\"\n    df1 = pd.DataFrame(np.random.randint(0,100,size=(a, b)))\n    df2 = pd.DataFrame(np.random.randint(0,100,size=(a, b)))\n\n    X = df1.values\n    y = df2.values\n\n    model = LinearRegression().fit(X, y)\n\n    plt.scatter(X, y, color='blue')\n    plt.plot(X, model.predict(X), color='red')\n\n    return model\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, scipy.stats, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\n\n# Constants\nROWS = 5\nCOLS = 3\n\ndef compare_dataframes_with_ttest(a, b):\n    \"\"\"\n    Generate two random dataframes with given dimensions, perform a t-test using scipy's ttest_ind,\n    and plot a histogram for each dataframe.\n\n    Parameters:\n    a (int): Number of rows for the dataframes.\n    b (int): Number of columns for the dataframes.\n\n    Returns:\n    float: The t-statistic and p-value.\n\n    Requirements:\n    - pandas\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n    \n    Example:\n    >>> t_stat, p_val = compare_dataframes_with_ttest(5, 3)\n    >>> print('t-statistic:', t_stat)\n    >>> print('p-value:', p_val)\n    \"\"\"\n    df1 = pd.DataFrame(np.random.randint(0,100,size=(a, b)))\n    df2 = pd.DataFrame(np.random.randint(0,100,size=(a, b)))\n\n    t_stat, p_val = stats.ttest_ind(df1.values.ravel(), df2.values.ravel())\n\n    plt.hist(df1.values.ravel(), alpha=0.5, label='DF1')\n    plt.hist(df2.values.ravel(), alpha=0.5, label='DF2')\n    plt.legend(loc='upper right')\n\n    return t_stat, p_val\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, sklearn.cluster, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\n# Constants\nROWS = 5\nCOLS = 3\n\ndef cluster_dataframes_and_plot(a, b):\n    \"\"\"\n    Generate two random dataframes with given dimensions, cluster them using sklearn's KMeans,\n    and plot a scatter plot with different colors for each cluster.\n\n    Parameters:\n    a (int): Number of rows for the dataframes.\n    b (int): Number of columns for the dataframes.\n\n    Returns:\n    KMeans: The KMeans model.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.cluster.KMeans\n    - matplotlib.pyplot\n    \n    Example:\n    >>> model = cluster_dataframes_and_plot(5, 3)\n    >>> print(model.cluster_centers_)\n    \"\"\"\n    df1 = pd.DataFrame(np.random.randint(0,100,size=(a, b)))\n    df2 = pd.DataFrame(np.random.randint(0,100,size=(a, b)))\n\n    X = pd.concat([df1, df2])\n\n    model = KMeans(n_clusters=2).fit(X)\n\n    plt.scatter(X[0], X[1], c=model.labels_, cmap='viridis')\n\n    return model\n```"}
{"seed": "{\"task_id\": 24438, \"prompt\": \"def f_24438(file):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"codecs.open(file, 'r', 'utf-8')\", \"test_start\": \"\\nimport codecs\\n\\ndef check(candidate):\", \"test\": [\"\\n    with open('test.txt', 'w') as fw:\\n        fw.write('hello world!')\\n    fr = candidate('test.txt')\\n    assert fr.name == 'test.txt'\\n\"], \"entry_point\": \"f_24438\", \"intent\": \"\\u6587\\u5b57\\u30b3\\u30fc\\u30c9\\u3092utf-8\\u306b\\u6307\\u5b9a\\u3057\\u3066\\u30d5\\u30a1\\u30a4\\u30eb`file`\\u3092\\u958b\\u304f\", \"library\": [\"codecs\"]}", "generation": "Scenario 3: \npandas, os, re, codecs\n```python\nimport pandas as pd\nimport os\nimport re\nimport codecs\n\n# Constants\nCSV_EXTENSION = '.csv'\nDEFAULT_ENCODING = 'utf-8'\n\ndef read_and_clean_csv_files(directory):\n    \"\"\"\n    Read all CSV files in a directory, clean the data by removing any non-numeric characters \n    from numeric columns, and return a dictionary with filename (without extension) as keys and pandas DataFrame as values.\n\n    Parameters:\n    directory (str): The directory where the CSV files are located.\n\n    Returns:\n    dict: A dictionary where keys are the filename (without extension) and values are pandas DataFrame.\n\n    Requirements:\n    - pandas\n    - os\n    - re\n    - codecs\n\n    Example:\n    >>> dataframes = read_and_clean_csv_files('/path/to/csv/files')\n    >>> df_1 = dataframes['file1']\n    \"\"\"\n    dataframes = {}\n\n    for file in os.listdir(directory):\n        if file.endswith(CSV_EXTENSION):\n            file_path = os.path.join(directory, file)\n            df = pd.read_csv(codecs.open(file_path, 'r', DEFAULT_ENCODING))\n\n            for column in df.columns:\n                if df[column].dtype == 'object':\n                    df[column] = df[column].str.replace(r'\\D', '', regex=True)\n                    df[column] = pd.to_numeric(df[column], errors='coerce')\n\n            filename_without_ext = os.path.splitext(file)[0]\n            dataframes[filename_without_ext] = df\n\n    return dataframes\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ncodecs, json, os, datetime\n```python\nimport codecs\nimport json\nimport os\nfrom datetime import datetime\n\n# Constants\nJSON_EXTENSION = '.json'\nDEFAULT_ENCODING = 'utf-8'\n\ndef extract_create_date_from_json(directory):\n    \"\"\"\n    Extract the 'create_date' field from all JSON files in a directory, \n    and return a dictionary with filename (without extension) as keys and create_date as values.\n\n    Parameters:\n    directory (str): The directory where the JSON files are located.\n\n    Returns:\n    dict: A dictionary where keys are the filename (without extension) and values are create_date.\n\n    Requirements:\n    - codecs\n    - json\n    - os\n    - datetime\n\n    Example:\n    >>> create_dates = extract_create_date_from_json('/path/to/json/files')\n    >>> create_date_1 = create_dates['file1']\n    \"\"\"\n    create_dates = {}\n\n    for file in os.listdir(directory):\n        if file.endswith(JSON_EXTENSION):\n            file_path = os.path.join(directory, file)\n            with codecs.open(file_path, 'r', DEFAULT_ENCODING) as f:\n                data = json.load(f)\n                if 'create_date' in data:\n                    create_date_str = data['create_date']\n                    create_date = datetime.strptime(create_date_str, '%Y-%m-%d %H:%M:%S')\n                    filename_without_ext = os.path.splitext(file)[0]\n                    create_dates[filename_without_ext] = create_date\n\n    return create_dates\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\ncodecs, os, shutil, zipfile\n```python\nimport codecs\nimport os\nimport shutil\nimport zipfile\n\n# Constants\nZIP_EXTENSION = '.zip'\nDEFAULT_ENCODING = 'utf-8'\n\ndef unzip_files_and_write_content(directory):\n    \"\"\"\n    Unzip all zip files in a directory, read the content of text files in the unzipped folders, and \n    write the content to a new text file named after the original zip file.\n\n    Parameters:\n    directory (str): The directory where the zip files are located.\n\n    Requirements:\n    - codecs\n    - os\n    - shutil\n    - zipfile\n\n    Example:\n    >>> unzip_files_and_write_content('/path/to/zip/files')\n    \"\"\"\n    for file in os.listdir(directory):\n        if file.endswith(ZIP_EXTENSION):\n            file_path = os.path.join(directory, file)\n            with zipfile.ZipFile(file_path, 'r') as zip_ref:\n                zip_ref.extractall(directory)\n\n            folder_name = os.path.splitext(file)[0]\n            folder_path = os.path.join(directory, folder_name)\n\n            for filename in os.listdir(folder_path):\n                if filename.endswith('.txt'):\n                    text_file_path = os.path.join(folder_path, filename)\n                    with codecs.open(text_file_path, 'r', DEFAULT_ENCODING) as f:\n                        content = f.read()\n\n                    new_file_path = os.path.join(directory, folder_name + '.txt')\n                    with codecs.open(new_file_path, 'w', DEFAULT_ENCODING) as f:\n                        f.write(content)\n\n            shutil.rmtree(folder_path)\n```\n`GPT_ODEX_BREAK`\n\nScenario 6:\ncodecs, csv, os, random\n```python\nimport codecs\nimport csv\nimport os\nimport random\n\n# Constants\nCSV_EXTENSION = '.csv'\nDEFAULT_ENCODING = 'utf-8'\n\ndef shuffle_csv_rows(directory):\n    \"\"\"\n    Shuffle all rows in each CSV file in a directory and write the shuffled rows back to the file.\n\n    Parameters:\n    directory (str): The directory where the CSV files are located.\n\n    Requirements:\n    - codecs\n    - csv\n    - os\n    - random\n\n    Example:\n    >>> shuffle_csv_rows('/path/to/csv/files')\n    \"\"\"\n    for file in os.listdir(directory):\n        if file.endswith(CSV_EXTENSION):\n            file_path = os.path.join(directory, file)\n            with codecs.open(file_path, 'r', DEFAULT_ENCODING) as f:\n                reader = csv.reader(f)\n                rows = list(reader)\n\n            random.shuffle(rows)\n\n            with codecs.open(file_path, 'w', DEFAULT_ENCODING) as f:\n                writer = csv.writer(f)\n                writer.writerows(rows)\n```\n`GPT_ODEX_BREAK`\n\nScenario 7:\ncodecs, os, json, datetime\n```python\nimport codecs\nimport os\nimport json\nfrom datetime import datetime\n\n# Constants\nJSON_EXTENSION = '.json'\nDEFAULT_ENCODING = 'utf-8'\n\ndef filter_json_files_by_date(directory, date_str):\n    \"\"\"\n    Filter all JSON files in a directory by 'create_date' field, \n    keep only those files where 'create_date' is later than a given date, \n    and return a list of these filenames without extension.\n\n    Parameters:\n    directory (str): The directory where the JSON files are located.\n    date_str (str): The date string in \"yyyy-mm-dd\" format.\n\n    Returns:\n    list: A list of filenames (without extension) where 'create_date' is later than the given date.\n\n    Requirements:\n    - codecs\n    - os\n    - json\n    - datetime\n\n    Example:\n    >>> filtered_files = filter_json_files_by_date('/path/to/json/files', '2023-01-01')\n    \"\"\"\n    filtered_files = []\n    given_date = datetime.strptime(date_str, '%Y-%m-%d')\n\n    for file in os.listdir(directory):\n        if file.endswith(JSON_EXTENSION):\n            file_path = os.path.join(directory, file)\n            with codecs.open(file_path, 'r', DEFAULT_ENCODING) as f:\n                data = json.load(f)\n                if 'create_date' in data:\n                    create_date = datetime.strptime(data['create_date'], '%Y-%m-%d %H:%M:%S')\n                    if create_date > given_date:\n                        filename_without_ext = os.path.splitext(file)[0]\n                        filtered_files.append(filename_without_ext)\n\n    return filtered_files\n```"}
{"seed": "{\"task_id\": 10215, \"prompt\": \"def f_10215(file):\\n\\t\", \"suffix\": \"\\n\\treturn data\", \"canonical_solution\": \"with open(file, 'rb') as f:\\n\\t    data = f.read()\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    with open('tmp.pkl', 'wb') as fw:\\n        fw.write(b\\\"hello world!\\\")\\n    assert candidate('tmp.pkl') == b\\\"hello world!\\\"\\n\"], \"entry_point\": \"f_10215\", \"intent\": \"\\u30d5\\u30a1\\u30a4\\u30eb`file`\\u3092\\u30d0\\u30a4\\u30ca\\u30ea\\u30c7\\u30fc\\u30bf\\u3068\\u3057\\u3066\\u958b\\u304f\", \"library\": []}", "generation": "Scenario 1:\nnumpy, pandas, matplotlib, os\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\n\n# Constants\nFILE_PATH = 'data.csv'\n\ndef plot_data_distribution(file):\n    \"\"\"\n    Read a CSV file and plot a histogram of the data distribution.\n    \n    Parameters:\n    file (str): The file to read.\n    \n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - os\n    \n    Example:\n    >>> plot_data_distribution(FILE_PATH)\n    \"\"\"\n    if not os.path.isfile(file):\n        raise FileNotFoundError(f\"{file} not found.\")\n\n    # Read the file\n    data = pd.read_csv(file)\n\n    # Plot the histogram\n    data.hist(bins=50, figsize=(20,15))\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, pandas, seaborn, matplotlib\n```python\nimport os\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nFILE_PATH = 'data.csv'\n\ndef visualize_file_data(file):\n    \"\"\"\n    Read a CSV file and visualize the data using a pairplot.\n    \n    Parameters:\n    file (str): The file to read.\n    \n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - seaborn\n    - matplotlib.pyplot\n    - os\n    \n    Example:\n    >>> visualize_file_data(FILE_PATH)\n    \"\"\"\n    if not os.path.isfile(file):\n        raise FileNotFoundError(f\"{file} not found.\")\n\n    # Read the file\n    data = pd.read_csv(file)\n\n    # Visualize the data\n    sns.pairplot(data)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, pandas, numpy, sklearn\n```python\nimport os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\n# Constants\nFILE_PATH = 'data.csv'\n\ndef split_data(file):\n    \"\"\"\n    Read a CSV file, split the data into training and test sets, and return the split data.\n    \n    Parameters:\n    file (str): The file to read.\n    \n    Returns:\n    tuple: The training and test data as pandas DataFrames.\n\n    Requirements:\n    - pandas\n    - numpy\n    - os\n    - sklearn.model_selection\n    \n    Example:\n    >>> X_train, X_test, y_train, y_test = split_data(FILE_PATH)\n    \"\"\"\n    if not os.path.isfile(file):\n        raise FileNotFoundError(f\"{file} not found.\")\n\n    # Read the file\n    data = pd.read_csv(file)\n\n    # Split the data\n    X = data.drop('target', axis=1)\n    y = data['target']\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    return X_train, X_test, y_train, y_test\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, json, pandas, numpy\n```python\nimport os\nimport json\nimport pandas as pd\nimport numpy as np\n\n# Constants\nFILE_PATH = 'data.json'\n\ndef process_json(file):\n    \"\"\"\n    Read a JSON file, convert it into a pandas DataFrame, and perform some simple processing.\n    \n    Parameters:\n    file (str): The file to read.\n    \n    Returns:\n    DataFrame: The processed data as a pandas DataFrame.\n\n    Requirements:\n    - json\n    - pandas\n    - numpy\n    - os\n    \n    Example:\n    >>> df = process_json(FILE_PATH)\n    \"\"\"\n    if not os.path.isfile(file):\n        raise FileNotFoundError(f\"{file} not found.\")\n\n    # Read the file\n    with open(file, 'r') as f:\n        data = json.load(f)\n\n    # Convert the data into a DataFrame\n    df = pd.DataFrame(data)\n\n    # Perform some simple processing\n    df = df.dropna()\n    df = df.astype(np.float64)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, pandas, numpy, matplotlib\n```python\nimport os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nFILE_PATH = 'data.csv'\n\ndef analyze_file_data(file):\n    \"\"\"\n    Read a CSV file, analyze the data, and plot a boxplot for each column.\n    \n    Parameters:\n    file (str): The file to read.\n    \n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - os\n    \n    Example:\n    >>> analyze_file_data(FILE_PATH)\n    \"\"\"\n    if not os.path.isfile(file):\n        raise FileNotFoundError(f\"{file} not found.\")\n\n    # Read the file\n    data = pd.read_csv(file)\n\n    # Plot a boxplot for each column\n    for column in data.columns:\n        plt.figure(figsize=(10, 4))\n        plt.title(f\"Boxplot of {column}\")\n        data.boxplot(column=column)\n        plt.show()\n```\n"}
{"seed": "{\"task_id\": 18992, \"prompt\": \"def f_18992(x):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"pickle.dump(x, open('hoge.pkl', 'wb'))\", \"test_start\": \"\\nimport pickle\\n\\ndef check(candidate):\", \"test\": [\"\\n    x = [100, 435, 56, 2, 99]\\n    candidate(x)\\n    with open('hoge.pkl', 'rb') as fr:\\n        data = pickle.load(fr)\\n        assert data == x\\n\"], \"entry_point\": \"f_18992\", \"intent\": \"\\u30aa\\u30d6\\u30b8\\u30a7\\u30af\\u30c8`x`\\u3092\\u30d5\\u30a1\\u30a4\\u30eb`hoge.pkl`\\u306b\\u4fdd\\u5b58\\u3059\\u308b\", \"library\": [\"pickle\"]}", "generation": "Scenario 1: \npandas, os, pickle, seaborn, matplotlib\n```python\nimport pandas as pd\nimport os\nimport pickle\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nFILENAME = 'iris_dataset.pkl'\n\ndef save_and_load_dataset():\n    \"\"\"\n    Save 'iris' dataset from seaborn library to a pickle file and load it back.\n    Then, plot the pairwise relationships in the dataset.\n\n    Returns:\n    DataFrame: The loaded DataFrame from the pickle file.\n\n    Requirements:\n    - pandas\n    - os\n    - pickle\n    - seaborn\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = save_and_load_dataset()\n    >>> print(df.head())\n    >>> sns.pairplot(df, hue='species')\n    \"\"\"\n    # Load dataset\n    iris = sns.load_dataset('iris')\n    # Save to pickle file\n    with open(FILENAME, 'wb') as f:\n        pickle.dump(iris, f)\n\n    # Load from pickle file\n    if os.path.exists(FILENAME):\n        with open(FILENAME, 'rb') as f:\n            df = pickle.load(f)\n    else:\n        raise FileNotFoundError(f\"{FILENAME} does not exist.\")\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, pickle, scipy.optimize, matplotlib\n```python\nimport numpy as np\nimport pickle\nfrom scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\n\n# Constants\nPICKLE_FILE = 'fit_data.pkl'\nX = np.linspace(0, 10, 100)\nY = 3 * X + 2 + np.random.normal(0, 1, len(X))\n\ndef save_and_fit_data():\n    \"\"\"\n    Save x and y data to a pickle file and load it back.\n    Then, fit a linear model to the data and plot the original data and the fitted line.\n\n    Returns:\n    list: The optimized parameters of the linear model.\n\n    Requirements:\n    - numpy\n    - pickle\n    - scipy.optimize.curve_fit\n    - matplotlib.pyplot\n\n    Example:\n    >>> params = save_and_fit_data()\n    >>> print(params)\n    \"\"\"\n    # Save data to pickle file\n    with open(PICKLE_FILE, 'wb') as f:\n        pickle.dump((X, Y), f)\n\n    # Load data from pickle file\n    with open(PICKLE_FILE, 'rb') as f:\n        x, y = pickle.load(f)\n\n    # Define the form of the function we want to fit\n    def func(x, a, b):\n        return a * x + b\n\n    # Fit the function to the data\n    params, _ = curve_fit(func, x, y)\n\n    # Plot the data and the fitted line\n    plt.scatter(x, y, label='Data')\n    plt.plot(x, func(x, *params), 'r', label='Fitted line')\n    plt.legend()\n\n    return params.tolist()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, pickle, sklearn.cluster, matplotlib\n```python\nimport numpy as np\nimport pickle\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\n# Constants\nPICKLE_FILE = 'cluster_data.pkl'\nDATA = np.random.rand(100, 2)\n\ndef save_and_cluster_data():\n    \"\"\"\n    Save 2D data to a pickle file and load it back.\n    Then, perform K-means clustering and plot the data with different colors for each cluster.\n\n    Returns:\n    ndarray: The cluster centers.\n\n    Requirements:\n    - numpy\n    - pickle\n    - sklearn.cluster.KMeans\n    - matplotlib.pyplot\n\n    Example:\n    >>> centers = save_and_cluster_data()\n    >>> print(centers)\n    \"\"\"\n    # Save data to pickle file\n    with open(PICKLE_FILE, 'wb') as f:\n        pickle.dump(DATA, f)\n\n    # Load data from pickle file\n    with open(PICKLE_FILE, 'rb') as f:\n        data = pickle.load(f)\n\n    # Perform K-means clustering\n    kmeans = KMeans(n_clusters=3, random_state=0).fit(data)\n\n    # Plot the data with different colors for each cluster\n    plt.scatter(data[:, 0], data[:, 1], c=kmeans.labels_)\n    plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red')\n\n    return kmeans.cluster_centers_\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, pickle, sklearn.preprocessing, matplotlib\n```python\nimport pandas as pd\nimport pickle\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\n# Constants\nPICKLE_FILE = 'preprocess_data.pkl'\nDATA = pd.DataFrame({\n    'x': np.random.normal(10, 5, 1000),\n    'y': np.random.normal(20, 10, 1000)\n})\n\ndef save_and_preprocess_data():\n    \"\"\"\n    Save a DataFrame to a pickle file and load it back.\n    Then, standardize the features to have zero mean and unit variance.\n\n    Returns:\n    DataFrame: The DataFrame after standardization.\n\n    Requirements:\n    - pandas\n    - pickle\n    - sklearn.preprocessing.StandardScaler\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = save_and_preprocess_data()\n    >>> print(df.describe())\n    \"\"\"\n    # Save DataFrame to pickle file\n    with open(PICKLE_FILE, 'wb') as f:\n        pickle.dump(DATA, f)\n\n    # Load DataFrame from pickle file\n    with open(PICKLE_FILE, 'rb') as f:\n        df = pickle.load(f)\n\n    # Standardize the features to have zero mean and unit variance\n    scaler = StandardScaler()\n    df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n\n    return df_scaled\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, pickle, sklearn.decomposition, matplotlib\n```python\nimport numpy as np\nimport pickle\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\n# Constants\nPICKLE_FILE = 'decomposition_data.pkl'\nDATA = np.random.rand(100, 10)\n\ndef save_and_decompose_data():\n    \"\"\"\n    Save high-dimensional data to a pickle file and load it back.\n    Then, perform PCA to reduce the dimensionality of the data.\n\n    Returns:\n    ndarray: The 2D array after PCA.\n\n    Requirements:\n    - numpy\n    - pickle\n    - sklearn.decomposition.PCA\n    - matplotlib.pyplot\n\n    Example:\n    >>> reduced_data = save_and_decompose_data()\n    >>> print(reduced_data.shape)\n    \"\"\"\n    # Save data to pickle file\n    with open(PICKLE_FILE, 'wb') as f:\n        pickle.dump(DATA, f)\n\n    # Load data from pickle file\n    with open(PICKLE_FILE, 'rb') as f:\n        data = pickle.load(f)\n\n    # Perform PCA to reduce the dimensionality of the data\n    pca = PCA(n_components=2)\n    reduced_data = pca.fit_transform(data)\n\n    return reduced_data\n```\n"}
{"seed": "{\"task_id\": 38400, \"prompt\": \"def f_38400(df, change_dict):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"df.replace(change_dict)\", \"test_start\": \"\\nimport pandas as pd\\n\\ndef check(candidate):\", \"test\": [\"\\n    df = pd.DataFrame({'kai': ['2', 'B1', '23-49', 'M2']}, index=[1, 2, 3, 4])\\n    change_dict = {'2': '22', 'B1': 'B2'}\\n    assert candidate(df, change_dict).equals(df.replace(change_dict))\\n\"], \"entry_point\": \"f_38400\", \"intent\": \"\\u30c7\\u30fc\\u30bf\\u30d5\\u30ec\\u30fc\\u30e0`df`\\u306e\\u8907\\u6570\\u306e\\u7570\\u306a\\u308b\\u8981\\u7d20\\u3092\\u8f9e\\u66f8\\u578b\\u30aa\\u30d6\\u30b8\\u30a7\\u30af\\u30c8`change_dict`\\u306e\\u30ad\\u30fc\\u3068\\u8981\\u7d20\\u306e\\u30da\\u30a2\\u306b\\u5f93\\u3063\\u3066\\u7f6e\\u304d\\u63db\\u3048\\u308b\", \"library\": [\"pandas\"]}", "generation": "Scenario 1:\npandas, re, numpy, matplotlib\n```python\nimport pandas as pd\nimport re\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nPATTERN = r'^\\d+$'\n\ndef analyze_numeric_texts(df, change_dict):\n    \"\"\"\n    Replace multiple elements in the DataFrame `df` according to the key-value pairs in the dictionary `change_dict`. \n    Then analyze numeric texts in the transformed DataFrame and plot their distribution.\n\n    Parameters:\n    df (DataFrame): The input DataFrame.\n    change_dict (dict): The dictionary of key-value pairs for replacement.\n\n    Returns:\n    DataFrame: The DataFrame after replacement.\n    int: The number of numeric texts.\n    \n    Requirements:\n    - pandas\n    - re\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame({'Text': ['2', 'B1', '23-49', 'M2']}, index=[1, 2, 3, 4])\n    >>> change_dict = {'2': '22', 'B1': 'B2'}\n    >>> df_transformed, num_numeric_texts = analyze_numeric_texts(df, change_dict)\n    >>> print(df_transformed)\n    >>> print(num_numeric_texts)\n    \"\"\"\n    df = df.replace(change_dict)\n    numeric_texts = df['Text'].apply(lambda x: bool(re.match(PATTERN, x)))\n    num_numeric_texts = numeric_texts.sum()\n    numeric_texts.value_counts().plot(kind='bar')\n    plt.show()\n\n    return df, num_numeric_texts\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, sklearn.preprocessing, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nimport matplotlib.pyplot as plt\n\n# Constants\nCATEGORIES = ['A', 'B', 'C', 'D', 'E']\n\ndef categorical_analysis(df, change_dict):\n    \"\"\"\n    Replace multiple elements in the DataFrame `df` according to the key-value pairs in the dictionary `change_dict`. \n    Then encode the categorical variables and analyze their distributions.\n\n    Parameters:\n    df (DataFrame): The input DataFrame.\n    change_dict (dict): The dictionary of key-value pairs for replacement.\n\n    Returns:\n    DataFrame: The DataFrame after replacement and encoding.\n    Series: The distribution of encoded categories.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.preprocessing\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame({'Category': ['A', 'B', 'C', 'D', 'E']}, index=[1, 2, 3, 4, 5])\n    >>> change_dict = {'A': 'AA', 'B': 'BB'}\n    >>> df_transformed, distribution = categorical_analysis(df, change_dict)\n    >>> print(df_transformed)\n    >>> print(distribution)\n    \"\"\"\n    df = df.replace(change_dict)\n    le = LabelEncoder()\n    df['Category'] = le.fit_transform(df['Category'])\n    distribution = df['Category'].value_counts()\n    distribution.plot(kind='bar')\n    plt.show()\n\n    return df, distribution\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, seaborn, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['Column1', 'Column2', 'Column3']\n\ndef analyze_correlation(df, change_dict):\n    \"\"\"\n    Replace multiple elements in the DataFrame `df` according to the key-value pairs in the dictionary `change_dict`. \n    Then calculate the correlation matrix of the transformed DataFrame and visualize it with a heatmap.\n\n    Parameters:\n    df (DataFrame): The input DataFrame.\n    change_dict (dict): The dictionary of key-value pairs for replacement.\n\n    Returns:\n    DataFrame: The DataFrame after replacement.\n    DataFrame: The correlation matrix of the DataFrame.\n\n    Requirements:\n    - pandas\n    - numpy\n    - seaborn\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame(np.random.random((5,3)), columns=COLUMNS)\n    >>> change_dict = {0.5: 0.6, 0.7: 0.8}\n    >>> df_transformed, corr_matrix = analyze_correlation(df, change_dict)\n    >>> print(df_transformed)\n    >>> print(corr_matrix)\n    \"\"\"\n    df = df.replace(change_dict)\n    corr_matrix = df.corr()\n    sns.heatmap(corr_matrix, annot=True)\n    plt.show()\n\n    return df, corr_matrix\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, sklearn.decomposition, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4']\n\ndef perform_pca(df, change_dict):\n    \"\"\"\n    Replace multiple elements in the DataFrame `df` according to the key-value pairs in the dictionary `change_dict`. \n    Then perform Principal Component Analysis (PCA) on the transformed DataFrame.\n\n    Parameters:\n    df (DataFrame): The input DataFrame.\n    change_dict (dict): The dictionary of key-value pairs for replacement.\n\n    Returns:\n    DataFrame: The DataFrame after replacement.\n    ndarray: The transformed array after PCA.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.decomposition\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame(np.random.random((5,4)), columns=COLUMNS)\n    >>> change_dict = {0.5: 0.6, 0.7: 0.8}\n    >>> df_transformed, transformed = perform_pca(df, change_dict)\n    >>> print(df_transformed)\n    >>> print(transformed)\n    \"\"\"\n    df = df.replace(change_dict)\n    pca = PCA(n_components=2)\n    transformed = pca.fit_transform(df)\n\n    fig, ax = plt.subplots()\n    ax.scatter(transformed[:,0], transformed[:,1])\n    plt.show()\n\n    return df, transformed\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, scipy.stats, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['Column1', 'Column2', 'Column3']\n\ndef analyze_normality(df, change_dict):\n    \"\"\"\n    Replace multiple elements in the DataFrame `df` according to the key-value pairs in the dictionary `change_dict`. \n    Then check the normality of the distribution of each column in the transformed DataFrame.\n\n    Parameters:\n    df (DataFrame): The input DataFrame.\n    change_dict (dict): The dictionary of key-value pairs for replacement.\n\n    Returns:\n    DataFrame: The DataFrame after replacement.\n    dict: The p-values of the normality test for each column.\n\n    Requirements:\n    - pandas\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame(np.random.random((5,3)), columns=COLUMNS)\n    >>> change_dict = {0.5: 0.6, 0.7: 0.8}\n    >>> df_transformed, p_values = analyze_normality(df, change_dict)\n    >>> print(df_transformed)\n    >>> print(p_values)\n    \"\"\"\n    df = df.replace(change_dict)\n    p_values = {}\n\n    for column in df.columns:\n        _, p = stats.normaltest(df[column])\n        p_values[column] = p\n\n    return df, p_values\n```"}
{"seed": "{\"task_id\": 35793, \"prompt\": \"def f_35793():\\n\\t\", \"suffix\": \"\\n\\treturn data\", \"canonical_solution\": \"data = []\\n\\ti = 0\\n\\twhile(i<100):\\n\\t    data.append(pd.read_csv('file_%d.csv'%i))\\n\\t    i+=1\", \"test_start\": \"\\nimport pandas as pd\\n\\ndef check(candidate):\", \"test\": [\"\\n    for i in range(0, 100):\\n        with open ('file_'+str(i)+'.csv', 'w') as f:\\n            f.write(str(i))\\n    \\n    assert len(candidate()) == 100\\n\"], \"entry_point\": \"f_35793\", \"intent\": \"\\u9023\\u756a\\u306b\\u306a\\u3063\\u3066\\u3044\\u308b100\\u500b\\u306eCSV\\u30d5\\u30a1\\u30a4\\u30eb'file_%d'\\u3092\\u30ea\\u30b9\\u30c8\\u306b\\u53d6\\u308a\\u8fbc\\u3080\", \"library\": [\"pandas\"]}", "generation": "Scenario 1: \npandas, os, re, csv\n```python\nimport pandas as pd\nimport os\nimport re\nimport csv\n\n# Constants\nDIRECTORY = \"/path/to/directory\"\n\ndef load_csv_files(directory=DIRECTORY):\n    \"\"\"\n    Load all CSV files in a directory into a list of pandas DataFrames.\n\n    Parameters:\n    directory (str): The directory to scan for CSV files.\n\n    Returns:\n    list: A list of pandas DataFrames.\n\n    Requirements:\n    - pandas\n    - os\n    - re\n    - csv\n\n    Example:\n    >>> dataframes = load_csv_files()\n    >>> print(len(dataframes))\n    \"\"\"\n    csv_files = [f for f in os.listdir(directory) if re.match(r'.*\\.csv', f)]\n    data = [pd.read_csv(os.path.join(directory, csv_file)) for csv_file in csv_files]\n\n    return data\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nnumpy, pandas, datetime, os\n```python\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\nimport os\n\n# Constants\nDIRECTORY = \"/path/to/directory\"\nDATE_PATTERN = \"%Y%m%d\"\n\ndef load_csv_files_with_date(directory=DIRECTORY, date_pattern=DATE_PATTERN):\n    \"\"\"\n    Load all CSV files in a directory into a list of pandas DataFrames,\n    and add a new column 'Date' extracted from the file name.\n\n    Parameters:\n    directory (str): The directory to scan for CSV files.\n    date_pattern (str): The date pattern in the file name.\n\n    Returns:\n    list: A list of pandas DataFrames.\n\n    Requirements:\n    - numpy\n    - pandas\n    - datetime\n    - os\n\n    Example:\n    >>> dataframes = load_csv_files_with_date()\n    >>> print(len(dataframes))\n    \"\"\"\n    csv_files = [f for f in os.listdir(directory) if f.endswith('.csv')]\n    data = []\n\n    for csv_file in csv_files:\n        date_str = re.findall(r'\\d+', csv_file)[0]\n        date = datetime.strptime(date_str, date_pattern)\n        df = pd.read_csv(os.path.join(directory, csv_file))\n        df['Date'] = date\n        data.append(df)\n\n    return data\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, os, csv, glob\n```python\nimport pandas as pd\nimport os\nimport csv\nimport glob\n\n# Constants\nDIRECTORY = \"/path/to/directory\"\n\ndef load_and_concatenate_csv_files(directory=DIRECTORY):\n    \"\"\"\n    Load all CSV files in a directory into a single pandas DataFrame.\n\n    Parameters:\n    directory (str): The directory to scan for CSV files.\n\n    Returns:\n    DataFrame: A pandas DataFrame.\n\n    Requirements:\n    - pandas\n    - os\n    - csv\n    - glob\n\n    Example:\n    >>> df = load_and_concatenate_csv_files()\n    >>> print(df.shape)\n    \"\"\"\n    csv_files = glob.glob(os.path.join(directory, \"*.csv\"))\n    data = pd.concat([pd.read_csv(csv_file) for csv_file in csv_files])\n\n    return data\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, os, re, csv, numpy\n```python\nimport pandas as pd\nimport os\nimport re\nimport csv\nimport numpy as np\n\n# Constants\nDIRECTORY = \"/path/to/directory\"\n\ndef load_csv_files_and_calculate_mean(directory=DIRECTORY):\n    \"\"\"\n    Load all CSV files in a directory into a list of pandas DataFrames,\n    and calculate the mean of each DataFrame.\n\n    Parameters:\n    directory (str): The directory to scan for CSV files.\n\n    Returns:\n    list: A list of means.\n\n    Requirements:\n    - pandas\n    - os\n    - re\n    - csv\n    - numpy\n\n    Example:\n    >>> means = load_csv_files_and_calculate_mean()\n    >>> print(means)\n    \"\"\"\n    csv_files = [f for f in os.listdir(directory) if re.match(r'.*\\.csv', f)]\n    data = [pd.read_csv(os.path.join(directory, csv_file)) for csv_file in csv_files]\n    means = [np.mean(df) for df in data]\n\n    return means\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, os, csv, matplotlib\n```python\nimport pandas as pd\nimport os\nimport csv\nimport matplotlib.pyplot as plt\n\n# Constants\nDIRECTORY = \"/path/to/directory\"\n\ndef load_csv_files_and_plot(directory=DIRECTORY):\n    \"\"\"\n    Load the first CSV file in a directory into a pandas DataFrame,\n    and plot the data.\n\n    Parameters:\n    directory (str): The directory to scan for CSV files.\n\n    Returns:\n    DataFrame: A pandas DataFrame.\n\n    Requirements:\n    - pandas\n    - os\n    - csv\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = load_csv_files_and_plot()\n    >>> print(df.shape)\n    \"\"\"\n    csv_file = next(f for f in os.listdir(directory) if f.endswith('.csv'))\n    data = pd.read_csv(os.path.join(directory, csv_file))\n    data.plot(kind='line')\n\n    return data\n```\n"}
{"seed": "{\"task_id\": 20549, \"prompt\": \"def f_20549(vectorized):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"numpy.save('my_vector.npy', vectorized.toarray())\", \"test_start\": \"\\nimport os\\nimport numpy\\nfrom sklearn.feature_extraction import DictVectorizer\\n\\ndef check(candidate):\", \"test\": [\"\\n    measurements = [\\n        {'city': 'Dubai', 'temperature': 33.},\\n        {'city': 'London', 'temperature': 12.},\\n        {'city': 'San Francisco', 'temperature': 18.},\\n    ]\\n    vec = DictVectorizer()\\n    candidate(vec.fit_transform(measurements))\\n    assert os.path.exists('my_vector.npy')\\n\"], \"entry_point\": \"f_20549\", \"intent\": \"\\u5b66\\u7fd2\\u30c7\\u30fc\\u30bf\\u306e\\u30d9\\u30af\\u30c8\\u30eb`vectorized`\\u3092\\u30d5\\u30a1\\u30a4\\u30eb'my_vector.npy`\\u306b\\u4fdd\\u5b58\\u3059\\u308b\", \"library\": [\"numpy\", \"os\", \"sklearn\"]}", "generation": "Scenario 1: numpy, os, sklearn, pandas, matplotlib\n```python\nimport numpy as np\nimport os\nfrom sklearn.feature_extraction import DictVectorizer\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nFILE_NAME = 'my_vector.npy'\n\ndef save_and_plot(vectorized):\n    \"\"\"\n    Save the learned data vector to a file and plot the values of the vectorized data.\n\n    Parameters:\n    vectorized (scipy.sparse.csr.csr_matrix): The vectorized data.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - os\n    - sklearn\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> measurements = [{'city': 'Dubai', 'temperature': 33.}, {'city': 'London', 'temperature': 12.}, {'city': 'San Francisco', 'temperature': 18.}]\n    >>> vec = DictVectorizer()\n    >>> vectorized = vec.fit_transform(measurements)\n    >>> save_and_plot(vectorized)\n    \"\"\"\n    np.save(FILE_NAME, vectorized.toarray())\n    df = pd.DataFrame(vectorized.toarray())\n    df.plot(kind='bar')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: numpy, os, sklearn, pandas, seaborn\n```python\nimport numpy as np\nimport os\nfrom sklearn.feature_extraction import DictVectorizer\nimport pandas as pd\nimport seaborn as sns\n\n# Constants\nFILE_NAME = 'my_vector.npy'\n\ndef save_and_heatmap(vectorized):\n    \"\"\"\n    Save the learned data vector to a file and plot a heatmap of the vectorized data.\n\n    Parameters:\n    vectorized (scipy.sparse.csr.csr_matrix): The vectorized data.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - os\n    - sklearn\n    - pandas\n    - seaborn\n\n    Example:\n    >>> measurements = [{'city': 'Dubai', 'temperature': 33.}, {'city': 'London', 'temperature': 12.}, {'city': 'San Francisco', 'temperature': 18.}]\n    >>> vec = DictVectorizer()\n    >>> vectorized = vec.fit_transform(measurements)\n    >>> save_and_heatmap(vectorized)\n    \"\"\"\n    np.save(FILE_NAME, vectorized.toarray())\n    df = pd.DataFrame(vectorized.toarray())\n    sns.heatmap(df, cmap='YlGnBu')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: numpy, os, sklearn, pandas, matplotlib\n```python\nimport numpy as np\nimport os\nfrom sklearn.feature_extraction import DictVectorizer\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nFILE_NAME = 'my_vector.npy'\n\ndef save_and_histogram(vectorized):\n    \"\"\"\n    Save the learned data vector to a file and plot a histogram of the vectorized data.\n\n    Parameters:\n    vectorized (scipy.sparse.csr.csr_matrix): The vectorized data.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - os\n    - sklearn\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> measurements = [{'city': 'Dubai', 'temperature': 33.}, {'city': 'London', 'temperature': 12.}, {'city': 'San Francisco', 'temperature': 18.}]\n    >>> vec = DictVectorizer()\n    >>> vectorized = vec.fit_transform(measurements)\n    >>> save_and_histogram(vectorized)\n    \"\"\"\n    np.save(FILE_NAME, vectorized.toarray())\n    df = pd.DataFrame(vectorized.toarray())\n    df.hist(bins=10)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: numpy, os, sklearn, pandas, matplotlib\n```python\nimport numpy as np\nimport os\nfrom sklearn.feature_extraction import DictVectorizer\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nFILE_NAME = 'my_vector.npy'\n\ndef save_and_boxplot(vectorized):\n    \"\"\"\n    Save the learned data vector to a file and plot a boxplot of the vectorized data.\n\n    Parameters:\n    vectorized (scipy.sparse.csr.csr_matrix): The vectorized data.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - os\n    - sklearn\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> measurements = [{'city': 'Dubai', 'temperature': 33.}, {'city': 'London', 'temperature': 12.}, {'city': 'San Francisco', 'temperature': 18.}]\n    >>> vec = DictVectorizer()\n    >>> vectorized = vec.fit_transform(measurements)\n    >>> save_and_boxplot(vectorized)\n    \"\"\"\n    np.save(FILE_NAME, vectorized.toarray())\n    df = pd.DataFrame(vectorized.toarray())\n    df.boxplot()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: numpy, os, sklearn, scipy, matplotlib\n```python\nimport numpy as np\nimport os\nfrom sklearn.feature_extraction import DictVectorizer\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\n# Constants\nFILE_NAME = 'my_vector.npy'\n\ndef save_and_density(vectorized):\n    \"\"\"\n    Save the learned data vector to a file and plot a density plot of the vectorized data.\n\n    Parameters:\n    vectorized (scipy.sparse.csr.csr_matrix): The vectorized data.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - os\n    - sklearn\n    - scipy\n    - matplotlib.pyplot\n\n    Example:\n    >>> measurements = [{'city': 'Dubai', 'temperature': 33.}, {'city': 'London', 'temperature': 12.}, {'city': 'San Francisco', 'temperature': 18.}]\n    >>> vec = DictVectorizer()\n    >>> vectorized = vec.fit_transform(measurements)\n    >>> save_and_density(vectorized)\n    \"\"\"\n    np.save(FILE_NAME, vectorized.toarray())\n    density = stats.gaussian_kde(vectorized.toarray().flatten())\n    x = np.linspace(0, max(vectorized.toarray().flatten()), 100)\n    plt.plot(x, density(x))\n    plt.show()\n```\n"}
{"seed": "{\"task_id\": 9518, \"prompt\": \"def f_9518(li):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"for i, name in enumerate(li):\\n\\t    print(i, name)\", \"test_start\": \"\\nimport sys\\n\\ndef check(candidate):\", \"test\": [\"\\n    file_name = 'output.txt'\\n    f = open(file_name, 'w')\\n    sys.stdout = f\\n    candidate([1, 3])\\n    f.close()\\n    with open (file_name, 'r') as f:\\n        lines = f.readlines()\\n        assert lines[0] == '0 1\\\\n'\\n        assert lines[1] == '1 3\\\\n'\\n        \\n    f = open(file_name, 'w')\\n    sys.stdout = f\\n    candidate(['abc', 'def'])\\n    f.close()\\n    with open (file_name, 'r') as f:\\n        lines = f.readlines()\\n        assert lines[0] == '0 abc\\\\n'\\n        assert lines[1] == '1 def\\\\n'\\n\"], \"entry_point\": \"f_9518\", \"intent\": \"\\u30ea\\u30b9\\u30c8'li'\\u306e\\u30a4\\u30f3\\u30c7\\u30c3\\u30af\\u30b9\\u3068\\u8981\\u7d20\\u306b\\u7e70\\u308a\\u8fd4\\u3057\\u51e6\\u7406\\u3092\\u884c\\u3063\\u3066\\u8868\\u793a\\u3059\\u308b\", \"library\": [\"sys\"]}", "generation": "Scenario 1: \npandas, numpy, matplotlib, random\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import sample\n\n# Constants\nCOLUMNS = ['Name', 'Age', 'Gender', 'Salary']\nGENDERS = ['Male', 'Female']\n\n# Sample data\nnames = ['John', 'Emma', 'Robert', 'Olivia', 'Michael', 'Sophia', 'James', 'Mia', 'David', 'Harper']\n\ndef draw_salary_distribution(data_size):\n    \"\"\"\n    Generate a DataFrame with specified number of rows and 4 columns (Name, Age, Gender, Salary) \n    and then draw the salary distribution for each gender.\n    \n    Parameters:\n    data_size (int): The number of rows in the DataFrame.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the generated data.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - random.sample\n    \n    Example:\n    >>> df = draw_salary_distribution(10)\n    >>> print(df)\n    \"\"\"\n    data = []\n    for _ in range(data_size):\n        name = sample(names, 1)[0]\n        age = np.random.randint(20, 60)\n        gender = sample(GENDERS, 1)[0]\n        salary = np.random.randint(3000, 10000)\n        data.append([name, age, gender, salary])\n\n    df = pd.DataFrame(data, columns=COLUMNS)\n    \n    plt.figure(figsize=(8, 6))\n    for gender in GENDERS:\n        plt.hist(df[df['Gender'] == gender]['Salary'], alpha=0.5, label=gender)\n\n    plt.legend()\n    plt.xlabel('Salary')\n    plt.ylabel('Count')\n    plt.title('Salary Distribution by Gender')\n    plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nnumpy, itertools, pandas, matplotlib\n```python\nimport numpy as np\nfrom itertools import combinations\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['Product A', 'Product B', 'Product C', 'Product D', 'Product E']\nCOMBO_SIZE = 2\n\ndef analyze_sales_combinations(sales_data, combo_size):\n    \"\"\"\n    Analyze the sales combinations and plot the top 5 combinations with the highest frequency.\n    \n    Parameters:\n    sales_data (DataFrame): The sales data where each row represents a transaction \n                            and each column represents a product. \n                            1 means the product is bought, 0 means not bought.\n    combo_size (int): The size of the combinations to consider.\n\n    Returns:\n    DataFrame: A DataFrame with the top 5 combinations and their counts.\n    \n    Requirements:\n    - numpy\n    - itertools.combinations\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> sales_data = pd.DataFrame(np.random.choice([0, 1], size=(1000, 5)), columns=COLUMNS)\n    >>> top_combos = analyze_sales_combinations(sales_data, COMBO_SIZE)\n    >>> print(top_combos)\n    \"\"\"\n    combo_counts = {}\n\n    for row in sales_data.itertuples(index=False):\n        bought_products = [product for product, bought in zip(COLUMNS, row) if bought == 1]\n        for combo in combinations(bought_products, combo_size):\n            combo = tuple(sorted(combo))\n            if combo in combo_counts:\n                combo_counts[combo] += 1\n            else:\n                combo_counts[combo] = 1\n\n    combo_counts_df = pd.DataFrame(list(combo_counts.items()), columns=['Combination', 'Count'])\n    top_combos = combo_counts_df.sort_values(by='Count', ascending=False).head(5)\n\n    plt.figure(figsize=(8, 6))\n    plt.barh(top_combos['Combination'].astype(str), top_combos['Count'], color='skyblue')\n    plt.xlabel('Count')\n    plt.ylabel('Combination')\n    plt.title('Top 5 Product Combinations')\n    plt.gca().invert_yaxis()\n    plt.show()\n\n    return top_combos\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnumpy, pandas, matplotlib, random\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import sample\n\n# Constants\nCOLUMNS = ['Name', 'Age', 'Height', 'Weight']\n\n# Sample data\nnames = ['John', 'Emma', 'Robert', 'Olivia', 'Michael', 'Sophia', 'James', 'Mia', 'David', 'Harper']\n\ndef draw_bmi_distribution(data_size):\n    \"\"\"\n    Generate a DataFrame with specified number of rows and 4 columns (Name, Age, Height, Weight) \n    and then draw the BMI distribution.\n    \n    Parameters:\n    data_size (int): The number of rows in the DataFrame.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the generated data.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - random.sample\n    \n    Example:\n    >>> df = draw_bmi_distribution(10)\n    >>> print(df)\n    \"\"\"\n    data = []\n    for _ in range(data_size):\n        name = sample(names, 1)[0]\n        age = np.random.randint(20, 60)\n        height = np.random.uniform(1.5, 2.0)\n        weight = np.random.uniform(50, 100)\n        data.append([name, age, height, weight])\n\n    df = pd.DataFrame(data, columns=COLUMNS)\n    df['BMI'] = df['Weight'] / (df['Height']**2)\n\n    plt.figure(figsize=(8, 6))\n    plt.hist(df['BMI'], bins=20, color='skyblue', edgecolor='black')\n    plt.xlabel('BMI')\n    plt.ylabel('Count')\n    plt.title('BMI Distribution')\n    plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \npandas, matplotlib, random, numpy\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import choice\nimport numpy as np\n\n# Constants\nCATEGORIES = ['Grocery', 'Utilities', 'Rent', 'Transport', 'Entertainment']\n\ndef analyze_expenses(num_days):\n    \"\"\"\n    Generate a DataFrame with daily expenses data for given number of days and plot the expenses by category.\n    \n    Parameters:\n    num_days (int): The number of days for which to generate the data.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the generated data.\n    \n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - random.choice\n    - numpy\n    \n    Example:\n    >>> df = analyze_expenses(365)\n    >>> print(df)\n    \"\"\"\n    dates = pd.date_range(start='2022-01-01', periods=num_days)\n    categories = [choice(CATEGORIES) for _ in range(num_days)]\n    amounts = np.random.uniform(10, 100, size=num_days)\n    \n    df = pd.DataFrame({'Date': dates, 'Category': categories, 'Amount': amounts})\n    \n    df.groupby('Category')['Amount'].sum().plot(kind='barh', color='skyblue', edgecolor='black')\n    plt.xlabel('Total Expense')\n    plt.ylabel('Category')\n    plt.title('Expenses by Category')\n    plt.show()\n    \n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, matplotlib, random, numpy\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import choice\nimport numpy as np\n\n# Constants\nCITIES = ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix']\n\ndef analyze_temperatures(num_days):\n    \"\"\"\n    Generate a DataFrame with daily temperature data for given number of days for various cities and \n    plot the average temperature by city.\n    \n    Parameters:\n    num_days (int): The number of days for which to generate the data.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the generated data.\n    \n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - random.choice\n    - numpy\n    \n    Example:\n    >>> df = analyze_temperatures(365)\n    >>> print(df)\n    \"\"\"\n    dates = pd.date_range(start='2022-01-01', periods=num_days)\n    cities = [choice(CITIES) for _ in range(num_days)]\n    temperatures = np.random.uniform(-10, 40, size=num_days)\n    \n    df = pd.DataFrame({'Date': dates, 'City': cities, 'Temperature': temperatures})\n    \n    df.groupby('City')['Temperature'].mean().plot(kind='barh', color='skyblue', edgecolor='black')\n    plt.xlabel('Average Temperature')\n    plt.ylabel('City')\n    plt.title('Average Temperature by City')\n    plt.show()\n    \n    return df\n```\n"}
{"seed": "{\"task_id\": 38760, \"prompt\": \"def f_38760(arr, n):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"arr[arr > n].sum(), numpy.sum(arr > n)\", \"test_start\": \"\\nimport numpy\\nimport numpy as np \\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(np.array([1,2,3,4]), 3) == (4, 1)\\n\"], \"entry_point\": \"f_38760\", \"intent\": \"numpy\\u914d\\u5217`arr`\\u306b\\u5bfe\\u3057\\u3066\\u6570\\u5024`n`\\u3088\\u308a\\u5927\\u304d\\u3044\\u8981\\u7d20\\u306e\\u5408\\u8a08\\u53ca\\u3073\\u500b\\u6570\\u3092\\u6c42\\u3081\\u3066\\u8868\\u793a\\u3059\\u308b\", \"library\": [\"numpy\"]}", "generation": "Scenario 1:\nnumpy, pandas, matplotlib, scikit-learn\n\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n# Constants\nMIN_VALUE = 0\nMAX_VALUE = 100\nSIZE = 50\n\ndef analyze_greater_than_value(arr, n):\n    \"\"\"\n    In a given numpy array 'arr', find the elements greater than a number 'n', \n    calculate their sum and count. Then, perform a linear regression on the \n    array with a new array that consists of the count of elements greater than \n    'n' in each bin when the array is divided into 10 equal bins, and plot the \n    regression line.\n    \n    Parameters:\n    arr (numpy.ndarray): The numpy array.\n    n (int): The number to be compared with.\n\n    Returns:\n    float, int: The sum and count of elements greater than 'n'.\n    numpy.ndarray: The coefficients of the linear regression.\n    matplotlib.figure.Figure: The figure object of the plot.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - sklearn.linear_model.LinearRegression\n\n    Example:\n    >>> arr = np.random.randint(MIN_VALUE, MAX_VALUE, SIZE)\n    >>> sum_, count, coeffs, fig = analyze_greater_than_value(arr, 50)\n    >>> print(sum_, count)\n    >>> print(coeffs)\n    >>> plt.show()\n    \"\"\"\n    greater_elements = arr[arr > n]\n    sum_ = greater_elements.sum()\n    count = len(greater_elements)\n\n    bins = pd.cut(arr, 10)\n    counts_in_bins = bins.value_counts().sort_index().values.reshape(-1, 1)\n    model = LinearRegression().fit(counts_in_bins, np.arange(10).reshape(-1, 1))\n    coeffs = model.coef_\n\n    fig, ax = plt.subplots()\n    ax.plot(counts_in_bins, model.predict(counts_in_bins), color='red')\n\n    return sum_, count, coeffs, fig\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, scipy.stats, matplotlib, seaborn\n\n```python\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nSIZE = 100\n\ndef analyze_distribution(arr, n):\n    \"\"\"\n    In a given numpy array 'arr', find the elements greater than a number 'n', \n    calculate their sum and count. Then, perform a normal test to check the \n    normality of the array, and plot the distribution of the array.\n    \n    Parameters:\n    arr (numpy.ndarray): The numpy array.\n    n (int): The number to be compared with.\n\n    Returns:\n    float, int: The sum and count of elements greater than 'n'.\n    float: The p-value of the normal test.\n    matplotlib.figure.Figure: The figure object of the plot.\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> arr = np.random.normal(0, 1, SIZE)\n    >>> sum_, count, p_value, fig = analyze_distribution(arr, 0)\n    >>> print(sum_, count)\n    >>> print(p_value)\n    >>> plt.show()\n    \"\"\"\n    greater_elements = arr[arr > n]\n    sum_ = greater_elements.sum()\n    count = len(greater_elements)\n\n    _, p_value = stats.normaltest(arr)\n\n    fig, ax = plt.subplots()\n    sns.distplot(arr, ax=ax)\n\n    return sum_, count, p_value, fig\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, matplotlib, sklearn.decomposition, sklearn.preprocessing\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\n# Constants\nMIN_VALUE = 0\nMAX_VALUE = 100\nSIZE = 200\nN_FEATURES = 10\nCOMPONENTS = 2\n\ndef analyze_and_decompose(arr, n):\n    \"\"\"\n    In a given numpy array 'arr', find the elements greater than a number 'n',\n    calculate their sum and count. Then, perform PCA on the array assuming it \n    has 'N_FEATURES' features, and plot the first two principal components.\n    \n    Parameters:\n    arr (numpy.ndarray): The numpy array.\n    n (int): The number to be compared with.\n\n    Returns:\n    float, int: The sum and count of elements greater than 'n'.\n    numpy.ndarray: The transformed array after PCA.\n    matplotlib.figure.Figure: The figure object of the plot.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.decomposition.PCA\n    - sklearn.preprocessing.StandardScaler\n\n    Example:\n    >>> arr = np.random.randint(MIN_VALUE, MAX_VALUE, (SIZE, N_FEATURES))\n    >>> sum_, count, transformed, fig = analyze_and_decompose(arr, 50)\n    >>> print(sum_, count)\n    >>> print(transformed)\n    >>> plt.show()\n    \"\"\"\n    greater_elements = arr[arr > n]\n    sum_ = greater_elements.sum()\n    count = len(greater_elements)\n\n    arr_scaled = StandardScaler().fit_transform(arr)\n    pca = PCA(n_components=COMPONENTS)\n    transformed = pca.fit_transform(arr_scaled)\n\n    fig, ax = plt.subplots()\n    ax.scatter(transformed[:, 0], transformed[:, 1])\n\n    return sum_, count, transformed, fig\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, pandas, seaborn, matplotlib\n\n```python\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nMIN_VALUE = 0\nMAX_VALUE = 100\nSIZE = 1000\nN_COLS = 10\n\ndef analyze_and_correlate(arr, n):\n    \"\"\"\n    In a given numpy array 'arr', find the elements greater than a number 'n',\n    calculate their sum and count. Then, create a DataFrame from 'arr' assuming \n    it has 'N_COLS' columns, calculate the correlation matrix, and plot a heatmap \n    of the correlation matrix.\n    \n    Parameters:\n    arr (numpy.ndarray): The numpy array.\n    n (int): The number to be compared with.\n\n    Returns:\n    float, int: The sum and count of elements greater than 'n'.\n    pandas.DataFrame: The correlation matrix.\n    matplotlib.figure.Figure: The figure object of the plot.\n\n    Requirements:\n    - numpy\n    - pandas\n    - seaborn\n    - matplotlib.pyplot\n\n    Example:\n    >>> arr = np.random.randint(MIN_VALUE, MAX_VALUE, (SIZE, N_COLS))\n    >>> sum_, count, corr_matrix, fig = analyze_and_correlate(arr, 50)\n    >>> print(sum_, count)\n    >>> print(corr_matrix)\n    >>> plt.show()\n    \"\"\"\n    greater_elements = arr[arr > n]\n    sum_ = greater_elements.sum()\n    count = len(greater_elements)\n\n    df = pd.DataFrame(arr)\n    corr_matrix = df.corr()\n\n    fig, ax = plt.subplots()\n    sns.heatmap(corr_matrix, ax=ax)\n\n    return sum_, count, corr_matrix, fig\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, matplotlib, scipy.signal\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import signal\n\n# Constants\nMIN_VALUE = 0\nMAX_VALUE = 100\nSIZE = 1000\n\ndef analyze_and_detrend(arr, n):\n    \"\"\"\n    In a given numpy array 'arr', find the elements greater than a number 'n', \n    calculate their sum and count. Then, detrend the array, and plot the \n    original and detrended array.\n    \n    Parameters:\n    arr (numpy.ndarray): The numpy array.\n    n (int): The number to be compared with.\n\n    Returns:\n    float, int: The sum and count of elements greater than 'n'.\n    numpy.ndarray: The detrended array.\n    matplotlib.figure.Figure: The figure object of the plot.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - scipy.signal\n\n    Example:\n    >>> arr = np.random.randint(MIN_VALUE, MAX_VALUE, SIZE)\n    >>> sum_, count, detrended, fig = analyze_and_detrend(arr, 50)\n    >>> print(sum_, count)\n    >>> print(detrended)\n    >>> plt.show()\n    \"\"\"\n    greater_elements = arr[arr > n]\n    sum_ = greater_elements.sum()\n    count = len(greater_elements)\n\n    detrended = signal.detrend(arr)\n\n    fig, ax = plt.subplots()\n    ax.plot(arr, label='Original')\n    ax.plot(detrended, label='Detrended')\n    ax.legend()\n\n    return sum_, count, detrended, fig\n```\n"}
{"seed": "{\"task_id\": 35102, \"prompt\": \"def f_35102(data):\\n\\t\", \"suffix\": \"\\n\\treturn results\", \"canonical_solution\": \"results = {}\\n\\tfor item in data:\\n\\t    results[item.find('areacode').text] = item.find('prefecture').text\", \"test_start\": \"\\nimport xml.etree.ElementTree as ET\\n\\ndef check(candidate):\", \"test\": [\"\\n    data_temp = [\\n        '<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\" ?><root><areacode>area1</areacode><prefecture>prefecture1</prefecture></root>', \\n        '<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\" ?><root><areacode>area2</areacode><prefecture>prefecture2</prefecture></root>'\\n    ]\\n    data = []\\n    for tr in data_temp:\\n        data.append(ET.ElementTree(ET.fromstring(tr)))\\n    \\n    res = candidate(data)\\n    assert \\\"area1\\\" in res\\n    assert \\\"area2\\\" in res\\n    assert \\\"prefecture1\\\" == res[\\\"area1\\\"]\\n    assert \\\"prefecture2\\\" == res[\\\"area2\\\"]\\n\"], \"entry_point\": \"f_35102\", \"intent\": \"\\u30a4\\u30c6\\u30e9\\u30d6\\u30eb\\u30aa\\u30d6\\u30b8\\u30a7\\u30af\\u30c8`data`\\u306e\\u8981\\u7d20\\u304b\\u3089\\u6587\\u5b57\\u5217`area_code`\\u3068`prefecture`\\u3092\\u63a2\\u3057\\u3001\\u305d\\u308c\\u305e\\u308c\\u30ad\\u30fc\\u3068\\u8981\\u7d20\\u306b\\u6301\\u3064\\u8f9e\\u66f8`results`\\u3092\\u4f5c\\u308b\", \"library\": [\"xml\"]}", "generation": "Scenario 1:\nxml.etree.ElementTree, pandas, matplotlib, numpy\n```python\nimport xml.etree.ElementTree as ET\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef analyze_population_xml(data):\n    \"\"\"\n    Parse XML data and create a pandas DataFrame. Then, analyze population data and \n    plot histogram of the distribution.\n    \n    Parameters:\n    data (list): A list of XML strings. Each XML string represents area code, prefecture \n                 and population.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with area code, prefecture and population.\n    \n    Requirements:\n    - xml.etree.ElementTree\n    - pandas\n    - matplotlib.pyplot\n    - numpy\n    \n    Example:\n    >>> data = [\n    ...     '<?xml version=\"1.0\" encoding=\"UTF-8\" ?><root><areacode>area1</areacode><prefecture>pref1</prefecture><population>10000</population></root>', \n    ...     '<?xml version=\"1.0\" encoding=\"UTF-8\" ?><root><areacode>area2</areacode><prefecture>pref2</prefecture><population>20000</population></root>'\n    ... ]\n    >>> df = analyze_population_xml(data)\n    >>> print(df)\n    >>> plt.hist(df['Population'], bins=np.arange(0, 30000, 1000))\n    \"\"\"\n    result = []\n    for item in data:\n        root = ET.fromstring(item)\n        area_code = root.find('areacode').text\n        prefecture = root.find('prefecture').text\n        population = int(root.find('population').text)\n        result.append([area_code, prefecture, population])\n    \n    df = pd.DataFrame(result, columns=['Area Code', 'Prefecture', 'Population'])\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nxml.etree.ElementTree, random, datetime, matplotlib\n```python\nimport xml.etree.ElementTree as ET\nfrom random import choice\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\n\n# Constants\nMONTHS = ['January', 'February', 'March', 'April', 'May', 'June', \n          'July', 'August', 'September', 'October', 'November', 'December']\n\ndef analyze_sales_xml(data):\n    \"\"\"\n    Parse XML data and create a dictionary. Then, analyze sales data and \n    plot the sales trend for a randomly selected month.\n\n    Parameters:\n    data (list): A list of XML strings. Each XML string represents area code, month \n                 and sales.\n\n    Returns:\n    Dict: A dictionary with month as key and sales as value.\n\n    Requirements:\n    - xml.etree.ElementTree\n    - random\n    - datetime\n    - matplotlib.pyplot\n\n    Example:\n    >>> data = [\n    ...     '<?xml version=\"1.0\" encoding=\"UTF-8\" ?><root><areacode>area1</areacode><month>January</month><sales>10000</sales></root>', \n    ...     '<?xml version=\"1.0\" encoding=\"UTF-8\" ?><root><areacode>area2</areacode><month>February</month><sales>20000</sales></root>'\n    ... ]\n    >>> sales_dict = analyze_sales_xml(data)\n    >>> print(sales_dict)\n    >>> plt.plot(list(sales_dict.keys()), list(sales_dict.values()))\n    \"\"\"\n    sales_dict = {}\n    for item in data:\n        root = ET.fromstring(item)\n        area_code = root.find('areacode').text\n        month = root.find('month').text\n        sales = int(root.find('sales').text)\n        if month in sales_dict:\n            sales_dict[month] += sales\n        else:\n            sales_dict[month] = sales\n\n    random_month = choice(MONTHS)\n    print(f\"Sales trend for {random_month}:\")\n    for month in sales_dict:\n        if month == random_month:\n            print(f\"{month}: {sales_dict[month]}\")\n\n    return sales_dict\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nxml.etree.ElementTree, os, hashlib\n```python\nimport xml.etree.ElementTree as ET\nimport os\nimport hashlib\n\ndef verify_xml_files(data, dir_path):\n    \"\"\"\n    Parse XML data and create a dictionary. Then, compare the hashes of the XML content \n    with the hashes of the files in a given directory.\n\n    Parameters:\n    data (list): A list of XML strings. Each XML string represents area code and prefecture.\n    dir_path (str): The directory path where the XML files are located.\n\n    Returns:\n    List: A list of file names that have matching hashes.\n\n    Requirements:\n    - xml.etree.ElementTree\n    - os\n    - hashlib\n\n    Example:\n    >>> data = [\n    ...     '<?xml version=\"1.0\" encoding=\"UTF-8\" ?><root><areacode>area1</areacode><prefecture>pref1</prefecture></root>', \n    ...     '<?xml version=\"1.0\" encoding=\"UTF-8\" ?><root><areacode>area2</areacode><prefecture>pref2</prefecture></root>'\n    ... ]\n    >>> verify_xml_files(data, '/path/to/xml/files')\n    \"\"\"\n    xml_hashes = []\n    for item in data:\n        hash_object = hashlib.md5(item.encode())\n        xml_hashes.append(hash_object.hexdigest())\n\n    matching_files = []\n    for filename in os.listdir(dir_path):\n        if filename.endswith(\".xml\"):\n            with open(os.path.join(dir_path, filename), \"r\") as file:\n                content = file.read()\n                hash_object = hashlib.md5(content.encode())\n                if hash_object.hexdigest() in xml_hashes:\n                    matching_files.append(filename)\n\n    return matching_files\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nxml.etree.ElementTree, pandas, matplotlib, seaborn\n```python\nimport xml.etree.ElementTree as ET\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef analyze_students_xml(data):\n    \"\"\"\n    Parse XML data and create a pandas DataFrame. Then, analyze students data and \n    plot a boxplot for the scores.\n\n    Parameters:\n    data (list): A list of XML strings. Each XML string represents student id, name \n                 and score.\n\n    Returns:\n    DataFrame: A pandas DataFrame with student id, name and score.\n\n    Requirements:\n    - xml.etree.ElementTree\n    - pandas\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> data = [\n    ...     '<?xml version=\"1.0\" encoding=\"UTF-8\" ?><root><id>1</id><name>John</name><score>85</score></root>', \n    ...     '<?xml version=\"1.0\" encoding=\"UTF-8\" ?><root><id>2</id><name>Jane</name><score>90</score></root>'\n    ... ]\n    >>> df = analyze_students_xml(data)\n    >>> print(df)\n    >>> sns.boxplot(x=df['Score'])\n    \"\"\"\n    result = []\n    for item in data:\n        root = ET.fromstring(item)\n        id = int(root.find('id').text)\n        name = root.find('name').text\n        score = int(root.find('score').text)\n        result.append([id, name, score])\n    \n    df = pd.DataFrame(result, columns=['ID', 'Name', 'Score'])\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nxml.etree.ElementTree, re, collections\n```python\nimport xml.etree.ElementTree as ET\nimport re\nimport collections\n\ndef analyze_text_xml(data):\n    \"\"\"\n    Parse XML data and create a dictionary with word frequency. Then, analyze the text data \n    and return the ten most common words.\n\n    Parameters:\n    data (list): A list of XML strings. Each XML string represents text id and text content.\n\n    Returns:\n    List: A list of tuples with the ten most common words and their frequencies.\n\n    Requirements:\n    - xml.etree.ElementTree\n    - re\n    - collections\n\n    Example:\n    >>> data = [\n    ...     '<?xml version=\"1.0\" encoding=\"UTF-8\" ?><root><id>1</id><text>Hello world</text></root>', \n    ...     '<?xml version=\"1.0\" encoding=\"UTF-8\" ?><root><id>2</id><text>World is beautiful</text></root>'\n    ... ]\n    >>> analyze_text_xml(data)\n    \"\"\"\n    word_freq = collections.Counter()\n    for item in data:\n        root = ET.fromstring(item)\n        id = int(root.find('id').text)\n        text = root.find('text').text.lower()\n        words = re.findall(r'\\w+', text)\n        word_freq.update(words)\n\n    return word_freq.most_common(10)\n```"}
{"seed": "{\"task_id\": 41440, \"prompt\": \"def f_41440(arr_list):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"np.stack(arr_list)\", \"test_start\": \"\\nimport numpy as np \\n\\ndef check(candidate):\", \"test\": [\"\\n    arr_list = [np.array([1,2]), np.array([3,4]), np.array([5,6])]\\n    assert candidate(arr_list).tolist() == [[1, 2], [3, 4], [5, 6]]\\n\"], \"entry_point\": \"f_41440\", \"intent\": \"\\u8981\\u7d20\\u304cNumPy\\u914d\\u5217\\u306e\\u30ea\\u30b9\\u30c8`arr_list`\\u30922\\u6b21\\u5143\\u306eNumPy\\u914d\\u5217\\u306b\\u5909\\u63db\\u3059\\u308b\", \"library\": [\"numpy\"]}", "generation": "Scenario 1: \nnumpy, scipy, matplotlib, random\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom random import randint\n\n# Constants\nX_LIM = (-10, 10)\nY_LIM = (-10, 10)\nSAMPLE_SIZE = 50\n\ndef generate_scatter_plot(arr_list):\n    \"\"\"\n    Generate a scatter plot from a list of 2D numpy arrays. Each array represents \n    a different set of points and is plotted with a different color.\n    \n    Parameters:\n    arr_list (list): A list of 2D numpy arrays.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - numpy\n    - scipy\n    - matplotlib.pyplot\n    - random\n    \n    Example:\n    >>> arr_list = [np.random.randn(SAMPLE_SIZE, 2) for _ in range(3)]\n    >>> generate_scatter_plot(arr_list)\n    \"\"\"\n    colors = ['r', 'g', 'b', 'y', 'm', 'c']\n    \n    for idx, arr in enumerate(arr_list):\n        x = arr[:, 0]\n        y = arr[:, 1]\n        color = colors[idx % len(colors)]\n        plt.scatter(x, y, c=color)\n        \n    plt.xlim(X_LIM)\n    plt.ylim(Y_LIM)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nnumpy, matplotlib, pandas\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants\nCOLUMNS = ['A', 'B', 'C']\n\ndef generate_line_plot(arr_list):\n    \"\"\"\n    Generate a line plot from a list of 1D numpy arrays. Each array represents \n    a different line in the plot.\n    \n    Parameters:\n    arr_list (list): A list of 1D numpy arrays.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - pandas\n    \n    Example:\n    >>> arr_list = [np.random.randn(50) for _ in range(3)]\n    >>> generate_line_plot(arr_list)\n    \"\"\"\n    df = pd.DataFrame(np.stack(arr_list).T, columns=COLUMNS)\n    df.plot(kind='line')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnumpy, seaborn, pandas\n```python\nimport numpy as np\nimport seaborn as sns\nimport pandas as pd\n\n# Constants\nCOLUMNS = ['A', 'B', 'C']\n\ndef generate_heatmap(arr_list):\n    \"\"\"\n    Generate a heatmap from a list of 2D numpy arrays. Each array is stacked \n    horizontally to form a bigger array for the heatmap.\n    \n    Parameters:\n    arr_list (list): A list of 2D numpy arrays.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - numpy\n    - seaborn\n    - pandas\n    \n    Example:\n    >>> arr_list = [np.random.randn(50, 50) for _ in range(3)]\n    >>> generate_heatmap(arr_list)\n    \"\"\"\n    combined_arr = np.hstack(arr_list)\n    df = pd.DataFrame(combined_arr)\n    sns.heatmap(df)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, matplotlib, random\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nCOLORS = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n\ndef generate_bar_plot(arr_list):\n    \"\"\"\n    Generate a bar plot from a list of 1D numpy arrays. Each array represents \n    a different bar in the plot.\n    \n    Parameters:\n    arr_list (list): A list of 1D numpy arrays.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - random\n    \n    Example:\n    >>> arr_list = [np.random.rand(10) for _ in range(3)]\n    >>> generate_bar_plot(arr_list)\n    \"\"\"\n    for idx, arr in enumerate(arr_list):\n        color = COLORS[randint(0, len(COLORS) - 1)]\n        plt.bar(np.arange(len(arr)) + idx*0.2, arr, width=0.2, color=color)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnumpy, sklearn, matplotlib\n```python\nimport numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\n# Constants\nN_COMPONENTS = 2\n\ndef apply_pca_and_plot(arr_list):\n    \"\"\"\n    Apply PCA on a 2D numpy array stacked from a list of 1D numpy arrays and \n    plot the first two principal components.\n    \n    Parameters:\n    arr_list (list): A list of 1D numpy arrays.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - numpy\n    - sklearn.decomposition.PCA\n    - matplotlib.pyplot\n    \n    Example:\n    >>> arr_list = [np.random.randn(100) for _ in range(3)]\n    >>> apply_pca_and_plot(arr_list)\n    \"\"\"\n    pca = PCA(n_components=N_COMPONENTS)\n    transformed_data = pca.fit_transform(np.stack(arr_list).T)\n    \n    plt.scatter(transformed_data[:, 0], transformed_data[:, 1])\n    plt.show()\n```\n`GPT_ODEX_BREAK`"}
{"seed": "{\"task_id\": 35741, \"prompt\": \"def f_35741(soup):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"soup.find_all('p')\", \"test_start\": \"\\nfrom bs4 import BeautifulSoup\\n\\ndef check(candidate):\", \"test\": [\"\\n    soup = BeautifulSoup('<p>text</p>')\\n    assert candidate(soup)[0].contents == ['text']\\n\"], \"entry_point\": \"f_35741\", \"intent\": \"HTML\\u3092\\u30d1\\u30fc\\u30b9\\u3057\\u305f\\u30aa\\u30d6\\u30b8\\u30a7\\u30af\\u30c8`soup`\\u304b\\u3089\\u30bf\\u30b0`p`\\u3092\\u3059\\u3079\\u3066\\u898b\\u3064\\u3051\\u308b\", \"library\": [\"bs4\"]}", "generation": "Scenario 3:\nbs4, re, urllib, codecs\n```python\nimport re\nfrom urllib.request import urlopen\nfrom bs4 import BeautifulSoup\nimport codecs\n\ndef find_urls_with_paragraphs(url):\n    \"\"\"\n    Find and return all URLs within a webpage which contain paragraphs (`p` tag).\n\n    Parameters:\n    url (str): The URL of the webpage.\n\n    Returns:\n    list: A list of URLs that contain paragraphs.\n\n    Requirements:\n    - bs4\n    - urllib.request\n    - re\n    - codecs\n\n    Example:\n    >>> find_urls_with_paragraphs('https://www.example.com')\n    \"\"\"\n    html = urlopen(url)\n    soup = BeautifulSoup(html, 'html.parser')\n\n    urls_with_paragraphs = []\n    for link in soup.find_all('a'):\n        href = link.get('href')\n\n        try:\n            sub_html = urlopen(href)\n            sub_soup = BeautifulSoup(sub_html, 'html.parser')\n            if sub_soup.find('p'):\n                urls_with_paragraphs.append(href)\n        except:\n            pass\n\n    return urls_with_paragraphs\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nbs4, urllib, lxml, pandas\n```python\nimport pandas as pd\nfrom urllib.request import urlopen\nfrom bs4 import BeautifulSoup\n\ndef scrape_table(url):\n    \"\"\"\n    Scrape all tables on a webpage and return them as pandas DataFrame objects.\n\n    Parameters:\n    url (str): The URL of the webpage.\n\n    Returns:\n    list: A list of pandas DataFrame objects that represent the tables.\n\n    Requirements:\n    - bs4\n    - urllib.request\n    - pandas\n\n    Example:\n    >>> scrape_table('https://www.example.com')\n    \"\"\"\n    html = urlopen(url)\n    soup = BeautifulSoup(html, 'lxml')\n\n    tables = []\n    for table in soup.find_all('table'):\n        df = pd.read_html(str(table))[0]\n        tables.append(df)\n\n    return tables\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nbs4, requests, urllib, os\n```python\nimport os\nimport requests\nfrom urllib.parse import urljoin\nfrom bs4 import BeautifulSoup\n\ndef download_images(url, folder):\n    \"\"\"\n    Download all images from a webpage to a specific folder.\n\n    Parameters:\n    url (str): The URL of the webpage.\n    folder (str): The folder to download the images to.\n\n    Requirements:\n    - bs4\n    - requests\n    - urllib.parse\n    - os\n\n    Example:\n    >>> download_images('https://www.example.com', '/path/to/folder')\n    \"\"\"\n    if not os.path.exists(folder):\n        os.makedirs(folder)\n\n    response = requests.get(url)\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    for img in soup.find_all('img'):\n        img_url = urljoin(url, img.get('src'))\n        file_name = os.path.join(folder, img_url.split(\"/\")[-1])\n\n        with open(file_name, 'wb') as file:\n            response = requests.get(img_url)\n            file.write(response.content)\n```\n`GPT_ODEX_BREAK`\n\nScenario 6:\nbs4, urllib, lxml, csv\n```python\nimport csv\nfrom urllib.request import urlopen\nfrom bs4 import BeautifulSoup\n\ndef extract_links_to_csv(url, file_path):\n    \"\"\"\n    Extract all links from a webpage and save them to a CSV file.\n\n    Parameters:\n    url (str): The URL of the webpage.\n    file_path (str): The path to the CSV file.\n\n    Requirements:\n    - bs4\n    - urllib.request\n    - lxml\n    - csv\n\n    Example:\n    >>> extract_links_to_csv('https://www.example.com', '/path/to/file.csv')\n    \"\"\"\n    html = urlopen(url)\n    soup = BeautifulSoup(html, 'lxml')\n\n    with open(file_path, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Link Text', 'URL'])\n\n        for link in soup.find_all('a'):\n            writer.writerow([link.text, link.get('href')])\n```\n`GPT_ODEX_BREAK`\n\nScenario 7:\nbs4, urllib, lxml, pandas\n```python\nimport pandas as pd\nfrom urllib.request import urlopen\nfrom bs4 import BeautifulSoup\n\ndef extract_data_from_html_table(url, table_id):\n    \"\"\"\n    Extract data from a specific HTML table on a webpage and return it as a pandas DataFrame.\n\n    Parameters:\n    url (str): The URL of the webpage.\n    table_id (str): The ID of the table.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the table data.\n\n    Requirements:\n    - bs4\n    - urllib.request\n    - lxml\n    - pandas\n\n    Example:\n    >>> extract_data_from_html_table('https://www.example.com', 'table_id')\n    \"\"\"\n    html = urlopen(url)\n    soup = BeautifulSoup(html, 'lxml')\n\n    table = soup.find('table', {'id': table_id})\n    df = pd.read_html(str(table))[0]\n\n    return df\n```\n`GPT_ODEX_BREAK`"}
{"seed": "{\"task_id\": 40444, \"prompt\": \"def f_40444(df, c_label):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"df.groupby([c_label]).last()\", \"test_start\": \"\\nimport pandas as pd\\n\\ndef check(candidate):\", \"test\": [\"\\n    df = pd.DataFrame({'\\u5229\\u7528\\u8005ID': [1, 2], '\\u30b3\\u30f3\\u30c6\\u30f3\\u30c4ID': ['a', 'b'], '\\u5024': [170, 45]})\\n    c_label = ['\\u5229\\u7528\\u8005ID', '\\u30b3\\u30f3\\u30c6\\u30f3\\u30c4ID']\\n    assert candidate(df, c_label).equals(df.groupby([c_label]).last())\\n\"], \"entry_point\": \"f_40444\", \"intent\": \"\\u30c7\\u30fc\\u30bf\\u30d5\\u30ec\\u30fc\\u30e0`df`\\u306e\\u5217`c_label`\\u3092Groupby\\u3067\\u307e\\u3068\\u3081\\u305f\\u30c7\\u30fc\\u30bf\\u306e\\u6700\\u5f8c\\u306e\\u884c\\u3092\\u53d6\\u308a\\u51fa\\u3059\", \"library\": [\"pandas\"]}", "generation": "Scenario 1: \npandas, numpy, matplotlib, seaborn, random\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom random import choice\n\n# Constants\nPRODUCTS = ['TV', 'Laptop', 'Mobile', 'Tablet', 'Headphone', 'Camera']\nTIME_PERIODS = ['Q1', 'Q2', 'Q3', 'Q4']\n\ndef generate_sales_report(df, c_label):\n    \"\"\"\n    Generate a quarterly sales report for different products based on the provided data.\n    The function will return the last sales data for each quarter and also plot a \n    heatmap showing the sales data.\n\n    Parameters:\n    df (DataFrame): The sales data.\n    c_label (list): The columns to group by.\n\n    Returns:\n    DataFrame: The last sales data for each product for each quarter.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - seaborn\n    - random\n\n    Example:\n    >>> df = pd.DataFrame({\n    ...     'Product': [choice(PRODUCTS) for _ in range(100)],\n    ...     'Quarter': [choice(TIME_PERIODS) for _ in range(100)],\n    ...     'Sales': np.random.randint(50, 500, 100)\n    ... })\n    >>> sales_report = generate_sales_report(df, ['Product', 'Quarter'])\n    >>> print(sales_report)\n    >>> sns.heatmap(sales_report.unstack(), annot=True, fmt=\".0f\")\n    \"\"\"\n    sales_report = df.groupby(c_label).last().unstack()\n    return sales_report\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, sklearn.preprocessing, sklearn.cluster\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\n\n# Constants\nN_CLUSTERS = 3\n\ndef customer_segmentation(df, c_label):\n    \"\"\"\n    Perform customer segmentation based on the provided customer data.\n    The function will group the data based on the provided columns, calculate the \n    average of the remaining columns, standardize the data and apply KMeans clustering.\n\n    Parameters:\n    df (DataFrame): The customer data.\n    c_label (list): The columns to group by.\n\n    Returns:\n    DataFrame: The customer data with an additional 'Cluster' column.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.preprocessing.StandardScaler\n    - sklearn.cluster.KMeans\n\n    Example:\n    >>> df = pd.DataFrame({\n    ...     'Customer ID': np.arange(1, 101),\n    ...     'Age': np.random.randint(20, 70, 100),\n    ...     'Income': np.random.randint(30000, 80000, 100),\n    ...     'Score': np.random.randint(1, 10, 100)\n    ... })\n    >>> segmented_customers = customer_segmentation(df, ['Customer ID'])\n    >>> print(segmented_customers)\n    \"\"\"\n    grouped_df = df.groupby(c_label).mean()\n\n    scaler = StandardScaler()\n    scaled_data = scaler.fit_transform(grouped_df)\n\n    kmeans = KMeans(n_clusters=N_CLUSTERS, random_state=0)\n    clusters = kmeans.fit_predict(scaled_data)\n\n    grouped_df['Cluster'] = clusters\n\n    return grouped_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, matplotlib, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef visualize_correlation(df, c_label):\n    \"\"\"\n    Visualize the correlation between different columns in the provided data.\n    The function will group the data based on the provided columns and calculate \n    the correlation between the remaining columns. It will then plot a heatmap of the correlation.\n\n    Parameters:\n    df (DataFrame): The data.\n    c_label (list): The columns to group by.\n\n    Returns:\n    DataFrame: The correlation matrix.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> df = pd.DataFrame({\n    ...     'A': np.random.rand(100),\n    ...     'B': np.random.rand(100),\n    ...     'C': np.random.rand(100),\n    ...     'D': np.random.rand(100),\n    ...     'E': np.random.rand(100)\n    ... })\n    >>> correlation = visualize_correlation(df, ['A', 'B'])\n    >>> print(correlation)\n    >>> sns.heatmap(correlation, annot=True, cmap='coolwarm')\n    \"\"\"\n    grouped_df = df.groupby(c_label).mean()\n    correlation = grouped_df.corr()\n\n    return correlation\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, sklearn.model_selection, sklearn.linear_model\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\ndef predict_sales(df, c_label, target):\n    \"\"\"\n    Predict the sales based on the provided data using a linear regression model.\n    The function will group the data based on the provided columns, split the data \n    into training and test sets, train a linear regression model, and make predictions.\n\n    Parameters:\n    df (DataFrame): The data.\n    c_label (list): The columns to group by.\n    target (str): The target column.\n\n    Returns:\n    float: The R-squared score of the model.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.model_selection.train_test_split\n    - sklearn.linear_model.LinearRegression\n\n    Example:\n    >>> df = pd.DataFrame({\n    ...     'Advertising': np.random.rand(100),\n    ...     'Price': np.random.rand(100),\n    ...     'Sales': np.random.rand(100)\n    ... })\n    >>> score = predict_sales(df, ['Advertising', 'Price'], 'Sales')\n    >>> print(f'R-squared score: {score:.2f}')\n    \"\"\"\n    X = df[c_label]\n    y = df[target]\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    score = model.score(X_test, y_test)\n\n    return score\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, matplotlib, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef analyze_trends(df, c_label, target):\n    \"\"\"\n    Analyze the trends in the target column based on the provided data.\n    The function will group the data based on the provided columns, plot the \n    target column over time, and also plot a boxplot showing the distribution of the target column.\n\n    Parameters:\n    df (DataFrame): The data.\n    c_label (list): The columns to group by.\n    target (str): The target column.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> df = pd.DataFrame({\n    ...     'Date': pd.date_range(start='1/1/2020', periods=100),\n    ...     'Sales': np.random.randint(50, 500, 100)\n    ... })\n    >>> analyze_trends(df, ['Date'], 'Sales')\n    \"\"\"\n    grouped_df = df.groupby(c_label).mean()\n\n    plt.figure(figsize=(12, 6))\n    plt.plot(grouped_df.index, grouped_df[target])\n    plt.title('Trends over time')\n    plt.xlabel(c_label[0])\n    plt.ylabel(target)\n\n    plt.figure(figsize=(12, 6))\n    sns.boxplot(y=grouped_df[target])\n    plt.title('Distribution')\n    plt.ylabel(target)\n\n    plt.show()\n```\n"}
{"seed": "{\"task_id\": 39240, \"prompt\": \"def f_39240(a_list, b_list):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"[i for i in b_list if i in a_list]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([1,2,3], [4,1,2]) == [1,2]\\n\", \"\\n    assert sorted(candidate([1,2,3,4,5], [4,1,2])) == [1,2,4]\\n\", \"\\n    assert candidate([1,2,3], []) == []\\n\"], \"entry_point\": \"f_39240\", \"intent\": \"\\u30ea\\u30b9\\u30c8`a_list`\\u306e\\u8981\\u7d20\\u306e\\u4e2d\\u306e\\u30ea\\u30b9\\u30c8`b_list`\\u306e\\u8981\\u7d20\\u3068\\u4e00\\u81f4\\u3059\\u308b\\u3082\\u306e\\u3092\\u8868\\u793a\\u3059\\u308b\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, random, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import sample\nimport matplotlib.pyplot as plt\n\n# Constants\nELEMENTS = ['Hydrogen', 'Helium', 'Lithium', 'Beryllium', 'Boron', \n            'Carbon', 'Nitrogen', 'Oxygen', 'Fluorine', 'Neon']\n\ndef elements_in_sample(a_list, b_list):\n    \"\"\"\n    Select elements from list 'a_list' which are also present in 'b_list'. \n    Generate a series with these elements and their randomly assigned quantities.\n    Use this data to create a DataFrame and plot a bar graph of the quantities.\n\n    Parameters:\n    a_list (list): List of elements.\n    b_list (list): Sample list of elements.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with selected elements and their quantities.\n\n    Requirements:\n    - pandas\n    - numpy\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> elements_in_sample(ELEMENTS, sample(ELEMENTS, 5))\n    \"\"\"\n    selected_elements = [i for i in b_list if i in a_list]\n    quantities = np.random.randint(1, 100, size=len(selected_elements))\n    \n    elements_df = pd.DataFrame({\"Element\": selected_elements, \"Quantity\": quantities})\n    \n    elements_df.plot.bar(x='Element', y='Quantity', rot=0)\n    \n    return elements_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, pathlib, fnmatch\n```python\nimport os\nfrom pathlib import Path\nimport fnmatch\n\ndef files_in_directory_with_extension(directory_path, file_extension_list):\n    \"\"\"\n    List all files in a directory that have an extension present in a given list.\n    \n    Parameters:\n    directory_path (str): The path of the directory.\n    file_extension_list (list): List of file extensions.\n    \n    Returns:\n    list: List of files.\n\n    Requirements:\n    - os\n    - pathlib.Path\n    - fnmatch\n    \n    Example:\n    >>> files_in_directory_with_extension('/home/user/documents', ['.txt', '.docx'])\n    \"\"\"\n    all_files = os.listdir(directory_path)\n    \n    selected_files = [file for file in all_files if any(fnmatch.fnmatch(file, '*'+ext) for ext in file_extension_list)]\n    \n    return selected_files\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ncsv, random, statistics\n```python\nimport csv\nfrom random import sample\nimport statistics\n\ndef calculate_mean_of_column_samples(csv_file, column_name, num_samples):\n    \"\"\"\n    Select a number of random samples from a column in a CSV file and calculate their mean.\n    \n    Parameters:\n    csv_file (str): The path to the CSV file.\n    column_name (str): The name of the column.\n    num_samples (int): The number of samples to select.\n    \n    Returns:\n    float: The mean of the selected samples.\n\n    Requirements:\n    - csv\n    - random.sample\n    - statistics.mean\n    \n    Example:\n    >>> calculate_mean_of_column_samples('data.csv', 'Age', 10)\n    \"\"\"\n    with open(csv_file, 'r') as file:\n        reader = csv.DictReader(file)\n        column_data = [float(row[column_name]) for row in reader]\n        \n    samples = sample(column_data, num_samples)\n    mean = statistics.mean(samples)\n    \n    return mean\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, matplotlib, seaborn\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef plot_correlation_of_selected_columns(csv_file, column_list):\n    \"\"\"\n    Plot a heatmap of the correlation matrix of selected columns from a CSV file.\n    \n    Parameters:\n    csv_file (str): The path to the CSV file.\n    column_list (list): List of column names.\n    \n    Returns:\n    None.\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - seaborn\n    \n    Example:\n    >>> plot_correlation_of_selected_columns('data.csv', ['Age', 'Income', 'Education'])\n    \"\"\"\n    df = pd.read_csv(csv_file)\n    correlation_matrix = df[column_list].corr()\n    \n    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, random, matplotlib\n```python\nimport numpy as np\nfrom random import sample\nimport matplotlib.pyplot as plt\n\ndef plot_distribution_of_selected_numbers(num_list, num_samples, num_bins):\n    \"\"\"\n    Select a number of random samples from a list of numbers and plot their distribution.\n    \n    Parameters:\n    num_list (list): List of numbers.\n    num_samples (int): The number of samples to select.\n    num_bins (int): The number of bins for the distribution plot.\n    \n    Returns:\n    None.\n\n    Requirements:\n    - numpy\n    - random.sample\n    - matplotlib.pyplot\n    \n    Example:\n    >>> plot_distribution_of_selected_numbers(range(1, 101), 50, 10)\n    \"\"\"\n    samples = sample(num_list, num_samples)\n    plt.hist(samples, bins=num_bins)\n    plt.show()\n```\n"}
{"seed": "{\"task_id\": 39375, \"prompt\": \"def f_39375(dt_s):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"datetime.strptime(dt_s,'%d%b%Y')\", \"test_start\": \"\\nfrom datetime import datetime\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('10OCT2017') == datetime.strptime('10OCT2017','%d%b%Y')\\n\"], \"entry_point\": \"f_39375\", \"intent\": \"\\u82f1\\u540d\\u306e\\u6708\\u3092\\u542b\\u3080\\u65e5\\u4ed8\\u30d5\\u30a9\\u30fc\\u30de\\u30c3\\u30c8'%d%b%Y'\\u306e\\u6587\\u5b57\\u5217`dt_s`\\u3092datetime\\u578b\\u306b\\u5909\\u63db\\u3059\\u308b\", \"library\": [\"datetime\"]}", "generation": "Scenario 1:\npandas, datetime, numpy, matplotlib\n```python\nimport pandas as pd\nfrom datetime import datetime, timedelta\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nDAYS_OF_WEEK = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n\ndef plot_weekly_sales(date_str):\n    \"\"\"\n    Given a date string in '%d%b%Y' format, generate a plot of mock weekly sales data starting from that \n    date for a period of one year.\n    \n    Parameters:\n    date_str (str): The start date string in '%d%b%Y' format.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - pandas\n    - datetime\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> plot_weekly_sales('10OCT2017')\n    \"\"\"\n    start_date = datetime.strptime(date_str, '%d%b%Y')\n    dates = pd.date_range(start_date, start_date + timedelta(days=365))\n    sales = np.random.rand(len(dates)) * 1000\n    df = pd.DataFrame({'Date': dates, 'Sales': sales})\n    df['Day of Week'] = df['Date'].dt.day_name()\n    weekly_sales = df.groupby('Day of Week')['Sales'].sum().reindex(DAYS_OF_WEEK)\n    weekly_sales.plot(kind='bar')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ndatetime, pytz, dateutil\n```python\nfrom datetime import datetime\nimport pytz\nfrom dateutil.parser import parse\n\ndef convert_to_date_in_tz(date_str, tz):\n    \"\"\"\n    Convert a date string in '%d%b%Y' format to a datetime object in a specific timezone.\n    \n    Parameters:\n    date_str (str): The date string in '%d%b%Y' format.\n    tz (str): The timezone to which the date should be converted.\n\n    Returns:\n    datetime: The datetime object in the specified timezone.\n\n    Requirements:\n    - datetime\n    - pytz\n    - dateutil.parser\n\n    Example:\n    >>> convert_to_date_in_tz('10OCT2017', 'Asia/Tokyo')\n    \"\"\"\n    date = parse(date_str)\n    tz = pytz.timezone(tz)\n    date = date.replace(tzinfo=tz)\n    return date\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ndatetime, pandas, matplotlib\n```python\nfrom datetime import datetime, timedelta\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef plot_temperature_trends(start_date_str):\n    \"\"\"\n    Given a date string in '%d%b%Y' format, generate a plot of mock temperature data starting from that \n    date for a period of 30 days.\n    \n    Parameters:\n    start_date_str (str): The start date string in '%d%b%Y' format.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - pandas\n    - datetime\n    - matplotlib.pyplot\n    \n    Example:\n    >>> plot_temperature_trends('10OCT2017')\n    \"\"\"\n    start_date = datetime.strptime(start_date_str, '%d%b%Y')\n    dates = pd.date_range(start_date, start_date + timedelta(days=30))\n    temperatures = pd.Series(np.random.randn(len(dates)) + 20, index=dates)\n    temperatures.plot()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ndatetime, pytz, dateutil, random\n```python\nfrom datetime import datetime\nimport pytz\nfrom dateutil.parser import parse\nimport random\n\ndef random_date_in_future(date_str, tz):\n    \"\"\"\n    Given a date string in '%d%b%Y' format and a timezone, generate a random date in the future \n    within the same timezone.\n    \n    Parameters:\n    date_str (str): The date string in '%d%b%Y' format.\n    tz (str): The timezone in which the dates should be generated.\n\n    Returns:\n    datetime: The random future date in the specified timezone.\n\n    Requirements:\n    - datetime\n    - pytz\n    - dateutil.parser\n    - random\n\n    Example:\n    >>> random_date_in_future('10OCT2017', 'Asia/Shanghai')\n    \"\"\"\n    date = parse(date_str)\n    tz = pytz.timezone(tz)\n    date = date.replace(tzinfo=tz)\n    random_days = random.randint(1, 365)\n    future_date = date + timedelta(days=random_days)\n    return future_date\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\ndatetime, pandas, matplotlib, numpy\n```python\nfrom datetime import datetime, timedelta\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_cumulative_rainfall(start_date_str):\n    \"\"\"\n    Given a date string in '%d%b%Y' format, generate a plot of cumulative mock rainfall data starting from \n    that date for a period of 60 days.\n    \n    Parameters:\n    start_date_str (str): The start date string in '%d%b%Y' format.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - pandas\n    - datetime\n    - matplotlib.pyplot\n    - numpy\n    \n    Example:\n    >>> plot_cumulative_rainfall('10OCT2017')\n    \"\"\"\n    start_date = datetime.strptime(start_date_str, '%d%b%Y')\n    dates = pd.date_range(start_date, start_date + timedelta(days=60))\n    rainfall = pd.Series(np.random.rand(len(dates)), index=dates)\n    cumulative_rainfall = rainfall.cumsum()\n    cumulative_rainfall.plot()\n    plt.show()\n```\n"}
{"seed": "{\"task_id\": 38960, \"prompt\": \"def f_38960(n):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"[int(c) for c in n]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('12345') == [1,2,3,4,5]\\n\", \"\\n    assert candidate('') == []\\n\", \"\\n    assert candidate('0') == [0]\\n\"], \"entry_point\": \"f_38960\", \"intent\": \"\\u6570\\u5024`n`\\u3092\\u5206\\u5272\\u3057\\u3066\\u30ea\\u30b9\\u30c8\\u306b\\u683c\\u7d0d\\u3059\\u308b\", \"library\": []}", "generation": "Scenario 1: \npandas, re, random, matplotlib\n```python\nimport pandas as pd\nimport re\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nCATEGORIES = ['Electronics', 'Fashion', 'Books', 'Home & Kitchen', 'Toys & Games']\n\ndef generate_sales_report(product_codes):\n    \"\"\"\n    Generate a sales report for different categories of products.\n\n    Parameters:\n    product_codes (list): A list of product codes where the first character indicates the category.\n\n    Returns:\n    DataFrame: A pandas DataFrame with sales data for the categories.\n    \n    Requirements:\n    - pandas\n    - re\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> product_codes = ['E1234', 'F5678', 'B91011', 'H1213', 'T1415']\n    >>> report = generate_sales_report(product_codes)\n    >>> print(report)\n    >>> report['Sales'].plot(kind='bar')\n    \"\"\"\n    report_data = []\n\n    for code in product_codes:\n        category = CATEGORIES[int(code[0]) - 1]\n        sales = randint(100, 1000)\n        report_data.append([category, sales])\n\n    report_df = pd.DataFrame(report_data, columns=['Category', 'Sales'])\n\n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, matplotlib, math\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\n\n# Constants\nPI = math.pi\n\ndef plot_sine_wave(frequency, amplitude, phase_shift):\n    \"\"\"\n    Plot a sine wave with given frequency, amplitude and phase shift.\n\n    Parameters:\n    frequency (float): The frequency of the sine wave.\n    amplitude (float): The amplitude of the sine wave.\n    phase_shift (float): The phase shift of the sine wave in degrees.\n\n    Returns: \n    None\n\n    Requirements:\n    - matplotlib.pyplot\n    - numpy\n    - math\n\n    Example:\n    >>> plot_sine_wave(1, 2, 45)\n    \"\"\"\n    t = np.linspace(0, 1, 500, endpoint=False) # Create time array\n    wave = amplitude * np.sin(2 * PI * frequency * t + np.deg2rad(phase_shift)) # Create sine wave\n    plt.plot(t, wave) # Plot the wave\n    plt.show() # Show the plot\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, random, matplotlib, numpy\n```python\nimport pandas as pd\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef generate_stock_price(ticker):\n    \"\"\"\n    Generate a random stock price history for a given ticker and plot it.\n\n    Parameters:\n    ticker (str): The ticker symbol for the stock.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the stock price history.\n\n    Requirements:\n    - pandas\n    - random\n    - matplotlib.pyplot\n    - numpy\n\n    Example:\n    >>> prices = generate_stock_price('AAPL')\n    >>> print(prices)\n    >>> prices.plot()\n    \"\"\"\n    dates = pd.date_range(start='01-01-2020', end='12-31-2020')\n    prices = [randint(100, 200) for _ in range(len(dates))]\n    price_df = pd.DataFrame(prices, index=dates, columns=[ticker])\n    return price_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, matplotlib, math\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\n\n# Constants\nPI = math.pi\n\ndef plot_cosine_wave(frequency, amplitude, phase_shift):\n    \"\"\"\n    Plot a cosine wave with given frequency, amplitude and phase shift.\n\n    Parameters:\n    frequency (float): The frequency of the cosine wave.\n    amplitude (float): The amplitude of the cosine wave.\n    phase_shift (float): The phase shift of the cosine wave in degrees.\n\n    Returns: \n    None\n\n    Requirements:\n    - matplotlib.pyplot\n    - numpy\n    - math\n\n    Example:\n    >>> plot_cosine_wave(1, 2, 45)\n    \"\"\"\n    t = np.linspace(0, 1, 500, endpoint=False) # Create time array\n    wave = amplitude * np.cos(2 * PI * frequency * t + np.deg2rad(phase_shift)) # Create cosine wave\n    plt.plot(t, wave) # Plot the wave\n    plt.show() # Show the plot\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, random, matplotlib, numpy\n```python\nimport pandas as pd\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef generate_exchange_rate(currency):\n    \"\"\"\n    Generate a random exchange rate history for a given currency and plot it.\n\n    Parameters:\n    currency (str): The currency symbol.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the exchange rate history.\n\n    Requirements:\n    - pandas\n    - random\n    - matplotlib.pyplot\n    - numpy\n\n    Example:\n    >>> rates = generate_exchange_rate('EUR')\n    >>> print(rates)\n    >>> rates.plot()\n    \"\"\"\n    dates = pd.date_range(start='01-01-2020', end='12-31-2020')\n    rates = [randint(1, 2) + np.random.random() for _ in range(len(dates))]\n    rate_df = pd.DataFrame(rates, index=dates, columns=[currency])\n    return rate_df\n```"}
{"seed": "{\"task_id\": 33908, \"prompt\": \"def f_33908():\\n\\t\", \"suffix\": \"\\n\\treturn profile\", \"canonical_solution\": \"profile = webdriver.FirefoxProfile()\\n\\tprofile.DEFAULT_PREFERENCES['frozen']['javascript.enabled'] = False\\n\\tprofile.set_preference(\\\"app.update.auto\\\", False)\\n\\tprofile.set_preference(\\\"app.update.enabled\\\", False)\\n\\tprofile.update_preferences()\", \"test_start\": \"\\nimport selenium\\nfrom selenium import webdriver\\nfrom selenium.webdriver.firefox.options import Options\\n\\ndef check(candidate):\", \"test\": [\"\\n    profile = candidate()\\n    assert profile.__class__ == selenium.webdriver.firefox.firefox_profile.FirefoxProfile\\n\"], \"entry_point\": \"f_33908\", \"intent\": \"seleniumt\\u3067Firefox\\u4ed5\\u69d8\\u6642\\u306bjavascript\\u3092\\u7121\\u52b9\\u306b\\u3059\\u308b\", \"library\": [\"selenium\"]}", "generation": "Scenario 1:\nselenium, os, time, datetime\n```python\nimport selenium\nfrom selenium import webdriver\nfrom selenium.webdriver.firefox.options import Options\nimport os\nimport time\nfrom datetime import datetime\n\nDOWNLOAD_PATH = '/path/to/download/directory'\nURL = 'http://example.com'\nWAIT_TIME = 10  # seconds\n\ndef download_file_with_firefox():\n    \"\"\"\n    Download a file from a specific URL using Firefox with Selenium, disable Javascript, \n    and save the file to a specific directory. Set the download path for Firefox \n    and wait for the download to complete before closing the browser.\n\n    Returns:\n    str: The path of the downloaded file.\n\n    Requirements:\n    - selenium\n    - os\n    - time\n    - datetime\n\n    Example:\n    >>> download_file_with_firefox()\n    \"\"\"\n    profile = webdriver.FirefoxProfile()\n    profile.DEFAULT_PREFERENCES['frozen']['javascript.enabled'] = False\n    profile.set_preference(\"browser.download.folderList\", 2)\n    profile.set_preference(\"browser.download.dir\", DOWNLOAD_PATH)\n    profile.update_preferences()\n\n    driver = webdriver.Firefox(firefox_profile=profile)\n    driver.get(URL)\n    time.sleep(WAIT_TIME)\n\n    # Get the downloaded file\n    files = os.listdir(DOWNLOAD_PATH)\n    files.sort(key=lambda x: os.path.getmtime(os.path.join(DOWNLOAD_PATH, x)))\n    downloaded_file = os.path.join(DOWNLOAD_PATH, files[-1])\n\n    # Close the driver\n    driver.quit()\n\n    return downloaded_file\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nselenium, time, random, string\n```python\nimport selenium\nfrom selenium import webdriver\nfrom selenium.webdriver.firefox.options import Options\nimport time\nimport random\nimport string\n\nURL = 'http://example.com'\nWAIT_TIME = 10  # seconds\nINPUT_FIELD_ID = 'input_field_id'\nSUBMIT_BUTTON_ID = 'submit_button_id'\n\ndef fill_form_with_random_data():\n    \"\"\"\n    Fill a form on a webpage using Firefox with Selenium, generate a random string,\n    input it into a text field, and then click a submit button.\n\n    Returns:\n    str: The generated random string.\n\n    Requirements:\n    - selenium\n    - time\n    - random\n    - string\n\n    Example:\n    >>> fill_form_with_random_data()\n    \"\"\"\n    profile = webdriver.FirefoxProfile()\n    profile.DEFAULT_PREFERENCES['frozen']['javascript.enabled'] = False\n    profile.update_preferences()\n\n    driver = webdriver.Firefox(firefox_profile=profile)\n    driver.get(URL)\n\n    # Generate a random string\n    random_string = ''.join(random.choices(string.ascii_uppercase + string.digits, k=10))\n\n    # Fill the form and submit\n    input_field = driver.find_element_by_id(INPUT_FIELD_ID)\n    input_field.send_keys(random_string)\n    submit_button = driver.find_element_by_id(SUBMIT_BUTTON_ID)\n    submit_button.click()\n    time.sleep(WAIT_TIME)\n\n    # Close the driver\n    driver.quit()\n\n    return random_string\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nselenium, os, time, datetime\n```python\nimport selenium\nfrom selenium import webdriver\nfrom selenium.webdriver.firefox.options import Options\nimport os\nimport time\nfrom datetime import datetime\n\nURL = 'http://example.com'\nWAIT_TIME = 10  # seconds\nSCREENSHOT_PATH = '/path/to/screenshot/directory'\n\ndef take_screenshot_with_firefox():\n    \"\"\"\n    Open a webpage using Firefox with Selenium, wait for a specific amount \n    of time, and then take a screenshot of the webpage.\n\n    Returns:\n    str: The path of the screenshot.\n\n    Requirements:\n    - selenium\n    - os\n    - time\n    - datetime\n\n    Example:\n    >>> take_screenshot_with_firefox()\n    \"\"\"\n    profile = webdriver.FirefoxProfile()\n    profile.DEFAULT_PREFERENCES['frozen']['javascript.enabled'] = False\n    profile.update_preferences()\n\n    driver = webdriver.Firefox(firefox_profile=profile)\n    driver.get(URL)\n    time.sleep(WAIT_TIME)\n\n    # Take a screenshot\n    screenshot_path = os.path.join(SCREENSHOT_PATH, f'screenshot_{datetime.now()}.png')\n    driver.save_screenshot(screenshot_path)\n\n    # Close the driver\n    driver.quit()\n\n    return screenshot_path\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nselenium, time, getpass\n```python\nimport selenium\nfrom selenium import webdriver\nfrom selenium.webdriver.firefox.options import Options\nimport time\nimport getpass\n\nURL = 'http://example.com'\nWAIT_TIME = 10  # seconds\nUSERNAME_FIELD_ID = 'username_field_id'\nPASSWORD_FIELD_ID = 'password_field_id'\nSUBMIT_BUTTON_ID = 'submit_button_id'\n\ndef login_with_firefox():\n    \"\"\"\n    Login to a webpage using Firefox with Selenium, get the username and password \n    from the user, input them into the relevant fields, and then click a login button.\n\n    Returns:\n    None\n\n    Requirements:\n    - selenium\n    - time\n    - getpass\n\n    Example:\n    >>> login_with_firefox()\n    \"\"\"\n    profile = webdriver.FirefoxProfile()\n    profile.DEFAULT_PREFERENCES['frozen']['javascript.enabled'] = False\n    profile.update_preferences()\n\n    driver = webdriver.Firefox(firefox_profile=profile)\n    driver.get(URL)\n\n    # Get the username and password\n    username = input(\"Enter your username: \")\n    password = getpass.getpass(\"Enter your password: \")\n\n    # Fill the form and submit\n    username_field = driver.find_element_by_id(USERNAME_FIELD_ID)\n    username_field.send_keys(username)\n    password_field = driver.find_element_by_id(PASSWORD_FIELD_ID)\n    password_field.send_keys(password)\n    login_button = driver.find_element_by_id(SUBMIT_BUTTON_ID)\n    login_button.click()\n    time.sleep(WAIT_TIME)\n\n    # Close the driver\n    driver.quit()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nselenium, csv, time\n```python\nimport selenium\nfrom selenium import webdriver\nfrom selenium.webdriver.firefox.options import Options\nimport csv\nimport time\n\nURL = 'http://example.com'\nWAIT_TIME = 10  # seconds\nTABLE_ID = 'table_id'\nCSV_PATH = '/path/to/csv/file.csv'\n\ndef export_table_to_csv_with_firefox():\n    \"\"\"\n    Open a webpage using Firefox with Selenium, find a table on the webpage, \n    extract its data, and save the data to a CSV file.\n\n    Returns:\n    None\n\n    Requirements:\n    - selenium\n    - csv\n    - time\n\n    Example:\n    >>> export_table_to_csv_with_firefox()\n    \"\"\"\n    profile = webdriver.FirefoxProfile()\n    profile.DEFAULT_PREFERENCES['frozen']['javascript.enabled'] = False\n    profile.update_preferences()\n\n    driver = webdriver.Firefox(firefox_profile=profile)\n    driver.get(URL)\n    time.sleep(WAIT_TIME)\n\n    # Extract the table data\n    table = driver.find_element_by_id(TABLE_ID)\n    rows = table.find_elements_by_tag_name('tr')\n    data = [[cell.text for cell in row.find_elements_by_tag_name('td')] for row in rows]\n\n    # Write the data to a CSV file\n    with open(CSV_PATH, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerows(data)\n\n    # Close the driver\n    driver.quit()\n```"}
{"seed": "{\"task_id\": 33908, \"prompt\": \"def f_33908():\\n\\t\", \"suffix\": \"\\n\\treturn options\", \"canonical_solution\": \"\\n\\toptions = Options()\\n\\toptions.set_preference('javascript.enabled', False)\\n\", \"test_start\": \"\\nimport selenium\\nfrom selenium import webdriver\\nfrom selenium.webdriver.firefox.options import Options\\n\\ndef check(candidate):\", \"test\": [\"\\n    options = candidate()\\n    assert options.preferences == {'javascript.enabled': False}\\n\"], \"entry_point\": \"f_33908\", \"intent\": \"seleniumt\\u3067Firefox\\u4ed5\\u69d8\\u6642\\u306bjavascript\\u3092\\u7121\\u52b9\\u306b\\u3059\\u308b\", \"library\": [\"selenium\"]}", "generation": "Scenario 1: \nselenium, time, os, shutil \n```python\nimport selenium\nfrom selenium import webdriver\nfrom selenium.webdriver.firefox.options import Options\nimport time\nimport os\nimport shutil\n\nDOWNLOAD_DIR = \"tmp_download_dir/\"\n\ndef f_4530069():\n    \"\"\"\n    Set up a Firefox browser via Selenium to download files to a specific directory,\n    disable javascript, and automatically accept dangerous downloads. \n\n    Returns:\n    webdriver.Firefox: A Firefox webdriver with the specified preferences.\n\n    Requirements:\n    - selenium\n    - time\n    - os\n    - shutil\n\n    Example:\n    >>> driver = f_4530069()\n    \"\"\"\n    options = Options()\n    options.set_preference(\"javascript.enabled\", False)\n    \n    # Set up custom download directory\n    if not os.path.isdir(DOWNLOAD_DIR):\n        os.makedirs(DOWNLOAD_DIR)\n    else:\n        shutil.rmtree(DOWNLOAD_DIR) # clear directory if it already exists\n        os.makedirs(DOWNLOAD_DIR)\n\n    # Set browser preferences\n    options.set_preference(\"browser.download.folderList\", 2)\n    options.set_preference(\"browser.download.dir\", DOWNLOAD_DIR)\n    options.set_preference(\"browser.download.manager.showWhenStarting\", False)\n    options.set_preference(\"browser.helperApps.neverAsk.saveToDisk\", \"application/octet-stream\")\n    options.set_preference(\"browser.download.manager.focusWhenStarting\", False)\n    options.set_preference(\"browser.download.useDownloadDir\", True)\n    options.set_preference(\"browser.helperApps.alwaysAsk.force\", False)\n    options.set_preference(\"browser.download.manager.alertOnEXEOpen\", False)\n    options.set_preference(\"browser.download.manager.closeWhenDone\", True)\n    options.set_preference(\"browser.download.manager.showAlertOnComplete\", False)\n    options.set_preference(\"browser.download.manager.useWindow\", False)\n\n    driver = webdriver.Firefox(options=options)\n    driver.implicitly_wait(30)\n    return driver\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nselenium, random, string, os\n```python\nimport selenium\nfrom selenium import webdriver\nfrom selenium.webdriver.firefox.options import Options\nimport random\nimport string\nimport os\n\ndef f_4530069():\n    \"\"\"\n    Set up a Firefox browser via Selenium to run in headless mode, with a custom user agent, \n    and save screenshots to a unique directory.\n\n    Returns:\n    webdriver.Firefox: A Firefox webdriver with the specified preferences.\n\n    Requirements:\n    - selenium\n    - random\n    - string\n    - os\n\n    Example:\n    >>> driver = f_4530069()\n    \"\"\"\n    options = Options()\n    options.set_preference(\"javascript.enabled\", False)\n    options.add_argument(\"--headless\")\n\n    # Generate a random user agent\n    letters = string.ascii_lowercase\n    random_string = ''.join(random.choice(letters) for i in range(10))\n    user_agent = f\"Mozilla/5.0 ({random_string}) Gecko/20100101 Firefox/89.0\"\n    options.set_preference(\"general.useragent.override\", user_agent)\n\n    # Set up custom screenshot directory\n    screenshot_dir = f\"screenshots_{random_string}/\"\n    if not os.path.isdir(screenshot_dir):\n        os.makedirs(screenshot_dir)\n\n    # Set browser preferences\n    options.set_preference(\"browser.download.folderList\", 2)\n    options.set_preference(\"browser.download.dir\", screenshot_dir)\n    options.set_preference(\"browser.download.manager.showWhenStarting\", False)\n    options.set_preference(\"browser.helperApps.neverAsk.saveToDisk\", \"image/png\")\n\n    driver = webdriver.Firefox(options=options)\n    driver.implicitly_wait(30)\n    return driver\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nselenium, json, os\n```python\nimport selenium\nfrom selenium import webdriver\nfrom selenium.webdriver.firefox.options import Options\nimport json\nimport os\n\ndef f_4530069():\n    \"\"\"\n    Set up a Firefox browser via Selenium to load a set of custom preferences from a JSON file.\n\n    Returns:\n    webdriver.Firefox: A Firefox webdriver with the specified preferences.\n\n    Requirements:\n    - selenium\n    - json\n    - os\n\n    Example:\n    >>> driver = f_4530069()\n    \"\"\"\n    options = Options()\n\n    # Load preferences from JSON file\n    with open('browser_preferences.json', 'r') as f:\n        prefs = json.load(f)\n\n    for pref, value in prefs.items():\n        options.set_preference(pref, value)\n\n    driver = webdriver.Firefox(options=options)\n    driver.implicitly_wait(30)\n    return driver\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nselenium, time, os, shutil \n```python\nimport selenium\nfrom selenium import webdriver\nfrom selenium.webdriver.firefox.options import Options\nimport time\nimport os\nimport shutil\n\nSCREENSHOT_DIR = \"tmp_screenshot_dir/\"\n\ndef f_4530069():\n    \"\"\"\n    Set up a Firefox browser via Selenium to run in headless mode and save screenshots \n    to a specified directory.\n\n    Returns:\n    webdriver.Firefox: A Firefox webdriver with the specified preferences.\n\n    Requirements:\n    - selenium\n    - time\n    - os\n    - shutil\n\n    Example:\n    >>> driver = f_4530069()\n    \"\"\"\n    options = Options()\n    options.set_preference(\"javascript.enabled\", False)\n    options.add_argument(\"--headless\")\n    \n    # Set up custom screenshot directory\n    if not os.path.isdir(SCREENSHOT_DIR):\n        os.makedirs(SCREENSHOT_DIR)\n    else:\n        shutil.rmtree(SCREENSHOT_DIR) # clear directory if it already exists\n        os.makedirs(SCREENSHOT_DIR)\n\n    driver = webdriver.Firefox(options=options)\n    driver.implicitly_wait(30)\n    return driver\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nselenium, time, os, shutil \n```python\nimport selenium\nfrom selenium import webdriver\nfrom selenium.webdriver.firefox.options import Options\nimport time\nimport os\nimport shutil\n\nPROFILE_DIR = \"tmp_profile_dir/\"\n\ndef f_4530069():\n    \"\"\"\n    Set up a Firefox browser via Selenium to use a custom profile directory.\n\n    Returns:\n    webdriver.Firefox: A Firefox webdriver with the specified preferences.\n\n    Requirements:\n    - selenium\n    - time\n    - os\n    - shutil\n\n    Example:\n    >>> driver = f_4530069()\n    \"\"\"\n    options = Options()\n    options.set_preference(\"javascript.enabled\", False)\n    \n    # Set up custom profile directory\n    if not os.path.isdir(PROFILE_DIR):\n        os.makedirs(PROFILE_DIR)\n    else:\n        shutil.rmtree(PROFILE_DIR) # clear directory if it already exists\n        os.makedirs(PROFILE_DIR)\n\n    profile = webdriver.FirefoxProfile(PROFILE_DIR)\n\n    driver = webdriver.Firefox(firefox_profile=profile, options=options)\n    driver.implicitly_wait(30)\n    return driver\n```"}
{"seed": "{\"task_id\": 19770, \"prompt\": \"def f_19770(s):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"s.isnumeric()\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('1') == True\\n\", \"\\n    assert candidate('a') == False\\n\"], \"entry_point\": \"f_19770\", \"intent\": \"\\u6587\\u5b57\\u5217`s`\\u304c\\u6570\\u3092\\u8868\\u3059\\u6587\\u5b57\\u304b\\u3069\\u3046\\u304b\\u5224\\u5b9a\\u3059\\u308b\", \"library\": []}", "generation": "Scenario 1:\npandas, numpy, matplotlib, re\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re\n\n# Sample Data\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n    'Age': [25, 32, 37, 19, 45],\n    'Salary': ['5000$', '7000$', '8000$', '3000$', '9000$']\n}\n\ndf = pd.DataFrame(data)\n\ndef calculate_average_numeric_values(dataframe, column):\n    \"\"\"\n    Calculate the average of numeric values in a specific column of a pandas DataFrame. \n    The column can contain non-numeric characters.\n\n    Parameters:\n    dataframe (DataFrame): The pandas DataFrame.\n    column (str): The column name.\n\n    Returns:\n    float: The average of numeric values.\n\n    Requirements:\n    - pandas\n    - numpy\n    - re\n\n    Example:\n    >>> calculate_average_numeric_values(df, 'Salary')\n    \"\"\"\n    numeric_values = []\n    for value in dataframe[column]:\n        numeric_value = re.sub(r'\\D', '', value)\n        numeric_values.append(int(numeric_value))\n\n    average = np.mean(numeric_values)\n\n    return average\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, matplotlib, re\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re\n\n# Sample Data\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n    'Score': ['90, 80, 85', '75, 85, 80', '60, 70, 65', '80, 90, 85', '70, 75, 80']\n}\n\ndf = pd.DataFrame(data)\n\ndef calculate_average_scores(dataframe, column):\n    \"\"\"\n    Calculate the average scores for each row in a specific column of a pandas DataFrame. \n    The column contains scores separated by commas.\n\n    Parameters:\n    dataframe (DataFrame): The pandas DataFrame.\n    column (str): The column name.\n\n    Returns:\n    Series: A pandas Series with average scores.\n\n    Requirements:\n    - pandas\n    - numpy\n    - re\n\n    Example:\n    >>> calculate_average_scores(df, 'Score')\n    \"\"\"\n    averages = []\n    for scores in dataframe[column]:\n        score_list = [int(score) for score in re.split(r',\\s*', scores)]\n        averages.append(np.mean(score_list))\n\n    return pd.Series(averages, name='Average Score')\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, re, sklearn.model_selection, sklearn.ensemble\n```python\nimport pandas as pd\nimport re\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Sample Data\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n    'Age': ['25 years', '32 years', '37 years', '19 years', '45 years'],\n    'Label': [0, 1, 1, 0, 1]\n}\n\ndf = pd.DataFrame(data)\n\ndef train_model(dataframe, feature_column, label_column):\n    \"\"\"\n    Train a RandomForestClassifier on a pandas DataFrame. \n    The feature column can contain non-numeric characters.\n\n    Parameters:\n    dataframe (DataFrame): The pandas DataFrame.\n    feature_column (str): The feature column name.\n    label_column (str): The label column name.\n\n    Returns:\n    RandomForestClassifier: The trained model.\n\n    Requirements:\n    - pandas\n    - re\n    - sklearn.model_selection\n    - sklearn.ensemble\n\n    Example:\n    >>> train_model(df, 'Age', 'Label')\n    \"\"\"\n    # Extract numeric values from the feature column\n    dataframe[feature_column] = dataframe[feature_column].apply(lambda x: int(re.sub(r'\\D', '', x)))\n\n    # Split the data\n    X_train, X_test, y_train, y_test = train_test_split(dataframe[[feature_column]], dataframe[label_column], test_size=0.2, random_state=42)\n\n    # Train the model\n    clf = RandomForestClassifier()\n    clf.fit(X_train, y_train)\n\n    return clf\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, re, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nimport re\nimport seaborn as sns\n\n# Sample Data\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n    'Height': ['160 cm', '175 cm', '180 cm', '165 cm', '170 cm']\n}\n\ndf = pd.DataFrame(data)\n\ndef plot_height_distribution(dataframe, column):\n    \"\"\"\n    Plot a histogram of the height distribution in a pandas DataFrame. \n    The column can contain non-numeric characters.\n\n    Parameters:\n    dataframe (DataFrame): The pandas DataFrame.\n    column (str): The column name.\n\n    Requirements:\n    - pandas\n    - numpy\n    - re\n    - seaborn\n\n    Example:\n    >>> plot_height_distribution(df, 'Height')\n    \"\"\"\n    # Extract numeric values from the column\n    dataframe[column] = dataframe[column].apply(lambda x: int(re.sub(r'\\D', '', x)))\n\n    # Plot the histogram\n    sns.histplot(dataframe[column], kde=True)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, re, sklearn.linear_model\n```python\nimport pandas as pd\nimport numpy as np\nimport re\nfrom sklearn.linear_model import LinearRegression\n\n# Sample Data\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n    'Score': ['90%', '80%', '85%', '75%', '90%'],\n    'Passed': [1, 0, 1, 0, 1]\n}\n\ndf = pd.DataFrame(data)\n\ndef predict_pass_status(dataframe, feature_column, label_column):\n    \"\"\"\n    Predict the pass status for each student in a pandas DataFrame based on their score. \n    The score column can contain non-numeric characters.\n\n    Parameters:\n    dataframe (DataFrame): The pandas DataFrame.\n    feature_column (str): The feature column name.\n    label_column (str): The label column name.\n\n    Returns:\n    numpy.array: The predicted pass status.\n\n    Requirements:\n    - pandas\n    - numpy\n    - re\n    - sklearn.linear_model\n\n    Example:\n    >>> predict_pass_status(df, 'Score', 'Passed')\n    \"\"\"\n    # Extract numeric values from the feature column\n    dataframe[feature_column] = dataframe[feature_column].apply(lambda x: int(re.sub(r'\\D', '', x)))\n\n    # Train the model\n    clf = LinearRegression()\n    clf.fit(dataframe[[feature_column]], dataframe[label_column])\n\n    # Predict the pass status\n    predictions = clf.predict(dataframe[[feature_column]])\n\n    return np.round(predictions).astype(int)\n```"}
{"seed": "{\"task_id\": 29614, \"prompt\": \"def f_29614():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"socket.socket()\", \"test_start\": \"\\nimport socket\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate().__class__ == socket.socket\\n\"], \"entry_point\": \"f_29614\", \"intent\": \"\\u30bd\\u30b1\\u30c3\\u30c8\\u60c5\\u5831\\u3092\\u4fdd\\u5b58\\u3059\\u308b\", \"library\": [\"socket\"]}", "generation": "Scenario 1: \nsocket, os, hashlib, base64\n```python\nimport socket\nimport os\nimport hashlib\nimport base64\n\n# Constants\nBUFFER_SIZE = 1024\nENCRYPTION_KEY = b'secret-key'\n\ndef receive_file_over_socket(socket_address, file_path):\n    \"\"\"\n    Receive a file over a socket, decrypt it using a secret key and save it to a given path.\n    \n    Parameters:\n    socket_address (tuple): A tuple containing the IP address and port number.\n    file_path (str): The path where the received file should be saved.\n    \n    Returns:\n    str: The SHA256 hash of the received file.\n    \n    Requirements:\n    - socket\n    - os\n    - hashlib\n    - base64\n    \n    Example:\n    >>> receive_file_over_socket(('localhost', 12345), '/path/to/save/file')\n    \"\"\"\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n        s.connect(socket_address)\n        with open(file_path, 'wb') as f:\n            while True:\n                data = s.recv(BUFFER_SIZE)\n                if not data:\n                    break\n                decrypted_data = base64.b64decode(data)\n                f.write(decrypted_data)\n    \n    with open(file_path, 'rb') as f:\n        file_hash = hashlib.sha256()\n        while chunk := f.read(BUFFER_SIZE):\n            file_hash.update(chunk)\n    \n    return file_hash.hexdigest()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nsocket, threading, queue, time\n```python\nimport socket\nimport threading\nimport queue\nimport time\n\n# Constants\nBUFFER_SIZE = 1024\nQUEUE_MAX_SIZE = 10\nMESSAGE_END_TOKEN = b'\\n'\n\ndef threaded_socket_server(socket_address, num_workers):\n    \"\"\"\n    Start a threaded socket server that can handle multiple client connections simultaneously.\n    \n    Parameters:\n    socket_address (tuple): A tuple containing the IP address and port number.\n    num_workers (int): The number of worker threads to spawn.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - socket\n    - threading\n    - queue\n    - time\n    \n    Example:\n    >>> threaded_socket_server(('localhost', 12345), 5)\n    \"\"\"\n    message_queue = queue.Queue(maxsize=QUEUE_MAX_SIZE)\n\n    def worker():\n        while True:\n            client_socket, _ = message_queue.get()\n            with client_socket:\n                while data := client_socket.recv(BUFFER_SIZE):\n                    if data == MESSAGE_END_TOKEN:\n                        break\n                    time.sleep(1)  # Simulate processing delay\n                    client_socket.sendall(data)  # Echo back the received data\n            message_queue.task_done()\n\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n        s.bind(socket_address)\n        s.listen()\n        for _ in range(num_workers):\n            threading.Thread(target=worker, daemon=True).start()\n        while True:\n            message_queue.put(s.accept())\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nsocket, select, sys\n```python\nimport socket\nimport select\nimport sys\n\n# Constants\nBUFFER_SIZE = 1024\n\ndef non_blocking_socket_client(socket_address):\n    \"\"\"\n    Create a non-blocking socket client that can receive and send data simultaneously.\n    \n    Parameters:\n    socket_address (tuple): A tuple containing the IP address and port number.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - socket\n    - select\n    - sys\n    \n    Example:\n    >>> non_blocking_socket_client(('localhost', 12345))\n    \"\"\"\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n        s.connect(socket_address)\n        while True:\n            read_sockets, _, _ = select.select([s, sys.stdin], [], [])\n            for sock in read_sockets:\n                if sock == s:\n                    data = sock.recv(BUFFER_SIZE)\n                    if not data:\n                        print('Disconnected from server')\n                        return\n                    else:\n                        print(data.decode())\n                else:\n                    message = sys.stdin.readline()\n                    s.send(message.encode())\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nsocket, os, shutil\n```python\nimport socket\nimport os\nimport shutil\n\n# Constants\nBUFFER_SIZE = 1024\n\ndef send_directory_over_socket(socket_address, dir_path):\n    \"\"\"\n    Send the contents of a directory over a socket.\n    \n    Parameters:\n    socket_address (tuple): A tuple containing the IP address and port number.\n    dir_path (str): The path of the directory to be sent.\n    \n    Returns:\n    int: The number of files sent.\n    \n    Requirements:\n    - socket\n    - os\n    - shutil\n    \n    Example:\n    >>> send_directory_over_socket(('localhost', 12345), '/path/to/send/directory')\n    \"\"\"\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n        s.connect(socket_address)\n        file_count = 0\n        for root, dirs, files in os.walk(dir_path):\n            for file in files:\n                file_path = os.path.join(root, file)\n                with open(file_path, 'rb') as f:\n                    shutil.copyfileobj(f, s.makefile('wb'), BUFFER_SIZE)\n                file_count += 1\n        return file_count\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nsocket, ssl, http.client\n```python\nimport socket\nimport ssl\nimport http.client\n\n# Constants\nSERVER_NAME = 'www.example.com'\nSERVER_PORT = 443\n\ndef make_https_request(path):\n    \"\"\"\n    Make an HTTPS request to a server using a socket.\n    \n    Parameters:\n    path (str): The path for the HTTP request.\n    \n    Returns:\n    str: The response from the server.\n    \n    Requirements:\n    - socket\n    - ssl\n    - http.client\n    \n    Example:\n    >>> make_https_request('/path/to/request')\n    \"\"\"\n    context = ssl.create_default_context()\n\n    with socket.create_connection((SERVER_NAME, SERVER_PORT)) as sock:\n        with context.wrap_socket(sock, server_hostname=SERVER_NAME) as ssock:\n            conn = http.client.HTTPSConnection(SERVER_NAME, context=context)\n            conn.request('GET', path)\n            response = conn.getresponse()\n            return response.read().decode()\n```\n"}
{"seed": "{\"task_id\": 41032, \"prompt\": \"def f_41032(dir):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"os.listdir(dir)\", \"test_start\": \"\\nimport os\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('.') == os.listdir('.')\\n\"], \"entry_point\": \"f_41032\", \"intent\": \"\\u30c7\\u30a3\\u30ec\\u30af\\u30c8\\u30ea`dir`\\u5185\\u306b\\u3042\\u308b\\u30d5\\u30a1\\u30a4\\u30eb\\u306e\\u30ea\\u30b9\\u30c8\\u3092\\u53d6\\u5f97\\u3059\\u308b\", \"library\": [\"os\"]}", "generation": "Scenario 1:\nos, pandas, matplotlib.pyplot, statistics\n```python\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom statistics import mean, median\n\n# Constants\nFILE_TYPES = ['.txt', '.docx', '.xlsx', '.pdf', '.jpg', '.png']\n\ndef analyze_directory(dir):\n    \"\"\"\n    Analyze a directory and generate a report of file types and their counts, \n    average and median file sizes.\n\n    Parameters:\n    dir (str): The directory to analyze.\n\n    Returns:\n    dict: A dictionary containing counts of files by type, average and median file sizes.\n\n    Requirements:\n    - os\n    - pandas\n    - matplotlib.pyplot\n    - statistics\n\n    Example:\n    >>> res = analyze_directory('./test_directory')\n    >>> print(res)\n    \"\"\"\n    report_data = {file_type: 0 for file_type in FILE_TYPES}\n    file_sizes = []\n\n    for file in os.listdir(dir):\n        _, ext = os.path.splitext(file)\n        if ext in FILE_TYPES:\n            report_data[ext] += 1\n            file_sizes.append(os.path.getsize(os.path.join(dir, file)))\n\n    report_data['Average File Size'] = mean(file_sizes)\n    report_data['Median File Size'] = median(file_sizes)\n\n    return report_data\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, shutil, datetime, hashlib\n```python\nimport os\nimport shutil\nfrom datetime import datetime\nimport hashlib\n\nBACKUP_DIR = './backup'\n\ndef backup_and_hash(dir):\n    \"\"\"\n    Backup a directory and generate a hash of each file in the directory.\n\n    Parameters:\n    dir (str): The directory to backup.\n\n    Returns:\n    dict: A dictionary containing file names as keys and their hashes as values.\n\n    Requirements:\n    - os\n    - shutil\n    - datetime\n    - hashlib\n\n    Example:\n    >>> hashes = backup_and_hash('./test_directory')\n    >>> print(hashes)\n    \"\"\"\n    if not os.path.exists(BACKUP_DIR):\n        os.makedirs(BACKUP_DIR)\n\n    timestamp = datetime.now().strftime('%Y%m%d%H%M%S')\n    backup_dir = os.path.join(BACKUP_DIR, f'{os.path.basename(dir)}_{timestamp}')\n    shutil.copytree(dir, backup_dir)\n\n    file_hashes = {}\n\n    for file in os.listdir(backup_dir):\n        with open(os.path.join(backup_dir, file), 'rb') as f:\n            file_hashes[file] = hashlib.md5(f.read()).hexdigest()\n\n    return file_hashes\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, csv, collections, operator\n```python\nimport os\nimport csv\nfrom collections import Counter\nfrom operator import itemgetter\n\ndef analyze_csv_files(dir):\n    \"\"\"\n    Analyze all CSV files in a directory, aggregate data from all files, \n    and find the most common value for each column.\n\n    Parameters:\n    dir (str): The directory containing the CSV files.\n\n    Returns:\n    dict: A dictionary with column names as keys and most common values as values.\n\n    Requirements:\n    - os\n    - csv\n    - collections\n    - operator\n\n    Example:\n    >>> common_values = analyze_csv_files('./csv_directory')\n    >>> print(common_values)\n    \"\"\"\n    data = {}\n\n    for file in os.listdir(dir):\n        if file.endswith('.csv'):\n            with open(os.path.join(dir, file), 'r') as csv_file:\n                reader = csv.DictReader(csv_file)\n                for row in reader:\n                    for column, value in row.items():\n                        if column not in data:\n                            data[column] = []\n                        data[column].append(value)\n\n    common_values = {column: Counter(values).most_common(1)[0][0] for column, values in data.items()}\n\n    return common_values\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, sendgrid, python_http_client.exceptions\n```python\nimport os\nfrom sendgrid import SendGridAPIClient\nfrom sendgrid.helpers.mail import Mail\nfrom python_http_client.exceptions import HTTPError\n\nSENDGRID_API_KEY = 'YOUR_SENDGRID_API_KEY'\nRECIPIENT_EMAIL = 'YOUR_EMAIL'\n\ndef email_directory_listing(dir):\n    \"\"\"\n    Get a list of files in a directory and email this list to a specified recipient.\n\n    Parameters:\n    dir (str): The directory to list.\n\n    Returns:\n    bool: True if the email was sent successfully, False otherwise.\n\n    Requirements:\n    - os\n    - sendgrid\n    - python_http_client.exceptions\n\n    Example:\n    >>> email_directory_listing('./test_directory')\n    \"\"\"\n    file_list = os.listdir(dir)\n    file_list_str = ', '.join(file_list)\n\n    message = Mail(\n        from_email='from_email@example.com',\n        to_emails=RECIPIENT_EMAIL,\n        subject=f'Directory Listing for {dir}',\n        plain_text_content=file_list_str)\n\n    try:\n        sg = SendGridAPIClient(SENDGRID_API_KEY)\n        response = sg.send(message)\n        return response.status_code == 202\n    except HTTPError as e:\n        print(e)\n        return False\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, zipfile, pathlib, time\n```python\nimport os\nimport zipfile\nfrom pathlib import Path\nimport time\n\ndef zip_directory(dir):\n    \"\"\"\n    Compress a directory into a zip file and return the time it took to compress.\n\n    Parameters:\n    dir (str): The directory to compress.\n\n    Returns:\n    float: The time it took to compress the directory.\n\n    Requirements:\n    - os\n    - zipfile\n    - pathlib\n    - time\n\n    Example:\n    >>> compress_time = zip_directory('./test_directory')\n    >>> print(compress_time)\n    \"\"\"\n    start_time = time.time()\n\n    with zipfile.ZipFile(f'{dir}.zip', 'w', zipfile.ZIP_DEFLATED) as zipf:\n        for root, dirs, files in os.walk(dir):\n            for file in files:\n                full_path = Path(root, file)\n                relative_path = full_path.relative_to(dir)\n                zipf.write(full_path, arcname=relative_path)\n\n    end_time = time.time()\n\n    return end_time - start_time\n```"}
{"seed": "{\"task_id\": 37709, \"prompt\": \"def f_37709(img):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"img is None\", \"test_start\": \"\\nimport cv2\\nimport numpy as np \\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(None) == True\\n    blank_image = np.zeros((10,5,3), np.uint8)\\n    assert candidate(blank_image) == False\\n\"], \"entry_point\": \"f_37709\", \"intent\": \"\\u753b\\u50cf`img`\\u304c\\u7a7a\\u304b\\u3069\\u3046\\u304b\\u3092\\u5224\\u5b9a\\u3059\\u308b\", \"library\": [\"cv2\", \"numpy\"]}", "generation": "Scenario 1: \ncv2, numpy, random, matplotlib.pyplot, os\n```python\nimport cv2\nimport numpy as np\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport os\n\n# Constants\nIMAGE_DIMENSIONS = (100, 100, 3)  # Height, Width, Channels\nIMAGE_PATH = 'images/'\n\ndef generate_and_save_random_images(num_images):\n    \"\"\"\n    Generate a specified number of random images with predefined dimensions \n    and save them to a specified directory.\n\n    Parameters:\n    num_images (int): The number of random images to generate.\n\n    Returns:\n    None\n\n    Requirements:\n    - cv2\n    - numpy\n    - random\n    - matplotlib.pyplot\n    - os\n\n    Example:\n    >>> generate_and_save_random_images(5)\n    \"\"\"\n    if not os.path.exists(IMAGE_PATH):\n        os.makedirs(IMAGE_PATH)\n    \n    for i in range(num_images):\n        img = np.random.randint(0, 256, IMAGE_DIMENSIONS, dtype=np.uint8)\n        cv2.imwrite(f'{IMAGE_PATH}/image_{i}.png', img)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncv2, numpy, PIL, scipy\n```python\nimport cv2\nimport numpy as np\nfrom PIL import Image\nfrom scipy import ndimage\n\n# Constants\nKERNEL_SIZE = 5\n\ndef apply_gaussian_blur(image_path):\n    \"\"\"\n    Apply a Gaussian blur to an image and return the blurred image.\n\n    Parameters:\n    image_path (str): The path to the image.\n\n    Returns:\n    ndarray: The blurred image.\n\n    Requirements:\n    - cv2\n    - numpy\n    - PIL\n    - scipy\n\n    Example:\n    >>> blurred_image = apply_gaussian_blur('images/image.png')\n    >>> Image.fromarray(blurred_image).show()\n    \"\"\"\n    image = cv2.imread(image_path)\n    blurred_image = ndimage.gaussian_filter(image, sigma=KERNEL_SIZE)\n    return blurred_image\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ncv2, numpy, PIL, matplotlib.pyplot\n```python\nimport cv2\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\ndef display_image_histogram(image_path):\n    \"\"\"\n    Display the color histogram of an image.\n\n    Parameters:\n    image_path (str): The path to the image.\n\n    Returns:\n    None\n\n    Requirements:\n    - cv2\n    - numpy\n    - PIL\n    - matplotlib.pyplot\n\n    Example:\n    >>> display_image_histogram('images/image.png')\n    \"\"\"\n    image = cv2.imread(image_path)\n    color = ('b', 'g', 'r')\n    for i, col in enumerate(color):\n        histr = cv2.calcHist([image], [i], None, [256], [0,256])\n        plt.plot(histr, color=col)\n    plt.xlim([0,256])\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ncv2, numpy, PIL, os\n```python\nimport cv2\nimport numpy as np\nfrom PIL import Image\nimport os\n\ndef check_and_delete_empty_images(directory_path):\n    \"\"\"\n    Check all images in a directory, and if an image is empty, delete it.\n\n    Parameters:\n    directory_path (str): The path to the directory.\n\n    Returns:\n    int: The number of deleted images.\n\n    Requirements:\n    - cv2\n    - numpy\n    - PIL\n    - os\n\n    Example:\n    >>> deleted_images = check_and_delete_empty_images('images/')\n    >>> print(f'Deleted {deleted_images} empty images.')\n    \"\"\"\n    deleted_images = 0\n    for filename in os.listdir(directory_path):\n        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n            img_path = os.path.join(directory_path, filename)\n            img = cv2.imread(img_path)\n            if img is None or np.all(img == 0):\n                os.remove(img_path)\n                deleted_images += 1\n    return deleted_images\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\ncv2, numpy, PIL, matplotlib.pyplot\n```python\nimport cv2\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\ndef compare_images(image1_path, image2_path):\n    \"\"\"\n    Compare two images and calculate the mean squared error.\n\n    Parameters:\n    image1_path (str): The path to the first image.\n    image2_path (str): The path to the second image.\n\n    Returns:\n    float: The mean squared error.\n\n    Requirements:\n    - cv2\n    - numpy\n    - PIL\n    - matplotlib.pyplot\n\n    Example:\n    >>> mse = compare_images('images/image1.png', 'images/image2.png')\n    >>> print(f'Mean Squared Error: {mse}')\n    \"\"\"\n    image1 = cv2.imread(image1_path)\n    image2 = cv2.imread(image2_path)\n    mse = np.mean((image1.astype(\"float\") - image2.astype(\"float\")) ** 2)\n    return mse\n```"}
{"seed": "{\"task_id\": 33677, \"prompt\": \"def f_33677(f):\\n\\t\", \"suffix\": \"\\n\\treturn coeffs\", \"canonical_solution\": \"\\n\\tp = Poly(f, x)\\n\\tcoeffs = p.coeffs()\\n\", \"test_start\": \"\\nfrom sympy import Poly, var\\n\\ndef check(candidate):\", \"test\": [\"\\n    var('x a b')\\n    f = a*(2*x**2 - 1) + 4*x**3 + x*(b - 3)\\n    co = candidate(f)\\n    assert co == [4, 2*a, b - 3, -a]\\n\"], \"entry_point\": \"f_33677\", \"intent\": \"`x`\\u306b\\u95a2\\u3059\\u308b\\u591a\\u9805\\u5f0f`f`\\u306e\\u5404\\u6b21\\u6570\\u306e\\u4fc2\\u6570\\u3092\\u6c42\\u3081\\u3066\\u30ea\\u30b9\\u30c8\\u306b\\u3059\\u308b `coeffs`\", \"library\": [\"sympy\"]}", "generation": "Scenario 1:\npandas, sympy, matplotlib, numpy\n```python\nimport pandas as pd\nfrom sympy import Poly, var, symbols\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nvar('x a b')\n\ndef plot_and_return_coeffs(f):\n    \"\"\"\n    Function to plot a polynomial, calculate and return coefficients of\n    the polynomial. The polynomial is defined with respect to 'x'.\n\n    Parameters:\n    f (sympy.Poly): The polynomial to be plotted and whose coefficients are to be calculated.\n\n    Returns:\n    list: List of coefficients of the polynomial.\n\n    Requirements:\n    - pandas\n    - sympy\n    - matplotlib.pyplot\n    - numpy\n    \n    Example:\n    >>> f = a*(2*x**2 - 1) + 4*x**3 + x*(b - 3)\n    >>> plot_and_return_coeffs(f)\n    \"\"\"\n    p = Poly(f, x)\n    coeffs = p.coeffs()\n\n    # generate x values\n    x_values = np.linspace(-10, 10, 400)\n    # generate y values\n    y_values = sum(coefficient*x_values**i for i, coefficient in enumerate(coeffs[::-1]))\n\n    # create dataframe\n    df = pd.DataFrame({\n        'x': x_values,\n        'y': y_values\n    })\n\n    # plot\n    plt.figure(figsize=(10,6))\n    plt.plot(df['x'], df['y'], label=str(f))\n    plt.title('Plot of the polynomial')\n    plt.xlabel('x')\n    plt.ylabel('f(x)')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n    return coeffs\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nsympy, pandas, matplotlib, numpy\n```python\nfrom sympy import Poly, var, symbols\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nvar('x y z a b')\n\ndef plot_3D_and_return_coeffs(f):\n    \"\"\"\n    Function to plot a 3D polynomial, calculate and return coefficients of\n    the polynomial. The polynomial is defined with respect to 'x', 'y' and 'z'.\n\n    Parameters:\n    f (sympy.Poly): The 3D polynomial to be plotted and whose coefficients are to be calculated.\n\n    Returns:\n    dict: Dictionary of coefficients of the polynomial.\n\n    Requirements:\n    - sympy\n    - pandas\n    - matplotlib.pyplot\n    - numpy\n    \n    Example:\n    >>> f = a*(2*x**2 - y) + 4*z**3 + x*y*(b - 3*z)\n    >>> plot_3D_and_return_coeffs(f)\n    \"\"\"\n    p = Poly(f, x, y, z)\n    coeffs = p.coeffs()\n\n    # generate x, y and z values\n    x_values = np.linspace(-10, 10, 400)\n    y_values = np.linspace(-10, 10, 400)\n    z_values = np.linspace(-10, 10, 400)\n\n    # create dataframe\n    df = pd.DataFrame({\n        'x': x_values,\n        'y': y_values,\n        'z': z_values,\n        'f': sum(coefficient*x_values**i*y_values**j*z_values**k for i, j, k, coefficient in coeffs.items())\n    })\n\n    # plot\n    fig = plt.figure(figsize=(10,6))\n    ax = fig.add_subplot(111, projection='3d')\n    ax.scatter(df['x'], df['y'], df['z'], c=df['f'], cmap='viridis')\n    ax.set_title('3D plot of the polynomial')\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_zlabel('z')\n    plt.show()\n\n    return coeffs\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nsympy, pandas, matplotlib, numpy\n```python\nfrom sympy import Poly, var, symbols\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nvar('x a b')\n\ndef plot_derivative_and_return_coeffs(f):\n    \"\"\"\n    Function to plot the derivative of a polynomial, calculate and return coefficients of\n    the original polynomial and its derivative. The polynomial is defined with respect to 'x'.\n\n    Parameters:\n    f (sympy.Poly): The polynomial to be plotted and whose coefficients are to be calculated.\n\n    Returns:\n    tuple: Tuple containing list of coefficients of the polynomial and its derivative.\n\n    Requirements:\n    - sympy\n    - pandas\n    - matplotlib.pyplot\n    - numpy\n    \n    Example:\n    >>> f = a*(2*x**2 - 1) + 4*x**3 + x*(b - 3)\n    >>> plot_derivative_and_return_coeffs(f)\n    \"\"\"\n    p = Poly(f, x)\n    coeffs = p.coeffs()\n\n    p_derivative = p.deriv()\n    coeffs_derivative = p_derivative.coeffs()\n\n    # generate x values\n    x_values = np.linspace(-10, 10, 400)\n    # generate y values\n    y_values = sum(coefficient*x_values**i for i, coefficient in enumerate(coeffs[::-1]))\n    y_values_derivative = sum(coefficient*x_values**i for i, coefficient in enumerate(coeffs_derivative[::-1]))\n\n    # create dataframe\n    df = pd.DataFrame({\n        'x': x_values,\n        'y': y_values,\n        'y_derivative': y_values_derivative\n    })\n\n    # plot\n    plt.figure(figsize=(10,6))\n    plt.plot(df['x'], df['y'], label='f(x)')\n    plt.plot(df['x'], df['y_derivative'], label=\"f'(x)\")\n    plt.title('Plot of the polynomial and its derivative')\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n    return coeffs, coeffs_derivative\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nsympy, pandas, matplotlib, numpy\n```python\nfrom sympy import Poly, var, symbols, solveset, S\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nvar('x a b')\n\ndef plot_solve_and_return_coeffs(f):\n    \"\"\"\n    Function to plot a polynomial, calculate and return coefficients of the polynomial,\n    and solve the polynomial equation f(x) = 0. The polynomial is defined with respect to 'x'.\n\n    Parameters:\n    f (sympy.Poly): The polynomial to be plotted, solved and whose coefficients are to be calculated.\n\n    Returns:\n    tuple: Tuple containing list of coefficients of the polynomial and solutions to the equation f(x) = 0.\n\n    Requirements:\n    - sympy\n    - pandas\n    - matplotlib.pyplot\n    - numpy\n    \n    Example:\n    >>> f = a*(2*x**2 - 1) + 4*x**3 + x*(b - 3)\n    >>> plot_solve_and_return_coeffs(f)\n    \"\"\"\n    p = Poly(f, x)\n    coeffs = p.coeffs()\n\n    # solve the equation f(x) = 0\n    solutions = solveset(p.as_expr(), x, domain=S.Reals)\n\n    # generate x values\n    x_values = np.linspace(-10, 10, 400)\n    # generate y values\n    y_values = sum(coefficient*x_values**i for i, coefficient in enumerate(coeffs[::-1]))\n\n    # create dataframe\n    df = pd.DataFrame({\n        'x': x_values,\n        'y': y_values\n    })\n\n    # plot\n    plt.figure(figsize=(10,6))\n    plt.plot(df['x'], df['y'], label=str(f))\n    plt.title('Plot of the polynomial')\n    plt.xlabel('x')\n    plt.ylabel('f(x)')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n    return coeffs, solutions\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nsympy, pandas, matplotlib, numpy\n```python\nfrom sympy import Poly, var, symbols, integrate\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nvar('x a b')\n\ndef plot_integrate_and_return_coeffs(f):\n    \"\"\"\n    Function to plot a polynomial, calculate and return coefficients of the polynomial,\n    and calculate the indefinite integral of the polynomial. The polynomial is defined with respect to 'x'.\n\n    Parameters:\n    f (sympy.Poly): The polynomial to be plotted, integrated and whose coefficients are to be calculated.\n\n    Returns:\n    tuple: Tuple containing list of coefficients of the polynomial and the indefinite integral of the polynomial.\n\n    Requirements:\n    - sympy\n    - pandas\n    - matplotlib.pyplot\n    - numpy\n    \n    Example:\n    >>> f = a*(2*x**2 - 1) + 4*x**3 + x*(b - 3)\n    >>> plot_integrate_and_return_coeffs(f)\n    \"\"\"\n    p = Poly(f, x)\n    coeffs = p.coeffs()\n\n    # calculate the indefinite integral of the polynomial\n    integral = integrate(p, x)\n\n    # generate x values\n    x_values = np.linspace(-10, 10, 400)\n    # generate y values\n    y_values = sum(coefficient*x_values**i for i, coefficient in enumerate(coeffs[::-1]))\n\n    # create dataframe\n    df = pd.DataFrame({\n        'x': x_values,\n        'y': y_values\n    })\n\n    # plot\n    plt.figure(figsize=(10,6))\n    plt.plot(df['x'], df['y'], label=str(f))\n    plt.title('Plot of the polynomial')\n    plt.xlabel('x')\n    plt.ylabel('f(x)')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n    return coeffs, integral\n```"}
{"seed": "{\"task_id\": 37449, \"prompt\": \"def f_37449(a, b):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"a & b\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(22, 56678) == 6\\n\", \"\\n    assert candidate(0, -1) == 0\\n\", \"\\n    assert candidate(1000, 1) == 0\\n\", \"\\n    assert candidate(479, 234) == 202\\n\"], \"entry_point\": \"f_37449\", \"intent\": \"\\u5909\\u6570`a`\\u3068`b`\\u306e\\u30d3\\u30c3\\u30c8\\u6f14\\u7b97\", \"library\": []}", "generation": "Scenario 1: \nnumpy, scipy.stats, random, matplotlib\n```python\nimport numpy as np\nfrom scipy.stats import norm\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nMEAN_RANGE = (0, 100)\nSD_RANGE = (1, 20)\n\ndef generate_gaussian_plots(a, b):\n    \"\"\"\n    Generate Gaussian distribution plots for a number of datasets. Each dataset \n    is defined by two parameters: the mean and standard deviation. These parameters \n    are generated by performing bitwise AND operation on two random numbers within \n    a certain range.\n    \n    Parameters:\n    a, b (int): The number of datasets to generate.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - numpy\n    - scipy.stats\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> generate_gaussian_plots(3, 5)\n    \"\"\"\n    num_datasets = a & b\n\n    for i in range(num_datasets):\n        mean = randint(*MEAN_RANGE)\n        sd = randint(*SD_RANGE)\n        x = np.linspace(mean - 3*sd, mean + 3*sd, 100)\n        y = norm.pdf(x, mean, sd)\n        \n        plt.plot(x, y, label=f'Dataset {i+1}')\n        \n    plt.legend()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, zipfile, glob, hashlib\n```python\nimport os\nimport zipfile\nfrom glob import glob\nimport hashlib\n\n# Constants\nFILE_NAME = 'sample.txt'\nARCHIVE_NAME = 'archive.zip'\n\ndef archive_and_hash(a, b):\n    \"\"\"\n    Archive a file and generate a hash of the archive. The file to archive and \n    the archive name are derived from performing bitwise AND operation on two \n    inputs.\n    \n    Parameters:\n    a, b (int): The inputs for the bitwise AND operation.\n    \n    Returns:\n    str: The hash of the archive.\n    \n    Requirements:\n    - os\n    - zipfile\n    - glob\n    - hashlib\n    \n    Example:\n    >>> archive_and_hash(22, 56678)\n    \"\"\"\n    if a & b:\n        with zipfile.ZipFile(ARCHIVE_NAME, 'w') as zipf:\n            zipf.write(FILE_NAME)\n        \n        with open(ARCHIVE_NAME, 'rb') as f:\n            data = f.read()\n            return hashlib.md5(data).hexdigest()\n    else:\n        return ''\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \npandas, numpy, scipy.stats, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\n# Constants\nDATA = {'A': np.random.randn(1000), 'B': np.random.randn(1000)}\n\ndef create_histogram(a, b):\n    \"\"\"\n    Create a histogram for two sets of data. The number of bins for each histogram \n    is determined by performing a bitwise AND operation on two inputs.\n    \n    Parameters:\n    a, b (int): The inputs for the bitwise AND operation.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - pandas\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n    \n    Example:\n    >>> create_histogram(22, 56678)\n    \"\"\"\n    df = pd.DataFrame(DATA)\n    num_bins = a & b\n\n    plt.hist(df['A'], bins=num_bins, alpha=0.5, label='A')\n    plt.hist(df['B'], bins=num_bins, alpha=0.5, label='B')\n    plt.legend(loc='upper right')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \npillow, numpy, matplotlib\n```python\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nIMAGE_PATH = 'sample.jpg'\n\ndef process_and_display_image(a, b):\n    \"\"\"\n    Open an image, apply a bitwise AND operation on the image data with two inputs, \n    and display the processed image.\n    \n    Parameters:\n    a, b (int): The inputs for the bitwise AND operation.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - pillow\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> process_and_display_image(22, 56678)\n    \"\"\"\n    img = Image.open(IMAGE_PATH)\n    img_data = np.array(img)\n    processed_data = img_data & a & b\n    processed_img = Image.fromarray(processed_data)\n\n    plt.imshow(processed_img)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnumpy, scipy.signal, matplotlib\n```python\nimport numpy as np\nfrom scipy import signal\nimport matplotlib.pyplot as plt\n\n# Constants\nSAMPLE_RATE = 1000\nFREQ = 5\n\ndef generate_signals_and_plot(a, b):\n    \"\"\"\n    Generate two sinusoidal signals with frequencies determined by a bitwise AND \n    operation on two inputs, and plot the signals.\n    \n    Parameters:\n    a, b (int): The inputs for the bitwise AND operation.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - numpy\n    - scipy.signal\n    - matplotlib.pyplot\n    \n    Example:\n    >>> generate_signals_and_plot(22, 56678)\n    \"\"\"\n    t = np.linspace(0, 1, SAMPLE_RATE, False)  # 1 second\n    freq_1 = a & b\n    freq_2 = b & a\n    sig_1 = np.sin(2*np.pi*freq_1*t)\n    sig_2 = np.sin(2*np.pi*freq_2*t)\n\n    plt.figure()\n    plt.plot(t, sig_1, label='Signal 1')\n    plt.plot(t, sig_2, label='Signal 2')\n    plt.legend()\n    plt.show()\n```\n"}
{"seed": "{\"task_id\": 42442, \"prompt\": \"def f_42442():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"globals()\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate() == globals()\\n\"], \"entry_point\": \"f_42442\", \"intent\": \"\\u30b0\\u30ed\\u30fc\\u30d0\\u30eb\\u5909\\u6570\\u306e\\u4e00\\u89a7\\u3092\\u5f97\\u308b\", \"library\": []}", "generation": "Scenario 1:\nos, datetime, psutil, matplotlib\n```python\nimport os\nimport datetime\nimport psutil\nimport matplotlib.pyplot as plt\n\ndef cpu_usage_over_time(duration):\n    \"\"\"\n    Monitor and plot the CPU usage over a specified duration in seconds.\n\n    Parameters:\n    duration (int): The duration in seconds for which to monitor the CPU usage.\n\n    Returns:\n    None\n\n    Requirements:\n    - os\n    - datetime\n    - psutil\n    - matplotlib.pyplot\n\n    Example:\n    >>> cpu_usage_over_time(60)\n    \"\"\"\n    end_time = datetime.datetime.now() + datetime.timedelta(seconds=duration)\n    usage_data = []\n\n    while datetime.datetime.now() < end_time:\n        usage = psutil.cpu_percent(interval=1)\n        usage_data.append(usage)\n\n    plt.plot(usage_data)\n    plt.ylabel('CPU Usage (%)')\n    plt.xlabel('Time (s)')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, sys, glob, hashlib\n```python\nimport os\nimport sys\nimport glob\nimport hashlib\n\ndef file_checksum(directory, extension):\n    \"\"\"\n    Compute the MD5 checksum for all files in a specified directory with a specified extension.\n\n    Parameters:\n    directory (str): The directory to search for files.\n    extension (str): The extension of the files.\n\n    Returns:\n    dict: A dictionary with file names as keys and their MD5 checksums as values.\n\n    Requirements:\n    - os\n    - sys\n    - glob\n    - hashlib\n\n    Example:\n    >>> file_checksum('./', '.txt')\n    \"\"\"\n    checksums = {}\n\n    for filename in glob.glob(os.path.join(directory, '*'+extension)):\n        md5_hash = hashlib.md5()\n\n        with open(filename,\"rb\") as f:\n            for byte_block in iter(lambda: f.read(4096),b\"\"):\n                md5_hash.update(byte_block)\n            checksums[os.path.basename(filename)] = md5_hash.hexdigest()\n\n    return checksums\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, pandas, numpy, matplotlib\n```python\nimport os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef analyze_directory(directory):\n    \"\"\"\n    Analyze a directory to get the count, mean, median, and mode of the file sizes \n    and plot a histogram of the file sizes.\n\n    Parameters:\n    directory (str): The directory to analyze.\n\n    Returns:\n    dict: A dictionary with count, mean, median, and mode of the file sizes.\n\n    Requirements:\n    - os\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> analyze_directory('./')\n    \"\"\"\n    file_sizes = [os.path.getsize(os.path.join(directory, f)) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n    file_sizes_series = pd.Series(file_sizes)\n\n    analysis = {\n        'count': len(file_sizes),\n        'mean': np.mean(file_sizes),\n        'median': np.median(file_sizes),\n        'mode': file_sizes_series.mode().tolist()\n    }\n\n    plt.hist(file_sizes, bins='auto')\n    plt.title('Histogram of file sizes')\n    plt.xlabel('File size')\n    plt.ylabel('Count')\n    plt.show()\n\n    return analysis\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, time, random, matplotlib\n```python\nimport os\nimport time\nimport random\nimport matplotlib.pyplot as plt\n\ndef disk_io_simulation(duration, file_path):\n    \"\"\"\n    Simulate disk I/O operations by writing random data to a file and plot the write speed over time.\n\n    Parameters:\n    duration (int): The duration in seconds for which to run the simulation.\n    file_path (str): The path to the file to write to.\n\n    Returns:\n    None\n\n    Requirements:\n    - os\n    - time\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> disk_io_simulation(60, './test.txt')\n    \"\"\"\n    end_time = time.time() + duration\n    write_speeds = []\n\n    while time.time() < end_time:\n        start_time = time.time()\n        with open(file_path, 'w') as f:\n            f.write(''.join(random.choices('abcdefghijklmnopqrstuvwxyz', k=1024*1024)))  # Write 1MB of data\n        elapsed_time = time.time() - start_time\n        write_speeds.append(1024 / elapsed_time)  # Write speed in MB/s\n\n    plt.plot(write_speeds)\n    plt.ylabel('Write Speed (MB/s)')\n    plt.xlabel('Time (s)')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nsys, time, threading, matplotlib\n```python\nimport sys\nimport time\nimport threading\nimport matplotlib.pyplot as plt\n\ndef multi_threaded_cpu_load(duration, num_threads):\n    \"\"\"\n    Generate CPU load using multiple threads and plot the load over time.\n\n    Parameters:\n    duration (int): The duration in seconds for which to generate the load.\n    num_threads (int): The number of threads to use.\n\n    Returns:\n    None\n\n    Requirements:\n    - sys\n    - time\n    - threading\n    - matplotlib.pyplot\n\n    Example:\n    >>> multi_threaded_cpu_load(60, 4)\n    \"\"\"\n    end_time = time.time() + duration\n    load_data = []\n\n    def load_generator():\n        while time.time() < end_time:\n            pass\n\n    threads = [threading.Thread(target=load_generator) for _ in range(num_threads)]\n\n    for t in threads:\n        t.start()\n\n    while time.time() < end_time:\n        load = sum(t.is_alive() for t in threads) / num_threads\n        load_data.append(load)\n        time.sleep(0.1)\n\n    for t in threads:\n        t.join()\n\n    plt.plot(load_data)\n    plt.ylabel('CPU Load (%)')\n    plt.xlabel('Time (s)')\n    plt.show()\n```"}
{"seed": "{\"task_id\": 38030, \"prompt\": \"def f_38030(word_list):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"Counter(word_list)\", \"test_start\": \"\\nfrom collections import Counter\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(['this', 'is', 'a', 'word', 'List']) == Counter({'List': 1, 'a': 1, 'is': 1, 'this': 1, 'word': 1})\\n\", \"\\n    assert candidate(['List']) == Counter({'List': 1})\\n\", \"\\n    assert candidate(['this', 'this', 'this', 'this', 'this']) == Counter({'this': 5})\\n\", \"\\n    assert candidate([]) == Counter({})\\n\"], \"entry_point\": \"f_38030\", \"intent\": \"\\u30ea\\u30b9\\u30c8`word_list'\\u5185\\u306b\\u51fa\\u73fe\\u3059\\u308b\\u5358\\u8a9e\\u3092\\u6570\\u3048\\u308b\", \"library\": [\"collections\"]}", "generation": "Scenario 3:\nnumpy, pandas, matplotlib, collections\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n\n# Constants\nEXCLUSIVE_WORDS = ['the', 'and', 'is', 'in', 'of', 'an', 'or', 'to', 'a']\n\ndef word_frequency_analysis(word_list):\n    \"\"\"\n    Analyze the frequency of words in a list excluding certain common words, \n    and visualize the top 10 most frequent words with a bar chart.\n\n    Parameters:\n    word_list (list): A list of words.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the word frequencies.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib\n    - collections\n\n    Example:\n    >>> word_list = ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', \n    ...              'lazy', 'dog', 'and', 'the', 'dog', 'was', 'not', 'happy']\n    >>> df = word_frequency_analysis(word_list)\n    >>> print(df)\n    >>> df.plot(kind='bar', x='Word', y='Frequency')\n    \"\"\"\n    word_list = [word.lower() for word in word_list if word.lower() not in EXCLUSIVE_WORDS]\n    counter = Counter(word_list)\n    \n    df = pd.DataFrame.from_dict(counter, orient='index').reset_index()\n    df = df.rename(columns={'index':'Word', 0:'Frequency'})\n    df = df.sort_values('Frequency', ascending=False)\n    df = df.head(10)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\njson, os, collections, time\n```python\nimport json\nimport os\nfrom collections import Counter\nimport time\n\n# Constants\nFILES_DIR = '/path/to/directory'\nFILE_EXTENSION = '.json'\n\ndef count_keywords_in_json_files():\n    \"\"\"\n    Count the occurrence of keywords in all json files in a given directory and \n    return the top 10 most common keywords.\n\n    Returns:\n    Counter: A Counter object with the keyword frequencies.\n\n    Requirements:\n    - json\n    - os\n    - collections\n    - time\n\n    Example:\n    >>> counter = count_keywords_in_json_files()\n    >>> print(counter.most_common(10))\n    \"\"\"\n    counter = Counter()\n\n    for filename in os.listdir(FILES_DIR):\n        if filename.endswith(FILE_EXTENSION):\n            with open(os.path.join(FILES_DIR, filename), 'r') as file:\n                data = json.load(file)\n                counter.update(data.keys())\n\n    return counter\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, pandas, collections, seaborn\n```python\nimport numpy as np\nimport pandas as pd\nfrom collections import Counter\nimport seaborn as sns\n\n# Constants\nEXCLUSIVE_WORDS = ['the', 'and', 'is', 'in', 'of', 'an', 'or', 'to', 'a']\n\ndef word_frequency_heatmap(word_list):\n    \"\"\"\n    Analyze the frequency of words in a list excluding certain common words, \n    and visualize the frequencies with a heatmap.\n\n    Parameters:\n    word_list (list): A list of words.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the word frequencies.\n\n    Requirements:\n    - numpy\n    - pandas\n    - collections\n    - seaborn\n\n    Example:\n    >>> word_list = ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', \n    ...              'lazy', 'dog', 'and', 'the', 'dog', 'was', 'not', 'happy']\n    >>> df = word_frequency_heatmap(word_list)\n    >>> print(df)\n    >>> sns.heatmap(df)\n    \"\"\"\n    word_list = [word.lower() for word in word_list if word.lower() not in EXCLUSIVE_WORDS]\n    counter = Counter(word_list)\n\n    df = pd.DataFrame.from_dict(counter, orient='index').reset_index()\n    df = df.rename(columns={'index':'Word', 0:'Frequency'})\n    df = df.sort_values('Frequency', ascending=False)\n    df = df.set_index('Word')\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 6:\npandas, collections, matplotlib, nltk\n```python\nimport pandas as pd\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\n\n# Constants\nEXCLUSIVE_WORDS = set(stopwords.words('english'))\n\ndef analyze_word_frequency(text_series):\n    \"\"\"\n    Analyze the frequency of words in a pandas Series excluding certain common words, \n    and visualize the top 10 most frequent words with a bar chart.\n\n    Parameters:\n    text_series (pd.Series): A pandas Series of texts.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the word frequencies.\n\n    Requirements:\n    - pandas\n    - collections\n    - matplotlib\n    - nltk.corpus\n\n    Example:\n    >>> text_series = pd.Series(['This is the first text', 'This is the second text'])\n    >>> df = analyze_word_frequency(text_series)\n    >>> print(df)\n    >>> df.plot(kind='bar', x='Word', y='Frequency')\n    \"\"\"\n    word_list = ' '.join(text_series).lower().split()\n    word_list = [word for word in word_list if word not in EXCLUSIVE_WORDS]\n    counter = Counter(word_list)\n\n    df = pd.DataFrame.from_dict(counter, orient='index').reset_index()\n    df = df.rename(columns={'index':'Word', 0:'Frequency'})\n    df = df.sort_values('Frequency', ascending=False)\n    df = df.head(10)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 7:\nnumpy, pandas, collections, sklearn\n```python\nimport numpy as np\nimport pandas as pd\nfrom collections import Counter\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Constants\nEXCLUSIVE_WORDS = ['the', 'and', 'is', 'in', 'of', 'an', 'or', 'to', 'a']\n\ndef analyze_word_frequency_sklearn(text_series):\n    \"\"\"\n    Analyze the frequency of words in a pandas Series excluding certain common words \n    using sklearn's CountVectorizer.\n\n    Parameters:\n    text_series (pd.Series): A pandas Series of texts.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the word frequencies.\n\n    Requirements:\n    - numpy\n    - pandas\n    - collections\n    - sklearn.feature_extraction.text\n\n    Example:\n    >>> text_series = pd.Series(['This is the first text', 'This is the second text'])\n    >>> df = analyze_word_frequency_sklearn(text_series)\n    >>> print(df)\n    \"\"\"\n    vectorizer = CountVectorizer(stop_words=EXCLUSIVE_WORDS)\n    X = vectorizer.fit_transform(text_series)\n    word_list = vectorizer.get_feature_names_out()\n    counter = Counter(word_list)\n\n    df = pd.DataFrame.from_dict(counter, orient='index').reset_index()\n    df = df.rename(columns={'index':'Word', 0:'Frequency'})\n    df = df.sort_values('Frequency', ascending=False)\n\n    return df\n```\n"}
{"seed": "{\"task_id\": 38724, \"prompt\": \"def f_38724(f, g):\\n\\t\", \"suffix\": \"\\n\\treturn add_functions\", \"canonical_solution\": \"def add_functions(f, g):\\n\\t    return lambda x: f(x) + g(x)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    def f(x): return x\\n    def g(y): return 1\\n    assert candidate(f,g)(f,g)(3) == 4\\n\"], \"entry_point\": \"f_38724\", \"intent\": \"\\u95a2\\u6570`f`\\u3068`g`\\u3092\\u53d7\\u3051\\u53d6\\u3063\\u3066\\u95a2\\u6570\\u540c\\u58eb\\u306e\\u548c\\u3092\\u8a08\\u7b97\\u3059\\u308b\\u95a2\\u6570`add_functions`\\u3092\\u5b9a\\u7fa9\\u3059\\u308b\", \"library\": []}", "generation": "Scenario 1: \nnumpy, scipy.optimize, matplotlib, functools\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import curve_fit\nfrom functools import reduce\n\ndef fit_and_plot(functions, x, y):\n    \"\"\"\n    Fit a set of functions to data and plot the individual fits and the combined fit.\n\n    Parameters:\n    functions (list): A list of functions to fit to the data.\n    x (np.array): The independent variable data.\n    y (np.array): The dependent variable data.\n\n    Returns:\n    list: A list of fitted parameter arrays for each function.\n\n    Requirements:\n    - numpy\n    - scipy.optimize.curve_fit\n    - matplotlib.pyplot\n    - functools.reduce\n\n    Example:\n    >>> x = np.linspace(0, 10, 100)\n    >>> y = x ** 2 + np.random.normal(size=100)\n    >>> fit_and_plot([np.square, np.sin], x, y)\n    \"\"\"\n    def combined_function(x, *params):\n        split_params = np.split(np.array(params), len(functions))\n        return sum(f(x, *p) for f, p in zip(functions, split_params))\n\n    popt, _ = curve_fit(combined_function, x, y)\n\n    plt.figure(figsize=(10, 6))\n    plt.plot(x, y, 'ko', label='Original data')\n\n    for i, function in enumerate(functions):\n        params = popt[i*2:(i+1)*2]\n        plt.plot(x, function(x, *params), label=f'Fit {i+1}: {function.__name__}')\n\n    plt.plot(x, combined_function(x, *popt), label='Combined fit')\n    plt.legend()\n\n    return np.split(popt, len(functions))\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, scipy.integrate, matplotlib, functools\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.integrate import odeint\nfrom functools import reduce\n\ndef solve_and_plot_system(functions, y0, t):\n    \"\"\"\n    Solve a system of ordinary differential equations (ODEs) and plot the solutions.\n\n    Parameters:\n    functions (list): A list of functions representing the ODEs.\n    y0 (list): Initial conditions for the ODEs.\n    t (np.array): Array of time points at which to solve.\n\n    Returns:\n    np.array: Array of the solution at each time point.\n\n    Requirements:\n    - numpy\n    - scipy.integrate.odeint\n    - matplotlib.pyplot\n    - functools.reduce\n\n    Example:\n    >>> def f(y, t):\n    >>>     return -2 * y\n    >>> y0 = [1]\n    >>> t = np.linspace(0, 5, 100)\n    >>> solve_and_plot_system([f], y0, t)\n    \"\"\"\n    def system(y, t):\n        return [f(y, t) for f in functions]\n\n    sol = odeint(system, y0, t)\n\n    plt.figure(figsize=(10, 6))\n    for i in range(sol.shape[1]):\n        plt.plot(t, sol[:, i], label=f'y{i+1}(t)')\n    plt.legend()\n\n    return sol\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, scipy.stats, matplotlib, functools\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nfrom functools import reduce\n\ndef plot_distribution_sum(distributions, x):\n    \"\"\"\n    Plot the sum of a set of distributions.\n\n    Parameters:\n    distributions (list): A list of scipy.stats distributions.\n    x (np.array): The x values at which to evaluate the distributions.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n    - functools.reduce\n\n    Example:\n    >>> x = np.linspace(-10, 10, 1000)\n    >>> dist1 = norm(loc=0, scale=1)\n    >>> dist2 = norm(loc=2, scale=2)\n    >>> plot_distribution_sum([dist1, dist2], x)\n    \"\"\"\n    y = reduce(lambda a, b: a + b.pdf(x), distributions, np.zeros_like(x))\n\n    plt.figure(figsize=(10, 6))\n    for dist in distributions:\n        plt.plot(x, dist.pdf(x), label=f'Distribution: mean {dist.mean()}, std {dist.std()}')\n    plt.plot(x, y, label='Sum of distributions', linewidth=2)\n    plt.legend()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, scipy.optimize, matplotlib, functools\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import minimize\nfrom functools import reduce\n\ndef minimize_and_plot(functions, x0):\n    \"\"\"\n    Minimize a set of functions and plot the minimization path.\n\n    Parameters:\n    functions (list): A list of functions to minimize.\n    x0 (np.array): The initial guess for the minimization.\n\n    Returns:\n    list: A list of scipy.optimize.OptimizeResult objects.\n\n    Requirements:\n    - numpy\n    - scipy.optimize.minimize\n    - matplotlib.pyplot\n    - functools.reduce\n\n    Example:\n    >>> def f(x):\n    >>>     return (x - 2) ** 2\n    >>> x0 = np.array([0])\n    >>> minimize_and_plot([f], x0)\n    \"\"\"\n    results = [minimize(f, x0) for f in functions]\n\n    plt.figure(figsize=(10, 6))\n    for i, result in enumerate(results):\n        plt.plot(result.x, functions[i](result.x), 'o', label=f'Minimum {i+1}')\n    plt.legend()\n\n    return results\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, scipy.special, matplotlib, functools\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.special import jn\nfrom functools import reduce\n\ndef plot_bessel_sum(orders, x):\n    \"\"\"\n    Plot the sum of a set of Bessel functions.\n\n    Parameters:\n    orders (list): A list of orders for the Bessel functions.\n    x (np.array): The x values at which to evaluate the Bessel functions.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - scipy.special.jn\n    - matplotlib.pyplot\n    - functools.reduce\n\n    Example:\n    >>> x = np.linspace(0, 10, 1000)\n    >>> plot_bessel_sum([0, 1, 2], x)\n    \"\"\"\n    y = reduce(lambda a, b: a + jn(b, x), orders, np.zeros_like(x))\n\n    plt.figure(figsize=(10, 6))\n    for order in orders:\n        plt.plot(x, jn(order, x), label=f'Bessel function of order {order}')\n    plt.plot(x, y, label='Sum of Bessel functions', linewidth=2)\n    plt.legend()\n```\n"}
{"seed": "{\"task_id\": 22439, \"prompt\": \"def f_22439(obj):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"type(obj)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('this is a string') == str\\n\", \"\\n    assert candidate(123.4) == float\\n\", \"\\n    assert candidate(400) == int\\n\", \"\\n    assert candidate({}) == dict \\n\", \"\\n    assert candidate([{}]) == list\\n\"], \"entry_point\": \"f_22439\", \"intent\": \"\\u30aa\\u30d6\\u30b8\\u30a7\\u30af\\u30c8`obj`\\u306e\\u30af\\u30e9\\u30b9\\u3092\\u5f97\\u308b\", \"library\": []}", "generation": "Scenario 1: \ncollections, json, re\n```python\nimport collections\nimport json\nimport re\n\ndef count_types_in_json(json_str):\n    \"\"\"\n    Count the number of each type of value in a JSON string.\n    \n    Parameters:\n    json_str (str): The JSON string.\n    \n    Returns:\n    dict: A dictionary with data types as keys and their counts as values.\n    \n    Requirements:\n    - collections\n    - json\n    - re\n    \n    Example:\n    >>> json_str = '{\"name\": \"John\", \"age\": 30, \"city\": \"New York\", \"isMarried\": true, \"friends\": [\"Jane\", \"Doe\"]}'\n    >>> print(count_types_in_json(json_str))\n    \"\"\"\n    data = json.loads(json_str)\n    type_counts = collections.defaultdict(int)\n\n    def count_types(obj):\n        if isinstance(obj, dict):\n            for value in obj.values():\n                count_types(value)\n        elif isinstance(obj, list):\n            for item in obj:\n                count_types(item)\n        else:\n            type_counts[type(obj).__name__] += 1\n\n    count_types(data)\n\n    return dict(type_counts)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, scipy, matplotlib\n```python\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\n# Constants\nMU = 0\nSTD_DEV = 1\nX = np.linspace(norm.ppf(0.01, MU, STD_DEV), norm.ppf(0.99, MU, STD_DEV), 100)\n\ndef plot_normal_distribution(mu=MU, std_dev=STD_DEV):\n    \"\"\"\n    Plot a normal distribution with a given mean and standard deviation.\n    \n    Parameters:\n    mu (float): The mean of the distribution.\n    std_dev (float): The standard deviation of the distribution.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n    \n    Example:\n    >>> plot_normal_distribution()\n    \"\"\"\n    y = norm.pdf(X, mu, std_dev)\n\n    plt.plot(X, y)\n    plt.title('Normal Distribution')\n    plt.xlabel('X')\n    plt.ylabel('Probability Density')\n    plt.grid(True)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, pathlib, collections\n```python\nimport os\nfrom pathlib import Path\nimport collections\n\ndef count_file_types_in_directory(directory):\n    \"\"\"\n    Count the number of each type of file in a directory.\n    \n    Parameters:\n    directory (str): The directory path.\n    \n    Returns:\n    dict: A dictionary with file extensions as keys and their counts as values.\n    \n    Requirements:\n    - os\n    - pathlib\n    - collections\n    \n    Example:\n    >>> print(count_file_types_in_directory('/home/user/Documents'))\n    \"\"\"\n    file_counts = collections.defaultdict(int)\n\n    for dirpath, dirnames, filenames in os.walk(directory):\n        for filename in filenames:\n            file_counts[Path(filename).suffix] += 1\n\n    return dict(file_counts)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, pandas, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nCATEGORIES = ['A', 'B', 'C', 'D', 'E']\n\ndef plot_random_categorical_data(n=100):\n    \"\"\"\n    Generate n random categorical data and plot a bar chart.\n    \n    Parameters:\n    n (int): The number of data points.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    \n    Example:\n    >>> plot_random_categorical_data()\n    \"\"\"\n    data = np.random.choice(CATEGORIES, n)\n    df = pd.DataFrame(data, columns=['Category'])\n\n    df['Category'].value_counts().plot(kind='bar')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\ncollections, string, re\n```python\nimport collections\nimport string\nimport re\n\ndef count_words_in_text(text):\n    \"\"\"\n    Count the number of each word in a text.\n    \n    Parameters:\n    text (str): The text.\n    \n    Returns:\n    dict: A dictionary with words as keys and their counts as values.\n    \n    Requirements:\n    - collections\n    - string\n    - re\n    \n    Example:\n    >>> text = 'This is a test. This is only a test.'\n    >>> print(count_words_in_text(text))\n    \"\"\"\n    text = text.lower()\n    text = re.sub('['+string.punctuation+']', '', text)\n    words = text.split()\n\n    return dict(collections.Counter(words))\n```"}
{"seed": "{\"task_id\": 22439, \"prompt\": \"def f_22439(obj):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"obj.__class__\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('this is a string') == str\\n\", \"\\n    assert candidate(123.4) == float\\n\", \"\\n    assert candidate(400) == int\\n\", \"\\n    assert candidate({}) == dict \\n\", \"\\n    assert candidate([{}]) == list\\n\"], \"entry_point\": \"f_22439\", \"intent\": \"\\u30aa\\u30d6\\u30b8\\u30a7\\u30af\\u30c8`obj`\\u306e\\u30af\\u30e9\\u30b9\\u3092\\u5f97\\u308b\", \"library\": []}", "generation": "Scenario 3: \npandas, numpy, random, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import choice, randint\nimport matplotlib.pyplot as plt\n\n# Constants\nANIMAL_CLASSES = ['Mammal', 'Bird', 'Reptile', 'Fish', 'Amphibian', 'Invertebrate']\nANIMALS = {\n    'Mammal': ['Dog', 'Cat', 'Elephant', 'Tiger', 'Bear'],\n    'Bird': ['Eagle', 'Sparrow', 'Penguin', 'Parrot', 'Owl'],\n    'Reptile': ['Snake', 'Crocodile', 'Tortoise', 'Lizard', 'Chameleon'],\n    'Fish': ['Shark', 'Salmon', 'Goldfish', 'Tuna', 'Trout'],\n    'Amphibian': ['Frog', 'Salamander', 'Newt', 'Toad', 'Caecilian'],\n    'Invertebrate': ['Spider', 'Scorpion', 'Butterfly', 'Bee', 'Ant']\n}\n\ndef generate_zoo_population_report():\n    \"\"\"\n    Generate a report of a zoo's animal population with different animal classes \n    and types, and plot a pie chart to show the percentage of each animal class.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the animal population report.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> report = generate_zoo_population_report()\n    >>> print(report)\n    >>> report['Animal Class'].value_counts().plot(kind='pie', autopct='%1.1f%%')\n    \"\"\"\n    population_data = []\n\n    for _ in range(1000):\n        animal_class = choice(ANIMAL_CLASSES)\n        animal_type = choice(ANIMALS[animal_class])\n        population_data.append([animal_class, animal_type])\n\n    population_df = pd.DataFrame(population_data, columns=['Animal Class', 'Animal Type'])\n\n    return population_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, scipy.stats, random\n```python\nimport numpy as np\nfrom scipy.stats import norm\nfrom random import randint\n\n# Constants\nPOPULATION_SIZE = 10000\nAVG_AGE = 40\nAGE_STD_DEV = 15\n\ndef generate_population_ages():\n    \"\"\"\n    Generate a population of ages with a given average and standard deviation, \n    and calculate the z-score for a randomly selected individual.\n    \n    Returns:\n    float: The z-score of the selected individual.\n    \n    Requirements:\n    - numpy\n    - scipy.stats\n    - random\n    \n    Example:\n    >>> z_score = generate_population_ages()\n    >>> print(f'The z-score of the selected individual is {z_score:.2f}.')\n    \"\"\"\n    population_ages = np.random.normal(AVG_AGE, AGE_STD_DEV, POPULATION_SIZE)\n    selected_individual = randint(0, POPULATION_SIZE-1)\n    z_score = norm.ppf((selected_individual + 0.5) / POPULATION_SIZE)\n\n    return z_score\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \npandas, os, json, numpy\n```python\nimport pandas as pd\nimport os\nimport json\nimport numpy as np\n\n# Constants\nDIRECTORY = 'data'\nFILE_EXTENSION = '.json'\n\ndef read_json_files_to_df():\n    \"\"\"\n    Read all .json files in a given directory and append them to a pandas DataFrame.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with all data from the .json files.\n    \n    Requirements:\n    - pandas\n    - os\n    - json\n    - numpy\n    \n    Example:\n    >>> df = read_json_files_to_df()\n    >>> print(df)\n    \"\"\"\n    data = []\n\n    for file in os.listdir(DIRECTORY):\n        if file.endswith(FILE_EXTENSION):\n            with open(os.path.join(DIRECTORY, file)) as json_file:\n                file_data = json.load(json_file)\n                data.append(file_data)\n\n    df = pd.DataFrame(data)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 6: \npandas, numpy, matplotlib, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef generate_data_and_plot_correlation():\n    \"\"\"\n    Generate a pandas DataFrame with random data and plot a correlation heatmap.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the generated data.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - seaborn\n    \n    Example:\n    >>> df = generate_data_and_plot_correlation()\n    >>> print(df)\n    >>> correlation = df.corr()\n    >>> sns.heatmap(correlation, annot=True, cmap='coolwarm')\n    \"\"\"\n    data = np.random.randint(0,100,size=(100, 5))\n    df = pd.DataFrame(data, columns=COLUMNS)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 7: \nnumpy, sklearn.model_selection, sklearn.ensemble, sklearn.metrics\n```python\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Constants\nFEATURES = np.random.rand(1000, 10)\nTARGET = np.random.randint(0, 2, 1000)\n\ndef train_random_forest_and_get_accuracy():\n    \"\"\"\n    Train a Random Forest classifier with generated data and return the accuracy score.\n    \n    Returns:\n    float: The accuracy score of the trained classifier.\n    \n    Requirements:\n    - numpy\n    - sklearn.model_selection\n    - sklearn.ensemble\n    - sklearn.metrics\n    \n    Example:\n    >>> accuracy = train_random_forest_and_get_accuracy()\n    >>> print(f'The accuracy of the trained Random Forest classifier is {accuracy:.2f}.')\n    \"\"\"\n    X_train, X_test, y_train, y_test = train_test_split(FEATURES, TARGET, test_size=0.2, random_state=42)\n\n    clf = RandomForestClassifier(random_state=42)\n    clf.fit(X_train, y_train)\n\n    y_pred = clf.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n\n    return accuracy\n```"}
{"seed": "{\"task_id\": 39340, \"prompt\": \"def f_39340(url):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"urllib.request.urlopen(url).read()\", \"test_start\": \"\\nimport urllib\\n\\ndef check(candidate):\", \"test\": [\"\\n    url = \\\"http://www.google.com\\\"\\n    text = b\\\"google\\\"\\n    assert text in candidate(url)\\n\"], \"entry_point\": \"f_39340\", \"intent\": \"\\u6307\\u5b9a\\u3057\\u305fURL`url`\\u306e\\u5185\\u5bb9\\u3092\\u8868\\u793a\\u3059\\u308b\", \"library\": [\"urllib\"]}", "generation": "Scenario 1:\nrequests, bs4, pandas, matplotlib\n```python\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef analyze_most_common_words(url):\n    \"\"\"\n    Fetch the content of a webpage and analyze the most common words in the text of the page.\n    \n    Parameters:\n    url (str): The url of the webpage.\n\n    Returns:\n    Series: A pandas Series object with the count of the most common words.\n    \n    Requirements:\n    - requests\n    - bs4\n    - pandas\n    - matplotlib.pyplot\n    \n    Example:\n    >>> common_words = analyze_most_common_words('http://www.google.com')\n    >>> print(common_words)\n    >>> common_words.plot(kind='bar')\n    \"\"\"\n    response = requests.get(url)\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    text = soup.get_text()\n    words = text.split()\n    \n    word_count = pd.Series(words).value_counts()\n\n    return word_count\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nurllib.request, zipfile, os, shutil\n```python\nimport urllib.request\nimport zipfile\nimport os\nimport shutil\n\nPATH = \"/tmp\"\n\ndef download_and_extract_zip(url, destination):\n    \"\"\"\n    Download a zip file from a given URL and extract it to a specified directory.\n    \n    Parameters:\n    url (str): The url of the zip file.\n    destination (str): The path to the directory where the zip file should be extracted.\n\n    Returns:\n    str: The path to the extracted directory.\n    \n    Requirements:\n    - urllib.request\n    - zipfile\n    - os\n    - shutil\n    \n    Example:\n    >>> download_and_extract_zip('http://example.com/file.zip', '/path/to/extract')\n    \"\"\"\n    filename = os.path.join(PATH, os.path.basename(url))\n    urllib.request.urlretrieve(url, filename)\n    \n    with zipfile.ZipFile(filename, 'r') as zip_ref:\n        zip_ref.extractall(destination)\n\n    os.remove(filename)\n\n    return destination\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nrequests, json, csv, time\n```python\nimport requests\nimport json\nimport csv\nimport time\n\ndef fetch_and_save_json(url, destination):\n    \"\"\"\n    Fetch JSON data from a given URL and save it as a CSV file to a specified directory.\n    \n    Parameters:\n    url (str): The url of the JSON data.\n    destination (str): The path to the directory where the CSV file should be saved.\n\n    Returns:\n    str: The path to the saved CSV file.\n    \n    Requirements:\n    - requests\n    - json\n    - csv\n    - time\n    \n    Example:\n    >>> fetch_and_save_json('http://example.com/data.json', '/path/to/save')\n    \"\"\"\n    response = requests.get(url)\n    data = json.loads(response.text)\n    \n    filename = os.path.join(destination, f'data_{int(time.time())}.csv')\n    \n    with open(filename, 'w', newline='') as csv_file:\n        writer = csv.writer(csv_file)\n        writer.writerow(data[0].keys())\n        for row in data:\n            writer.writerow(row.values())\n\n    return filename\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nurllib.request, PIL, numpy, matplotlib\n```python\nimport urllib.request\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef fetch_and_analyze_image(url):\n    \"\"\"\n    Fetch an image from a given URL, convert it to grayscale and display a histogram of pixel intensities.\n    \n    Parameters:\n    url (str): The url of the image.\n\n    Returns:\n    None\n    \n    Requirements:\n    - urllib.request\n    - PIL\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> fetch_and_analyze_image('http://example.com/image.jpg')\n    \"\"\"\n    with urllib.request.urlopen(url) as response:\n        img = Image.open(response)\n    \n    img = img.convert('L')\n    \n    pixel_values = np.array(img.getdata())\n    \n    plt.hist(pixel_values, bins=256, color='gray')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nurllib.request, re, collections\n```python\nimport urllib.request\nimport re\nimport collections\n\ndef find_most_common_words_in_url(url):\n    \"\"\"\n    Fetch the content of a webpage and find the most common words in the text of the page.\n    \n    Parameters:\n    url (str): The url of the webpage.\n\n    Returns:\n    dict: A dictionary with the most common words and their counts.\n    \n    Requirements:\n    - urllib.request\n    - re\n    - collections\n    \n    Example:\n    >>> common_words = find_most_common_words_in_url('http://www.google.com')\n    >>> print(common_words)\n    \"\"\"\n    with urllib.request.urlopen(url) as response:\n        text = response.read().decode()\n    \n    words = re.findall(r'\\w+', text.lower())\n    \n    word_count = collections.Counter(words).most_common()\n\n    return dict(word_count)\n```"}
{"seed": "{\"task_id\": 39589, \"prompt\": \"def f_39589(foldername, filename):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"os.path.join(foldername, filename)\", \"test_start\": \"\\nimport os\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('folder', 'file') == 'folder/file'\\n\", \"\\n    assert candidate('', 'file') == 'file'\\n\", \"\\n    assert candidate('.', 'file') == './file'\\n\"], \"entry_point\": \"f_39589\", \"intent\": \"\\u30d5\\u30a9\\u30eb\\u30c0\\u540d`foldername'\\u3068\\u30d5\\u30a1\\u30a4\\u30eb\\u540d`filename`\\u3092\\u7d50\\u5408\\u3057\\u305f\\u30d1\\u30b9\\u3092\\u5f97\\u308b\", \"library\": [\"os\"]}", "generation": "Scenario 1: \nos, shutil, csv, pandas\n```python\nimport os\nimport shutil\nimport csv\nimport pandas as pd\n\n# Constants\nCSV_SUFFIX = '.csv'\nFOLDER_NAME = 'data_folder'\nFILE_NAME = 'data_file'\n\ndef move_and_rename_csv_files(foldername, filename):\n    \"\"\"\n    Move all CSV files from a specified source folder to a destination folder and \n    rename them with a given filename. The new filename is appended with numbers \n    to avoid duplicate names. \n    \n    Parameters:\n    foldername (str): The source folder name.\n    filename (str): The new base filename.\n    \n    Returns:\n    list: A list of new file paths.\n    \n    Requirements:\n    - os\n    - shutil\n    - csv\n    - pandas\n    \n    Example:\n    >>> move_and_rename_csv_files('old_folder', 'new_file')\n    \"\"\"\n    files = os.listdir(foldername)\n    csv_files = [file for file in files if file.endswith(CSV_SUFFIX)]\n    new_file_paths = []\n\n    for i, csv_file in enumerate(csv_files):\n        new_file_name = f\"{filename}_{i}{CSV_SUFFIX}\"\n        new_file_path = os.path.join(FOLDER_NAME, new_file_name)\n\n        shutil.move(os.path.join(foldername, csv_file), new_file_path)\n        new_file_paths.append(new_file_path)\n\n    return new_file_paths\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, csv, pandas, matplotlib.pyplot\n```python\nimport os\nimport csv\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nCSV_SUFFIX = '.csv'\nFOLDER_NAME = 'data_folder'\nFILE_NAME = 'data_file'\n\ndef analyze_csv_files(foldername, filename):\n    \"\"\"\n    Search for a CSV file with a given filename in a specified folder and analyze \n    the data. The analysis includes loading the data into a pandas DataFrame, \n    providing basic statistics, and plotting a histogram for numerical columns.\n    \n    Parameters:\n    foldername (str): The folder name.\n    filename (str): The filename.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with basic statistics.\n    \n    Requirements:\n    - os\n    - csv\n    - pandas\n    - matplotlib.pyplot\n    \n    Example:\n    >>> analyze_csv_files('data_folder', 'data_file')\n    \"\"\"\n    filepath = os.path.join(foldername, filename)\n    if os.path.exists(filepath):\n        df = pd.read_csv(filepath)\n        stats = df.describe()\n\n        for col in df.columns:\n            if pd.api.types.is_numeric_dtype(df[col]):\n                df[col].plot(kind='hist')\n                plt.show()\n\n        return stats\n    else:\n        return None\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, shutil, csv, pandas\n```python\nimport os\nimport shutil\nimport csv\nimport pandas as pd\n\n# Constants\nCSV_SUFFIX = '.csv'\nFOLDER_NAME = 'data_folder'\nFILE_NAME = 'data_file'\n\ndef concatenate_csv_files(foldername, filename):\n    \"\"\"\n    Concatenate all CSV files from a specified source folder into a single file \n    with a given filename. The new file is created in the same folder.\n    \n    Parameters:\n    foldername (str): The source folder name.\n    filename (str): The new filename.\n    \n    Returns:\n    str: The path to the new file.\n    \n    Requirements:\n    - os\n    - shutil\n    - csv\n    - pandas\n    \n    Example:\n    >>> concatenate_csv_files('data_folder', 'combined_data')\n    \"\"\"\n    files = os.listdir(foldername)\n    csv_files = [file for file in files if file.endswith(CSV_SUFFIX)]\n\n    concatenated_df = pd.concat([pd.read_csv(os.path.join(foldername, csv_file)) for csv_file in csv_files])\n\n    new_file_path = os.path.join(foldername, f\"{filename}{CSV_SUFFIX}\")\n    concatenated_df.to_csv(new_file_path, index=False)\n\n    return new_file_path\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, random, csv, pandas\n```python\nimport os\nimport random\nimport csv\nimport pandas as pd\n\n# Constants\nCSV_SUFFIX = '.csv'\nFOLDER_NAME = 'data_folder'\nFILE_NAME = 'data_file'\n\ndef random_sample_csv(foldername, filename):\n    \"\"\"\n    Select a CSV file at random from a specified folder and load a random sample \n    of rows into a pandas DataFrame.\n    \n    Parameters:\n    foldername (str): The folder name.\n    filename (str): The filename.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the random sample.\n    \n    Requirements:\n    - os\n    - random\n    - csv\n    - pandas\n    \n    Example:\n    >>> random_sample_csv('data_folder', 'data_file')\n    \"\"\"\n    files = os.listdir(foldername)\n    csv_files = [file for file in files if file.endswith(CSV_SUFFIX)]\n    random_file = random.choice(csv_files)\n\n    df = pd.read_csv(os.path.join(foldername, random_file))\n\n    sample_df = df.sample(n=100)\n\n    return sample_df\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, glob, csv, pandas\n```python\nimport os\nimport glob\nimport csv\nimport pandas as pd\n\n# Constants\nCSV_SUFFIX = '.csv'\nFOLDER_NAME = 'data_folder'\nFILE_NAME = 'data_file'\n\ndef list_csv_files(foldername, filename):\n    \"\"\"\n    List all CSV files from a specified folder that contain a certain string \n    in their filename.\n    \n    Parameters:\n    foldername (str): The folder name.\n    filename (str): The string to search for in filenames.\n    \n    Returns:\n    list: A list of file paths.\n    \n    Requirements:\n    - os\n    - glob\n    - csv\n    - pandas\n    \n    Example:\n    >>> list_csv_files('data_folder', 'data')\n    \"\"\"\n    filepath_pattern = os.path.join(foldername, f\"*{filename}*{CSV_SUFFIX}\")\n    file_list = glob.glob(filepath_pattern)\n\n    return file_list\n```\n"}
{"seed": "{\"task_id\": 23577, \"prompt\": \"def f_23577(ax, l, h):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"ax.set_xlim(l, h)\", \"test_start\": \"\\nimport matplotlib.pyplot as plt\\n\\ndef check(candidate):\", \"test\": [\"\\n    fig, ax = plt.subplots()\\n    assert candidate(ax, 10, 100) == (10.0, 100.0)\\n\"], \"entry_point\": \"f_23577\", \"intent\": \"X\\u8ef8\\u306e\\u7bc4\\u56f2\\u3092\\u4e0b\\u9650`l`\\u3068\\u4e0a\\u9650`h`\\u306b\\u6307\\u5b9a\\u3059\\u308b\", \"library\": [\"matplotlib\"]}", "generation": "Scenario 1:\npandas, random, matplotlib, numpy, scipy.stats\n```python\nimport pandas as pd\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.stats as stats\n\n# Constants\nDATA_CATEGORIES = ['A', 'B', 'C', 'D', 'E']\nNUM_DATA_POINTS = 1000\n\ndef generate_data_visualize_distribution(ax, l, h):\n    \"\"\"\n    Generate a pandas DataFrame of random categorical data and visualize \n    the distribution within a specified range via a histogram on a given axis.\n\n    Parameters:\n    ax : matplotlib axis object\n    l (float): Lower limit of the x-axis.\n    h (float): Upper limit of the x-axis.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the generated data.\n    \n    Requirements:\n    - pandas\n    - random\n    - matplotlib.pyplot\n    - numpy\n    - scipy.stats\n    \n    Example:\n    >>> fig, ax = plt.subplots()\n    >>> data = generate_data_visualize_distribution(ax, 10, 100)\n    >>> print(data)\n    \"\"\"\n    # Generate data\n    data = pd.DataFrame({\n        'Category': [DATA_CATEGORIES[randint(0, len(DATA_CATEGORIES)-1)] for _ in range(NUM_DATA_POINTS)]\n    })\n    \n    # Calculate frequency\n    freq = data['Category'].value_counts(normalize=True)\n    \n    # Set x-axis limit\n    ax.set_xlim(l, h)\n    \n    # Plot histogram\n    ax.hist(freq, bins=20, edgecolor='black', alpha=0.7)\n\n    # Fit a normal distribution curve\n    mu, std = stats.norm.fit(freq)\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, mu, std)\n    ax.plot(x, p, 'k', linewidth=2)\n\n    return data\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, random, matplotlib, seaborn, numpy\n```python\nimport pandas as pd\nfrom random import randint, uniform\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Constants\nNUM_DATA_POINTS = 1000\n\ndef generate_scatter_plot(ax, l, h):\n    \"\"\"\n    Generate a pandas DataFrame of random numeric data and visualize \n    the data via a scatter plot on a given axis within a specified range.\n\n    Parameters:\n    ax : matplotlib axis object\n    l (float): Lower limit of the x-axis.\n    h (float): Upper limit of the x-axis.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the generated data.\n    \n    Requirements:\n    - pandas\n    - random\n    - matplotlib.pyplot\n    - seaborn\n    - numpy\n    \n    Example:\n    >>> fig, ax = plt.subplots()\n    >>> data = generate_scatter_plot(ax, 10, 100)\n    >>> print(data)\n    \"\"\"\n    # Generate data\n    data = pd.DataFrame({\n        'x': [uniform(0, 100) for _ in range(NUM_DATA_POINTS)],\n        'y': [uniform(0, 100) for _ in range(NUM_DATA_POINTS)]\n    })\n    \n    # Set x-axis limit\n    ax.set_xlim(l, h)\n    \n    # Plot scatter plot\n    sns.scatterplot(x='x', y='y', data=data, ax=ax)\n\n    return data\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, random, matplotlib, seaborn, numpy\n```python\nimport pandas as pd\nfrom random import randint, uniform\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Constants\nDATA_CATEGORIES = ['A', 'B', 'C', 'D', 'E']\nNUM_DATA_POINTS = 1000\n\ndef generate_box_plot(ax, l, h):\n    \"\"\"\n    Generate a pandas DataFrame of random numeric data and visualize \n    the data via a box plot on a given axis within a specified range.\n\n    Parameters:\n    ax : matplotlib axis object\n    l (float): Lower limit of the x-axis.\n    h (float): Upper limit of the x-axis.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the generated data.\n    \n    Requirements:\n    - pandas\n    - random\n    - matplotlib.pyplot\n    - seaborn\n    - numpy\n    \n    Example:\n    >>> fig, ax = plt.subplots()\n    >>> data = generate_box_plot(ax, 10, 100)\n    >>> print(data)\n    \"\"\"\n    # Generate data\n    data = pd.DataFrame({\n        'Category': [DATA_CATEGORIES[randint(0, len(DATA_CATEGORIES)-1)] for _ in range(NUM_DATA_POINTS)],\n        'Value': [uniform(0, 100) for _ in range(NUM_DATA_POINTS)]\n    })\n    \n    # Set x-axis limit\n    ax.set_xlim(l, h)\n    \n    # Plot box plot\n    sns.boxplot(x='Category', y='Value', data=data, ax=ax)\n\n    return data\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, random, matplotlib, seaborn, numpy\n```python\nimport pandas as pd\nfrom random import randint, uniform\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Constants\nDATA_CATEGORIES = ['A', 'B', 'C', 'D', 'E']\nNUM_DATA_POINTS = 1000\n\ndef generate_violin_plot(ax, l, h):\n    \"\"\"\n    Generate a pandas DataFrame of random numeric data and visualize \n    the data via a violin plot on a given axis within a specified range.\n\n    Parameters:\n    ax : matplotlib axis object\n    l (float): Lower limit of the x-axis.\n    h (float): Upper limit of the x-axis.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the generated data.\n    \n    Requirements:\n    - pandas\n    - random\n    - matplotlib.pyplot\n    - seaborn\n    - numpy\n    \n    Example:\n    >>> fig, ax = plt.subplots()\n    >>> data = generate_violin_plot(ax, 10, 100)\n    >>> print(data)\n    \"\"\"\n    # Generate data\n    data = pd.DataFrame({\n        'Category': [DATA_CATEGORIES[randint(0, len(DATA_CATEGORIES)-1)] for _ in range(NUM_DATA_POINTS)],\n        'Value': [uniform(0, 100) for _ in range(NUM_DATA_POINTS)]\n    })\n    \n    # Set x-axis limit\n    ax.set_xlim(l, h)\n    \n    # Plot violin plot\n    sns.violinplot(x='Category', y='Value', data=data, ax=ax)\n\n    return data\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, random, matplotlib, seaborn, numpy\n```python\nimport pandas as pd\nfrom random import randint, uniform\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Constants\nDATA_CATEGORIES = ['A', 'B', 'C', 'D', 'E']\nNUM_DATA_POINTS = 1000\n\ndef generate_bar_plot(ax, l, h):\n    \"\"\"\n    Generate a pandas DataFrame of random categorical data and visualize \n    the frequency of categories via a bar plot on a given axis within a specified range.\n\n    Parameters:\n    ax : matplotlib axis object\n    l (float): Lower limit of the x-axis.\n    h (float): Upper limit of the x-axis.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the generated data.\n    \n    Requirements:\n    - pandas\n    - random\n    - matplotlib.pyplot\n    - seaborn\n    - numpy\n    \n    Example:\n    >>> fig, ax = plt.subplots()\n    >>> data = generate_bar_plot(ax, 10, 100)\n    >>> print(data)\n    \"\"\"\n    # Generate data\n    data = pd.DataFrame({\n        'Category': [DATA_CATEGORIES[randint(0, len(DATA_CATEGORIES)-1)] for _ in range(NUM_DATA_POINTS)]\n    })\n    \n    # Calculate frequency\n    freq = data['Category'].value_counts()\n    \n    # Set x-axis limit\n    ax.set_xlim(l, h)\n    \n    # Plot bar plot\n    sns.barplot(x=freq.index, y=freq.values, ax=ax)\n\n    return data\n```\n"}
{"seed": "{\"task_id\": 41087, \"prompt\": \"def f_41087(src, range):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"int(math.ceil(src/float(range)) * range)\", \"test_start\": \"\\nimport math\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(22, 50) == 50\\n\", \"\\n    assert candidate(100, 23) == 115\\n\", \"\\n    assert candidate(0, 13) == 0\\n\", \"\\n    assert candidate(12, 1) == 12\\n\", \"\\n    assert candidate(34, 23) == 46\\n\"], \"entry_point\": \"f_41087\", \"intent\": \"\\u6574\\u6570`src`\\u3092\\u7279\\u5b9a\\u306e\\u7bc4\\u56f2`range`\\u306e\\u500d\\u6570\\u3067\\u5207\\u308a\\u4e0a\\u3052\\u308b\", \"library\": [\"math\"]}", "generation": "Scenario 1:\nnumpy, pandas, math, random\n```python\nimport numpy as np\nimport pandas as pd\nimport math\nimport random\n\n# Constants\nVALUES = [i for i in range(1, 101)]\nRANGE = [i for i in range(1, 26)]\n\ndef create_dataframe(values, range):\n    \"\"\"\n    Create a pandas DataFrame with two columns: 'Values' and 'Rounded Values'.\n    The 'Values' column contains the original values from the list.\n    The 'Rounded Values' column contains the rounded up values to the nearest multiple of a random range.\n\n    Parameters:\n    values (list): The list of values.\n    range (list): The list of possible ranges.\n\n    Returns:\n    DataFrame: A pandas DataFrame with original and rounded values.\n\n    Requirements:\n    - pandas\n    - numpy\n    - math\n    - random\n\n    Example:\n    >>> df = create_dataframe(VALUES, RANGE)\n    >>> print(df)\n    \"\"\"\n    df = pd.DataFrame(values, columns=['Values'])\n    df['Rounded Values'] = df['Values'].apply(lambda x: int(math.ceil(x/float(random.choice(range))) * random.choice(range)))\n    \n    return df\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, math, matplotlib.pyplot\n```python\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\n\n# Constants\nVALUES = [i for i in range(1, 101)]\nRANGE = 5\n\ndef plot_rounded_values(values, range):\n    \"\"\"\n    Plot a scatter plot of original values against their rounded values to a certain range.\n\n    Parameters:\n    values (list): The list of values.\n    range (int): The rounding range.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - math\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_rounded_values(VALUES, RANGE)\n    \"\"\"\n    rounded_values = [int(math.ceil(x/float(range)) * range) for x in values]\n\n    plt.scatter(values, rounded_values)\n    plt.xlabel('Original Values')\n    plt.ylabel('Rounded Values')\n    plt.title('Rounded Values vs Original Values')\n    plt.show()\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, math, scipy.stats\n```python\nimport numpy as np\nimport math\nimport scipy.stats as stats\n\n# Constants\nVALUES = np.array([i for i in range(1, 101)])\nRANGE = 5\n\ndef calculate_statistics(values, range):\n    \"\"\"\n    Calculate and return the mean, median, mode and standard deviation of the rounded values of an array to a certain range.\n\n    Parameters:\n    values (numpy array): The array of values.\n    range (int): The rounding range.\n\n    Returns:\n    float: mean, median, mode, std\n\n    Requirements:\n    - numpy\n    - math\n    - scipy.stats\n\n    Example:\n    >>> mean, median, mode, std = calculate_statistics(VALUES, RANGE)\n    \"\"\"\n    rounded_values = np.array([int(math.ceil(x/float(range)) * range) for x in values])\n\n    mean = np.mean(rounded_values)\n    median = np.median(rounded_values)\n    mode = stats.mode(rounded_values)[0][0]\n    std = np.std(rounded_values)\n\n    return mean, median, mode, std\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, math, seaborn\n```python\nimport numpy as np\nimport math\nimport seaborn as sns\n\n# Constants\nVALUES = [i for i in range(1, 101)]\nRANGE = 5\n\ndef plot_distribution(values, range):\n    \"\"\"\n    Plot a distribution plot of the rounded values of a list to a certain range.\n\n    Parameters:\n    values (list): The list of values.\n    range (int): The rounding range.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - math\n    - seaborn\n\n    Example:\n    >>> plot_distribution(VALUES, RANGE)\n    \"\"\"\n    rounded_values = [int(math.ceil(x/float(range)) * range) for x in values]\n\n    sns.histplot(rounded_values, kde=True)\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, math, scipy.optimize\n```python\nimport numpy as np\nimport math\nfrom scipy.optimize import curve_fit\n\n# Constants\nVALUES = np.array([i for i in range(1, 101)])\nRANGE = 5\n\ndef fit_curve(values, range):\n    \"\"\"\n    Fit a curve to the rounded values of an array to a certain range.\n\n    Parameters:\n    values (numpy array): The array of values.\n    range (int): The rounding range.\n\n    Returns:\n    list: Optimal values for the parameters so that the sum of the squared residuals is minimized.\n\n    Requirements:\n    - numpy\n    - math\n    - scipy.optimize\n\n    Example:\n    >>> popt = fit_curve(VALUES, RANGE)\n    \"\"\"\n    rounded_values = np.array([int(math.ceil(x/float(range)) * range) for x in values])\n\n    def func(x, a, b, c):\n        return a * np.exp(-b * x) + c\n\n    popt, _ = curve_fit(func, values, rounded_values)\n\n    return popt\n```\n"}
{"seed": "{\"task_id\": 41087, \"prompt\": \"def f_41087(src, range):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"src if src % range == 0 else src + range - src % range\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(22, 50) == 50\\n\", \"\\n    assert candidate(100, 23) == 115\\n\", \"\\n    assert candidate(0, 13) == 0\\n\", \"\\n    assert candidate(12, 1) == 12\\n\", \"\\n    assert candidate(34, 23) == 46\\n\"], \"entry_point\": \"f_41087\", \"intent\": \"\\u6574\\u6570`src`\\u3092\\u7279\\u5b9a\\u306e\\u7bc4\\u56f2`range`\\u306e\\u500d\\u6570\\u3067\\u5207\\u308a\\u4e0a\\u3052\\u308b\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, random, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\n\n# Constants\nRANGE = 50\nN_SAMPLES = 100\n\ndef f_41087_1():\n    \"\"\"\n    Generate a pandas DataFrame of N_SAMPLES random integers. Apply a function \n    to round up each integer to the nearest multiple of RANGE. Then plot the \n    distribution of the original and modified integers.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the original and modified integers.\n\n    Requirements:\n    - pandas\n    - numpy\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = f_41087_1()\n    >>> print(df)\n    >>> df.plot(kind='hist', bins=20, alpha=0.5)\n    \"\"\"\n    # Generate random integers\n    src = np.random.randint(1, 1000, N_SAMPLES)\n\n    # Apply the function\n    mod = src if src % RANGE == 0 else src + RANGE - src % RANGE\n\n    # Create a DataFrame\n    df = pd.DataFrame({\n        'Original': src,\n        'Modified': mod\n    })\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, scipy.stats, matplotlib\n```python\nimport numpy as np\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\n\n# Constants\nRANGE = 50\nN_SAMPLES = 1000\n\ndef f_41087_2():\n    \"\"\"\n    Generate a numpy array of N_SAMPLES random integers. Apply a function \n    to round up each integer to the nearest multiple of RANGE. Then plot the \n    probability density function (PDF) of the original and modified integers.\n\n    Returns:\n    ndarray: A numpy array with the original and modified integers.\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Example:\n    >>> arr = f_41087_2()\n    >>> print(arr)\n    >>> stats.probplot(arr, dist=\"norm\", plot=plt)\n    \"\"\"\n    # Generate random integers\n    src = np.random.randint(1, 1000, N_SAMPLES)\n\n    # Apply the function\n    mod = src if src % RANGE == 0 else src + RANGE - src % RANGE\n\n    # Plot the PDF\n    plt.figure(figsize=(10, 6))\n    plt.hist(mod, bins=50, density=True, alpha=0.6, color='g')\n\n    # Plot the PDF of the normal distribution for comparison\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, 0, RANGE)\n    plt.plot(x, p, 'k', linewidth=2)\n\n    plt.show()\n\n    return mod\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, sklearn.preprocessing, random\n```python\nimport pandas as pd\nimport numpy as np\nimport random\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Constants\nRANGE = 50\nN_SAMPLES = 100\n\ndef f_41087_3():\n    \"\"\"\n    Generate a pandas DataFrame of N_SAMPLES random integers. Apply a function \n    to round up each integer to the nearest multiple of RANGE. Then use MinMaxScaler \n    to scale the modified integers between 0 and 1.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the original and modified, scaled integers.\n\n    Requirements:\n    - pandas\n    - numpy\n    - random\n    - sklearn.preprocessing.MinMaxScaler\n\n    Example:\n    >>> df = f_41087_3()\n    >>> print(df)\n    \"\"\"\n    # Generate random integers\n    src = np.random.randint(1, 1000, N_SAMPLES)\n\n    # Apply the function\n    mod = src if src % RANGE == 0 else src + RANGE - src % RANGE\n\n    # Reshape the array for scaling\n    mod = mod.reshape(-1, 1)\n\n    # Scale the modified integers between 0 and 1\n    scaler = MinMaxScaler()\n    mod_scaled = scaler.fit_transform(mod)\n\n    # Create a DataFrame\n    df = pd.DataFrame({\n        'Original': src,\n        'Modified, Scaled': mod_scaled.flatten()\n    })\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, random, matplotlib, seaborn\n```python\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nRANGE = 50\nN_SAMPLES = 1000\n\ndef f_41087_4():\n    \"\"\"\n    Generate a numpy array of N_SAMPLES random integers. Apply a function \n    to round up each integer to the nearest multiple of RANGE. Then plot the \n    distribution of the modified integers using a seaborn distplot.\n\n    Returns:\n    ndarray: A numpy array with the modified integers.\n\n    Requirements:\n    - numpy\n    - random\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> arr = f_41087_4()\n    >>> print(arr)\n    >>> sns.distplot(arr)\n    \"\"\"\n    # Generate random integers\n    src = np.random.randint(1, 1000, N_SAMPLES)\n\n    # Apply the function\n    mod = src if src % RANGE == 0 else src + RANGE - src % RANGE\n\n    # Plot the distribution\n    sns.distplot(mod, kde=False, color='blue', bins=30)\n\n    return mod\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, random, sklearn.model_selection\n```python\nimport pandas as pd\nimport numpy as np\nimport random\nfrom sklearn.model_selection import train_test_split\n\n# Constants\nRANGE = 50\nN_SAMPLES = 100\n\ndef f_41087_5():\n    \"\"\"\n    Generate a pandas DataFrame of N_SAMPLES random integers. Apply a function \n    to round up each integer to the nearest multiple of RANGE. Then split the \n    DataFrame into a training set and a test set.\n\n    Returns:\n    tuple: A tuple with the training set and test set DataFrames.\n\n    Requirements:\n    - pandas\n    - numpy\n    - random\n    - sklearn.model_selection.train_test_split\n\n    Example:\n    >>> train_set, test_set = f_41087_5()\n    >>> print(train_set)\n    >>> print(test_set)\n    \"\"\"\n    # Generate random integers\n    src = np.random.randint(1, 1000, N_SAMPLES)\n\n    # Apply the function\n    mod = src if src % RANGE == 0 else src + RANGE - src % RANGE\n\n    # Create a DataFrame\n    df = pd.DataFrame({\n        'Original': src,\n        'Modified': mod\n    })\n\n    # Split the DataFrame into a training set and a test set\n    train_set, test_set = train_test_split(df, test_size=0.2)\n\n    return train_set, test_set\n```\n"}
{"seed": "{\"task_id\": 40711, \"prompt\": \"def f_40711(n, N):\\n\\t\", \"suffix\": \"\\n\\treturn answer\", \"canonical_solution\": \"random_numbers = np.random.rand(n)\\n\\tanswer = N * random_numbers / np.sum(random_numbers)\", \"test_start\": \"\\nimport numpy as np \\n\\ndef check(candidate):\", \"test\": [\"\\n    answer = candidate(20, 5)\\n    assert answer.shape == (20,)\\n    assert max(answer) < 5\\n\"], \"entry_point\": \"f_40711\", \"intent\": \"\\u8981\\u7d20\\u6570\\u306e\\u7dcf\\u548c\\u304c`N`\\u3068\\u306a\\u308b\\u5236\\u7d04\\u306e\\u4e0b\\u3001`n`\\u6b21\\u5143\\u306e\\u30e9\\u30f3\\u30c0\\u30e0\\u30d9\\u30af\\u30c8\\u30eb`answer`\\u3092\\u751f\\u6210\\u3059\\u308b\", \"library\": [\"numpy\"]}", "generation": "Scenario 1: \nnumpy, pandas, matplotlib, random\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nN = 100\nBINS = 10\n\ndef generate_random_histogram(n, N=N, bins=BINS):\n    \"\"\"\n    Generate a list of n random numbers whose sum is N and plot a histogram \n    of these numbers divided into a specified number of bins.\n    \n    Parameters:\n    n (int): The number of random numbers to generate.\n    N (int, optional): The total sum of the random numbers. Default is 100.\n    bins (int, optional): The number of bins for the histogram. Default is 10.\n    \n    Returns:\n    list: A list of n random numbers whose sum is N.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - random\n    \n    Example:\n    >>> random_numbers = generate_random_histogram(50)\n    >>> plt.hist(random_numbers, bins=BINS)\n    \"\"\"\n    random_numbers = np.random.rand(n)\n    random_numbers = N * random_numbers / np.sum(random_numbers)\n    return random_numbers.tolist()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, pandas, matplotlib, random, seaborn\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\nimport seaborn as sns\n\n# Constants\nN = 100\nBINS = 10\n\ndef generate_random_boxplot(n, N=N, bins=BINS):\n    \"\"\"\n    Generate a list of n random numbers whose sum is N and plot a boxplot \n    of these numbers.\n    \n    Parameters:\n    n (int): The number of random numbers to generate.\n    N (int, optional): The total sum of the random numbers. Default is 100.\n    bins (int, optional): The number of bins for the histogram. Default is 10.\n    \n    Returns:\n    list: A list of n random numbers whose sum is N.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - random\n    - seaborn\n    \n    Example:\n    >>> random_numbers = generate_random_boxplot(50)\n    >>> sns.boxplot(x=random_numbers)\n    \"\"\"\n    random_numbers = np.random.rand(n)\n    random_numbers = N * random_numbers / np.sum(random_numbers)\n    return random_numbers.tolist()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, pandas, matplotlib, random, sklearn.preprocessing\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\nfrom sklearn.preprocessing import normalize\n\n# Constants\nN = 100\n\ndef generate_random_normalized_data(n, N=N):\n    \"\"\"\n    Generate a list of n random numbers whose sum is N and normalize\n    the data so that the values range between 0 and 1.\n    \n    Parameters:\n    n (int): The number of random numbers to generate.\n    N (int, optional): The total sum of the random numbers. Default is 100.\n    \n    Returns:\n    list: A list of n normalized random numbers whose sum before normalization was N.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - random\n    - sklearn.preprocessing\n    \n    Example:\n    >>> normalized_random_numbers = generate_random_normalized_data(50)\n    >>> pd.Series(normalized_random_numbers).plot(kind='bar')\n    \"\"\"\n    random_numbers = np.random.rand(n)\n    random_numbers = N * random_numbers / np.sum(random_numbers)\n    normalized_random_numbers = normalize(random_numbers[:,np.newaxis], axis=0).ravel()\n    return normalized_random_numbers.tolist()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, pandas, matplotlib, random, scipy.stats\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\nfrom scipy import stats\n\n# Constants\nN = 100\n\ndef generate_random_poisson_distribution(n, N=N):\n    \"\"\"\n    Generate a list of n random numbers whose sum is N and fit a Poisson \n    distribution to the data.\n    \n    Parameters:\n    n (int): The number of random numbers to generate.\n    N (int, optional): The total sum of the random numbers. Default is 100.\n    \n    Returns:\n    list: A list of n random numbers whose sum is N.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - random\n    - scipy.stats\n    \n    Example:\n    >>> random_numbers = generate_random_poisson_distribution(50)\n    >>> plt.hist(random_numbers, bins=10, density=True)\n    >>> mu = np.mean(random_numbers)\n    >>> x = np.arange(0, np.max(random_numbers), 0.5)\n    >>> plt.plot(x, stats.poisson.pmf(x, mu), 'r--')\n    \"\"\"\n    random_numbers = np.random.rand(n)\n    random_numbers = N * random_numbers / np.sum(random_numbers)\n    return random_numbers.tolist()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, pandas, matplotlib, random, sklearn.cluster\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\nfrom sklearn.cluster import KMeans\n\n# Constants\nN = 100\nCLUSTERS = 3\n\ndef generate_random_clustered_data(n, N=N, clusters=CLUSTERS):\n    \"\"\"\n    Generate a list of n random numbers whose sum is N and apply K-means clustering \n    to the data.\n    \n    Parameters:\n    n (int): The number of random numbers to generate.\n    N (int, optional): The total sum of the random numbers. Default is 100.\n    clusters (int, optional): The number of clusters for K-means. Default is 3.\n    \n    Returns:\n    list: A list of n random numbers whose sum is N.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - random\n    - sklearn.cluster\n    \n    Example:\n    >>> random_numbers = generate_random_clustered_data(50)\n    >>> kmeans = KMeans(n_clusters=clusters)\n    >>> kmeans.fit(np.array(random_numbers).reshape(-1, 1))\n    >>> plt.scatter(random_numbers, [0]*len(random_numbers), c=kmeans.labels_)\n    \"\"\"\n    random_numbers = np.random.rand(n)\n    random_numbers = N * random_numbers / np.sum(random_numbers)\n    return random_numbers.tolist()\n```\n"}
{"seed": "{\"task_id\": 40711, \"prompt\": \"def f_40711(n, N):\\n\\t\", \"suffix\": \"\\n\\treturn answer\", \"canonical_solution\": \"answer = np.random.dirichlet(np.ones(n)) * N\", \"test_start\": \"\\nimport numpy as np \\n\\ndef check(candidate):\", \"test\": [\"\\n    answer = candidate(20, 5)\\n    assert answer.shape == (20,)\\n\"], \"entry_point\": \"f_40711\", \"intent\": \"\\u8981\\u7d20\\u6570\\u306e\\u7dcf\\u548c\\u304c`N`\\u3068\\u306a\\u308b\\u5236\\u7d04\\u306e\\u4e0b\\u3001`n`\\u6b21\\u5143\\u306e\\u30e9\\u30f3\\u30c0\\u30e0\\u30d9\\u30af\\u30c8\\u30eb`answer`\\u3092\\u751f\\u6210\\u3059\\u308b\", \"library\": [\"numpy\"]}", "generation": "Scenario 1:\nnumpy, matplotlib.pyplot, random\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nCOLORS = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n\ndef generate_random_vector_and_plot(n, N):\n    \"\"\"\n    Generate an n-dimensional random vector with elements summing to N and plot a bar graph of the elements.\n\n    Parameters:\n    n (int): The number of dimensions.\n    N (float): The sum of the elements.\n\n    Returns:\n    np.ndarray: The generated random vector.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> vector = generate_random_vector_and_plot(5, 10)\n    >>> print(vector)\n    \"\"\"\n    vector = np.random.dirichlet(np.ones(n)) * N\n    plt.bar(range(n), vector, color=COLORS[randint(0, len(COLORS)-1)])\n    plt.show()\n\n    return vector\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, pandas, sklearn.preprocessing\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef generate_random_vector_and_normalize(n, N):\n    \"\"\"\n    Generate an n-dimensional random vector with elements summing to N and normalize the vector to have values between 0 and 1.\n\n    Parameters:\n    n (int): The number of dimensions.\n    N (float): The sum of the elements.\n\n    Returns:\n    np.ndarray: The normalized random vector.\n\n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.preprocessing\n\n    Example:\n    >>> vector = generate_random_vector_and_normalize(5, 10)\n    >>> print(vector)\n    \"\"\"\n    vector = np.random.dirichlet(np.ones(n)) * N\n    vector = vector.reshape(-1, 1)\n    scaler = MinMaxScaler()\n    normalized_vector = scaler.fit_transform(vector)\n\n    return normalized_vector\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, matplotlib.pyplot, seaborn\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef generate_random_vector_and_plot_distribution(n, N):\n    \"\"\"\n    Generate an n-dimensional random vector with elements summing to N and plot a histogram of the elements.\n\n    Parameters:\n    n (int): The number of dimensions.\n    N (float): The sum of the elements.\n\n    Returns:\n    np.ndarray: The generated random vector.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> vector = generate_random_vector_and_plot_distribution(5, 10)\n    >>> print(vector)\n    \"\"\"\n    vector = np.random.dirichlet(np.ones(n)) * N\n    plt.figure(figsize=(7,5))\n    sns.histplot(vector, bins=10, color='skyblue', kde=True)\n    plt.show()\n\n    return vector\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, pandas, sklearn.decomposition\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.decomposition import PCA\n\ndef generate_random_vector_and_pca(n, N):\n    \"\"\"\n    Generate an n-dimensional random vector with elements summing to N and perform Principal Component Analysis on the vector.\n\n    Parameters:\n    n (int): The number of dimensions.\n    N (float): The sum of the elements.\n\n    Returns:\n    np.ndarray: The transformed vector after PCA.\n\n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.decomposition\n\n    Example:\n    >>> vector = generate_random_vector_and_pca(5, 10)\n    >>> print(vector)\n    \"\"\"\n    vector = np.random.dirichlet(np.ones(n)) * N\n    pca = PCA(n_components=2)\n    transformed_vector = pca.fit_transform(vector.reshape(-1, 1))\n\n    return transformed_vector\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, pandas, sklearn.cluster\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import KMeans\n\ndef generate_random_vector_and_clustering(n, N):\n    \"\"\"\n    Generate an n-dimensional random vector with elements summing to N and apply K-Means clustering on the vector.\n\n    Parameters:\n    n (int): The number of dimensions.\n    N (float): The sum of the elements.\n\n    Returns:\n    np.ndarray: The cluster labels for the vector's elements.\n\n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.cluster\n\n    Example:\n    >>> vector = generate_random_vector_and_clustering(5, 10)\n    >>> print(vector)\n    \"\"\"\n    vector = np.random.dirichlet(np.ones(n)) * N\n    kmeans = KMeans(n_clusters=2, random_state=0)\n    labels = kmeans.fit_predict(vector.reshape(-1, 1))\n\n    return labels\n```"}
{"seed": "{\"task_id\": 31924, \"prompt\": \"def f_31924(li):\\n\\t\", \"suffix\": \"\\n\\treturn li\", \"canonical_solution\": \"random.shuffle(li)\", \"test_start\": \"\\nimport random\\n\\ndef check(candidate):\", \"test\": [\"\\n    li_a = [i for i in range(10)]\\n    li_a = candidate(li_a)\\n    assert sorted(li_a) == [i for i in range(10)]\\n\"], \"entry_point\": \"f_31924\", \"intent\": \"\\u30ea\\u30b9\\u30c8`li`\\u3092\\u30e9\\u30f3\\u30c0\\u30e0\\u306b\\u4e26\\u3073\\u66ff\\u3048\\u308b\", \"library\": [\"random\"]}", "generation": "Scenario 1: \npandas, random, matplotlib, numpy\n```python\nimport pandas as pd\nimport random\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nCATEGORIES = ['Fruit', 'Vegetable', 'Meat', 'Dairy', 'Bakery']\nITEMS = {\n    'Fruit': ['Apple', 'Banana', 'Cherry'],\n    'Vegetable': ['Carrot', 'Potato', 'Onion'],\n    'Meat': ['Chicken', 'Beef', 'Pork'],\n    'Dairy': ['Milk', 'Cheese', 'Butter'],\n    'Bakery': ['Bread', 'Cake', 'Cookie']\n}\n\ndef generate_random_shopping_list(num_items):\n    \"\"\"\n    Generate a random shopping list with a given number of items. The items are \n    randomly selected from a set of categories, and for each category, an item \n    is also randomly selected. The function then creates a pandas DataFrame \n    that represents the shopping list, with columns for the category and item.\n    The DataFrame is then shuffled to randomize the order of items.\n    \n    Parameters:\n    num_items (int): The number of items to generate for the shopping list.\n    \n    Returns:\n    DataFrame: A pandas DataFrame representing the shopping list.\n    \n    Requirements:\n    - pandas\n    - random\n    - matplotlib.pyplot\n    - numpy\n    \n    Example:\n    >>> shopping_list = generate_random_shopping_list(10)\n    >>> print(shopping_list)\n    \"\"\"\n    shopping_list = []\n    for _ in range(num_items):\n        category = random.choice(CATEGORIES)\n        item = random.choice(ITEMS[category])\n        shopping_list.append([category, item])\n    \n    df = pd.DataFrame(shopping_list, columns=['Category', 'Item'])\n    df = df.sample(frac=1).reset_index(drop=True)\n    \n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nrandom, numpy, itertools, collections\n```python\nimport random\nimport numpy as np\nfrom itertools import combinations\nfrom collections import Counter\n\n# Constants\nNUMBERS = list(range(1, 50))\nCOMBINATION_SIZE = 5\n\ndef random_combination_counter(num_combinations):\n    \"\"\"\n    Generate a given number of random combinations of a predefined set of numbers, \n    then count how many times each combination was generated.\n    \n    Parameters:\n    num_combinations (int): The number of random combinations to generate.\n    \n    Returns:\n    dict: A dictionary where the keys are the combinations and the values are \n    the counts of how many times each combination was generated.\n    \n    Requirements:\n    - random\n    - numpy\n    - itertools\n    - collections\n    \n    Example:\n    >>> counts = random_combination_counter(10000)\n    >>> print(counts)\n    \"\"\"\n    all_combinations = list(combinations(NUMBERS, COMBINATION_SIZE))\n    \n    chosen_combinations = []\n    for _ in range(num_combinations):\n        chosen_combinations.append(random.choice(all_combinations))\n    \n    counts = Counter(chosen_combinations)\n    \n    return dict(counts)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nrandom, matplotlib, numpy, collections\n```python\nimport random\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom collections import Counter\n\n# Constants\nLETTERS = list('ABCDEFGHIJKLMNOPQRSTUVWXYZ')\n\ndef random_letter_distribution(num_letters):\n    \"\"\"\n    Generate a distribution of random letters, then plot a histogram of the \n    distribution.\n    \n    Parameters:\n    num_letters (int): The number of random letters to generate.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - random\n    - matplotlib\n    - numpy\n    - collections\n    \n    Example:\n    >>> random_letter_distribution(10000)\n    \"\"\"\n    letters = [random.choice(LETTERS) for _ in range(num_letters)]\n    counts = Counter(letters)\n    \n    plt.bar(counts.keys(), counts.values())\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nrandom, pandas, numpy, matplotlib\n```python\nimport random\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nGENDERS = ['Male', 'Female']\nAGES = list(range(15, 100))\n\ndef generate_random_demographics(num_people):\n    \"\"\"\n    Generate a random demographics data for a given number of people. The data \n    includes gender and age. The function then creates a pandas DataFrame that \n    represents the data, and plots a histogram of the age distribution for each \n    gender.\n    \n    Parameters:\n    num_people (int): The number of people to generate data for.\n    \n    Returns:\n    DataFrame: A pandas DataFrame representing the data.\n    \n    Requirements:\n    - random\n    - pandas\n    - numpy\n    - matplotlib\n    \n    Example:\n    >>> demographics = generate_random_demographics(1000)\n    >>> print(demographics)\n    \"\"\"\n    data = []\n    for _ in range(num_people):\n        gender = random.choice(GENDERS)\n        age = random.choice(AGES)\n        data.append([gender, age])\n    \n    df = pd.DataFrame(data, columns=['Gender', 'Age'])\n    \n    df.groupby('Gender')['Age'].plot(kind='hist', alpha=0.5, bins=20)\n    plt.legend()\n    plt.show()\n    \n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nrandom, pandas, numpy, matplotlib\n```python\nimport random\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCOUNTRIES = ['USA', 'UK', 'Germany', 'France', 'Japan']\nPRODUCTS = ['Product A', 'Product B', 'Product C', 'Product D', 'Product E']\n\ndef generate_random_sales_data(num_sales):\n    \"\"\"\n    Generate random sales data for a given number of sales. The data includes \n    country, product, and sales amount. The function then creates a pandas \n    DataFrame that represents the data, and plots a bar chart of the total \n    sales amount for each product by country.\n    \n    Parameters:\n    num_sales (int): The number of sales to generate data for.\n    \n    Returns:\n    DataFrame: A pandas DataFrame representing the data.\n    \n    Requirements:\n    - random\n    - pandas\n    - numpy\n    - matplotlib\n    \n    Example:\n    >>> sales_data = generate_random_sales_data(1000)\n    >>> print(sales_data)\n    \"\"\"\n    data = []\n    for _ in range(num_sales):\n        country = random.choice(COUNTRIES)\n        product = random.choice(PRODUCTS)\n        sales_amount = random.uniform(100, 1000)\n        data.append([country, product, sales_amount])\n    \n    df = pd.DataFrame(data, columns=['Country', 'Product', 'Sales Amount'])\n    \n    df.groupby(['Country', 'Product'])['Sales Amount'].sum().unstack().plot(kind='bar', stacked=True)\n    plt.show()\n    \n    return df\n```\n`GPT_ODEX_BREAK`"}
{"seed": "{\"task_id\": 40343, \"prompt\": \"def f_40343(a, b):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"itertools.product(a,b)\", \"test_start\": \"\\nimport itertools\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert list(candidate([1,2],[3,4])) == [(1, 3), (1, 4), (2, 3), (2, 4)]\\n\"], \"entry_point\": \"f_40343\", \"intent\": \"\\u8907\\u6570\\u306e\\u30ea\\u30b9\\u30c8`a`\\u3068`b`\\u306e\\u76f4\\u7a4d\\uff08\\u30c7\\u30ab\\u30eb\\u30c8\\u7a4d\\uff09\\u3092\\u751f\\u6210\\u3057\\u3001\\u8981\\u7d20\\u306e\\u7d44\\u307f\\u5408\\u308f\\u305b\\u306e\\u7d50\\u679c\\u3092\\u5f97\\u308b\", \"library\": [\"itertools\"]}", "generation": "Scenario 1:\npandas, itertools, numpy, matplotlib\n```python\nimport pandas as pd\nimport itertools\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nPRODUCT_CATEGORIES = ['Electronics', 'Clothing', 'Home & Kitchen', 'Books', 'Sports']\nCITIES = ['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']\n\ndef generate_sales_report(product_categories, cities):\n    \"\"\"\n    Generate a sales report for each combination of product category and city. The sales \n    are randomly generated.\n\n    Parameters:\n    product_categories (list): A list of product categories.\n    cities (list): A list of cities.\n\n    Returns:\n    DataFrame: A pandas DataFrame with sales data for each combination of product category and city.\n\n    Requirements:\n    - pandas\n    - itertools\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> report = generate_sales_report(PRODUCT_CATEGORIES, CITIES)\n    >>> print(report)\n    >>> report.plot(kind='bar')\n    \"\"\"\n    sales_data = []\n    \n    for product, city in itertools.product(product_categories, cities):\n        sales = np.random.randint(1000, 10000)\n        sales_data.append([product, city, sales])\n\n    sales_df = pd.DataFrame(sales_data, columns=['Product Category', 'City', 'Sales'])\n\n    return sales_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nitertools, string, random, collections\n```python\nimport itertools\nimport string\nimport random\nfrom collections import Counter\n\n# Constants\nLETTERS = string.ascii_lowercase\nN = 5\n\ndef generate_random_strings(n, number_of_strings):\n    \"\"\"\n    Generate a number of strings of length n, then count the occurrence of each letter in \n    all strings combined.\n\n    Parameters:\n    n (int): The length of each string.\n    number_of_strings (int): The number of strings to generate.\n\n    Returns:\n    dict: A dictionary with the occurrence of each letter.\n\n    Requirements:\n    - itertools\n    - string\n    - random\n    - collections\n\n    Example:\n    >>> generate_random_strings(N, 1000)\n    \"\"\"\n    strings = [''.join(random.choices(LETTERS, k=n)) for _ in range(number_of_strings)]\n    combined = ''.join(strings)\n    letter_counts = dict(Counter(combined))\n\n    return letter_counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nitertools, numpy, scipy.stats, matplotlib\n```python\nimport itertools\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\n# Constants\nN = 5\nM = 1000\n\ndef generate_normal_distributions(n, m):\n    \"\"\"\n    Generate n normal distributions with m samples each, then calculate and plot the \n    correlation matrix of these distributions.\n\n    Parameters:\n    n (int): The number of distributions.\n    m (int): The number of samples in each distribution.\n\n    Returns:\n    numpy.ndarray: The correlation matrix of the distributions.\n\n    Requirements:\n    - itertools\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Example:\n    >>> generate_normal_distributions(N, M)\n    \"\"\"\n    distributions = [np.random.normal(size=m) for _ in range(n)]\n    correlations = np.corrcoef(distributions)\n\n    plt.imshow(correlations, cmap='hot', interpolation='nearest')\n    plt.show()\n\n    return correlations\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nitertools, numpy, sklearn.preprocessing, pandas\n```python\nimport itertools\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\n\n# Constants\nFEATURES = ['feature1', 'feature2', 'feature3', 'feature4', 'feature5']\nN = 1000\n\ndef generate_and_scale_data(features, n):\n    \"\"\"\n    Generate a dataset with n samples and a number of features. Then scale the features\n    to have zero mean and unit variance.\n\n    Parameters:\n    features (list): A list of feature names.\n    n (int): The number of samples.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the scaled data.\n\n    Requirements:\n    - itertools\n    - numpy\n    - sklearn.preprocessing\n    - pandas\n\n    Example:\n    >>> generate_and_scale_data(FEATURES, N)\n    \"\"\"\n    data = np.random.rand(n, len(features))\n    scaler = StandardScaler()\n    scaled_data = scaler.fit_transform(data)\n    df = pd.DataFrame(scaled_data, columns=features)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nitertools, random, numpy, scipy.optimize\n```python\nimport itertools\nimport random\nimport numpy as np\nfrom scipy.optimize import curve_fit\n\n# Constants\nX = np.linspace(0, 10, 100)\nNOISE = 0.1\n\ndef generate_and_fit_data(x, noise):\n    \"\"\"\n    Generate data for a sinusoidal function with added noise, then fit a sinusoidal\n    function to the data.\n\n    Parameters:\n    x (numpy.ndarray): The x values.\n    noise (float): The standard deviation of the noise.\n\n    Returns:\n    tuple: The optimal values for the parameters of the sinusoidal function.\n\n    Requirements:\n    - itertools\n    - random\n    - numpy\n    - scipy.optimize\n\n    Example:\n    >>> generate_and_fit_data(X, NOISE)\n    \"\"\"\n    y = np.sin(x) + np.random.normal(scale=noise, size=len(x))\n    popt, pcov = curve_fit(lambda t, a, b: a*np.sin(b*t), x, y)\n\n    return popt\n```\n"}
{"seed": "{\"task_id\": 36217, \"prompt\": \"def f_36217(df, reg):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"df['a'].str.extract(reg, expand=True)\", \"test_start\": \"\\nimport pandas as pd\\n\\ndef check(candidate):\", \"test\": [\"\\n    df = pd.DataFrame([['abc def'],['123 567'], ['qqq eee']], columns=['a'])\\n    reg = r'(.{3})$'\\n    assert candidate(df, reg).equals(df['a'].str.extract(reg, expand=True))\\n\"], \"entry_point\": \"f_36217\", \"intent\": \"\\u30c7\\u30fc\\u30bf\\u30d5\\u30ec\\u30fc\\u30e0`df`\\u306e\\u5217`a`\\u3092\\u6b63\\u898f\\u8868\\u73fe`reg'\\u3067\\u62bd\\u51fa\\u3059\\u308b\", \"library\": [\"pandas\"]}", "generation": "Scenario 1:\npandas, datetime, re, numpy, matplotlib\n```python\nimport pandas as pd\nfrom datetime import datetime\nimport re\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nREGEX = r'(?i)\\b[a-z]+\\b'\n\ndef generate_word_frequency_report(df, column_name):\n    \"\"\"\n    Generate a frequency report of words in a specific column of a DataFrame \n    and plot the top 10 most frequent words.\n    \n    Parameters:\n    df (DataFrame): The input pandas DataFrame.\n    column_name (str): The column for word frequency analysis.\n    \n    Returns:\n    Series: A pandas Series with word counts.\n    \n    Requirements:\n    - pandas\n    - datetime\n    - re\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = pd.DataFrame({'comments': ['This is a test', 'Another test', 'Test again']})\n    >>> report = generate_word_frequency_report(df, 'comments')\n    >>> print(report)\n    >>> report.nlargest(10).plot(kind='bar')\n    \"\"\"\n    words = df[column_name].apply(lambda x: re.findall(REGEX, x)).explode().value_counts()\n\n    return words\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, sklearn, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\n\ndef linear_regression(df, x, y):\n    \"\"\"\n    Fit a linear regression model using sklearn and plot the regression line over the data points.\n    \n    Parameters:\n    df (DataFrame): The input pandas DataFrame.\n    x (str): The column name for the independent variable.\n    y (str): The column name for the dependent variable.\n    \n    Returns:\n    LinearRegression: The fitted linear regression model.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.linear_model.LinearRegression\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = pd.DataFrame({'x': np.arange(10), 'y': np.arange(10) + np.random.normal(0, 1, 10)})\n    >>> model = linear_regression(df, 'x', 'y')\n    >>> print(model.coef_, model.intercept_)\n    >>> plt.scatter(df['x'], df['y'])\n    >>> plt.plot(df['x'], model.predict(df[['x']]), color='red')\n    \"\"\"\n    model = LinearRegression().fit(df[[x]], df[y])\n\n    return model\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, re, json, datetime\n```python\nimport pandas as pd\nimport re\nimport json\nfrom datetime import datetime\n\n# Constants\nDATE_FORMAT = \"%Y-%m-%d\"\n\ndef extract_and_convert(df, column_name, reg, date_format):\n    \"\"\"\n    Extract data from a column of a DataFrame based on a regular expression and convert it to a date format.\n    \n    Parameters:\n    df (DataFrame): The input pandas DataFrame.\n    column_name (str): The column for data extraction.\n    reg (str): The regular expression for data extraction.\n    date_format (str): The date format for conversion.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the extracted and converted data.\n    \n    Requirements:\n    - pandas\n    - re\n    - json\n    - datetime\n    \n    Example:\n    >>> df = pd.DataFrame({'text': ['The date is 2021-10-01', '2021-10-02 is another date', 'One more date: 2021-10-03']})\n    >>> reg = r'\\b\\d{4}-\\d{2}-\\d{2}\\b'\n    >>> date_format = \"%Y-%m-%d\"\n    >>> result = extract_and_convert(df, 'text', reg, date_format)\n    >>> print(result)\n    \"\"\"\n    df[column_name] = df[column_name].apply(lambda x: re.findall(reg, x)).apply(lambda x: datetime.strptime(x[0], date_format) if x else np.nan)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, json, requests, os\n```python\nimport pandas as pd\nimport json\nimport requests\nimport os\n\n# Constants\nAPI_KEY = os.getenv('API_KEY')\nURL = 'https://api.example.com/data'\n\ndef fetch_and_store(df, column_name):\n    \"\"\"\n    Fetch data from an API for each value in a DataFrame column and store the results in a new column.\n    \n    Parameters:\n    df (DataFrame): The input pandas DataFrame.\n    column_name (str): The column for fetching data.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the fetched data.\n    \n    Requirements:\n    - pandas\n    - json\n    - requests\n    - os\n    \n    Example:\n    >>> df = pd.DataFrame({'id': ['1', '2', '3']})\n    >>> result = fetch_and_store(df, 'id')\n    >>> print(result)\n    \"\"\"\n    df['result'] = df[column_name].apply(lambda x: requests.get(URL, params={'id': x, 'api_key': API_KEY}).json())\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, seaborn, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef plot_correlation(df):\n    \"\"\"\n    Plot a correlation matrix heatmap for a DataFrame using seaborn.\n    \n    Parameters:\n    df (DataFrame): The input pandas DataFrame.\n    \n    Returns:\n    AxesSubplot: The seaborn heatmap.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - seaborn\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = pd.DataFrame({'x': np.arange(10), 'y': np.arange(10) + np.random.normal(0, 1, 10), 'z': np.random.normal(0, 1, 10)})\n    >>> heatmap = plot_correlation(df)\n    >>> plt.show()\n    \"\"\"\n    correlation = df.corr()\n    heatmap = sns.heatmap(correlation, annot=True, cmap='coolwarm')\n\n    return heatmap\n```\n"}
{"seed": "{\"task_id\": 27871, \"prompt\": \"def f_27871(factories, shops, costs):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"{f+s : cost for ((f,s), cost) in zip(product(factories,shops), costs)}\", \"test_start\": \"\\nfrom itertools import product\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(['A', 'B'], ['1', '2'], [8, 10, 12, 16]) == {'A1': 8, 'A2': 10, 'B1': 12, 'B2': 16}\\n\"], \"entry_point\": \"f_27871\", \"intent\": \"2\\u3064\\u306e\\u30ea\\u30b9\\u30c8`factories'\\u3068`shops`\\u306e\\u8981\\u7d20\\u306e\\u7d44\\u307f\\u5408\\u308f\\u305b\\u3092\\u30ad\\u30fc\\u3068\\u3057\\u3001\\u30bf\\u30d7\\u30eb`costs`\\u5404\\u8981\\u7d20\\u3092\\u8981\\u7d20\\u7b49\\u3059\\u308b\\u8f9e\\u66f8\\u578b\\u30aa\\u30d6\\u30b8\\u30a7\\u30af\\u30c8\\u3092\\u4f5c\\u308b\", \"library\": [\"itertools\"]}", "generation": "Scenario 1:\npandas, itertools, matplotlib, seaborn, numpy\n```python\nimport pandas as pd\nfrom itertools import product\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Constants\nFACTORIES = ['Factory A', 'Factory B', 'Factory C', 'Factory D', 'Factory E']\nSHOPS = ['Shop 1', 'Shop 2', 'Shop 3', 'Shop 4', 'Shop 5']\n\ndef generate_cost_analysis(factories=FACTORIES, shops=SHOPS):\n    \"\"\"\n    Generate a cost analysis report of product manufacturing from different factories \n    and selling to different shops. The costs are randomly generated and the report \n    is visualized using seaborn heatmap.\n\n    Parameters:\n    factories (list): List of factories.\n    shops (list): List of shops.\n\n    Returns:\n    DataFrame: A pandas DataFrame with cost analysis for the factories and shops.\n    \n    Requirements:\n    - pandas\n    - itertools.product\n    - matplotlib.pyplot\n    - seaborn\n    - numpy\n\n    Example:\n    >>> report = generate_cost_analysis()\n    >>> sns.heatmap(report, annot=True, cmap='coolwarm')\n    \"\"\"\n    costs = np.random.randint(50, 200, len(factories)*len(shops))\n    costs_dict = {f+s: cost for ((f, s), cost) in zip(product(factories, shops), costs)}\n\n    cost_data = []\n    for factory in factories:\n        factory_costs = []\n        for shop in shops:\n            factory_costs.append(costs_dict[factory+shop])\n        cost_data.append(factory_costs)\n\n    cost_df = pd.DataFrame(cost_data, columns=shops, index=factories)\n\n    return cost_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nitertools, collections, random, matplotlib\n```python\nfrom itertools import product\nfrom collections import defaultdict\nimport random\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLORS = ['Red', 'Blue', 'Green', 'Yellow', 'Purple']\nSHAPES = ['Circle', 'Square', 'Triangle', 'Rectangle', 'Pentagon']\n\ndef generate_color_shape_distribution(colors=COLORS, shapes=SHAPES):\n    \"\"\"\n    Generate a distribution of color and shape combinations. \n    The distribution is calculated based on randomly generated \n    combinations and the result is visualized using a bar graph.\n\n    Parameters:\n    colors (list): List of colors.\n    shapes (list): List of shapes.\n\n    Returns:\n    dict: A dictionary with color and shape combinations as keys and their counts as values.\n\n    Requirements:\n    - itertools.product\n    - collections.defaultdict\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> distribution = generate_color_shape_distribution()\n    >>> plt.bar(distribution.keys(), distribution.values())\n    >>> plt.xticks(rotation=90)\n    \"\"\"\n    combinations = list(product(colors, shapes))\n    distribution = defaultdict(int)\n\n    for _ in range(1000):\n        combo = random.choice(combinations)\n        distribution[combo] += 1\n\n    return dict(distribution)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nitertools, pandas, numpy, matplotlib\n```python\nfrom itertools import product\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nPRODUCTS = ['Product A', 'Product B', 'Product C', 'Product D', 'Product E']\nSTORES = ['Store 1', 'Store 2', 'Store 3', 'Store 4', 'Store 5']\n\ndef generate_sales_report(products=PRODUCTS, stores=STORES):\n    \"\"\"\n    Generate a sales report for different products sold in different stores. \n    The sales are randomly generated and the report is visualized using a bar graph.\n\n    Parameters:\n    products (list): List of products.\n    stores (list): List of stores.\n\n    Returns:\n    DataFrame: A pandas DataFrame with sales data for the products and stores.\n\n    Requirements:\n    - itertools.product\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> report = generate_sales_report()\n    >>> report.plot(kind='bar', stacked=True)\n    \"\"\"\n    sales = np.random.randint(10, 100, len(products)*len(stores))\n    sales_dict = {p+s: sale for ((p, s), sale) in zip(product(products, stores), sales)}\n\n    sales_data = []\n    for product in products:\n        product_sales = []\n        for store in stores:\n            product_sales.append(sales_dict[product+store])\n        sales_data.append(product_sales)\n\n    sales_df = pd.DataFrame(sales_data, columns=stores, index=products)\n\n    return sales_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nitertools, os, shutil, random\n```python\nfrom itertools import product\nimport os\nimport shutil\nimport random\n\n# Constants\nDIRECTORIES = ['Directory A', 'Directory B', 'Directory C', 'Directory D', 'Directory E']\nFILES = ['File 1', 'File 2', 'File 3', 'File 4', 'File 5']\n\ndef generate_directory_file_map(dirs=DIRECTORIES, files=FILES):\n    \"\"\"\n    Generate a map of directory and file combinations. The combinations are \n    calculated based on randomly generated combinations and then the files\n    are created in the respective directories.\n\n    Parameters:\n    dirs (list): List of directories.\n    files (list): List of files.\n\n    Returns:\n    dict: A dictionary with directory and file combinations.\n\n    Requirements:\n    - itertools.product\n    - os\n    - shutil\n    - random\n\n    Example:\n    >>> directory_file_map = generate_directory_file_map()\n    \"\"\"\n    combinations = list(product(dirs, files))\n    dir_file_map = defaultdict(list)\n\n    for _ in range(len(dirs)*len(files)):\n        combo = random.choice(combinations)\n        dir_file_map[combo[0]].append(combo[1])\n        os.makedirs(combo[0], exist_ok=True)\n        with open(os.path.join(combo[0], combo[1]), 'w') as f:\n            f.write('')\n\n    return dict(dir_file_map)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nitertools, collections, string, random\n```python\nfrom itertools import product\nfrom collections import defaultdict\nimport string\nimport random\n\n# Constants\nLETTERS = list(string.ascii_lowercase)\nDIGITS = list(string.digits)\n\ndef generate_letter_digit_combinations(letters=LETTERS, digits=DIGITS):\n    \"\"\"\n    Generate a distribution of letter and digit combinations. \n    The distribution is calculated based on randomly generated \n    combinations and the result is returned as a dictionary.\n\n    Parameters:\n    letters (list): List of letters.\n    digits (list): List of digits.\n\n    Returns:\n    dict: A dictionary with letter and digit combinations as keys and their counts as values.\n\n    Requirements:\n    - itertools.product\n    - collections.defaultdict\n    - string\n    - random\n\n    Example:\n    >>> distribution = generate_letter_digit_combinations()\n    \"\"\"\n    combinations = list(product(letters, digits))\n    distribution = defaultdict(int)\n\n    for _ in range(1000):\n        combo = random.choice(combinations)\n        distribution[combo] += 1\n\n    return dict(distribution)\n```\n"}
{"seed": "{\"task_id\": 27871, \"prompt\": \"def f_27871(factories, shops, cost):\\n\\t\", \"suffix\": \"\\n\\treturn d\", \"canonical_solution\": \"root = [''.join((x, y)) for x, y in itertools.product(factories, shops)]\\n\\td = dict(zip(root, cost))\", \"test_start\": \"\\nimport itertools\\n\\ndef check(candidate):\", \"test\": [\"\\n    factories = ['A', 'B', 'C', 'D']\\n    shops = ['1', '2', '3', '4', '5']\\n    costs = ( 8, 10, 12, 16, 20,\\n             12,  8,  6, 10, 16,\\n             18,  7,  4,  3,  4,\\n             12, 10, 12, 16, 20 )    \\n    res_dict = candidate(factories, shops, costs)\\n    assert list(res_dict.items()) == [\\n        ('A1', 8), ('A2', 10), ('A3', 12), ('A4', 16), ('A5', 20), \\n        ('B1', 12), ('B2', 8), ('B3', 6), ('B4', 10), ('B5', 16), \\n        ('C1', 18), ('C2', 7), ('C3', 4), ('C4', 3), ('C5', 4), \\n        ('D1', 12), ('D2', 10), ('D3', 12), ('D4', 16), ('D5', 20), \\n    ] \\n\"], \"entry_point\": \"f_27871\", \"intent\": \"2\\u3064\\u306e\\u30ea\\u30b9\\u30c8`factories'\\u3068`shops`\\u306e\\u8981\\u7d20\\u306e\\u7d44\\u307f\\u5408\\u308f\\u305b\\u3092\\u30ad\\u30fc\\u3068\\u3057\\u3001\\u30bf\\u30d7\\u30eb`costs`\\u5404\\u8981\\u7d20\\u3092\\u8981\\u7d20\\u7b49\\u3059\\u308b\\u8f9e\\u66f8\\u578b\\u30aa\\u30d6\\u30b8\\u30a7\\u30af\\u30c8\\u3092\\u4f5c\\u308b\", \"library\": [\"itertools\"]}", "generation": "Scenario 1: \npandas, itertools, numpy, matplotlib\n```python\nimport pandas as pd\nimport itertools\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCITIES = ['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']\nPRODUCTS = ['Apples', 'Bananas', 'Oranges', 'Pears', 'Grapes']\n\ndef create_price_list(cities, products, prices):\n    \"\"\"\n    Create a Pandas DataFrame price list from the given lists of cities and products, \n    and a list of prices generated from itertools.product of cities and products.\n    \n    Parameters:\n    - cities (list): The list of cities.\n    - products (list): The list of products.\n    - prices (list): The list of prices.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with price list.\n    \n    Requirements:\n    - pandas\n    - itertools\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> cities = ['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']\n    >>> products = ['Apples', 'Bananas', 'Oranges', 'Pears', 'Grapes']\n    >>> prices = np.random.randint(5, 20, size=len(cities)*len(products))\n    >>> price_list = create_price_list(cities, products, prices)\n    >>> print(price_list)\n    >>> price_list.plot.bar(x='City_Product', y='Price', rot=0)\n    \"\"\"\n    city_product = ['_'.join((city, product)) for city, product in itertools.product(cities, products)]\n    price_list = pd.DataFrame({'City_Product': city_product, 'Price': prices})\n    \n    return price_list\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nitertools, statistics, matplotlib, random\n```python\nimport itertools\nimport statistics\nimport matplotlib.pyplot as plt\nimport random\n\n# Constants\nSTUDENTS = ['Alice', 'Bob', 'Charlie', 'David', 'Eve']\nSUBJECTS = ['Math', 'English', 'Science', 'History', 'Geography']\n\ndef generate_and_plot_student_scores(students, subjects):\n    \"\"\"\n    Generate a dictionary of student scores for each subject using itertools.product, \n    calculate the mean score for each student, and plot the scores.\n    \n    Parameters:\n    - students (list): The list of students.\n    - subjects (list): The list of subjects.\n    \n    Returns:\n    dict: A dictionary with student scores for each subject.\n    \n    Requirements:\n    - itertools\n    - statistics\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> students = ['Alice', 'Bob', 'Charlie', 'David', 'Eve']\n    >>> subjects = ['Math', 'English', 'Science', 'History', 'Geography']\n    >>> scores = generate_and_plot_student_scores(students, subjects)\n    >>> print(scores)\n    >>> means = {student: statistics.mean(scores[student].values()) for student in students}\n    >>> plt.bar(means.keys(), means.values())\n    >>> plt.show()\n    \"\"\"\n    scores = {student: {subject: random.randint(50, 100) for subject in subjects} for student in students}\n    return scores\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nitertools, numpy, pandas, seaborn\n```python\nimport itertools\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n# Constants\nANIMALS = ['Dog', 'Cat', 'Rabbit', 'Hamster', 'Bird']\nFOODS = ['Beef', 'Chicken', 'Carrots', 'Seeds', 'Berries']\n\ndef create_and_plot_food_preferences(animals, foods):\n    \"\"\"\n    Create a pandas DataFrame of food preferences for different animals using itertools.product \n    and numpy.random, and plot the data using seaborn.\n    \n    Parameters:\n    - animals (list): The list of animals.\n    - foods (list): The list of foods.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with food preferences.\n    \n    Requirements:\n    - itertools\n    - numpy\n    - pandas\n    - seaborn\n\n    Example:\n    >>> animals = ['Dog', 'Cat', 'Rabbit', 'Hamster', 'Bird']\n    >>> foods = ['Beef', 'Chicken', 'Carrots', 'Seeds', 'Berries']\n    >>> preferences = create_and_plot_food_preferences(animals, foods)\n    >>> print(preferences)\n    >>> sns.heatmap(preferences)\n    \"\"\"\n    animal_food = ['_'.join((animal, food)) for animal, food in itertools.product(animals, foods)]\n    preferences_data = np.random.randint(1, 5, size=len(animal_food))\n    preferences = pd.Series(preferences_data, index=animal_food, name='Preference')\n    \n    return preferences\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nitertools, math, matplotlib, numpy\n```python\nimport itertools\nimport math\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nANGLES = list(range(0, 360, 45))\nFUNCTIONS = [math.sin, math.cos, math.tan]\n\ndef plot_trigonometric_functions(angles, functions):\n    \"\"\"\n    Plot trigonometric functions (sin, cos, tan) for a range of angles using itertools.product.\n    \n    Parameters:\n    - angles (list): The list of angles in degrees.\n    - functions (list): The list of trigonometric functions.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - itertools\n    - math\n    - matplotlib.pyplot\n    - numpy\n\n    Example:\n    >>> angles = list(range(0, 360, 45))\n    >>> functions = [math.sin, math.cos, math.tan]\n    >>> plot_trigonometric_functions(angles, functions)\n    \"\"\"\n    x_values = np.array(angles) * np.pi / 180.0  # Convert to radians\n    \n    for function in functions:\n        y_values = [function(x) for x in x_values]\n        plt.plot(angles, y_values, label=function.__name__)\n    \n    plt.legend()\n    plt.grid(True)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nitertools, pandas, numpy, seaborn\n```python\nimport itertools\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n\n# Constants\nCOUNTRIES = ['USA', 'China', 'Japan', 'Germany', 'UK']\nSECTORS = ['Tech', 'Healthcare', 'Finance', 'Energy', 'Consumer Goods']\n\ndef create_and_plot_gdp_breakdown(countries, sectors):\n    \"\"\"\n    Create a pandas DataFrame of GDP breakdown by sector for different countries using itertools.product \n    and numpy.random, and plot the data using seaborn.\n    \n    Parameters:\n    - countries (list): The list of countries.\n    - sectors (list): The list of sectors.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with GDP breakdown.\n    \n    Requirements:\n    - itertools\n    - pandas\n    - numpy\n    - seaborn\n\n    Example:\n    >>> countries = ['USA', 'China', 'Japan', 'Germany', 'UK']\n    >>> sectors = ['Tech', 'Healthcare', 'Finance', 'Energy', 'Consumer Goods']\n    >>> gdp_breakdown = create_and_plot_gdp_breakdown(countries, sectors)\n    >>> print(gdp_breakdown)\n    >>> sns.heatmap(gdp_breakdown)\n    \"\"\"\n    country_sector = ['_'.join((country, sector)) for country, sector in itertools.product(countries, sectors)]\n    gdp_data = np.random.randint(1, 10, size=len(country_sector))\n    gdp_breakdown = pd.Series(gdp_data, index=country_sector, name='GDP')\n    \n    return gdp_breakdown\n```\n"}
{"seed": "{\"task_id\": 40676, \"prompt\": \"def f_40676(soup):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"soup.find_all(attrs={\\\"data-locate\\\": \\\"address\\\"})\", \"test_start\": \"\\nfrom bs4 import BeautifulSoup\\n\\ndef check(candidate):\", \"test\": [\"\\n    soup = BeautifulSoup('<div data-locate=\\\"address\\\">foo!</div>')\\n    res = candidate(soup)\\n    assert len(res) == 1\\n    assert res[0].attrs == {'data-locate': 'address'} \\n    assert res[0].text == \\\"foo!\\\"\\n\"], \"entry_point\": \"f_40676\", \"intent\": \"\\u30ad\\u30fc\\u30ef\\u30fc\\u30c9\\u5f15\\u6570\\u3068\\u3057\\u3066\\u7528\\u3044\\u308b\\u4e8b\\u304c\\u3067\\u304d\\u306a\\u3044HTML5\\u306edata-\\u5c5e\\u6027\\u3001\\u4f8b\\u3048\\u3070`data-locel`\\u304c`address`\\u306b\\u4e00\\u81f4\\u3059\\u308b\\u3082\\u306e\\u3092\\u30aa\\u30d6\\u30b8\\u30a7\\u30af\\u30c8`soup`\\u304b\\u3089\\u691c\\u7d22\\u3059\\u308b\", \"library\": [\"bs4\"]}", "generation": "Scenario 1: \nbs4, requests, re, os\n```python\nimport os\nimport re\nimport requests\nfrom bs4 import BeautifulSoup\n\n# Constants\nURL = 'https://example.com'\nDATA_LOCATE = 'address'\n\ndef download_images(soup):\n    \"\"\"\n    Download all images from a BeautifulSoup object where the \"data-locate\" attribute equals \"address\".\n \n    Parameters:\n    soup (BeautifulSoup): A BeautifulSoup instance.\n \n    Returns:\n    list: List of file paths for the downloaded images.\n \n    Requirements:\n    - BeautifulSoup (bs4)\n    - requests\n    - re\n    - os\n \n    Example:\n    >>> soup = BeautifulSoup('<div data-locate=\"address\"><img src=\"https://example.com/image.jpg\" /></div>')\n    >>> download_images(soup)\n    \"\"\"\n    images = soup.find_all(attrs={\"data-locate\": DATA_LOCATE})\n    img_srcs = [img['src'] for img in images if 'src' in img.attrs]\n    filepaths = []\n\n    for src in img_srcs:\n        response = requests.get(src)\n        filename = re.search(r'/([\\w_-]+[.](jpg|gif|png))$', src)\n        if filename is not None:\n            filename = filename.group(1)\n            with open(filename, 'wb') as out_file:\n                out_file.write(response.content)\n            filepaths.append(os.path.abspath(filename))\n\n    return filepaths\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nbs4, urllib, ssl, hashlib\n```python\nfrom bs4 import BeautifulSoup\nimport urllib.request\nimport ssl\nimport hashlib\n\n# Constants\nURL = 'https://example.com'\nDATA_LOCATE = 'address'\n\ndef hash_images(soup):\n    \"\"\"\n    Create a sha256 hash for all images from a BeautifulSoup object where the \"data-locate\" attribute equals \"address\".\n \n    Parameters:\n    soup (BeautifulSoup): A BeautifulSoup instance.\n \n    Returns:\n    list: List of sha256 hashes for the images.\n \n    Requirements:\n    - BeautifulSoup (bs4)\n    - urllib.request\n    - ssl\n    - hashlib\n \n    Example:\n    >>> soup = BeautifulSoup('<div data-locate=\"address\"><img src=\"https://example.com/image.jpg\" /></div>')\n    >>> hash_images(soup)\n    \"\"\"\n    images = soup.find_all(attrs={\"data-locate\": DATA_LOCATE})\n    img_srcs = [img['src'] for img in images if 'src' in img.attrs]\n    hashes = []\n\n    for src in img_srcs:\n        gcontext = ssl.SSLContext()  # to bypass SSL certificate verification\n        response = urllib.request.urlopen(src, context=gcontext)\n        img_data = response.read()\n        hash_obj = hashlib.sha256(img_data)\n        img_hash = hash_obj.hexdigest()\n        hashes.append(img_hash)\n    \n    return hashes\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nbs4, requests, json\n```python\nimport requests\nfrom bs4 import BeautifulSoup\nimport json\n\n# Constants\nURL = 'https://example.com'\nDATA_LOCATE = 'address'\n\ndef get_json_data(soup):\n    \"\"\"\n    Extract JSON data from script tags within elements where the \"data-locate\" attribute equals \"address\" in a BeautifulSoup object.\n \n    Parameters:\n    soup (BeautifulSoup): A BeautifulSoup instance.\n \n    Returns:\n    list: List of JSON data.\n \n    Requirements:\n    - BeautifulSoup (bs4)\n    - requests\n    - json\n \n    Example:\n    >>> soup = BeautifulSoup('<div data-locate=\"address\"><script type=\"application/ld+json\">{\"name\": \"John\"}</script></div>')\n    >>> get_json_data(soup)\n    \"\"\"\n    elements = soup.find_all(attrs={\"data-locate\": DATA_LOCATE})\n    data = []\n\n    for element in elements:\n        scripts = element.find_all('script', type=\"application/ld+json\")\n        for script in scripts:\n            json_data = json.loads(script.string)\n            data.append(json_data)\n    \n    return data\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nbs4, csv\n```python\nfrom bs4 import BeautifulSoup\nimport csv\n\n# Constants\nDATA_LOCATE = 'address'\n\ndef extract_text_to_csv(soup):\n    \"\"\"\n    Extract text from elements where \"data-locate\" attribute equals \"address\" in a BeautifulSoup object.\n    Save the text into a CSV file.\n \n    Parameters:\n    soup (BeautifulSoup): A BeautifulSoup instance.\n \n    Returns:\n    str: Filepath of the CSV file.\n \n    Requirements:\n    - BeautifulSoup (bs4)\n    - csv\n \n    Example:\n    >>> soup = BeautifulSoup('<div data-locate=\"address\">Hello, world!</div>')\n    >>> extract_text_to_csv(soup)\n    \"\"\"\n    elements = soup.find_all(attrs={\"data-locate\": DATA_LOCATE})\n    text_list = [ele.text for ele in elements]\n\n    with open('output.csv', 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow([\"Text\"])\n        for text in text_list:\n            writer.writerow([text])\n    \n    return 'output.csv'\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nbs4, urllib, lxml\n```python\nfrom bs4 import BeautifulSoup\nimport urllib.request\nfrom lxml import html\n\n# Constants\nURL = 'https://example.com'\nDATA_LOCATE = 'address'\n\ndef extract_links(soup):\n    \"\"\"\n    Extract all href links from a BeautifulSoup object where the \"data-locate\" attribute equals \"address\".\n \n    Parameters:\n    soup (BeautifulSoup): A BeautifulSoup instance.\n \n    Returns:\n    list: List of href links.\n \n    Requirements:\n    - BeautifulSoup (bs4)\n    - urllib.request\n    - lxml\n \n    Example:\n    >>> soup = BeautifulSoup('<div data-locate=\"address\"><a href=\"https://example.com/home\">Home</a></div>')\n    >>> extract_links(soup)\n    \"\"\"\n    elements = soup.find_all(attrs={\"data-locate\": DATA_LOCATE})\n    links = [ele.get('href') for ele in elements if ele.get('href') is not None]\n\n    return links\n```"}
{"seed": "{\"task_id\": 42256, \"prompt\": \"def f_42256(li):\\n\\t\", \"suffix\": \"\\n\\treturn s\", \"canonical_solution\": \"s = ''.join(i[0] for i in li)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(['sda', 'dahkdja', 'uehjkw', 'ebhjda']) == 'sdue'\\n\", \"\\n    assert candidate(['happy', 'apple', 'pear', 'pie', 'yummy']) == 'happy'\\n\", \"\\n    assert candidate(['a', 'b', 'c', 'd']) == 'abcd'\\n\", \"\\n    assert candidate([str(i) for i in range(10)]) == '0123456789'\\n\"], \"entry_point\": \"f_42256\", \"intent\": \"\\u6587\\u5b57\\u5217\\u3092\\u8981\\u7d20\\u306b\\u6301\\u3064\\u30ea\\u30b9\\u30c8`li`\\u306e\\u982d\\u6587\\u5b57\\u3092\\u7d50\\u5408\\u3057\\u305f\\u6587\\u5b57\\u5217`s`\\u3092\\u5f97\\u308b\", \"library\": []}", "generation": "Scenario 1:\npandas, numpy, itertools, random\n```python\nimport pandas as pd\nimport numpy as np\nfrom itertools import combinations\nimport random\n\n# Constants\nLETTERS = list('abcdefghijklmnopqrstuvwxyz')\n\ndef generate_random_combinations(n, k):\n    \"\"\"\n    Generate a dataframe of random combinations of size k from a list of n letters.\n    Each row in the dataframe represents a unique combination of letters, and the \n    number of rows is equal to the number of combinations.\n    \n    Parameters:\n    n (int): The number of letters to consider for combinations.\n    k (int): The size of each combination.\n\n    Returns:\n    DataFrame: A pandas DataFrame with combinations of letters.\n\n    Requirements:\n    - pandas\n    - numpy\n    - itertools\n    - random\n\n    Example:\n    >>> df = generate_random_combinations(5, 3)\n    >>> print(df)\n    \"\"\"\n    selected_letters = random.sample(LETTERS, n)\n    combos = list(combinations(selected_letters, k))\n    df = pd.DataFrame(combos, columns=[f'Letter_{i+1}' for i in range(k)])\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncollections, itertools, functools, operator\n```python\nfrom collections import Counter\nfrom itertools import groupby\nfrom functools import reduce\nimport operator\n\n# Constants\nLETTERS = list('abcdefghijklmnopqrstuvwxyz')\n\ndef count_letters_in_combinations(combinations):\n    \"\"\"\n    Count the occurrence of each letter in a list of combinations and return the \n    result as a dictionary.\n    \n    Parameters:\n    combinations (list): A list of combinations, where each combination is a tuple of letters.\n\n    Returns:\n    dict: A dictionary with letters as keys and counts as values.\n\n    Requirements:\n    - collections\n    - itertools\n    - functools\n    - operator\n\n    Example:\n    >>> combinations = [('a', 'b', 'c'), ('a', 'b', 'd'), ('a', 'c', 'd'), ('b', 'c', 'd')]\n    >>> print(count_letters_in_combinations(combinations))\n    \"\"\"\n    flattened = reduce(operator.concat, combinations)\n    counts = Counter(flattened)\n    return dict(counts)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nitertools, random, string\n```python\nfrom itertools import permutations\nimport random\nimport string\n\n# Constants\nLETTERS = string.ascii_lowercase\n\ndef generate_random_permutations(n, k):\n    \"\"\"\n    Generate a list of random permutations of size k from a list of n letters.\n    Each element in the list represents a unique permutation of letters.\n\n    Parameters:\n    n (int): The number of letters to consider for permutations.\n    k (int): The size of each permutation.\n\n    Returns:\n    list: A list with permutations of letters.\n\n    Requirements:\n    - itertools\n    - random\n    - string\n\n    Example:\n    >>> perms = generate_random_permutations(5, 3)\n    >>> print(perms)\n    \"\"\"\n    selected_letters = random.sample(LETTERS, n)\n    perms = list(permutations(selected_letters, k))\n    return [''.join(perm) for perm in perms]\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, scipy.stats, matplotlib.pyplot\n```python\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\ndef plot_normal_distribution(mu, sigma):\n    \"\"\"\n    Generate and plot a normal distribution with a given mean and standard deviation.\n    \n    Parameters:\n    mu (float): The mean of the normal distribution.\n    sigma (float): The standard deviation of the normal distribution.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_normal_distribution(0, 1)\n    \"\"\"\n    x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n    y = norm.pdf(x, mu, sigma)\n    plt.plot(x, y)\n    plt.title(f'Normal Distribution: mu={mu}, sigma={sigma}')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\ncollections, itertools, string\n```python\nfrom collections import Counter\nfrom itertools import combinations\nimport string\n\n# Constants\nLETTERS = string.ascii_lowercase\n\ndef count_letter_combinations(n, k):\n    \"\"\"\n    Count the occurrence of each letter in all combinations of size k from a list \n    of n letters and return the result as a dictionary.\n    \n    Parameters:\n    n (int): The number of letters to consider for combinations.\n    k (int): The size of each combination.\n\n    Returns:\n    dict: A dictionary with letters as keys and counts as values.\n\n    Requirements:\n    - collections\n    - itertools\n    - string\n\n    Example:\n    >>> counts = count_letter_combinations(5, 3)\n    >>> print(counts)\n    \"\"\"\n    selected_letters = LETTERS[:n]\n    combos = list(combinations(selected_letters, k))\n    flattened = [letter for combo in combos for letter in combo]\n    counts = Counter(flattened)\n    return dict(counts)\n```"}
{"seed": "{\"task_id\": 42256, \"prompt\": \"def f_42256(li):\\n\\t\", \"suffix\": \"\\n\\treturn s\", \"canonical_solution\": \"\\n\\ts = ''\\n\\tfor line in li:\\n\\t\\ts += line[0]\\n\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(['sda', 'dahkdja', 'uehjkw', 'ebhjda']) == 'sdue'\\n\", \"\\n    assert candidate(['happy', 'apple', 'pear', 'pie', 'yummy']) == 'happy'\\n\", \"\\n    assert candidate(['a', 'b', 'c', 'd']) == 'abcd'\\n\", \"\\n    assert candidate([str(i) for i in range(10)]) == '0123456789'\\n\"], \"entry_point\": \"f_42256\", \"intent\": \"\\u6587\\u5b57\\u5217\\u3092\\u8981\\u7d20\\u306b\\u6301\\u3064\\u30ea\\u30b9\\u30c8`li`\\u306e\\u982d\\u6587\\u5b57\\u3092\\u7d50\\u5408\\u3057\\u305f\\u6587\\u5b57\\u5217`s`\\u3092\\u5f97\\u308b\", \"library\": []}", "generation": "Scenario 1: \nnumpy, pandas, matplotlib, random\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import choice\n\n# Constants\nSTUDENTS = ['Adam', 'Brenda', 'Charlie', 'Diana', 'Ethan']\nSUBJECTS = ['Math', 'English', 'Science', 'History', 'Art']\n\ndef generate_student_grades():\n    \"\"\"\n    Generate a report of student grades for a list of subjects.\n    \n    The grades are randomly generated between 50 and 100 (inclusive).\n    \n    Returns:\n    DataFrame: A pandas DataFrame with student grades for the subjects.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - random\n    \n    Example:\n    >>> grades = generate_student_grades()\n    >>> print(grades)\n    >>> grades.plot(kind='bar')\n    \"\"\"\n    grades_data = []\n\n    for student in STUDENTS:\n        student_grades = [choice(range(50, 101)) for _ in SUBJECTS]\n        grades_data.append(student_grades)\n\n    grades_df = pd.DataFrame(grades_data, columns=SUBJECTS, index=STUDENTS)\n\n    return grades_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \ndatetime, pytz, random\n```python\nfrom datetime import datetime, timedelta\nimport pytz\nfrom random import randint\n\n# Constants\nCITIES = ['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']\n\n# Time zones for the cities\nTIMEZONES = {\n    'New York': 'America/New_York',\n    'London': 'Europe/London',\n    'Beijing': 'Asia/Shanghai',\n    'Tokyo': 'Asia/Tokyo',\n    'Sydney': 'Australia/Sydney'\n}\n\ndef random_future_time_in_city(city):\n    \"\"\"\n    Generate a future datetime in a given city's timezone. The future time is \n    randomly generated between 1 and 24 hours from the current time.\n    \n    Parameters:\n    city (str): The city for which to generate the future time. Must be one of \n    the cities in the CITIES list.\n    \n    Returns:\n    datetime: The future datetime in the city's timezone.\n    \n    Requirements:\n    - datetime\n    - pytz\n    - random\n    \n    Example:\n    >>> future_time = random_future_time_in_city('London')\n    >>> print(future_time)\n    \"\"\"\n    city_tz = pytz.timezone(TIMEZONES[city])\n    current_time = datetime.now().astimezone(city_tz)\n    future_time = current_time + timedelta(hours=randint(1, 24))\n\n    return future_time\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nrandom, string, hashlib\n```python\nfrom random import choice\nimport string\nimport hashlib\n\n# Constants\nLENGTH = 10\nHASH_METHOD = 'sha256'\n\ndef generate_random_hashed_string():\n    \"\"\"\n    Generate a random string of fixed length and its hash.\n    \n    Returns:\n    tuple: A tuple containing the random string and its hash.\n    \n    Requirements:\n    - random\n    - string\n    - hashlib\n    \n    Example:\n    >>> random_string, hashed_string = generate_random_hashed_string()\n    >>> print(random_string, hashed_string)\n    \"\"\"\n    random_string = ''.join(choice(string.ascii_letters + string.digits) for _ in range(LENGTH))\n    hashed_string = hashlib.new(HASH_METHOD, random_string.encode()).hexdigest()\n\n    return random_string, hashed_string\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, scipy.sparse, random\n```python\nimport numpy as np\nfrom scipy.sparse import csr_matrix\nfrom random import randint\n\n# Constants\nSIZE = 50\nDENSITY = 0.1\n\ndef generate_sparse_matrix():\n    \"\"\"\n    Generate a random sparse matrix using the Compressed Sparse Row (CSR) format.\n    \n    Returns:\n    csr_matrix: The generated sparse matrix.\n    \n    Requirements:\n    - numpy\n    - scipy.sparse\n    - random\n    \n    Example:\n    >>> sparse_mat = generate_sparse_matrix()\n    >>> print(sparse_mat)\n    \"\"\"\n    dense_mat = np.zeros((SIZE, SIZE))\n\n    for _ in range(int(SIZE * SIZE * DENSITY)):\n        i = randint(0, SIZE-1)\n        j = randint(0, SIZE-1)\n        dense_mat[i, j] = randint(1, 10)\n\n    sparse_mat = csr_matrix(dense_mat)\n\n    return sparse_mat\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \ndatetime, random, pandas\n```python\nfrom datetime import datetime, timedelta\nfrom random import randint\nimport pandas as pd\n\n# Constants\nDAYS = 30\n\ndef generate_random_dates():\n    \"\"\"\n    Generate a pandas Series of random dates for the past month.\n    \n    Returns:\n    Series: A pandas Series with the generated dates.\n    \n    Requirements:\n    - datetime\n    - random\n    - pandas\n    \n    Example:\n    >>> dates = generate_random_dates()\n    >>> print(dates)\n    \"\"\"\n    end_date = datetime.now()\n    start_date = end_date - timedelta(days=DAYS)\n    delta = end_date - start_date\n\n    dates = []\n\n    for _ in range(DAYS):\n        random_days = randint(0, delta.days)\n        date = start_date + timedelta(days=random_days)\n        dates.append(date)\n\n    dates_series = pd.Series(dates)\n\n    return dates_series\n```"}
{"seed": "{\"task_id\": 18967, \"prompt\": \"def f_18967(li, i):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"i not in li\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(['sda', 'dahkdja', 'uehjkw'], \\\"sda\\\") == False\\n\", \"\\n    assert candidate(['happy', 'apple', 'pear', 'pie', 'yummy'], \\\"dog\\\") == True\\n\", \"\\n    assert candidate([str(i) for i in range(10)], 10) == True\\n\"], \"entry_point\": \"f_18967\", \"intent\": \"\\u30ea\\u30b9\\u30c8`li`\\u306e\\u4e2d\\u306b\\u8981\\u7d20`i`\\u304c\\u542b\\u307e\\u308c\\u3066\\u3044\\u306a\\u3044\\u6761\\u4ef6\\u5206\\u5c90\\u3092\\u884c\\u3046\", \"library\": []}", "generation": "Scenario 1: \nnumpy, pandas, random, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nfrom random import choice\nimport matplotlib.pyplot as plt\n\n# Constants\nCATEGORIES = ['Fruits', 'Vegetables', 'Dairy', 'Meat', 'Beverages']\nITEMS = {\n    'Fruits': ['Apple', 'Banana', 'Cherry', 'Dates', 'Elderberry'],\n    'Vegetables': ['Asparagus', 'Broccoli', 'Carrot', 'Daikon', 'Eggplant'],\n    'Dairy': ['Milk', 'Cheese', 'Yogurt', 'Butter', 'Cream'],\n    'Meat': ['Beef', 'Chicken', 'Duck', 'Lamb', 'Pork'],\n    'Beverages': ['Water', 'Tea', 'Coffee', 'Juice', 'Soda']\n}\n\ndef generate_shopping_list(num_items):\n    \"\"\"\n    Generate a shopping list of a given number of items, randomly selected from \n    predefined categories.\n    \n    Parameters:\n    num_items (int): The number of items to be included in the shopping list.\n    \n    Returns:\n    Series: A pandas Series with the shopping list items.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> shopping_list = generate_shopping_list(10)\n    >>> print(shopping_list)\n    >>> shopping_list.value_counts().plot(kind='bar')\n    \"\"\"\n    shopping_list = []\n\n    for _ in range(num_items):\n        category = choice(CATEGORIES)\n        item = choice(ITEMS[category])\n        shopping_list.append(item)\n\n    shopping_series = pd.Series(shopping_list)\n\n    return shopping_series\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, datetime, pytz, random\n```python\nimport numpy as np\nfrom datetime import datetime, timedelta\nimport pytz\nfrom random import randint\n\n# Constants\nEVENTS = ['Meeting', 'Workout', 'Lunch', 'Dinner', 'Movie']\nTIMEZONES = ['America/New_York', 'Europe/London', 'Asia/Shanghai', 'Asia/Tokyo', 'Australia/Sydney']\n\ndef generate_schedule(start_date, num_days):\n    \"\"\"\n    Generate a schedule of events for a given number of days, starting from a \n    given date, with random times in different timezones.\n    \n    Parameters:\n    start_date (str): The starting date in 'yyyy-mm-dd' format.\n    num_days (int): The number of days for which the schedule should be generated.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the schedule of events.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - datetime\n    - pytz\n    - random\n    \n    Example:\n    >>> schedule = generate_schedule('2023-06-15', 7)\n    >>> print(schedule)\n    \"\"\"\n    schedule_data = []\n\n    start_date = np.datetime64(start_date)\n    dates = np.arange(start_date, start_date + np.timedelta64(num_days, 'D'))\n\n    for date in dates:\n        event = EVENTS[randint(0, len(EVENTS) - 1)]\n        hour = randint(0, 23)\n        minute = randint(0, 59)\n        timezone = TIMEZONES[randint(0, len(TIMEZONES) - 1)]\n        event_time = datetime(date.astype(object).year, date.astype(object).month, date.astype(object).day, hour, minute, tzinfo=pytz.timezone(timezone))\n        schedule_data.append([event_time, event])\n\n    schedule_df = pd.DataFrame(schedule_data, columns=['Time', 'Event'])\n\n    return schedule_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, random, matplotlib\n```python\nimport numpy as np\nfrom random import choice\nimport matplotlib.pyplot as plt\n\n# Constants\nCOINS = ['Heads', 'Tails']\n\ndef simulate_coin_tosses(num_tosses):\n    \"\"\"\n    Simulate a given number of coin tosses and plot the distribution of results.\n    \n    Parameters:\n    num_tosses (int): The number of coin tosses to simulate.\n    \n    Returns:\n    ndarray: A numpy array with the results of the coin tosses.\n    \n    Requirements:\n    - numpy\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> results = simulate_coin_tosses(100)\n    >>> print(np.count_nonzero(results == 'Heads'))\n    >>> plt.hist(results, bins=2)\n    \"\"\"\n    results = np.array([choice(COINS) for _ in range(num_tosses)])\n\n    return results\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, random, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import choice\nimport matplotlib.pyplot as plt\n\n# Constants\nANIMALS = ['Cat', 'Dog', 'Bird', 'Fish', 'Hamster']\nAGES = range(0, 20)\n\ndef generate_pet_data(num_entries):\n    \"\"\"\n    Generate a dataset of pet data with a given number of entries, including \n    random animal types and ages.\n    \n    Parameters:\n    num_entries (int): The number of entries to be included in the dataset.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the pet data.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> pet_data = generate_pet_data(100)\n    >>> print(pet_data)\n    >>> pet_data['Animal'].value_counts().plot(kind='bar')\n    >>> plt.hist(pet_data['Age'], bins=20)\n    \"\"\"\n    pet_data = []\n\n    for _ in range(num_entries):\n        animal = choice(ANIMALS)\n        age = choice(AGES)\n        pet_data.append([animal, age])\n\n    pet_df = pd.DataFrame(pet_data, columns=['Animal', 'Age'])\n\n    return pet_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, random, matplotlib\n```python\nimport numpy as np\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nDICE_SIDES = 6\n\ndef simulate_dice_rolls(num_rolls):\n    \"\"\"\n    Simulate a given number of dice rolls and plot the distribution of results.\n    \n    Parameters:\n    num_rolls (int): The number of dice rolls to simulate.\n    \n    Returns:\n    ndarray: A numpy array with the results of the dice rolls.\n    \n    Requirements:\n    - numpy\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> results = simulate_dice_rolls(100)\n    >>> print(np.bincount(results))\n    >>> plt.hist(results, bins=6)\n    \"\"\"\n    results = np.array([randint(1, DICE_SIDES) for _ in range(num_rolls)])\n\n    return results\n```"}
{"seed": "{\"task_id\": 37648, \"prompt\": \"def f_37648(req_data):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"json.dumps(req_data).encode('utf-8')\", \"test_start\": \"\\nimport json\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate({'test': 'just a test'}) == b'{\\\"test\\\": \\\"just a test\\\"}'\\n\"], \"entry_point\": \"f_37648\", \"intent\": \"\\u30b5\\u30fc\\u30d0\\u30fc\\u306b\\u9001\\u4fe1\\u3059\\u308b\\u30c7\\u30fc\\u30bf`req_data`\\u3092UTF-8\\u3067\\u7b26\\u53f7\\u5316\\u3059\\u308b\", \"library\": [\"json\"]}", "generation": "Scenario 1: \njson, base64, binascii, hashlib\n```python\nimport json\nimport base64\nimport binascii\nimport hashlib\n\ndef hash_and_encode_req_data(req_data):\n    \"\"\"\n    Hashes the given request data with SHA256, then base64 encodes it and finally \n    converts it into hexadecimal representation.\n    \n    Parameters:\n    req_data (dict): The request data to be sent to the server.\n    \n    Returns:\n    str: The hexadecimal representation of base64 encoded SHA256 hash of request data.\n    \n    Requirements:\n    - json\n    - base64\n    - binascii\n    - hashlib\n    \n    Example:\n    >>> hash_and_encode_req_data({'test': 'just a test'})\n    \"\"\"\n    # Convert request data to json string\n    json_req_data = json.dumps(req_data)\n    # Hash the request data using SHA256\n    sha256_hash = hashlib.sha256(json_req_data.encode('utf-8')).digest()\n    # Base64 encode the hash\n    b64_encoded_hash = base64.b64encode(sha256_hash)\n    # Convert to hexadecimal representation\n    hex_representation = binascii.hexlify(b64_encoded_hash).decode()\n\n    return hex_representation\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \njson, zlib, codecs\n```python\nimport json\nimport zlib\nimport codecs\n\ndef compress_encode_req_data(req_data):\n    \"\"\"\n    Compresses the given request data using zlib, then base64 encodes the compressed data.\n    \n    Parameters:\n    req_data (dict): The request data to be sent to the server.\n    \n    Returns:\n    str: The base64 encoded representation of compressed request data.\n    \n    Requirements:\n    - json\n    - zlib\n    - codecs\n    \n    Example:\n    >>> compress_encode_req_data({'test': 'just a test'})\n    \"\"\"\n    # Convert request data to json string\n    json_req_data = json.dumps(req_data)\n    # Compress the request data using zlib\n    compressed_data = zlib.compress(json_req_data.encode('utf-8'))\n    # Base64 encode the compressed data\n    b64_encoded_data = codecs.encode(compressed_data, 'base64').decode()\n\n    return b64_encoded_data\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \njson, urllib, hmac, hashlib\n```python\nimport json\nimport urllib.parse\nimport hmac\nimport hashlib\n\nSECRET_KEY = 'my_secret_key'\n\ndef hmac_sign_req_data(req_data):\n    \"\"\"\n    HMAC signs the given request data with a secret key using SHA256, then URL encodes the signature.\n    \n    Parameters:\n    req_data (dict): The request data to be sent to the server.\n    \n    Returns:\n    str: The URL encoded HMAC signature.\n    \n    Requirements:\n    - json\n    - urllib.parse\n    - hmac\n    - hashlib\n    \n    Example:\n    >>> hmac_sign_req_data({'test': 'just a test'})\n    \"\"\"\n    # Convert request data to json string\n    json_req_data = json.dumps(req_data)\n    # Create a new hmac object with the secret key and the json string as the message\n    hmac_obj = hmac.new(SECRET_KEY.encode(), json_req_data.encode(), hashlib.sha256)\n    # Get the hmac signature\n    hmac_signature = hmac_obj.digest()\n    # URL encode the hmac signature\n    url_encoded_signature = urllib.parse.quote_plus(hmac_signature)\n\n    return url_encoded_signature\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \njson, hashlib, blake3\n```python\nimport json\nimport hashlib\nimport blake3\n\ndef blake3_hash_req_data(req_data):\n    \"\"\"\n    Hashes the given request data with BLAKE3, then converts it into hexadecimal representation.\n    \n    Parameters:\n    req_data (dict): The request data to be sent to the server.\n    \n    Returns:\n    str: The hexadecimal representation of BLAKE3 hash of request data.\n    \n    Requirements:\n    - json\n    - hashlib\n    - blake3\n    \n    Example:\n    >>> blake3_hash_req_data({'test': 'just a test'})\n    \"\"\"\n    # Convert request data to json string\n    json_req_data = json.dumps(req_data)\n    # Hash the request data using BLAKE3\n    blake3_hash = blake3.blake3(json_req_data.encode('utf-8')).digest()\n    # Convert to hexadecimal representation\n    hex_representation = blake3_hash.hex()\n\n    return hex_representation\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \njson, pytz, datetime\n```python\nimport json\nimport pytz\nfrom datetime import datetime\n\ndef timestamp_and_encode_req_data(req_data):\n    \"\"\"\n    Adds a timestamp to the given request data in UTC and then converts it into a JSON string.\n    \n    Parameters:\n    req_data (dict): The request data to be sent to the server.\n    \n    Returns:\n    str: The JSON string representation of request data with UTC timestamp.\n    \n    Requirements:\n    - json\n    - pytz\n    - datetime\n    \n    Example:\n    >>> timestamp_and_encode_req_data({'test': 'just a test'})\n    \"\"\"\n    # Add a timestamp in UTC to the request data\n    req_data['timestamp'] = datetime.now(pytz.UTC).isoformat()\n    # Convert request data to json string\n    json_req_data = json.dumps(req_data)\n\n    return json_req_data\n```"}
{"seed": "{\"task_id\": 39502, \"prompt\": \"def f_39502(str):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"re.sub('([\\u3042-\\u3093\\u30a2-\\u30f3\\u4e00-\\u9fd0\\u30fc])\\\\s+((?=[\\u3042-\\u3093\\u30a2-\\u30f3\\u4e00-\\u9fd0\\u30fc]))',r'\\\\1\\\\2', str)\", \"test_start\": \"\\nimport re\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('\\u65e5 \\u672c \\u8a9e \\u3067 \\u631f \\u307e \\u308c \\u305f \\u7a7a \\u767d \\u3092 \\u524a \\u9664 \\u3059 \\u308b') == '\\u65e5\\u672c\\u8a9e\\u3067\\u631f\\u307e\\u308c\\u305f\\u7a7a\\u767d\\u3092\\u524a\\u9664\\u3059\\u308b'\\n\"], \"entry_point\": \"f_39502\", \"intent\": \"\\u6587\\u5b57\\u5217`str`\\u5185\\u306e\\u3001\\u65e5\\u672c\\u8a9e\\u3067\\u631f\\u307e\\u308c\\u305f\\u7a7a\\u767d\\u3092\\u524a\\u9664\\u3059\\u308b\", \"library\": [\"re\"]}", "generation": "Scenario 1:\nre, os, string\n```python\nimport re\nimport os\nimport string\n\n# Constants\nPUNCTUATIONS = string.punctuation\n\ndef clean_file_content(file_path):\n    \"\"\"\n    Read a text file, remove Japanese characters and punctuations from the file content, and write the cleaned content back to the file.\n\n    Parameters:\n    file_path (str): The path to the text file.\n\n    Returns:\n    str: The cleaned file content.\n\n    Requirements:\n    - re\n    - os\n    - string\n\n    Example:\n    >>> clean_file_content('test.txt')\n    \"\"\"\n    with open(file_path, 'r') as f:\n        content = f.read()\n\n    cleaned_content = re.sub(r'[\\u3042-\\u3093\\u30a2-\\u30f3\\u4e00-\\u9fd0\\u30fc]', '', content)\n    cleaned_content = re.sub(r'[' + re.escape(PUNCTUATIONS) + r']', '', cleaned_content)\n\n    with open(file_path, 'w') as f:\n        f.write(cleaned_content)\n\n    return cleaned_content\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nre, urllib, bs4\n```python\nimport re\nfrom urllib.request import urlopen\nfrom bs4 import BeautifulSoup\n\ndef extract_text_from_webpage(url):\n    \"\"\"\n    Extract text from a webpage, remove Japanese characters from the text.\n\n    Parameters:\n    url (str): The URL of the webpage.\n\n    Returns:\n    str: The cleaned text.\n\n    Requirements:\n    - re\n    - urllib\n    - bs4\n\n    Example:\n    >>> extract_text_from_webpage('https://www.example.com')\n    \"\"\"\n    response = urlopen(url)\n    html = response.read().decode()\n\n    soup = BeautifulSoup(html, 'html.parser')\n    text = soup.get_text()\n\n    cleaned_text = re.sub(r'[\\u3042-\\u3093\\u30a2-\\u30f3\\u4e00-\\u9fd0\\u30fc]', '', text)\n\n    return cleaned_text\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nre, pandas, nltk\n```python\nimport re\nimport pandas as pd\nfrom nltk.corpus import stopwords\n\n# Load stopwords\nSTOPWORDS = set(stopwords.words('english'))\n\ndef clean_dataframe_column(df, column):\n    \"\"\"\n    Clean a column in a pandas DataFrame, remove Japanese characters and English stopwords from the content.\n\n    Parameters:\n    df (pandas.DataFrame): The DataFrame.\n    column (str): The column name.\n\n    Returns:\n    pandas.DataFrame: The DataFrame with the cleaned column.\n\n    Requirements:\n    - re\n    - pandas\n    - nltk\n\n    Example:\n    >>> df = pd.DataFrame({'text': ['This is a test.', '\u65e5\u672c\u8a9e\u3067\u631f\u307e\u308c\u305f\u7a7a\u767d\u3092\u524a\u9664\u3059\u308b', 'Remove stopwords']})\n    >>> clean_dataframe_column(df, 'text')\n    \"\"\"\n    def clean_text(text):\n        cleaned_text = re.sub(r'[\\u3042-\\u3093\\u30a2-\\u30f3\\u4e00-\\u9fd0\\u30fc]', '', text)\n        cleaned_text = ' '.join(word for word in cleaned_text.split() if word not in STOPWORDS)\n        return cleaned_text\n\n    df[column] = df[column].apply(clean_text)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nre, json, requests\n```python\nimport re\nimport json\nimport requests\n\ndef fetch_and_clean_json_data(url):\n    \"\"\"\n    Fetch JSON data from a URL, remove Japanese characters from the keys and values of the JSON data.\n\n    Parameters:\n    url (str): The URL.\n\n    Returns:\n    dict: The cleaned JSON data.\n\n    Requirements:\n    - re\n    - json\n    - requests\n\n    Example:\n    >>> fetch_and_clean_json_data('https://api.example.com/data')\n    \"\"\"\n    response = requests.get(url)\n    data = response.json()\n\n    def clean_data(data):\n        if isinstance(data, dict):\n            return {re.sub(r'[\\u3042-\\u3093\\u30a2-\\u30f3\\u4e00-\\u9fd0\\u30fc]', '', k): clean_data(v) for k, v in data.items()}\n        elif isinstance(data, list):\n            return [clean_data(x) for x in data]\n        elif isinstance(data, str):\n            return re.sub(r'[\\u3042-\\u3093\\u30a2-\\u30f3\\u4e00-\\u9fd0\\u30fc]', '', data)\n        else:\n            return data\n\n    cleaned_data = clean_data(data)\n\n    return cleaned_data\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nre, docx, io\n```python\nimport re\nfrom docx import Document\nfrom io import BytesIO\n\ndef clean_docx_content(docx_bytes):\n    \"\"\"\n    Clean the content of a docx file in bytes, remove Japanese characters from the content.\n\n    Parameters:\n    docx_bytes (bytes): The docx file in bytes.\n\n    Returns:\n    str: The cleaned content.\n\n    Requirements:\n    - re\n    - docx\n    - io\n\n    Example:\n    >>> with open('test.docx', 'rb') as f:\n    >>>     docx_bytes = f.read()\n    >>> clean_docx_content(docx_bytes)\n    \"\"\"\n    doc = Document(BytesIO(docx_bytes))\n    content = ' '.join([p.text for p in doc.paragraphs])\n\n    cleaned_content = re.sub(r'[\\u3042-\\u3093\\u30a2-\\u30f3\\u4e00-\\u9fd0\\u30fc]', '', content)\n\n    return cleaned_content\n```"}
{"seed": "{\"task_id\": 16805, \"prompt\": \"def f_16805(s, n):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"u'{0}{1}'.format(s, n)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('abd', 35) == 'abd35'\\n\", \"\\n    assert candidate('', 12.34) == '12.34'\\n\", \"\\n    assert candidate([1,2,3], 'string') == '[1, 2, 3]string'\\n\"], \"entry_point\": \"f_16805\", \"intent\": \"\\u6587\\u5b57\\u5217\\u306e\\u5909\\u6570`s`\\u3068`n`\\u3092UTF-8\\u306b\\u5909\\u63db\\u3057\\u3066\\u7d50\\u5408\\u3059\\u308b\", \"library\": []}", "generation": "Scenario 1:\nnumpy, pandas, random, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nimport random\nimport matplotlib.pyplot as plt\n\n# Constants\nLETTERS = list('abcdefghijklmnopqrstuvwxyz')\nDIGITS = list('0123456789')\n\ndef generate_string_and_number_combinations(s, n):\n    \"\"\"\n    Generate a DataFrame of all combinations of a list of strings and numbers. \n    Add random noise to numbers and plot the distribution of the numbers.\n\n    Parameters:\n    s (list): The list of strings.\n    n (list): The list of numbers.\n\n    Returns:\n    DataFrame: A pandas DataFrame with string and number combinations.\n\n    Requirements:\n    - numpy\n    - pandas\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> s = ['abc', 'def', 'ghi']\n    >>> n = [1, 2, 3]\n    >>> df = generate_string_and_number_combinations(s, n)\n    >>> print(df)\n    >>> df['Number'].plot(kind='hist')\n    \"\"\"\n    combinations = []\n\n    for string in s:\n        for number in n:\n            noise = random.uniform(-1, 1)\n            combinations.append([string, number + noise])\n\n    df = pd.DataFrame(combinations, columns=['String', 'Number'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nitertools, numpy, pandas\n```python\nimport itertools\nimport numpy as np\nimport pandas as pd\n\n# Constants\nLETTERS = list('abcdefghijklmnopqrstuvwxyz')\nDIGITS = list('0123456789')\n\ndef generate_cartesian_product(s, n):\n    \"\"\"\n    Generate a DataFrame of the cartesian product of a list of strings and numbers. \n\n    Parameters:\n    s (list): The list of strings.\n    n (list): The list of numbers.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the cartesian product of strings and numbers.\n\n    Requirements:\n    - itertools\n    - numpy\n    - pandas\n\n    Example:\n    >>> s = ['abc', 'def', 'ghi']\n    >>> n = [1, 2, 3]\n    >>> df = generate_cartesian_product(s, n)\n    >>> print(df)\n    \"\"\"\n    cartesian_product = list(itertools.product(s, n))\n    df = pd.DataFrame(cartesian_product, columns=['String', 'Number'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, pandas, string, random\n```python\nimport numpy as np\nimport pandas as pd\nimport string\nimport random\n\n# Constants\nLETTERS = list(string.ascii_lowercase)\nDIGITS = list(string.digits)\n\ndef generate_random_string_and_number_combinations(s, n, length):\n    \"\"\"\n    Generate a DataFrame of random combinations of a list of strings and numbers of a given length. \n\n    Parameters:\n    s (list): The list of strings.\n    n (list): The list of numbers.\n    length (int): The length of the combinations.\n\n    Returns:\n    DataFrame: A pandas DataFrame with random combinations of strings and numbers.\n\n    Requirements:\n    - numpy\n    - pandas\n    - string\n    - random\n\n    Example:\n    >>> s = ['abc', 'def', 'ghi']\n    >>> n = [1, 2, 3]\n    >>> length = 10\n    >>> df = generate_random_string_and_number_combinations(s, n, length)\n    >>> print(df)\n    \"\"\"\n    combinations = []\n\n    for _ in range(length):\n        string = random.choice(s)\n        number = random.choice(n)\n        combinations.append([string, number])\n\n    df = pd.DataFrame(combinations, columns=['String', 'Number'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, pandas, random, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nimport random\nimport matplotlib.pyplot as plt\n\n# Constants\nLETTERS = list('abcdefghijklmnopqrstuvwxyz')\nDIGITS = list('0123456789')\n\ndef generate_string_and_number_combinations_with_distribution(s, n):\n    \"\"\"\n    Generate a DataFrame of all combinations of a list of strings and numbers. \n    Add random noise to numbers and plot the distribution of the numbers.\n\n    Parameters:\n    s (list): The list of strings.\n    n (list): The list of numbers.\n\n    Returns:\n    DataFrame: A pandas DataFrame with string and number combinations.\n\n    Requirements:\n    - numpy\n    - pandas\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> s = ['abc', 'def', 'ghi']\n    >>> n = [1, 2, 3]\n    >>> df = generate_string_and_number_combinations_with_distribution(s, n)\n    >>> print(df)\n    >>> df['Number'].plot(kind='hist')\n    \"\"\"\n    combinations = []\n\n    for string in s:\n        for number in n:\n            noise = random.uniform(-1, 1)\n            combinations.append([string, number + noise])\n\n    df = pd.DataFrame(combinations, columns=['String', 'Number'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nitertools, numpy, pandas, string\n```python\nimport itertools\nimport numpy as np\nimport pandas as pd\nimport string\n\n# Constants\nLETTERS = list(string.ascii_lowercase)\nDIGITS = list(string.digits)\n\ndef generate_string_and_number_permutations(s, n):\n    \"\"\"\n    Generate a DataFrame of the permutations of a list of strings and numbers. \n\n    Parameters:\n    s (list): The list of strings.\n    n (list): The list of numbers.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the permutations of strings and numbers.\n\n    Requirements:\n    - itertools\n    - numpy\n    - pandas\n    - string\n\n    Example:\n    >>> s = ['abc', 'def', 'ghi']\n    >>> n = [1, 2, 3]\n    >>> df = generate_string_and_number_permutations(s, n)\n    >>> print(df)\n    \"\"\"\n    permutations = list(itertools.permutations(s + n))\n    df = pd.DataFrame(permutations)\n\n    return df\n```"}
{"seed": "{\"task_id\": 40978, \"prompt\": \"def f_40978(M, N):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"[x+1 for x in range(M) for y in range(N)]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(2, 3) == [1,1,1,2,2,2]\\n\", \"\\n    assert candidate(2, 1) == [1,2]\\n\"], \"entry_point\": \"f_40978\", \"intent\": \"1\\u304c`N`\\u500b, 2\\u304c`N`\\u500b, ..., `M`\\u304c`N`\\u500b\\u4e26\\u3076\\u30ea\\u30b9\\u30c8\\u3092\\u751f\\u6210\\u3059\\u308b\", \"library\": []}", "generation": "Scenario 1:\nnumpy, itertools, pandas\n```python\nimport numpy as np\nfrom itertools import product\nimport pandas as pd\n\ndef generate_dataframe(M, N):\n    \"\"\"\n    Generate a pandas DataFrame where each row is a tuple from the cartesian product \n    of N arrays of length M, each array being a sequence from 1 to M.\n    \n    Parameters:\n    M, N (int): The parameters to determine the sequence arrays and \n    the number of arrays in the cartesian product.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the tuples from the cartesian product.\n    \n    Requirements:\n    - numpy\n    - itertools\n    - pandas\n    \n    Example:\n    >>> generate_dataframe(2, 3)\n    \"\"\"\n    arrays = [np.arange(1, M+1) for _ in range(N)]\n    cartesian_product = list(product(*arrays))\n    df = pd.DataFrame(cartesian_product)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, matplotlib.pyplot, seaborn\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef plot_heatmap(M, N):\n    \"\"\"\n    Generate a heatmap of a 2D numpy array of size MxN with values from 1 to M \n    repeated N times in each row.\n    \n    Parameters:\n    M, N (int): The dimensions of the 2D array.\n    \n    Returns:\n    None: This function does not return a value but plots a heatmap.\n    \n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - seaborn\n    \n    Example:\n    >>> plot_heatmap(2, 3)\n    \"\"\"\n    data = np.array([np.arange(1, M+1) for _ in range(N)])\n    sns.heatmap(data)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, sklearn.preprocessing, pandas\n```python\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\n\ndef scale_dataframe(M, N):\n    \"\"\"\n    Generate a pandas DataFrame where each row is a sequence from 1 to M, repeated N times, \n    and scale each column to the range [0, 1] using MinMaxScaler.\n    \n    Parameters:\n    M, N (int): The parameters to determine the sequence and the number of repetitions.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the scaled sequences.\n    \n    Requirements:\n    - numpy\n    - sklearn.preprocessing\n    - pandas\n    \n    Example:\n    >>> scale_dataframe(2, 3)\n    \"\"\"\n    data = np.array([np.arange(1, M+1) for _ in range(N)])\n    scaler = MinMaxScaler()\n    scaled_data = scaler.fit_transform(data)\n    df = pd.DataFrame(scaled_data)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, matplotlib.pyplot, itertools\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom itertools import product\n\ndef plot_graphs(M, N):\n    \"\"\"\n    Generate N line graphs in a single plot, each line graph representing a sequence from 1 to M.\n    \n    Parameters:\n    M, N (int): The parameters to determine the sequence and the number of line graphs.\n    \n    Returns:\n    None: This function does not return a value but plots line graphs.\n    \n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - itertools\n    \n    Example:\n    >>> plot_graphs(2, 3)\n    \"\"\"\n    sequences = [np.arange(1, M+1) for _ in range(N)]\n    for sequence in sequences:\n        plt.plot(sequence)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, scipy.stats, pandas\n```python\nimport numpy as np\nfrom scipy.stats import norm\nimport pandas as pd\n\ndef generate_normal_distributions(M, N):\n    \"\"\"\n    Generate a pandas DataFrame with N columns, each column being a normal distribution \n    with mean varying from 1 to M.\n    \n    Parameters:\n    M, N (int): The parameters to determine the means and the number of distributions.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the normal distributions.\n    \n    Requirements:\n    - numpy\n    - scipy.stats\n    - pandas\n    \n    Example:\n    >>> generate_normal_distributions(2, 3)\n    \"\"\"\n    means = np.arange(1, M+1)\n    distributions = [norm.rvs(loc=mean, size=N) for mean in means]\n    df = pd.DataFrame(np.array(distributions).T)\n\n    return df\n```"}
{"seed": "{\"task_id\": 40978, \"prompt\": \"def f_40978(M, N):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"[i // N + 1 for i in range(N * M)]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(2, 3) == [1,1,1,2,2,2]\\n\", \"\\n    assert candidate(2, 1) == [1,2]\\n\"], \"entry_point\": \"f_40978\", \"intent\": \"1\\u304c`N`\\u500b, 2\\u304c`N`\\u500b, ..., `M`\\u304c`N`\\u500b\\u4e26\\u3076\\u30ea\\u30b9\\u30c8\\u3092\\u751f\\u6210\\u3059\\u308b\", \"library\": []}", "generation": "Scenario 1: \nnumpy, matplotlib, random\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nCOLORS = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n\ndef plot_random_walk(M, N):\n    \"\"\"\n    Generate a 2D random walk and plot the path. Each step in the random walk \n    is determined by generating a list of `N` random steps and repeating it `M` times.\n    \n    Parameters:\n    M (int): The number of times to repeat the `N` steps.\n    N (int): The number of random steps to generate.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> plot_random_walk(5, 10)\n    \"\"\"\n    steps = np.random.choice([-1, 1], N)\n    walk = np.tile(steps, M)\n    x = np.cumsum(walk)\n\n    plt.plot(x, color=COLORS[randint(0, len(COLORS)-1)])\n    plt.title('2D Random Walk')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \npandas, random\n```python\nimport pandas as pd\nfrom random import randint\n\n# Constants\nLETTERS = list('abcdefghijklmnopqrstuvwxyz')\n\ndef create_df_from_list(M, N):\n    \"\"\"\n    Create a pandas DataFrame from a list of `N` random letters, repeating it `M` times.\n    \n    Parameters:\n    M (int): The number of times to repeat the `N` letters.\n    N (int): The number of random letters to generate.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the generated letters.\n\n    Requirements:\n    - pandas\n    - random\n\n    Example:\n    >>> create_df_from_list(2, 3)\n    \"\"\"\n    letters = [LETTERS[randint(0, len(LETTERS)-1)] for _ in range(N)]\n    df = pd.DataFrame([letters] * M, columns=[f'Letter_{i+1}' for i in range(N)])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnumpy, matplotlib\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_sin_wave(M, N):\n    \"\"\"\n    Generate a plot of a sine wave with `N` periods, repeating the pattern `M` times.\n    \n    Parameters:\n    M (int): The number of times to repeat the `N` periods.\n    N (int): The number of periods in the sine wave.\n    \n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_sin_wave(2, 3)\n    \"\"\"\n    x = np.linspace(0, 2 * np.pi * N, 1000)\n    y = np.sin(x)\n    y = np.tile(y, M)\n\n    plt.plot(y)\n    plt.title('Sine Wave')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, random, matplotlib\n```python\nimport numpy as np\nfrom random import randint\nimport matplotlib.pyplot as plt\n\ndef plot_histogram(M, N):\n    \"\"\"\n    Generate a histogram from a list of `N` random integers, repeating it `M` times.\n    \n    Parameters:\n    M (int): The number of times to repeat the `N` integers.\n    N (int): The number of random integers to generate.\n    \n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_histogram(2, 3)\n    \"\"\"\n    numbers = np.array([randint(1, 10) for _ in range(N)])\n    numbers = np.tile(numbers, M)\n\n    plt.hist(numbers, bins=10)\n    plt.title('Histogram')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnumpy, random, pandas\n```python\nimport numpy as np\nfrom random import randint\nimport pandas as pd\n\ndef create_random_array(M, N):\n    \"\"\"\n    Create a numpy array of `N` random integers, repeating it `M` times.\n    \n    Parameters:\n    M (int): The number of times to repeat the `N` integers.\n    N (int): The number of random integers to generate.\n    \n    Returns:\n    ndarray: A numpy array with the generated integers.\n\n    Requirements:\n    - numpy\n    - random\n    - pandas\n\n    Example:\n    >>> create_random_array(2, 3)\n    \"\"\"\n    numbers = np.array([randint(1, 10) for _ in range(N)])\n    numbers = np.tile(numbers, M)\n\n    return numbers\n```"}
{"seed": "{\"task_id\": 39379, \"prompt\": \"def f_39379(x):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"[h.get_height() for h in sns.distplot(x).patches]\", \"test_start\": \"\\nimport seaborn as sns\\nimport numpy as np\\nsns.set()\\nnp.random.seed(0)\\n\\ndef check(candidate):\", \"test\": [\"\\n      x = np.random.rand(100)\\n      res = candidate(x)\\n      assert res == [\\n          1.2707405677074517,\\n          0.8132739633327691,\\n          1.0674220768742593,\\n          1.0674220768742597,\\n          0.8641035860410673\\n      ]\\n\"], \"entry_point\": \"f_39379\", \"intent\": \"distplot\\u3067\\u8868\\u793a\\u3057\\u305f\\u30c7\\u30fc\\u30bf`x`\\u306b\\u95a2\\u3059\\u308b\\u30d2\\u30b9\\u30c8\\u30b0\\u30e9\\u30e0\\u4e0a\\u306e\\u30d4\\u30f3\\u306e\\u9ad8\\u3055\\u3092\\u30ea\\u30b9\\u30c8\\u3068\\u3057\\u3066\\u5f97\\u308b\", \"library\": [\"numpy\", \"seaborn\"]}", "generation": "Scenario 3:\nnumpy, pandas, seaborn, matplotlib, random\n```python\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nCATEGORIES = ['A', 'B', 'C', 'D', 'E']\n\ndef generate_distplot_with_categories(number_of_samples):\n    \"\"\"\n    Generate a seaborn distribution plot for a given number of samples for each \n    category in a list of categories.\n\n    Parameters:\n    number_of_samples (int): The number of samples for each category.\n\n    Returns:\n    dict: A dictionary with category as key and list of heights of histogram bins as value.\n\n    Requirements:\n    - numpy\n    - pandas\n    - seaborn\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> heights = generate_distplot_with_categories(100)\n    >>> print(heights)\n    \"\"\"\n    data = {}\n\n    for category in CATEGORIES:\n        data[category] = np.random.normal(loc=randint(0, 10), scale=1, size=number_of_samples)\n\n    df = pd.DataFrame(data)\n\n    heights = {}\n\n    for category in CATEGORIES:\n        plot = sns.distplot(df[category], bins=10)\n        heights[category] = [h.get_height() for h in plot.patches]\n\n        # Clear the plot for next iteration\n        plt.clf()\n\n    return heights\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, pandas, seaborn, matplotlib, random\n```python\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nCATEGORIES = ['A', 'B', 'C', 'D', 'E']\n\ndef generate_boxplot_with_categories(number_of_samples):\n    \"\"\"\n    Generate a seaborn box plot for a given number of samples for each \n    category in a list of categories.\n\n    Parameters:\n    number_of_samples (int): The number of samples for each category.\n\n    Returns:\n    dict: A dictionary with category as key and box plot quartiles as value.\n\n    Requirements:\n    - numpy\n    - pandas\n    - seaborn\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> quartiles = generate_boxplot_with_categories(100)\n    >>> print(quartiles)\n    \"\"\"\n    data = {}\n\n    for category in CATEGORIES:\n        data[category] = np.random.normal(loc=randint(0, 10), scale=1, size=number_of_samples)\n\n    df = pd.DataFrame(data)\n\n    quartiles = {}\n\n    for category in CATEGORIES:\n        plot = sns.boxplot(df[category])\n\n        # Seaborn box plots have 6 artists: 4 quartile lines, 1 median line, and 1 box\n        quartile_data = [line.get_ydata()[0] for line in plot.get_lines()[:4]]\n        quartiles[category] = quartile_data\n\n        # Clear the plot for next iteration\n        plt.clf()\n\n    return quartiles\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, pandas, seaborn, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef generate_correlation_matrix(dataframe):\n    \"\"\"\n    Generate a seaborn heatmap of the correlation matrix for a given pandas DataFrame.\n\n    Parameters:\n    dataframe (DataFrame): The pandas DataFrame.\n\n    Returns:\n    ndarray: The correlation matrix.\n\n    Requirements:\n    - numpy\n    - pandas\n    - seaborn\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame(np.random.rand(10, 5), columns=['A', 'B', 'C', 'D', 'E'])\n    >>> corr_matrix = generate_correlation_matrix(df)\n    >>> print(corr_matrix)\n    \"\"\"\n    corr_matrix = dataframe.corr()\n    sns.heatmap(corr_matrix, annot=True)\n\n    return corr_matrix.values\n```\n`GPT_ODEX_BREAK`\n\nScenario 6:\nnumpy, seaborn, random, matplotlib\n```python\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nNUM_DISTS = 5\n\ndef generate_multiple_distplots(number_of_samples):\n    \"\"\"\n    Generate multiple seaborn distribution plots for a given number of samples \n    for each distribution.\n\n    Parameters:\n    number_of_samples (int): The number of samples for each distribution.\n\n    Returns:\n    dict: A dictionary with distribution number as key and list of heights of histogram \n    bins as value.\n\n    Requirements:\n    - numpy\n    - seaborn\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> heights = generate_multiple_distplots(100)\n    >>> print(heights)\n    \"\"\"\n    heights = {}\n\n    for i in range(1, NUM_DISTS+1):\n        samples = np.random.normal(loc=randint(0, 10), scale=1, size=number_of_samples)\n        plot = sns.distplot(samples, bins=10)\n        heights[i] = [h.get_height() for h in plot.patches]\n\n        # Clear the plot for next iteration\n        plt.clf()\n\n    return heights\n```\n`GPT_ODEX_BREAK`\n\nScenario 7:\nnumpy, seaborn, scipy, matplotlib\n```python\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\ndef generate_distplot_with_fit(number_of_samples):\n    \"\"\"\n    Generate a seaborn distribution plot for a given number of samples and fit a \n    normal distribution to the data.\n\n    Parameters:\n    number_of_samples (int): The number of samples.\n\n    Returns:\n    list: A list of heights of histogram bins.\n\n    Requirements:\n    - numpy\n    - seaborn\n    - matplotlib.pyplot\n    - scipy.stats.norm\n\n    Example:\n    >>> heights = generate_distplot_with_fit(100)\n    >>> print(heights)\n    \"\"\"\n    samples = np.random.normal(loc=0, scale=1, size=number_of_samples)\n    plot = sns.distplot(samples, bins=10, fit=norm)\n    heights = [h.get_height() for h in plot.patches]\n\n    return heights\n```\n"}
{"seed": "{\"task_id\": 38415, \"prompt\": \"def f_38415():\\n\\t\", \"suffix\": \"\\n\\treturn ax\", \"canonical_solution\": \"ax=plt.subplot(aspect='equal')\", \"test_start\": \"\\nimport matplotlib.pyplot as plt \\n\\ndef check(candidate):\", \"test\": [\"\\n    res_ax = candidate()\\n    assert res_ax.get_xlim() == res_ax.get_ylim()\\n\"], \"entry_point\": \"f_38415\", \"intent\": \"\\u30b0\\u30e9\\u30d5\\u306e\\u63cf\\u753b\\u7bc4\\u56f2`ax`\\u3092\\u6b63\\u65b9\\u5f62\\u306b\\u3059\\u308b\", \"library\": [\"matplotlib\"]}", "generation": "Scenario 1: \npandas, matplotlib, seaborn, numpy\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Constants\nCOLUMNS = ['Age', 'Salary', 'Experience']\nNUM_RECORDS = 100\n\ndef generate_and_plot_data():\n    \"\"\"\n    Generate a DataFrame with random values for 'Age', 'Salary', and 'Experience' columns.\n    Plot the pairwise relationships in the dataset using a scatterplot matrix.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with generated data.\n    \n    Requirements:\n    - pandas\n    - matplotlib\n    - seaborn\n    - numpy\n    \n    Example:\n    >>> df = generate_and_plot_data()\n    >>> print(df)\n    \"\"\"\n    data = np.random.randint(1, 100, size=(NUM_RECORDS, len(COLUMNS)))\n    df = pd.DataFrame(data, columns=COLUMNS)\n\n    sns.pairplot(df, kind='scatter')\n    plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nmatplotlib, numpy, scipy.signal\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.signal\n\n# Constants\nFREQUENCIES = [5, 10, 15, 20, 25]\nAMPLITUDES = [1, 0.5, 0.25, 0.125, 0.0625]\n\ndef plot_signals_and_spectra():\n    \"\"\"\n    Generate and plot a set of signals with different frequencies and amplitudes and their spectra.\n    \n    Requirements:\n    - matplotlib\n    - numpy\n    - scipy.signal\n    \n    Example:\n    >>> plot_signals_and_spectra()\n    \"\"\"\n    t = np.linspace(0, 1, 1000, False)\n    signal = np.zeros_like(t)\n    \n    # Generate signals\n    for freq, amp in zip(FREQUENCIES, AMPLITUDES):\n        signal += amp * np.sin(2 * np.pi * freq * t)\n    \n    # Compute and plot the spectrum\n    freq, spectra = scipy.signal.welch(signal, fs=1000)\n    \n    fig, axs = plt.subplots(2)\n    axs[0].plot(t, signal)\n    axs[0].set_title('Time domain signal')\n    axs[1].semilogy(freq, spectra)\n    axs[1].set_title('Frequency domain spectrum')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nmatplotlib, numpy, scipy.stats\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.stats\n\n# Constants\nMEAN = 0\nSTD_DEV = 1\nNUM_SAMPLES = 1000\n\ndef plot_normal_distribution():\n    \"\"\"\n    Generate and plot a histogram of samples from a normal distribution and its PDF.\n    \n    Requirements:\n    - matplotlib\n    - numpy\n    - scipy.stats\n    \n    Example:\n    >>> plot_normal_distribution()\n    \"\"\"\n    samples = np.random.normal(MEAN, STD_DEV, NUM_SAMPLES)\n    pdf_x = np.linspace(np.min(samples), np.max(samples), 100)\n    pdf_y = scipy.stats.norm.pdf(pdf_x, MEAN, STD_DEV)\n    \n    plt.hist(samples, bins=30, density=True, alpha=0.6, color='g')\n    plt.plot(pdf_x, pdf_y, 'orange')\n    plt.grid()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nmatplotlib, pandas, numpy, sklearn.decomposition\n```python\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom sklearn.decomposition import PCA\n\n# Constants\nNUM_RECORDS = 200\nNUM_FEATURES = 5\n\ndef perform_pca_and_plot():\n    \"\"\"\n    Generate a DataFrame with random values for several features.\n    Perform PCA and plot the explained variance ratio of each principal component.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with generated data.\n    \n    Requirements:\n    - matplotlib\n    - pandas\n    - numpy\n    - sklearn.decomposition\n    \n    Example:\n    >>> df = perform_pca_and_plot()\n    >>> print(df)\n    \"\"\"\n    data = np.random.rand(NUM_RECORDS, NUM_FEATURES)\n    df = pd.DataFrame(data, columns=[f'feature_{i}' for i in range(NUM_FEATURES)])\n\n    pca = PCA(n_components=NUM_FEATURES)\n    pca.fit(df)\n\n    plt.bar(range(NUM_FEATURES), pca.explained_variance_ratio_)\n    plt.xlabel('Principal Component')\n    plt.ylabel('Explained Variance Ratio')\n    plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nmatplotlib, numpy, scipy.optimize\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.optimize import curve_fit\n\n# Constants\nNOISE_STD_DEV = 0.2\nNUM_POINTS = 100\n\n# Function to model\ndef f(x, a, b, c):\n    return a * np.exp(-b * x) + c\n\ndef fit_and_plot_curve():\n    \"\"\"\n    Generate noisy data from an exponential function, fit the function to the data\n    using non-linear least squares, and plot the data and the fitted function.\n    \n    Returns:\n    tuple: Optimal values for the parameters so that the sum of the squared residuals is minimized.\n    \n    Requirements:\n    - matplotlib\n    - numpy\n    - scipy.optimize\n    \n    Example:\n    >>> params = fit_and_plot_curve()\n    >>> print(params)\n    \"\"\"\n    x = np.linspace(0, 4, NUM_POINTS)\n    y = f(x, 2.5, 1.3, 0.5)\n    \n    # Add noise to the data\n    y_noise = y + NOISE_STD_DEV * np.random.normal(size=x.size)\n\n    popt, pcov = curve_fit(f, x, y_noise)\n\n    plt.figure()\n    plt.plot(x, y_noise, 'b-', label='data with noise')\n    plt.plot(x, f(x, *popt), 'r-', label='fit: a=%5.3f, b=%5.3f, c=%5.3f' % tuple(popt))\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.legend()\n    plt.show()\n\n    return popt\n```"}
{"seed": "{\"task_id\": 37757, \"prompt\": \"def f_37757(string):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"eval(string)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(\\\"[1,1,1,2,2,2]\\\") == [1,1,1,2,2,2]\\n\", \"\\n    assert candidate(\\\"[1,2]\\\") == [1,2]\\n\"], \"entry_point\": \"f_37757\", \"intent\": \"\\u6587\\u5b57\\u5217\\u578b\\u5909\\u6570`string`\\u306e\\u5024\\u3092\\u6570\\u5024\\u578b\\u306e\\u30a4\\u30f3\\u30b9\\u30bf\\u30f3\\u30b9\\u5909\\u6570\\u3068\\u3057\\u3066\\u8a55\\u4fa1\\u3059\\u308b\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, matplotlib, random\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nNUM_STUDENTS = 100\nGRADES = [\"A\", \"B\", \"C\", \"D\", \"F\"]\n\ndef generate_student_data():\n    \"\"\"\n    Generate a report of student data with random grades.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with student data.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - random\n    \n    Example:\n    >>> student_data = generate_student_data()\n    >>> print(student_data)\n    >>> student_data['Grade'].value_counts().plot(kind='bar')\n    \"\"\"\n    student_data = pd.DataFrame({\n        'Student ID': np.arange(1, NUM_STUDENTS+1),\n        'Grade': [GRADES[randint(0, len(GRADES)-1)] for _ in range(NUM_STUDENTS)]\n    })\n\n    return student_data\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nast, json, yaml\n```python\nimport ast\nimport json\nimport yaml\n\ndef convert_string_to_dict(input_string, input_format, output_format):\n    \"\"\"\n    Convert a string in a specific format (Python dict, JSON, or YAML) to a dictionary in \n    another specific format (Python dict, JSON, or YAML).\n\n    Parameters:\n    input_string (str): The input string.\n    input_format (str): The format of the input string. It should be one of \"dict\", \"json\", or \"yaml\".\n    output_format (str): The format of the output. It should be one of \"dict\", \"json\", or \"yaml\".\n\n    Returns:\n    dict or str: The converted dictionary or string.\n\n    Requirements:\n    - ast\n    - json\n    - yaml\n\n    Example:\n    >>> convert_string_to_dict(\"{\\\"key\\\": \\\"value\\\"}\", \"json\", \"yaml\")\n    \"\"\"\n    if input_format == \"dict\":\n        input_dict = ast.literal_eval(input_string)\n    elif input_format == \"json\":\n        input_dict = json.loads(input_string)\n    elif input_format == \"yaml\":\n        input_dict = yaml.safe_load(input_string)\n    else:\n        raise ValueError(\"Invalid input format\")\n\n    if output_format == \"dict\":\n        return input_dict\n    elif output_format == \"json\":\n        return json.dumps(input_dict)\n    elif output_format == \"yaml\":\n        return yaml.safe_dump(input_dict)\n    else:\n        raise ValueError(\"Invalid output format\")\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, time, datetime\n```python\nimport os\nimport time\nfrom datetime import datetime\n\ndef get_file_info(file_path):\n    \"\"\"\n    Get information about a file including its size, creation time, and last modification time.\n\n    Parameters:\n    file_path (str): The path to the file.\n\n    Returns:\n    dict: A dictionary with the file information.\n\n    Requirements:\n    - os\n    - time\n    - datetime\n\n    Example:\n    >>> get_file_info(\"/path/to/file.txt\")\n    \"\"\"\n    file_info = os.stat(file_path)\n\n    return {\n        \"size\": file_info.st_size,\n        \"creation_time\": datetime.fromtimestamp(file_info.st_ctime).isoformat(),\n        \"last_modification_time\": datetime.fromtimestamp(file_info.st_mtime).isoformat()\n    }\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, pandas, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nNUM_SAMPLES = 1000\n\ndef generate_and_plot_data():\n    \"\"\"\n    Generate a sample of data from a normal distribution and plot a histogram.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> generate_and_plot_data()\n    \"\"\"\n    data = np.random.normal(size=NUM_SAMPLES)\n    pd.Series(data).plot(kind='hist', bins=50, title='Histogram of Generated Data')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nast, math, random\n```python\nimport ast\nimport math\nfrom random import randint, random\n\n# Constants\nNUM_OPERATIONS = 100\n\ndef evaluate_random_expression():\n    \"\"\"\n    Generate a random mathematical expression and evaluate it.\n\n    Returns:\n    float: The result of the evaluation.\n\n    Requirements:\n    - ast\n    - math\n    - random\n\n    Example:\n    >>> evaluate_random_expression()\n    \"\"\"\n    operations = ['+', '-', '*', '/']\n    expression = str(randint(1, 10))\n\n    for _ in range(NUM_OPERATIONS):\n        operation = operations[randint(0, len(operations) - 1)]\n        operand = str(randint(1, 10) if operation != '/' else random())\n        expression += ' ' + operation + ' ' + operand\n\n    return eval(expression)\n```\nAbove is the illustration."}
{"seed": "{\"task_id\": 34422, \"prompt\": \"def f_34422(s_json):\\n\\t\", \"suffix\": \"\\n\\treturn d\", \"canonical_solution\": \"d = json.loads(s_json)\", \"test_start\": \"\\nimport json\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('{\\\"a\\\":123,\\\"b\\\":45.6}') == {'a':123, 'b':45.6}\\n\"], \"entry_point\": \"f_34422\", \"intent\": \"JSON\\u3092\\u8868\\u3059\\u6587\\u5b57\\u5217`s_json`\\u304b\\u3089\\u8f9e\\u66f8\\u578b\\u30aa\\u30d6\\u30b8\\u30a7\\u30af\\u30c8`d`\\u3092\\u5f97\\u308b\", \"library\": [\"json\"]}", "generation": "Scenario 1: \njson, pandas, matplotlib, datetime\n```python\nimport json\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\n\ndef convert_json_to_df_and_plot(s_json):\n    \"\"\"\n    Convert the JSON string representing a list of records with 'date' and 'value' fields to pandas DataFrame \n    and plot the 'value' over 'date'.\n    \n    Parameters:\n    s_json (str): The JSON string.\n    \n    Returns:\n    DataFrame: The pandas DataFrame obtained from the JSON string.\n    \n    Requirements:\n    - json\n    - pandas\n    - matplotlib.pyplot\n    - datetime\n    \n    Example:\n    >>> s_json = '[{\"date\": \"2020-01-01\", \"value\": 10}, {\"date\": \"2020-02-01\", \"value\": 15}, {\"date\": \"2020-03-01\", \"value\": 20}]'\n    >>> df = convert_json_to_df_and_plot(s_json)\n    >>> print(df)\n    \"\"\"\n    records = json.loads(s_json)\n    df = pd.DataFrame(records)\n    df['date'] = pd.to_datetime(df['date'])\n    df.set_index('date', inplace=True)\n    df['value'].plot()\n    plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \njson, os, shutil\n```python\nimport json\nimport os\nimport shutil\n\ndef move_files_based_on_json(s_json, source_dir, target_dir):\n    \"\"\"\n    Move files from source directory to target directory based on the JSON string representing a \n    list of filenames.\n    \n    Parameters:\n    s_json (str): The JSON string.\n    source_dir (str): The source directory.\n    target_dir (str): The target directory.\n    \n    Returns:\n    list: The list of files successfully moved.\n    \n    Requirements:\n    - json\n    - os\n    - shutil\n    \n    Example:\n    >>> s_json = '[\"file1.txt\", \"file2.txt\", \"file3.txt\"]'\n    >>> moved_files = move_files_based_on_json(s_json, '/path/to/source', '/path/to/target')\n    >>> print(moved_files)\n    \"\"\"\n    filenames = json.loads(s_json)\n    moved_files = []\n\n    for filename in filenames:\n        source_file = os.path.join(source_dir, filename)\n        target_file = os.path.join(target_dir, filename)\n\n        if os.path.exists(source_file):\n            shutil.move(source_file, target_file)\n            moved_files.append(filename)\n\n    return moved_files\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \njson, requests, csv\n```python\nimport json\nimport requests\nimport csv\n\ndef fetch_and_convert_json_to_csv(s_json, csv_file_path):\n    \"\"\"\n    Fetch JSON data from URLs provided in the JSON string and convert the JSON response to CSV file.\n    \n    Parameters:\n    s_json (str): The JSON string representing a list of URLs.\n    csv_file_path (str): The path to the CSV file.\n    \n    Requirements:\n    - json\n    - requests\n    - csv\n    \n    Example:\n    >>> s_json = '[\"https://api.example.com/data1\", \"https://api.example.com/data2\"]'\n    >>> fetch_and_convert_json_to_csv(s_json, '/path/to/file.csv')\n    \"\"\"\n    urls = json.loads(s_json)\n    data = []\n\n    for url in urls:\n        response = requests.get(url)\n        data.extend(response.json())\n\n    with open(csv_file_path, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(data[0].keys())  # column headers\n        for row in data:\n            writer.writerow(row.values())\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \njson, re, collections\n```python\nimport json\nimport re\nimport collections\n\ndef count_word_frequency_in_json(s_json):\n    \"\"\"\n    Count the frequency of words in the JSON string.\n    \n    Parameters:\n    s_json (str): The JSON string.\n    \n    Returns:\n    dict: A dictionary with words as keys and their frequencies as values.\n    \n    Requirements:\n    - json\n    - re\n    - collections\n    \n    Example:\n    >>> s_json = '{\"text\": \"Hello, world! Hello, Python!\"}'\n    >>> freq = count_word_frequency_in_json(s_json)\n    >>> print(freq)\n    \"\"\"\n    data = json.loads(s_json)\n    words = re.findall(r'\\w+', data['text'])\n    freq = collections.Counter(words)\n\n    return dict(freq)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \njson, hashlib, binascii\n```python\nimport json\nimport hashlib\nimport binascii\n\ndef hash_values_in_json(s_json, hash_func='sha256'):\n    \"\"\"\n    Compute the hash of values in the JSON string using the specified hash function.\n    \n    Parameters:\n    s_json (str): The JSON string.\n    hash_func (str): The hash function to use (default is 'sha256').\n    \n    Returns:\n    dict: A dictionary with the same keys as the input JSON and hashed values.\n    \n    Requirements:\n    - json\n    - hashlib\n    - binascii\n    \n    Example:\n    >>> s_json = '{\"key1\": \"value1\", \"key2\": \"value2\"}'\n    >>> hashed_values = hash_values_in_json(s_json)\n    >>> print(hashed_values)\n    \"\"\"\n    data = json.loads(s_json)\n    hashed_values = {}\n\n    for key, value in data.items():\n        hash_obj = hashlib.new(hash_func)\n        hash_obj.update(value.encode())\n        hashed_value = binascii.hexlify(hash_obj.digest()).decode()\n        hashed_values[key] = hashed_value\n\n    return hashed_values\n```"}
{"seed": "{\"task_id\": 27686, \"prompt\": \"def f_27686(soup):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"soup.get('a_id')\", \"test_start\": \"\\nfrom bs4 import BeautifulSoup\\n\\ndef check(candidate):\", \"test\": [\"\\n    soup = BeautifulSoup('<p>riginsf</p>')\\n    soup['a_id'] = 'some value'\\n    assert candidate(soup) == 'some value'\\n\"], \"entry_point\": \"f_27686\", \"intent\": \"HTML\\u30d1\\u30fc\\u30b9\\u30aa\\u30d6\\u30b8\\u30a7\\u30af\\u30c8`soup`\\u306e\\u4e2d\\u3067HTML\\u30bf\\u30b0\\u306e`a_id`\\u306e\\u5c5e\\u6027\\u5024\\u3092\\u53d6\\u5f97\\u3059\\u308b\", \"library\": [\"bs4\"]}", "generation": "Scenario 1: \nbs4, requests, re, pandas\n```python\nfrom bs4 import BeautifulSoup\nimport requests\nimport re\nimport pandas as pd\n\nURL = \"https://en.wikipedia.org/wiki/List_of_countries_by_population_(United_Nations)\"\n\ndef scrape_population_data():\n    \"\"\"\n    Scrape population data from a Wikipedia page and return it as a pandas DataFrame.\n    \n    Requirements:\n    - bs4\n    - requests\n    - re\n    - pandas\n\n    Example:\n    >>> df = scrape_population_data()\n    >>> print(df.head())\n    \"\"\"\n\n    res = requests.get(URL)\n    soup = BeautifulSoup(res.text, 'html.parser')\n    table = soup.find('table', attrs={'class':'wikitable'})\n\n    data = []\n    for row in table.findAll('tr'):\n        columns = row.findAll('td')\n        output_row = []\n        for column in columns:\n            output_row.append(re.sub(r'\\n', '', column.text))\n        if output_row:\n            data.append(output_row)\n\n    df = pd.DataFrame(data, columns=['Rank', 'Country', 'Population', 'Date', 'Percentage of World Population'])\n\n    return df\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 2:\nbs4, requests, json\n```python\nfrom bs4 import BeautifulSoup\nimport requests\nimport json\n\nURL = \"https://www.imdb.com/chart/top\"\n\ndef scrape_imdb_top_movies():\n    \"\"\"\n    Scrape the top 250 movies from IMDB and return it as a JSON object.\n\n    Requirements:\n    - bs4\n    - requests\n    - json\n\n    Example:\n    >>> json_obj = scrape_imdb_top_movies()\n    >>> print(json.dumps(json_obj, indent=2))\n    \"\"\"\n\n    res = requests.get(URL)\n    soup = BeautifulSoup(res.text, 'html.parser')\n    movies_table = soup.find('table', attrs={'class':'chart'})\n\n    movies = []\n    for row in movies_table.findAll('tr'):\n        title_col = row.find('td', attrs={'class':'titleColumn'})\n        if title_col:\n            title = title_col.find('a').text\n            year = title_col.find('span').text.strip('()')\n            movies.append({'title': title, 'year': year})\n\n    return json.dumps(movies)\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 3:\nbs4, requests, csv\n```python\nfrom bs4 import BeautifulSoup\nimport requests\nimport csv\n\nURL = \"https://realpython.github.io/fake-jobs/\"\n\ndef scrape_job_listings():\n    \"\"\"\n    Scrape job listings from a URL and write them into a CSV file.\n\n    Requirements:\n    - bs4\n    - requests\n    - csv\n\n    Example:\n    >>> scrape_job_listings()\n    \"\"\"\n\n    res = requests.get(URL)\n    soup = BeautifulSoup(res.text, 'html.parser')\n    jobs_table = soup.find('table', attrs={'id':'jobs'})\n\n    jobs = []\n    for row in jobs_table.findAll('tr'):\n        job_data = row.findAll('td')\n        if job_data:\n            job = [j.text for j in job_data]\n            jobs.append(job)\n\n    with open('jobs.csv', 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerows(jobs)\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 4:\nbs4, requests, matplotlib, pandas\n```python\nfrom bs4 import BeautifulSoup\nimport requests\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nURL = \"https://www.worldometers.info/coronavirus/\"\n\ndef plot_covid_data():\n    \"\"\"\n    Scrape COVID-19 case data from a URL and plot a bar chart of the top 10 countries with the most cases.\n\n    Requirements:\n    - bs4\n    - requests\n    - matplotlib.pyplot\n    - pandas\n\n    Example:\n    >>> plot_covid_data()\n    \"\"\"\n\n    res = requests.get(URL)\n    soup = BeautifulSoup(res.text, 'html.parser')\n    table = soup.find('table', attrs={'id':'main_table_countries_today'})\n\n    data = []\n    for row in table.findAll('tr')[1:]:\n        columns = row.findAll('td')\n        if columns[1].text not in ['World', 'Total:']:\n            data.append([columns[1].text, int(columns[2].text.replace(',', ''))])\n\n    df = pd.DataFrame(data, columns=['Country', 'Cases']).sort_values('Cases', ascending=False)[:10]\n    df.plot(kind='bar', x='Country', y='Cases')\n    plt.show()\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 5:\nbs4, requests, lxml, csv\n```python\nfrom bs4 import BeautifulSoup\nimport requests\nimport csv\nimport lxml\n\nURL = \"https://www.basketball-reference.com/leagues/NBA_2021_totals.html\"\n\ndef scrape_basketball_stats():\n    \"\"\"\n    Scrape basketball stats from a URL and write them into a CSV file.\n\n    Requirements:\n    - bs4\n    - requests\n    - lxml\n    - csv\n\n    Example:\n    >>> scrape_basketball_stats()\n    \"\"\"\n\n    res = requests.get(URL)\n    soup = BeautifulSoup(res.text, 'lxml')\n    table = soup.find('table')\n\n    headers = [th.getText() for th in table.findAll('tr')[0].findAll('th')][1:]\n    rows = table.findAll('tr')[1:]\n    player_stats = [[td.getText() for td in rows[i].findAll('td')]\n                for i in range(len(rows))]\n\n    with open('basketball_stats.csv', 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(headers)\n        writer.writerows(player_stats)\n```\n"}
{"seed": "{\"task_id\": 41054, \"prompt\": \"def f_41054():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"[os.rename(f, f.replace('.dat', '.gui')) for f in os.listdir('.') if not f.startswith('.')]\", \"test_start\": \"\\nimport os\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert all([((item is None) or item.endswiths('.gui')) for item in candidate()])\\n\"], \"entry_point\": \"f_41054\", \"intent\": \"\\u30ab\\u30ec\\u30f3\\u30c8\\u30c7\\u30a3\\u30ec\\u30af\\u30c8\\u30ea\\u306b\\u3042\\u308b\\u7279\\u5b9a\\u306e\\u62e1\\u5f35\\u5b50`.dat`\\u3092\\u3082\\u3064\\u30d5\\u30a1\\u30a4\\u30eb\\u306e\\u62e1\\u5f35\\u5b50\\u3092`.gui`\\u306b\\u3059\\u3079\\u3066\\u66f8\\u304d\\u63db\\u3048\\u308b\", \"library\": [\"os\"]}", "generation": "Scenario 1: \nos, shutil, glob, random\n```python\nimport os\nimport shutil\nfrom glob import glob\nfrom random import choice\n\n# Constants\nFILE_EXTENSIONS = ['.txt', '.doc', '.pdf', '.jpg', '.png']\n\ndef random_file_renamer(directory):\n    \"\"\"\n    Rename a random file in a given directory by changing its extension to a different one randomly chosen from a list.\n\n    Parameters:\n    directory (str): The directory in which to find and rename a file.\n    \n    Returns:\n    str: The new filename.\n    \n    Requirements:\n    - os\n    - shutil\n    - glob.glob\n    - random.choice\n\n    Example:\n    >>> random_file_renamer('/home/user/Documents')\n    \"\"\"\n    files = glob(os.path.join(directory, '*'))\n    file_to_rename = choice(files)\n    filename, _ = os.path.splitext(file_to_rename)\n    new_extension = choice(FILE_EXTENSIONS)\n    new_filename = filename + new_extension\n    shutil.move(file_to_rename, new_filename)\n    \n    return new_filename\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, time, datetime\n```python\nimport os\nimport time\nfrom datetime import datetime\n\ndef delete_files_older_than_days(directory, days):\n    \"\"\"\n    Delete all files in a given directory that are older than a certain number of days.\n\n    Parameters:\n    directory (str): The directory in which to delete files.\n    days (int): The number of days.\n    \n    Returns:\n    list: The list of deleted files.\n\n    Requirements:\n    - os\n    - time\n    - datetime.datetime\n\n    Example:\n    >>> delete_files_older_than_days('/home/user/Documents', 30)\n    \"\"\"\n    files_deleted = []\n    now = time.mktime(datetime.now().timetuple())\n\n    for filename in os.listdir(directory):\n        file_path = os.path.join(directory, filename)\n        file_time = os.path.getmtime(file_path)\n        file_age_days = (now - file_time) / (60 * 60 * 24)\n\n        if file_age_days > days:\n            os.remove(file_path)\n            files_deleted.append(filename)\n\n    return files_deleted\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, shutil, re\n```python\nimport os\nimport shutil\nimport re\n\ndef rename_files_with_number(directory):\n    \"\"\"\n    Rename all files in a given directory that contain numbers in their names by replacing the numbers with their word equivalents.\n\n    Parameters:\n    directory (str): The directory in which to rename files.\n\n    Returns:\n    list: The list of new filenames.\n\n    Requirements:\n    - os\n    - shutil\n    - re\n\n    Example:\n    >>> rename_files_with_number('/home/user/Documents')\n    \"\"\"\n    num2words = {1: \"one\", 2: \"two\", 3: \"three\", 4: \"four\", 5: \"five\", \n                 6: \"six\", 7: \"seven\", 8: \"eight\", 9: \"nine\"}\n\n    new_filenames = []\n\n    for filename in os.listdir(directory):\n        new_filename = re.sub(r'\\d', lambda x: num2words[int(x.group())], filename)\n        shutil.move(os.path.join(directory, filename), os.path.join(directory, new_filename))\n        new_filenames.append(new_filename)\n\n    return new_filenames\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, fnmatch, shutil\n```python\nimport os\nimport fnmatch\nimport shutil\n\ndef move_files_with_extension(source_directory, destination_directory, extension):\n    \"\"\"\n    Move all files with a certain extension from a source directory to a destination directory.\n\n    Parameters:\n    source_directory (str): The source directory.\n    destination_directory (str): The destination directory.\n    extension (str): The file extension.\n\n    Returns:\n    list: The list of moved files.\n\n    Requirements:\n    - os\n    - fnmatch\n    - shutil\n\n    Example:\n    >>> move_files_with_extension('/home/user/Documents', '/home/user/Images', '.jpg')\n    \"\"\"\n    moved_files = []\n\n    for filename in os.listdir(source_directory):\n        if fnmatch.fnmatch(filename, '*'+extension):\n            shutil.move(os.path.join(source_directory, filename), destination_directory)\n            moved_files.append(filename)\n    \n    return moved_files\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, glob\n```python\nimport os\nfrom glob import glob\n\ndef find_files_by_size(directory, min_size, max_size):\n    \"\"\"\n    Find all files in a given directory that are within a certain size range.\n\n    Parameters:\n    directory (str): The directory in which to find files.\n    min_size (int): The minimum file size in bytes.\n    max_size (int): The maximum file size in bytes.\n\n    Returns:\n    list: The list of found files.\n\n    Requirements:\n    - os\n    - glob.glob\n\n    Example:\n    >>> find_files_by_size('/home/user/Documents', 1000, 2000)\n    \"\"\"\n    files_in_range = []\n\n    for filename in glob(os.path.join(directory, '*')):\n        file_size = os.path.getsize(filename)\n        if min_size <= file_size <= max_size:\n            files_in_range.append(filename)\n\n    return files_in_range\n```"}
{"seed": "{\"task_id\": 43303, \"prompt\": \"def f_43303(df):\\n\\t\", \"suffix\": \"\\n\\treturn df2\", \"canonical_solution\": \"df2 = df.reset_index(drop=True)\", \"test_start\": \"\\nimport pandas as pd\\n\\ndef check(candidate):\", \"test\": [\"\\n    df = pd.DataFrame(data={'col1':[0,1,2,3], 'col2': pd.Series([2,3], index=[2,3])}, index=[0,2,1,3])\\n    assert candidate(df).equals(pd.DataFrame(data={'col1': [0,1,2,3], 'col2': pd.Series([2,3], index=[1,3])}, index=[0,1,2,3]))\\n\"], \"entry_point\": \"f_43303\", \"intent\": \"\\u30c7\\u30fc\\u30bf\\u30d5\\u30ec\\u30fc\\u30e0`df`\\u306e\\u30a4\\u30f3\\u30c7\\u30c3\\u30af\\u30b9\\u3092\\u30ea\\u30bb\\u30c3\\u30c8\\u3057\\u305f\\u65b0\\u305f\\u306a\\u30c7\\u30fc\\u30bf\\u30d5\\u30ec\\u30fc\\u30e0`df2`\\u3092\\u5f97\\u308b\", \"library\": [\"pandas\"]}", "generation": "Scenario 1:\npandas, matplotlib, seaborn, numpy\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Constants\nCOLUMNS = ['age', 'sex', 'bmi', 'children', 'smoker', 'region', 'charges']\n\ndef process_and_plot(df):\n    \"\"\"\n    Process a DataFrame by resetting its index, replacing missing values with the column mean, \n    and plotting a heatmap of the correlation matrix.\n\n    Parameters:\n    df (DataFrame): The input DataFrame.\n\n    Returns:\n    DataFrame: The processed DataFrame.\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - seaborn\n    - numpy\n\n    Example:\n    >>> df = pd.DataFrame(data={'age':[19,18,28,33,32], 'sex':['female','male','male','male','male'], 'bmi':[27.9,33.77,33.0,22.705,28.88], \n    >>> 'children':[0,1,3,0,0], 'smoker':['yes','no','no','no','no'], 'region':['southwest','southeast','southeast','northwest','northwest'], \n    >>> 'charges':[16884.924, 1725.5523, 4449.462, 21984.47061, 3866.8552]})\n    >>> df = process_and_plot(df)\n    >>> print(df)\n    \"\"\"\n    df = df.reset_index(drop=True)\n    df = df.fillna(df.mean())\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n    plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, sklearn, numpy, matplotlib\n```python\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef standardize_and_histogram(df):\n    \"\"\"\n    Standardize a DataFrame by resetting its index, replacing missing values with the column mean, \n    and plotting a histogram of the standardized values.\n\n    Parameters:\n    df (DataFrame): The input DataFrame.\n\n    Returns:\n    DataFrame: The standardized DataFrame.\n\n    Requirements:\n    - pandas\n    - sklearn.preprocessing.StandardScaler\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame(np.random.randint(0,100,size=(100, 5)), columns=list('ABCDE'))\n    >>> df = standardize_and_histogram(df)\n    >>> print(df)\n    \"\"\"\n    df = df.reset_index(drop=True)\n    df = df.fillna(df.mean())\n    scaler = StandardScaler()\n    df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=COLUMNS)\n    df_scaled.hist(figsize=(10, 8))\n    plt.tight_layout()\n    plt.show()\n\n    return df_scaled\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, matplotlib, numpy, seaborn\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\n# Constants\nCOLUMNS = ['age', 'sex', 'bmi', 'children', 'smoker', 'region', 'charges']\n\ndef process_and_pairplot(df):\n    \"\"\"\n    Process a DataFrame by resetting its index, replacing missing values with the column mean, \n    and plotting a pairplot of the DataFrame columns.\n\n    Parameters:\n    df (DataFrame): The input DataFrame.\n\n    Returns:\n    DataFrame: The processed DataFrame.\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - seaborn\n    - numpy\n\n    Example:\n    >>> df = pd.DataFrame(data={'age':[19,18,28,33,32], 'sex':['female','male','male','male','male'], 'bmi':[27.9,33.77,33.0,22.705,28.88], \n    >>> 'children':[0,1,3,0,0], 'smoker':['yes','no','no','no','no'], 'region':['southwest','southeast','southeast','northwest','northwest'], \n    >>> 'charges':[16884.924, 1725.5523, 4449.462, 21984.47061, 3866.8552]})\n    >>> df = process_and_pairplot(df)\n    >>> print(df)\n    \"\"\"\n    df = df.reset_index(drop=True)\n    df = df.fillna(df.mean())\n    sns.pairplot(df, hue='smoker')\n    plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, matplotlib, numpy, seaborn\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\n# Constants\nCOLUMNS = ['age', 'sex', 'bmi', 'children', 'smoker', 'region', 'charges']\n\ndef process_and_violinplot(df):\n    \"\"\"\n    Process a DataFrame by resetting its index, replacing missing values with the column mean, \n    and plotting a violin plot of the DataFrame columns.\n\n    Parameters:\n    df (DataFrame): The input DataFrame.\n\n    Returns:\n    DataFrame: The processed DataFrame.\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - seaborn\n    - numpy\n\n    Example:\n    >>> df = pd.DataFrame(data={'age':[19,18,28,33,32], 'sex':['female','male','male','male','male'], 'bmi':[27.9,33.77,33.0,22.705,28.88], \n    >>> 'children':[0,1,3,0,0], 'smoker':['yes','no','no','no','no'], 'region':['southwest','southeast','southeast','northwest','northwest'], \n    >>> 'charges':[16884.924, 1725.5523, 4449.462, 21984.47061, 3866.8552]})\n    >>> df = process_and_violinplot(df)\n    >>> print(df)\n    \"\"\"\n    df = df.reset_index(drop=True)\n    df = df.fillna(df.mean())\n    sns.violinplot(x='children', y='charges', hue='smoker', data=df, palette='muted', split=True)\n    plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, sklearn, numpy, matplotlib\n```python\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef minmaxscale_and_boxplot(df):\n    \"\"\"\n    Scale a DataFrame by resetting its index, replacing missing values with the column mean, \n    and plotting a box plot of the scaled values.\n\n    Parameters:\n    df (DataFrame): The input DataFrame.\n\n    Returns:\n    DataFrame: The scaled DataFrame.\n\n    Requirements:\n    - pandas\n    - sklearn.preprocessing.MinMaxScaler\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame(np.random.randint(0,100,size=(100, 5)), columns=list('ABCDE'))\n    >>> df = minmaxscale_and_boxplot(df)\n    >>> print(df)\n    \"\"\"\n    df = df.reset_index(drop=True)\n    df = df.fillna(df.mean())\n    scaler = MinMaxScaler()\n    df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=COLUMNS)\n    df_scaled.boxplot(figsize=(10, 8))\n    plt.tight_layout()\n    plt.show()\n\n    return df_scaled\n```\n`GPT_ODEX_BREAK`"}
{"seed": "{\"task_id\": 43322, \"prompt\": \"def f_43322(df):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"df.resample('1min').ffill()\", \"test_start\": \"\\nimport io\\nimport pandas as pd\\n\\ndef check(candidate):\", \"test\": [\"\\n    data = (\\n        \\\"\\u5e74\\u6708\\u65e5\\u6642,\\u6c17\\u6e29(\\u2103),\\u964d\\u6c34\\u91cf(mm),\\u98a8\\u901f(m/s),\\u65e5\\u5c04\\u91cf(MJ/\\u33a1)\\\\n\\\"\\n        \\\"2017-01-01 00:00:00,5.8,0.0,1.5,0.0\\\\n\\\"\\n        \\\"2017-01-01 01:00:00,4.9,0.0,0.8,0.0\\\\n\\\"\\n        \\\"2017-01-01 02:00:00,4.9,0.0,1.5,0.0\\\\n\\\"\\n        \\\"2017-01-01 03:00:00,4.2,0.0,0.8,0.0\\\\n\\\"\\n        \\\"2017-01-01 04:00:00,4.4,0.0,1.0,0.0\\\\n\\\"\\n    )\\n    df = pd.read_csv(io.StringIO(data), parse_dates=['\\u5e74\\u6708\\u65e5\\u6642'], index_col='\\u5e74\\u6708\\u65e5\\u6642')\\n    res = candidate(df)\\n    assert len(res) == 241\\n\"], \"entry_point\": \"f_43322\", \"intent\": \"\\u6642\\u7cfb\\u5217\\u30c7\\u30fc\\u30bf\\u306e\\u5165\\u3063\\u305f\\u30c7\\u30fc\\u30bf\\u30d5\\u30ec\\u30fc\\u30e0`df`\\u30921\\u5206\\u3054\\u3068\\u306b\\u30ea\\u30b5\\u30f3\\u30d7\\u30eb\\u3057\\u3001\\u9593\\u306e\\u5024\\u306f\\u76f4\\u524d\\u306e\\u5024\\u3067\\u88dc\\u5b8c\\u3059\\u308b\", \"library\": [\"io\", \"pandas\"]}", "generation": "Scenario 1: \npandas, numpy, datetime, random, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nPRODUCTS = ['Apple', 'Banana', 'Orange', 'Pear', 'Grapes']\nSTORES = ['Store1', 'Store2', 'Store3', 'Store4', 'Store5']\n\ndef generate_sales_report(start_date, end_date):\n    \"\"\"\n    Generate a report of sales for a list of products across various \n    stores between two dates.\n    \n    Parameters:\n    start_date (datetime): The start date.\n    end_date (datetime): The end date.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with sales data for the products.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - datetime\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> start_date = datetime(2023, 6, 1)\n    >>> end_date = datetime(2023, 6, 30)\n    >>> report = generate_sales_report(start_date, end_date)\n    >>> print(report)\n    >>> report.groupby('Product')['Sales'].sum().plot(kind='bar')\n    \"\"\"\n    data = []\n\n    dates = pd.date_range(start=start_date, end=end_date)\n\n    for date in dates:\n        for product in PRODUCTS:\n            for store in STORES:\n                sales = randint(10, 100)\n                data.append([date, product, store, sales])\n\n    df = pd.DataFrame(data, columns=['Date', 'Product', 'Store', 'Sales'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, matplotlib, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nFEATURES = ['Feature1', 'Feature2', 'Feature3', 'Feature4', 'Feature5']\n\ndef generate_and_plot_correlation_matrix(df):\n    \"\"\"\n    Generate a correlation matrix from a DataFrame and plot it using a heatmap.\n    \n    Parameters:\n    df (DataFrame): The input DataFrame.\n    \n    Returns:\n    DataFrame: The correlation matrix.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - seaborn\n    \n    Example:\n    >>> df = pd.DataFrame(np.random.rand(50, 5), columns=FEATURES)\n    >>> corr_matrix = generate_and_plot_correlation_matrix(df)\n    >>> print(corr_matrix)\n    \"\"\"\n    corr_matrix = df[FEATURES].corr()\n    \n    plt.figure(figsize=(10, 8))\n    sns.heatmap(corr_matrix, annot=True, cmap=plt.cm.Reds)\n    plt.show()\n\n    return corr_matrix\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \npandas, numpy, sklearn, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\n\n# Constants\nFEATURES = ['Feature1', 'Feature2', 'Feature3', 'Feature4', 'Feature5']\nTARGET = 'Target'\n\ndef perform_linear_regression(df):\n    \"\"\"\n    Perform linear regression on a DataFrame with a given target and plot the \n    resulting regression line.\n    \n    Parameters:\n    df (DataFrame): The input DataFrame.\n    \n    Returns:\n    LinearRegression: The trained model.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.linear_model.LinearRegression\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = pd.DataFrame(np.random.rand(50, 6), columns=FEATURES + [TARGET])\n    >>> model = perform_linear_regression(df)\n    >>> print(model.coef_)\n    \"\"\"\n    X = df[FEATURES]\n    y = df[TARGET]\n\n    model = LinearRegression().fit(X, y)\n\n    plt.figure(figsize=(10, 8))\n    plt.scatter(X, y, color='blue')\n    plt.plot(X, model.predict(X), color='red')\n    plt.show()\n\n    return model\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \npandas, numpy, sklearn, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\n# Constants\nFEATURES = ['Feature1', 'Feature2']\n\ndef perform_kmeans_clustering(df, n_clusters):\n    \"\"\"\n    Perform k-means clustering on a DataFrame and plot the resulting clusters.\n    \n    Parameters:\n    df (DataFrame): The input DataFrame.\n    n_clusters (int): The number of clusters.\n    \n    Returns:\n    KMeans: The trained model.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.cluster.KMeans\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = pd.DataFrame(np.random.rand(50, 2), columns=FEATURES)\n    >>> model = perform_kmeans_clustering(df, 3)\n    >>> print(model.cluster_centers_)\n    \"\"\"\n    X = df[FEATURES]\n\n    model = KMeans(n_clusters=n_clusters).fit(X)\n\n    plt.figure(figsize=(10, 8))\n    plt.scatter(X[FEATURES[0]], X[FEATURES[1]], c=model.labels_, cmap='viridis')\n    plt.scatter(model.cluster_centers_[:, 0], model.cluster_centers_[:, 1], s=300, c='red')\n    plt.show()\n\n    return model\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \npandas, numpy, sklearn, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nimport matplotlib.pyplot as plt\n\n# Constants\nFEATURES = ['Feature1', 'Feature2', 'Feature3', 'Feature4', 'Feature5']\nTARGET = 'Target'\n\ndef perform_random_forest_classification(df):\n    \"\"\"\n    Perform Random Forest classification on a DataFrame and plot the importance of \n    each feature.\n    \n    Parameters:\n    df (DataFrame): The input DataFrame.\n    \n    Returns:\n    RandomForestClassifier: The trained model.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.ensemble.RandomForestClassifier\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = pd.DataFrame(np.random.rand(50, 6), columns=FEATURES + [TARGET])\n    >>> model = perform_random_forest_classification(df)\n    >>> print(model.feature_importances_)\n    \"\"\"\n    X = df[FEATURES]\n    y = df[TARGET]\n\n    model = RandomForestClassifier().fit(X, y)\n\n    plt.figure(figsize=(10, 8))\n    plt.barh(FEATURES, model.feature_importances_)\n    plt.show()\n\n    return model\n```\n"}
{"seed": "{\"task_id\": 35683, \"prompt\": \"def f_35683(z):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"z.real\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(1.23-0j) == 1.23\\n\", \"\\n    assert candidate(1.23+0j) == 1.23\\n\", \"\\n    assert candidate(0.0-1j) == 0.0\\n\"], \"entry_point\": \"f_35683\", \"intent\": \"\\u8907\\u7d20\\u6570`z`\\u306e\\u5b9f\\u6570\\u90e8\\u306e\\u307f\\u3092\\u5f97\\u308b\", \"library\": []}", "generation": "Scenario 1:\nnumpy, math, cmath, matplotlib, scipy\n```python\nimport numpy as np\nimport math\nimport cmath\nimport matplotlib.pyplot as plt\nfrom scipy import signal\n\n# Constants\nAMPLITUDE = 1\nFREQUENCY = 5\nTIME = np.linspace(0, 1, 500, endpoint=False)\n\ndef generate_complex_wave(amplitude, frequency, time):\n    \"\"\"\n    Generate a complex wave with given amplitude and frequency, and plot its real and imaginary parts.\n\n    Parameters:\n    amplitude (float): The amplitude of the complex wave.\n    frequency (float): The frequency of the complex wave.\n    time (numpy.ndarray): The time points to generate the wave.\n\n    Returns:\n    numpy.ndarray: The generated complex wave.\n\n    Requirements:\n    - numpy\n    - math\n    - cmath\n    - matplotlib.pyplot\n    - scipy.signal\n\n    Example:\n    >>> wave = generate_complex_wave(AMPLITUDE, FREQUENCY, TIME)\n    >>> print(wave)\n    >>> plt.plot(TIME, wave.real)\n    >>> plt.plot(TIME, wave.imag)\n    \"\"\"\n    wave = amplitude * np.exp(1j * 2 * math.pi * frequency * time)\n\n    return wave\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, scipy.optimize, matplotlib\n```python\nimport numpy as np\nfrom scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\n\n# Constants\nX_DATA = np.linspace(0, 4, 50)\nNOISE = 0.1 * np.random.normal(size=X_DATA.size)\n\ndef fit_complex_curve(x_data, noise):\n    \"\"\"\n    Fit a complex curve to the given noisy data and plot the original and fitted data.\n\n    Parameters:\n    x_data (numpy.ndarray): The x coordinates of the data.\n    noise (numpy.ndarray): The noise added to the y coordinates of the data.\n\n    Returns:\n    tuple: The optimized parameters of the curve.\n\n    Requirements:\n    - numpy\n    - scipy.optimize.curve_fit\n    - matplotlib.pyplot\n\n    Example:\n    >>> params = fit_complex_curve(X_DATA, NOISE)\n    >>> print(params)\n    >>> plt.plot(X_DATA, params[0]*np.exp(-params[1]*X_DATA) + params[2])\n    \"\"\"\n    y_data = 2.5 * np.exp(-1.3 * x_data) + 0.5 + noise\n    guess = [3, 1, 0.5]\n    params, _ = curve_fit(lambda t, a, b, c: a*np.exp(-b*t) + c, x_data, y_data, p0=guess)\n\n    return params\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, scipy, matplotlib\n```python\nimport numpy as np\nfrom scipy.fft import fft\nimport matplotlib.pyplot as plt\n\n# Constants\nTIME = np.linspace(0, 10, 2000)\nAMPLITUDE = np.array([2, 6, 10])\nFREQUENCY = np.array([3, 5, 10])\n\ndef compute_fft(time, amplitude, frequency):\n    \"\"\"\n    Compute the Fast Fourier Transform (FFT) of a complex signal generated by multiple sine waves with different \n    frequencies and amplitudes, and plot the FFT spectrum.\n\n    Parameters:\n    time (numpy.ndarray): The time points to generate the wave.\n    amplitude (numpy.ndarray): The amplitudes of the sine waves.\n    frequency (numpy.ndarray): The frequencies of the sine waves.\n\n    Returns:\n    numpy.ndarray: The FFT spectrum.\n\n    Requirements:\n    - numpy\n    - scipy.fft\n    - matplotlib.pyplot\n\n    Example:\n    >>> spectrum = compute_fft(TIME, AMPLITUDE, FREQUENCY)\n    >>> print(spectrum)\n    >>> plt.plot(np.abs(spectrum))\n    \"\"\"\n    signal = amplitude.dot(np.sin(2 * np.pi * frequency.reshape(-1, 1) * time))\n    spectrum = fft(signal)\n\n    return spectrum\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, matplotlib, cmath\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cmath\n\n# Constants\nX = np.linspace(-2, 2, 400)\nY = np.linspace(-2, 2, 400)\n\ndef plot_complex_function(x, y):\n    \"\"\"\n    Plot the phase of a complex function over a range of x and y.\n\n    Parameters:\n    x (numpy.ndarray): The range of x values.\n    y (numpy.ndarray): The range of y values.\n\n    Returns:\n    None.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - cmath\n\n    Example:\n    >>> plot_complex_function(X, Y)\n    \"\"\"\n    Z = np.zeros((len(y), len(x)), dtype=float)\n    for i in range(len(y)):\n        for j in range(len(x)):\n            z = complex(x[j], y[i])\n            Z[i, j] = cmath.phase(z**2 - 1)\n\n    plt.imshow(Z, extent=(np.amin(x), np.amax(x), np.amin(y), np.amax(y)), origin='lower', cmap='hsv')\n    plt.colorbar(label=\"Phase (radians)\")\n    plt.grid()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, scipy, matplotlib, cmath\n```python\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nimport cmath\n\n# Constants\nX = np.linspace(-10, 10, 1000)\n\ndef plot_complex_distribution(x):\n    \"\"\"\n    Plot a complex distribution generated from a real Gaussian distribution.\n\n    Parameters:\n    x (numpy.ndarray): The range of x values.\n\n    Returns:\n    None.\n\n    Requirements:\n    - numpy\n    - scipy.stats.norm\n    - matplotlib.pyplot\n    - cmath\n\n    Example:\n    >>> plot_complex_distribution(X)\n    \"\"\"\n    real_part = norm.pdf(x, 0, 1)\n    imag_part = norm.pdf(x, 2, 2)\n    complex_dist = real_part + 1j * imag_part\n\n    plt.plot(x, complex_dist.real, label='Real part')\n    plt.plot(x, complex_dist.imag, label='Imaginary part')\n    plt.legend()\n    plt.grid()\n    plt.show()\n```"}
{"seed": "{\"task_id\": 41058, \"prompt\": \"def f_41058(iter, r):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"list(itertools.combinations(iter, r))\", \"test_start\": \"\\nimport itertools \\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([1,2,3], 2) == [(1, 2), (1, 3), (2, 3)]\\n\", \"\\n    assert candidate([1], 2) == []\\n\", \"\\n    assert candidate([1], 1) == [(1, )]\\n\"], \"entry_point\": \"f_41058\", \"intent\": \"\\u30a4\\u30c6\\u30e9\\u30d6\\u30eb\\u30aa\\u30d6\\u30b8\\u30a7\\u30af\\u30c8`iter`\\u306e`r`\\u500b\\u306e\\u8981\\u7d20\\u306e\\u7d44\\u307f\\u5408\\u308f\\u305b\\u3092\\u30ea\\u30b9\\u30c8\\u3068\\u3057\\u3066\\u5f97\\u308b\", \"library\": [\"itertools\"]}", "generation": "Scenario 1:\nitertools, math, random, re\n```python\nimport itertools\nimport math\nimport random\nimport re\n\n# Constants\nLETTERS = 'abcdefghijklmnopqrstuvwxyz'\n\ndef generate_random_strings(n, r):\n    \"\"\"\n    Generate a list of n random strings, each of length r, using letters from the alphabet.\n    Then, generate all possible r-combinations of these strings.\n\n    Parameters:\n    n (int): The number of random strings to generate.\n    r (int): The length of each random string.\n\n    Returns:\n    list: The list of all possible r-combinations of the generated strings.\n\n    Requirements:\n    - itertools\n    - math\n    - random\n    - re\n\n    Example:\n    >>> generate_random_strings(5, 3)\n    \"\"\"\n    strings = [''.join(random.choices(LETTERS, k=r)) for _ in range(n)]\n    combinations = list(itertools.combinations(strings, r))\n\n    return combinations\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nitertools, numpy, scipy.stats\n```python\nimport itertools\nimport numpy as np\nfrom scipy import stats\n\ndef calculate_combination_means(arr, r):\n    \"\"\"\n    Given an array of numbers, generate all r-combinations of the elements and calculate \n    their means. Return a list of the calculated means and their frequency.\n\n    Parameters:\n    arr (list): The list of numbers.\n    r (int): The number of elements in each combination.\n\n    Returns:\n    list: The list of means and their frequency.\n\n    Requirements:\n    - itertools\n    - numpy\n    - scipy.stats\n\n    Example:\n    >>> calculate_combination_means([1, 2, 3, 4], 2)\n    \"\"\"\n    combinations = list(itertools.combinations(arr, r))\n    means = [np.mean(comb) for comb in combinations]\n    freq = stats.itemfreq(means)\n\n    return freq.tolist()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nitertools, random, sys\n```python\nimport itertools\nimport random\nimport sys\n\ndef random_combinations(iter, r):\n    \"\"\"\n    Generate a list of all r-combinations of the elements in iter, randomly shuffle the list,\n    and return the first n combinations, where n is the machine's word length in bytes.\n\n    Parameters:\n    iter (list): The list of elements.\n    r (int): The number of elements in each combination.\n\n    Returns:\n    list: The first n combinations after shuffling.\n\n    Requirements:\n    - itertools\n    - random\n    - sys\n\n    Example:\n    >>> random_combinations([1, 2, 3, 4, 5], 2)\n    \"\"\"\n    combinations = list(itertools.combinations(iter, r))\n    random.shuffle(combinations)\n\n    return combinations[:sys.int_info.sizeof_digit]\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nitertools, pandas, sklearn.preprocessing\n```python\nimport itertools\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\ndef encode_combinations(iter, r):\n    \"\"\"\n    Generate all r-combinations of the elements in iter, encode each combination as a unique\n    integer using sklearn's LabelEncoder, and return a DataFrame containing each combination\n    and its corresponding encoding.\n\n    Parameters:\n    iter (list): The list of elements.\n    r (int): The number of elements in each combination.\n\n    Returns:\n    DataFrame: A pandas DataFrame containing each combination and its corresponding encoding.\n\n    Requirements:\n    - itertools\n    - pandas\n    - sklearn.preprocessing\n\n    Example:\n    >>> encode_combinations(['a', 'b', 'c', 'd', 'e'], 3)\n    \"\"\"\n    combinations = list(itertools.combinations(iter, r))\n    le = LabelEncoder()\n    encodings = le.fit_transform([''.join(comb) for comb in combinations])\n    df = pd.DataFrame({'Combination': combinations, 'Encoding': encodings})\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nitertools, numpy, matplotlib\n```python\nimport itertools\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_combination_lengths(iter, r):\n    \"\"\"\n    Generate all r-combinations of the elements in iter, calculate the length of each\n    combination (assuming the elements are vectors), and plot a histogram of the lengths.\n\n    Parameters:\n    iter (list): The list of elements (vectors).\n    r (int): The number of elements in each combination.\n\n    Returns:\n    None\n\n    Requirements:\n    - itertools\n    - numpy\n    - matplotlib\n\n    Example:\n    >>> plot_combination_lengths([[1, 0], [0, 1], [1, 1], [-1, 1], [-1, -1]], 2)\n    \"\"\"\n    combinations = list(itertools.combinations(iter, r))\n    lengths = [np.linalg.norm(np.sum(comb, axis=0)) for comb in combinations]\n    plt.hist(lengths, bins=10)\n    plt.show()\n```"}
{"seed": "{\"task_id\": 42573, \"prompt\": \"def f_42573(sheet, row, col):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"sheet.cell_value(row, col)\", \"test_start\": \"\\nimport xlrd\\nfrom xlwt import Workbook\\n\\ndef check(candidate):\", \"test\": [\"\\n    file_location = \\\"test.xlsx\\\"\\n\\n    book = Workbook()\\n    sheet1 = book.add_sheet('Sheet 1')\\n    sheet1.write(0, 0, 'A1')\\n    sheet1.write(0, 1, 'B1')\\n    sheet1.write(8, 5, \\\"Hello, world!\\\")\\n    book.save(file_location)\\n\\n    workbook = xlrd.open_workbook(file_location)\\n    sheet = workbook.sheet_by_index(0)\\n    assert candidate(sheet, 0, 0) == \\\"A1\\\"\\n    assert candidate(sheet, 0, 1) == \\\"B1\\\"\\n    assert candidate(sheet, 8, 5) == \\\"Hello, world!\\\"\\n\"], \"entry_point\": \"f_42573\", \"intent\": \"Excel\\u30b7\\u30fc\\u30c8\\u30aa\\u30d6\\u30b8\\u30a7\\u30af\\u30c8`sheet`\\u5185\\u306e\\u884c`row`\\u3001\\u5217`col`\\u306e\\u30bb\\u30eb\\u306e\\u5024\\u3092\\u5f97\\u308b\", \"library\": [\"xlrd\", \"xlwt\"]}", "generation": "Scenario 1:\nxlrd, xlwt, pandas, numpy, matplotlib\n```python\nimport xlrd\nfrom xlwt import Workbook\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nFILE_LOCATION = \"test.xlsx\"\n\n# Sample Data in comments\n# A1: 10, B1: 20, A2: 30, B2: 40\n\ndef analyze_excel_data(sheet_name):\n    \"\"\"\n    Load data from an Excel sheet, calculate the mean and standard deviation of each column, and plot a bar graph.\n\n    Parameters:\n    sheet_name (str): The name of the sheet to load data from.\n\n    Returns:\n    dict: A dictionary with mean and standard deviation of each column.\n\n    Requirements:\n    - xlrd\n    - xlwt\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> analyze_excel_data('Sheet 1')\n    \"\"\"\n    workbook = xlrd.open_workbook(FILE_LOCATION)\n    sheet = workbook.sheet_by_name(sheet_name)\n\n    data = [sheet.row_values(i) for i in range(sheet.nrows)]\n    df = pd.DataFrame(data[1:], columns=data[0])\n\n    result = {}\n    for column in df.columns:\n        mean = np.mean(df[column])\n        std = np.std(df[column])\n        result[column] = {\"mean\": mean, \"std\": std}\n\n        plt.bar(column, mean, yerr=std)\n        plt.title('Mean and Standard Deviation')\n        plt.xlabel('Columns')\n        plt.ylabel('Values')\n        plt.show()\n\n    return result\n\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nxlrd, xlwt, csv, numpy, pandas\n```python\nimport xlrd\nfrom xlwt import Workbook\nimport csv\nimport numpy as np\nimport pandas as pd\n\n# Constants\nEXCEL_FILE_LOCATION = \"test.xlsx\"\nCSV_FILE_LOCATION = \"test.csv\"\n\n# Sample Data in comments\n# A1: 10, B1: 20, A2: 30, B2: 40\n\ndef convert_excel_to_csv(sheet_name):\n    \"\"\"\n    Load data from an Excel sheet and convert it into a CSV file. Then calculate the sum of \n    each column in the CSV file and return it.\n\n    Parameters:\n    sheet_name (str): The name of the sheet to load data from.\n\n    Returns:\n    dict: A dictionary with the sum of each column.\n\n    Requirements:\n    - xlrd\n    - xlwt\n    - csv\n    - numpy\n    - pandas\n\n    Example:\n    >>> convert_excel_to_csv('Sheet 1')\n    \"\"\"\n    workbook = xlrd.open_workbook(EXCEL_FILE_LOCATION)\n    sheet = workbook.sheet_by_name(sheet_name)\n\n    with open(CSV_FILE_LOCATION, 'w', newline='') as file:\n        writer = csv.writer(file)\n        for row_num in range(sheet.nrows):\n            writer.writerow(sheet.row_values(row_num))\n\n    df = pd.read_csv(CSV_FILE_LOCATION)\n    column_sum = df.sum()\n\n    return column_sum.to_dict()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nxlrd, xlwt, json, pandas, os\n```python\nimport xlrd\nfrom xlwt import Workbook\nimport json\nimport pandas as pd\nimport os\n\n# Constants\nEXCEL_FILE_LOCATION = \"test.xlsx\"\nJSON_FILE_LOCATION = \"test.json\"\n\n# Sample Data in comments\n# A1: 10, B1: 20, A2: 30, B2: 40\n\ndef convert_excel_to_json(sheet_name):\n    \"\"\"\n    Load data from an Excel sheet and convert it into a JSON file. Then read the JSON file \n    and return its content.\n\n    Parameters:\n    sheet_name (str): The name of the sheet to load data from.\n\n    Returns:\n    list: A list of dictionaries representing the content of the JSON file.\n\n    Requirements:\n    - xlrd\n    - xlwt\n    - json\n    - pandas\n    - os\n\n    Example:\n    >>> convert_excel_to_json('Sheet 1')\n    \"\"\"\n    workbook = xlrd.open_workbook(EXCEL_FILE_LOCATION)\n    sheet = workbook.sheet_by_name(sheet_name)\n\n    data = [sheet.row_values(i) for i in range(sheet.nrows)]\n    df = pd.DataFrame(data[1:], columns=data[0])\n\n    df.to_json(JSON_FILE_LOCATION, orient='records')\n\n    with open(JSON_FILE_LOCATION) as file:\n        json_data = json.load(file)\n\n    return json_data\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nxlrd, xlwt, openpyxl, pandas, os\n```python\nimport xlrd\nfrom xlwt import Workbook\nfrom openpyxl import load_workbook\nimport pandas as pd\nimport os\n\n# Constants\nEXCEL_FILE_LOCATION = \"test.xlsx\"\nNEW_EXCEL_FILE_LOCATION = \"new_test.xlsx\"\n\n# Sample Data in comments\n# A1: 10, B1: 20, A2: 30, B2: 40\n\ndef copy_excel_data(sheet_name):\n    \"\"\"\n    Load data from an Excel sheet and copy it into a new Excel file. Then read the new Excel \n    file and return its content.\n\n    Parameters:\n    sheet_name (str): The name of the sheet to load data from.\n\n    Returns:\n    DataFrame: A pandas DataFrame representing the content of the new Excel file.\n\n    Requirements:\n    - xlrd\n    - xlwt\n    - openpyxl\n    - pandas\n    - os\n\n    Example:\n    >>> copy_excel_data('Sheet 1')\n    \"\"\"\n    workbook = xlrd.open_workbook(EXCEL_FILE_LOCATION)\n    sheet = workbook.sheet_by_name(sheet_name)\n\n    new_workbook = Workbook()\n    new_sheet = new_workbook.add_sheet(sheet_name)\n\n    for row_num in range(sheet.nrows):\n        row_values = sheet.row_values(row_num)\n        for col_num, cell_value in enumerate(row_values):\n            new_sheet.write(row_num, col_num, cell_value)\n\n    new_workbook.save(NEW_EXCEL_FILE_LOCATION)\n\n    new_workbook = load_workbook(NEW_EXCEL_FILE_LOCATION)\n    new_sheet = new_workbook[sheet_name]\n    data = new_sheet.values\n    df = pd.DataFrame(data)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nxlrd, xlwt, pdfkit, pandas, os\n```python\nimport xlrd\nfrom xlwt import Workbook\nimport pdfkit\nimport pandas as pd\nimport os\n\n# Constants\nEXCEL_FILE_LOCATION = \"test.xlsx\"\nPDF_FILE_LOCATION = \"test.pdf\"\n\n# Sample Data in comments\n# A1: 10, B1: 20, A2: 30, B2: 40\n\ndef convert_excel_to_pdf(sheet_name):\n    \"\"\"\n    Load data from an Excel sheet and convert it into a PDF file using HTML.\n\n    Parameters:\n    sheet_name (str): The name of the sheet to load data from.\n\n    Returns:\n    str: The location of the PDF file.\n\n    Requirements:\n    - xlrd\n    - xlwt\n    - pdfkit\n    - pandas\n    - os\n\n    Example:\n    >>> convert_excel_to_pdf('Sheet 1')\n    \"\"\"\n    workbook = xlrd.open_workbook(EXCEL_FILE_LOCATION)\n    sheet = workbook.sheet_by_name(sheet_name)\n\n    data = [sheet.row_values(i) for i in range(sheet.nrows)]\n    df = pd.DataFrame(data[1:], columns=data[0])\n\n    html = df.to_html()\n\n    pdfkit.from_string(html, PDF_FILE_LOCATION)\n\n    return PDF_FILE_LOCATION\n```"}
{"seed": "{\"task_id\": 40361, \"prompt\": \"def f_40361(func, args):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"func(*args)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    def func1(x, y, z): return x + y + z \\n    assert candidate(func1, [1,2,3]) == 6\\n\", \"\\n    def func2(a): return 0.8\\n    assert candidate(func2, ['random']) == 0.8\\n\"], \"entry_point\": \"f_40361\", \"intent\": \"\\u5f15\\u6570`args`\\u3092\\u30a2\\u30f3\\u30d1\\u30c3\\u30af\\u3057\\u3066\\u95a2\\u6570`func`\\u306b\\u6e21\\u3059\", \"library\": []}", "generation": "Scenario 1: \nnumpy, scipy.optimize, matplotlib\n```python\nimport numpy as np\nfrom scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\n\ndef fit_curve(func, xdata, ydata, p0=None):\n    \"\"\"\n    Fit a curve to the given x and y data using the specified function and initial parameters.\n    \n    Parameters:\n    func (callable): The function to fit. It should take the independent variable as the first argument \n    and the parameters to fit as separate remaining arguments.\n    xdata (array_like or object): The independent variable where the data is measured.\n    ydata (array_like or object): The dependent data - nominally f(xdata, ...)\n    p0 (None, scalar, or N-length sequence, optional): Initial guess for the parameters. \n    If None, then the initial values will all be 1 (if the number of parameters for the function can be determined using introspection, otherwise a ValueError is raised).\n\n    Returns:\n    tuple: popt, pcov. popt: Optimal values for the parameters so that the sum of the squared residuals \n    of f(xdata, *popt) - ydata is minimized. pcov: The estimated covariance of popt.\n\n    Requirements:\n    - numpy\n    - scipy.optimize.curve_fit\n\n    Example:\n    >>> xdata = np.linspace(0, 4, 50)\n    >>> y = func(xdata, 2.5, 1.3, 0.5)\n    >>> np.random.seed(1729)\n    >>> y_noise = 0.2 * np.random.normal(size=xdata.size)\n    >>> ydata = y + y_noise\n    >>> popt, pcov = fit_curve(func, xdata, ydata)\n    \"\"\"\n    popt, pcov = curve_fit(func, xdata, ydata, p0)\n    return popt, pcov\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nmultiprocessing, itertools, math\n```python\nfrom multiprocessing import Pool\nimport itertools\nimport math\n\ndef parallel_factorial(numbers):\n    \"\"\"\n    Calculate factorials for a list of numbers in parallel using multiprocessing.\n    \n    Parameters:\n    numbers (list): List of numbers to calculate factorials.\n    \n    Returns:\n    dict: A dictionary with numbers as keys and their factorial as values.\n    \n    Requirements:\n    - multiprocessing.Pool\n    - itertools.starmap\n    - math.factorial\n\n    Example:\n    >>> numbers = [5, 6, 7, 8, 9]\n    >>> factorials = parallel_factorial(numbers)\n    >>> print(factorials)\n    {5: 120, 6: 720, 7: 5040, 8: 40320, 9: 362880}\n    \"\"\"\n    with Pool() as pool:\n        factorial_dict = dict(pool.starmap((lambda x: (x, math.factorial(x))), [(i,) for i in numbers]))\n    return factorial_dict\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, sklearn.preprocessing, pandas\n```python\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\n\ndef scale_dataframe(df):\n    \"\"\"\n    Scale the numerical features in a pandas DataFrame using StandardScaler from sklearn.preprocessing.\n    \n    Parameters:\n    df (DataFrame): pandas DataFrame to be scaled.\n    \n    Returns:\n    DataFrame: Scaled pandas DataFrame.\n    \n    Requirements:\n    - numpy\n    - sklearn.preprocessing.StandardScaler\n    - pandas\n\n    Example:\n    >>> df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6], 'C': [7, 8, 9]})\n    >>> scaled_df = scale_dataframe(df)\n    \"\"\"\n    scaler = StandardScaler()\n    scaled_array = scaler.fit_transform(df.values)\n    scaled_df = pd.DataFrame(scaled_array, index=df.index, columns=df.columns)\n    return scaled_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, scipy.integrate, matplotlib\n```python\nimport numpy as np\nfrom scipy.integrate import quad\nimport matplotlib.pyplot as plt\n\ndef integrate_plot(func, a, b):\n    \"\"\"\n    Integrate a function over a given interval and plot the function and the area under the curve.\n    \n    Parameters:\n    func (callable): Function to integrate.\n    a (float): Lower limit of integration.\n    b (float): Upper limit of integration.\n    \n    Returns:\n    float: The integral of the function over the interval.\n    \n    Requirements:\n    - numpy\n    - scipy.integrate.quad\n    - matplotlib.pyplot\n\n    Example:\n    >>> integrate_plot(np.sin, 0, np.pi)\n    \"\"\"\n    integral, error = quad(func, a, b)\n    \n    x = np.linspace(a, b, 1000)\n    y = func(x)\n    plt.plot(x, y, 'r', linewidth=2)\n    plt.fill_between(x, y, color='silver')\n    plt.ylim(bottom=0)\n    plt.show()\n    \n    return integral\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, scipy.optimize, matplotlib\n```python\nimport numpy as np\nfrom scipy.optimize import minimize\nimport matplotlib.pyplot as plt\n\ndef find_min_plot(func, x0):\n    \"\"\"\n    Find the minimum of a function using scipy.optimize.minimize and plot the function and the minimum point.\n    \n    Parameters:\n    func (callable): The objective function to be minimized.\n    x0 (ndarray): Initial guess.\n    \n    Returns:\n    float: The function's value at the minimum point.\n    \n    Requirements:\n    - numpy\n    - scipy.optimize.minimize\n    - matplotlib.pyplot\n\n    Example:\n    >>> find_min_plot(np.square, np.array([2.0]))\n    \"\"\"\n    res = minimize(func, x0)\n    \n    x = np.linspace(-3, 3, 1000)\n    y = func(x)\n    plt.plot(x, y, 'r', linewidth=2)\n    plt.plot(res.x, res.fun, 'bo')\n    plt.show()\n    \n    return res.fun\n```"}
{"seed": "{\"task_id\": 43333, \"prompt\": \"def f_43333(r, l):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"pd.DataFrame(data={'range': r, 'result': l})\", \"test_start\": \"\\nimport pandas as pd\\n\\ndef check(candidate):\", \"test\": [\"\\n    r, l = [1,2,3], [4,5,6]\\n    assert candidate(r, l).equals(pd.DataFrame(data={'range': r, 'result': l}))\\n\"], \"entry_point\": \"f_43333\", \"intent\": \"\\u5217\\u540d`range`\\u306e\\u8981\\u7d20\\u3092\\u30ea\\u30b9\\u30c8`r`\\u3001\\u5217\\u540d`result`\\u306e\\u8981\\u7d20\\u3092\\u30ea\\u30b9\\u30c8`l`\\u3068\\u3057\\u3066\\u30c7\\u30fc\\u30bf\\u30d5\\u30ec\\u30fc\\u30e0\\u3092\\u4f5c\\u308b\", \"library\": [\"pandas\"]}", "generation": "Scenario 1: \npandas, random, matplotlib, numpy, datetime\n```python\nimport pandas as pd\nimport random\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom datetime import datetime, timedelta\n\n# Constants\nCATEGORIES = ['Electronics', 'Clothing', 'Groceries', 'Books', 'Sports Equipment']\nSALES_RANGE = range(1, 101)\n\ndef generate_sales_report(start_date, days):\n    \"\"\"\n    Generate a sales report for different categories over a certain number of days\n    starting from a given date.\n\n    Parameters:\n    start_date (datetime): The starting date of the report.\n    days (int): The number of days for which the report is to be generated.\n\n    Returns:\n    DataFrame: A pandas DataFrame with sales data for different categories.\n\n    Requirements:\n    - pandas\n    - random\n    - matplotlib.pyplot\n    - numpy\n    - datetime\n\n    Example:\n    >>> start_date = datetime(2022, 1, 1)\n    >>> report = generate_sales_report(start_date, 30)\n    >>> print(report)\n    >>> report.groupby('Category')['Sales'].sum().plot(kind='bar')\n    \"\"\"\n    report_data = []\n\n    for day in range(days):\n        for category in CATEGORIES:\n            sales = random.choice(SALES_RANGE)\n            report_data.append([start_date + timedelta(days=day), category, sales])\n\n    report_df = pd.DataFrame(report_data, columns=['Date', 'Category', 'Sales'])\n\n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, matplotlib, sklearn.preprocessing\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Constants\nSTUDENTS = ['John', 'Sarah', 'Ali', 'Emma', 'Mike']\nSUBJECTS = ['Math', 'Science', 'English', 'History', 'Geography']\n\ndef generate_normalized_scores():\n    \"\"\"\n    Generate a DataFrame of students' scores in various subjects and normalize \n    the scores using MinMaxScaler from sklearn.preprocessing.\n\n    Returns:\n    DataFrame: A pandas DataFrame with normalized scores.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.preprocessing\n\n    Example:\n    >>> scores = generate_normalized_scores()\n    >>> print(scores)\n    >>> scores.plot(kind='bar')\n    \"\"\"\n    scores = np.random.randint(50, 100, (len(STUDENTS), len(SUBJECTS)))\n    scores_df = pd.DataFrame(scores, columns=SUBJECTS, index=STUDENTS)\n\n    scaler = MinMaxScaler()\n    normalized_scores = scaler.fit_transform(scores_df)\n    normalized_scores_df = pd.DataFrame(normalized_scores, columns=SUBJECTS, index=STUDENTS)\n\n    return normalized_scores_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, random, matplotlib, numpy, datetime\n```python\nimport pandas as pd\nimport random\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom datetime import datetime, timedelta\n\n# Constants\nPRODUCTS = ['Product A', 'Product B', 'Product C', 'Product D', 'Product E']\nQUANTITY_RANGE = range(1, 21)\n\ndef generate_inventory_report(start_date, days):\n    \"\"\"\n    Generate an inventory report for different products over a certain number of days\n    starting from a given date.\n\n    Parameters:\n    start_date (datetime): The starting date of the report.\n    days (int): The number of days for which the report is to be generated.\n\n    Returns:\n    DataFrame: A pandas DataFrame with inventory data for different products.\n\n    Requirements:\n    - pandas\n    - random\n    - matplotlib.pyplot\n    - numpy\n    - datetime\n\n    Example:\n    >>> start_date = datetime(2022, 1, 1)\n    >>> report = generate_inventory_report(start_date, 30)\n    >>> print(report)\n    >>> report.groupby('Product')['Quantity'].sum().plot(kind='bar')\n    \"\"\"\n    report_data = []\n\n    for day in range(days):\n        for product in PRODUCTS:\n            quantity = random.choice(QUANTITY_RANGE)\n            report_data.append([start_date + timedelta(days=day), product, quantity])\n\n    report_df = pd.DataFrame(report_data, columns=['Date', 'Product', 'Quantity'])\n\n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, random, matplotlib, numpy, datetime\n```python\nimport pandas as pd\nimport random\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom datetime import datetime, timedelta\n\n# Constants\nCUSTOMERS = ['Customer A', 'Customer B', 'Customer C', 'Customer D', 'Customer E']\nTRANSACTION_AMOUNT_RANGE = range(100, 1001)\n\ndef generate_transaction_report(start_date, days):\n    \"\"\"\n    Generate a transaction report for different customers over a certain number of days\n    starting from a given date.\n\n    Parameters:\n    start_date (datetime): The starting date of the report.\n    days (int): The number of days for which the report is to be generated.\n\n    Returns:\n    DataFrame: A pandas DataFrame with transaction data for different customers.\n\n    Requirements:\n    - pandas\n    - random\n    - matplotlib.pyplot\n    - numpy\n    - datetime\n\n    Example:\n    >>> start_date = datetime(2022, 1, 1)\n    >>> report = generate_transaction_report(start_date, 30)\n    >>> print(report)\n    >>> report.groupby('Customer')['Transaction Amount'].sum().plot(kind='bar')\n    \"\"\"\n    report_data = []\n\n    for day in range(days):\n        for customer in CUSTOMERS:\n            transaction_amount = random.choice(TRANSACTION_AMOUNT_RANGE)\n            report_data.append([start_date + timedelta(days=day), customer, transaction_amount])\n\n    report_df = pd.DataFrame(report_data, columns=['Date', 'Customer', 'Transaction Amount'])\n\n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, random, matplotlib, numpy, datetime\n```python\nimport pandas as pd\nimport random\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom datetime import datetime, timedelta\n\n# Constants\nEMPLOYEES = ['Employee A', 'Employee B', 'Employee C', 'Employee D', 'Employee E']\nHOURS_WORKED_RANGE = range(1, 9)\n\ndef generate_work_hours_report(start_date, days):\n    \"\"\"\n    Generate a work hours report for different employees over a certain number of days\n    starting from a given date.\n\n    Parameters:\n    start_date (datetime): The starting date of the report.\n    days (int): The number of days for which the report is to be generated.\n\n    Returns:\n    DataFrame: A pandas DataFrame with work hours data for different employees.\n\n    Requirements:\n    - pandas\n    - random\n    - matplotlib.pyplot\n    - numpy\n    - datetime\n\n    Example:\n    >>> start_date = datetime(2022, 1, 1)\n    >>> report = generate_work_hours_report(start_date, 30)\n    >>> print(report)\n    >>> report.groupby('Employee')['Hours Worked'].sum().plot(kind='bar')\n    \"\"\"\n    report_data = []\n\n    for day in range(days):\n        for employee in EMPLOYEES:\n            hours_worked = random.choice(HOURS_WORKED_RANGE)\n            report_data.append([start_date + timedelta(days=day), employee, hours_worked])\n\n    report_df = pd.DataFrame(report_data, columns=['Date', 'Employee', 'Hours Worked'])\n\n    return report_df\n```\n`GPT_ODEX_BREAK`"}
{"seed": "{\"task_id\": 11582, \"prompt\": \"def f_11582():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"open('C:\\\\\\\\Users\\\\\\\\Documents\\\\\\\\python programs', 'r', encoding='utf-8')\", \"test_start\": \"\\nimport builtins\\nfrom unittest.mock import Mock\\n\\ndef check(candidate):\", \"test\": [\"\\n    with open('a.txt', 'w') as f:\\n        f.write('t')\\n    f1 = open('a.txt')\\n    builtins.open = Mock(return_value = f1)\\n    assert candidate() == f1\\n\"], \"entry_point\": \"f_11582\", \"intent\": \"\\u30d5\\u30a1\\u30a4\\u30eb`C:\\\\Users\\\\Documents\\\\python programs`\\u3092\\u958b\\u304f\", \"library\": [\"builtins\"]}", "generation": "Scenario 1:\nos, csv, collections, numpy, matplotlib\n```python\nimport os\nimport csv\nfrom collections import Counter\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nDIRECTORY_PATH = 'C:\\\\Users\\\\Documents\\\\python programs'\nFILE_EXTENSION = '.py'\n\ndef analyze_python_files():\n    \"\"\"\n    Open a directory, read all Python (*.py) files, count the frequency of function names, \n    and plot the top 10 most frequently used function names in a bar chart.\n    \n    Returns:\n    dict: A dictionary with function names and their frequency.\n    \n    Requirements:\n    - os\n    - csv\n    - collections.Counter\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> freqs = analyze_python_files()\n    >>> plt.bar(*zip(*freqs.items()))\n    \"\"\"\n    function_freqs = Counter()\n    \n    for filename in os.listdir(DIRECTORY_PATH):\n        if filename.endswith(FILE_EXTENSION):\n            with open(os.path.join(DIRECTORY_PATH, filename), 'r', encoding='utf-8') as file:\n                reader = csv.reader(file)\n                for row in reader:\n                    if row[0].startswith('def '):\n                        function_name = row[0].split()[1].split('(')[0]\n                        function_freqs[function_name] += 1\n\n    top_10_freqs = dict(function_freqs.most_common(10))\n    \n    return top_10_freqs\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, json, datetime, time, collections\n```python\nimport os\nimport json\nfrom datetime import datetime\nimport time\nfrom collections import defaultdict\n\n# Constants\nDIRECTORY_PATH = 'C:\\\\Users\\\\Documents\\\\python programs'\nFILE_EXTENSION = '.json'\n\ndef analyze_json_files():\n    \"\"\"\n    Open a directory, read all JSON files, group the JSON objects based on \n    their creation date (yyyy-mm-dd), and return the count of JSON objects per date.\n\n    Returns:\n    dict: A dictionary with dates and the count of JSON objects.\n\n    Requirements:\n    - os\n    - json\n    - datetime\n    - time\n    - collections.defaultdict\n\n    Example:\n    >>> counts_per_date = analyze_json_files()\n    \"\"\"\n    counts_per_date = defaultdict(int)\n    \n    for filename in os.listdir(DIRECTORY_PATH):\n        if filename.endswith(FILE_EXTENSION):\n            with open(os.path.join(DIRECTORY_PATH, filename), 'r', encoding='utf-8') as file:\n                json_obj = json.load(file)\n                creation_date = datetime.fromtimestamp(json_obj['creation_time']).strftime('%Y-%m-%d')\n                counts_per_date[creation_date] += 1\n\n    return dict(counts_per_date)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, xml.etree.ElementTree, pandas, collections\n```python\nimport os\nfrom xml.etree import ElementTree as ET\nimport pandas as pd\nfrom collections import Counter\n\n# Constants\nDIRECTORY_PATH = 'C:\\\\Users\\\\Documents\\\\python programs'\nFILE_EXTENSION = '.xml'\n\ndef analyze_xml_files():\n    \"\"\"\n    Open a directory, read all XML files, extract the text of a specific element, \n    count the frequency of words, and return a DataFrame with words and their frequencies.\n\n    Returns:\n    DataFrame: A pandas DataFrame with words and their frequencies.\n\n    Requirements:\n    - os\n    - xml.etree.ElementTree\n    - pandas\n    - collections.Counter\n    \n    Example:\n    >>> word_freq_df = analyze_xml_files()\n    \"\"\"\n    word_freqs = Counter()\n    \n    for filename in os.listdir(DIRECTORY_PATH):\n        if filename.endswith(FILE_EXTENSION):\n            tree = ET.parse(os.path.join(DIRECTORY_PATH, filename))\n            root = tree.getroot()\n            for elem in root.iter('text'):\n                words = elem.text.split()\n                word_freqs.update(words)\n\n    word_freq_df = pd.DataFrame.from_dict(word_freqs, orient='index', columns=['Frequency'])\n\n    return word_freq_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, re, collections, matplotlib\n```python\nimport os\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\n# Constants\nDIRECTORY_PATH = 'C:\\\\Users\\\\Documents\\\\python programs'\nFILE_EXTENSION = '.txt'\n\ndef analyze_text_files():\n    \"\"\"\n    Open a directory, read all text files, count the frequency of email addresses, \n    and plot a pie chart of the top 5 most frequent email domains.\n\n    Returns:\n    dict: A dictionary with email domains and their frequency.\n\n    Requirements:\n    - os\n    - re\n    - collections.Counter\n    - matplotlib.pyplot\n    \n    Example:\n    >>> email_freqs = analyze_text_files()\n    >>> plt.pie(*zip(*email_freqs.items()), autopct='%1.1f%%')\n    \"\"\"\n    email_freqs = Counter()\n    \n    for filename in os.listdir(DIRECTORY_PATH):\n        if filename.endswith(FILE_EXTENSION):\n            with open(os.path.join(DIRECTORY_PATH, filename), 'r', encoding='utf-8') as file:\n                for line in file:\n                    emails = re.findall(r\"[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+\", line)\n                    for email in emails:\n                        domain = email.split('@')[1]\n                        email_freqs[domain] += 1\n\n    top_5_freqs = dict(email_freqs.most_common(5))\n    \n    return top_5_freqs\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, pandas, csv, matplotlib\n```python\nimport os\nimport pandas as pd\nimport csv\nimport matplotlib.pyplot as plt\n\n# Constants\nDIRECTORY_PATH = 'C:\\\\Users\\\\Documents\\\\python programs'\nFILE_EXTENSION = '.csv'\n\ndef analyze_csv_files():\n    \"\"\"\n    Open a directory, read all CSV files, concatenate the data into a DataFrame, \n    group by a specific column, and plot a bar chart of the count per group.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the counts per group.\n\n    Requirements:\n    - os\n    - pandas\n    - csv\n    - matplotlib.pyplot\n    \n    Example:\n    >>> group_counts_df = analyze_csv_files()\n    >>> group_counts_df.plot(kind='bar')\n    \"\"\"\n    dfs = []\n\n    for filename in os.listdir(DIRECTORY_PATH):\n        if filename.endswith(FILE_EXTENSION):\n            df = pd.read_csv(os.path.join(DIRECTORY_PATH, filename))\n            dfs.append(df)\n\n    all_data_df = pd.concat(dfs)\n    group_counts_df = all_data_df['group'].value_counts().to_frame()\n\n    return group_counts_df\n```\n"}
{"seed": "{\"task_id\": 12174, \"prompt\": \"def f_12174():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"sys.path\", \"test_start\": \"\\nimport sys\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate() == sys.path\\n\"], \"entry_point\": \"f_12174\", \"intent\": \"PYTHONPATH\\u3092\\u8868\\u793a\\u3059\\u308b\", \"library\": [\"sys\"]}", "generation": "Scenario 1: \nos, sys, shutil, pandas, csv\n```python\nimport os\nimport sys\nimport shutil\nimport pandas as pd\nimport csv\n\n# Constants\nDEFAULT_PATH = '/tmp'\n\ndef transfer_file_to_default_path(file_path, file_name):\n    \"\"\"\n    Transfer a CSV file from the given path to a default path, and return the DataFrame of the CSV file.\n\n    Parameters:\n    file_path (str): The path of the CSV file.\n    file_name (str): The name of the CSV file.\n\n    Returns:\n    DataFrame: The DataFrame of the transferred CSV file.\n\n    Requirements:\n    - os\n    - sys\n    - shutil\n    - pandas\n    - csv\n\n    Example:\n    >>> transfer_file_to_default_path('/home/user/data', 'sample.csv')\n    \"\"\"\n    # Full path of the file\n    full_path = os.path.join(file_path, file_name)\n\n    # Check if the file exists\n    if not os.path.isfile(full_path):\n        print(f\"File does not exist: {full_path}\")\n        sys.exit(1)\n\n    # Transfer the file\n    shutil.copy(full_path, DEFAULT_PATH)\n\n    # Load the CSV file into a DataFrame\n    df = pd.read_csv(os.path.join(DEFAULT_PATH, file_name))\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, sys, shutil, zipfile, glob\n```python\nimport os\nimport sys\nimport shutil\nimport zipfile\nimport glob\n\n# Constants\nDEFAULT_PATH = '/tmp'\n\ndef unzip_file_to_default_path(zip_file_path):\n    \"\"\"\n    Unzip a ZIP file from the given path to a default path, and return the list of unzipped files.\n\n    Parameters:\n    zip_file_path (str): The path of the ZIP file.\n\n    Returns:\n    list: The list of unzipped files.\n\n    Requirements:\n    - os\n    - sys\n    - shutil\n    - zipfile\n    - glob\n\n    Example:\n    >>> unzip_file_to_default_path('/home/user/data/sample.zip')\n    \"\"\"\n    # Check if the file exists\n    if not os.path.isfile(zip_file_path):\n        print(f\"File does not exist: {zip_file_path}\")\n        sys.exit(1)\n\n    # Unzip the file\n    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n        zip_ref.extractall(DEFAULT_PATH)\n\n    # Get the list of unzipped files\n    unzipped_files = glob.glob(os.path.join(DEFAULT_PATH, '*'))\n\n    return unzipped_files\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, sys, pathlib, json, collections\n```python\nimport os\nimport sys\nimport pathlib\nimport json\nimport collections\n\n# Constants\nDEFAULT_PATH = '/tmp'\n\ndef count_file_types_in_directory(directory_path):\n    \"\"\"\n    Count the number of each file type in a given directory, and return the counts in a dictionary.\n\n    Parameters:\n    directory_path (str): The path of the directory.\n\n    Returns:\n    dict: The dictionary of file type counts.\n\n    Requirements:\n    - os\n    - sys\n    - pathlib\n    - json\n    - collections\n\n    Example:\n    >>> count_file_types_in_directory('/home/user/data')\n    \"\"\"\n    # Check if the directory exists\n    if not os.path.isdir(directory_path):\n        print(f\"Directory does not exist: {directory_path}\")\n        sys.exit(1)\n\n    # Get the list of files in the directory\n    files = os.listdir(directory_path)\n\n    # Count the file types\n    file_types = [pathlib.Path(f).suffix for f in files]\n    file_type_counts = collections.Counter(file_types)\n\n    return dict(file_type_counts)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, sys, subprocess, shlex\n```python\nimport os\nimport sys\nimport subprocess\nimport shlex\n\ndef run_command_and_return_output(command):\n    \"\"\"\n    Run a shell command and return its output.\n\n    Parameters:\n    command (str): The shell command.\n\n    Returns:\n    str: The output of the shell command.\n\n    Requirements:\n    - os\n    - sys\n    - subprocess\n    - shlex\n\n    Example:\n    >>> run_command_and_return_output('ls -l')\n    \"\"\"\n    # Parse the command\n    command = shlex.split(command)\n\n    # Run the command and capture the output\n    try:\n        output = subprocess.check_output(command)\n    except subprocess.CalledProcessError as e:\n        print(f\"Command '{command}' returned non-zero exit status {e.returncode}.\")\n        sys.exit(1)\n\n    return output.decode('utf-8')\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, sys, re, glob\n```python\nimport os\nimport sys\nimport re\nimport glob\n\n# Constants\nDEFAULT_PATH = '/tmp'\n\ndef find_files_matching_pattern(pattern, directory_path):\n    \"\"\"\n    Find files in a given directory that match a given pattern, and return their paths.\n\n    Parameters:\n    pattern (str): The regex pattern.\n    directory_path (str): The path of the directory.\n\n    Returns:\n    list: The list of file paths.\n\n    Requirements:\n    - os\n    - sys\n    - re\n    - glob\n\n    Example:\n    >>> find_files_matching_pattern(r'^sample', '/home/user/data')\n    \"\"\"\n    # Check if the directory exists\n    if not os.path.isdir(directory_path):\n        print(f\"Directory does not exist: {directory_path}\")\n        sys.exit(1)\n\n    # Compile the pattern\n    pattern = re.compile(pattern)\n\n    # Get the list of files in the directory\n    files = glob.glob(os.path.join(directory_path, '*'))\n\n    # Find files that match the pattern\n    matching_files = [f for f in files if pattern.match(os.path.basename(f))]\n\n    return matching_files\n```\n"}
{"seed": "{\"task_id\": 6225, \"prompt\": \"def f_6225():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"sum(1 for line in open('myfile.txt'))\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    with open('myfile.txt', 'w') as fw:\\n        for i in range(10): fw.write(f\\\"{i}\\\\n\\\")\\n    assert candidate() == 10\\n\", \"\\n    with open('myfile.txt', 'w') as fw:\\n        for i in range(88): fw.write(f\\\"{i}\\\\n\\\")\\n    assert candidate() == 88\\n\"], \"entry_point\": \"f_6225\", \"intent\": \"\\u30c6\\u30ad\\u30b9\\u30c8\\u30d5\\u30a1\\u30a4\\u30eb`myfile.txt`\\u306e\\u884c\\u6570\\u3092\\u53d6\\u5f97\\u3059\\u308b\", \"library\": []}", "generation": "Scenario 1: \nos, pathlib, shutil, random\n```python\nimport os\nfrom pathlib import Path\nimport shutil\nfrom random import randint\n\n# Constants\nDIRECTORY_NAME = 'test_directory'\nNUMBER_OF_FILES = 50\n\n# Sample files\nSAMPLE_FILES = [f'file_{i}.txt' for i in range(NUMBER_OF_FILES)]\n\ndef count_files_in_directory():\n    \"\"\"\n    Create a directory, generate a certain number of files in it, and count the number \n    of files in the directory.\n    \n    Returns:\n    int: The number of files in the directory.\n\n    Requirements:\n    - os\n    - pathlib\n    - shutil\n    - random\n    \n    Example:\n    >>> print(count_files_in_directory())\n    \"\"\"\n    Path(DIRECTORY_NAME).mkdir(parents=True, exist_ok=True)\n\n    for file in SAMPLE_FILES:\n        with open(os.path.join(DIRECTORY_NAME, file), 'w') as f:\n            f.write(str(randint(0, 100)))\n\n    files = len([name for name in os.listdir(DIRECTORY_NAME) if os.path.isfile(os.path.join(DIRECTORY_NAME, name))])\n\n    shutil.rmtree(DIRECTORY_NAME)\n\n    return files\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncsv, pandas, os\n```python\nimport csv\nimport pandas as pd\nimport os\n\n# Constants\nCSV_FILE_PATH = 'test_data.csv'\nCOLUMNS = ['Name', 'Age', 'Occupation']\n\n# Sample data\nDATA = [\n    ['John Doe', 30, 'Software Engineer'],\n    ['Jane Doe', 28, 'Data Scientist'],\n    ['Alice', 35, 'Product Manager'],\n    ['Bob', 40, 'Project Manager'],\n    ['Charlie', 32, 'UX Designer'],\n]\n\ndef write_and_read_csv():\n    \"\"\"\n    Write data to a CSV file and read the CSV file using pandas.\n    \n    Returns:\n    DataFrame: A pandas DataFrame obtained from the CSV file.\n\n    Requirements:\n    - csv\n    - pandas\n    - os\n    \n    Example:\n    >>> print(write_and_read_csv())\n    \"\"\"\n    with open(CSV_FILE_PATH, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(COLUMNS)\n        writer.writerows(DATA)\n\n    df = pd.read_csv(CSV_FILE_PATH)\n\n    os.remove(CSV_FILE_PATH)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nstatistics, random, numpy\n```python\nimport statistics\nfrom random import randint\nimport numpy as np\n\n# Constants\nNUMBER_OF_ELEMENTS = 100\nMIN_VALUE = 1\nMAX_VALUE = 100\n\ndef calculate_statistics():\n    \"\"\"\n    Generate a list of random integers and calculate some statistical values of \n    the list (mean, median, mode, variance, standard deviation).\n    \n    Returns:\n    dict: A dictionary of statistics of the list.\n\n    Requirements:\n    - statistics\n    - random\n    - numpy\n    \n    Example:\n    >>> print(calculate_statistics())\n    \"\"\"\n    data = [randint(MIN_VALUE, MAX_VALUE) for _ in range(NUMBER_OF_ELEMENTS)]\n    \n    stats = {\n        'Mean': statistics.mean(data),\n        'Median': statistics.median(data),\n        'Mode': statistics.mode(data),\n        'Variance': np.var(data),\n        'Standard Deviation': np.std(data)\n    }\n\n    return stats\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\njson, os, glob\n```python\nimport json\nimport os\nfrom glob import glob\n\n# Constants\nDIRECTORY_NAME = 'json_files'\nJSON_FILES = ['file1.json', 'file2.json', 'file3.json']\n\n# Sample data\nDATA = {\n    'Name': 'John Doe',\n    'Age': 30,\n    'Occupation': 'Software Engineer'\n}\n\ndef write_and_read_json_files():\n    \"\"\"\n    Write data to multiple JSON files, read all JSON files from a directory, \n    and return the combined data from all files.\n    \n    Returns:\n    list[dict]: A list of dictionaries obtained from the JSON files.\n\n    Requirements:\n    - json\n    - os\n    - glob\n    \n    Example:\n    >>> print(write_and_read_json_files())\n    \"\"\"\n    os.makedirs(DIRECTORY_NAME, exist_ok=True)\n\n    for file_name in JSON_FILES:\n        with open(os.path.join(DIRECTORY_NAME, file_name), 'w') as json_file:\n            json.dump(DATA, json_file)\n\n    data = []\n    for file_name in glob(os.path.join(DIRECTORY_NAME, '*.json')):\n        with open(file_name, 'r') as json_file:\n            data.append(json.load(json_file))\n\n    for file_name in glob(os.path.join(DIRECTORY_NAME, '*.json')):\n        os.remove(file_name)\n\n    return data\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, pandas, random\n```python\nimport numpy as np\nimport pandas as pd\nfrom random import randint\n\n# Constants\nNUMBER_OF_ROWS = 1000\nNUMBER_OF_COLUMNS = 50\nCOLUMN_NAMES = [f'col{i}' for i in range(NUMBER_OF_COLUMNS)]\n\ndef generate_dataframe():\n    \"\"\"\n    Generate a large DataFrame with random integers and calculate the sum of each column.\n    \n    Returns:\n    Series: A pandas Series with the sum of each column.\n\n    Requirements:\n    - numpy\n    - pandas\n    - random\n    \n    Example:\n    >>> print(generate_dataframe())\n    \"\"\"\n    data = np.random.randint(0, 100, size=(NUMBER_OF_ROWS, NUMBER_OF_COLUMNS))\n    df = pd.DataFrame(data, columns=COLUMN_NAMES)\n    \n    return df.sum()\n```"}
{"seed": "{\"task_id\": 6225, \"prompt\": \"def f_6225():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"len(open('myfile.txt').readlines())\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    with open('myfile.txt', 'w') as fw:\\n        for i in range(10): fw.write(f\\\"{i}\\\\n\\\")\\n    assert candidate() == 10\\n\", \"\\n    with open('myfile.txt', 'w') as fw:\\n        for i in range(88): fw.write(f\\\"{i}\\\\n\\\")\\n    assert candidate() == 88\\n\"], \"entry_point\": \"f_6225\", \"intent\": \"\\u30c6\\u30ad\\u30b9\\u30c8\\u30d5\\u30a1\\u30a4\\u30eb`myfile.txt`\\u306e\\u884c\\u6570\\u3092\\u53d6\\u5f97\\u3059\\u308b\", \"library\": []}", "generation": "Scenario 1:\ncsv, pandas, itertools, os\n```python\nimport csv\nimport pandas as pd\nimport itertools\nimport os\n\n# Constants\nFILE_PATH = 'myfile.txt'\n\ndef count_words_and_chars():\n    \"\"\"\n    Count the total words and characters in a text file excluding whitespaces and \n    new line characters. Also, count the frequency of each word in the file.\n\n    Requirements:\n    - csv\n    - itertools\n    - pandas\n    - os\n\n    Example:\n    >>> count_words_and_chars()\n    (10, 88, {'word1': 2, 'word2': 3, 'word3': 5})\n    \"\"\"\n    with open(FILE_PATH, 'r') as file:\n        lines = file.readlines()\n\n    total_words = total_chars = 0\n    word_freq = {}\n\n    for line in lines:\n        words = line.split()\n        total_words += len(words)\n        total_chars += sum(len(word) for word in words)\n\n        for word in words:\n            if word not in word_freq:\n                word_freq[word] = 0\n            word_freq[word] += 1\n\n    return total_words, total_chars, word_freq\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, collections, os, numpy\n```python\nimport pandas as pd\nimport collections\nimport os\nimport numpy as np\n\n# Constants\nFILE_PATH = 'myfile.txt'\n\ndef analyze_file():\n    \"\"\"\n    Analyze a text file and provide the following statistics:\n    - Number of lines\n    - Number of words\n    - Number of unique words\n    - Most frequent word\n    - Average word length\n\n    Requirements:\n    - pandas\n    - collections\n    - os\n    - numpy\n\n    Example:\n    >>> analyze_file()\n    (10, 88, 20, 'the', 4.2)\n    \"\"\"\n    with open(FILE_PATH, 'r') as file:\n        text = file.read()\n\n    lines = text.split('\\n')\n    words = text.split()\n\n    num_lines = len(lines)\n    num_words = len(words)\n    unique_words = len(set(words))\n\n    word_counts = collections.Counter(words)\n    most_common_word = word_counts.most_common(1)[0][0]\n\n    avg_word_length = np.mean([len(word) for word in words])\n\n    return num_lines, num_words, unique_words, most_common_word, avg_word_length\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nre, collections, pandas, os\n```python\nimport re\nimport collections\nimport pandas as pd\nimport os\n\n# Constants\nFILE_PATH = 'myfile.txt'\n\ndef count_word_frequency():\n    \"\"\"\n    Count the frequency of each word in a text file and return a dictionary where \n    the keys are the words and the values are their frequencies.\n\n    Requirements:\n    - re\n    - collections\n    - pandas\n    - os\n\n    Example:\n    >>> count_word_frequency()\n    {'word1': 2, 'word2': 3, 'word3': 5}\n    \"\"\"\n    with open(FILE_PATH, 'r') as file:\n        text = file.read()\n\n    words = re.findall(r'\\b\\w+\\b', text)\n    word_counts = collections.Counter(words)\n\n    return dict(word_counts)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, os, nltk, collections\n```python\nimport pandas as pd\nimport os\nimport nltk\nfrom nltk.corpus import stopwords\nfrom collections import Counter\n\n# Constants\nFILE_PATH = 'myfile.txt'\n\ndef count_non_stopword_frequency():\n    \"\"\"\n    Count the frequency of each non-stopword in a text file and return a dictionary \n    where the keys are the words and the values are their frequencies.\n\n    Requirements:\n    - pandas\n    - os\n    - nltk\n    - collections\n\n    Example:\n    >>> count_non_stopword_frequency()\n    {'word1': 2, 'word2': 3, 'word3': 5}\n    \"\"\"\n    nltk.download('stopwords')\n    stop_words = set(stopwords.words('english'))\n\n    with open(FILE_PATH, 'r') as file:\n        text = file.read()\n\n    words = text.split()\n    non_stopwords = [word for word in words if word not in stop_words]\n\n    word_counts = Counter(non_stopwords)\n\n    return dict(word_counts)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, matplotlib, os\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\n# Constants\nFILE_PATH = 'myfile.txt'\n\ndef plot_word_length_distribution():\n    \"\"\"\n    Plot the distribution of word lengths in a text file.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - os\n\n    Example:\n    >>> plot_word_length_distribution()\n    \"\"\"\n    with open(FILE_PATH, 'r') as file:\n        text = file.read()\n\n    words = text.split()\n    word_lengths = [len(word) for word in words]\n\n    plt.hist(word_lengths, bins=np.arange(1, max(word_lengths)+2)-0.5, edgecolor='black')\n    plt.title('Word Length Distribution')\n    plt.xlabel('Word Length')\n    plt.ylabel('Frequency')\n    plt.show()\n```\n"}
{"seed": "{\"task_id\": 47199, \"prompt\": \"def f_47199(a):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"a is not None\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(None) == False\\n\", \"\\n    assert candidate(0) == True\\n\", \"\\n    assert candidate(0.00) == True\\n\", \"\\n    assert candidate([]) == True\\n\", \"\\n    assert candidate(102) == True\\n\"], \"entry_point\": \"f_47199\", \"intent\": \"\\u5909\\u6570`a`\\u304cNone\\u3067\\u306a\\u3044\\u5834\\u5408\\u306b\\u5909\\u6570\\u3092\\u8868\\u793a\\u3059\\u308b\", \"library\": []}", "generation": "Scenario 1:\nnumpy, pandas, matplotlib.pyplot, sklearn.preprocessing\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\ndef f_47199_1(df, column):\n    \"\"\"\n    Check if a column in a DataFrame is not None, standardize its values using \n    sklearn's StandardScaler, and plot the distribution of the standardized values.\n\n    Parameters:\n    df (pandas.DataFrame): DataFrame input.\n    column (str): The column in the DataFrame to check.\n\n    Returns:\n    Boolean: True if the operation was successful, otherwise False.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - sklearn.preprocessing\n\n    Example:\n    >>> df = pd.DataFrame({'A': [1, 2, 3, 4, 5]})\n    >>> f_47199_1(df, 'A')\n    \"\"\"\n    if column in df and df[column] is not None:\n        data = df[column].values.reshape(-1, 1)\n        scaler = StandardScaler()\n        standardized = scaler.fit_transform(data)\n        plt.hist(standardized, bins=10)\n        plt.show()\n        return True\n    else:\n        return False\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, scipy.optimize, matplotlib.pyplot\n```python\nimport numpy as np\nfrom scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\n\ndef f_47199_2(x, y):\n    \"\"\"\n    Check if x and y are not None, fit a second degree polynomial to the data, \n    and plot the original data and the fitted curve.\n\n    Parameters:\n    x, y (numpy.array): x and y coordinates of the data.\n\n    Returns:\n    Boolean: True if the operation was successful, otherwise False.\n\n    Requirements:\n    - numpy\n    - scipy.optimize\n    - matplotlib.pyplot\n\n    Example:\n    >>> x = np.array([1, 2, 3, 4, 5])\n    >>> y = np.array([1, 4, 9, 16, 25])\n    >>> f_47199_2(x, y)\n    \"\"\"\n    if x is not None and y is not None:\n        def func(x, a, b, c):\n            return a * x**2 + b * x + c\n\n        popt, _ = curve_fit(func, x, y)\n\n        plt.scatter(x, y, label='Data')\n        plt.plot(x, func(x, *popt), 'r-', label='Fit: a=%5.3f, b=%5.3f, c=%5.3f' % tuple(popt))\n        plt.legend()\n        plt.show()\n        return True\n    else:\n        return False\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, shutil, pathlib\n```python\nimport os\nimport shutil\nfrom pathlib import Path\n\ndef f_47199_3(source, destination):\n    \"\"\"\n    Check if source and destination directories are not None and exist, \n    then copy all files from source to destination.\n\n    Parameters:\n    source, destination (str): Source and destination directories.\n\n    Returns:\n    Boolean: True if the operation was successful, otherwise False.\n\n    Requirements:\n    - os\n    - shutil\n    - pathlib\n\n    Example:\n    >>> f_47199_3('/path/to/source', '/path/to/destination')\n    \"\"\"\n    if source is not None and destination is not None:\n        source = Path(source)\n        destination = Path(destination)\n\n        if source.is_dir() and destination.is_dir():\n            for file_name in os.listdir(source):\n                shutil.copy(source / file_name, destination)\n            return True\n    return False\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nsqlite3, pandas\n```python\nimport sqlite3\nimport pandas as pd\n\ndef f_47199_4(db_name, table_name):\n    \"\"\"\n    Check if db_name and table_name are not None, then load the table from the SQLite \n    database into a pandas DataFrame.\n\n    Parameters:\n    db_name, table_name (str): Name of the database and table.\n\n    Returns:\n    pandas.DataFrame: DataFrame with the table data if the operation was successful, \n                      otherwise None.\n\n    Requirements:\n    - sqlite3\n    - pandas\n\n    Example:\n    >>> f_47199_4('my_database.db', 'my_table')\n    \"\"\"\n    if db_name is not None and table_name is not None:\n        conn = sqlite3.connect(db_name)\n        df = pd.read_sql_query(f\"SELECT * from {table_name}\", conn)\n        conn.close()\n        return df\n    else:\n        return None\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\njson, requests\n```python\nimport json\nimport requests\n\ndef f_47199_5(url):\n    \"\"\"\n    Check if url is not None, then send a GET request to the URL and parse the JSON response.\n\n    Parameters:\n    url (str): The URL to send the request to.\n\n    Returns:\n    dict: Dictionary with the parsed JSON data if the operation was successful, \n          otherwise None.\n\n    Requirements:\n    - json\n    - requests\n\n    Example:\n    >>> f_47199_5('https://api.github.com')\n    \"\"\"\n    if url is not None:\n        response = requests.get(url)\n        if response.status_code == 200:\n            data = json.loads(response.text)\n            return data\n    else:\n        return None\n```"}
{"seed": "{\"task_id\": 23332, \"prompt\": \"def f_23332(data):\\n\\t\", \"suffix\": \"\\n\\treturn list\", \"canonical_solution\": \"\\n\\tlist = []\\n\\tfor r in data:\\n\\t\\tlist.append(', '.join(r))\\n\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    data = [['a','b','c','x','y','z'],\\n            ['f', 'g', 'h', 'i', 'j', 'k']]\\n    assert candidate(data) == [\\\"a, b, c, x, y, z\\\", \\\"f, g, h, i, j, k\\\"]\\n\"], \"entry_point\": \"f_23332\", \"intent\": \"\\u4e8c\\u6b21\\u5143\\u30ea\\u30b9\\u30c8`list`\\u306e\\u4e2d\\u8eab\\u3092\\u5168\\u3066\", \"library\": []}", "generation": "Scenario 1: \npandas, nltk, matplotlib, random\n```python\nimport pandas as pd\nimport nltk\nimport matplotlib.pyplot as plt\nfrom random import choice\n\n# Constants\nWORDS = nltk.corpus.words.words()\nSENTENCES_COUNT = 10\n\ndef generate_random_sentences(data):\n    \"\"\"\n    Given a 2D list of words, generate a pandas DataFrame containing random sentences \n    and their lengths. Also, plot a histogram of the sentence lengths.\n    \n    Parameters:\n    data (list): A 2D list of words.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the generated sentences and their lengths.\n    \n    Requirements:\n    - pandas\n    - nltk\n    - matplotlib.pyplot\n    - random\n    \n    Example:\n    >>> data = [['apple', 'banana', 'cherry'], ['dog', 'elephant', 'fox']]\n    >>> sentences_df = generate_random_sentences(data)\n    >>> print(sentences_df)\n    >>> sentences_df['Length'].hist(bins=5)\n    \"\"\"\n    sentences = []\n    for words_list in data:\n        for _ in range(SENTENCES_COUNT):\n            sentence = ' '.join([choice(words_list) for _ in range(len(words_list))])\n            sentences.append([sentence, len(sentence)])\n\n    sentences_df = pd.DataFrame(sentences, columns=['Sentence', 'Length'])\n\n    return sentences_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, itertools, math, random\n```python\nimport numpy as np\nimport itertools\nimport math\nfrom random import sample\n\n# Constants\nSAMPLE_SIZE = 5\nPOWER_SET_SIZE = 2 ** SAMPLE_SIZE\n\ndef generate_power_set(data):\n    \"\"\"\n    Given a 2D list of numbers, generate a power set of a random sample from the list, \n    calculate the sum of each subset, and return their mean and standard deviation.\n\n    Parameters:\n    data (list): A 2D list of numbers.\n\n    Returns:\n    tuple: A tuple with the mean and standard deviation of the sums.\n\n    Requirements:\n    - numpy\n    - itertools\n    - math\n    - random\n\n    Example:\n    >>> data = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n    >>> mean, std_dev = generate_power_set(data)\n    \"\"\"\n    flat_list = [item for sublist in data for item in sublist]\n    sample_list = sample(flat_list, SAMPLE_SIZE)\n    \n    power_set = list(itertools.chain.from_iterable(itertools.combinations(sample_list, r) for r in range(len(sample_list)+1)))\n    \n    sums = [sum(subset) for subset in power_set if subset]\n    \n    mean = np.mean(sums)\n    std_dev = np.std(sums)\n    \n    return mean, std_dev\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, sklearn, random, matplotlib\n```python\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\nfrom random import choice\n\n# Constants\nSAMPLE_SIZE = 10\nWORDS = ['apple', 'banana', 'cherry', 'dog', 'elephant', 'fox']\n\ndef generate_word_frequencies(data):\n    \"\"\"\n    Given a 2D list of words, generate random sentences, calculate word frequencies using \n    CountVectorizer and plot a bar chart of the frequencies.\n\n    Parameters:\n    data (list): A 2D list of words.\n\n    Returns:\n    DataFrame: A pandas DataFrame with word frequencies.\n\n    Requirements:\n    - pandas\n    - sklearn.feature_extraction.text.CountVectorizer\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> data = [['apple', 'banana', 'cherry'], ['dog', 'elephant', 'fox']]\n    >>> frequencies = generate_word_frequencies(data)\n    >>> print(frequencies)\n    >>> frequencies.plot(kind='bar', legend=False)\n    \"\"\"\n    sentences = []\n    for words_list in data:\n        sentence = ' '.join([choice(words_list) for _ in range(SAMPLE_SIZE)])\n        sentences.append(sentence)\n\n    vectorizer = CountVectorizer()\n    X = vectorizer.fit_transform(sentences)\n\n    frequencies_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n\n    return frequencies_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, itertools, random, math\n```python\nimport numpy as np\nimport itertools\nfrom random import choice, sample\nimport math\n\n# Constants\nSAMPLE_SIZE = 5\n\ndef calculate_geometric_mean(data):\n    \"\"\"\n    Given a 2D list of numbers, generate a random sample, calculate the geometric mean of \n    all non-empty subsets of the sample.\n\n    Parameters:\n    data (list): A 2D list of numbers.\n\n    Returns:\n    float: The geometric mean of the subsets.\n\n    Requirements:\n    - numpy\n    - itertools\n    - random\n    - math\n\n    Example:\n    >>> data = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n    >>> geo_mean = calculate_geometric_mean(data)\n    \"\"\"\n    flat_list = [item for sublist in data for item in sublist]\n    sample_list = sample(flat_list, SAMPLE_SIZE)\n    \n    subsets = list(itertools.chain.from_iterable(itertools.combinations(sample_list, r) for r in range(1, len(sample_list)+1)))\n    \n    geo_means = [math.prod(subset) ** (1/len(subset)) for subset in subsets]\n    \n    return np.mean(geo_means)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, sklearn, random, matplotlib\n```python\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport matplotlib.pyplot as plt\nfrom random import choice\n\n# Constants\nSAMPLE_SIZE = 10\nWORDS = ['apple', 'banana', 'cherry', 'dog', 'elephant', 'fox']\n\ndef generate_tfidf(data):\n    \"\"\"\n    Given a 2D list of words, generate random sentences, calculate TF-IDF scores using \n    TfidfVectorizer and plot a bar chart of the scores.\n\n    Parameters:\n    data (list): A 2D list of words.\n\n    Returns:\n    DataFrame: A pandas DataFrame with TF-IDF scores.\n\n    Requirements:\n    - pandas\n    - sklearn.feature_extraction.text.TfidfVectorizer\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> data = [['apple', 'banana', 'cherry'], ['dog', 'elephant', 'fox']]\n    >>> tfidf_scores = generate_tfidf(data)\n    >>> print(tfidf_scores)\n    >>> tfidf_scores.plot(kind='bar', legend=False)\n    \"\"\"\n    sentences = []\n    for words_list in data:\n        sentence = ' '.join([choice(words_list) for _ in range(SAMPLE_SIZE)])\n        sentences.append(sentence)\n\n    vectorizer = TfidfVectorizer()\n    X = vectorizer.fit_transform(sentences)\n\n    tfidf_scores_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n\n    return tfidf_scores_df\n```"}
{"seed": "{\"task_id\": 34431, \"prompt\": \"def f_34431():\\n\\t\", \"suffix\": \"\\n\\treturn f\", \"canonical_solution\": \"f = open('all_names.csv', 'w', encoding='UTF-8') \", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    f = candidate()\\n    assert f.name == 'all_names.csv'\\n    assert f.mode == 'w'\\n    assert f.encoding == 'UTF-8'\\n\"], \"entry_point\": \"f_34431\", \"intent\": \"\\u6587\\u5b57\\u30b3\\u30fc\\u30c9\", \"library\": []}", "generation": "Scenario 1:\npandas, os, csv, random\n```python\nimport pandas as pd\nimport os\nimport csv\nfrom random import randint\n\n# Constants\nFILE_NAME = 'all_names.csv'\nNEW_FILE_NAME = 'randomized_names.csv'\nENCODING = 'UTF-8'\nFIELDS = ['First Name', 'Last Name']\n\ndef randomize_names():\n    \"\"\"\n    Read a CSV file that contains first names and last names, randomize the order of the names,\n    and write the randomized names to a new CSV file.\n\n    Returns:\n    str: The name of the new CSV file.\n\n    Requirements:\n    - pandas\n    - os\n    - csv\n    - random\n\n    Example:\n    >>> randomize_names()\n    'randomized_names.csv'\n    \"\"\"\n    if not os.path.exists(FILE_NAME):\n        raise FileNotFoundError(f'{FILE_NAME} does not exist')\n\n    df = pd.read_csv(FILE_NAME, encoding=ENCODING)\n\n    # Randomize the order of the names\n    df = df.sample(frac=1).reset_index(drop=True)\n\n    # Write the randomized names to a new CSV file\n    df.to_csv(NEW_FILE_NAME, encoding=ENCODING, index=False, quoting=csv.QUOTE_ALL)\n\n    return NEW_FILE_NAME\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, csv, random, string\n```python\nimport os\nimport csv\nfrom random import choice, randint\nimport string\n\n# Constants\nFILE_NAME = 'all_names.csv'\nNEW_FILE_NAME = 'modified_names.csv'\nENCODING = 'UTF-8'\nFIELDS = ['First Name', 'Last Name']\n\ndef modify_names():\n    \"\"\"\n    Read a CSV file that contains first names and last names, modify each name by replacing a random character\n    in the name with a random character from the alphabet, and write the modified names to a new CSV file.\n\n    Returns:\n    str: The name of the new CSV file.\n\n    Requirements:\n    - os\n    - csv\n    - random\n    - string\n\n    Example:\n    >>> modify_names()\n    'modified_names.csv'\n    \"\"\"\n    if not os.path.exists(FILE_NAME):\n        raise FileNotFoundError(f'{FILE_NAME} does not exist')\n\n    with open(FILE_NAME, 'r', encoding=ENCODING) as f:\n        reader = csv.DictReader(f)\n        rows = list(reader)\n\n    for row in rows:\n        for field in FIELDS:\n            name = list(row[field])\n            name[randint(0, len(name)-1)] = choice(string.ascii_letters)\n            row[field] = ''.join(name)\n\n    with open(NEW_FILE_NAME, 'w', encoding=ENCODING) as f:\n        writer = csv.DictWriter(f, fieldnames=FIELDS, quoting=csv.QUOTE_ALL)\n        writer.writeheader()\n        writer.writerows(rows)\n\n    return NEW_FILE_NAME\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, csv, random, string\n```python\nimport os\nimport csv\nfrom random import randint\nimport string\n\n# Constants\nFILE_NAME = 'all_names.csv'\nNEW_FILE_NAME = 'name_initials.csv'\nENCODING = 'UTF-8'\nFIELDS = ['First Name', 'Last Name']\n\ndef extract_name_initials():\n    \"\"\"\n    Read a CSV file that contains first names and last names, extract the initials of each name, \n    and write the initials to a new CSV file.\n\n    Returns:\n    str: The name of the new CSV file.\n\n    Requirements:\n    - os\n    - csv\n    - random\n    - string\n\n    Example:\n    >>> extract_name_initials()\n    'name_initials.csv'\n    \"\"\"\n    if not os.path.exists(FILE_NAME):\n        raise FileNotFoundError(f'{FILE_NAME} does not exist')\n\n    with open(FILE_NAME, 'r', encoding=ENCODING) as f:\n        reader = csv.DictReader(f)\n        rows = list(reader)\n\n    for row in rows:\n        for field in FIELDS:\n            row[field] = row[field][0] if row[field] else ''\n\n    with open(NEW_FILE_NAME, 'w', encoding=ENCODING) as f:\n        writer = csv.DictWriter(f, fieldnames=FIELDS, quoting=csv.QUOTE_ALL)\n        writer.writeheader()\n        writer.writerows(rows)\n\n    return NEW_FILE_NAME\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, csv, pandas, matplotlib\n```python\nimport os\nimport csv\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nFILE_NAME = 'all_names.csv'\nENCODING = 'UTF-8'\nFIELDS = ['First Name', 'Last Name']\n\ndef plot_name_lengths():\n    \"\"\"\n    Read a CSV file that contains first names and last names, calculate the length of each name, \n    and plot a histogram of the name lengths.\n\n    Returns:\n    None\n\n    Requirements:\n    - os\n    - csv\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_name_lengths()\n    \"\"\"\n    if not os.path.exists(FILE_NAME):\n        raise FileNotFoundError(f'{FILE_NAME} does not exist')\n\n    df = pd.read_csv(FILE_NAME, encoding=ENCODING)\n\n    name_lengths = df['First Name'].str.len().append(df['Last Name'].str.len())\n\n    plt.hist(name_lengths, bins=20, alpha=0.5)\n    plt.title('Histogram of Name Lengths')\n    plt.xlabel('Name Length')\n    plt.ylabel('Frequency')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, csv, pandas, numpy\n```python\nimport os\nimport csv\nimport pandas as pd\nimport numpy as np\n\n# Constants\nFILE_NAME = 'all_names.csv'\nENCODING = 'UTF-8'\nFIELDS = ['First Name', 'Last Name']\n\ndef calculate_name_statistics():\n    \"\"\"\n    Read a CSV file that contains first names and last names, calculate statistics (mean, median, \n    mode) of the length of the names, and return the statistics as a dictionary.\n\n    Returns:\n    dict: A dictionary that contains the statistics of the name lengths.\n\n    Requirements:\n    - os\n    - csv\n    - pandas\n    - numpy\n\n    Example:\n    >>> calculate_name_statistics()\n    {'mean': 5.0, 'median': 5.0, 'mode': 5.0}\n    \"\"\"\n    if not os.path.exists(FILE_NAME):\n        raise FileNotFoundError(f'{FILE_NAME} does not exist')\n\n    df = pd.read_csv(FILE_NAME, encoding=ENCODING)\n\n    name_lengths = df['First Name'].str.len().append(df['Last Name'].str.len())\n\n    statistics = {\n        'mean': np.mean(name_lengths),\n        'median': np.median(name_lengths),\n        'mode': name_lengths.mode()[0]\n    }\n\n    return statistics\n```\nAbove are the illustrations."}
{"seed": "{\"task_id\": 33700, \"prompt\": \"def f_33700(list):\\n\\t\", \"suffix\": \"\\n\\treturn newlist\", \"canonical_solution\": \"\\n\\tnewlist = []\\n\\tfor s in list: \\n\\t\\tif s.endswith('string'): \\n\\t\\t\\tnewlist.append(s)\\n\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(['abcstring', 'bbbb', 'fhstringyjn', '1326546']) == ['abcstring']\\n\"], \"entry_point\": \"f_33700\", \"intent\": \"\\u30ea\\u30b9\\u30c8`list`\\u304b\\u3089\\u6761\\u4ef6\\u3068\\u306a\\u308b\\u6587\\u5b57\\u5217`string`\\u3068\\u90e8\\u5206\\u4e00\\u81f4\\u3059\\u308b\\u8981\\u7d20\\u3092\\u53d6\\u308a\\u51fa\\u3057\\u3066\\u65b0\\u3057\\u3044\\u30ea\\u30b9\\u30c8`newlist`\\u3092\\u4f5c\\u308b\", \"library\": []}", "generation": "Scenario 1: \npandas, re, random, matplotlib\n```python\nimport pandas as pd\nimport re\nfrom random import choice\nimport matplotlib.pyplot as plt\n\n# Constants\nDOMAINS = ['@gmail.com', '@yahoo.com', '@hotmail.com', '@outlook.com', '@custom.com']\nNAMES = ['John', 'Sarah', 'Mike', 'Amanda', 'Chris', 'Jessica', 'Robert', 'Emily', 'Daniel', 'Emma']\n\ndef generate_email_data(n):\n    \"\"\"\n    Generate a pandas DataFrame of n rows with emails. Emails are generated \n    using names from NAMES list and domains from DOMAINS list.\n    \n    Parameters:\n    n (int): The number of rows of data to generate.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with emails.\n    \n    Requirements:\n    - pandas\n    - re\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = generate_email_data(10)\n    >>> df\n    \"\"\"\n    email_list = []\n\n    for _ in range(n):\n        name = choice(NAMES)\n        domain = choice(DOMAINS)\n        email = name.lower() + domain\n        email_list.append(email)\n\n    df = pd.DataFrame(email_list, columns=['Email'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, matplotlib, random\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nMIN_NUM = 1\nMAX_NUM = 100\n\ndef generate_random_list_and_plot_histogram(n):\n    \"\"\"\n    Generate a list of n random integers between MIN_NUM and MAX_NUM and plot a histogram of the values.\n    \n    Parameters:\n    n (int): The number of random integers to generate.\n    \n    Returns:\n    list: A list of n random integers.\n    \n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> random_list = generate_random_list_and_plot_histogram(100)\n    >>> plt.hist(random_list, bins=10)\n    \"\"\"\n    random_list = [randint(MIN_NUM, MAX_NUM) for _ in range(n)]\n    plt.hist(random_list, bins=10)\n    plt.show()\n\n    return random_list\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, pandas, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef generate_random_data_and_scatter_plot(n):\n    \"\"\"\n    Generate a pandas DataFrame of n rows with random x, y coordinates and plot a scatter plot of the values.\n    \n    Parameters:\n    n (int): The number of rows of data to generate.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with x, y coordinates.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = generate_random_data_and_scatter_plot(100)\n    >>> plt.scatter(df['x'], df['y'])\n    \"\"\"\n    x = np.random.randn(n)\n    y = np.random.randn(n)\n    df = pd.DataFrame({'x': x, 'y': y})\n\n    plt.scatter(df['x'], df['y'])\n    plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, re, random\n```python\nimport pandas as pd\nimport re\nfrom random import choice, randint\n\n# Constants\nWORDS = ['apple', 'ball', 'cat', 'dog', 'elephant', 'fish', 'grape', 'hand', 'ant', 'eagle', 'ice cream', 'air', 'East', 'Exit', 'Door', 'cam', 'bad', 'fast', 'Zealot', 'intern', 'Bead', 'Bead', 'Exit', 'act', 'Zealot', 'bad', 'fast', 'act', 'Exit', 'act', 'car', 'Zealot', 'Exit', 'act', 'act', 'East', 'cam', 'bad', 'fast', 'act', 'Zealot', 'Exit', 'act', 'Door', 'cam', 'bad', 'fast', 'act', 'Door', 'Door', 'cam', 'bad', 'fast', 'intern', 'Bead', 'act', 'act', 'act', 'Zealot', 'bad', 'fast', 'Exit', 'act', 'Door', 'act', 'act', 'East', 'Exit', 'Door', 'cam', 'bad', 'fast', 'Zealot', 'Exit', 'act', 'Door', 'East', 'Exit', 'cam', 'bad', 'fast', 'Zealot', 'Exit', 'act', 'Door', 'Door', 'Exit', 'car', 'bad', 'fast', 'Zealot', 'Exit', 'act', 'East', 'intern', 'Bead', 'Bead', 'Exit', 'act', 'Door', 'East', 'cam', 'bad', 'fast', 'act', 'Door', 'Door', 'Door', 'East', 'cam', 'bad', 'fast', 'act', 'Zealot', 'Exit', 'act', 'East', 'Door', 'cam', 'bad', 'fast', 'act', 'East', 'Door', 'cam', 'bad', 'fast', 'act', 'Door', 'Door', 'Door', 'East', 'cam', 'bad', 'fast', 'act', 'Zealot', 'Exit', 'act', 'East', 'Door', 'Door', 'cam', 'bad', 'fast', 'act', 'East', 'Door', 'Door', 'cam', 'bad', 'fast', 'act', 'Door', 'Door', 'Door', 'East', 'cam', 'bad', 'fast', 'act', 'Zealot', 'Exit', 'act', 'East', 'Door', 'Door', 'cam', 'bad', 'fast', 'act', 'East', 'Door', 'Door', 'cam', 'bad', 'fast', 'act', 'Door', 'Door', 'Door', 'East', 'cam', 'bad', 'fast', 'act', 'Zealot', 'Exit', 'act']\n\ndef generate_text_data(n):\n    \"\"\"\n    Generate a pandas DataFrame of n rows with random text data. \n    Each row contains a string constructed from random words from the WORDS list.\n    \n    Parameters:\n    n (int): The number of rows of data to generate.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with text data.\n    \n    Requirements:\n    - pandas\n    - re\n    - random\n\n    Example:\n    >>> df = generate_text_data(10)\n    >>> df\n    \"\"\"\n    text_data = []\n\n    for _ in range(n):\n        num_words = randint(1, len(WORDS))\n        text = ' '.join([choice(WORDS) for _ in range(num_words)])\n        text_data.append(text)\n\n    df = pd.DataFrame(text_data, columns=['Text'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, pandas, re, sklearn.model_selection\n```python\nimport numpy as np\nimport pandas as pd\nimport re\nfrom sklearn.model_selection import train_test_split\n\n# Constants\nWORDS = ['apple', 'ball', 'cat', 'dog', 'elephant', 'fish', 'grape', 'hand', 'ant', 'eagle', 'ice cream', 'air', 'East', 'Exit', 'Door', 'cam', 'bad', 'fast', 'Zealot', 'intern', 'Bead', 'Bead', 'Exit', 'act', 'Zealot', 'bad', 'fast', 'act', 'Exit', 'act', 'car', 'Zealot', 'Exit', 'act', 'act', 'East', 'cam', 'bad', 'fast', 'act', 'Zealot', 'Exit', 'act', 'Door', 'cam', 'bad', 'fast', 'act', 'Door', 'Door', 'cam', 'bad', 'fast', 'intern', 'Bead', 'act', 'act', 'act', 'Zealot', 'bad', 'fast', 'Exit', 'act', 'Door', 'act', 'act', 'East', 'Exit', 'Door', 'cam', 'bad', 'fast', 'Zealot', 'Exit', 'act', 'Door', 'East', 'Exit', 'cam', 'bad', 'fast', 'Zealot', 'Exit', 'act', 'Door', 'Door', 'Exit', 'car', 'bad', 'fast', 'Zealot', 'Exit', 'act', 'East', 'intern', 'Bead', 'Bead', 'Exit', 'act', 'Door', 'East', 'cam', 'bad', 'fast', 'act', 'Door', 'Door', 'Door', 'East', 'cam', 'bad', 'fast', 'act', 'Zealot', 'Exit', 'act', 'East', 'Door', 'cam', 'bad', 'fast', 'act', 'East', 'Door', 'cam', 'bad', 'fast', 'act', 'Door', 'Door', 'Door', 'East', 'cam', 'bad', 'fast', 'act', 'Zealot', 'Exit', 'act', 'East', 'Door', 'Door', 'cam', 'bad', 'fast', 'act', 'East', 'Door', 'Door', 'cam', 'bad', 'fast', 'act', 'Door', 'Door', 'Door', 'East', 'cam', 'bad', 'fast', 'act', 'Zealot', 'Exit', 'act', 'East', 'Door', 'Door', 'cam', 'bad', 'fast', 'act', 'East', 'Door', 'Door', 'cam', 'bad', 'fast', 'act', 'Door', 'Door', 'Door', 'East', 'cam', 'bad', 'fast', 'act', 'Zealot', 'Exit', 'act']\n\ndef generate_text_data_and_split(n, test_size=0.2, random_state=42):\n    \"\"\"\n    Generate a pandas DataFrame of n rows with random text data and split it into train and test sets. \n    Each row contains a string constructed from random words from the WORDS list.\n    \n    Parameters:\n    n (int): The number of rows of data to generate.\n    test_size (float): The proportion of the dataset to include in the test split.\n    random_state (int): The seed used by the random number generator.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with text data.\n    \n    Requirements:\n    - pandas\n    - re\n    - numpy\n    - sklearn.model_selection.train_test_split\n\n    Example:\n    >>> train, test = generate_text_data_and_split(100)\n    >>> len(train), len(test)\n    \"\"\"\n    text_data = []\n\n    for _ in range(n):\n        num_words = np.random.randint(1, len(WORDS))\n        text = ' '.join([np.random.choice(WORDS) for _ in range(num_words)])\n        text_data.append(text)\n\n    df = pd.DataFrame(text_data, columns=['Text'])\n\n    train, test = train_test_split(df, test_size=test_size, random_state=random_state)\n\n    return train, test\n```"}
{"seed": "{\"task_id\": 33700, \"prompt\": \"def f_33700(list):\\n\\t\", \"suffix\": \"\\n\\treturn newlist\", \"canonical_solution\": \"newlist = []\\n\\tfor s in list:\\n\\t    if 'string' in s:\\n\\t        newlist.append(s)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(['abcstring', 'bbbb', 'fhstringyjn', '1326546']) == ['abcstring', 'fhstringyjn']\\n\"], \"entry_point\": \"f_33700\", \"intent\": \"\\u30ea\\u30b9\\u30c8`list`\\u304b\\u3089\\u6761\\u4ef6\\u3068\\u306a\\u308b\\u6587\\u5b57\\u5217`string`\\u3068\\u90e8\\u5206\\u4e00\\u81f4\\u3059\\u308b\\u8981\\u7d20\\u3092\\u53d6\\u308a\\u51fa\\u3057\\u3066\\u65b0\\u3057\\u3044\\u30ea\\u30b9\\u30c8`newlist`\\u3092\\u4f5c\\u308b\", \"library\": []}", "generation": "Scenario 1: \npandas, re, random, matplotlib\n```python\nimport pandas as pd\nimport re\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nCATEGORIES = ['Electronics', 'Home Decor', 'Kitchen Appliances', 'Books', 'Toys']\nPRODUCTS = ['Smartphone', 'Table Lamp', 'Microwave', 'Harry Potter', 'Lego']\n\ndef filter_and_analyse_products(products_list, substring):\n    \"\"\"\n    Filters a list of products based on a substring and returns a new list.\n    Also, displays a bar chart of the product categories for the filtered products.\n\n    Parameters:\n    products_list (list of dicts): The list of products. Each product is a dict with 'name' and 'category'.\n    substring (str): The substring to filter products.\n\n    Returns:\n    list of dicts: The filtered list of products.\n\n    Requirements:\n    - pandas\n    - re\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> products = [{\"name\": \"Smartphone\", \"category\": \"Electronics\"},\n                    {\"name\": \"Table Lamp\", \"category\": \"Home Decor\"},\n                    {\"name\": \"Microwave\", \"category\": \"Kitchen Appliances\"},\n                    {\"name\": \"Harry Potter\", \"category\": \"Books\"},\n                    {\"name\": \"Lego\", \"category\": \"Toys\"}]\n    >>> filter_and_analyse_products(products, 'o')\n    \"\"\"\n    filtered_products = [product for product in products_list if re.search(substring, product['name'], re.I)]\n\n    # Create a pandas DataFrame from the filtered list\n    df = pd.DataFrame(filtered_products)\n\n    # Plot a bar chart of the product categories\n    df['category'].value_counts().plot(kind='bar')\n\n    return filtered_products\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nrandom, itertools, string\n```python\nfrom random import sample, randint\nfrom itertools import combinations\nimport string\n\n# Constants\nLETTERS = string.ascii_lowercase\n\ndef generate_combinations(word_list, substring_length):\n    \"\"\"\n    Generate a list of all possible combinations of a given length from a list of words.\n    Then filter this list to only include combinations containing a random substring of a specific length.\n\n    Parameters:\n    word_list (list): The list of words.\n    substring_length (int): The length of the random substring.\n\n    Returns:\n    list: The filtered list of combinations.\n\n    Requirements:\n    - random\n    - itertools\n    - string\n\n    Example:\n    >>> generate_combinations(['abc', 'def', 'ghi', 'jkl', 'mno'], 2)\n    \"\"\"\n    # Generate all possible combinations\n    all_combinations = list(combinations(word_list, 2))\n\n    # Generate a random substring of specified length\n    substring = ''.join(sample(LETTERS, substring_length))\n\n    # Filter the combinations to only include those containing the random substring\n    filtered_combinations = [combo for combo in all_combinations if any(substring in word for word in combo)]\n\n    return filtered_combinations\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \npandas, numpy, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef analyse_dataframe(df, substring):\n    \"\"\"\n    Filter a DataFrame based on a substring and then perform a statistical analysis on the filtered DataFrame.\n\n    Parameters:\n    df (DataFrame): The input DataFrame.\n    substring (str): The substring to filter rows.\n\n    Returns:\n    DataFrame: The DataFrame with statistical analysis.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame(np.random.randint(0,100,size=(100, 5)), columns=COLUMNS)\n    >>> analyse_dataframe(df, '0')\n    \"\"\"\n    # Filter dataframe\n    filtered_df = df[df.astype(str).apply(lambda row: row.str.contains(substring).any(), axis=1)]\n\n    # Perform statistical analysis\n    stats_df = filtered_df.describe()\n\n    # Plot histogram for each column\n    for col in filtered_df.columns:\n        plt.figure()\n        filtered_df[col].plot(kind='hist', title=col)\n\n    return stats_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nrandom, string, collections\n```python\nfrom random import sample\nimport string\nfrom collections import Counter\n\n# Constants\nLETTERS = string.ascii_lowercase\n\ndef analyse_strings(string_list, substring_length):\n    \"\"\"\n    Generate a random substring of a specific length and count its occurrence in a list of strings.\n\n    Parameters:\n    string_list (list): The list of strings.\n    substring_length (int): The length of the random substring.\n\n    Returns:\n    int: The count of the random substring in the list of strings.\n\n    Requirements:\n    - random\n    - string\n    - collections\n\n    Example:\n    >>> analyse_strings(['abc', 'def', 'ghi', 'jkl', 'mno'], 2)\n    \"\"\"\n    # Generate a random substring of specified length\n    substring = ''.join(sample(LETTERS, substring_length))\n\n    # Count the occurrence of the substring in the string list\n    count = sum(Counter(s)[''.join(sorted(substring))] for s in string_list)\n\n    return count\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, scipy.stats, matplotlib\n```python\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\n# Constants\nSAMPLE_SIZE = 1000\n\ndef analyse_distribution(data_list, substring):\n    \"\"\"\n    Filter a list of data based on a substring and then perform statistical analysis on the filtered data.\n\n    Parameters:\n    data_list (list): The list of data.\n    substring (str): The substring to filter data.\n\n    Returns:\n    tuple: The mean and standard deviation of the filtered data.\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Example:\n    >>> data = np.random.normal(0, 1, SAMPLE_SIZE).tolist()\n    >>> analyse_distribution(data, '0')\n    \"\"\"\n    # Convert data to strings and filter based on substring\n    filtered_data = [x for x in data_list if substring in str(x)]\n\n    # Perform statistical analysis\n    mean, std_dev = np.mean(filtered_data), np.std(filtered_data)\n\n    # Plot histogram\n    plt.hist(filtered_data, bins='auto', density=True, alpha=0.7, color='#0504aa')\n\n    # Plot normal distribution with mean and standard deviation\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, mean, std_dev)\n    plt.plot(x, p, 'k', linewidth=2)\n\n    return mean, std_dev\n```"}
{"seed": "{\"task_id\": 27556, \"prompt\": \"def f_27556():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"plt.figure() \", \"test_start\": \"\\nimport matplotlib\\nimport matplotlib.pyplot as plt\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert isinstance(candidate(), matplotlib.figure.Figure)\\n\"], \"entry_point\": \"f_27556\", \"intent\": \"\\u30b0\\u30e9\\u30d5\\u3092\\u8868\\u793a\\u3059\\u308b\", \"library\": [\"matplotlib\"]}", "generation": "Scenario 1:\npandas, matplotlib, numpy, datetime, random\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom datetime import datetime, timedelta\nfrom random import randint\n\n# Constants\nCATEGORIES = ['Electronics', 'Groceries', 'Clothing', 'Automotive', 'Health']\n\ndef generate_sales_report(start_date, end_date, num_entries):\n    \"\"\"\n    Generates a sales report for different categories over a specific period and plots the sales trend.\n    \n    Parameters:\n    start_date (datetime): The start date for the sales report.\n    end_date (datetime): The end date for the sales report.\n    num_entries (int): The number of entries to simulate for each category.\n    \n    Returns: \n    tuple: A tuple containing the DataFrame of the simulated sales report and the matplotlib Figure object of the sales trend.\n    \n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - numpy\n    - datetime\n    - random\n    \n    Example:\n    >>> start_date = datetime(2021, 1, 1)\n    >>> end_date = datetime(2021, 12, 31)\n    >>> report, fig = generate_sales_report(start_date, end_date, 1000)\n    >>> print(report)\n    >>> plt.show(fig)\n    \"\"\"\n    dates = pd.date_range(start_date, end_date)\n    data = []\n    \n    for category in CATEGORIES:\n        for _ in range(num_entries):\n            date = np.random.choice(dates)\n            sales = randint(1, 100)\n            data.append([date, category, sales])\n            \n    df = pd.DataFrame(data, columns=['Date', 'Category', 'Sales'])\n    \n    fig, ax = plt.subplots()\n    for category in CATEGORIES:\n        df_category = df[df['Category'] == category]\n        df_category.groupby(df_category['Date'].dt.to_period('M')).sum()['Sales'].plot(ax=ax, label=category)\n        \n    ax.legend()\n    \n    return df, fig\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\njson, os, matplotlib, pandas\n```python\nimport json\nimport os\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants\nJSON_FILES_DIR = './json_files_dir/'\n\ndef analyze_json_files():\n    \"\"\"\n    Analyze JSON files in a specific directory, count the number of keys in each file, and plot a histogram of the key counts.\n\n    Returns:\n    tuple: A tuple containing a DataFrame with the counts of keys in each JSON file and the matplotlib Figure object of the histogram.\n\n    Requirements:\n    - json\n    - os\n    - matplotlib.pyplot\n    - pandas\n\n    Example:\n    >>> df, fig = analyze_json_files()\n    >>> print(df)\n    >>> plt.show(fig)\n    \"\"\"\n    files = os.listdir(JSON_FILES_DIR)\n    data = []\n\n    for file in files:\n        if file.endswith('.json'):\n            with open(JSON_FILES_DIR + file, 'r') as f:\n                json_obj = json.load(f)\n                num_keys = len(json_obj.keys())\n                data.append([file, num_keys])\n\n    df = pd.DataFrame(data, columns=['File', 'Number of Keys'])\n    \n    fig, ax = plt.subplots()\n    df['Number of Keys'].plot(kind='hist', ax=ax, bins=20)\n    \n    return df, fig\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nscipy.stats, matplotlib, numpy, pandas\n```python\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# Constants\nMU = 0\nSIGMA = 1\nSAMPLE_SIZE = 1000\n\ndef sample_and_plot_normal_distribution():\n    \"\"\"\n    Generate samples from a normal distribution and plot the histogram and the probability density function (PDF).\n\n    Returns:\n    tuple: A tuple containing a numpy array of the generated samples and the matplotlib Figure object of the histogram and PDF.\n\n    Requirements:\n    - scipy.stats\n    - matplotlib.pyplot\n    - numpy\n    - pandas\n\n    Example:\n    >>> samples, fig = sample_and_plot_normal_distribution()\n    >>> print(samples)\n    >>> plt.show(fig)\n    \"\"\"\n    samples = np.random.normal(MU, SIGMA, SAMPLE_SIZE)\n    \n    fig, ax = plt.subplots()\n    \n    # Plot histogram\n    ax.hist(samples, bins=30, density=True, alpha=0.6, color='g')\n    \n    # Plot PDF\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, MU, SIGMA)\n    ax.plot(x, p, 'k', linewidth=2)\n    \n    return samples, fig\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, matplotlib, math\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\n\n# Constants\nX_START = 0\nX_END = 2 * math.pi\nNUM_POINTS = 1000\n\ndef plot_sine_and_cosine():\n    \"\"\"\n    Generate points for sine and cosine functions and plot both functions on the same graph.\n\n    Returns:\n    tuple: A tuple containing a numpy array of the x coordinates and the matplotlib Figure object of the plot.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - math\n\n    Example:\n    >>> x, fig = plot_sine_and_cosine()\n    >>> print(x)\n    >>> plt.show(fig)\n    \"\"\"\n    x = np.linspace(X_START, X_END, NUM_POINTS)\n    y_sin = np.sin(x)\n    y_cos = np.cos(x)\n    \n    fig, ax = plt.subplots()\n    ax.plot(x, y_sin, label='sin(x)')\n    ax.plot(x, y_cos, label='cos(x)')\n    ax.legend()\n    \n    return x, fig\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nmatplotlib, pandas, numpy, random\n```python\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom random import choices\n\n# Constants\nCATEGORIES = ['A', 'B', 'C', 'D', 'E']\nWEIGHTS = [0.1, 0.2, 0.3, 0.25, 0.15]\nNUM_SAMPLES = 1000\n\ndef plot_pie_chart_of_samples():\n    \"\"\"\n    Generate a random sample from a specified list of categories according to their weights, and plot a pie chart of the sample proportions.\n\n    Returns:\n    tuple: A tuple containing a pandas Series of the sample proportions and the matplotlib Figure object of the pie chart.\n\n    Requirements:\n    - matplotlib.pyplot\n    - pandas\n    - numpy\n    - random\n\n    Example:\n    >>> sample_proportions, fig = plot_pie_chart_of_samples()\n    >>> print(sample_proportions)\n    >>> plt.show(fig)\n    \"\"\"\n    samples = choices(CATEGORIES, weights=WEIGHTS, k=NUM_SAMPLES)\n    sample_proportions = pd.Series(samples).value_counts(normalize=True)\n    \n    fig, ax = plt.subplots()\n    ax.pie(sample_proportions, labels=sample_proportions.index, autopct='%1.1f%%')\n    \n    return sample_proportions, fig\n```"}
{"seed": "{\"task_id\": 37060, \"prompt\": \"def f_37060(data_frame):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"display(data_frame)\", \"test_start\": \"\\nimport pandas as pd\\nfrom IPython.display import display\\n\\ndef check(candidate):\", \"test\": [\"\\n    df = pd.DataFrame([1, 2, 3])\\n    try:\\n        candidate(df)\\n    except:\\n        assert False\\n\"], \"entry_point\": \"f_37060\", \"intent\": \"\\u30c7\\u30fc\\u30bf\\u30d5\\u30ec\\u30fc\\u30e0`data_frame`\\u3092\\u8868\\u793a\\u3059\\u308b\", \"library\": [\"IPython\", \"pandas\"]}", "generation": "Scenario 1:\n```python\nimport pandas as pd\nfrom IPython.display import display\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\ndef normalize_and_display(data_frame):\n    \"\"\"\n    This function takes a pandas DataFrame, normalizes its numerical columns to have a mean of 0 and \n    standard deviation of 1, then displays the DataFrame and a histogram of each column.\n\n    Parameters:\n    data_frame (DataFrame): The pandas DataFrame to normalize and display\n\n    Returns:\n    DataFrame: The normalized DataFrame\n\n    Requirements:\n    - pandas\n    - IPython.display\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.preprocessing\n\n    Example:\n    >>> df = pd.DataFrame(np.random.rand(5, 3), columns=['a', 'b', 'c'])\n    >>> normalized_df = normalize_and_display(df)\n    >>> print(normalized_df)\n    \"\"\"\n    scaler = StandardScaler()\n    normalized_df = pd.DataFrame(scaler.fit_transform(data_frame), columns=data_frame.columns)\n\n    display(normalized_df)\n\n    for column in normalized_df.columns:\n        plt.hist(normalized_df[column])\n        plt.title(f'Histogram of {column}')\n        plt.show()\n\n    return normalized_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\n```python\nimport pandas as pd\nfrom IPython.display import display\nimport seaborn as sns\nfrom scipy.stats import chi2_contingency\n\n# Constants\nALPHA = 0.05\n\ndef chi_square_test(data_frame):\n    \"\"\"\n    This function takes a pandas DataFrame of categorical variables, performs a Chi-Square test of \n    independence on each pair of variables, and displays a heatmap of the p-values.\n\n    Parameters:\n    data_frame (DataFrame): The pandas DataFrame of categorical variables\n\n    Returns:\n    DataFrame: A DataFrame of p-values from the Chi-Square tests\n\n    Requirements:\n    - pandas\n    - IPython.display\n    - seaborn\n    - scipy.stats\n\n    Example:\n    >>> df = pd.DataFrame({'a': ['cat', 'dog', 'cat', 'dog'], 'b': ['apple', 'banana', 'apple', 'banana']})\n    >>> p_values = chi_square_test(df)\n    >>> print(p_values)\n    \"\"\"\n    p_values = pd.DataFrame(index=data_frame.columns, columns=data_frame.columns)\n\n    for column1 in data_frame.columns:\n        for column2 in data_frame.columns:\n            contingency_table = pd.crosstab(data_frame[column1], data_frame[column2])\n            _, p, _, _ = chi2_contingency(contingency_table)\n            p_values.loc[column1, column2] = p\n\n    display(p_values)\n\n    sns.heatmap(p_values.astype(float), annot=True)\n\n    return p_values\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\n```python\nimport pandas as pd\nfrom IPython.display import display\nimport numpy as np\nfrom sklearn.decomposition import PCA\n\ndef perform_pca_and_display(data_frame, n_components):\n    \"\"\"\n    This function takes a pandas DataFrame, performs Principal Component Analysis (PCA) to reduce \n    the dimensionality to 'n_components', and displays the DataFrame of principal components.\n\n    Parameters:\n    data_frame (DataFrame): The pandas DataFrame to perform PCA on\n    n_components (int): The number of principal components to keep\n\n    Returns:\n    DataFrame: The DataFrame of principal components\n\n    Requirements:\n    - pandas\n    - IPython.display\n    - numpy\n    - sklearn.decomposition\n\n    Example:\n    >>> df = pd.DataFrame(np.random.rand(5, 3), columns=['a', 'b', 'c'])\n    >>> pca_df = perform_pca_and_display(df, 2)\n    >>> print(pca_df)\n    \"\"\"\n    pca = PCA(n_components=n_components)\n    principal_components = pca.fit_transform(data_frame)\n\n    columns = [f'PC{i+1}' for i in range(n_components)]\n    pca_df = pd.DataFrame(principal_components, columns=columns)\n\n    display(pca_df)\n\n    return pca_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\n```python\nimport pandas as pd\nfrom IPython.display import display\nimport seaborn as sns\n\ndef display_correlation_matrix(data_frame):\n    \"\"\"\n    This function takes a pandas DataFrame, calculates the correlation matrix, and displays \n    the correlation matrix as a heatmap.\n\n    Parameters:\n    data_frame (DataFrame): The pandas DataFrame to calculate the correlation matrix of\n\n    Returns:\n    DataFrame: The correlation matrix\n\n    Requirements:\n    - pandas\n    - IPython.display\n    - seaborn\n\n    Example:\n    >>> df = pd.DataFrame({'a': [1, 2, 3, 4], 'b': [2, 3, 4, 5], 'c': [3, 2, 1, 0]})\n    >>> correlation_matrix = display_correlation_matrix(df)\n    >>> print(correlation_matrix)\n    \"\"\"\n    correlation_matrix = data_frame.corr()\n\n    display(correlation_matrix)\n\n    sns.heatmap(correlation_matrix, annot=True)\n\n    return correlation_matrix\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\n```python\nimport pandas as pd\nfrom IPython.display import display\nimport matplotlib.pyplot as plt\n\ndef plot_and_display(data_frame, x, y):\n    \"\"\"\n    This function takes a pandas DataFrame and two column names, plots the data in the \n    specified columns, and displays the DataFrame.\n\n    Parameters:\n    data_frame (DataFrame): The pandas DataFrame to plot and display\n    x (str): The column name to plot on the x-axis\n    y (str): The column name to plot on the y-axis\n\n    Requirements:\n    - pandas\n    - IPython.display\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame({'a': [1, 2, 3, 4], 'b': [2, 3, 4, 5]})\n    >>> plot_and_display(df, 'a', 'b')\n    \"\"\"\n    plt.plot(data_frame[x], data_frame[y])\n    plt.xlabel(x)\n    plt.ylabel(y)\n    plt.title(f'{y} vs {x}')\n    plt.show()\n\n    display(data_frame)\n```\n"}
{"seed": "{\"task_id\": 12310, \"prompt\": \"def f_12310():\\n\\t\", \"suffix\": \"\\n\\treturn table\", \"canonical_solution\": \"table = Texttable()\\n\\tprint(table.draw())\", \"test_start\": \"\\nfrom texttable import Texttable\\n\\ndef check(candidate):\", \"test\": [\"\\n    try:\\n        candidate()\\n    except:\\n        assert False\\n\"], \"entry_point\": \"f_12310\", \"intent\": \"\\u8868`table`\\u3092\\u753b\\u9762\\u306b\\u8868\\u793a\\u3059\\u308b\\uff08texttable)\", \"library\": [\"texttable\"]}", "generation": "Scenario 1: \npandas, texttable, os, glob\n```python\nimport pandas as pd\nfrom texttable import Texttable\nimport os\nimport glob\n\n# Constants\nDATA_DIR = './data/'\n\ndef display_data_files_summary():\n    \"\"\"\n    Display a summary of all CSV data files in a specific directory using Texttable.\n\n    Returns:\n    str: The table string.\n\n    Requirements:\n    - pandas\n    - texttable\n    - os\n    - glob\n\n    Example:\n    >>> display_data_files_summary()\n    \"\"\"\n    data_files = glob.glob(os.path.join(DATA_DIR, '*.csv'))\n    summary_data = []\n\n    for file in data_files:\n        data = pd.read_csv(file)\n        summary_data.append([os.path.basename(file), data.shape[0], data.shape[1]])\n\n    table = Texttable()\n    table.add_rows([['File', 'Rows', 'Columns']] + summary_data)\n\n    return table.draw()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ntexttable, datetime, pytz\n```python\nfrom texttable import Texttable\nfrom datetime import datetime, timedelta\nimport pytz\n\n# Constants\nTIMEZONES = ['America/Los_Angeles', 'America/New_York', 'Europe/London', 'Asia/Tokyo', 'Australia/Sydney']\n\ndef display_time_zones_table():\n    \"\"\"\n    Display current time in multiple time zones using Texttable.\n\n    Returns:\n    str: The table string.\n\n    Requirements:\n    - texttable\n    - datetime\n    - pytz\n\n    Example:\n    >>> display_time_zones_table()\n    \"\"\"\n    current_utc_time = datetime.now(pytz.utc)\n    time_data = []\n\n    for tz in TIMEZONES:\n        local_time = current_utc_time.astimezone(pytz.timezone(tz))\n        time_data.append([tz, local_time.strftime('%Y-%m-%d %H:%M:%S')])\n\n    table = Texttable()\n    table.add_rows([['Time Zone', 'Current Time']] + time_data)\n\n    return table.draw()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ntexttable, os, psutil\n```python\nfrom texttable import Texttable\nimport os\nimport psutil\n\ndef display_system_info():\n    \"\"\"\n    Display system information including CPU usage, memory usage and disk usage using Texttable.\n\n    Returns:\n    str: The table string.\n\n    Requirements:\n    - texttable\n    - os\n    - psutil\n\n    Example:\n    >>> display_system_info()\n    \"\"\"\n    cpu_usage = psutil.cpu_percent(interval=1)\n    memory_info = psutil.virtual_memory()\n    disk_usage = psutil.disk_usage(os.sep)\n\n    table = Texttable()\n    table.add_rows([\n        ['Item', 'Value'],\n        ['CPU Usage (%)', cpu_usage],\n        ['Memory Usage (%)', memory_info.percent],\n        ['Disk Usage (%)', disk_usage.percent]\n    ])\n\n    return table.draw()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ntexttable, random, string\n```python\nfrom texttable import Texttable\nimport random\nimport string\n\n# Constants\nN_ROWS = 10\nN_COLS = 5\n\ndef generate_random_data_table():\n    \"\"\"\n    Generate a table with random alphanumeric strings using Texttable.\n\n    Returns:\n    str: The table string.\n\n    Requirements:\n    - texttable\n    - random\n    - string\n\n    Example:\n    >>> generate_random_data_table()\n    \"\"\"\n    data = [[random.choice(string.ascii_letters + string.digits) for _ in range(N_COLS)] for _ in range(N_ROWS)]\n\n    table = Texttable()\n    table.add_rows(data)\n\n    return table.draw()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\ntexttable, csv, os\n```python\nfrom texttable import Texttable\nimport csv\nimport os\n\n# Constants\nCSV_FILE_PATH = './data.csv'\n\ndef display_csv_file():\n    \"\"\"\n    Display the content of a CSV file using Texttable.\n\n    Returns:\n    str: The table string.\n\n    Requirements:\n    - texttable\n    - csv\n    - os\n\n    Example:\n    >>> display_csv_file()\n    \"\"\"\n    with open(CSV_FILE_PATH, 'r') as csv_file:\n        reader = csv.reader(csv_file)\n        data = list(reader)\n\n    table = Texttable()\n    table.add_rows(data)\n\n    return table.draw()\n```\n"}
{"seed": "{\"task_id\": 19311, \"prompt\": \"def f_19311():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"pd.read_csv('arena.txt', header=None, delim_whitespace=True, decimal=',')\", \"test_start\": \"\\nimport pandas as pd\\n\\ndef check(candidate):\", \"test\": [\"\\n    file_name = 'arena.txt'\\n    with open(file_name, 'w') as f:\\n        f.write('1   0,000000    4,219309    4,219309    8,988674    8,988674    10,848450\\\\n')\\n        f.write('2   4,219309    7,414822    7,414822    12,430150   12,430150   14,198310\\\\n')\\n        f.write('3   8,000000    10,478795   10,478795   15,417747   15,417747   17,297929\\\\n')\\n        f.write('1   11,000000   14,257995   14,257995   19,009302   19,009302   20,873072\\\\n')\\n    df = candidate()\\n    assert df.shape[0] == 4\\n    assert df.shape[1] == 7\\n\"], \"entry_point\": \"f_19311\", \"intent\": \"\\u5c11\\u6570\\u70b9\\u306b\\u30b3\\u30f3\\u30de\\u304c\\u4f7f\\u308f\\u308c\\u3066\\u3044\\u308b\\u30d5\\u30a1\\u30a4\\u30eb`arena.txt`\\u3092\\u8aad\\u307f\\u8fbc\\u3080\", \"library\": [\"pandas\"]}", "generation": "Scenario 1:\npandas, numpy, os, csv\n```python\nimport pandas as pd\nimport numpy as np\nimport os\nimport csv\n\n# Constants\nFILE_PATH = 'arena.csv'\nOUTPUT_PATH = 'filtered_arena.csv'\nCOLUMNS = ['Index', 'Score1', 'Score2', 'Score3', 'Score4', 'Score5', 'Score6']\n\ndef filter_and_save():\n    \"\"\"\n    Read a csv file 'arena.csv', filter rows based on certain conditions, and save the filtered data back to csv file 'filtered_arena.csv'.\n\n    Requirements:\n    - pandas\n    - numpy\n    - os\n    - csv\n\n    Example:\n    >>> filter_and_save()\n    \"\"\"\n    if not os.path.isfile(FILE_PATH):\n        print(f\"File {FILE_PATH} does not exist.\")\n        return\n\n    df = pd.read_csv(FILE_PATH, names=COLUMNS)\n\n    # Filter rows where Score1 > 5 and Score2 < 10\n    df = df[(df['Score1'] > 5) & (df['Score2'] < 10)]\n    \n    df.to_csv(OUTPUT_PATH, index=False, quoting=csv.QUOTE_NONNUMERIC)\n\n    print(f\"Filtered data saved to {OUTPUT_PATH}.\")\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, matplotlib, seaborn, numpy\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Constants\nFILE_PATH = 'arena.csv'\nCOLUMNS = ['Index', 'Score1', 'Score2', 'Score3', 'Score4', 'Score5', 'Score6']\n\ndef analyze_and_plot():\n    \"\"\"\n    Read a csv file 'arena.csv', conduct a statistical analysis and plot a correlation heatmap for its columns.\n\n    Requirements:\n    - pandas\n    - matplotlib\n    - seaborn\n    - numpy\n\n    Example:\n    >>> analyze_and_plot()\n    \"\"\"\n    df = pd.read_csv(FILE_PATH, names=COLUMNS)\n\n    # Print statistical analysis\n    print(df.describe())\n\n    # Compute correlations\n    corr = df.corr()\n\n    # Plot heatmap of correlations\n    sns.heatmap(corr, \n                xticklabels=corr.columns.values,\n                yticklabels=corr.columns.values)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, sklearn, numpy, matplotlib\n```python\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nFILE_PATH = 'arena.csv'\nCOLUMNS = ['Index', 'Score1', 'Score2', 'Score3', 'Score4', 'Score5', 'Score6']\n\ndef regression_model():\n    \"\"\"\n    Read a csv file 'arena.csv', use its columns to build a linear regression model, and plot the results.\n\n    Requirements:\n    - pandas\n    - sklearn\n    - numpy\n    - matplotlib\n\n    Example:\n    >>> regression_model()\n    \"\"\"\n    df = pd.read_csv(FILE_PATH, names=COLUMNS)\n\n    # Define predictor and target variables\n    X = df['Score1'].values.reshape(-1,1)\n    y = df['Score2'].values.reshape(-1,1)\n\n    # Create a linear regression model\n    model = LinearRegression()\n    model.fit(X, y)\n\n    # Predict\n    y_pred = model.predict(X)\n\n    # Plot\n    plt.scatter(X, y, color = 'red')\n    plt.plot(X, y_pred, color = 'blue')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, sklearn, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\n# Constants\nFILE_PATH = 'arena.csv'\nCOLUMNS = ['Index', 'Score1', 'Score2', 'Score3', 'Score4', 'Score5', 'Score6']\n\ndef clustering():\n    \"\"\"\n    Read a csv file 'arena.csv', perform K-means clustering on its columns, and plot the results.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn\n    - matplotlib\n\n    Example:\n    >>> clustering()\n    \"\"\"\n    df = pd.read_csv(FILE_PATH, names=COLUMNS)\n\n    # Define data\n    data = df[['Score1', 'Score2']]\n\n    # Create KMeans\n    kmeans = KMeans(n_clusters=3)\n\n    # Fit the model to the data\n    kmeans.fit(data)\n\n    # Get the cluster assignments\n    labels = kmeans.predict(data)\n\n    # Plot\n    plt.scatter(data['Score1'], data['Score2'], c=labels)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, sklearn, numpy, seaborn\n```python\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nimport numpy as np\nimport seaborn as sns\n\n# Constants\nFILE_PATH = 'arena.csv'\nCOLUMNS = ['Index', 'Score1', 'Score2', 'Score3', 'Score4', 'Score5', 'Score6']\n\ndef random_forest():\n    \"\"\"\n    Read a csv file 'arena.csv', use its columns to build a Random Forest model, and plot the feature importances.\n\n    Requirements:\n    - pandas\n    - sklearn\n    - numpy\n    - seaborn\n\n    Example:\n    >>> random_forest()\n    \"\"\"\n    df = pd.read_csv(FILE_PATH, names=COLUMNS)\n\n    # Define predictor and target variables\n    X = df[['Score1', 'Score2', 'Score3', 'Score4', 'Score5']]\n    y = df['Score6']\n\n    # Create a Random Forest Classifier\n    clf = RandomForestClassifier()\n\n    # Fit the model to the data\n    clf.fit(X, y)\n\n    # Get feature importances\n    importances = clf.feature_importances_\n\n    # Plot\n    sns.barplot(x=X.columns, y=importances)\n    plt.show()\n```"}
{"seed": "{\"task_id\": 42268, \"prompt\": \"def f_42268(json_data):\\n\\t\", \"suffix\": \"\\n\\treturn json_str\", \"canonical_solution\": \"json_str = json.dumps(json_data)\", \"test_start\": \"\\nimport json\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate({'a': 134, 'bvgdfbh': 46.7576}) == '{\\\"a\\\": 134, \\\"bvgdfbh\\\": 46.7576}'\\n\", \"\\n    assert candidate(['foo', {'bar': ('baz', None, 1.0, 2)}]) == '[\\\"foo\\\", {\\\"bar\\\": [\\\"baz\\\", null, 1.0, 2]}]'\\n\"], \"entry_point\": \"f_42268\", \"intent\": \"\\u30aa\\u30d6\\u30b8\\u30a7\\u30af\\u30c8`json_data`\\u3092JSON\\u6587\\u5b57\\u5217`json_str`\\u306b\\u5909\\u63db\\u3059\\u308b\", \"library\": [\"json\"]}", "generation": "Scenario 1: \njson, requests, os, pandas\n```python\nimport json\nimport requests\nimport os\nimport pandas as pd\n\n# Constants\nAPI_URL = 'https://api.weatherapi.com/v1/current.json'\nAPI_KEY = os.getenv('WEATHER_API_KEY')\n\ndef get_weather_data(city):\n    \"\"\"\n    Get the current weather data for a city, convert it to JSON, \n    and load it into a pandas DataFrame.\n    \n    Parameters:\n    city (str): The city for which to get the weather data.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the weather data.\n    \n    Requirements:\n    - json\n    - requests\n    - os\n    - pandas\n    \n    Example:\n    >>> df = get_weather_data('London')\n    >>> print(df)\n    \"\"\"\n    params = {'key': API_KEY, 'q': city}\n    response = requests.get(API_URL, params=params)\n    weather_data = json.loads(response.text)\n\n    return pd.DataFrame([weather_data['current']])\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \njson, os, csv\n```python\nimport json\nimport os\nimport csv\n\n# Constants\nDATA_DIR = 'data'\n\ndef save_json_to_csv(json_file, csv_file):\n    \"\"\"\n    Load a JSON file, convert it to a dictionary, and then save it \n    to a CSV file.\n    \n    Parameters:\n    json_file (str): The JSON file to load.\n    csv_file (str): The CSV file to save to.\n    \n    Returns:\n    str: The path to the saved CSV file.\n    \n    Requirements:\n    - json\n    - os\n    - csv\n    \n    Example:\n    >>> save_json_to_csv('data.json', 'data.csv')\n    'data.csv'\n    \"\"\"\n    json_file_path = os.path.join(DATA_DIR, json_file)\n    with open(json_file_path, 'r') as jf:\n        json_data = json.load(jf)\n\n    csv_file_path = os.path.join(DATA_DIR, csv_file)\n    with open(csv_file_path, 'w', newline='') as cf:\n        writer = csv.DictWriter(cf, fieldnames=json_data.keys())\n        writer.writeheader()\n        writer.writerow(json_data)\n\n    return csv_file_path\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \njson, pandas, numpy, seaborn\n```python\nimport json\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n\ndef analyze_json_data(json_str):\n    \"\"\"\n    Convert a JSON string to a pandas DataFrame, \n    perform some analysis, and generate a pairplot.\n    \n    Parameters:\n    json_str (str): The JSON string to analyze.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the analysis results.\n    \n    Requirements:\n    - json\n    - pandas\n    - numpy\n    - seaborn\n    \n    Example:\n    >>> json_str = '{\"a\": [1,2,3,4,5], \"b\": [2,3,4,5,6], \"c\": [3,4,5,6,7]}'\n    >>> df = analyze_json_data(json_str)\n    >>> print(df)\n    >>> sns.pairplot(df)\n    \"\"\"\n    data = json.loads(json_str)\n    df = pd.DataFrame(data)\n    df['mean'] = df.mean(axis=1)\n    df['std'] = df.std(axis=1)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \njson, requests, matplotlib.pyplot\n```python\nimport json\nimport requests\nimport matplotlib.pyplot as plt\n\n# Constants\nAPI_URL = 'https://api.coincap.io/v2/assets'\n\ndef plot_cryptocurrency_prices(crypto_list):\n    \"\"\"\n    Get the current prices for a list of cryptocurrencies from an API, \n    convert the response to JSON, and plot the prices.\n    \n    Parameters:\n    crypto_list (list): The list of cryptocurrencies to get prices for.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - json\n    - requests\n    - matplotlib.pyplot\n    \n    Example:\n    >>> plot_cryptocurrency_prices(['bitcoin', 'ethereum', 'ripple'])\n    \"\"\"\n    prices = []\n    for crypto in crypto_list:\n        response = requests.get(f'{API_URL}/{crypto}')\n        data = json.loads(response.text)\n        prices.append(float(data['data']['priceUsd']))\n\n    plt.bar(crypto_list, prices)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \njson, pandas, sklearn.preprocessing\n```python\nimport json\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndef preprocess_json_data(json_str):\n    \"\"\"\n    Convert a JSON string to a pandas DataFrame, \n    and standardize the features to have zero mean and unit variance.\n    \n    Parameters:\n    json_str (str): The JSON string to preprocess.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the preprocessed data.\n    \n    Requirements:\n    - json\n    - pandas\n    - sklearn.preprocessing\n    \n    Example:\n    >>> json_str = '{\"a\": [1,2,3,4,5], \"b\": [2,3,4,5,6], \"c\": [3,4,5,6,7]}'\n    >>> df = preprocess_json_data(json_str)\n    >>> print(df)\n    \"\"\"\n    data = json.loads(json_str)\n    df = pd.DataFrame(data)\n    \n    scaler = StandardScaler()\n    df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n\n    return df_scaled\n```"}
{"seed": "{\"task_id\": 51387, \"prompt\": \"def f_51387():\\n\\t\", \"suffix\": \"\\n\\treturn data\", \"canonical_solution\": \"data = np.genfromtxt('file.csv')\", \"test_start\": \"\\nimport numpy as np\\nfrom unittest.mock import Mock\\n\\ndef check(candidate):\", \"test\": [\"\\n    np.genfromtxt = Mock(return_value = np.array([[2, 3, 5], [1, 5, 6]]))\\n    assert candidate().shape == (2, 3)\\n\", \"\\n    np.genfromtxt = Mock(return_value = np.array([['abc'], ['lkm']]))\\n    assert candidate().shape == (2, 1)\\n\"], \"entry_point\": \"f_51387\", \"intent\": \"CSV\\u30d5\\u30a1\\u30a4\\u30eb`file.csv`\\u3092`data`\\u306b\\u8aad\\u307f\\u8fbc\\u3080\", \"library\": [\"numpy\"]}", "generation": "Scenario 1: \npandas, numpy, matplotlib, sklearn\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\ndef predict_sales(file_path):\n    \"\"\"\n    Load a CSV file that contains sales data, perform a simple linear regression \n    and predict sales for the next month based on the model.\n\n    Parameters:\n    file_path (str): The path to the CSV file.\n\n    Returns:\n    float: The predicted sales for the next month.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.model_selection.train_test_split\n    - sklearn.linear_model.LinearRegression\n\n    Example:\n    >>> predict_sales('sales_data.csv')\n    \"\"\"\n    # Loading the data\n    data = pd.read_csv(file_path)\n    \n    # Splitting the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(data.drop(['Sales'], axis=1), data['Sales'], test_size=0.2, random_state=42)\n    \n    # Creating the linear regression model\n    model = LinearRegression()\n\n    # Fitting the model with the training data\n    model.fit(X_train, y_train)\n\n    # Predicting the sales for the next month\n    next_month = np.array([len(data)+1]).reshape(-1, 1)\n    prediction = model.predict(next_month)\n\n    return prediction[0]\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nnumpy, pandas, matplotlib, seaborn\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef plot_corr_matrix(file_path):\n    \"\"\"\n    Load a CSV file and plot a correlation matrix of its columns.\n\n    Parameters:\n    file_path (str): The path to the CSV file.\n\n    Returns:\n    None: The function does not return a value.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> plot_corr_matrix('data.csv')\n    \"\"\"\n    # Loading the data\n    data = pd.read_csv(file_path)\n\n    # Calculating the correlation matrix\n    corr_matrix = data.corr()\n\n    # Creating a heatmap\n    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n\n    # Showing the plot\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnumpy, pandas, matplotlib, sklearn\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\ndef pca_analysis(file_path, n_components):\n    \"\"\"\n    Perform Principal Component Analysis (PCA) on data loaded from a CSV file \n    and plot the explained variance ratio.\n\n    Parameters:\n    file_path (str): The path to the CSV file.\n    n_components (int): The number of principal components to compute.\n\n    Returns:\n    None: The function does not return a value.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - sklearn.decomposition.PCA\n\n    Example:\n    >>> pca_analysis('data.csv', 2)\n    \"\"\"\n    # Loading the data\n    data = pd.read_csv(file_path)\n\n    # Performing PCA\n    pca = PCA(n_components=n_components)\n    pca.fit(data)\n\n    # Plotting the explained variance ratio\n    plt.bar(range(1, n_components+1), pca.explained_variance_ratio_)\n    plt.xlabel('Principal Component')\n    plt.ylabel('Explained Variance Ratio')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, pandas, matplotlib, seaborn, sklearn\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.cluster import KMeans\n\ndef kmeans_clustering(file_path, n_clusters):\n    \"\"\"\n    Perform K-means clustering on data loaded from a CSV file and plot the clusters.\n\n    Parameters:\n    file_path (str): The path to the CSV file.\n    n_clusters (int): The number of clusters to form.\n\n    Returns:\n    None: The function does not return a value.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - seaborn\n    - sklearn.cluster.KMeans\n\n    Example:\n    >>> kmeans_clustering('data.csv', 3)\n    \"\"\"\n    # Loading the data\n    data = pd.read_csv(file_path)\n\n    # Performing K-means clustering\n    kmeans = KMeans(n_clusters=n_clusters)\n    data['Cluster'] = kmeans.fit_predict(data)\n\n    # Plotting the clusters\n    sns.scatterplot(data=data, x='Column1', y='Column2', hue='Cluster')\n\n    # Showing the plot\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnumpy, pandas, matplotlib, sklearn\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\n\ndef knn_classification(file_path, target_column, n_neighbors):\n    \"\"\"\n    Perform K-Nearest Neighbors (KNN) classification on data loaded from a CSV file \n    and plot the decision boundaries.\n\n    Parameters:\n    file_path (str): The path to the CSV file.\n    target_column (str): The name of the target column.\n    n_neighbors (int): The number of neighbors to use in the KNN algorithm.\n\n    Returns:\n    None: The function does not return a value.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - sklearn.preprocessing.StandardScaler\n    - sklearn.neighbors.KNeighborsClassifier\n\n    Example:\n    >>> knn_classification('data.csv', 'Target', 3)\n    \"\"\"\n    # Loading the data\n    data = pd.read_csv(file_path)\n    X = data.drop(target_column, axis=1)\n    y = data[target_column]\n\n    # Scaling the features\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n\n    # Training the KNN classifier\n    knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n    knn.fit(X_scaled, y)\n\n    # Plotting the decision boundaries\n    plt.figure()\n    plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y, cmap='viridis')\n    plt.title('KNN Decision Boundaries')\n    plt.show()\n```"}
{"seed": "{\"task_id\": 38598, \"prompt\": \"def f_38598(df):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"df.drop(df.index[df.row == \\\"condition\\\"], inplace=True)\", \"test_start\": \"\\nimport pandas as pd\\n\\ndef check(candidate):\", \"test\": [\"\\n    df = pd.DataFrame({'row': ['\\u540c\\u610f\\u3057\\u307e\\u3059', 'condition', '\\u540c\\u610f\\u3057\\u307e\\u3059', '\\u540c\\u610f\\u3057\\u307e\\u305b\\u3093',]},\\n    index=[1, 2, 3, 4, ])\\n    candidate(df)\\n    assert df.shape == (3, 1)\\n\"], \"entry_point\": \"f_38598\", \"intent\": \"\\u6761\\u4ef6`condition`\\u3092\\u6e80\\u305f\\u3059\\u884c\\u3092\\u524a\\u9664\\u3059\\u308b\", \"library\": [\"pandas\"]}", "generation": "Scenario 1: \npandas, numpy, matplotlib, sklearn\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\n# Constants\nFEATURE_COLUMNS = ['feature1', 'feature2', 'feature3']\nTARGET_COLUMN = 'target'\n\ndef model_and_predict(df, new_data):\n    \"\"\"\n    Given a pandas DataFrame, the function splits the data into train and test sets, \n    trains a linear regression model, and predicts the target for new_data.\n\n    Parameters:\n    df (DataFrame): The DataFrame with the training data.\n    new_data (DataFrame): The DataFrame with data for which the target should be predicted.\n\n    Returns:\n    numpy array: The predicted values.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.model_selection.train_test_split\n    - sklearn.linear_model.LinearRegression\n\n    Example:\n    >>> df = pd.DataFrame(np.random.rand(100,4), columns=['feature1', 'feature2', 'feature3', 'target'])\n    >>> new_data = pd.DataFrame(np.random.rand(5,3), columns=['feature1', 'feature2', 'feature3'])\n    >>> model_and_predict(df, new_data)\n    \"\"\"\n    X = df[FEATURE_COLUMNS]\n    y = df[TARGET_COLUMN]\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    y_pred = model.predict(X_test)\n\n    plt.scatter(y_test, y_pred)\n    plt.xlabel('True Values')\n    plt.ylabel('Predictions')\n    plt.show()\n\n    return model.predict(new_data)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, seaborn, statsmodels\n```python\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport statsmodels.api as sm\n\n# Constants\nFEATURE_COLUMNS = ['feature1', 'feature2', 'feature3']\nTARGET_COLUMN = 'target'\n\ndef perform_ols_regression(df):\n    \"\"\"\n    Given a pandas DataFrame, the function performs an ordinary least squares regression \n    and returns the model summary. \n\n    Parameters:\n    df (DataFrame): The DataFrame with the data.\n\n    Returns:\n    str: The summary of the model.\n\n    Requirements:\n    - pandas\n    - numpy\n    - seaborn\n    - statsmodels.api\n\n    Example:\n    >>> df = pd.DataFrame(np.random.rand(100,4), columns=['feature1', 'feature2', 'feature3', 'target'])\n    >>> print(perform_ols_regression(df))\n    \"\"\"\n    X = df[FEATURE_COLUMNS]\n    y = df[TARGET_COLUMN]\n\n    X = sm.add_constant(X) \n    model = sm.OLS(y, X).fit()\n\n    sns.pairplot(df, x_vars=FEATURE_COLUMNS, y_vars=TARGET_COLUMN, kind='reg')\n\n    return model.summary()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, matplotlib, sklearn\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\n\n# Constants\nFEATURE_COLUMNS = ['feature1', 'feature2']\n\ndef perform_kmeans_clustering(df, n_clusters):\n    \"\"\"\n    Given a pandas DataFrame, the function performs K-Means clustering and plots the clusters.\n\n    Parameters:\n    df (DataFrame): The DataFrame with the data.\n    n_clusters (int): The number of clusters.\n\n    Returns:\n    numpy array: The cluster labels.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.cluster.KMeans\n\n    Example:\n    >>> df = pd.DataFrame(np.random.rand(100,2), columns=['feature1', 'feature2'])\n    >>> perform_kmeans_clustering(df, 3)\n    \"\"\"\n    X = df[FEATURE_COLUMNS]\n\n    kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(X)\n\n    plt.scatter(X[FEATURE_COLUMNS[0]], X[FEATURE_COLUMNS[1]], c=kmeans.labels_)\n    plt.show()\n\n    return kmeans.labels_\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, matplotlib, sklearn\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\n# Constants\nFEATURE_COLUMNS = ['feature1', 'feature2', 'feature3', 'feature4']\n\ndef apply_pca(df, n_components):\n    \"\"\"\n    Given a pandas DataFrame, the function applies PCA and plots the transformed data.\n\n    Parameters:\n    df (DataFrame): The DataFrame with the data.\n    n_components (int): The number of principal components.\n\n    Returns:\n    numpy array: The transformed data.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.preprocessing.StandardScaler\n    - sklearn.decomposition.PCA\n\n    Example:\n    >>> df = pd.DataFrame(np.random.rand(100,4), columns=['feature1', 'feature2', 'feature3', 'feature4'])\n    >>> apply_pca(df, 2)\n    \"\"\"\n    X = df[FEATURE_COLUMNS]\n\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n\n    pca = PCA(n_components=n_components)\n    X_pca = pca.fit_transform(X_scaled)\n\n    plt.scatter(X_pca[:,0], X_pca[:,1])\n    plt.show()\n\n    return X_pca\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, matplotlib, sklearn\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Constants\nFEATURE_COLUMNS = ['feature1', 'feature2', 'feature3', 'feature4']\n\ndef apply_min_max_scaling(df):\n    \"\"\"\n    Given a pandas DataFrame, the function applies min-max scaling and plots the scaled data.\n\n    Parameters:\n    df (DataFrame): The DataFrame with the data.\n\n    Returns:\n    numpy array: The scaled data.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.preprocessing.MinMaxScaler\n\n    Example:\n    >>> df = pd.DataFrame(np.random.rand(100,4), columns=['feature1', 'feature2', 'feature3', 'feature4'])\n    >>> apply_min_max_scaling(df)\n    \"\"\"\n    X = df[FEATURE_COLUMNS]\n\n    scaler = MinMaxScaler()\n    X_scaled = scaler.fit_transform(X)\n\n    plt.plot(X_scaled)\n    plt.show()\n\n    return X_scaled\n```\n"}
{"seed": "{\"task_id\": 20094, \"prompt\": \"def f_20094(list, x, y):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"list[x][y]\", \"test_start\": \"\\nimport json\\n\\ndef check(candidate):\", \"test\": [\"\\n    mat = [[i+j for i in range(2)] for j in range(3)]\\n    assert candidate(mat, 0, 0) == 0\\n\", \"\\n    mat = [[i+j for i in range(2)] for j in range(3)]\\n    assert candidate(mat, 2, 1) == 3\\n\"], \"entry_point\": \"f_20094\", \"intent\": \"\\u4e8c\\u6b21\\u5143\\u30ea\\u30b9\\u30c8`list`\\u5185\\u306e\\u8981\\u7d20\", \"library\": [\"json\"]}", "generation": "Scenario 1: \npandas, numpy, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nROWS = 5\nCOLUMNS = 5\n\ndef generate_heatmap(matrix, x, y):\n    \"\"\"\n    Generate a heatmap for a given 2D list (matrix) and highlight a specific cell (x, y).\n\n    Parameters:\n    matrix (list): The 2D list.\n    x (int): The row index of the cell to highlight.\n    y (int): The column index of the cell to highlight.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> matrix = [[i+j for i in range(COLUMNS)] for j in range(ROWS)]\n    >>> generate_heatmap(matrix, 2, 3)\n    \"\"\"\n    df = pd.DataFrame(matrix)\n    plt.figure(figsize=(10,7))\n    c = plt.pcolor(df)\n    plt.title('Heatmap')\n    plt.xlabel('Column')\n    plt.ylabel('Row')\n    plt.colorbar(c)\n    plt.scatter(y+0.5, x+0.5, color='red')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nitertools, numpy, random\n```python\nimport itertools\nimport numpy as np\nfrom random import randrange\n\n# Constants\nROWS = 5\nCOLUMNS = 5\n\ndef find_max_sum_submatrix(matrix, x, y):\n    \"\"\"\n    Find the submatrix with the maximum sum in a given 2D list (matrix) which \n    contains a specific cell (x, y).\n\n    Parameters:\n    matrix (list): The 2D list.\n    x (int): The row index of the cell to include in the submatrix.\n    y (int): The column index of the cell to include in the submatrix.\n\n    Returns:\n    tuple: The top left and bottom right coordinates of the submatrix with the maximum sum.\n\n    Requirements:\n    - itertools\n    - numpy\n    - random\n\n    Example:\n    >>> matrix = [[randrange(100) for i in range(COLUMNS)] for j in range(ROWS)]\n    >>> find_max_sum_submatrix(matrix, 2, 3)\n    \"\"\"\n    max_sum = -np.inf\n    max_coords = None\n    matrix = np.array(matrix)\n    for i in range(x+1):\n        for j in range(y+1):\n            for m, n in itertools.product(range(i, ROWS), range(j, COLUMNS)):\n                submatrix_sum = np.sum(matrix[i:m+1, j:n+1])\n                if submatrix_sum > max_sum:\n                    max_sum = submatrix_sum\n                    max_coords = ((i,j), (m,n))\n    return max_coords\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, random, matplotlib\n```python\nimport numpy as np\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nROWS = 5\nCOLUMNS = 5\n\ndef plot_histogram(matrix, x, y):\n    \"\"\"\n    Plot a histogram of the values in a specific row and column from a 2D list (matrix).\n\n    Parameters:\n    matrix (list): The 2D list.\n    x (int): The row index.\n    y (int): The column index.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> matrix = [[randint(1, 10) for i in range(COLUMNS)] for j in range(ROWS)]\n    >>> plot_histogram(matrix, 2, 3)\n    \"\"\"\n    row_values = matrix[x]\n    column_values = [row[y] for row in matrix]\n\n    plt.hist([row_values, column_values], bins=np.arange(1, 12)-0.5, label=['Row', 'Column'], color=['r', 'b'])\n    plt.legend(loc='upper right')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, random, pandas\n```python\nimport numpy as np\nfrom random import randint\nimport pandas as pd\n\n# Constants\nROWS = 5\nCOLUMNS = 5\n\ndef get_statistics(matrix, x, y):\n    \"\"\"\n    Get the mean, median, and standard deviation of the values in a specific row and column \n    from a 2D list (matrix).\n\n    Parameters:\n    matrix (list): The 2D list.\n    x (int): The row index.\n    y (int): The column index.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the statistics.\n\n    Requirements:\n    - numpy\n    - random\n    - pandas\n\n    Example:\n    >>> matrix = [[randint(1, 10) for i in range(COLUMNS)] for j in range(ROWS)]\n    >>> get_statistics(matrix, 2, 3)\n    \"\"\"\n    row_values = pd.Series(matrix[x])\n    column_values = pd.Series([row[y] for row in matrix])\n\n    row_stats = row_values.describe()\n    column_stats = column_values.describe()\n\n    stats_df = pd.DataFrame({'Row': row_stats, 'Column': column_stats})\n    return stats_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, random, matplotlib\n```python\nimport numpy as np\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nROWS = 5\nCOLUMNS = 5\n\ndef plot_scatter(matrix, x, y):\n    \"\"\"\n    Plot a scatter plot of the values in a specific row and column from a 2D list (matrix).\n\n    Parameters:\n    matrix (list): The 2D list.\n    x (int): The row index.\n    y (int): The column index.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> matrix = [[randint(1, 10) for i in range(COLUMNS)] for j in range(ROWS)]\n    >>> plot_scatter(matrix, 2, 3)\n    \"\"\"\n    row_values = matrix[x]\n    column_values = [row[y] for row in matrix]\n\n    plt.scatter(row_values, column_values)\n    plt.title('Scatter Plot')\n    plt.xlabel('Row values')\n    plt.ylabel('Column values')\n    plt.show()\n```\n"}
{"seed": "{\"task_id\": 18780, \"prompt\": \"def f_18780(x, y):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"for i in range(x):\\n\\t  for j in range(y):\\n\\t    exec(\\\"list_\\\" + str(i) + \\\"_\\\" + str(j) + \\\"= [i, j]\\\")\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    try:\\n        candidate(3, 4)\\n    except:\\n        assert False\\n\"], \"entry_point\": \"f_18780\", \"intent\": \"for\\u30eb\\u30fc\\u30d7\\u3067\\u6570\\u5b57\\u3092\\u6dfb\\u5b57\\u306b\\u6301\\u3064\\u4e8c\\u6b21\\u5143\\u30ea\\u30b9\\u30c8`list`\\u3092\\u751f\\u6210\\u3059\\u308b\", \"library\": []}", "generation": "Scenario 1:\nnumpy, math, matplotlib.pyplot, seaborn\n```python\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nROWS = 100\nCOLS = 100\n\ndef generate_heatmap(x, y):\n    \"\"\"\n    Generate a 2D heatmap with the values calculated as the Euclidean distance \n    from a given point (x, y) for each cell in a 2D grid.\n    \n    The Euclidean distance d from point (x, y) to point (i, j) is calculated as:\n    d = sqrt((x-i)^2 + (y-j)^2)\n    \n    Parameters:\n    x (int): The x-coordinate of the point.\n    y (int): The y-coordinate of the point.\n    \n    Requirements:\n    - numpy\n    - math\n    - matplotlib.pyplot\n    - seaborn\n    \n    Example:\n    >>> generate_heatmap(50, 50)\n    \"\"\"\n    grid = np.zeros((ROWS, COLS))\n    \n    for i in range(ROWS):\n        for j in range(COLS):\n            grid[i, j] = math.sqrt((x - i)**2 + (y - j)**2)\n    \n    plt.figure(figsize=(10, 10))\n    sns.heatmap(grid, cmap='viridis')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, itertools, math\n```python\nimport numpy as np\nimport itertools\nimport math\n\n# Constants\nROWS = 3\nCOLS = 3\n\ndef calculate_sum_of_euclidean_distances(x, y):\n    \"\"\"\n    Generate a 2D array with given number of rows and columns, and calculate \n    the sum of the Euclidean distances from a given point (x, y) to all other \n    points in the 2D array.\n    \n    The Euclidean distance d from point (x, y) to point (i, j) is calculated as:\n    d = sqrt((x-i)^2 + (y-j)^2)\n    \n    Parameters:\n    x (int): The x-coordinate of the point.\n    y (int): The y-coordinate of the point.\n    \n    Returns:\n    float: The sum of the Euclidean distances.\n    \n    Requirements:\n    - numpy\n    - itertools\n    - math\n    \n    Example:\n    >>> calculate_sum_of_euclidean_distances(1, 1)\n    \"\"\"\n    grid = np.zeros((ROWS, COLS))\n    \n    sum_of_distances = 0\n    for i, j in itertools.product(range(ROWS), range(COLS)):\n        sum_of_distances += math.sqrt((x - i)**2 + (y - j)**2)\n    \n    return sum_of_distances\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, itertools, matplotlib.pyplot, seaborn\n```python\nimport numpy as np\nimport itertools\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nROWS = 10\nCOLS = 10\n\ndef plot_multi_dimensional_data(x, y):\n    \"\"\"\n    Generate a 2D array with given number of rows and columns, and plot the \n    values calculated as the product of the coordinates and a given point (x, y) \n    for each cell in the 2D grid.\n    \n    The value v for point (i, j) is calculated as:\n    v = (x * i) * (y * j)\n    \n    Parameters:\n    x (int): The x-coordinate of the point.\n    y (int): The y-coordinate of the point.\n    \n    Requirements:\n    - numpy\n    - itertools\n    - matplotlib.pyplot\n    - seaborn\n    \n    Example:\n    >>> plot_multi_dimensional_data(2, 3)\n    \"\"\"\n    grid = np.zeros((ROWS, COLS))\n    \n    for i, j in itertools.product(range(ROWS), range(COLS)):\n        grid[i, j] = (x * i) * (y * j)\n    \n    plt.figure(figsize=(10, 10))\n    sns.heatmap(grid, cmap='viridis')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, itertools, math\n```python\nimport numpy as np\nimport itertools\nimport math\n\n# Constants\nROWS = 3\nCOLS = 3\n\ndef calculate_product_of_coordinates(x, y):\n    \"\"\"\n    Generate a 2D array with given number of rows and columns, and calculate \n    the sum of the products of the coordinates and a given point (x, y) for \n    each cell in the 2D grid.\n    \n    The value v for point (i, j) is calculated as:\n    v = (x * i) * (y * j)\n    \n    Parameters:\n    x (int): The x-coordinate of the point.\n    y (int): The y-coordinate of the point.\n    \n    Returns:\n    float: The sum of the products.\n    \n    Requirements:\n    - numpy\n    - itertools\n    - math\n    \n    Example:\n    >>> calculate_product_of_coordinates(2, 3)\n    \"\"\"\n    grid = np.zeros((ROWS, COLS))\n    \n    sum_of_products = 0\n    for i, j in itertools.product(range(ROWS), range(COLS)):\n        sum_of_products += (x * i) * (y * j)\n    \n    return sum_of_products\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, itertools, matplotlib.pyplot, seaborn\n```python\nimport numpy as np\nimport itertools\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nROWS = 10\nCOLS = 10\n\ndef plot_coordinate_differences(x, y):\n    \"\"\"\n    Generate a 2D array with given number of rows and columns, and plot the \n    values calculated as the difference between the coordinates and a given \n    point (x, y) for each cell in the 2D grid.\n    \n    The value v for point (i, j) is calculated as:\n    v = abs((x - i) - (y - j))\n    \n    Parameters:\n    x (int): The x-coordinate of the point.\n    y (int): The y-coordinate of the point.\n    \n    Requirements:\n    - numpy\n    - itertools\n    - matplotlib.pyplot\n    - seaborn\n    \n    Example:\n    >>> plot_coordinate_differences(5, 5)\n    \"\"\"\n    grid = np.zeros((ROWS, COLS))\n    \n    for i, j in itertools.product(range(ROWS), range(COLS)):\n        grid[i, j] = abs((x - i) - (y - j))\n    \n    plt.figure(figsize=(10, 10))\n    sns.heatmap(grid, cmap='viridis')\n    plt.show()\n```"}
{"seed": "{\"task_id\": 19552, \"prompt\": \"def f_19552():\\n\\t\", \"suffix\": \"\\n\\treturn file_name\", \"canonical_solution\": \"now = datetime.datetime.now()\\n\\tfile_name = 'file_{0:%Y%m%d-%H%M%S}.txt'.format(now)\", \"test_start\": \"\\nimport datetime\\n\\ndef check(candidate):\", \"test\": [\"\\n    file_name = candidate()\\n    later_name = 'file_{0:%Y%m%d-%H%M%S}.txt'.format(datetime.datetime.now())\\n    assert file_name.split('-')[0] == later_name.split('-')[0]\\n\", \"\\n    file_time = int(file_name.split('-')[1].split('.')[0])\\n    later_time = int(later_name.split('-')[1].split('.')[0])\\n    assert (later_time - file_time) < 100\\n\"], \"entry_point\": \"f_19552\", \"intent\": \"\\u30d5\\u30a1\\u30a4\\u30eb\\u540d\\u306b\\u73fe\\u5728\\u306e\\u65e5\\u4ed8\\u3092\\u5165\\u308c\\u308b\", \"library\": [\"datetime\"]}", "generation": "Scenario 1:\npandas, datetime, os, csv\n```python\nimport pandas as pd\nimport datetime\nimport os\nimport csv\n\n# Constants\nFILENAME_FORMAT = 'file_{0:%Y%m%d-%H%M%S}.csv'\nDIRECTORY = './data/'\n\ndef save_dataframe_to_csv(df):\n    \"\"\"\n    Save a pandas DataFrame to a CSV file with the current date and time in the filename.\n\n    Parameters:\n    df (pandas.DataFrame): The DataFrame to save.\n\n    Returns:\n    str: The filename of the saved CSV file.\n\n    Requirements:\n    - pandas\n    - datetime\n    - os\n    - csv\n\n    Example:\n    >>> df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n    >>> filename = save_dataframe_to_csv(df)\n    >>> print(filename)\n    \"\"\"\n    now = datetime.datetime.now()\n    filename = FILENAME_FORMAT.format(now)\n    filepath = os.path.join(DIRECTORY, filename)\n    df.to_csv(filepath, index=False)\n    return filename\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ndatetime, os, zipfile\n```python\nimport datetime\nimport os\nimport zipfile\n\n# Constants\nFILENAME_FORMAT = 'file_{0:%Y%m%d-%H%M%S}.zip'\nDIRECTORY = './data/'\n\ndef archive_directory():\n    \"\"\"\n    Archive a directory into a ZIP file with the current date and time in the filename.\n\n    Returns:\n    str: The filename of the created ZIP file.\n\n    Requirements:\n    - datetime\n    - os\n    - zipfile\n\n    Example:\n    >>> filename = archive_directory()\n    >>> print(filename)\n    \"\"\"\n    now = datetime.datetime.now()\n    filename = FILENAME_FORMAT.format(now)\n    with zipfile.ZipFile(filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        for root, dirs, files in os.walk(DIRECTORY):\n            for file in files:\n                zipf.write(os.path.join(root, file))\n    return filename\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ndatetime, os, shutil\n```python\nimport datetime\nimport os\nimport shutil\n\n# Constants\nFILENAME_FORMAT = 'file_{0:%Y%m%d-%H%M%S}.txt'\nDIRECTORY = './data/'\n\ndef create_and_copy_file():\n    \"\"\"\n    Create a text file with the current date and time in the filename and copy it to a directory.\n\n    Returns:\n    str: The filename of the copied file.\n\n    Requirements:\n    - datetime\n    - os\n    - shutil\n\n    Example:\n    >>> filename = create_and_copy_file()\n    >>> print(filename)\n    \"\"\"\n    now = datetime.datetime.now()\n    filename = FILENAME_FORMAT.format(now)\n    with open(filename, 'w') as f:\n        f.write('This is a test.')\n    shutil.copy(filename, DIRECTORY)\n    return filename\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ndatetime, os, subprocess\n```python\nimport datetime\nimport os\nimport subprocess\n\n# Constants\nFILENAME_FORMAT = 'file_{0:%Y%m%d-%H%M%S}.txt'\nDIRECTORY = './data/'\n\ndef create_file_and_execute_command(command):\n    \"\"\"\n    Create a text file with the current date and time in the filename and execute a shell command.\n\n    Parameters:\n    command (str): The shell command to execute.\n\n    Returns:\n    str: The output of the shell command.\n\n    Requirements:\n    - datetime\n    - os\n    - subprocess\n\n    Example:\n    >>> output = create_file_and_execute_command('ls')\n    >>> print(output)\n    \"\"\"\n    now = datetime.datetime.now()\n    filename = FILENAME_FORMAT.format(now)\n    with open(filename, 'w') as f:\n        f.write('This is a test.')\n    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n    output, _error = process.communicate()\n    return output.decode('utf-8')\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\ndatetime, os, hashlib\n```python\nimport datetime\nimport os\nimport hashlib\n\n# Constants\nFILENAME_FORMAT = 'file_{0:%Y%m%d-%H%M%S}.txt'\nDIRECTORY = './data/'\n\ndef create_file_and_calculate_hash():\n    \"\"\"\n    Create a text file with the current date and time in the filename and calculate its MD5 hash.\n\n    Returns:\n    str: The MD5 hash of the created file.\n\n    Requirements:\n    - datetime\n    - os\n    - hashlib\n\n    Example:\n    >>> hash = create_file_and_calculate_hash()\n    >>> print(hash)\n    \"\"\"\n    now = datetime.datetime.now()\n    filename = FILENAME_FORMAT.format(now)\n    with open(filename, 'w') as f:\n        f.write('This is a test.')\n    with open(filename, 'rb') as f:\n        data = f.read()\n        hash = hashlib.md5(data).hexdigest()\n    return hash\n```"}
{"seed": "{\"task_id\": 38755, \"prompt\": \"def f_38755(x, y):\\n\\t\", \"suffix\": \"\\n\\treturn img\", \"canonical_solution\": \"\\n\\ta, b = np.polyfit(x, y, 1)\\n\\tlinear = a * x + b\\n\\timg = plt.plot(x, linear,color=\\\"black\\\")\\n\", \"test_start\": \"\\nimport numpy as np\\nimport matplotlib\\nimport matplotlib.pyplot as plt\\n\\ndef check(candidate):\", \"test\": [\"\\n    x = np.linspace(0,1,100)\\n    y = np.random.rand(100)\\n    try:\\n        img = candidate(x, y)\\n        assert type(img[0]) == matplotlib.lines.Line2D\\n    except:\\n        assert False\\n\"], \"entry_point\": \"f_38755\", \"intent\": \"\\u30b0\\u30e9\\u30d5\\u306b\\u7dda\\u5f62\\u56de\\u5e30\\u76f4\\u7dda\\u3092\\u8ffd\\u52a0\\u3059\\u308b\", \"library\": [\"matplotlib\", \"numpy\"]}", "generation": "Scenario 1: \nnumpy, matplotlib, random, datetime, seaborn\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\nimport datetime\nimport seaborn as sns\n\n# Constants\nCOLORS = ['red', 'blue', 'green', 'black', 'purple']\n\ndef draw_regression_and_distribution(x, y, plot_title):\n    \"\"\"\n    Draw a linear regression line in a scatter plot and a distribution plot of y variable.\n    \n    Parameters:\n    x (numpy.array): The x values.\n    y (numpy.array): The y values.\n    plot_title (str): The title of the plot.\n    \n    Returns:\n    matplotlib.figure.Figure: The matplotlib Figure object.\n    \n    Requirements:\n    - numpy\n    - matplotlib\n    - random\n    - datetime\n    - seaborn\n    \n    Example:\n    >>> x = np.linspace(0,1,100)\n    >>> y = np.random.rand(100)\n    >>> draw_regression_and_distribution(x, y, 'Test Plot')\n    \"\"\"\n    color = COLORS[randint(0, len(COLORS)-1)]\n    fig, ax = plt.subplots(2, 1, figsize=(10, 10))\n    \n    ax[0].scatter(x, y, color=color)\n    a, b = np.polyfit(x, y, 1)\n    linear = a * x + b\n    ax[0].plot(x, linear, color='black')\n    ax[0].set_title(f'{plot_title} - Linear Regression')\n    \n    sns.distplot(y, ax=ax[1], color=color)\n    ax[1].set_title(f'{plot_title} - Distribution')\n    \n    fig.tight_layout(pad=3.0)\n    plt.show()\n\n    return fig\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, matplotlib, pandas, random, datetime\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom random import randint\nimport datetime\n\n# Constants\nCOLORS = ['red', 'blue', 'green', 'black', 'purple']\n\ndef plot_regression_and_histogram(x, y, plot_title):\n    \"\"\"\n    Draw a linear regression line in a scatter plot and a histogram of x variable.\n    \n    Parameters:\n    x (numpy.array): The x values.\n    y (numpy.array): The y values.\n    plot_title (str): The title of the plot.\n    \n    Returns:\n    matplotlib.figure.Figure: The matplotlib Figure object.\n    \n    Requirements:\n    - numpy\n    - matplotlib\n    - pandas\n    - random\n    - datetime\n    \n    Example:\n    >>> x = np.linspace(0,1,100)\n    >>> y = np.random.rand(100)\n    >>> plot_regression_and_histogram(x, y, 'Test Plot')\n    \"\"\"\n    color = COLORS[randint(0, len(COLORS)-1)]\n    fig, ax = plt.subplots(2, 1, figsize=(10, 10))\n    \n    ax[0].scatter(x, y, color=color)\n    a, b = np.polyfit(x, y, 1)\n    linear = a * x + b\n    ax[0].plot(x, linear, color='black')\n    ax[0].set_title(f'{plot_title} - Linear Regression')\n    \n    pd.Series(x).plot.hist(grid=True, bins=20, rwidth=0.9, color=color, ax=ax[1])\n    ax[1].set_title(f'{plot_title} - Histogram')\n    \n    fig.tight_layout(pad=3.0)\n    plt.show()\n\n    return fig\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, matplotlib, scipy.stats, random, datetime\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom random import randint\nimport datetime\n\n# Constants\nCOLORS = ['red', 'blue', 'green', 'black', 'purple']\n\ndef plot_regression_and_density(x, y, plot_title):\n    \"\"\"\n    Draw a linear regression line in a scatter plot and a density plot of y variable.\n    \n    Parameters:\n    x (numpy.array): The x values.\n    y (numpy.array): The y values.\n    plot_title (str): The title of the plot.\n    \n    Returns:\n    matplotlib.figure.Figure: The matplotlib Figure object.\n    \n    Requirements:\n    - numpy\n    - matplotlib\n    - scipy.stats\n    - random\n    - datetime\n    \n    Example:\n    >>> x = np.linspace(0,1,100)\n    >>> y = np.random.rand(100)\n    >>> plot_regression_and_density(x, y, 'Test Plot')\n    \"\"\"\n    color = COLORS[randint(0, len(COLORS)-1)]\n    fig, ax = plt.subplots(2, 1, figsize=(10, 10))\n    \n    ax[0].scatter(x, y, color=color)\n    a, b = np.polyfit(x, y, 1)\n    linear = a * x + b\n    ax[0].plot(x, linear, color='black')\n    ax[0].set_title(f'{plot_title} - Linear Regression')\n    \n    density = stats.gaussian_kde(y)\n    n, x, _ = ax[1].hist(y, bins=np.linspace(0, 1, 100), \n                         histtype=u'step', density=True)  \n    ax[1].plot(x, density(x), color=color)\n    ax[1].set_title(f'{plot_title} - Density Plot')\n    \n    fig.tight_layout(pad=3.0)\n    plt.show()\n\n    return fig\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, matplotlib, random, datetime, statistics\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\nimport datetime\nimport statistics\n\n# Constants\nCOLORS = ['red', 'blue', 'green', 'black', 'purple']\n\ndef plot_regression_and_boxplot(x, y, plot_title):\n    \"\"\"\n    Draw a linear regression line in a scatter plot and a boxplot of y variable.\n    \n    Parameters:\n    x (numpy.array): The x values.\n    y (numpy.array): The y values.\n    plot_title (str): The title of the plot.\n    \n    Returns:\n    matplotlib.figure.Figure: The matplotlib Figure object.\n    \n    Requirements:\n    - numpy\n    - matplotlib\n    - random\n    - datetime\n    - statistics\n    \n    Example:\n    >>> x = np.linspace(0,1,100)\n    >>> y = np.random.rand(100)\n    >>> plot_regression_and_boxplot(x, y, 'Test Plot')\n    \"\"\"\n    color = COLORS[randint(0, len(COLORS)-1)]\n    fig, ax = plt.subplots(2, 1, figsize=(10, 10))\n    \n    ax[0].scatter(x, y, color=color)\n    a, b = np.polyfit(x, y, 1)\n    linear = a * x + b\n    ax[0].plot(x, linear, color='black')\n    ax[0].set_title(f'{plot_title} - Linear Regression')\n    \n    ax[1].boxplot(y, vert=False, patch_artist=True,\n                  boxprops=dict(facecolor=color, color=color),\n                  capprops=dict(color=color),\n                  whiskerprops=dict(color=color),\n                  flierprops=dict(color=color, markeredgecolor=color),\n                  medianprops=dict(color='yellow'))\n    ax[1].set_title(f'{plot_title} - Boxplot')\n    \n    fig.tight_layout(pad=3.0)\n    plt.show()\n\n    return fig\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, matplotlib, random, datetime, pandas\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\nimport datetime\nimport pandas as pd\n\n# Constants\nCOLORS = ['red', 'blue', 'green', 'black', 'purple']\n\ndef plot_regression_and_kde(x, y, plot_title):\n    \"\"\"\n    Draw a linear regression line in a scatter plot and a Kernel Density Estimate (KDE) plot of y variable.\n    \n    Parameters:\n    x (numpy.array): The x values.\n    y (numpy.array): The y values.\n    plot_title (str): The title of the plot.\n    \n    Returns:\n    matplotlib.figure.Figure: The matplotlib Figure object.\n    \n    Requirements:\n    - numpy\n    - matplotlib\n    - random\n    - datetime\n    - pandas\n    \n    Example:\n    >>> x = np.linspace(0,1,100)\n    >>> y = np.random.rand(100)\n    >>> plot_regression_and_kde(x, y, 'Test Plot')\n    \"\"\"\n    color = COLORS[randint(0, len(COLORS)-1)]\n    fig, ax = plt.subplots(2, 1, figsize=(10, 10))\n    \n    ax[0].scatter(x, y, color=color)\n    a, b = np.polyfit(x, y, 1)\n    linear = a * x + b\n    ax[0].plot(x, linear, color='black')\n    ax[0].set_title(f'{plot_title} - Linear Regression')\n    \n    pd.Series(y).plot.kde(ax=ax[1], color=color)\n    ax[1].set_title(f'{plot_title} - KDE Plot')\n    \n    fig.tight_layout(pad=3.0)\n    plt.show()\n\n    return fig\n```\n"}
{"seed": "{\"task_id\": 4556, \"prompt\": \"def f_4556():\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"class Foo:\\n\\t    def whoAmI(self):\\n\\t        print( \\\"I am \\\" + self.__class__.__name__)\\n\\t\\n\\tFoo().whoAmI()\", \"test_start\": \"\\nimport sys\\n\\ndef check(candidate):\", \"test\": [\"\\n    f = open('output', 'w')\\n    sys.stdout = f\\n    candidate()\\n    f.close()\\n    with open ('output', 'r') as f1:\\n        lines = f1.readlines()\\n        assert 'I am Foo' in lines[0]\\n\"], \"entry_point\": \"f_4556\", \"intent\": \"\\u30e1\\u30f3\\u30d0\\u95a2\\u6570\\u304b\\u3089\\u30af\\u30e9\\u30b9\\u306e\\u540d\\u524d\\u3092\\u53d6\\u5f97\\u3059\\u308b\", \"library\": [\"sys\"]}", "generation": "Scenario 1: \nos, sys, datetime, random\n```python\nimport os\nimport sys\nimport datetime\nimport random\n\n# Constants\nFILES_DIR = \"/path/to/files\"\nFILE_EXTENSIONS = ['.txt', '.csv', '.docx', '.xlsx', '.pdf']\n\ndef create_random_files(n):\n    \"\"\"\n    Create n number of random files with random file extensions in a specified directory.\n\n    Parameters:\n    n (int): Number of files to be created.\n\n    Returns:\n    int: Number of files actually created.\n\n    Requirements:\n    - os\n    - sys\n    - datetime\n    - random\n\n    Example:\n    >>> create_random_files(10)\n    \"\"\"\n    files_created = 0\n\n    for i in range(n):\n        filename = f'file_{i}{random.choice(FILE_EXTENSIONS)}'\n        filepath = os.path.join(FILES_DIR, filename)\n\n        try:\n            with open(filepath, 'w') as f:\n                f.write(f'File created at {datetime.datetime.now()}')\n            files_created += 1\n        except Exception as e:\n            print(f'Failed to create file: {filepath}. Error: {e}', file=sys.stderr)\n\n    return files_created\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nrandom, string, hashlib\n```python\nimport random\nimport string\nimport hashlib\n\n# Constants\nPASSWORD_LENGTH = 10\nSALT_LENGTH = 5\nCHARSET = string.ascii_letters + string.digits + string.punctuation\n\ndef generate_hashed_password_with_salt():\n    \"\"\"\n    Generate a random password of a given length and its hashed version with a salt.\n\n    Returns:\n    tuple: A tuple containing the plaintext password, the salt, and the hashed password.\n\n    Requirements:\n    - random\n    - string\n    - hashlib\n\n    Example:\n    >>> password, salt, hashed_password = generate_hashed_password_with_salt()\n    \"\"\"\n    password = ''.join(random.choice(CHARSET) for _ in range(PASSWORD_LENGTH))\n    salt = ''.join(random.choice(CHARSET) for _ in range(SALT_LENGTH))\n\n    hashed_password = hashlib.sha256((password + salt).encode()).hexdigest()\n\n    return password, salt, hashed_password\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \ncsv, random, datetime\n```python\nimport csv\nimport random\nimport datetime\n\n# Constants\nCSV_FILE = \"/path/to/file.csv\"\nNAMES = ['John', 'Jane', 'Bob', 'Alice', 'Charlie', 'Emma']\nDATES = [datetime.datetime(2020, i, 1) for i in range(1, 13)]\n\ndef create_csv_with_random_data():\n    \"\"\"\n    Create a CSV file with random data of names and dates.\n\n    Returns:\n    int: Number of rows written to the CSV file.\n\n    Requirements:\n    - csv\n    - random\n    - datetime\n\n    Example:\n    >>> create_csv_with_random_data()\n    \"\"\"\n    rows_written = 0\n\n    with open(CSV_FILE, 'w', newline='') as csvfile:\n        fieldnames = ['Name', 'Date']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n\n        writer.writeheader()\n        for _ in range(100):\n            writer.writerow({'Name': random.choice(NAMES), 'Date': random.choice(DATES).date()})\n            rows_written += 1\n\n    return rows_written\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nos, shutil, glob\n```python\nimport os\nimport shutil\nimport glob\n\n# Constants\nSOURCE_DIR = \"/path/to/source\"\nDEST_DIR = \"/path/to/dest\"\nFILE_TYPES = ['*.txt', '*.csv']\n\ndef move_files_of_certain_types():\n    \"\"\"\n    Move files of certain types from the source directory to the destination directory.\n\n    Returns:\n    int: Number of files moved.\n\n    Requirements:\n    - os\n    - shutil\n    - glob\n\n    Example:\n    >>> move_files_of_certain_types()\n    \"\"\"\n    files_moved = 0\n\n    for file_type in FILE_TYPES:\n        files = glob.glob(os.path.join(SOURCE_DIR, file_type))\n\n        for file in files:\n            shutil.move(file, DEST_DIR)\n            files_moved += 1\n\n    return files_moved\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nrandom, string, json, datetime\n```python\nimport random\nimport string\nimport json\nimport datetime\n\n# Constants\nJSON_FILE = \"/path/to/file.json\"\nNAMES = ['John', 'Jane', 'Bob', 'Alice', 'Charlie', 'Emma']\n\ndef create_json_with_random_data():\n    \"\"\"\n    Create a JSON file with random data of names and dates.\n\n    Returns:\n    int: Number of records written to the JSON file.\n\n    Requirements:\n    - random\n    - string\n    - json\n    - datetime\n\n    Example:\n    >>> create_json_with_random_data()\n    \"\"\"\n    data = []\n\n    for _ in range(100):\n        name = random.choice(NAMES)\n        date = str(datetime.datetime.now().date())\n        data.append({'Name': name, 'Date': date})\n\n    with open(JSON_FILE, 'w') as f:\n        json.dump(data, f)\n\n    return len(data)\n```\n"}
{"seed": "{\"task_id\": 27922, \"prompt\": \"def f_27922():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"{\\\"User-Agent\\\": \\\"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:47.0) Gecko/20100101 Firefox/47.0\\\"}\", \"test_start\": \"\\nimport urllib.request\\n\\ndef check(candidate):\", \"test\": [\"\\n    url = \\\"https://en.wikipedia.org/wiki/List_of_national_independence_days\\\"\\n    request = urllib.request.Request(url=url, headers=candidate())\\n    response = urllib.request.urlopen(request)\\n    assert response.getcode() == 200\\n\"], \"entry_point\": \"f_27922\", \"intent\": \"\\u30e6\\u30fc\\u30b6\\u30fc\\u30a8\\u30fc\\u30b8\\u30a7\\u30f3\\u30c8\\u3092Firefox\\u306b\\u5909\\u66f4\\u3059\\u308b\", \"library\": [\"urllib\"]}", "generation": "Scenario 1:\nrequests, urllib.parse, bs4, os, json\n```python\nimport requests\nfrom urllib.parse import urlparse\nfrom bs4 import BeautifulSoup\nimport os\nimport json\n\n# Constants\nUSER_AGENT = {\"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:47.0) Gecko/20100101 Firefox/47.0\"}\n\ndef download_images_from_wikipedia_page(url, download_dir):\n    \"\"\"\n    This function downloads all the images from a given Wikipedia page and save them in a specified directory.\n\n    Parameters:\n    url (str): The URL of the Wikipedia page.\n    download_dir (str): The directory to save the images.\n\n    Returns:\n    list: A list of the saved image file paths.\n\n    Requirements:\n    - requests\n    - urllib.parse\n    - bs4\n    - os\n    - json\n\n    Example:\n    >>> download_images_from_wikipedia_page('https://en.wikipedia.org/wiki/Cat', '/path/to/download/dir')\n    \"\"\"\n    response = requests.get(url, headers=USER_AGENT)\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    imgs = soup.find_all('img')\n    img_urls = ['https:' + img['src'] for img in imgs]\n    \n    saved_img_files = []\n    for img_url in img_urls:\n        img_data = requests.get(img_url, headers=USER_AGENT).content\n        img_file_name = os.path.join(download_dir, os.path.basename(urlparse(img_url).path))\n        with open(img_file_name, 'wb') as handler:\n            handler.write(img_data)\n        saved_img_files.append(img_file_name)\n    \n    return saved_img_files\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nrequests, bs4, pandas, matplotlib.pyplot\n```python\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nUSER_AGENT = {\"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:47.0) Gecko/20100101 Firefox/47.0\"}\n\ndef get_and_plot_page_views(url):\n    \"\"\"\n    This function retrieves the page views data of a given Wikipedia page and plot the data.\n\n    Parameters:\n    url (str): The URL of the Wikipedia page.\n\n    Returns:\n    DataFrame: A pandas DataFrame with page views data.\n    \n    Requirements:\n    - requests\n    - bs4\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> get_and_plot_page_views('https://en.wikipedia.org/wiki/Cat')\n    \"\"\"\n    response = requests.get(url, headers=USER_AGENT)\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    page_views_data = soup.find('div', {'class': 'mw-pageviews'}).text\n    page_views_data = json.loads(page_views_data)\n\n    df = pd.DataFrame(page_views_data)\n    df.plot(kind='line', x='timestamp', y='views')\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nrequests, urllib.parse, bs4, os, shutil\n```python\nimport requests\nfrom urllib.parse import urlparse\nfrom bs4 import BeautifulSoup\nimport os\nimport shutil\n\n# Constants\nUSER_AGENT = {\"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:47.0) Gecko/20100101 Firefox/47.0\"}\n\ndef download_and_zip_wikipedia_page_resources(url, zip_file_path):\n    \"\"\"\n    This function downloads all the resources from a given Wikipedia page and save them in a zip file.\n\n    Parameters:\n    url (str): The URL of the Wikipedia page.\n    zip_file_path (str): The path of the zip file to be saved.\n\n    Returns:\n    bool: True if successful, False otherwise.\n\n    Requirements:\n    - requests\n    - urllib.parse\n    - bs4\n    - os\n    - shutil\n\n    Example:\n    >>> download_and_zip_wikipedia_page_resources('https://en.wikipedia.org/wiki/Cat', '/path/to/zipfile.zip')\n    \"\"\"\n    response = requests.get(url, headers=USER_AGENT)\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    resources = soup.find_all(['img', 'script', 'link'])\n\n    temp_dir = '/tmp/wiki_resources'\n    os.makedirs(temp_dir, exist_ok=True)\n\n    for resource in resources:\n        if resource.name == 'img':\n            resource_url = 'https:' + resource['src']\n        elif resource.name == 'script' and resource.has_attr('src'):\n            resource_url = 'https:' + resource['src']\n        elif resource.name == 'link' and resource.has_attr('href'):\n            resource_url = 'https:' + resource['href']\n        else:\n            continue\n        \n        resource_data = requests.get(resource_url, headers=USER_AGENT).content\n        resource_file_name = os.path.join(temp_dir, os.path.basename(urlparse(resource_url).path))\n        with open(resource_file_name, 'wb') as handler:\n            handler.write(resource_data)\n    \n    shutil.make_archive(zip_file_path, 'zip', temp_dir)\n    shutil.rmtree(temp_dir)\n\n    return True\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nrequests, bs4, json, csv\n```python\nimport requests\nfrom bs4 import BeautifulSoup\nimport json\nimport csv\n\n# Constants\nUSER_AGENT = {\"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:47.0) Gecko/20100101 Firefox/47.0\"}\n\ndef extract_table_data_from_wikipedia_page(url, table_index, csv_file_path):\n    \"\"\"\n    This function extracts table data from a given Wikipedia page and save them in a CSV file.\n\n    Parameters:\n    url (str): The URL of the Wikipedia page.\n    table_index (int): The index of the table on the page (starting from 0).\n    csv_file_path (str): The path of the CSV file to be saved.\n\n    Returns:\n    bool: True if successful, False otherwise.\n\n    Requirements:\n    - requests\n    - bs4\n    - json\n    - csv\n\n    Example:\n    >>> extract_table_data_from_wikipedia_page('https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)', 0, '/path/to/file.csv')\n    \"\"\"\n    response = requests.get(url, headers=USER_AGENT)\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    tables = soup.find_all('table')\n    if table_index >= len(tables):\n        return False\n\n    table = tables[table_index]\n    rows = table.find_all('tr')\n    \n    data = []\n    for row in rows:\n        cols = row.find_all('td')\n        cols = [col.text.strip() for col in cols]\n        data.append([col for col in cols if col])\n\n    with open(csv_file_path, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerows(data)\n\n    return True\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nrequests, bs4, re, pandas\n```python\nimport requests\nfrom bs4 import BeautifulSoup\nimport re\nimport pandas as pd\n\n# Constants\nUSER_AGENT = {\"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:47.0) Gecko/20100101 Firefox/47.0\"}\n\ndef extract_references_from_wikipedia_page(url):\n    \"\"\"\n    This function extracts reference data from a given Wikipedia page and return them as a pandas DataFrame.\n\n    Parameters:\n    url (str): The URL of the Wikipedia page.\n\n    Returns:\n    DataFrame: A pandas DataFrame with reference data.\n    \n    Requirements:\n    - requests\n    - bs4\n    - re\n    - pandas\n\n    Example:\n    >>> extract_references_from_wikipedia_page('https://en.wikipedia.org/wiki/Cat')\n    \"\"\"\n    response = requests.get(url, headers=USER_AGENT)\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    references = soup.find_all('li', id=re.compile('^cite_note-'))\n    \n    data = []\n    for ref in references:\n        ref_id = ref['id']\n        ref_text = ref.text.strip()\n        data.append([ref_id, ref_text])\n\n    df = pd.DataFrame(data, columns=['ID', 'Text'])\n\n    return df\n```\n"}
{"seed": "{\"task_id\": 35394, \"prompt\": \"def f_35394(file):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"pd.read_csv(file, sep='\\\\s+')\", \"test_start\": \"\\nimport pandas as pd\\n\\ndef check(candidate):\", \"test\": [\"\\n    file_name = 'a.csv'\\n    with open (file_name, 'w') as f:\\n        f.write('1 2\\\\n')\\n    df = candidate(file_name)\\n    assert df.shape == (0, 2)\\n\", \"\\n    with open (file_name, 'w') as f:\\n        f.write('abc def\\\\nefg hij')\\n    df = candidate(file_name)\\n    assert df.shape == (1, 2)\\n\"], \"entry_point\": \"f_35394\", \"intent\": \"\\u7a7a\\u767d\\u3067\\u533a\\u5207\\u3089\\u308c\\u305fCSV\\u30d5\\u30a1\\u30a4\\u30eb`file`\\u3092\\u8aad\\u307f\\u8fbc\\u3080\", \"library\": [\"pandas\"]}", "generation": "Scenario 1: \npandas, numpy, matplotlib, seaborn, sklearn\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\n\ndef preprocess_and_visualize(file):\n    \"\"\"\n    Preprocesses a CSV file with white space seprated values and visualizes the correlation \n    between the features using Seaborn's heatmap.\n\n    Parameters:\n    file (str): The name of the CSV file.\n\n    Returns:\n    DataFrame: A pandas DataFrame that contains the normalized data.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - seaborn\n    - sklearn.preprocessing.StandardScaler\n\n    Example:\n    >>> file_name = 'a.csv'\n    >>> df = preprocess_and_visualize(file_name)\n    >>> print(df.head())\n    \"\"\"\n    df = pd.read_csv(file, sep='\\\\s+')\n    scaler = StandardScaler()\n    df_scaled = pd.DataFrame(scaler.fit_transform(df), columns = df.columns)\n\n    plt.figure(figsize=(10,8))\n    sns.heatmap(df_scaled.corr(), annot=True, cmap=\"coolwarm\")\n    plt.show()\n\n    return df_scaled\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, os, glob, csv\n```python\nimport pandas as pd\nimport os\nimport glob\nimport csv\n\ndef combine_csv_files(directory):\n    \"\"\"\n    Combine all CSV files in a directory into a single DataFrame and return it.\n\n    Parameters:\n    directory (str): The path to the directory containing the CSV files.\n\n    Returns:\n    DataFrame: A pandas DataFrame that contains all the data from the files.\n\n    Requirements:\n    - pandas\n    - os\n    - glob\n    - csv\n\n    Example:\n    >>> combined_df = combine_csv_files('/path/to/csv/files/')\n    >>> print(combined_df.head())\n    \"\"\"\n    all_files = glob.glob(os.path.join(directory, \"*.csv\"))\n    all_df = []\n    for file in all_files:\n        df = pd.read_csv(file, sep='\\\\s+', header=None)\n        all_df.append(df)\n    merged_df = pd.concat(all_df, ignore_index=True, axis=0)\n\n    return merged_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, matplotlib, sklearn\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\ndef train_test_linear_regression(file):\n    \"\"\"\n    Train and test a Linear Regression model on a dataset from a CSV file with white space separated values.\n\n    Parameters:\n    file (str): The name of the CSV file.\n\n    Returns:\n    float: The coefficient of determination R^2 of the prediction.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.model_selection.train_test_split\n    - sklearn.linear_model.LinearRegression\n\n    Example:\n    >>> file_name = 'a.csv'\n    >>> r_squared = train_test_linear_regression(file_name)\n    >>> print(r_squared)\n    \"\"\"\n    df = pd.read_csv(file, sep='\\\\s+')\n    X = df.iloc[:, :-1].values\n    y = df.iloc[:, -1].values\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\n    regressor = LinearRegression()\n    regressor.fit(X_train, y_train)\n\n    y_pred = regressor.predict(X_test)\n\n    return regressor.score(X_test, y_test)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, sklearn, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.decomposition import PCA\nimport seaborn as sns\n\ndef pca_and_visualize(file):\n    \"\"\"\n    Perform Principal Component Analysis (PCA) on a dataset from a CSV file with white space separated values \n    and visualize the first two principal components using a scatterplot.\n\n    Parameters:\n    file (str): The name of the CSV file.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.decomposition.PCA\n    - seaborn\n\n    Example:\n    >>> file_name = 'a.csv'\n    >>> pca_and_visualize(file_name)\n    \"\"\"\n    df = pd.read_csv(file, sep='\\\\s+')\n    pca = PCA(n_components=2)\n    principalComponents = pca.fit_transform(df)\n    principalDf = pd.DataFrame(data=principalComponents, columns=['principal component 1', 'principal component 2'])\n\n    sns.scatterplot(x=\"principal component 1\", y=\"principal component 2\", data=principalDf)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, sklearn, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\ndef kmeans_clustering(file):\n    \"\"\"\n    Perform K-Means clustering on a dataset from a CSV file with white space separated values \n    and visualize the clusters using a scatterplot.\n\n    Parameters:\n    file (str): The name of the CSV file.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.cluster.KMeans\n    - matplotlib.pyplot\n\n    Example:\n    >>> file_name = 'a.csv'\n    >>> kmeans_clustering(file_name)\n    \"\"\"\n    df = pd.read_csv(file, sep='\\\\s+')\n    kmeans = KMeans(n_clusters=3)\n    kmeans.fit(df)\n\n    plt.scatter(df.iloc[:, 0], df.iloc[:, 1], c=kmeans.labels_, cmap='viridis')\n    plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red')\n    plt.show()\n```\n"}
{"seed": "{\"task_id\": 35394, \"prompt\": \"def f_35394(file):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"pd.read_csv(file, delim_whitespace=True)\", \"test_start\": \"\\nimport pandas as pd\\n\\ndef check(candidate):\", \"test\": [\"\\n    file_name = 'a.csv'\\n    with open (file_name, 'w') as f:\\n        f.write('1 2\\\\n')\\n    df = candidate(file_name)\\n    assert df.shape == (0, 2)\\n\", \"\\n    with open (file_name, 'w') as f:\\n        f.write('abc def\\\\nefg hij')\\n    df = candidate(file_name)\\n    assert df.shape == (1, 2)\\n\"], \"entry_point\": \"f_35394\", \"intent\": \"\\u7a7a\\u767d\\u3067\\u533a\\u5207\\u3089\\u308c\\u305fCSV\\u30d5\\u30a1\\u30a4\\u30eb`file`\\u3092\\u8aad\\u307f\\u8fbc\\u3080\", \"library\": [\"pandas\"]}", "generation": "Scenario 3:\npandas, matplotlib, os, datetime\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nfrom datetime import datetime\n\n# Constants\nCSV_DELIMITER = ' '\n\ndef plot_csv_columns(file, x_column, y_column):\n    \"\"\"\n    Read a whitespace delimited CSV file and generate a line plot from two columns.\n\n    Parameters:\n    file (str): The CSV file path.\n    x_column (str): The name of the column to be plotted on the x-axis.\n    y_column (str): The name of the column to be plotted on the y-axis.\n\n    Returns:\n    str: The image file name with the generated plot.\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - os\n    - datetime\n\n    Example:\n    >>> plot_csv_columns('data.csv', 'Time', 'Temperature')\n    \"\"\"\n    df = pd.read_csv(file, delim_whitespace=True)\n    plt.figure(figsize=(10, 6))\n    plt.plot(df[x_column], df[y_column])\n    plt.xlabel(x_column)\n    plt.ylabel(y_column)\n    plt.title(f'{y_column} vs {x_column}')\n    file_name = f'plot_{datetime.now().strftime(\"%Y%m%d%H%M%S\")}.png'\n    plt.savefig(file_name)\n    plt.close()\n    \n    return file_name\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, seaborn, numpy, sklearn.preprocessing\n```python\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\n\n# Constants\nCSV_DELIMITER = ' '\n\ndef standardize_and_pairplot(file):\n    \"\"\"\n    Read a whitespace delimited CSV file, standardize all numeric columns, and\n    then generate a seaborn pairplot of the standardized data.\n\n    Parameters:\n    file (str): The CSV file path.\n\n    Returns:\n    str: The image file name with the generated pairplot.\n\n    Requirements:\n    - pandas\n    - seaborn\n    - numpy\n    - sklearn.preprocessing.StandardScaler\n\n    Example:\n    >>> standardize_and_pairplot('data.csv')\n    \"\"\"\n    df = pd.read_csv(file, delim_whitespace=True)\n\n    # Standardize numeric columns\n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n    df[numeric_cols] = StandardScaler().fit_transform(df[numeric_cols])\n\n    # Generate pairplot\n    pairplot = sns.pairplot(df)\n    file_name = f'pairplot_{datetime.now().strftime(\"%Y%m%d%H%M%S\")}.png'\n    pairplot.savefig(file_name)\n    \n    return file_name\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, os, csv, matplotlib\n```python\nimport pandas as pd\nimport os\nimport csv\nimport matplotlib.pyplot as plt\n\n# Constants\nCSV_DELIMITER = ' '\n\ndef plot_histogram(file, column):\n    \"\"\"\n    Read a whitespace delimited CSV file and generate a histogram from a column.\n\n    Parameters:\n    file (str): The CSV file path.\n    column (str): The name of the column to be plotted as a histogram.\n\n    Returns:\n    str: The image file name with the generated histogram.\n\n    Requirements:\n    - pandas\n    - os\n    - csv\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_histogram('data.csv', 'Age')\n    \"\"\"\n    df = pd.read_csv(file, delim_whitespace=True)\n    plt.figure(figsize=(10, 6))\n    plt.hist(df[column], bins=30)\n    plt.xlabel(column)\n    plt.ylabel('Frequency')\n    plt.title(f'Histogram of {column}')\n    file_name = f'histogram_{datetime.now().strftime(\"%Y%m%d%H%M%S\")}.png'\n    plt.savefig(file_name)\n    plt.close()\n    \n    return file_name\n```\n`GPT_ODEX_BREAK`\n\nScenario 6:\npandas, numpy, sklearn.model_selection, sklearn.linear_model\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\n# Constants\nCSV_DELIMITER = ' '\n\ndef linear_regression(file, target):\n    \"\"\"\n    Read a whitespace delimited CSV file, split it into training and testing sets,\n    and apply linear regression with the given target variable.\n\n    Parameters:\n    file (str): The CSV file path.\n    target (str): The name of the target variable.\n\n    Returns:\n    float: The R-squared score of the linear regression model on the test set.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.model_selection.train_test_split\n    - sklearn.linear_model.LinearRegression\n\n    Example:\n    >>> linear_regression('data.csv', 'Price')\n    \"\"\"\n    df = pd.read_csv(file, delim_whitespace=True)\n    X = df.drop(target, axis=1)\n    y = df[target]\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    model = LinearRegression().fit(X_train, y_train)\n    score = model.score(X_test, y_test)\n    \n    return score\n```\n`GPT_ODEX_BREAK`\n\nScenario 7:\npandas, matplotlib, numpy\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nCSV_DELIMITER = ' '\n\ndef plot_csv_data(file):\n    \"\"\"\n    Reads a whitespace delimited CSV file and generates a scatter plot of all numeric columns.\n\n    Parameters:\n    file (str): The CSV file path.\n\n    Returns:\n    str: The image file name with the generated scatter plot.\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - numpy\n\n    Example:\n    >>> plot_csv_data('data.csv')\n    \"\"\"\n    df = pd.read_csv(file, delim_whitespace=True)\n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n    plt.figure(figsize=(10, 6))\n    \n    for col in numeric_cols:\n        plt.scatter(df.index, df[col], label=col)\n    \n    plt.xlabel('Index')\n    plt.ylabel('Value')\n    plt.legend()\n    plt.title('Scatter plot of numeric columns')\n    file_name = f'scatter_{datetime.now().strftime(\"%Y%m%d%H%M%S\")}.png'\n    plt.savefig(file_name)\n    plt.close()\n    \n    return file_name\n```"}
{"seed": "{\"task_id\": 37591, \"prompt\": \"def f_37591(variable, value):\\n\\t\", \"suffix\": \"\\n\\treturn variable\", \"canonical_solution\": \"variable = value if variable is None else variable\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(123, 1) == 123\\n\", \"\\n    assert candidate(None, 1) == 1\\n\", \"\\n    assert candidate([], 1) == []\\n\"], \"entry_point\": \"f_37591\", \"intent\": \"\\u5909\\u6570`variable`\\u306b\\u5024\\u304c\\u5165\\u3063\\u3066\\u3044\\u306a\\u3044\\u5834\\u5408\\u306e\\u307f\\u5024\\u3092\\u4ee3\\u5165\\u3092\\u3059\\u308b\", \"library\": []}", "generation": "Scenario 1: \ncollections, random, matplotlib\n```python\nimport collections\nfrom random import choice\nimport matplotlib.pyplot as plt\n\n# Constants\nFRUITS = ['Apple', 'Banana', 'Cherry', 'Date', 'Elderberry', 'Fig', 'Grape', 'Honeydew']\n\ndef generate_fruit_basket(basket=None):\n    \"\"\"\n    Generate a fruit basket. If a basket is given, it will add fruits to the basket, \n    otherwise it will create a new basket with fruits.\n    \n    Parameters:\n    basket (dict): The initial basket with fruits and their counts.\n    \n    Returns:\n    dict: A dictionary with fruits and their counts.\n\n    Requirements:\n    - collections\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> basket = generate_fruit_basket({'Apple': 5, 'Banana': 2})\n    >>> print(basket)\n    >>> plt.bar(basket.keys(), basket.values())\n    \"\"\"\n    if not basket:\n        basket = collections.defaultdict(int)\n    else:\n        basket = collections.defaultdict(int, basket)\n\n    for _ in range(10):\n        fruit = choice(FRUITS)\n        basket[fruit] += 1\n\n    return dict(basket)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nrandom, string, re\n```python\nimport string\nimport random\nimport re\n\n# Constants\nPASSWORD_LENGTH = 10\nSPECIAL_CHARS = '!@#$%^&*()'\n\ndef generate_password(existing_password=None):\n    \"\"\"\n    Generate a password. If an existing password is given, it will validate the password \n    and return it if it's valid, otherwise it will generate a new password.\n\n    Parameters:\n    existing_password (str): The existing password.\n\n    Returns:\n    str: The password.\n\n    Requirements:\n    - string\n    - random\n    - re\n\n    Example:\n    >>> print(generate_password())\n    \"\"\"\n    def is_valid(password):\n        if len(password) != PASSWORD_LENGTH:\n            return False\n        if not re.search(r'\\d', password):\n            return False\n        if not re.search(r'[A-Z]', password):\n            return False\n        if not re.search(r'[a-z]', password):\n            return False\n        if not any(c in SPECIAL_CHARS for c in password):\n            return False\n        return True\n\n    if existing_password and is_valid(existing_password):\n        return existing_password\n\n    while True:\n        password = ''.join(random.choice(string.ascii_letters + string.digits + SPECIAL_CHARS) for _ in range(PASSWORD_LENGTH))\n        if is_valid(password):\n            return password\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnumpy, matplotlib.pyplot, random\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nPOINTS_COUNT = 50\n\ndef generate_random_walk(existing_walk=None):\n    \"\"\"\n    Generate a random walk. If an existing walk is given, it will continue the walk, \n    otherwise it will create a new walk.\n    \n    Parameters:\n    existing_walk (list): The existing walk.\n\n    Returns:\n    list: The walk.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> walk = generate_random_walk()\n    >>> plt.plot(walk)\n    \"\"\"\n    if not existing_walk:\n        walk = [0]\n    else:\n        walk = list(existing_walk)\n\n    for _ in range(POINTS_COUNT):\n        walk.append(walk[-1] + randint(-1, 1))\n\n    return walk\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \ncollections, re, string\n```python\nimport collections\nimport re\nimport string\n\n# Constants\nSTOPWORDS = ['a', 'an', 'the', 'is', 'in', 'to', 'for', 'from', 'of', 'at', 'on', 'it']\n\ndef count_words_in_text(text, existing_counts=None):\n    \"\"\"\n    Count words in a text. If existing counts are given, it will add counts to the existing counts, \n    otherwise it will create new counts.\n\n    Parameters:\n    text (str): The text to count words in.\n    existing_counts (dict): The existing word counts.\n\n    Returns:\n    dict: The word counts.\n\n    Requirements:\n    - collections\n    - re\n    - string\n\n    Example:\n    >>> print(count_words_in_text('Hello, world!'))\n    \"\"\"\n    if not existing_counts:\n        counts = collections.defaultdict(int)\n    else:\n        counts = collections.defaultdict(int, existing_counts)\n\n    text = text.lower()\n    text = re.sub('['+string.punctuation+']', ' ', text)\n    words = text.split()\n    words = [word for word in words if word not in STOPWORDS]\n\n    for word in words:\n        counts[word] += 1\n\n    return dict(counts)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nos, hashlib, binascii\n```python\nimport os\nimport hashlib\nimport binascii\n\n# Constants\nSALT_LENGTH = 16\nHASH_LENGTH = 64\nITERATIONS = 100000\n\ndef store_password(password, existing_hash=None):\n    \"\"\"\n    Store a password securely. If an existing hash is given, it will validate the password \n    against the hash, otherwise it will create a new hash.\n\n    Parameters:\n    password (str): The password to store.\n    existing_hash (str): The existing hash.\n\n    Returns:\n    str: The hash of the password.\n\n    Requirements:\n    - os\n    - hashlib\n    - binascii\n\n    Example:\n    >>> print(store_password('password123'))\n    \"\"\"\n    def hash_password(password, salt=None):\n        if not salt:\n            salt = os.urandom(SALT_LENGTH)\n        hash = hashlib.pbkdf2_hmac('sha256', password.encode(), salt, ITERATIONS, HASH_LENGTH)\n        return binascii.hexlify(salt + hash).decode()\n\n    if not existing_hash:\n        return hash_password(password)\n\n    salt = binascii.unhexlify(existing_hash)[:SALT_LENGTH]\n    return hash_password(password, salt) == existing_hash\n```"}
{"seed": "{\"task_id\": 37591, \"prompt\": \"def f_37591(variable, value):\\n\\t\", \"suffix\": \"\\n\\treturn variable\", \"canonical_solution\": \"variable = value if variable is None else variable\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(123, 1) == 123\\n\", \"\\n    assert candidate(None, 1) == 1\\n\", \"\\n    assert candidate([], 1) == []\\n\"], \"entry_point\": \"f_37591\", \"intent\": \"\\u5909\\u6570`variable`\\u306b\\u5024\\u304c\\u5165\\u3063\\u3066\\u3044\\u306a\\u3044\\u5834\\u5408\\u306e\\u307f\\u5024\\u3092\\u4ee3\\u5165\\u3092\\u3059\\u308b\", \"library\": []}", "generation": "Scenario 3: \npandas, numpy, matplotlib, sklearn\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Constants\nFEATURES = ['feature1', 'feature2', 'feature3']\nTARGET = 'target'\n\n# Sample data\n# df = pd.DataFrame({\n#     'feature1': [1, 2, 3, 4, 5],\n#     'feature2': [2, 3, 4, 5, 6],\n#     'feature3': [3, 4, 5, 6, 7],\n#     'target': [1, 0, 1, 0, 1]\n# })\n\n# Scaler\nscaler = MinMaxScaler()\n\ndef preprocess_data(df, features=FEATURES, target=TARGET, scaler=scaler):\n    \"\"\"\n    Preprocess the data by filling missing values with the column mean and \n    scaling the features to the range [0, 1].\n\n    Parameters:\n    df (DataFrame): The input data.\n    features (list): The list of feature columns.\n    target (str): The target column.\n    scaler (MinMaxScaler): The scaler object.\n\n    Returns:\n    DataFrame: The preprocessed data.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.preprocessing.MinMaxScaler\n\n    Example:\n    >>> df = pd.DataFrame({\n    ...     'feature1': [1, 2, np.nan, 4, 5],\n    ...     'feature2': [2, np.nan, 4, 5, 6],\n    ...     'feature3': [3, 4, 5, np.nan, 7],\n    ...     'target': [1, 0, 1, 0, np.nan]\n    ... })\n    >>> preprocess_data(df)\n    \"\"\"\n    df = df.copy()\n\n    # Fill missing values with the column mean\n    df[features + [target]] = df[features + [target]].fillna(df.mean())\n\n    # Scale the features\n    df[features] = scaler.fit_transform(df[features])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \npandas, numpy, matplotlib, sklearn\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Constants\nFEATURES = ['feature1', 'feature2', 'feature3']\nTARGET = 'target'\n\n# Sample data\n# df = pd.DataFrame({\n#     'feature1': [1, 2, 3, 4, 5],\n#     'feature2': [2, 3, 4, 5, 6],\n#     'feature3': [3, 4, 5, 6, 7],\n#     'target': [1, 0, 1, 0, 1]\n# })\n\ndef train_decision_tree(df, features=FEATURES, target=TARGET):\n    \"\"\"\n    Train a decision tree classifier and evaluate its accuracy on the test set.\n\n    Parameters:\n    df (DataFrame): The input data.\n    features (list): The list of feature columns.\n    target (str): The target column.\n\n    Returns:\n    float: The accuracy of the decision tree classifier on the test set.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.model_selection.train_test_split\n    - sklearn.metrics.accuracy_score\n    - sklearn.tree.DecisionTreeClassifier\n\n    Example:\n    >>> df = pd.DataFrame({\n    ...     'feature1': [1, 2, 3, 4, 5],\n    ...     'feature2': [2, 3, 4, 5, 6],\n    ...     'feature3': [3, 4, 5, 6, 7],\n    ...     'target': [1, 0, 1, 0, 1]\n    ... })\n    >>> train_decision_tree(df)\n    \"\"\"\n    X = df[features]\n    y = df[target]\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    clf = DecisionTreeClassifier(random_state=42)\n    clf.fit(X_train, y_train)\n\n    y_pred = clf.predict(X_test)\n\n    accuracy = accuracy_score(y_test, y_pred)\n\n    return accuracy\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \npandas, numpy, matplotlib, sklearn\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\n\n# Constants\nFEATURES = ['feature1', 'feature2', 'feature3']\n\n# Sample data\n# df = pd.DataFrame({\n#     'feature1': [1, 2, 3, 4, 5],\n#     'feature2': [2, 3, 4, 5, 6],\n#     'feature3': [3, 4, 5, 6, 7]\n# })\n\ndef find_optimal_clusters(df, features=FEATURES, max_clusters=10):\n    \"\"\"\n    Find the optimal number of clusters for KMeans clustering using the \n    elbow method.\n\n    Parameters:\n    df (DataFrame): The input data.\n    features (list): The list of feature columns.\n    max_clusters (int): The maximum number of clusters to consider.\n\n    Returns:\n    int: The optimal number of clusters.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.cluster.KMeans\n\n    Example:\n    >>> df = pd.DataFrame({\n    ...     'feature1': [1, 2, 3, 4, 5],\n    ...     'feature2': [2, 3, 4, 5, 6],\n    ...     'feature3': [3, 4, 5, 6, 7]\n    ... })\n    >>> find_optimal_clusters(df)\n    \"\"\"\n    X = df[features]\n\n    distortions = []\n\n    for i in range(1, max_clusters+1):\n        kmeans = KMeans(n_clusters=i, random_state=42)\n        kmeans.fit(X)\n        distortions.append(kmeans.inertia_)\n\n    plt.plot(range(1, max_clusters+1), distortions, marker='o')\n    plt.xlabel('Number of clusters')\n    plt.ylabel('Distortion')\n    plt.show()\n\n    # The optimal number of clusters is the value at the \"elbow\" point\n    elbow_point = np.argmin(np.diff(np.diff(distortions))) + 2\n\n    return elbow_point\n```\n`GPT_ODEX_BREAK`\n\nScenario 6: \npandas, numpy, matplotlib, sklearn\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n# Constants\nFEATURES = ['feature1', 'feature2', 'feature3']\nTARGET = 'target'\n\n# Sample data\n# df = pd.DataFrame({\n#     'feature1': [1, 2, 3, 4, 5],\n#     'feature2': [2, 3, 4, 5, 6],\n#     'feature3': [3, 4, 5, 6, 7],\n#     'target': [1, 2, 3, 4, 5]\n# })\n\ndef evaluate_linear_regression(df, features=FEATURES, target=TARGET):\n    \"\"\"\n    Train a linear regression model and evaluate its mean squared error.\n\n    Parameters:\n    df (DataFrame): The input data.\n    features (list): The list of feature columns.\n    target (str): The target column.\n\n    Returns:\n    float: The mean squared error of the linear regression model.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.linear_model.LinearRegression\n    - sklearn.metrics.mean_squared_error\n\n    Example:\n    >>> df = pd.DataFrame({\n    ...     'feature1': [1, 2, 3, 4, 5],\n    ...     'feature2': [2, 3, 4, 5, 6],\n    ...     'feature3': [3, 4, 5, 6, 7],\n    ...     'target': [1, 2, 3, 4, 5]\n    ... })\n    >>> evaluate_linear_regression(df)\n    \"\"\"\n    X = df[features]\n    y = df[target]\n\n    lr = LinearRegression()\n    lr.fit(X, y)\n\n    y_pred = lr.predict(X)\n\n    mse = mean_squared_error(y, y_pred)\n\n    return mse\n```\n`GPT_ODEX_BREAK`\n\nScenario 7: \npandas, numpy, matplotlib, sklearn\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\n# Constants\nFEATURES = ['feature1', 'feature2', 'feature3']\n\n# Sample data\n# df = pd.DataFrame({\n#     'feature1': [1, 2, 3, 4, 5],\n#     'feature2': [2, 3, 4, 5, 6],\n#     'feature3': [3, 4, 5, 6, 7]\n# })\n\ndef perform_pca(df, features=FEATURES, n_components=2):\n    \"\"\"\n    Perform Principal Component Analysis (PCA) on the data and plot the results.\n\n    Parameters:\n    df (DataFrame): The input data.\n    features (list): The list of feature columns.\n    n_components (int): The number of principal components.\n\n    Returns:\n    None.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.decomposition.PCA\n\n    Example:\n    >>> df = pd.DataFrame({\n    ...     'feature1': [1, 2, 3, 4, 5],\n    ...     'feature2': [2, 3, 4, 5, 6],\n    ...     'feature3': [3, 4, 5, 6, 7]\n    ... })\n    >>> perform_pca(df)\n    \"\"\"\n    X = df[features]\n\n    pca = PCA(n_components=n_components)\n    X_pca = pca.fit_transform(X)\n\n    plt.scatter(X_pca[:, 0], X_pca[:, 1])\n    plt.xlabel('First Principal Component')\n    plt.ylabel('Second Principal Component')\n    plt.show()\n```\n"}
{"seed": "{\"task_id\": 11601, \"prompt\": \"def f_11601():\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"os.startfile('C:\\\\Program Files\\\\....\\\\app.exe')\", \"test_start\": \"\\nimport os\\nfrom unittest.mock import Mock\\n\\ndef check(candidate):\", \"test\": [\"\\n    os.startfile = Mock()\\n    try:\\n        candidate()\\n    except:\\n        assert False\\n\"], \"entry_point\": \"f_11601\", \"intent\": \"Windows\\u4e0a\\u306e\\u30a2\\u30d7\\u30ea\\u30b1\\u30fc\\u30b7\\u30e7\\u30f3`app`\\u3092\\u5b9f\\u884c\\u3059\\u308b\", \"library\": [\"os\"]}", "generation": "Scenario 1:\nos, csv, datetime, time\n```python\nimport os\nimport csv\nfrom datetime import datetime\nimport time\n\n# Constants\nFILE_PATH = '/path/to/your/directory/files'\nBACKUP_PATH = '/path/to/your/directory/backup'\nFILE_EXTENSION = '.csv'\nBACKUP_EXTENSION = '.bak'\n\ndef backup_and_update_csv_files():\n    \"\"\"\n    Back up all CSV files in a directory and update the modified time of each file.\n    \n    Parameters:\n    None\n    \n    Returns:\n    None\n    \n    Requirements:\n    - os\n    - csv\n    - datetime\n    - time\n    \n    Example:\n    >>> backup_and_update_csv_files()\n    \"\"\"\n    for file_name in os.listdir(FILE_PATH):\n        if file_name.endswith(FILE_EXTENSION):\n            # Back up the file\n            os.rename(os.path.join(FILE_PATH, file_name),\n                      os.path.join(BACKUP_PATH, file_name.replace(FILE_EXTENSION, BACKUP_EXTENSION)))\n            \n            # Update the modified time of the file\n            os.utime(os.path.join(BACKUP_PATH, file_name.replace(FILE_EXTENSION, BACKUP_EXTENSION)),\n                     (time.mktime(datetime.now().timetuple()), time.mktime(datetime.now().timetuple())))\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, shutil, sys, argparse\n```python\nimport os\nimport shutil\nimport sys\nimport argparse\n\n# Constants\nDEFAULT_DESTINATION = '/path/to/your/directory/default_destination'\n\ndef copy_files_to_destination(source, destination=DEFAULT_DESTINATION):\n    \"\"\"\n    Copy all files from a source directory to a destination directory.\n\n    Parameters:\n    source (str): The source directory.\n    destination (str): The destination directory. If not provided, a default directory will be used.\n\n    Returns:\n    None\n\n    Requirements:\n    - os\n    - shutil\n    - sys\n    - argparse\n\n    Example:\n    >>> copy_files_to_destination('/path/to/your/directory/source', '/path/to/your/directory/destination')\n    \"\"\"\n    if not os.path.exists(destination):\n        os.mkdir(destination)\n\n    for file_name in os.listdir(source):\n        shutil.copy2(os.path.join(source, file_name), destination)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, subprocess, time, datetime\n```python\nimport os\nimport subprocess\nimport time\nfrom datetime import datetime\n\n# Constants\nLOG_PATH = '/path/to/your/directory/logs'\nAPP_PATH = '/path/to/your/application/app.exe'\n\ndef run_app_and_log_output():\n    \"\"\"\n    Run an application and log its output to a file with a timestamp.\n\n    Parameters:\n    None\n\n    Returns:\n    None\n\n    Requirements:\n    - os\n    - subprocess\n    - time\n    - datetime\n\n    Example:\n    >>> run_app_and_log_output()\n    \"\"\"\n    current_time = datetime.now()\n    log_file_name = f'{current_time.strftime(\"%Y%m%d%H%M%S\")}.log'\n\n    with open(os.path.join(LOG_PATH, log_file_name), 'w') as log_file:\n        process = subprocess.Popen(APP_PATH, stdout=log_file, stderr=log_file)\n\n        # Wait for the process to complete\n        while process.poll() is None:\n            time.sleep(0.1)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, json, datetime\n```python\nimport os\nimport json\nfrom datetime import datetime\n\n# Constants\nCONFIG_PATH = '/path/to/your/directory/config.json'\nDEFAULT_CONFIG = {\n    'last_run': None,\n    'run_count': 0\n}\n\ndef update_config():\n    \"\"\"\n    Update a configuration file with the current run time and increment the run count.\n\n    Parameters:\n    None\n\n    Returns:\n    None\n\n    Requirements:\n    - os\n    - json\n    - datetime\n\n    Example:\n    >>> update_config()\n    \"\"\"\n    if os.path.exists(CONFIG_PATH):\n        with open(CONFIG_PATH, 'r') as config_file:\n            config = json.load(config_file)\n    else:\n        config = DEFAULT_CONFIG\n\n    config['last_run'] = datetime.now().isoformat()\n    config['run_count'] += 1\n\n    with open(CONFIG_PATH, 'w') as config_file:\n        json.dump(config, config_file, indent=4)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, random, time, datetime\n```python\nimport os\nimport random\nimport time\nfrom datetime import datetime\n\n# Constants\nFILE_PATH = '/path/to/your/directory/files'\nFILE_EXTENSION = '.txt'\nCONTENT = 'Hello, World!'\n\ndef create_random_files():\n    \"\"\"\n    Create a random number of files with a specific content and a timestamp in their names.\n\n    Parameters:\n    None\n\n    Returns:\n    None\n\n    Requirements:\n    - os\n    - random\n    - time\n    - datetime\n\n    Example:\n    >>> create_random_files()\n    \"\"\"\n    for _ in range(random.randint(1, 10)):\n        file_name = f'{datetime.now().strftime(\"%Y%m%d%H%M%S\")}{random.randint(1, 1000)}{FILE_EXTENSION}'\n        with open(os.path.join(FILE_PATH, file_name), 'w') as file:\n            file.write(CONTENT)\n\n        time.sleep(0.1)\n```"}
{"seed": "{\"task_id\": 26837, \"prompt\": \"def f_26837(number):\\n\\t\", \"suffix\": \"\\n\\treturn num_list\", \"canonical_solution\": \"\\n\\tnum_list = []\\n\\twhile number != 0:\\n\\t\\tnum_list.append(number % 10)\\n\\t\\tnumber //= 10\\n\\tnum_list.reverse()\\n\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(123) == [1,2,3]\\n\"], \"entry_point\": \"f_26837\", \"intent\": \"\\u6570\\u5024`number`\\u3092\\u4e00\\u6841\\u305a\\u3064\\u53d6\\u5f97\\u3057\\u3066\\u30ea\\u30b9\\u30c8`num_list`\\u306b\\u3059\\u308b\", \"library\": []}", "generation": "Scenario 1: \nnumpy, random, matplotlib, statistics\n```python\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nimport statistics\n\n# Constants\nDIGITS = list(range(10))\n\ndef generate_random_distribution(number):\n    \"\"\"\n    Generate a distribution of random numbers where each number is a digit in the given number.\n    Calculate the mean, median, and mode of the distribution and plot a histogram.\n\n    Parameters:\n    number (int): The number from which to generate the distribution.\n\n    Returns:\n    dict: A dictionary with the mean, median, and mode of the distribution.\n\n    Requirements:\n    - numpy\n    - random\n    - matplotlib.pyplot\n    - statistics\n\n    Example:\n    >>> generate_random_distribution(1234567890)\n    \"\"\"\n    num_list = [int(digit) for digit in str(number)]\n    distribution = random.choices(DIGITS, k=len(num_list))\n\n    mean = np.mean(distribution)\n    median = np.median(distribution)\n    mode = statistics.mode(distribution)\n\n    plt.hist(distribution, bins=10, edgecolor='black')\n    plt.show()\n\n    return {'Mean': mean, 'Median': median, 'Mode': mode}\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, random, itertools, collections\n```python\nimport numpy as np\nimport random\nimport itertools\nimport collections\n\n# Constants\nDIGITS = list(range(10))\n\ndef calculate_distribution(number):\n    \"\"\"\n    Generate a random distribution of digits from the given number, and calculate the frequency of each digit.\n\n    Parameters:\n    number (int): The number from which to generate the distribution.\n\n    Returns:\n    dict: A dictionary with the frequency of each digit.\n\n    Requirements:\n    - numpy\n    - random\n    - itertools\n    - collections\n\n    Example:\n    >>> calculate_distribution(1234567890)\n    \"\"\"\n    num_list = [int(digit) for digit in str(number)]\n    distribution = random.choices(DIGITS, k=len(num_list))\n\n    frequency = collections.Counter(distribution)\n\n    return dict(frequency)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, random, math, statistics\n```python\nimport numpy as np\nimport random\nimport math\nimport statistics\n\n# Constants\nDIGITS = list(range(10))\n\ndef calculate_stats(number):\n    \"\"\"\n    Generate a random distribution of digits from the given number, \n    and calculate the mean, variance, and standard deviation.\n\n    Parameters:\n    number (int): The number from which to generate the distribution.\n\n    Returns:\n    dict: A dictionary with the mean, variance, and standard deviation.\n\n    Requirements:\n    - numpy\n    - random\n    - math\n    - statistics\n\n    Example:\n    >>> calculate_stats(1234567890)\n    \"\"\"\n    num_list = [int(digit) for digit in str(number)]\n    distribution = random.choices(DIGITS, k=len(num_list))\n\n    mean = np.mean(distribution)\n    variance = statistics.variance(distribution)\n    std_dev = math.sqrt(variance)\n\n    return {'Mean': mean, 'Variance': variance, 'Standard Deviation': std_dev}\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, random, scipy.stats, matplotlib\n```python\nimport numpy as np\nimport random\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\n# Constants\nDIGITS = list(range(10))\n\ndef generate_normal_distribution(number):\n    \"\"\"\n    Generate a normal distribution of random numbers where each number is a digit in the given number.\n    Plot the normal distribution and calculate the skewness and kurtosis.\n\n    Parameters:\n    number (int): The number from which to generate the distribution.\n\n    Returns:\n    dict: A dictionary with the skewness and kurtosis of the distribution.\n\n    Requirements:\n    - numpy\n    - random\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Example:\n    >>> generate_normal_distribution(1234567890)\n    \"\"\"\n    num_list = [int(digit) for digit in str(number)]\n    distribution = np.random.normal(loc=np.mean(num_list), scale=np.std(num_list), size=len(num_list))\n\n    skewness = stats.skew(distribution)\n    kurtosis = stats.kurtosis(distribution)\n\n    plt.hist(distribution, bins=10, edgecolor='black', density=True)\n    plt.show()\n\n    return {'Skewness': skewness, 'Kurtosis': kurtosis}\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, random, seaborn, pandas\n```python\nimport numpy as np\nimport random\nimport seaborn as sns\nimport pandas as pd\n\n# Constants\nDIGITS = list(range(10))\n\ndef plot_distribution(number):\n    \"\"\"\n    Generate a random distribution of digits from the given number and plot a density plot.\n\n    Parameters:\n    number (int): The number from which to generate the distribution.\n\n    Requirements:\n    - numpy\n    - random\n    - seaborn\n    - pandas\n\n    Example:\n    >>> plot_distribution(1234567890)\n    \"\"\"\n    num_list = [int(digit) for digit in str(number)]\n    distribution = random.choices(DIGITS, k=len(num_list))\n\n    df = pd.DataFrame(distribution, columns=['Digits'])\n    sns.kdeplot(data=df, x='Digits')\n    plt.show()\n```"}
{"seed": "{\"task_id\": 26837, \"prompt\": \"def f_26837(number):\\n\\t\", \"suffix\": \"\\n\\treturn num_list\", \"canonical_solution\": \"num_list = map(int, str(number))\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert list(candidate(123)) == [1,2,3]\\n\"], \"entry_point\": \"f_26837\", \"intent\": \"\\u6570\\u5024`number`\\u3092\\u4e00\\u6841\\u305a\\u3064\\u53d6\\u5f97\\u3057\\u3066\\u30ea\\u30b9\\u30c8`num_list`\\u306b\\u3059\\u308b\", \"library\": []}", "generation": "Scenario 1:\nnumpy, math, random\n```python\nimport numpy as np\nimport math\nfrom random import randint\n\n# Constants\nDIMENSION = 3\n\ndef generate_random_vectors(number):\n    \"\"\"\n    Generate a list of random 3D vectors and calculate their magnitudes.\n    \n    Parameters:\n    number (int): The number of vectors to generate.\n    \n    Returns:\n    list: A list of tuples where each tuple contains a vector and its magnitude.\n    \n    Requirements:\n    - numpy\n    - math\n    - random\n    \n    Example:\n    >>> vectors = generate_random_vectors(5)\n    >>> print(vectors)\n    \"\"\"\n    vectors = []\n    \n    for _ in range(number):\n        vector = np.array([randint(-10, 10) for _ in range(DIMENSION)])\n        magnitude = math.sqrt(sum(vector**2))\n        vectors.append((vector, magnitude))\n        \n    return vectors\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, scipy.stats, matplotlib\n```python\nimport numpy as np\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\n\ndef generate_normal_distribution(number, mean, std_dev):\n    \"\"\"\n    Generate a normal distribution of a given number of data points, mean, \n    and standard deviation, and plot a histogram and a probability density function (PDF).\n    \n    Parameters:\n    number (int): The number of data points.\n    mean (float): The mean of the distribution.\n    std_dev (float): The standard deviation of the distribution.\n    \n    Returns:\n    array: A numpy array of the generated data points.\n    \n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n    \n    Example:\n    >>> data = generate_normal_distribution(1000, 0, 1)\n    >>> print(data)\n    \"\"\"\n    data = np.random.normal(mean, std_dev, number)\n    \n    plt.hist(data, bins=30, density=True)\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, mean, std_dev)\n    plt.plot(x, p, 'k', linewidth=2)\n    plt.show()\n    \n    return data\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, math, matplotlib\n```python\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\n\n# Constants\nFREQUENCY = 5\nAMPLITUDE = 1\n\ndef generate_sine_wave(number, phase_shift):\n    \"\"\"\n    Generate a sine wave of a given number of points and phase shift, and plot the wave.\n    \n    Parameters:\n    number (int): The number of points in the wave.\n    phase_shift (float): The phase shift of the wave in radians.\n    \n    Returns:\n    array: A numpy array of the generated sine wave.\n    \n    Requirements:\n    - numpy\n    - math\n    - matplotlib.pyplot\n    \n    Example:\n    >>> wave = generate_sine_wave(1000, math.pi/2)\n    >>> print(wave)\n    \"\"\"\n    x = np.linspace(0, 2 * math.pi, number)\n    y = AMPLITUDE * np.sin(FREQUENCY * (x + phase_shift))\n    \n    plt.plot(x, y)\n    plt.show()\n    \n    return y\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, scipy.stats, matplotlib\n```python\nimport numpy as np\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\n\n# Constants\nDEGREES_OF_FREEDOM = 5\n\ndef generate_t_distribution(number):\n    \"\"\"\n    Generate a t-distribution of a given number of data points, and plot a histogram \n    and a probability density function (PDF).\n    \n    Parameters:\n    number (int): The number of data points.\n    \n    Returns:\n    array: A numpy array of the generated data points.\n    \n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n    \n    Example:\n    >>> data = generate_t_distribution(1000)\n    >>> print(data)\n    \"\"\"\n    data = np.random.standard_t(DEGREES_OF_FREEDOM, size=number)\n    \n    plt.hist(data, bins=30, density=True)\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.t.pdf(x, DEGREES_OF_FREEDOM)\n    plt.plot(x, p, 'k', linewidth=2)\n    plt.show()\n    \n    return data\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, scipy.optimize, matplotlib\n```python\nimport numpy as np\nfrom scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\n\n# Constants\nNOISE = 0.2\n\ndef fit_curve_to_data(number):\n    \"\"\"\n    Generate a set of data points based on a quadratic function with some noise, \n    fit a curve to the data using the least squares method, and plot the data and the fitted curve.\n    \n    Parameters:\n    number (int): The number of data points.\n    \n    Returns:\n    array, array, array: A numpy array of the x data, a numpy array of the y data, \n    and a numpy array of the fitted curve.\n    \n    Requirements:\n    - numpy\n    - scipy.optimize\n    - matplotlib.pyplot\n    \n    Example:\n    >>> x_data, y_data, fitted_curve = fit_curve_to_data(100)\n    >>> print(x_data, y_data, fitted_curve)\n    \"\"\"\n    x_data = np.linspace(-10, 10, number)\n    y_data = x_data**2 + NOISE * np.random.normal(size=number)\n    \n    def model(x, a, b, c):\n        return a * x**2 + b * x + c\n    \n    popt, _ = curve_fit(model, x_data, y_data)\n    fitted_curve = model(x_data, *popt)\n    \n    plt.scatter(x_data, y_data)\n    plt.plot(x_data, fitted_curve, 'r')\n    plt.show()\n    \n    return x_data, y_data, fitted_curve\n```"}
{"seed": "{\"task_id\": 59780, \"prompt\": \"def f_59780():\\n\\t\", \"suffix\": \"\\n\\treturn result\", \"canonical_solution\": \"\\n\\tdef example(a, b): return b\\n\\thello  = tf.constant(\\\"Hello\\\")\\n\\tf = tf.function(example)\\n\\tresult = eval(f([], hello))\\n\", \"test_start\": \"\\nimport tensorflow as tf\\nfrom tensorflow.keras.backend import eval\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate() == b'Hello'\\n\"], \"entry_point\": \"f_59780\", \"intent\": \"\\u5b9a\\u6570\\u306e\\u8a55\\u4fa1\\u7d50\\u679c\\u3092\\u8868\\u793a\\u3059\\u308b\", \"library\": [\"tensorflow\"]}", "generation": "Scenario 1: \n```python\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\n\n# Constants\nINPUT_SIZE = 10\nHIDDEN_SIZE = 5\nOUTPUT_SIZE = 1\n\n# Sample data\nnp.random.seed(0)\nX = np.random.rand(100, INPUT_SIZE)\ny = np.random.rand(100, OUTPUT_SIZE)\n\ndef build_and_evaluate_model():\n    \"\"\"\n    Build a simple neural network model, train it on random data, and evaluate its performance.\n\n    Returns:\n    float: The mean squared error of the model's predictions.\n\n    Requirements:\n    - tensorflow\n    - numpy\n    - sklearn.model_selection\n    - sklearn.preprocessing\n    - sklearn.metrics\n    \n    Example:\n    >>> build_and_evaluate_model()\n    \"\"\"\n    # Split the data into training and test sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    # Standardize the features\n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_test = scaler.transform(X_test)\n\n    # Build the model\n    model = Sequential()\n    model.add(Dense(HIDDEN_SIZE, input_dim=INPUT_SIZE, activation='relu'))\n    model.add(Dense(OUTPUT_SIZE, activation='linear'))\n\n    # Compile and train the model\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    model.fit(X_train, y_train, epochs=50, verbose=0)\n\n    # Evaluate the model\n    predictions = model.predict(X_test)\n    mse = mean_squared_error(y_test, predictions)\n\n    return mse\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \n```python\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n# Constants\nVOCAB_SIZE = 10000\nEMBEDDING_DIM = 64\nMAX_LENGTH = 100\nTRUNC_TYPE='post'\nPADDING_TYPE='post'\nOOV_TOK = \"<OOV>\"\nTRAINING_PORTION = .8\n\n# Sample data\nnp.random.seed(0)\ntexts = ['I love machine learning', 'I hate coding', 'Python is awesome', 'Java is good', 'I love Python']\nlabels = ['positive', 'negative', 'positive', 'positive', 'positive']\n\ndef text_classification():\n    \"\"\"\n    Build a text classification model using LSTM and train it on sample data.\n\n    Returns:\n    float: The accuracy of the model on the test set.\n\n    Requirements:\n    - tensorflow\n    - numpy\n    - sklearn.model_selection\n    - sklearn.preprocessing\n    \n    Example:\n    >>> text_classification()\n    \"\"\"\n    # Split the data into training and test sets\n    train_size = int(len(texts) * TRAINING_PORTION)\n\n    train_texts = texts[:train_size]\n    train_labels = labels[:train_size]\n\n    test_texts = texts[train_size:]\n    test_labels = labels[train_size:]\n\n    # Tokenize the texts\n    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=VOCAB_SIZE, oov_token=OOV_TOK)\n    tokenizer.fit_on_texts(train_texts)\n    train_sequences = tokenizer.texts_to_sequences(train_texts)\n    train_padded = pad_sequences(train_sequences, maxlen=MAX_LENGTH, padding=PADDING_TYPE, truncating=TRUNC_TYPE)\n\n    test_sequences = tokenizer.texts_to_sequences(test_texts)\n    test_padded = pad_sequences(test_sequences, maxlen=MAX_LENGTH, padding=PADDING_TYPE, truncating=TRUNC_TYPE)\n\n    # Encode the labels\n    label_encoder = LabelEncoder()\n    train_labels = label_encoder.fit_transform(train_labels)\n    test_labels = label_encoder.transform(test_labels)\n\n    # Build the model\n    model = Sequential()\n    model.add(Embedding(VOCAB_SIZE, EMBEDDING_DIM))\n    model.add(LSTM(32))\n    model.add(Dense(1, activation='sigmoid'))\n\n    # Compile and train the model\n    model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n    model.fit(train_padded, train_labels, epochs=10, verbose=0)\n\n    # Evaluate the model\n    _, accuracy = model.evaluate(test_padded, test_labels, verbose=0)\n\n    return accuracy\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \n```python\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport numpy as np\n\n# Constants\nINPUT_SHAPE = (64, 64, 3)\nBATCH_SIZE = 32\nNUM_CLASSES = 2\nEPOCHS = 10\n\n# Sample data\ntrain_datagen = ImageDataGenerator(rescale = 1./255,\n                                   shear_range = 0.2,\n                                   zoom_range = 0.2,\n                                   horizontal_flip = True)\ntest_datagen = ImageDataGenerator(rescale = 1./255)\ntraining_set = train_datagen.flow_from_directory('data/train',\n                                                 target_size = (64, 64),\n                                                 batch_size = 32,\n                                                 class_mode = 'binary')\ntest_set = test_datagen.flow_from_directory('data/test',\n                                            target_size = (64, 64),\n                                            batch_size = 32,\n                                            class_mode = 'binary')\n\ndef image_classification():\n    \"\"\"\n    Build a simple convolutional neural network model, train it on image data, and evaluate its performance.\n\n    Returns:\n    str: The classification report of the model's predictions.\n\n    Requirements:\n    - tensorflow\n    - sklearn.metrics\n    - numpy\n    \n    Example:\n    >>> image_classification()\n    \"\"\"\n    # Build the model\n    model = Sequential()\n    model.add(Conv2D(32, (3, 3), input_shape = INPUT_SHAPE, activation = 'relu'))\n    model.add(MaxPooling2D(pool_size = (2, 2)))\n    model.add(Flatten())\n    model.add(Dense(units = 128, activation = 'relu'))\n    model.add(Dense(units = 1, activation = 'sigmoid'))\n\n    # Compile and train the model\n    model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n    model.fit(training_set, epochs = EPOCHS, verbose=0)\n\n    # Predict the test set results\n    y_pred = model.predict(test_set)\n    y_pred = (y_pred > 0.5)\n\n    # Create the confusion matrix\n    cm = confusion_matrix(test_set.classes, y_pred)\n\n    # Create a classification report\n    report = classification_report(test_set.classes, y_pred)\n\n    return report\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \n```python\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow.keras.layers import Input, Dense\nfrom tensorflow.keras.models import Model\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\n# Constants\nINPUT_DIM = 100\nENCODING_DIM = 50\n\n# Sample data\nnp.random.seed(0)\nX = np.random.rand(10000, INPUT_DIM)\n\ndef autoencoder():\n    \"\"\"\n    Build a simple autoencoder model, train it on random data, and evaluate its performance.\n\n    Returns:\n    float: The mean squared error of the model's reconstructions.\n\n    Requirements:\n    - tensorflow\n    - numpy\n    - sklearn.preprocessing\n    - sklearn.model_selection\n    - sklearn.metrics\n    \n    Example:\n    >>> autoencoder()\n    \"\"\"\n    # Split the data into training and test sets\n    X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n\n    # Standardize the features\n    scaler = MinMaxScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_test = scaler.transform(X_test)\n\n    # Build the model\n    input_layer = Input(shape=(INPUT_DIM,))\n    encoded = Dense(ENCODING_DIM, activation='relu')(input_layer)\n    decoded = Dense(INPUT_DIM, activation='sigmoid')(encoded)\n    autoencoder = Model(input_layer, decoded)\n\n    # Compile and train the model\n    autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n    autoencoder.fit(X_train, X_train, epochs=50, batch_size=256, shuffle=True, verbose=0)\n\n    # Make predictions and calculate the mean squared error\n    predictions = autoencoder.predict(X_test)\n    mse = mean_squared_error(X_test, predictions)\n\n    return mse\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \n```python\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow.keras.layers import Input, LSTM, RepeatVector\nfrom tensorflow.keras.models import Model\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\n# Constants\nTIMESTEPS = 10\nINPUT_DIM = 1\nLATENT_DIM = 2\n\n# Sample data\nnp.random.seed(0)\nX = np.random.rand(1000, TIMESTEPS, INPUT_DIM)\n\ndef lstm_autoencoder():\n    \"\"\"\n    Build a LSTM-based autoencoder model, train it on random data, and evaluate its performance.\n\n    Returns:\n    float: The mean squared error of the model's reconstructions.\n\n    Requirements:\n    - tensorflow\n    - numpy\n    - sklearn.preprocessing\n    - sklearn.model_selection\n    - sklearn.metrics\n    \n    Example:\n    >>> lstm_autoencoder()\n    \"\"\"\n    # Split the data into training and test sets\n    X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n\n    # Build the model\n    inputs = Input(shape=(TIMESTEPS, INPUT_DIM))\n    encoded = LSTM(LATENT_DIM)(inputs)\n\n    decoded = RepeatVector(TIMESTEPS)(encoded)\n    decoded = LSTM(INPUT_DIM, return_sequences=True)(decoded)\n\n    sequence_autoencoder = Model(inputs, decoded)\n\n    # Compile and train the model\n    sequence_autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n    sequence_autoencoder.fit(X_train, X_train, epochs=100, batch_size=100, verbose=0)\n\n    # Make predictions and calculate the mean squared error\n    predictions = sequence_autoencoder.predict(X_test)\n    mse = mean_squared_error(X_test.flatten(), predictions.flatten())\n\n    return mse\n```"}
{"seed": "{\"task_id\": 38276, \"prompt\": \"def f_38276(text):\\n\\t\", \"suffix\": \"\\n\\treturn list\", \"canonical_solution\": \"\\n\\tpattern = r\\\"([0-9]+)\\\"\\n\\tlist=re.findall(pattern,text)\\n\", \"test_start\": \"\\nimport re\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('fg456fgxnd') == ['456']\\n\"], \"entry_point\": \"f_38276\", \"intent\": \"\\u6b63\\u898f\\u8868\\u73fe\\u3067\\u6587\\u5b57\\u5217`text`\\u306e\\u4e2d\\u304b\\u3089\\u6570\\u5024\\u3060\\u3051\\u3092\\u62bd\\u51fa\\u3057\\u3066\\u30ea\\u30b9\\u30c8`list`\\u306b\\u3059\\u308b\", \"library\": [\"re\"]}", "generation": "Scenario 1: \nre, string, random, pandas, numpy\n```python\nimport re\nimport string\nimport random\nimport pandas as pd\nimport numpy as np\n\n# Constants\nWORD_COUNT = 1000\nWORD_LENGTH = 5\nNUMERIC_REGEX = r'([0-9]+)'\n\ndef generate_numeric_dataframe():\n    \"\"\"\n    Generate a pandas DataFrame with a column of random alphanumeric strings and another column \n    of lists of numeric strings extracted from the alphanumeric strings.\n\n    The DataFrame should have 1000 rows.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the generated data.\n\n    Requirements:\n    - re\n    - string\n    - random\n    - pandas\n    - numpy\n\n    Example:\n    >>> df = generate_numeric_dataframe()\n    >>> print(df)\n    \"\"\"\n    words = [''.join(random.choices(string.ascii_letters + string.digits, k=WORD_LENGTH)) for _ in range(WORD_COUNT)]\n    numeric_values = [re.findall(NUMERIC_REGEX, word) for word in words]\n\n    df = pd.DataFrame({\n        'Alphanumeric Strings': words,\n        'Numeric Values': numeric_values\n    })\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nre, os, glob, natsort, pandas\n```python\nimport re\nimport os\nimport glob\nimport natsort\nimport pandas as pd\n\n# Constants\nDIRECTORY = './'\nFILE_PATTERN = '*.txt'\nREGEX = r'([0-9]+)'\n\ndef extract_numeric_data_from_files():\n    \"\"\"\n    Extract numeric data from all .txt files in a directory, \n    and then compile them into a pandas DataFrame.\n\n    The DataFrame should have two columns: 'Filename' and 'Numeric Data'.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the extracted data.\n\n    Requirements:\n    - re\n    - os\n    - glob\n    - natsort\n    - pandas\n\n    Example:\n    >>> df = extract_numeric_data_from_files()\n    >>> print(df)\n    \"\"\"\n    files = natsort.natsorted(glob.glob(os.path.join(DIRECTORY, FILE_PATTERN)))\n    data = []\n\n    for filename in files:\n        with open(filename, 'r') as file:\n            content = file.read()\n        numeric_data = re.findall(REGEX, content)\n        data.append([filename, numeric_data])\n\n    df = pd.DataFrame(data, columns=['Filename', 'Numeric Data'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nre, json, pandas, numpy\n```python\nimport re\nimport json\nimport pandas as pd\nimport numpy as np\n\n# Constants\nJSON_DATA = '[{\"text\": \"The price is $123.45.\"}, {\"text\": \"It costs 67.89 dollars.\"}]'\nREGEX = r'([0-9\\.]+)'\n\ndef extract_numeric_data_from_json():\n    \"\"\"\n    Extract numeric data from a JSON string, \n    and then compile them into a pandas DataFrame.\n\n    The DataFrame should have two columns: 'Text' and 'Numeric Data'.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the extracted data.\n\n    Requirements:\n    - re\n    - json\n    - pandas\n    - numpy\n\n    Example:\n    >>> df = extract_numeric_data_from_json()\n    >>> print(df)\n    \"\"\"\n    data = json.loads(JSON_DATA)\n    df_data = []\n\n    for item in data:\n        text = item['text']\n        numeric_data = re.findall(REGEX, text)\n        df_data.append([text, numeric_data])\n\n    df = pd.DataFrame(df_data, columns=['Text', 'Numeric Data'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nre, csv, pandas, numpy\n```python\nimport re\nimport csv\nimport pandas as pd\nimport numpy as np\n\n# Constants\nCSV_FILENAME = 'sample.csv'\nREGEX = r'([0-9]+)'\n\ndef extract_numeric_data_from_csv():\n    \"\"\"\n    Extract numeric data from a CSV file, \n    and then compile them into a pandas DataFrame.\n\n    The DataFrame should have two columns: 'Text' and 'Numeric Data'.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the extracted data.\n\n    Requirements:\n    - re\n    - csv\n    - pandas\n    - numpy\n\n    Example:\n    >>> df = extract_numeric_data_from_csv()\n    >>> print(df)\n    \"\"\"\n    with open(CSV_FILENAME, 'r') as file:\n        reader = csv.reader(file)\n        data = list(reader)\n\n    df_data = []\n\n    for row in data:\n        text = row[0]\n        numeric_data = re.findall(REGEX, text)\n        df_data.append([text, numeric_data])\n\n    df = pd.DataFrame(df_data, columns=['Text', 'Numeric Data'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nre, requests, bs4, pandas\n```python\nimport re\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\n# Constants\nURL = 'https://example.com'\nREGEX = r'([0-9]+)'\n\ndef extract_numeric_data_from_webpage():\n    \"\"\"\n    Extract numeric data from a webpage, \n    and then compile them into a pandas DataFrame.\n\n    The DataFrame should have two columns: 'Text' and 'Numeric Data'.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the extracted data.\n\n    Requirements:\n    - re\n    - requests\n    - bs4\n    - pandas\n\n    Example:\n    >>> df = extract_numeric_data_from_webpage()\n    >>> print(df)\n    \"\"\"\n    response = requests.get(URL)\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    paragraphs = soup.find_all('p')\n    df_data = []\n\n    for paragraph in paragraphs:\n        text = paragraph.text\n        numeric_data = re.findall(REGEX, text)\n        df_data.append([text, numeric_data])\n\n    df = pd.DataFrame(df_data, columns=['Text', 'Numeric Data'])\n\n    return df\n```"}
{"seed": "{\"task_id\": 49558, \"prompt\": \"def f_49558(x, y, df):\\n\\t\", \"suffix\": \"\\n\\treturn rp\", \"canonical_solution\": \"rp = sns.regplot(x, y, data=df, order=1, line_kws={\\\"color\\\":\\\"indianred\\\"})\\n\\trp.axes.set_ylim(0,)\", \"test_start\": \"\\nimport pandas as pd\\nimport seaborn as sns\\n\\ndef check(candidate):\", \"test\": [\"\\n    df = pd.DataFrame([[0, 1, 2], [7, 8, 9]])\\n    rp = candidate(df[0], df[1], df)\\n    assert 'Axes' in str(type(rp))\\n\"], \"entry_point\": \"f_49558\", \"intent\": \"y\\u8ef8\\u306e\\u4e0b\\u9650\\u5024\\u3092\\u6307\\u5b9a\\u3057\\u3001\\u4e0a\\u9650\\u5024\\u306f\\u81ea\\u52d5\\u306b\\u3059\\u308b\", \"library\": [\"pandas\", \"seaborn\"]}", "generation": "Scenario 1: pandas, seaborn, numpy\n```python\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\n\n# Constants\nCOLUMNS = ['Age', 'Salary', 'Experience']\n\ndef generate_and_plot(df):\n    \"\"\"\n    Generate a new column 'Performance' based on 'Age', 'Salary' and 'Experience' columns, \n    then plot a regression plot between 'Performance' and 'Salary' with the y-axis limit set to 0.\n\n    Parameters:\n    df (DataFrame): The input DataFrame with columns 'Age', 'Salary' and 'Experience'.\n\n    Returns:\n    Axes: The Seaborn Axes object.\n\n    Requirements:\n    - pandas\n    - seaborn\n    - numpy\n\n    Example:\n    >>> df = pd.DataFrame(np.random.rand(10, 3), columns=COLUMNS)\n    >>> ax = generate_and_plot(df)\n    >>> ax.set_title('Regression Plot')\n    >>> plt.show()\n    \"\"\"\n    df['Performance'] = df['Age'] * df['Salary'] * df['Experience']\n    ax = sns.regplot('Performance', 'Salary', data=df, order=1, line_kws={\"color\":\"indianred\"})\n    ax.axes.set_ylim(0,)\n    return ax\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: pandas, seaborn, numpy, matplotlib, sklearn.preprocessing\n```python\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Constants\nCOLUMNS = ['Age', 'Salary', 'Experience']\n\ndef scale_and_plot(df):\n    \"\"\"\n    Scale 'Age', 'Salary' and 'Experience' columns in the range [0, 1] using MinMaxScaler,\n    then plot a regression plot between 'Age' and 'Experience' with the y-axis limit set to 0.\n\n    Parameters:\n    df (DataFrame): The input DataFrame with columns 'Age', 'Salary' and 'Experience'.\n\n    Returns:\n    Axes: The Seaborn Axes object.\n\n    Requirements:\n    - pandas\n    - seaborn\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.preprocessing.MinMaxScaler\n\n    Example:\n    >>> df = pd.DataFrame(np.random.randint(20, 50, size=(10, 3)), columns=COLUMNS)\n    >>> ax = scale_and_plot(df)\n    >>> ax.set_title('Regression Plot')\n    >>> plt.show()\n    \"\"\"\n    scaler = MinMaxScaler()\n    df[COLUMNS] = scaler.fit_transform(df[COLUMNS])\n\n    ax = sns.regplot('Age', 'Experience', data=df, order=1, line_kws={\"color\":\"indianred\"})\n    ax.axes.set_ylim(0,)\n    return ax\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: pandas, seaborn, matplotlib, sklearn.linear_model\n```python\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n# Constants\nCOLUMNS = ['Age', 'Salary', 'Experience']\n\ndef fit_and_plot(df):\n    \"\"\"\n    Fit a linear regression model on 'Age' and 'Experience' columns,\n    then plot a regression plot between the predicted and actual values of 'Experience' with the y-axis limit set to 0.\n\n    Parameters:\n    df (DataFrame): The input DataFrame with columns 'Age', 'Salary' and 'Experience'.\n\n    Returns:\n    Axes: The Seaborn Axes object.\n\n    Requirements:\n    - pandas\n    - seaborn\n    - matplotlib.pyplot\n    - sklearn.linear_model.LinearRegression\n\n    Example:\n    >>> df = pd.DataFrame(np.random.rand(10, 3), columns=COLUMNS)\n    >>> ax = fit_and_plot(df)\n    >>> ax.set_title('Regression Plot')\n    >>> plt.show()\n    \"\"\"\n    X = df[['Age']]\n    y = df['Experience']\n    model = LinearRegression().fit(X, y)\n    df['Predicted Experience'] = model.predict(X)\n\n    ax = sns.regplot('Experience', 'Predicted Experience', data=df, order=1, line_kws={\"color\":\"indianred\"})\n    ax.axes.set_ylim(0,)\n    return ax\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: pandas, seaborn, numpy, matplotlib, sklearn.decomposition\n```python\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\n# Constants\nCOLUMNS = ['Age', 'Salary', 'Experience']\n\ndef pca_and_plot(df):\n    \"\"\"\n    Perform PCA on 'Age', 'Salary' and 'Experience' columns and keep only 1st principal component,\n    then plot a regression plot between 1st principal component and 'Salary' with the y-axis limit set to 0.\n\n    Parameters:\n    df (DataFrame): The input DataFrame with columns 'Age', 'Salary' and 'Experience'.\n\n    Returns:\n    Axes: The Seaborn Axes object.\n\n    Requirements:\n    - pandas\n    - seaborn\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.decomposition.PCA\n\n    Example:\n    >>> df = pd.DataFrame(np.random.rand(10, 3), columns=COLUMNS)\n    >>> ax = pca_and_plot(df)\n    >>> ax.set_title('Regression Plot')\n    >>> plt.show()\n    \"\"\"\n    pca = PCA(n_components=1)\n    df['First PC'] = pca.fit_transform(df[COLUMNS])\n\n    ax = sns.regplot('First PC', 'Salary', data=df, order=1, line_kws={\"color\":\"indianred\"})\n    ax.axes.set_ylim(0,)\n    return ax\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: pandas, seaborn, numpy, matplotlib, sklearn.cluster\n```python\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\n\n# Constants\nCOLUMNS = ['Age', 'Salary', 'Experience']\n\ndef cluster_and_plot(df):\n    \"\"\"\n    Perform K-means clustering on 'Age', 'Salary' and 'Experience' columns and keep cluster labels,\n    then plot a regression plot between cluster labels and 'Salary' with the y-axis limit set to 0.\n\n    Parameters:\n    df (DataFrame): The input DataFrame with columns 'Age', 'Salary' and 'Experience'.\n\n    Returns:\n    Axes: The Seaborn Axes object.\n\n    Requirements:\n    - pandas\n    - seaborn\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.cluster.KMeans\n\n    Example:\n    >>> df = pd.DataFrame(np.random.rand(10, 3), columns=COLUMNS)\n    >>> ax = cluster_and_plot(df)\n    >>> ax.set_title('Regression Plot')\n    >>> plt.show()\n    \"\"\"\n    kmeans = KMeans(n_clusters=3)\n    df['Cluster'] = kmeans.fit_predict(df[COLUMNS])\n\n    ax = sns.regplot('Cluster', 'Salary', data=df, order=1, line_kws={\"color\":\"indianred\"})\n    ax.axes.set_ylim(0,)\n    return ax\n```\nAbove is the illustration."}
{"seed": "{\"task_id\": 65284, \"prompt\": \"def f_65284(word, h):\\n\\t\", \"suffix\": \"\\n\\treturn word\", \"canonical_solution\": \"tmp1 = word[:h]\\n\\tword = word[h:]\\n\\tword.extend(tmp1)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([\\\"a\\\",\\\"b\\\",\\\"c\\\",\\\"d\\\",\\\"e\\\",\\\"f\\\",\\\"g\\\"], 4) == [\\\"e\\\",\\\"f\\\",\\\"g\\\",\\\"a\\\",\\\"b\\\",\\\"c\\\",\\\"d\\\"]\\n\"], \"entry_point\": \"f_65284\", \"intent\": \"\\u6587\\u5b57\\u5217`word`\\u306e\\u9806\\u756a\\u3092`h`\\u756a\\u76ee\\u3067\\u5165\\u308c\\u66ff\\u3048\\u308b\", \"library\": []}", "generation": "Scenario 1:\npandas, numpy, random, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nWORDS = ['apple', 'banana', 'cherry', 'date', 'elderberry', 'fig', 'grape']\nN = 1000\n\ndef generate_word_data(h):\n    \"\"\"\n    Generate a DataFrame of randomly selected words from a list, \n    with their order changed at the h-th index.\n    \n    Parameters:\n    h (int): The index at which to change the order of words.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the words and their frequencies.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> word_data = generate_word_data(3)\n    >>> print(word_data)\n    >>> word_data['Frequency'].plot(kind='bar')\n    \"\"\"\n    word_data = []\n\n    for _ in range(N):\n        tmp1 = WORDS[:h]\n        words = WORDS[h:]\n        words.extend(tmp1)\n        word = words[randint(0, len(words)-1)]\n        word_data.append(word)\n\n    word_df = pd.DataFrame(word_data, columns=['Word'])\n    word_df['Frequency'] = word_df.groupby('Word')['Word'].transform('count')\n\n    return word_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, itertools, collections\n```python\nimport numpy as np\nfrom itertools import islice\nfrom collections import Counter\n\n# Constants\nWORDS = np.array(['apple', 'banana', 'cherry', 'date', 'elderberry', 'fig', 'grape'])\nN = 1000\n\ndef count_word_frequency(h):\n    \"\"\"\n    Count the frequency of each word in a circularly shifted sequence of words.\n\n    Parameters:\n    h (int): The number of places to shift the sequence.\n\n    Returns:\n    Counter: A Counter object with word frequencies.\n\n    Requirements:\n    - numpy\n    - itertools.islice\n    - collections.Counter\n\n    Example:\n    >>> word_freq = count_word_frequency(3)\n    >>> print(word_freq)\n    \"\"\"\n    word_seq = np.roll(WORDS, h)\n    word_seq = islice(itertools.cycle(word_seq), N)\n    word_freq = Counter(word_seq)\n\n    return word_freq\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, matplotlib, random\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\n\n# Constants\nWORDS = ['apple', 'banana', 'cherry', 'date', 'elderberry', 'fig', 'grape']\nN = 1000\n\ndef plot_word_distribution(h):\n    \"\"\"\n    Plot the distribution of words in a sequence that has been circularly shifted.\n\n    Parameters:\n    h (int): The number of places to shift the sequence.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> plot_word_distribution(3)\n    \"\"\"\n    word_seq = np.roll(WORDS, h)\n    word_freq = [random.choice(word_seq) for _ in range(N)]\n    plt.hist(word_freq, bins=len(WORDS), edgecolor='black')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, scipy.stats, pandas\n```python\nimport numpy as np\nfrom scipy import stats\nimport pandas as pd\n\n# Constants\nWORDS = ['apple', 'banana', 'cherry', 'date', 'elderberry', 'fig', 'grape']\nN = 1000\n\ndef word_distribution_statistics(h):\n    \"\"\"\n    Calculate the mean, median, mode, and standard deviation of word frequencies \n    in a circularly shifted sequence of words.\n\n    Parameters:\n    h (int): The number of places to shift the sequence.\n\n    Returns:\n    dict: A dictionary with the mean, median, mode, and standard deviation.\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - pandas\n\n    Example:\n    >>> stats = word_distribution_statistics(3)\n    >>> print(stats)\n    \"\"\"\n    word_seq = np.roll(WORDS, h)\n    word_freq = pd.Series(word_seq).value_counts().values\n\n    mean = np.mean(word_freq)\n    median = np.median(word_freq)\n    mode = stats.mode(word_freq)[0][0]\n    std_dev = np.std(word_freq)\n\n    return {'mean': mean, 'median': median, 'mode': mode, 'std_dev': std_dev}\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, sklearn.preprocessing, matplotlib\n```python\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nimport matplotlib.pyplot as plt\n\n# Constants\nWORDS = ['apple', 'banana', 'cherry', 'date', 'elderberry', 'fig', 'grape']\nN = 1000\n\ndef encode_and_plot_words(h):\n    \"\"\"\n    Encode words in a circularly shifted sequence as integers and plot their distribution.\n\n    Parameters:\n    h (int): The number of places to shift the sequence.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - sklearn.preprocessing.LabelEncoder\n    - matplotlib.pyplot\n\n    Example:\n    >>> encode_and_plot_words(3)\n    \"\"\"\n    word_seq = np.roll(WORDS, h)\n    encoder = LabelEncoder()\n    encoded_words = encoder.fit_transform(word_seq)\n    plt.hist(encoded_words, bins=len(WORDS), edgecolor='black')\n    plt.show()\n```\n"}
{"seed": "{\"task_id\": 31916, \"prompt\": \"def f_31916(attributes):\\n\\t\", \"suffix\": \"\\n\\treturn attributes\", \"canonical_solution\": \"for i, attribute in enumerate(attributes):\\n\\t    attributes[i] = attribute-1\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([1,2,3]) == [0,1,2]\\n\", \"\\n    assert candidate([100]) == [99]\\n\"], \"entry_point\": \"f_31916\", \"intent\": \"\\u30ea\\u30b9\\u30c8`attributes`\\u306e\\u5168\\u8981\\u7d20\\u306e\\u6570\\u5024\\u306b\\u5bfe\\u3057\\u3066for\\u6587\\u3067\\u30de\\u30a4\\u30ca\\u30b91\\u3059\\u308b\", \"library\": []}", "generation": "Scenario 1:\npandas, numpy, json, ast\n```python\nimport pandas as pd\nimport numpy as np\nimport json\nfrom ast import literal_eval\n\n# Sample data\n# STRING_JSONS = '[{\"id\": 1, \"name\": \"John\", \"age\": 30, \"city\": \"New York\"}, {\"id\": 2, \"name\": \"Jane\", \"age\": 25, \"city\": \"London\"}]'\n\ndef process_jsons(string_jsons):\n    \"\"\"\n    Process a string of JSON objects: convert the string to a list of dictionaries, \n    create a pandas DataFrame from the list, and then subtract 1 from the 'age' \n    column for each row in the DataFrame.\n\n    Parameters:\n    string_jsons (str): The string of JSON objects.\n\n    Returns:\n    DataFrame: The processed DataFrame.\n\n    Requirements:\n    - pandas\n    - numpy\n    - json\n    - ast\n\n    Example:\n    >>> process_jsons('[{\"id\": 1, \"name\": \"John\", \"age\": 30, \"city\": \"New York\"}, {\"id\": 2, \"name\": \"Jane\", \"age\": 25, \"city\": \"London\"}]')\n    \"\"\"\n    json_list = literal_eval(string_jsons)\n    df = pd.DataFrame(json_list)\n    df['age'] = df['age'].apply(lambda x: x-1)\n    \n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, matplotlib, pandas, sklearn\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\n\n# Constants\nMIN_AGE = 1\nMAX_AGE = 100\nNUM_SAMPLES = 100\n\ndef generate_normalize_plot(attributes):\n    \"\"\"\n    Generate a list of random ages, normalize the ages using sklearn's MinMaxScaler, \n    and then plot a histogram of the normalized ages.\n\n    Parameters:\n    attributes (list of int): The list of ages.\n\n    Returns:\n    list: The normalized ages.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib\n    - sklearn\n\n    Example:\n    >>> generate_normalize_plot(list(range(1, 101)))\n    \"\"\"\n    attributes = np.array(attributes).reshape(-1, 1)\n    min_max_scaler = preprocessing.MinMaxScaler()\n    attributes_scaled = min_max_scaler.fit_transform(attributes)\n\n    plt.hist(attributes_scaled, bins=10)\n    plt.show()\n\n    return attributes_scaled.flatten().tolist()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, pandas, matplotlib, sklearn\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n# Constants\nNUM_POINTS = 100\n\ndef perform_linear_regression(attributes):\n    \"\"\"\n    Perform linear regression on a list of attributes.\n\n    Parameters:\n    attributes (list of int): The list of attributes.\n\n    Returns:\n    list: The predicted values based on the linear regression model.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib\n    - sklearn\n\n    Example:\n    >>> perform_linear_regression(list(range(1, 101)))\n    \"\"\"\n    X = np.array(attributes).reshape(-1, 1)\n    y = np.array(attributes).reshape(-1, 1)\n\n    reg = LinearRegression().fit(X, y)\n\n    predictions = reg.predict(X)\n\n    plt.scatter(X, y, color = 'blue')\n    plt.plot(X, predictions, color = 'red')\n    plt.show()\n\n    return predictions.flatten().tolist()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, pandas, matplotlib, sklearn\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\n\n# Constants\nNUM_CLUSTERS = 3\n\ndef perform_kmeans_clustering(attributes):\n    \"\"\"\n    Perform k-means clustering on a list of attributes.\n\n    Parameters:\n    attributes (list of int): The list of attributes.\n\n    Returns:\n    list: The cluster labels for each attribute.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib\n    - sklearn\n\n    Example:\n    >>> perform_kmeans_clustering(list(range(1, 101)))\n    \"\"\"\n    X = np.array(attributes).reshape(-1, 1)\n\n    kmeans = KMeans(n_clusters=NUM_CLUSTERS, random_state=0).fit(X)\n\n    labels = kmeans.labels_\n\n    plt.scatter(X, [0]*len(X), c=labels)\n    plt.show()\n\n    return labels.tolist()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, pandas, seaborn, sklearn\n```python\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\n\n# Constants\nNUM_SAMPLES = 100\n\ndef standardize_plot(attributes):\n    \"\"\"\n    Standardize a list of attributes using sklearn's StandardScaler and then plot a \n    distribution plot of the standardized attributes using seaborn.\n\n    Parameters:\n    attributes (list of int): The list of attributes.\n\n    Returns:\n    list: The standardized attributes.\n\n    Requirements:\n    - numpy\n    - pandas\n    - seaborn\n    - sklearn\n\n    Example:\n    >>> standardize_plot(list(range(1, 101)))\n    \"\"\"\n    attributes = np.array(attributes).reshape(-1, 1)\n    scaler = StandardScaler()\n    attributes_scaled = scaler.fit_transform(attributes)\n\n    sns.distplot(attributes_scaled)\n    plt.show()\n\n    return attributes_scaled.flatten().tolist()\n```"}
{"seed": "{\"task_id\": 31916, \"prompt\": \"def f_31916(attributes):\\n\\t\", \"suffix\": \"\\n\\treturn attributes\", \"canonical_solution\": \"for i in range(len(attributes)):\\n\\t    attributes[i] = attributes[i]-1\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([1,2,3,4]) == [0,1,2,3]\\n\", \"\\n    assert candidate([1., 2, 3.4, 5.5]) == [0., 1, 2.4, 4.5]\\n\"], \"entry_point\": \"f_31916\", \"intent\": \"\\u30ea\\u30b9\\u30c8`attributes`\\u306e\\u5168\\u8981\\u7d20\\u306e\\u6570\\u5024\\u306b\\u5bfe\\u3057\\u3066for\\u6587\\u3067\\u30de\\u30a4\\u30ca\\u30b91\\u3059\\u308b\", \"library\": []}", "generation": "Scenario 1:\nnumpy, pandas, matplotlib, random\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nCOLORS = ['Red', 'Blue', 'Green', 'Yellow', 'Black', 'White']\n\ndef generate_color_frequency(colors):\n    \"\"\"\n    Generate a bar plot of frequencies of colors in the given list and return the DataFrame.\n\n    Parameters:\n    colors (list): The list of colors.\n\n    Returns:\n    DataFrame: A pandas DataFrame with frequencies of colors.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> colors = [COLORS[randint(0, len(COLORS)-1)] for _ in range(1000)]\n    >>> df = generate_color_frequency(colors)\n    >>> print(df)\n    >>> df.plot(kind='bar')\n    \"\"\"\n    unique, counts = np.unique(colors, return_counts=True)\n    df = pd.DataFrame(zip(unique, counts), columns=['Color', 'Frequency'])\n    df.set_index('Color', inplace=True)\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, scipy.stats, matplotlib\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\ndef fit_normal_distribution(data):\n    \"\"\"\n    Fit a normal distribution to the given data and plot the histogram along with the fitted distribution.\n\n    Parameters:\n    data (numpy array): The data to fit the distribution.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - scipy.stats\n\n    Example:\n    >>> data = np.random.normal(size=1000)\n    >>> fit_normal_distribution(data)\n    \"\"\"\n    mu, std = norm.fit(data)\n\n    plt.hist(data, bins=30, density=True, alpha=0.6, color='g')\n\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, mu, std)\n    plt.plot(x, p, 'k', linewidth=2)\n    title = \"Fit results: mu = %.2f,  std = %.2f\" % (mu, std)\n    plt.title(title)\n\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, sklearn.preprocessing, matplotlib\n```python\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\ndef standardize_dataframe(df):\n    \"\"\"\n    Standardize the numerical columns of a pandas DataFrame and plot the histograms before and after standardization.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame to standardize.\n\n    Returns:\n    DataFrame: The standardized DataFrame.\n\n    Requirements:\n    - pandas\n    - sklearn.preprocessing\n    - matplotlib.pyplot\n\n    Example:\n    >>> data = {'score': [234,24,14,27,-74,46,73,-18,59,160]}\n    >>> df = pd.DataFrame(data)\n    >>> df_standardized = standardize_dataframe(df)\n    \"\"\"\n    df.hist()\n    scaler = StandardScaler()\n    df_standardized = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n    df_standardized.hist()\n    plt.show()\n    \n    return df_standardized\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, scipy, matplotlib\n```python\nimport numpy as np\nfrom scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\n\ndef fit_curve(x, y):\n    \"\"\"\n    Fit a curve to the given x and y data and plot the fitted curve.\n\n    Parameters:\n    x (numpy array): The x data.\n    y (numpy array): The y data.\n\n    Requirements:\n    - numpy\n    - scipy.optimize\n    - matplotlib.pyplot\n\n    Example:\n    >>> x = np.linspace(0, 10, 100)\n    >>> y = 3.45 * np.exp(1.01 * x) + np.random.normal(size=100)\n    >>> fit_curve(x, y)\n    \"\"\"\n    def func(x, a, b):\n        return a * np.exp(b * x)\n\n    popt, pcov = curve_fit(func, x, y)\n\n    plt.scatter(x, y)\n    plt.plot(x, func(x, *popt), 'r-')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_correlation_matrix(df):\n    \"\"\"\n    Plot the correlation matrix of a pandas DataFrame.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> data = {'A': np.random.rand(50), 'B': np.random.rand(50), 'C': np.random.rand(50)}\n    >>> df = pd.DataFrame(data)\n    >>> plot_correlation_matrix(df)\n    \"\"\"\n    corr = df.corr()\n    fig, ax = plt.subplots(figsize=(10, 10))\n    cax = ax.matshow(corr, cmap='coolwarm')\n    fig.colorbar(cax)\n    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n    plt.yticks(range(len(corr.columns)), corr.columns)\n    \n    plt.show()\n```"}
{"seed": "{\"task_id\": 31916, \"prompt\": \"def f_31916(attributes):\\n\\t\", \"suffix\": \"\\n\\treturn attributes\", \"canonical_solution\": \"attributes = [attribute-1 for attribute in attributes]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([1,2,3,4]) == [0,1,2,3]\\n\", \"\\n    assert candidate([1., 2, 3.4, 5.5]) == [0., 1, 2.4, 4.5]\\n\"], \"entry_point\": \"f_31916\", \"intent\": \"\\u30ea\\u30b9\\u30c8`attributes`\\u306e\\u5168\\u8981\\u7d20\\u306e\\u6570\\u5024\\u306b\\u5bfe\\u3057\\u3066for\\u6587\\u3067\\u30de\\u30a4\\u30ca\\u30b91\\u3059\\u308b\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, matplotlib, random\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import uniform\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\nROWS = 100\n\ndef generate_dataframe_and_plot(attributes):\n    \"\"\"\n    Generate a DataFrame using provided attributes and a random number and plot a histogram.\n\n    Parameters:\n    attributes (list): The list of attributes.\n\n    Returns:\n    DataFrame: A pandas DataFrame.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - random.uniform\n\n    Example:\n    >>> df = generate_dataframe_and_plot([1, 2, 3, 4, 5])\n    >>> print(df)\n    >>> df.hist(bins=10)\n    \"\"\"\n    data = {col: [attr + uniform(-0.5, 0.5) for _ in range(ROWS)] for attr, col in zip(attributes, COLUMNS)}\n    df = pd.DataFrame(data)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, matplotlib, itertools\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom itertools import cycle\n\n# Constants\nCOLORS = cycle(['b', 'g', 'r', 'c', 'm', 'y', 'k'])\n\ndef plot_attributes(attributes):\n    \"\"\"\n    Plot a sine wave for each attribute in the list with different colors.\n\n    Parameters:\n    attributes (list): The list of attributes.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - itertools.cycle\n\n    Example:\n    >>> plot_attributes([1, 2, 3, 4, 5])\n    \"\"\"\n    x = np.linspace(0, 2 * np.pi, 100)\n\n    for attr, color in zip(attributes, COLORS):\n        y = attr * np.sin(x)\n        plt.plot(x, y, color=color)\n\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, scipy.optimize, matplotlib\n```python\nimport numpy as np\nfrom scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\n\n# Constants\nX_VALUES = np.linspace(0, 4, 50)\n\ndef fit_curve(attributes):\n    \"\"\"\n    Fit a curve to the attributes using a polynomial function and plot the results.\n\n    Parameters:\n    attributes (list): The list of attributes.\n\n    Returns:\n    list: The optimized parameters.\n\n    Requirements:\n    - numpy\n    - scipy.optimize.curve_fit\n    - matplotlib.pyplot\n\n    Example:\n    >>> fit_curve([1, 2, 3, 4, 5])\n    \"\"\"\n    def func(x, a, b, c):\n        return a * np.exp(-b * x) + c\n\n    y_values = np.array(attributes) + 0.2 * np.random.normal(size=len(attributes))\n    popt, pcov = curve_fit(func, X_VALUES, y_values)\n\n    plt.figure()\n    plt.plot(X_VALUES, y_values, 'ko', label=\"Original Data\")\n    plt.plot(X_VALUES, func(X_VALUES, *popt), 'r-', label=\"Fitted Curve\")\n    plt.legend()\n    plt.show()\n\n    return popt.tolist()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, sklearn.decomposition, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\nROWS = 100\n\ndef perform_pca(attributes):\n    \"\"\"\n    Perform Principal Component Analysis (PCA) and plot the explained variance ratio.\n\n    Parameters:\n    attributes (list): The list of attributes.\n\n    Returns:\n    PCA: The PCA object.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.decomposition.PCA\n    - matplotlib.pyplot\n\n    Example:\n    >>> pca = perform_pca([1, 2, 3, 4, 5])\n    >>> print(pca.explained_variance_ratio_)\n    \"\"\"\n    data = {col: attr + np.random.randn(ROWS) for attr, col in zip(attributes, COLUMNS)}\n    df = pd.DataFrame(data)\n\n    pca = PCA(n_components=2)\n    pca.fit(df)\n\n    plt.bar(range(pca.n_components_), pca.explained_variance_ratio_)\n    plt.xlabel('PCA features')\n    plt.ylabel('variance %')\n    plt.xticks(range(pca.n_components_))\n\n    plt.show()\n\n    return pca\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, sklearn.cluster, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\nROWS = 100\n\ndef perform_kmeans_clustering(attributes):\n    \"\"\"\n    Perform KMeans clustering and plot the clusters.\n\n    Parameters:\n    attributes (list): The list of attributes.\n\n    Returns:\n    KMeans: The KMeans object.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.cluster.KMeans\n    - matplotlib.pyplot\n\n    Example:\n    >>> kmeans = perform_kmeans_clustering([1, 2, 3, 4, 5])\n    >>> print(kmeans.labels_)\n    \"\"\"\n    data = {col: attr + np.random.randn(ROWS) for attr, col in zip(attributes, COLUMNS)}\n    df = pd.DataFrame(data)\n\n    kmeans = KMeans(n_clusters=2)\n    kmeans.fit(df)\n\n    plt.scatter(df.iloc[:, 0], df.iloc[:, 1], c=kmeans.labels_, cmap='viridis')\n\n    plt.show()\n\n    return kmeans\n```"}
{"seed": "{\"task_id\": 21171, \"prompt\": \"def f_21171(list):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"Counter(map(tuple, list))\", \"test_start\": \"\\nfrom collections import Counter\\n\\ndef check(candidate):\", \"test\": [\"\\n    li=[[1,2,3],[2,3,4],[3,4,5],[4,5,6],[2,3,4],[1,2,3],[2,3,4],[5,6,7]]\\n    c = candidate(li)\\n    assert c.most_common() == [((2, 3, 4), 3), ((1, 2, 3), 2), ((3, 4, 5), 1), ((4, 5, 6), 1), ((5, 6, 7), 1)]\\n\", \"\\n    li = [['abc', 'def'], ['hij', 'klm']]\\n    c = candidate(li)\\n    assert c.most_common() ==[(('abc', 'def'), 1), (('hij', 'klm'), 1)]\\n\"], \"entry_point\": \"f_21171\", \"intent\": \"\\u4e8c\\u6b21\\u5143\\u30ea\\u30b9\\u30c8`list`\\u304b\\u3089\\u91cd\\u8907\\u3059\\u308b\\u8981\\u7d20\\u306e\\u307f\\u62bd\\u51fa\\u3059\\u308b\", \"library\": [\"collections\"]}", "generation": "Scenario 1: \ncollections, itertools, pandas, matplotlib\n```python\nimport collections\nimport itertools\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nPRODUCTS = ['Apple', 'Banana', 'Orange', 'Grape', 'Peach']\n\ndef analyze_purchase_patterns(purchase_lists):\n    \"\"\"\n    Analyze the purchase patterns from a list of purchase lists. \n    Determine the most common combination of products purchased together.\n    Generate a bar plot of the top 5 most common combinations.\n\n    Parameters:\n    purchase_lists (list): A list of lists, each list contains purchased products.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the most common combination of products.\n\n    Requirements:\n    - collections\n    - itertools\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> purchase_lists = [['Apple', 'Banana'], ['Apple', 'Orange', 'Grape'], ['Banana', 'Peach'], ['Apple', 'Banana', 'Peach'], ['Orange', 'Grape']]\n    >>> df = analyze_purchase_patterns(purchase_lists)\n    >>> print(df)\n    >>> df.plot(kind='bar', x='Combination', y='Count')\n    \"\"\"\n    combinations = [tuple(sorted(combination)) for sublist in purchase_lists for combination in itertools.combinations(sublist, 2)]\n    counter = collections.Counter(combinations)\n\n    df = pd.DataFrame(counter.most_common(), columns=['Combination', 'Count']).head(5)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncollections, random, string, pandas\n```python\nimport collections\nimport random\nimport string\nimport pandas as pd\n\n# Constants\nLETTERS = string.ascii_lowercase\n\ndef generate_random_strings(n, k):\n    \"\"\"\n    Generate n random strings of length k and calculate the frequency of each string.\n\n    Parameters:\n    n (int): The number of strings to generate.\n    k (int): The length of each string.\n\n    Returns:\n    DataFrame: A pandas DataFrame with frequencies of each string.\n\n    Requirements:\n    - collections\n    - random\n    - string\n    - pandas\n\n    Example:\n    >>> df = generate_random_strings(10000, 5)\n    >>> print(df)\n    \"\"\"\n    random_strings = [''.join(random.choices(LETTERS, k=k)) for _ in range(n)]\n    counter = collections.Counter(random_strings)\n\n    df = pd.DataFrame(counter.most_common(), columns=['String', 'Frequency'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ncollections, random, matplotlib, pandas\n```python\nimport collections\nimport random\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants\nDIE_SIDES = 6\n\ndef simulate_die_rolls(n):\n    \"\"\"\n    Simulate n rolls of a fair die and plot the frequency of each outcome.\n\n    Parameters:\n    n (int): The number of dice rolls to simulate.\n\n    Returns:\n    None\n\n    Requirements:\n    - collections\n    - random\n    - matplotlib.pyplot\n    - pandas\n\n    Example:\n    >>> simulate_die_rolls(10000)\n    \"\"\"\n    rolls = [random.randint(1, DIE_SIDES) for _ in range(n)]\n    counter = collections.Counter(rolls)\n\n    df = pd.DataFrame(counter.items(), columns=['Die Roll', 'Frequency'])\n\n    df.plot(kind='bar', x='Die Roll', y='Frequency', legend=False)\n    plt.ylabel('Frequency')\n    plt.title(f'Frequency of {n} Die Rolls')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ncollections, numpy, matplotlib\n```python\nimport collections\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef analyze_array_elements(array):\n    \"\"\"\n    Analyze an array of integers and plot a histogram of the frequency of each integer.\n\n    Parameters:\n    array (numpy array): The numpy array to analyze.\n\n    Returns:\n    None\n\n    Requirements:\n    - collections\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> array = np.random.randint(0, 10, size=1000)\n    >>> analyze_array_elements(array)\n    \"\"\"\n    counter = collections.Counter(array)\n\n    plt.bar(counter.keys(), counter.values())\n    plt.xlabel('Integer')\n    plt.ylabel('Frequency')\n    plt.title('Frequency of Integers in Array')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\ncollections, os, pandas\n```python\nimport collections\nimport os\nimport pandas as pd\n\ndef analyze_file_types(directory):\n    \"\"\"\n    Analyze a directory and return a pandas DataFrame with the count of each file type.\n\n    Parameters:\n    directory (str): The directory to analyze.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the counts of each file type.\n\n    Requirements:\n    - collections\n    - os\n    - pandas\n\n    Example:\n    >>> df = analyze_file_types('/home/user/Documents')\n    >>> print(df)\n    \"\"\"\n    file_types = [os.path.splitext(file)[1] for file in os.listdir(directory)]\n    counter = collections.Counter(file_types)\n\n    df = pd.DataFrame(counter.items(), columns=['File Type', 'Count'])\n\n    return df\n```"}
{"seed": "{\"task_id\": 45204, \"prompt\": \"def f_45204(num_of_file):\\n\\t\", \"suffix\": \"\\n\\treturn data\", \"canonical_solution\": \"data = [None] * num_of_file\\n\\tfor i in range(num_of_file):\\n\\t    with open('data{}.txt'.format(i + 1), mode=\\\"r\\\", encoding=\\\"utf-8\\\") as f:\\n\\t        data[i] = f.read()\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    num_of_file = 4\\n    for i in range(0, num_of_file):\\n        with open ('data'+str(i + 1)+'.txt', 'w') as f:\\n            f.write(str(i + 1)+'\\\\n')\\n    data = candidate(4)\\n    for i in range(0, num_of_file):\\n        assert data[i] == str(i + 1)+'\\\\n'\\n\"], \"entry_point\": \"f_45204\", \"intent\": \"\\u9023\\u756a\\u500b\\u6570`num_of_file`\\u306etxt\\u30d5\\u30a1\\u30a4\\u30eb`data{}.txt`\\u3092\\u8aad\\u307f\\u8fbc\\u3080\", \"library\": []}", "generation": "Scenario 1:\nos, glob, pandas, numpy, matplotlib\n```python\nimport os\nimport glob\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nDATA_DIR = \"data\"\n\ndef analyze_csv_data(num_of_file):\n    \"\"\"\n    Read the specified number of CSV files from a directory, calculate the mean, \n    median, and standard deviation of the numeric columns, and plot the histogram \n    for each numeric column.\n\n    Parameters:\n    num_of_file (int): The number of files to read.\n\n    Returns:\n    dict: A dictionary with the mean, median, and standard deviation for each file.\n\n    Requirements:\n    - os\n    - glob\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> analyze_csv_data(5)\n    \"\"\"\n    csv_files = glob.glob(os.path.join(DATA_DIR, \"*.csv\"))[:num_of_file]\n    analysis_results = {}\n\n    for file in csv_files:\n        df = pd.read_csv(file)\n        numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n\n        for col in numeric_cols:\n            plt.hist(df[col].dropna(), bins=30)\n            plt.title(f\"Histogram for {col} in {os.path.basename(file)}\")\n            plt.show()\n\n        analysis_results[os.path.basename(file)] = {\n            \"mean\": df[numeric_cols].mean().to_dict(),\n            \"median\": df[numeric_cols].median().to_dict(),\n            \"std_dev\": df[numeric_cols].std().to_dict()\n        }\n\n    return analysis_results\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nos, zipfile, json, pandas, matplotlib\n```python\nimport os\nimport zipfile\nimport json\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nDATA_DIR = \"data\"\n\ndef analyze_zip_json_data(num_of_file):\n    \"\"\"\n    Read the specified number of ZIP files containing JSON files from a directory, \n    convert the JSON data to a pandas DataFrame, calculate the mean of the numeric \n    columns, and plot a bar chart for categorical columns.\n\n    Parameters:\n    num_of_file (int): The number of files to read.\n\n    Returns:\n    dict: A dictionary with the mean for each file.\n\n    Requirements:\n    - os\n    - zipfile\n    - json\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> analyze_zip_json_data(5)\n    \"\"\"\n    zip_files = os.listdir(DATA_DIR)[:num_of_file]\n    analysis_results = {}\n\n    for file in zip_files:\n        with zipfile.ZipFile(os.path.join(DATA_DIR, file), 'r') as zip_ref:\n            json_files = [name for name in zip_ref.namelist() if name.endswith('.json')]\n\n            for json_file in json_files:\n                with zip_ref.open(json_file) as f:\n                    data = json.load(f)\n\n                df = pd.json_normalize(data)\n                numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n                categorical_cols = df.select_dtypes(include=[object]).columns.tolist()\n\n                for col in categorical_cols:\n                    df[col].value_counts().plot(kind='bar', title=f\"Bar chart for {col} in {json_file}\")\n                    plt.show()\n\n                analysis_results[json_file] = df[numeric_cols].mean().to_dict()\n\n    return analysis_results\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, shutil, subprocess, pandas\n```python\nimport os\nimport shutil\nimport subprocess\nimport pandas as pd\n\n# Constants\nDATA_DIR = \"data\"\nBACKUP_DIR = \"backup\"\n\ndef backup_and_process_data(num_of_file):\n    \"\"\"\n    Backup the specified number of files from a data directory to a backup directory,\n    then read each CSV file, calculate the sum of the numeric columns, and store the \n    results in a new CSV file.\n\n    Parameters:\n    num_of_file (int): The number of files to backup and process.\n\n    Returns:\n    None\n\n    Requirements:\n    - os\n    - shutil\n    - subprocess\n    - pandas\n\n    Example:\n    >>> backup_and_process_data(5)\n    \"\"\"\n    csv_files = os.listdir(DATA_DIR)[:num_of_file]\n\n    for file in csv_files:\n        shutil.copy(os.path.join(DATA_DIR, file), BACKUP_DIR)\n\n    for file in csv_files:\n        df = pd.read_csv(os.path.join(DATA_DIR, file))\n        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n        df[numeric_cols].sum().to_csv(os.path.join(DATA_DIR, \"sum_\" + file))\n\n    subprocess.run([\"ls\", \"-l\", DATA_DIR], check=True)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, csv, re, collections\n```python\nimport os\nimport csv\nimport re\nimport collections\n\n# Constants\nDATA_DIR = \"data\"\nWORD_PATTERN = re.compile(r'\\w+')\n\ndef count_words_in_files(num_of_file):\n    \"\"\"\n    Read the specified number of text files from a directory and count the number \n    of occurrences of each word in all files combined.\n\n    Parameters:\n    num_of_file (int): The number of files to read.\n\n    Returns:\n    dict: A dictionary with the word counts.\n\n    Requirements:\n    - os\n    - csv\n    - re\n    - collections\n\n    Example:\n    >>> count_words_in_files(5)\n    \"\"\"\n    text_files = os.listdir(DATA_DIR)[:num_of_file]\n    word_counts = collections.Counter()\n\n    for file in text_files:\n        with open(os.path.join(DATA_DIR, file), 'r') as f:\n            for line in f:\n                words = WORD_PATTERN.findall(line)\n                word_counts.update(words)\n\n    return dict(word_counts)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, glob, pandas, numpy, sklearn.preprocessing\n```python\nimport os\nimport glob\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\n\n# Constants\nDATA_DIR = \"data\"\n\ndef scale_numeric_data(num_of_file):\n    \"\"\"\n    Read the specified number of CSV files from a directory, scale the numeric \n    columns using StandardScaler, and save the scaled data as new CSV files.\n\n    Parameters:\n    num_of_file (int): The number of files to read.\n\n    Returns:\n    None\n\n    Requirements:\n    - os\n    - glob\n    - pandas\n    - numpy\n    - sklearn.preprocessing\n\n    Example:\n    >>> scale_numeric_data(5)\n    \"\"\"\n    csv_files = glob.glob(os.path.join(DATA_DIR, \"*.csv\"))[:num_of_file]\n    scaler = StandardScaler()\n\n    for file in csv_files:\n        df = pd.read_csv(file)\n        numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n        df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n        df.to_csv(os.path.join(DATA_DIR, \"scaled_\" + os.path.basename(file)), index=False)\n```\n"}
{"seed": "{\"task_id\": 24786, \"prompt\": \"def f_24786(string):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"urllib.parse.urlencode(string).encode('ascii')\", \"test_start\": \"\\nimport urllib\\n\\ndef check(candidate):\", \"test\": [\"\\n    s = {'mail':'admin@getgo.com', 'password':34}\\n    assert candidate(s) == b'mail=admin%40getgo.com&password=34'\\n\"], \"entry_point\": \"f_24786\", \"intent\": \"\\u6587\\u5b57\\u5217`string`\\u3092byte\\u578b\", \"library\": [\"urllib\"]}", "generation": "Scenario 1:\nurllib, base64, hashlib\n```python\nimport urllib.parse\nimport base64\nimport hashlib\n\n# Constants\nSECRET_KEY = 'secret-key-for-hashing'\n\ndef encode_and_hash_params(params):\n    \"\"\"\n    This function takes a dictionary of parameters, encodes it into a URL-encoded \n    string, converts the string into bytes, and then hashes the bytes using SHA256 \n    with a secret key.\n\n    Parameters:\n    params (dict): The dictionary of parameters.\n\n    Returns:\n    str: The hashed string.\n\n    Requirements:\n    - urllib.parse\n    - base64\n    - hashlib\n\n    Example:\n    >>> params = {'mail':'admin@getgo.com', 'password':34}\n    >>> print(encode_and_hash_params(params))\n    \"\"\"\n    url_encoded_params = urllib.parse.urlencode(params).encode('ascii')\n    hashed_params = hashlib.pbkdf2_hmac('sha256', url_encoded_params, SECRET_KEY.encode('utf-8'), 100000)\n    return base64.b64encode(hashed_params).decode('utf-8')\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nurllib, json, os\n```python\nimport urllib.parse\nimport json\nimport os\n\n# Constants\nCONFIG_FILE_PATH = 'config.json'\n\ndef save_url_parameters_to_config(params):\n    \"\"\"\n    This function takes a dictionary of parameters, encodes it into a URL-encoded \n    string, converts the string into bytes, and then saves the bytes in a config file.\n\n    Parameters:\n    params (dict): The dictionary of parameters.\n\n    Returns:\n    str: The path to the config file.\n\n    Requirements:\n    - urllib.parse\n    - json\n    - os\n\n    Example:\n    >>> params = {'mail':'admin@getgo.com', 'password':34}\n    >>> print(save_url_parameters_to_config(params))\n    \"\"\"\n    url_encoded_params = urllib.parse.urlencode(params).encode('ascii')\n    \n    with open(CONFIG_FILE_PATH, 'w') as config_file:\n        json.dump({ 'params': url_encoded_params.decode('utf-8') }, config_file)\n\n    return os.path.abspath(CONFIG_FILE_PATH)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nurllib, csv, sys\n```python\nimport urllib.parse\nimport csv\nimport sys\n\n# Constants\nCSV_FILE_PATH = 'params.csv'\n\ndef save_url_parameters_to_csv(params):\n    \"\"\"\n    This function takes a dictionary of parameters, encodes it into a URL-encoded \n    string, converts the string into bytes, and then saves the bytes in a CSV file.\n\n    Parameters:\n    params (dict): The dictionary of parameters.\n\n    Returns:\n    str: The path to the CSV file.\n\n    Requirements:\n    - urllib.parse\n    - csv\n    - sys\n\n    Example:\n    >>> params = {'mail':'admin@getgo.com', 'password':34}\n    >>> print(save_url_parameters_to_csv(params))\n    \"\"\"\n    url_encoded_params = urllib.parse.urlencode(params).encode('ascii')\n    \n    with open(CSV_FILE_PATH, 'w', newline='') as csv_file:\n        writer = csv.writer(csv_file)\n        writer.writerow(['Parameters'])\n        writer.writerow([url_encoded_params.decode('utf-8')])\n\n    return sys.path[0] + '/' + CSV_FILE_PATH\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nurllib, socket, ssl\n```python\nimport urllib.parse\nimport socket\nimport ssl\n\n# Constants\nHOST = 'www.google.com'\nPORT = 443\n\ndef send_url_parameters_over_ssl(params):\n    \"\"\"\n    This function takes a dictionary of parameters, encodes it into a URL-encoded \n    string, converts the string into bytes, and then sends the bytes to a server \n    over a SSL connection.\n\n    Parameters:\n    params (dict): The dictionary of parameters.\n\n    Returns:\n    bool: Whether the operation was successful.\n\n    Requirements:\n    - urllib.parse\n    - socket\n    - ssl\n\n    Example:\n    >>> params = {'mail':'admin@getgo.com', 'password':34}\n    >>> print(send_url_parameters_over_ssl(params))\n    \"\"\"\n    url_encoded_params = urllib.parse.urlencode(params).encode('ascii')\n    \n    context = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH)\n    conn = context.wrap_socket(socket.socket(socket.AF_INET), server_hostname=HOST)\n    \n    try:\n        conn.connect((HOST, PORT))\n        conn.sendall(url_encoded_params)\n        return True\n    except Exception as e:\n        print(str(e))\n        return False\n    finally:\n        conn.close()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nurllib, smtplib, email.mime\n```python\nimport urllib.parse\nimport smtplib\nfrom email.mime.text import MIMEText\n\n# Constants\nSMTP_SERVER = 'smtp.gmail.com'\nSMTP_PORT = 587\nSMTP_USERNAME = 'your-email@gmail.com'\nSMTP_PASSWORD = 'your-password'\n\ndef send_url_parameters_via_email(params):\n    \"\"\"\n    This function takes a dictionary of parameters, encodes it into a URL-encoded \n    string, converts the string into bytes, and then sends the bytes as an email \n    using SMTP.\n\n    Parameters:\n    params (dict): The dictionary of parameters.\n\n    Returns:\n    bool: Whether the operation was successful.\n\n    Requirements:\n    - urllib.parse\n    - smtplib\n    - email.mime.text\n\n    Example:\n    >>> params = {'mail':'admin@getgo.com', 'password':34}\n    >>> print(send_url_parameters_via_email(params))\n    \"\"\"\n    url_encoded_params = urllib.parse.urlencode(params).encode('ascii')\n    \n    msg = MIMEText(url_encoded_params.decode('utf-8'))\n    msg['Subject'] = 'URL Encoded Parameters'\n    msg['From'] = SMTP_USERNAME\n    msg['To'] = SMTP_USERNAME\n\n    try:\n        server = smtplib.SMTP(SMTP_SERVER, SMTP_PORT)\n        server.starttls()\n        server.login(SMTP_USERNAME, SMTP_PASSWORD)\n        server.send_message(msg)\n        server.quit()\n        return True\n    except Exception as e:\n        print(str(e))\n        return False\n```"}
{"seed": "{\"task_id\": 5822, \"prompt\": \"def f_5822(imgAry):\\n\\t\", \"suffix\": \"\\n\\treturn restoredImgAry\", \"canonical_solution\": \"pca = PCA()\\n\\tpca.fit(imgAry)\\n\\tpca_res = pca.transform(imgAry)\\n\\trestoredImgAry = pca.inverse_transform(pca_res)\", \"test_start\": \"\\nimport numpy as np \\nfrom sklearn.decomposition import PCA\\n\\ndef check(candidate):\", \"test\": [\"\\n    imgAry = np.array([[1, 2], [4, 3]])\\n    assert np.allclose(candidate(imgAry), np.array([[1,2],[4,3]], dtype=float))\\n\"], \"entry_point\": \"f_5822\", \"intent\": \"\\u4e3b\\u6210\\u5206\\u5206\\u6790 \", \"library\": [\"numpy\", \"sklearn\"]}", "generation": "Scenario 1: \nnumpy, sklearn, matplotlib, random\n```python\nimport numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nN_IMAGES = 10\nIMG_SIZE = 64\n\ndef generate_and_restore_images():\n    \"\"\"\n    Generate a set of random grayscale images, perform PCA on them, and then restore the images.\n    The original and restored images are plotted for comparison.\n    \n    Requirements:\n    - numpy\n    - sklearn.decomposition.PCA\n    - matplotlib.pyplot\n    - random\n    \n    Example:\n    >>> generate_and_restore_images()\n    \"\"\"\n    # Generate random images\n    images = np.random.randint(0, 256, (N_IMAGES, IMG_SIZE, IMG_SIZE))\n\n    # Flatten images for PCA\n    flattened_images = images.reshape(N_IMAGES, -1)\n\n    # Perform PCA and restore images\n    pca = PCA()\n    pca.fit(flattened_images)\n    pca_res = pca.transform(flattened_images)\n    restored_images = pca.inverse_transform(pca_res)\n\n    # Reshape restored images\n    restored_images = restored_images.reshape(N_IMAGES, IMG_SIZE, IMG_SIZE)\n\n    # Plot original and restored images\n    fig, axes = plt.subplots(2, N_IMAGES, figsize=(N_IMAGES*2, 4))\n    for i in range(N_IMAGES):\n        axes[0, i].imshow(images[i], cmap='gray')\n        axes[1, i].imshow(restored_images[i], cmap='gray')\n    plt.show()\n\n    return images, restored_images\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, sklearn, os, cv2\n```python\nimport numpy as np\nfrom sklearn.decomposition import PCA\nimport os\nimport cv2\n\n# Constants\nIMG_DIR = 'images/'\n\ndef pca_on_image_folder():\n    \"\"\"\n    Perform PCA on a folder of grayscale images, and then restore the images.\n    \n    Requirements:\n    - numpy\n    - sklearn.decomposition.PCA\n    - os\n    - cv2\n    \n    Example:\n    >>> original_images, restored_images = pca_on_image_folder()\n    \"\"\"\n    # Load images from folder\n    img_files = [f for f in os.listdir(IMG_DIR) if f.endswith('.jpg')]\n    images = np.array([cv2.imread(os.path.join(IMG_DIR, f), cv2.IMREAD_GRAYSCALE) for f in img_files])\n\n    # Flatten images for PCA\n    flattened_images = images.reshape(len(img_files), -1)\n\n    # Perform PCA and restore images\n    pca = PCA()\n    pca.fit(flattened_images)\n    pca_res = pca.transform(flattened_images)\n    restored_images = pca.inverse_transform(pca_res)\n\n    # Reshape restored images\n    restored_images = restored_images.reshape(len(img_files), images.shape[1], images.shape[2])\n\n    return images, restored_images\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, sklearn, scipy.ndimage, matplotlib\n```python\nimport numpy as np\nfrom sklearn.decomposition import PCA\nfrom scipy.ndimage import rotate\nimport matplotlib.pyplot as plt\n\n# Constants\nN_IMAGES = 10\nIMG_SIZE = 64\nROTATION_ANGLE = 45\n\ndef generate_rotate_and_restore_images():\n    \"\"\"\n    Generate a set of random grayscale images, rotate them, perform PCA on the rotated images, and then restore the images.\n    The original, rotated and restored images are plotted for comparison.\n    \n    Requirements:\n    - numpy\n    - sklearn.decomposition.PCA\n    - scipy.ndimage\n    - matplotlib.pyplot\n    \n    Example:\n    >>> generate_rotate_and_restore_images()\n    \"\"\"\n    # Generate random images\n    images = np.random.randint(0, 256, (N_IMAGES, IMG_SIZE, IMG_SIZE))\n\n    # Rotate images\n    rotated_images = np.array([rotate(img, ROTATION_ANGLE, reshape=False) for img in images])\n\n    # Flatten images for PCA\n    flattened_images = rotated_images.reshape(N_IMAGES, -1)\n\n    # Perform PCA and restore images\n    pca = PCA()\n    pca.fit(flattened_images)\n    pca_res = pca.transform(flattened_images)\n    restored_images = pca.inverse_transform(pca_res)\n\n    # Reshape restored images\n    restored_images = restored_images.reshape(N_IMAGES, IMG_SIZE, IMG_SIZE)\n\n    # Plot original, rotated and restored images\n    fig, axes = plt.subplots(3, N_IMAGES, figsize=(N_IMAGES*2, 6))\n    for i in range(N_IMAGES):\n        axes[0, i].imshow(images[i], cmap='gray')\n        axes[1, i].imshow(rotated_images[i], cmap='gray')\n        axes[2, i].imshow(restored_images[i], cmap='gray')\n    plt.show()\n\n    return images, rotated_images, restored_images\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, sklearn, scipy.ndimage, matplotlib\n```python\nimport numpy as np\nfrom sklearn.decomposition import PCA\nfrom scipy.ndimage import gaussian_filter\nimport matplotlib.pyplot as plt\n\n# Constants\nN_IMAGES = 10\nIMG_SIZE = 64\nSIGMA = 1\n\ndef generate_blur_and_restore_images():\n    \"\"\"\n    Generate a set of random grayscale images, blur them, perform PCA on the blurred images, and then restore the images.\n    The original, blurred and restored images are plotted for comparison.\n    \n    Requirements:\n    - numpy\n    - sklearn.decomposition.PCA\n    - scipy.ndimage\n    - matplotlib.pyplot\n    \n    Example:\n    >>> generate_blur_and_restore_images()\n    \"\"\"\n    # Generate random images\n    images = np.random.randint(0, 256, (N_IMAGES, IMG_SIZE, IMG_SIZE))\n\n    # Blur images\n    blurred_images = np.array([gaussian_filter(img, SIGMA) for img in images])\n\n    # Flatten images for PCA\n    flattened_images = blurred_images.reshape(N_IMAGES, -1)\n\n    # Perform PCA and restore images\n    pca = PCA()\n    pca.fit(flattened_images)\n    pca_res = pca.transform(flattened_images)\n    restored_images = pca.inverse_transform(pca_res)\n\n    # Reshape restored images\n    restored_images = restored_images.reshape(N_IMAGES, IMG_SIZE, IMG_SIZE)\n\n    # Plot original, blurred and restored images\n    fig, axes = plt.subplots(3, N_IMAGES, figsize=(N_IMAGES*2, 6))\n    for i in range(N_IMAGES):\n        axes[0, i].imshow(images[i], cmap='gray')\n        axes[1, i].imshow(blurred_images[i], cmap='gray')\n        axes[2, i].imshow(restored_images[i], cmap='gray')\n    plt.show()\n\n    return images, blurred_images, restored_images\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, sklearn, matplotlib, random\n```python\nimport numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nN_IMAGES = 100\nIMG_SIZE = 64\nN_COMPONENTS = 50\n\ndef generate_images_and_restore_with_pca():\n    \"\"\"\n    Generate a set of random grayscale images, perform PCA with a specified number of components on them, \n    and then restore the images. The original and restored images are plotted for comparison.\n    \n    Requirements:\n    - numpy\n    - sklearn.decomposition.PCA\n    - matplotlib.pyplot\n    - random\n    \n    Example:\n    >>> generate_images_and_restore_with_pca()\n    \"\"\"\n    # Generate random images\n    images = np.random.randint(0, 256, (N_IMAGES, IMG_SIZE, IMG_SIZE))\n\n    # Flatten images for PCA\n    flattened_images = images.reshape(N_IMAGES, -1)\n\n    # Perform PCA with N_COMPONENTS and restore images\n    pca = PCA(n_components=N_COMPONENTS)\n    pca.fit(flattened_images)\n    pca_res = pca.transform(flattened_images)\n    restored_images = pca.inverse_transform(pca_res)\n\n    # Reshape restored images\n    restored_images = restored_images.reshape(N_IMAGES, IMG_SIZE, IMG_SIZE)\n\n    # Plot original and restored images\n    fig, axes = plt.subplots(2, N_IMAGES, figsize=(N_IMAGES*2, 4))\n    for i in range(N_IMAGES):\n        axes[0, i].imshow(images[i], cmap='gray')\n        axes[1, i].imshow(restored_images[i], cmap='gray')\n    plt.show()\n\n    return images, restored_images\n```\n"}
{"seed": "{\"task_id\": 42516, \"prompt\": \"def f_42516(fnameF):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"plt.savefig(fnameF, dpi=200, bbox_inches=\\\"tight\\\", pad_inches=0.1)\", \"test_start\": \"\\nimport os\\nimport matplotlib.pyplot as plt\\n\\ndef check(candidate):\", \"test\": [\"\\n    candidate('v.jpg')\\n    assert os.path.exists('v.jpg')\\n\"], \"entry_point\": \"f_42516\", \"intent\": \"\\u30b0\\u30e9\\u30d5\\u30b5\\u30a4\\u30ba\\u3092\\u8abf\\u6574\\u3057\\u3066\\u4fdd\\u5b58\\u3059\\u308b\", \"library\": [\"matplotlib\", \"os\"]}", "generation": "Scenario 1:\nos, matplotlib, numpy, pandas\n```python\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# Constants\nDATA_DIR = 'data'\nGRAPH_DIR = 'graphs'\n\ndef generate_and_save_hist(data_file, graph_file, bins=10):\n    \"\"\"\n    Generate a histogram from the data in a CSV file and save the graph to a file.\n\n    Parameters:\n    data_file (str): Name of the CSV file containing the data.\n    graph_file (str): Name of the file where the graph will be saved.\n    bins (int, optional): Number of bins for the histogram. Defaults to 10.\n\n    Returns:\n    str: Path of the saved graph file.\n\n    Requirements:\n    - os\n    - matplotlib.pyplot\n    - numpy\n    - pandas\n\n    Example:\n    >>> generate_and_save_hist('data.csv', 'hist_graph.png')\n    \"\"\"\n    data_file_path = os.path.join(DATA_DIR, data_file)\n    graph_file_path = os.path.join(GRAPH_DIR, graph_file)\n\n    # Load data\n    data = pd.read_csv(data_file_path)\n\n    # Generate histogram\n    plt.hist(data, bins=bins)\n\n    # Save graph\n    plt.savefig(graph_file_path, dpi=200, bbox_inches=\"tight\", pad_inches=0.1)\n    \n    return graph_file_path\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nmatplotlib, numpy, scipy, os\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.interpolate import interp1d\nimport os\n\n# Constants\nPATH = 'graphs'\n\ndef generate_interpolation_graph(data_points, graph_file):\n    \"\"\"\n    Generate an interpolation graph from the given data points and save the graph to a file.\n\n    Parameters:\n    data_points (list of tuple): List of data points.\n    graph_file (str): Name of the file where the graph will be saved.\n\n    Returns:\n    str: Path of the saved graph file.\n\n    Requirements:\n    - matplotlib.pyplot\n    - numpy\n    - scipy.interpolate\n    - os\n\n    Example:\n    >>> generate_interpolation_graph([(1, 2), (2, 3), (3, 2)], 'interp_graph.png')\n    \"\"\"\n    x, y = zip(*data_points)\n    f = interp1d(x, y, kind='cubic')\n\n    xnew = np.linspace(min(x), max(x), num=1000, endpoint=True)\n    \n    plt.plot(x, y, 'o', xnew, f(xnew), '-')\n    plt.legend(['data', 'cubic'], loc='best')\n    \n    graph_file_path = os.path.join(PATH, graph_file)\n    plt.savefig(graph_file_path, dpi=200, bbox_inches=\"tight\", pad_inches=0.1)\n    \n    return graph_file_path\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, matplotlib, numpy, sklearn\n```python\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn import datasets\nfrom sklearn.cluster import KMeans\n\n# Constants\nDIR_PATH = 'graphs'\n\ndef generate_and_save_clusters_graph(n_clusters, graph_file):\n    \"\"\"\n    Generate a clusters graph using KMeans from sklearn and save the graph to a file.\n\n    Parameters:\n    n_clusters (int): The number of clusters.\n    graph_file (str): The name of the file where the graph will be saved.\n\n    Returns:\n    str: Path of the saved graph file.\n\n    Requirements:\n    - os\n    - matplotlib.pyplot\n    - numpy\n    - sklearn.datasets\n    - sklearn.cluster\n\n    Example:\n    >>> generate_and_save_clusters_graph(3, 'clusters_graph.png')\n    \"\"\"\n    # Load iris dataset\n    iris = datasets.load_iris()\n    X = iris.data\n\n    # Fit KMeans\n    kmeans = KMeans(n_clusters=n_clusters)\n    kmeans.fit(X)\n\n    # Plot clusters\n    plt.scatter(X[:,0], X[:,1], c=kmeans.labels_, cmap='viridis')\n    plt.scatter(kmeans.cluster_centers_[:,0] ,kmeans.cluster_centers_[:,1], color='black')\n\n    graph_file_path = os.path.join(DIR_PATH, graph_file)\n    plt.savefig(graph_file_path, dpi=200, bbox_inches=\"tight\", pad_inches=0.1)\n\n    return graph_file_path\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nmatplotlib, numpy, scipy, os\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy import stats\nimport os\n\n# Constants\nDIR_PATH = 'graphs'\n\ndef generate_and_save_normal_distribution_graph(mu, sigma, graph_file):\n    \"\"\"\n    Generate a normal distribution graph with given mean and standard deviation and \n    save the graph to a file.\n\n    Parameters:\n    mu (float): Mean of the normal distribution.\n    sigma (float): Standard deviation of the normal distribution.\n    graph_file (str): The name of the file where the graph will be saved.\n\n    Returns:\n    str: Path of the saved graph file.\n\n    Requirements:\n    - matplotlib.pyplot\n    - numpy\n    - scipy.stats\n    - os\n\n    Example:\n    >>> generate_and_save_normal_distribution_graph(0, 0.1, 'normal_distribution_graph.png')\n    \"\"\"\n    x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n    plt.plot(x, stats.norm.pdf(x, mu, sigma))\n\n    graph_file_path = os.path.join(DIR_PATH, graph_file)\n    plt.savefig(graph_file_path, dpi=200, bbox_inches=\"tight\", pad_inches=0.1)\n\n    return graph_file_path\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nos, matplotlib, numpy, pandas\n```python\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# Constants\nDATA_DIR = 'data'\nGRAPH_DIR = 'graphs'\n\ndef generate_and_save_boxplot(data_file, graph_file):\n    \"\"\"\n    Generate a box plot from the data in a CSV file and save the graph to a file.\n\n    Parameters:\n    data_file (str): Name of the CSV file containing the data.\n    graph_file (str): Name of the file where the graph will be saved.\n\n    Returns:\n    str: Path of the saved graph file.\n\n    Requirements:\n    - os\n    - matplotlib.pyplot\n    - numpy\n    - pandas\n\n    Example:\n    >>> generate_and_save_boxplot('data.csv', 'boxplot_graph.png')\n    \"\"\"\n    data_file_path = os.path.join(DATA_DIR, data_file)\n    graph_file_path = os.path.join(GRAPH_DIR, graph_file)\n\n    # Load data\n    data = pd.read_csv(data_file_path)\n\n    # Generate box plot\n    plt.boxplot(data)\n\n    # Save graph\n    plt.savefig(graph_file_path, dpi=200, bbox_inches=\"tight\", pad_inches=0.1)\n    \n    return graph_file_path\n```"}
{"seed": "{\"task_id\": 18685, \"prompt\": \"def f_18685(list):\\n\\t\", \"suffix\": \"\\n\\treturn list\", \"canonical_solution\": \"list = [x for x in list if x]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(['afrg ', 'fdbf', 13254, 54765.6]) == ['afrg ', 'fdbf', 13254, 54765.6]\\n\", \"\\n    assert candidate(['', None, 0]) == []\\n\"], \"entry_point\": \"f_18685\", \"intent\": \"\\u4e8c\\u6b21\\u5143\\u30ea\\u30b9\\u30c8`list`\\u304b\\u3089\\u7a7a\\u767d\\u306e\\u8981\\u7d20\\u3092\\u524a\\u9664\\u3059\\u308b\", \"library\": []}", "generation": "Scenario 1: \npandas, random, os, re\n```python\nimport pandas as pd\nfrom random import choice\nimport os\nimport re\n\n# Constants\nFILE_FORMATS = ['.txt', '.docx', '.pdf', '.csv', '.xlsx']\n\ndef generate_random_files(dir_path, num_files):\n    \"\"\"\n    Generate random files of different formats in a directory.\n    Then, remove the files that do not match a certain regex pattern.\n    \n    Parameters:\n    dir_path (str): The directory path where files should be generated.\n    num_files (int): The number of files to generate.\n    \n    Returns:\n    list: The list of remaining files in the directory.\n    \n    Requirements:\n    - pandas\n    - random.choice\n    - os\n    - re\n    \n    Example:\n    >>> generate_random_files('./test_dir', 10)\n    \"\"\"\n    for i in range(num_files):\n        file_name = f'file_{i}{choice(FILE_FORMATS)}'\n        file_path = os.path.join(dir_path, file_name)\n        with open(file_path, 'w') as f:\n            pass\n\n    files = os.listdir(dir_path)\n\n    for file in files:\n        if not re.match(r'file_[0-9]+\\.txt', file):\n            os.remove(os.path.join(dir_path, file))\n\n    remaining_files = os.listdir(dir_path)\n\n    return remaining_files\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 2: \nnumpy, matplotlib, math\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom math import sin, cos\n\n# Constants\nX = np.linspace(-np.pi, np.pi, 256, endpoint=True)\nCOS, SIN = cos(X), sin(X)\n\ndef plot_sine_cosine_wave(amplitude, frequency):\n    \"\"\"\n    Plot sine and cosine wave of given amplitude and frequency.\n    \n    Parameters:\n    amplitude (float): The amplitude of the wave.\n    frequency (float): The frequency of the wave.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - math.sin\n    - math.cos\n    \n    Example:\n    >>> plot_sine_cosine_wave(1, 1)\n    \"\"\"\n    Y_sin = amplitude * np.sin(2 * np.pi * frequency * X)\n    Y_cos = amplitude * np.cos(2 * np.pi * frequency * X)\n\n    plt.figure(figsize=(8, 6))\n    plt.plot(X, Y_sin, color='blue', linewidth=2.5, linestyle='-', label='sine')\n    plt.plot(X, Y_cos, color='red', linewidth=2.5, linestyle='-', label='cosine')\n    plt.legend(loc='upper left')\n    plt.show()\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 3: \npandas, numpy, matplotlib, os\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\n# Constants\nDATA_PATH = './data_dir'\n\ndef plot_histogram_from_csv_files(dir_path):\n    \"\"\"\n    Plot histogram for numeric columns for all CSV files in a directory.\n    \n    Parameters:\n    dir_path (str): The directory path where CSV files are located.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - os\n    \n    Example:\n    >>> plot_histogram_from_csv_files('./data_dir')\n    \"\"\"\n    file_list = os.listdir(dir_path)\n\n    for file in file_list:\n        if file.endswith('.csv'):\n            df = pd.read_csv(os.path.join(dir_path, file))\n            num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n\n            for col in num_cols:\n                plt.figure(figsize=(8, 6))\n                plt.hist(df[col], bins=30, edgecolor='k')\n                plt.title(f\"Histogram of {col} in {file}\")\n                plt.show()\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 4: \npandas, random, math, datetime\n```python\nimport pandas as pd\nfrom random import randint\nfrom math import ceil\nfrom datetime import datetime, timedelta\n\n# Constants\nPRODUCTS = ['product_A', 'product_B', 'product_C', 'product_D', 'product_E']\n\ndef generate_sales_data(start_date, end_date):\n    \"\"\"\n    Generate random sales data for a range of dates for different products.\n    Then, calculate the total sales and average sales per day for each product.\n    \n    Parameters:\n    start_date (datetime): The start date for the sales data.\n    end_date (datetime): The end date for the sales data.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with total and average sales for each product.\n    \n    Requirements:\n    - pandas\n    - random.randint\n    - math.ceil\n    - datetime.timedelta\n    \n    Example:\n    >>> start_date = datetime(2022, 1, 1)\n    >>> end_date = datetime(2022, 1, 31)\n    >>> sales_data = generate_sales_data(start_date, end_date)\n    >>> print(sales_data)\n    \"\"\"\n    date_range = pd.date_range(start_date, end_date)\n    data = []\n\n    for date in date_range:\n        for product in PRODUCTS:\n            sales = randint(50, 1000)\n            data.append([date, product, sales])\n\n    df = pd.DataFrame(data, columns=['Date', 'Product', 'Sales'])\n\n    total_sales = df.groupby('Product')['Sales'].sum().reset_index()\n    total_sales.columns = ['Product', 'Total Sales']\n\n    num_days = (end_date - start_date).days + 1\n    total_sales['Average Sales per Day'] = total_sales['Total Sales'].apply(lambda x: ceil(x / num_days))\n\n    return total_sales\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnumpy, pandas, random, os\n```python\nimport numpy as np\nimport pandas as pd\nfrom random import randint\nimport os\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef create_and_save_random_df(dir_path, num_rows, num_files):\n    \"\"\"\n    Generate and save random pandas DataFrames to CSV files in a directory.\n    \n    Parameters:\n    dir_path (str): The directory path where files should be saved.\n    num_rows (int): The number of rows for each DataFrame.\n    num_files (int): The number of files to generate.\n    \n    Returns:\n    list: The list of generated file paths.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - random.randint\n    - os\n    \n    Example:\n    >>> create_and_save_random_df('./data_dir', 10, 5)\n    \"\"\"\n    file_paths = []\n\n    for i in range(num_files):\n        df = pd.DataFrame(np.random.randint(0, 100, size=(num_rows, len(COLUMNS))), columns=COLUMNS)\n        file_path = os.path.join(dir_path, f'data_{i}.csv')\n        df.to_csv(file_path, index=False)\n        file_paths.append(file_path)\n\n    return file_paths\n```"}
{"seed": "{\"task_id\": 18685, \"prompt\": \"def f_18685(list):\\n\\t\", \"suffix\": \"\\n\\treturn list\", \"canonical_solution\": \"for i in range(len(list) - 1, -1, -1):\\n\\t  if not list[i]:\\n\\t    del list[i]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([4,3,2,0,1,0]) == [4,3,2,1]\\n\", \"\\n    assert candidate([4,3,2,[],1,0]) == [4,3,2,1]\\n\"], \"entry_point\": \"f_18685\", \"intent\": \"\\u4e8c\\u6b21\\u5143\\u30ea\\u30b9\\u30c8`list`\\u304b\\u3089\\u7a7a\\u767d\\u306e\\u8981\\u7d20\\u3092\\u524a\\u9664\\u3059\\u308b\", \"library\": []}", "generation": "Scenario 1: \npandas, numpy, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCOUNTRIES = ['USA', 'UK', 'Germany', 'Japan', 'Australia']\nPRODUCTS = ['Product A', 'Product B', 'Product C', 'Product D', 'Product E']\n\ndef remove_zero_sales_and_visualize(sales_data):\n    \"\"\"\n    From the given sales data, remove the records with zero sales (empty entries) and visualize the remaining data.\n\n    Parameters:\n    sales_data (list): The sales data as a list of dictionaries. Each dictionary has three keys - 'Country', 'Product' and 'Sales'.\n\n    Returns:\n    DataFrame: A pandas DataFrame with sales records after removing zero sales.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> sales_data = [{'Country': 'USA', 'Product': 'Product A', 'Sales': 100}, \n    ...               {'Country': 'UK', 'Product': 'Product B', 'Sales': 0}, \n    ...               {'Country': 'Germany', 'Product': 'Product C', 'Sales': 50}, \n    ...               {'Country': 'Japan', 'Product': 'Product D', 'Sales': 200}, \n    ...               {'Country': 'Australia', 'Product': 'Product E', 'Sales': 0}]\n    >>> df = remove_zero_sales_and_visualize(sales_data)\n    >>> print(df)\n    >>> df.plot(x='Country', y='Sales', kind='bar')\n    \"\"\"\n    df = pd.DataFrame(sales_data)\n    \n    # Remove zero sales records\n    df = df[df['Sales'] != 0]\n    \n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, math, matplotlib\n```python\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\n\n# Constants\nX_RANGE = np.linspace(-10, 10, 100)\n\ndef remove_zeros_and_plot_cosine(values):\n    \"\"\"\n    From the given list, remove zero values, apply cosine function to the remaining values and plot the result.\n    \n    Parameters:\n    values (list): A list of numeric values.\n\n    Returns:\n    list: A list with cosine values for the non-zero numbers from the input list.\n\n    Requirements:\n    - numpy\n    - math\n    - matplotlib.pyplot\n\n    Example:\n    >>> values = [0, 1, 2, 3, 0, 4, 5, 0, 6, 7, 8, 9, 0]\n    >>> cosine_values = remove_zeros_and_plot_cosine(values)\n    >>> print(cosine_values)\n    >>> plt.plot(values, cosine_values)\n    \"\"\"\n    # Remove zero values\n    non_zero_values = [x for x in values if x != 0]\n\n    # Calculate cosine values\n    cosine_values = [math.cos(x) for x in non_zero_values]\n    \n    return cosine_values\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, seaborn, matplotlib\n```python\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nCITIES = ['New York', 'London', 'Tokyo', 'Sydney', 'Berlin']\nTEMPERATURES = [-10, -5, 0, 5, 10, 15, 20, 25, 30, 35, 40]\n\ndef remove_zeros_and_visualize_temperature(data):\n    \"\"\"\n    From the given temperature data, remove the records with zero temperature and visualize the remaining data.\n\n    Parameters:\n    data (list): The temperature data as a list of dictionaries. Each dictionary has two keys - 'City' and 'Temperature'.\n\n    Returns:\n    DataFrame: A pandas DataFrame with temperature records after removing zero temperatures.\n\n    Requirements:\n    - pandas\n    - seaborn\n    - matplotlib.pyplot\n\n    Example:\n    >>> data = [{'City': 'New York', 'Temperature': 0}, \n    ...         {'City': 'London', 'Temperature': 10}, \n    ...         {'City': 'Tokyo', 'Temperature': 20}, \n    ...         {'City': 'Sydney', 'Temperature': 30}, \n    ...         {'City': 'Berlin', 'Temperature': 0}]\n    >>> df = remove_zeros_and_visualize_temperature(data)\n    >>> print(df)\n    >>> sns.lineplot(x='City', y='Temperature', data=df)\n    \"\"\"\n    df = pd.DataFrame(data)\n    \n    # Remove zero temperature records\n    df = df[df['Temperature'] != 0]\n    \n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nos, shutil, glob\n```python\nimport os\nimport shutil\nimport glob\n\ndef remove_empty_files_and_copy_rest(src_dir, dest_dir):\n    \"\"\"\n    From the given source directory, remove empty files and copy the rest of the files to the destination directory.\n\n    Parameters:\n    src_dir (str): The source directory path.\n    dest_dir (str): The destination directory path.\n\n    Returns:\n    list: A list of file paths that were copied to the destination directory.\n\n    Requirements:\n    - os\n    - shutil\n    - glob\n\n    Example:\n    >>> remove_empty_files_and_copy_rest('/path/to/source', '/path/to/destination')\n    \"\"\"\n    files = glob.glob(os.path.join(src_dir, '*'))\n    copied_files = []\n    \n    for file in files:\n        if os.path.getsize(file) > 0:\n            shutil.copy2(file, dest_dir)\n            copied_files.append(os.path.join(dest_dir, os.path.basename(file)))\n        else:\n            os.remove(file)\n    \n    return copied_files\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, scipy, matplotlib\n```python\nimport numpy as np\nfrom scipy import signal\nimport matplotlib.pyplot as plt\n\n# Constants\nSAMPLE_RATE = 1000\nFREQUENCIES = [10, 20, 30, 40, 50]\n\ndef remove_zeros_and_compute_fft(signal_data):\n    \"\"\"\n    From the given signal data, remove zero values, compute the Fast Fourier Transform (FFT) and plot the result.\n    \n    Parameters:\n    signal_data (list): A list of signal amplitude values.\n\n    Returns:\n    list, list: Two lists - frequencies and their corresponding FFT magnitudes.\n\n    Requirements:\n    - numpy\n    - scipy\n    - matplotlib.pyplot\n\n    Example:\n    >>> signal_data = [0, 1, 2, 3, 0, 4, 5, 0, 6, 7, 8, 9, 0]\n    >>> freqs, fft = remove_zeros_and_compute_fft(signal_data)\n    >>> print(freqs, fft)\n    >>> plt.plot(freqs, fft)\n    \"\"\"\n    # Remove zero values\n    non_zero_values = [x for x in signal_data if x != 0]\n    \n    # Compute FFT\n    freqs = np.fft.fftfreq(len(non_zero_values), 1/SAMPLE_RATE)\n    fft = np.fft.fft(non_zero_values)\n    \n    return freqs, np.abs(fft)\n```"}
{"seed": "{\"task_id\": 34692, \"prompt\": \"def f_34692(json_string):\\n\\t\", \"suffix\": \"\\n\\treturn json_obj\", \"canonical_solution\": \"json_obj = json.loads(json_string)\", \"test_start\": \"\\nimport json\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('{\\\"a\\\": 5}') == {'a': 5}\\n\"], \"entry_point\": \"f_34692\", \"intent\": \"JSON\\u6587\\u5b57\\u5217`json_string`\\u3092\\u30aa\\u30d6\\u30b8\\u30a7\\u30af\\u30c8`json_obj`\\u306b\\u8aad\\u307f\\u8fbc\\u3080\\uff08json)\", \"library\": [\"json\"]}", "generation": "Scenario 1: \njson, pandas, matplotlib, numpy\n```python\nimport json\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef analyze_json_data(json_string):\n    \"\"\"\n    Convert a JSON string to pandas DataFrame, then perform basic statistical analysis and plot a histogram of the data.\n\n    Parameters:\n    json_string (str): The JSON string to be converted.\n\n    Returns:\n    dict: A dictionary of basic statistical data.\n\n    Requirements:\n    - json\n    - pandas\n    - matplotlib.pyplot\n    - numpy\n\n    Example:\n    >>> json_string = '[{\"value\": 1}, {\"value\": 2}, {\"value\": 3}, {\"value\": 4}, {\"value\": 5}]'\n    >>> analyze_json_data(json_string)\n    \"\"\"\n    json_obj = json.loads(json_string)\n    df = pd.DataFrame(json_obj)\n    \n    result = df.describe().to_dict()\n\n    df.plot(kind='hist', rwidth=0.8)\n    plt.show()\n\n    return result\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\njson, os, shutil, zipfile\n```python\nimport json\nimport os\nimport shutil\nimport zipfile\n\ndef backup_json_data(json_string, backup_dir):\n    \"\"\"\n    Convert a JSON string to a JSON file, then backup the file to a specified directory.\n\n    Parameters:\n    json_string (str): The JSON string to be converted.\n    backup_dir (str): The directory to backup the file.\n\n    Returns:\n    str: The path of the backup file.\n\n    Requirements:\n    - json\n    - os\n    - shutil\n    - zipfile\n\n    Example:\n    >>> json_string = '{\"name\": \"John\", \"age\": 30, \"city\": \"New York\"}'\n    >>> backup_dir = '/path/to/backup'\n    >>> backup_json_data(json_string, backup_dir)\n    \"\"\"\n    json_obj = json.loads(json_string)\n    \n    if not os.path.exists(backup_dir):\n        os.makedirs(backup_dir)\n\n    with open('data.json', 'w') as outfile:\n        json.dump(json_obj, outfile)\n\n    shutil.copy('data.json', backup_dir)\n\n    with zipfile.ZipFile(os.path.join(backup_dir, 'backup.zip'), 'w') as zipf:\n        zipf.write(os.path.join(backup_dir, 'data.json'))\n        os.remove(os.path.join(backup_dir, 'data.json'))\n\n    return os.path.join(backup_dir, 'backup.zip')\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\njson, requests, pandas, matplotlib\n```python\nimport json\nimport requests\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef fetch_and_analyze_data(api_url, params):\n    \"\"\"\n    Fetch data from an API, convert the JSON response to a pandas DataFrame, perform basic statistical analysis, \n    and plot a histogram of the data.\n\n    Parameters:\n    api_url (str): The URL of the API to fetch data from.\n    params (dict): The parameters to be sent to the API.\n\n    Returns:\n    dict: A dictionary of basic statistical data.\n\n    Requirements:\n    - json\n    - requests\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> api_url = 'https://api.example.com/data'\n    >>> params = {'param1': 'value1', 'param2': 'value2'}\n    >>> fetch_and_analyze_data(api_url, params)\n    \"\"\"\n    response = requests.get(api_url, params=params)\n    data = json.loads(response.text)\n    \n    df = pd.DataFrame(data)\n    \n    result = df.describe().to_dict()\n\n    df.plot(kind='hist', rwidth=0.8)\n    plt.show()\n\n    return result\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\njson, os, csv, pandas, numpy\n```python\nimport json\nimport os\nimport csv\nimport pandas as pd\nimport numpy as np\n\ndef convert_json_to_csv(json_string, csv_file):\n    \"\"\"\n    Convert a JSON string to a CSV file, then read the file into a pandas DataFrame and calculate the mean of each column.\n\n    Parameters:\n    json_string (str): The JSON string to be converted.\n    csv_file (str): The path of the CSV file to be created.\n\n    Returns:\n    dict: A dictionary of mean values for each column.\n\n    Requirements:\n    - json\n    - os\n    - csv\n    - pandas\n    - numpy\n\n    Example:\n    >>> json_string = '[{\"value\": 1}, {\"value\": 2}, {\"value\": 3}, {\"value\": 4}, {\"value\": 5}]'\n    >>> csv_file = '/path/to/file.csv'\n    >>> convert_json_to_csv(json_string, csv_file)\n    \"\"\"\n    json_obj = json.loads(json_string)\n\n    with open(csv_file, 'w', newline='') as outfile:\n        writer = csv.DictWriter(outfile, fieldnames=json_obj[0].keys())\n        writer.writeheader()\n        writer.writerows(json_obj)\n\n    df = pd.read_csv(csv_file)\n\n    return df.mean().to_dict()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\njson, pandas, seaborn, matplotlib\n```python\nimport json\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef visualize_json_data(json_string):\n    \"\"\"\n    Convert a JSON string to a pandas DataFrame, then visualize the data using seaborn.\n\n    Parameters:\n    json_string (str): The JSON string to be converted.\n\n    Returns:\n    None.\n\n    Requirements:\n    - json\n    - pandas\n    - seaborn\n    - matplotlib.pyplot\n\n    Example:\n    >>> json_string = '[{\"value\": 1}, {\"value\": 2}, {\"value\": 3}, {\"value\": 4}, {\"value\": 5}]'\n    >>> visualize_json_data(json_string)\n    \"\"\"\n    json_obj = json.loads(json_string)\n    df = pd.DataFrame(json_obj)\n    \n    sns.pairplot(df)\n    plt.show()\n\n    return None\n```\n"}
{"seed": "{\"task_id\": 35864, \"prompt\": \"def f_35864(img):\\n\\t\", \"suffix\": \"\\n\\treturn gray_img\", \"canonical_solution\": \"gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\\n\\tplt.imshow(gray_img)\\n\\tplt.gray()\\n\\tplt.show()\", \"test_start\": \"\\nimport os\\nimport cv2\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom PIL import Image\\n\\ndef check(candidate):\", \"test\": [\"\\n    im = Image.new('RGBA', (200, 200), (255, 255, 255, 255))\\n    im.save('v.png')\\n    img = cv2.imread('v.png')\\n    gray_img = candidate(img)\\n    cv2.imwrite('v_gray.png', gray_img)\\n    assert os.path.exists('v_gray.png')\\n\"], \"entry_point\": \"f_35864\", \"intent\": \"\\u30ab\\u30e9\\u30fc\\u753b\\u50cf`img`\\u3092\\u30b0\\u30ec\\u30fc\\u30b9\\u30b1\\u30fc\\u30eb\\u3067\\u8868\\u793a\\u3059\\u308b\", \"library\": [\"PIL\", \"cv2\", \"matplotlib\", \"numpy\", \"os\"]}", "generation": "Scenario 1: \nPIL, cv2, matplotlib, numpy, os\n```python\nfrom PIL import Image, ImageFilter\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# Constants\nBLUR_RADIUS = 5\n\ndef blur_and_display_image(img_path):\n    \"\"\"\n    Open an image from a given path, apply a blur filter, convert it to greyscale, \n    then display both the original and the processed images side by side.\n\n    Parameters:\n    img_path (str): The path of the image file.\n\n    Returns:\n    None\n\n    Requirements:\n    - PIL\n    - cv2\n    - matplotlib\n    - numpy\n    - os\n\n    Example:\n    >>> blur_and_display_image('sample.png')\n    \"\"\"\n    assert os.path.exists(img_path), f\"No file found at {img_path}\"\n    \n    img = Image.open(img_path)\n    img = img.convert(\"RGB\")\n    \n    blurred_img = img.filter(ImageFilter.GaussianBlur(BLUR_RADIUS))\n    grey_img = cv2.cvtColor(np.array(blurred_img), cv2.COLOR_RGB2GRAY)\n\n    # Display original and processed images side by side\n    plt.figure(figsize=(10, 5))\n    plt.subplot(1, 2, 1)\n    plt.imshow(img)\n    plt.title('Original Image')\n    plt.subplot(1, 2, 2)\n    plt.imshow(grey_img, cmap='gray')\n    plt.title('Processed Image')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nPIL, cv2, matplotlib, numpy, os\n```python\nfrom PIL import Image\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\ndef find_and_draw_contours(img_path):\n    \"\"\"\n    Open an image, convert it to grayscale, find contours using the cv2 library, \n    and draw these contours on the original image.\n\n    Parameters:\n    img_path (str): The path of the image file.\n\n    Returns:\n    None\n\n    Requirements:\n    - PIL\n    - cv2\n    - matplotlib\n    - numpy\n    - os\n\n    Example:\n    >>> find_and_draw_contours('sample.png')\n    \"\"\"\n    assert os.path.exists(img_path), f\"No file found at {img_path}\"\n    \n    img = cv2.imread(img_path)\n    gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    \n    # Find contours\n    contours, _ = cv2.findContours(gray_img, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    \n    # Draw contours on the original image\n    cv2.drawContours(img, contours, -1, (0,255,0), 3)\n    \n    # Display the image\n    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nPIL, cv2, matplotlib, numpy, os\n```python\nfrom PIL import Image\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\ndef crop_and_save_image(img_path, output_path, top_left, bottom_right):\n    \"\"\"\n    Open an image, crop a region specified by the top left and bottom right coordinates, \n    then save the cropped image to a specified path.\n\n    Parameters:\n    img_path (str): The path of the image file.\n    output_path (str): The path to save the cropped image.\n    top_left (tuple): The coordinates (x, y) of the top left corner of the crop region.\n    bottom_right (tuple): The coordinates (x, y) of the bottom right corner of the crop region.\n\n    Returns:\n    None\n\n    Requirements:\n    - PIL\n    - cv2\n    - matplotlib\n    - numpy\n    - os\n\n    Example:\n    >>> crop_and_save_image('sample.png', 'cropped.png', (50, 50), (200, 200))\n    \"\"\"\n    assert os.path.exists(img_path), f\"No file found at {img_path}\"\n    \n    img = Image.open(img_path)\n    cropped_img = img.crop((*top_left, *bottom_right))\n    \n    cropped_img.save(output_path)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nPIL, cv2, matplotlib, numpy, os\n```python\nfrom PIL import Image\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\ndef rotate_and_display_image(img_path, angle):\n    \"\"\"\n    Open an image, rotate it by a specified angle, then display both the original and \n    the rotated images side by side.\n\n    Parameters:\n    img_path (str): The path of the image file.\n    angle (float): The angle to rotate the image (in degrees).\n\n    Returns:\n    None\n\n    Requirements:\n    - PIL\n    - cv2\n    - matplotlib\n    - numpy\n    - os\n\n    Example:\n    >>> rotate_and_display_image('sample.png', 45)\n    \"\"\"\n    assert os.path.exists(img_path), f\"No file found at {img_path}\"\n    \n    img = Image.open(img_path)\n    rotated_img = img.rotate(angle)\n    \n    # Display original and rotated images side by side\n    plt.figure(figsize=(10, 5))\n    plt.subplot(1, 2, 1)\n    plt.imshow(img)\n    plt.title('Original Image')\n    plt.subplot(1, 2, 2)\n    plt.imshow(rotated_img)\n    plt.title('Rotated Image')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nPIL, cv2, matplotlib, numpy, os\n```python\nfrom PIL import Image\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\ndef resize_and_save_image(img_path, output_path, size):\n    \"\"\"\n    Open an image, resize it to a specified size, then save the resized image to a specified path.\n\n    Parameters:\n    img_path (str): The path of the image file.\n    output_path (str): The path to save the resized image.\n    size (tuple): The new size of the image (width, height).\n\n    Returns:\n    None\n\n    Requirements:\n    - PIL\n    - cv2\n    - matplotlib\n    - numpy\n    - os\n\n    Example:\n    >>> resize_and_save_image('sample.png', 'resized.png', (100, 100))\n    \"\"\"\n    assert os.path.exists(img_path), f\"No file found at {img_path}\"\n    \n    img = Image.open(img_path)\n    resized_img = img.resize(size)\n    \n    resized_img.save(output_path)\n```"}
{"seed": "{\"task_id\": 49478, \"prompt\": \"def f_49478(ticks, labels):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"plt.xticks(range(0, len(labels), ticks), labels[::ticks])\", \"test_start\": \"\\nimport matplotlib.pyplot as plt\\n\\ndef check(candidate):\", \"test\": [\"\\n    ticks = 3\\n    labels = [1, 4]\\n    plt = candidate(ticks, labels)\\n    assert isinstance(plt, tuple)\\n\", \"\\n    labels = ['abc', 'xcv']\\n    plt = candidate(ticks, labels)\\n    assert isinstance(plt, tuple)\\n\"], \"entry_point\": \"f_49478\", \"intent\": \"X\\u8ef8\\u306e\\u9593\\u9694\\u3092`ticks`\\u306b\\u3001\\u30e9\\u30d9\\u30eb\\u3092`labels`\\u306b\\u3059\\u308b\", \"library\": [\"matplotlib\"]}", "generation": "Scenario 3:\npandas, numpy, matplotlib, seaborn, datetime\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\n\n# Constants\nTICKS = 3\nLABELS = ['Temperature', 'Humidity', 'Wind Speed']\n\ndef plot_weather_data(weather_data, city):\n    \"\"\"\n    Generates plots for weather data (temperature, humidity, wind speed) \n    for a specific city with x-axis intervals set to `ticks` and labels \n    set to `labels`.\n    \n    Parameters:\n    weather_data (DataFrame): The weather data in pandas DataFrame.\n    city (str): The city name.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - seaborn\n    - datetime\n\n    Example:\n    >>> weather_data = pd.DataFrame({\n    ... 'Temperature': np.random.randint(0, 35, 10),\n    ... 'Humidity': np.random.randint(40, 100, 10),\n    ... 'Wind Speed': np.random.randint(0, 20, 10)})\n    >>> plot_weather_data(weather_data, 'New York')\n    \"\"\"\n    sns.set_style('whitegrid')\n    fig, axs = plt.subplots(len(LABELS), figsize=(10, 15))\n\n    for i, label in enumerate(LABELS):\n        axs[i].plot(weather_data[label])\n        axs[i].set_title(f'{city} {label} over Time')\n        axs[i].set_xlabel('Time')\n        axs[i].set_ylabel(label)\n        axs[i].set_xticks(range(0, len(weather_data), TICKS))\n        axs[i].set_xticklabels(weather_data.index[::TICKS])\n        \n    plt.tight_layout()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nrandom, numpy, matplotlib, seaborn\n```python\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nTICKS = 3\nLABELS = ['Simulation 1', 'Simulation 2', 'Simulation 3']\n\ndef plot_simulation_results():\n    \"\"\"\n    Generates plots for simulation results with x-axis intervals set to `ticks` \n    and labels set to `labels`.\n    \n    Returns:\n    None\n\n    Requirements:\n    - random\n    - numpy\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> plot_simulation_results()\n    \"\"\"\n    results = np.array([random.sample(range(1, 100), 25) for _ in range(len(LABELS))])\n    sns.set_style('whitegrid')\n\n    fig, axs = plt.subplots(len(LABELS), figsize=(10, 15))\n    for i, label in enumerate(LABELS):\n        axs[i].plot(results[i])\n        axs[i].set_title(label)\n        axs[i].set_xlabel('Step')\n        axs[i].set_ylabel('Result')\n        axs[i].set_xticks(range(0, len(results[i]), TICKS))\n        axs[i].set_xticklabels(range(0, len(results[i]), TICKS))\n\n    plt.tight_layout()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, matplotlib, datetime\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom datetime import datetime, timedelta\n\n# Constants\nTICKS = 3\nLABELS = ['Open', 'Close', 'High', 'Low']\n\ndef plot_stock_data(stock_data):\n    \"\"\"\n    Generates plots for stock data (open, close, high, low prices) with \n    x-axis intervals set to `ticks` and labels set to `labels`.\n    \n    Parameters:\n    stock_data (DataFrame): The stock data in pandas DataFrame.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - datetime\n\n    Example:\n    >>> dates = pd.date_range(datetime.now(), periods=10).to_pydatetime().tolist()\n    >>> stock_data = pd.DataFrame({\n    ... 'Open': np.random.randint(100, 200, 10),\n    ... 'Close': np.random.randint(100, 200, 10),\n    ... 'High': np.random.randint(100, 200, 10),\n    ... 'Low': np.random.randint(100, 200, 10)}, index=dates)\n    >>> plot_stock_data(stock_data)\n    \"\"\"\n    fig, axs = plt.subplots(len(LABELS), figsize=(10, 15))\n\n    for i, label in enumerate(LABELS):\n        axs[i].plot(stock_data[label])\n        axs[i].set_title(f'{label} Price over Time')\n        axs[i].set_xlabel('Date')\n        axs[i].set_ylabel('Price')\n        axs[i].set_xticks(range(0, len(stock_data), TICKS))\n        axs[i].set_xticklabels(stock_data.index[::TICKS].strftime('%Y-%m-%d'))\n\n    plt.tight_layout()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 6:\nnumpy, matplotlib, seaborn, random\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\n\n# Constants\nTICKS = 3\nLABELS = ['Experiment 1', 'Experiment 2', 'Experiment 3']\n\ndef plot_experiment_results():\n    \"\"\"\n    Generates plots for experiment results with x-axis intervals set to `ticks` \n    and labels set to `labels`.\n    \n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - seaborn\n    - random\n\n    Example:\n    >>> plot_experiment_results()\n    \"\"\"\n    results = np.array([random.sample(range(1, 100), 25) for _ in range(len(LABELS))])\n    sns.set_style('whitegrid')\n\n    fig, axs = plt.subplots(len(LABELS), figsize=(10, 15))\n    for i, label in enumerate(LABELS):\n        axs[i].plot(results[i])\n        axs[i].set_title(label)\n        axs[i].set_xlabel('Step')\n        axs[i].set_ylabel('Result')\n        axs[i].set_xticks(range(0, len(results[i]), TICKS))\n        axs[i].set_xticklabels(range(0, len(results[i]), TICKS))\n\n    plt.tight_layout()\n    plt.show()\n```\n`GPT_ODEX_BREAK`"}
{"seed": "{\"task_id\": 16769, \"prompt\": \"def f_16769(num_of_file):\\n\\t\", \"suffix\": \"\\n\\treturn book_list\", \"canonical_solution\": \"book_list = []\\n\\tfor n in range(1, num_of_file + 1):\\n\\t    file_name = 'excel_file%d.xls' % (n)\\n\\t    book_list.append(xlrd.open_workbook(file_name))\", \"test_start\": \"\\nimport xlrd\\nimport xlwt\\n\\ndef check(candidate):\", \"test\": [\"\\n    num_files = 3\\n    for i in range(0, num_files):\\n        workbook = xlwt.Workbook()\\n        sheet = workbook.add_sheet('test')\\n        sheet.write(0, 1, 1)\\n\\n        workbook.save('excel_file%d.xls' % (i + 1))\\n    book_list = candidate(num_files)\\n    for bk in book_list:\\n        assert isinstance(bk, xlrd.book.Book)\\n\"], \"entry_point\": \"f_16769\", \"intent\": \"\\u9023\\u756a\\u306b\\u306a\\u3063\\u3066\\u3044\\u308b`num_of_file`\\u500b\\u306eExcel\\u30d5\\u30a1\\u30a4\\u30eb`excel_file`\\u3092\\u30ea\\u30b9\\u30c8\\u306b\\u8aad\\u307f\\u8fbc\\u3080\", \"library\": [\"xlrd\", \"xlwt\"]}", "generation": "Scenario 1:\nxlrd, xlwt, os, glob, pandas\n```python\nimport xlrd\nimport xlwt\nimport os\nimport glob\nimport pandas as pd\n\n# Constants\nEXCEL_FILE_PATH = '/path/to/excel/files/'\n\ndef get_excel_data(file_pattern):\n    \"\"\"\n    Read all Excel files in a directory that match a given pattern,\n    convert each to a pandas DataFrame, and return a list of DataFrames.\n    \n    Parameters:\n    file_pattern (str): The pattern for matching Excel file names.\n\n    Returns:\n    list: A list of pandas DataFrames.\n\n    Requirements:\n    - xlrd\n    - xlwt\n    - os\n    - glob\n    - pandas\n\n    Example:\n    >>> df_list = get_excel_data('excel_file*.xls')\n    >>> for df in df_list:\n    >>>     print(df.head())\n    \"\"\"\n    file_path = os.path.join(EXCEL_FILE_PATH, file_pattern)\n    excel_files = glob.glob(file_path)\n    df_list = []\n\n    for file in excel_files:\n        df = pd.read_excel(file)\n        df_list.append(df)\n    \n    return df_list\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nxlrd, xlwt, os, csv\n```python\nimport xlrd\nimport xlwt\nimport os\nimport csv\n\n# Constants\nEXCEL_FILE_PATH = '/path/to/excel/files/'\nCSV_FILE_PATH = '/path/to/csv/files/'\n\ndef convert_excel_to_csv(file_name):\n    \"\"\"\n    Convert an Excel file to a CSV file.\n\n    Parameters:\n    file_name (str): The name of the Excel file.\n\n    Returns:\n    str: The name of the converted CSV file.\n\n    Requirements:\n    - xlrd\n    - xlwt\n    - os\n    - csv\n\n    Example:\n    >>> csv_file_name = convert_excel_to_csv('excel_file1.xls')\n    >>> print(csv_file_name)\n    \"\"\"\n    excel_file = os.path.join(EXCEL_FILE_PATH, file_name)\n    workbook = xlrd.open_workbook(excel_file)\n    sheet = workbook.sheet_by_index(0)\n\n    csv_file_name = os.path.splitext(file_name)[0] + '.csv'\n    csv_file = os.path.join(CSV_FILE_PATH, csv_file_name)\n\n    with open(csv_file, 'w', newline='') as file:\n        writer = csv.writer(file)\n        for row_num in range(sheet.nrows):\n            row_value = sheet.row_values(row_num)\n            writer.writerow(row_value)\n\n    return csv_file_name\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nxlrd, xlwt, os, pandas, matplotlib\n```python\nimport xlrd\nimport xlwt\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nEXCEL_FILE_PATH = '/path/to/excel/files/'\n\ndef plot_excel_data(file_name, column_name):\n    \"\"\"\n    Plot a histogram of data from a specified column in an Excel file.\n\n    Parameters:\n    file_name (str): The name of the Excel file.\n    column_name (str): The name of the column to plot.\n\n    Returns:\n    None\n\n    Requirements:\n    - xlrd\n    - xlwt\n    - os\n    - pandas\n    - matplotlib\n\n    Example:\n    >>> plot_excel_data('excel_file1.xls', 'Sales')\n    \"\"\"\n    excel_file = os.path.join(EXCEL_FILE_PATH, file_name)\n    df = pd.read_excel(excel_file)\n\n    df[column_name].hist()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nxlrd, xlwt, os, pandas, numpy\n```python\nimport xlrd\nimport xlwt\nimport os\nimport pandas as pd\nimport numpy as np\n\n# Constants\nEXCEL_FILE_PATH = '/path/to/excel/files/'\n\ndef calculate_statistics(file_name, column_name):\n    \"\"\"\n    Calculate the mean, median, and standard deviation of data \n    from a specified column in an Excel file.\n\n    Parameters:\n    file_name (str): The name of the Excel file.\n    column_name (str): The name of the column to analyze.\n\n    Returns:\n    dict: A dictionary with the mean, median, and standard deviation.\n\n    Requirements:\n    - xlrd\n    - xlwt\n    - os\n    - pandas\n    - numpy\n\n    Example:\n    >>> stats = calculate_statistics('excel_file1.xls', 'Sales')\n    >>> print(stats)\n    \"\"\"\n    excel_file = os.path.join(EXCEL_FILE_PATH, file_name)\n    df = pd.read_excel(excel_file)\n\n    mean = np.mean(df[column_name])\n    median = np.median(df[column_name])\n    std_dev = np.std(df[column_name])\n\n    return {'mean': mean, 'median': median, 'std_dev': std_dev}\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nxlrd, xlwt, os, pandas, datetime\n```python\nimport xlrd\nimport xlwt\nimport os\nimport pandas as pd\nfrom datetime import datetime\n\n# Constants\nEXCEL_FILE_PATH = '/path/to/excel/files/'\n\ndef filter_by_date(file_name, column_name, start_date, end_date):\n    \"\"\"\n    Filter data in a specified date range from a column in an Excel file,\n    and return a pandas DataFrame of the filtered data.\n\n    Parameters:\n    file_name (str): The name of the Excel file.\n    column_name (str): The name of the date column to filter.\n    start_date (str): The start date in 'yyyy-mm-dd' format.\n    end_date (str): The end date in 'yyyy-mm-dd' format.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the filtered data.\n\n    Requirements:\n    - xlrd\n    - xlwt\n    - os\n    - pandas\n    - datetime\n\n    Example:\n    >>> filtered_df = filter_by_date('excel_file1.xls', 'Date', '2020-01-01', '2020-12-31')\n    >>> print(filtered_df)\n    \"\"\"\n    excel_file = os.path.join(EXCEL_FILE_PATH, file_name)\n    df = pd.read_excel(excel_file)\n\n    df[column_name] = pd.to_datetime(df[column_name])\n    start_date = datetime.strptime(start_date, '%Y-%m-%d')\n    end_date = datetime.strptime(end_date, '%Y-%m-%d')\n\n    filtered_df = df[(df[column_name] >= start_date) & (df[column_name] <= end_date)]\n\n    return filtered_df\n```"}
{"seed": "{\"task_id\": 2220, \"prompt\": \"def f_2220(dt):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"dt.timestamp()\", \"test_start\": \"\\nimport pytz\\nimport datetime\\n\\ndef check(candidate):\", \"test\": [\"\\n    dt = datetime.datetime.fromtimestamp(123456789.123456, pytz.timezone('America/Los_Angeles'))\\n    assert candidate(dt) == 123456789.123456\\n\"], \"entry_point\": \"f_2220\", \"intent\": \"datetime\\u30aa\\u30d6\\u30b8\\u30a7\\u30af\\u30c8`dt`\\u304b\\u3089Unix Time\\u3092\\u6c42\\u3081\\u308b\", \"library\": [\"datetime\", \"pytz\"]}", "generation": "Scenario 1: \npandas, pytz, datetime, random, seaborn\n```python\nimport pandas as pd\nimport pytz\nfrom datetime import datetime\nfrom random import randint\nimport seaborn as sns\n\n# Constants\nCITIES = ['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']\nTEMPERATURE_RANGES = {'New York': (-5, 30), 'London': (0, 25), 'Beijing': (-10, 30), 'Tokyo': (0, 35), 'Sydney': (10, 40)}\n\n# Time zones for the cities\nTIMEZONES = {\n    'New York': 'America/New_York',\n    'London': 'Europe/London',\n    'Beijing': 'Asia/Shanghai',\n    'Tokyo': 'Asia/Tokyo',\n    'Sydney': 'Australia/Sydney'\n}\n\ndef generate_temperature_report(utc_datetime):\n    \"\"\"\n    Generate a report of temperature for a list of cities across various \n    time zones at a given time (UTC).\n    \n    Parameters:\n    utc_datetime (datetime): The datetime in UTC.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with temperatures for the cities.\n    \n    Requirements:\n    - pandas\n    - pytz\n    - datetime\n    - random\n    - seaborn\n    \n    Example:\n    >>> utc_time = datetime(2023, 6, 15, 12, 0, 0, tzinfo=pytz.UTC)\n    >>> report = generate_temperature_report(utc_time)\n    >>> print(report)\n    >>> sns.boxplot(data=report, x='City', y='Temperature')\n    \"\"\"\n    report_data = []\n\n    for city in CITIES:\n        city_tz = pytz.timezone(TIMEZONES[city])\n        city_time = utc_datetime.astimezone(city_tz)\n        temperature = randint(*TEMPERATURE_RANGES[city])\n        report_data.append([city, city_time, temperature])\n\n    report_df = pd.DataFrame(report_data, columns=['City', 'Local Time', 'Temperature'])\n\n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ndatetime, pytz, math, numpy\n```python\nimport pytz\nfrom datetime import datetime, timedelta\nimport math\nimport numpy as np\n\n# Constants\nEARTH_RADIUS = 6371  # in kilometers\nLOCATIONS = {\n    'New York': (40.7128, -74.0060),\n    'London': (51.5074, -0.1278),\n    'Beijing': (39.9042, 116.4074),\n    'Tokyo': (35.6895, 139.6917),\n    'Sydney': (-33.8688, 151.2093)\n}\n\ndef calculate_distance(city1, city2):\n    \"\"\"\n    Calculate the distance between two cities using their coordinates.\n\n    Parameters:\n    city1 (str): The first city.\n    city2 (str): The second city.\n\n    Returns:\n    float: The distance in kilometers.\n\n    Requirements:\n    - datetime\n    - pytz\n    - math\n    - numpy\n\n    Example:\n    >>> calculate_distance('New York', 'London')\n    \"\"\"\n    lat1, lon1 = map(math.radians, LOCATIONS[city1])\n    lat2, lon2 = map(math.radians, LOCATIONS[city2])\n\n    dlon = lon2 - lon1\n    dlat = lat2 - lat1\n\n    a = math.sin(dlat/2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon/2)**2\n    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n\n    distance = EARTH_RADIUS * c\n\n    return distance\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ndatetime, pytz, numpy, pandas\n```python\nimport pytz\nfrom datetime import datetime\nimport numpy as np\nimport pandas as pd\n\n# Constants\nDATES = pd.date_range(start='1/1/2000', end='1/01/2020').to_pydatetime().tolist()\n\ndef find_nearest_date(target_date):\n    \"\"\"\n    Find the date in a list of dates that is nearest to a given date.\n\n    Parameters:\n    target_date (datetime): The target date.\n\n    Returns:\n    datetime: The nearest date.\n\n    Requirements:\n    - datetime\n    - pytz\n    - numpy\n    - pandas\n\n    Example:\n    >>> find_nearest_date(datetime(2010, 1, 1))\n    \"\"\"\n    nearest_date = min(DATES, key=lambda x: abs(x - target_date))\n\n    return nearest_date\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ndatetime, pytz, math, matplotlib\n```python\nimport pytz\nfrom datetime import datetime\nimport math\nimport matplotlib.pyplot as plt\n\n# Constants\nPERIOD = 24  # in hours\n\ndef plot_time_vs_sine_wave(start_time, end_time):\n    \"\"\"\n    Plot a sine wave with time on the x-axis and the sine of the time (in hours) on the y-axis.\n\n    Parameters:\n    start_time (datetime): The start time.\n    end_time (datetime): The end time.\n\n    Returns:\n    None\n\n    Requirements:\n    - datetime\n    - pytz\n    - math\n    - matplotlib\n\n    Example:\n    >>> plot_time_vs_sine_wave(datetime(2023, 6, 15, 0, 0, 0), datetime(2023, 6, 16, 0, 0, 0))\n    \"\"\"\n    time_difference = end_time - start_time\n    total_hours = time_difference.total_seconds() / 3600\n\n    hours = range(int(total_hours) + 1)\n    sine_values = [math.sin(2 * math.pi * (hour / PERIOD)) for hour in hours]\n\n    plt.plot(hours, sine_values)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\ndatetime, pytz, numpy, matplotlib\n```python\nimport pytz\nfrom datetime import datetime\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nSECONDS_IN_DAY = 24 * 60 * 60\n\ndef plot_time_vs_random_walk(start_time, end_time):\n    \"\"\"\n    Plot a random walk with time on the x-axis and the random walk value on the y-axis.\n\n    Parameters:\n    start_time (datetime): The start time.\n    end_time (datetime): The end time.\n\n    Returns:\n    None\n\n    Requirements:\n    - datetime\n    - pytz\n    - numpy\n    - matplotlib\n\n    Example:\n    >>> plot_time_vs_random_walk(datetime(2023, 6, 15, 0, 0, 0), datetime(2023, 6, 16, 0, 0, 0))\n    \"\"\"\n    time_difference = end_time - start_time\n    total_seconds = time_difference.total_seconds()\n\n    seconds = range(int(total_seconds) + 1)\n    random_walk = np.random.randn(len(seconds)).cumsum()\n\n    plt.plot(seconds, random_walk)\n    plt.show()\n```\n"}
{"seed": "{\"task_id\": 44723, \"prompt\": \"def f_44723(df, col_name, string):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"df[df[col_name].str.contains(string)]\", \"test_start\": \"\\nimport pandas as pd \\n\\ndef check(candidate):\", \"test\": [\"\\n    df = pd.DataFrame({'name': ['Mr. A', 'Ms. B'], 'age': [30, 23]})\\n    df_sub = pd.DataFrame({'name': ['Mr. A'], 'age': [30]})\\n    assert candidate(df, 'name', 'A').equals(df_sub)\\n\"], \"entry_point\": \"f_44723\", \"intent\": \"\\u884c`col_name`\\u306b\\u7279\\u5b9a\\u306e\\u6587\\u5b57\\u5217`string`\\u3092\\u542b\\u3080\\u884c\\u3092\\u30c7\\u30fc\\u30bf\\u30d5\\u30ec\\u30fc\\u30e0`df`\\u304b\\u3089\\u62bd\\u51fa\\u3059\\u308b\", \"library\": [\"pandas\"]}", "generation": "Scenario 1:\npandas, random, seaborn, matplotlib\n```python\nimport pandas as pd\nfrom random import choices\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nCATEGORIES = ['Electronics', 'Fashion', 'Groceries', 'Furniture', 'Books']\nCITIES = ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix']\n\ndef generate_sales_data(n):\n    \"\"\"\n    Generate a dataframe of sales data with 'n' rows across various categories and cities. \n    Then, plot a countplot for each category and city.\n    \n    Parameters:\n    n (int): The number of rows of sales data to generate.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the generated sales data.\n    \n    Requirements:\n    - pandas\n    - random\n    - seaborn\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = generate_sales_data(1000)\n    >>> print(df)\n    >>> plt.figure(figsize=(10,5))\n    >>> sns.countplot(x='Category', data=df)\n    >>> plt.figure(figsize=(10,5))\n    >>> sns.countplot(x='City', data=df)\n    \"\"\"\n    data = {\n        'Category': choices(CATEGORIES, k=n),\n        'City': choices(CITIES, k=n)\n    }\n\n    df = pd.DataFrame(data)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, matplotlib, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nSCORES = ['Excellent', 'Good', 'Average', 'Poor', 'Terrible']\nPRODUCTS = ['Product A', 'Product B', 'Product C', 'Product D', 'Product E']\n\ndef generate_product_reviews(n):\n    \"\"\"\n    Generate a dataframe of product reviews with 'n' rows across various products and scores. \n    Then, plot a histogram for each score and a boxplot for each product.\n    \n    Parameters:\n    n (int): The number of rows of product reviews to generate.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the generated product reviews.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - seaborn\n    \n    Example:\n    >>> df = generate_product_reviews(5000)\n    >>> print(df)\n    >>> plt.figure(figsize=(10,5))\n    >>> sns.histplot(data=df, x='Score', hue='Product', multiple='stack')\n    >>> plt.figure(figsize=(10,5))\n    >>> sns.boxplot(x='Product', y='Rating', data=df)\n    \"\"\"\n    data = {\n        'Product': np.random.choice(PRODUCTS, n),\n        'Score': np.random.choice(SCORES, n),\n        'Rating': np.random.uniform(1, 5, n)\n    }\n\n    df = pd.DataFrame(data)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, random, matplotlib, seaborn\n```python\nimport pandas as pd\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nAGE_RANGE = range(18, 65)\nGENDER = ['Male', 'Female']\nTITLES = ['Manager', 'Engineer', 'Sales', 'HR', 'Marketing']\n\ndef generate_employee_data(n):\n    \"\"\"\n    Generate a dataframe of employee data with 'n' rows across various titles, genders, and ages. \n    Then, plot a countplot for each title and a histogram for age distribution.\n    \n    Parameters:\n    n (int): The number of rows of employee data to generate.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the generated employee data.\n    \n    Requirements:\n    - pandas\n    - random\n    - matplotlib.pyplot\n    - seaborn\n    \n    Example:\n    >>> df = generate_employee_data(1000)\n    >>> print(df)\n    >>> plt.figure(figsize=(10,5))\n    >>> sns.countplot(x='Title', data=df)\n    >>> plt.figure(figsize=(10,5))\n    >>> sns.histplot(data=df, x='Age')\n    \"\"\"\n    data = {\n        'Title': [TITLES[randint(0, len(TITLES)-1)] for _ in range(n)],\n        'Gender': [GENDER[randint(0, len(GENDER)-1)] for _ in range(n)],\n        'Age': [AGE_RANGE[randint(0, len(AGE_RANGE)-1)] for _ in range(n)]\n    }\n\n    df = pd.DataFrame(data)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, matplotlib, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nCOUNTRIES = ['USA', 'Germany', 'China', 'Japan', 'India']\nPRODUCTS = ['Product 1', 'Product 2', 'Product 3', 'Product 4', 'Product 5']\n\ndef generate_sales_volume(n):\n    \"\"\"\n    Generate a dataframe of sales volume with 'n' rows across various countries and products. \n    Then, plot a barplot for each product and a pie chart for country distribution.\n    \n    Parameters:\n    n (int): The number of rows of sales volume to generate.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the generated sales volume.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - seaborn\n    \n    Example:\n    >>> df = generate_sales_volume(5000)\n    >>> print(df)\n    >>> plt.figure(figsize=(10,5))\n    >>> sns.barplot(x='Product', y='Sales Volume', data=df)\n    >>> plt.figure(figsize=(10,5))\n    >>> df['Country'].value_counts().plot(kind='pie', autopct='%1.1f%%')\n    \"\"\"\n    data = {\n        'Product': np.random.choice(PRODUCTS, n),\n        'Country': np.random.choice(COUNTRIES, n),\n        'Sales Volume': np.random.randint(1, 1000, n)\n    }\n\n    df = pd.DataFrame(data)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, random, matplotlib, seaborn\n```python\nimport pandas as pd\nfrom random import choices\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nCATEGORIES = ['Food', 'Drinks', 'Desserts', 'Appetizers', 'Mains']\nRESTAURANTS = ['Restaurant 1', 'Restaurant 2', 'Restaurant 3', 'Restaurant 4', 'Restaurant 5']\n\ndef generate_order_data(n):\n    \"\"\"\n    Generate a dataframe of order data with 'n' rows across various categories and restaurants. \n    Then, plot a countplot for each category and a barplot for each restaurant.\n    \n    Parameters:\n    n (int): The number of rows of order data to generate.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the generated order data.\n    \n    Requirements:\n    - pandas\n    - random\n    - matplotlib.pyplot\n    - seaborn\n    \n    Example:\n    >>> df = generate_order_data(1000)\n    >>> print(df)\n    >>> plt.figure(figsize=(10,5))\n    >>> sns.countplot(x='Category', data=df)\n    >>> plt.figure(figsize=(10,5))\n    >>> df['Restaurant'].value_counts().plot(kind='bar')\n    \"\"\"\n    data = {\n        'Category': choices(CATEGORIES, k=n),\n        'Restaurant': choices(RESTAURANTS, k=n)\n    }\n\n    df = pd.DataFrame(data)\n\n    return df\n```"}
{"seed": "{\"task_id\": 46711, \"prompt\": \"def f_46711(list, n):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"list[-n:]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([1,2,3,4,5,6,7], 2) == [6,7]\\n\", \"\\n    assert candidate([1,2,3], 0) == [1,2,3]\\n\"], \"entry_point\": \"f_46711\", \"intent\": \"\\u30ea\\u30b9\\u30c8`list`\\u306e\\u672b\\u5c3e\\u304b\\u3089`n`\\u500b\\u306e\\u8981\\u7d20\\u3092\\u53d6\\u308a\\u51fa\\u3059\", \"library\": []}", "generation": "Scenario 1:\npandas, numpy, random, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nPRODUCTS = ['Apple', 'Banana', 'Cherry', 'Date', 'Elderberry']\nSTORES = ['Store 1', 'Store 2', 'Store 3', 'Store 4', 'Store 5']\n\ndef generate_sales_report(n):\n    \"\"\"\n    Generate a report of sales for a list of products across various \n    stores for the last n days.\n    \n    Parameters:\n    n (int): The last number of days to generate the report.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with sales for the products.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> report = generate_sales_report(30)\n    >>> print(report)\n    >>> report.groupby('Product')['Quantity'].sum().plot(kind='bar')\n    \"\"\"\n    report_data = []\n\n    for _ in range(n):\n        for product in PRODUCTS:\n            for store in STORES:\n                quantity = randint(0, 100)\n                report_data.append([product, store, quantity])\n\n    report_df = pd.DataFrame(report_data, columns=['Product', 'Store', 'Quantity'])\n\n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, datetime, random\n```python\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nfrom random import randint\n\n# Constants\nUSERS = ['User 1', 'User 2', 'User 3', 'User 4', 'User 5']\nACTIONS = ['Login', 'Logout', 'View', 'Edit', 'Delete']\n\ndef generate_user_logs(n):\n    \"\"\"\n    Generate a log of user actions for a list of users over the past n days.\n    \n    Parameters:\n    n (int): The last number of days to generate the logs.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with user logs.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - datetime\n    - random\n    \n    Example:\n    >>> logs = generate_user_logs(7)\n    >>> print(logs)\n    >>> logs['User'].value_counts().plot(kind='bar')\n    \"\"\"\n    log_data = []\n\n    for i in range(n):\n        date = datetime.now() - timedelta(days=i)\n        for user in USERS:\n            action = ACTIONS[randint(0, len(ACTIONS)-1)]\n            log_data.append([date, user, action])\n\n    log_df = pd.DataFrame(log_data, columns=['Date', 'User', 'Action'])\n\n    return log_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, random, matplotlib\n```python\nimport numpy as np\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nTASKS = ['Task 1', 'Task 2', 'Task 3', 'Task 4', 'Task 5']\n\ndef plot_task_progress(n):\n    \"\"\"\n    Generate and plot the progress of a list of tasks over n days.\n    \n    Parameters:\n    n (int): The last number of days to plot the progress.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - numpy\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> plot_task_progress(30)\n    \"\"\"\n    progress_data = {}\n\n    for task in TASKS:\n        progress_data[task] = [randint(0, 100) for _ in range(n)]\n\n    for task, progress in progress_data.items():\n        plt.plot(progress, label=task)\n\n    plt.legend()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, random\n```python\nimport pandas as pd\nimport numpy as np\nfrom random import randint\n\n# Constants\nCATEGORIES = ['Category 1', 'Category 2', 'Category 3', 'Category 4', 'Category 5']\n\ndef generate_product_catalog(n):\n    \"\"\"\n    Generate a product catalog with n products across various categories.\n    \n    Parameters:\n    n (int): The number of products to generate in the catalog.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with the product catalog.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - random\n    \n    Example:\n    >>> catalog = generate_product_catalog(100)\n    >>> print(catalog)\n    \"\"\"\n    catalog_data = []\n\n    for i in range(n):\n        product = f'Product {i+1}'\n        category = CATEGORIES[randint(0, len(CATEGORIES)-1)]\n        price = randint(1, 100)\n        catalog_data.append([product, category, price])\n\n    catalog_df = pd.DataFrame(catalog_data, columns=['Product', 'Category', 'Price'])\n\n    return catalog_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, random, matplotlib\n```python\nimport numpy as np\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n# Constants\nTEAMS = ['Team 1', 'Team 2', 'Team 3', 'Team 4', 'Team 5']\n\ndef plot_team_scores(n):\n    \"\"\"\n    Generate and plot the scores of a list of teams over n games.\n    \n    Parameters:\n    n (int): The number of games to plot the scores.\n    \n    Returns:\n    None\n    \n    Requirements:\n    - numpy\n    - random\n    - matplotlib.pyplot\n    \n    Example:\n    >>> plot_team_scores(10)\n    \"\"\"\n    scores_data = {}\n\n    for team in TEAMS:\n        scores_data[team] = [randint(0, 100) for _ in range(n)]\n\n    for team, scores in scores_data.items():\n        plt.plot(scores, label=team)\n\n    plt.legend()\n    plt.show()\n```"}
{"seed": "{\"task_id\": 17648, \"prompt\": \"def f_17648(b_string):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"b_string.decode('unicode-escape')\", \"test_start\": \"\\nimport unicodedata\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(b'example-string') == 'example-string'\\n\"], \"entry_point\": \"f_17648\", \"intent\": \"Unicode\\u30a8\\u30b9\\u30b1\\u30fc\\u30d7\\u3055\\u308c\\u305f\\u30d0\\u30a4\\u30c8\\u5217`b_string`\\u3092\\u6587\\u5b57\\u5217\\u306b\\u5909\\u63db\", \"library\": [\"unicodedata\"]}", "generation": "Scenario 1:\n```python\nimport unicodedata\nimport json\nimport base64\n\n# Constants\nJSON_FILE = 'data.json'\nENCODING = 'utf-8'\n\ndef decode_json_base64_to_string(json_file):\n    \"\"\"\n    Read a json file where each key represents a unique identifier and the value is a base64 encoded string. \n    Decode the base64 string and return a dictionary with the same keys and the decoded strings as values.\n\n    Parameters:\n    json_file (str): The json file path.\n    \n    Returns:\n    dict: The decoded dictionary.\n\n    Requirements:\n    - unicodedata\n    - json\n    - base64\n\n    Example:\n    >>> decode_json_base64_to_string('data.json')\n    \"\"\"\n    with open(json_file, 'r') as f:\n        data = json.load(f)\n    \n    decoded_data = {k: base64.b64decode(v).decode(ENCODING) for k, v in data.items()}\n\n    return decoded_data\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\n```python\nimport unicodedata\nimport csv\nimport codecs\n\n# Constants\nCSV_FILE = 'data.csv'\nENCODING = 'utf-8'\n\ndef decode_csv_unicode_escape(csv_file):\n    \"\"\"\n    Read a CSV file where each row contains a unicode escaped string in its first column. \n    Decode the strings and return a list with the decoded strings.\n\n    Parameters:\n    csv_file (str): The CSV file path.\n    \n    Returns:\n    list: The list of decoded strings.\n\n    Requirements:\n    - unicodedata\n    - csv\n    - codecs\n\n    Example:\n    >>> decode_csv_unicode_escape('data.csv')\n    \"\"\"\n    decoded_strings = []\n\n    with open(csv_file, 'r') as f:\n        reader = csv.reader(f)\n        for row in reader:\n            decoded_string = codecs.decode(row[0], 'unicode_escape')\n            decoded_strings.append(decoded_string)\n\n    return decoded_strings\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\n```python\nimport unicodedata\nimport re\nimport os\n\n# Constants\nDIRECTORY_PATH = './'\nFILE_EXTENSION = '.txt'\nENCODING = 'utf-8'\n\ndef decode_txt_files_in_directory(directory_path):\n    \"\"\"\n    Decode all unicode escaped strings in all text files in a given directory and subdirectories.\n\n    Parameters:\n    directory_path (str): The directory path.\n    \n    Returns:\n    None\n\n    Requirements:\n    - unicodedata\n    - re\n    - os\n\n    Example:\n    >>> decode_txt_files_in_directory('./')\n    \"\"\"\n    for root, dirs, files in os.walk(directory_path):\n        for file in files:\n            if file.endswith(FILE_EXTENSION):\n                with open(os.path.join(root, file), 'r', encoding=ENCODING) as f:\n                    content = f.read()\n                decoded_content = codecs.decode(content, 'unicode_escape')\n                with open(os.path.join(root, file), 'w', encoding=ENCODING) as f:\n                    f.write(decoded_content)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\n```python\nimport unicodedata\nimport requests\nimport bs4\n\n# Constants\nURL = 'http://example.com'\n\ndef decode_unicode_escape_from_html(url):\n    \"\"\"\n    Fetch an HTML page from a given URL and decode all unicode escaped strings in the HTML content.\n\n    Parameters:\n    url (str): The URL.\n    \n    Returns:\n    str: The decoded HTML content.\n\n    Requirements:\n    - unicodedata\n    - requests\n    - bs4\n\n    Example:\n    >>> decode_unicode_escape_from_html('http://example.com')\n    \"\"\"\n    response = requests.get(url)\n    soup = bs4.BeautifulSoup(response.text, 'html.parser')\n    html_content = str(soup)\n    decoded_html_content = codecs.decode(html_content, 'unicode_escape')\n\n    return decoded_html_content\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\n```python\nimport unicodedata\nimport pandas as pd\nimport numpy as np\n\n# Constants\nDATAFRAME = pd.DataFrame({\n    'Name': ['John', 'Anna', 'Peter'],\n    'Age': [27, 23, 29],\n    'Salary': [50000, 60000, 70000],\n    'UnicodeString': ['\\u004A\\u006F\\u0068\\u006E', '\\u0041\\u006E\\u006E\\u0061', '\\u0050\\u0065\\u0074\\u0065\\u0072']\n})\n\ndef decode_unicode_escape_in_dataframe(dataframe):\n    \"\"\"\n    Decode all unicode escaped strings in a specific column ('UnicodeString') in a given pandas DataFrame.\n\n    Parameters:\n    dataframe (DataFrame): The pandas DataFrame.\n    \n    Returns:\n    DataFrame: The DataFrame with decoded strings.\n\n    Requirements:\n    - unicodedata\n    - pandas\n    - numpy\n\n    Example:\n    >>> decode_unicode_escape_in_dataframe(DATAFRAME)\n    \"\"\"\n    dataframe['UnicodeString'] = dataframe['UnicodeString'].apply(lambda x: codecs.decode(x, 'unicode_escape'))\n\n    return dataframe\n```"}
{"seed": "{\"task_id\": 9633, \"prompt\": \"def f_9633(str, old_s, new_s):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"str.replace(old_s, new_s)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('mystring', 'my', 'your') == 'yourstring'\\n\"], \"entry_point\": \"f_9633\", \"intent\": \"\\u6587\\u5b57\\u5217`str`\\u5185\\u306e\\u5bfe\\u8c61\\u6587\\u5b57\\u5217`old_s`\\u3092\\u5225\\u306e\\u6587\\u5b57\\u5217`new_s`\\u306b\\u7f6e\\u63db\\u3059\\u308b\", \"library\": []}", "generation": "Scenario 1:\npandas, re, string, random\n```python\nimport pandas as pd\nimport re\nimport string\nimport random\n\n# Constants\nLETTERS = string.ascii_lowercase\n\ndef replace_string_in_dataframe(dataframe, old_s, new_s):\n    \"\"\"\n    Given a pandas dataframe, replace all occurrences of a specific string 'old_s' \n    in the dataframe with a new string 'new_s'. The function return a new dataframe \n    with the replaced values.\n\n    Parameters:\n    dataframe (DataFrame): The pandas DataFrame.\n    old_s (str): The old string to be replaced.\n    new_s (str): The new string to replace the old string with.\n\n    Returns:\n    DataFrame: The pandas DataFrame with the replaced values.\n\n    Requirements:\n    - pandas\n    - re\n    - string\n    - random\n\n    Example:\n    >>> df = pd.DataFrame({'col1': ['abc', 'def'], 'col2': ['ghi', 'jkl']})\n    >>> replace_string_in_dataframe(df, 'a', 'z')\n    \"\"\"\n    dataframe = dataframe.copy()\n    for col in dataframe.columns:\n        dataframe[col] = dataframe[col].apply(lambda x: re.sub(old_s, new_s, x) if isinstance(x, str) else x)\n    \n    return dataframe\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, string, random, re\n```python\nimport numpy as np\nimport string\nimport random\nimport re\n\n# Constants\nLETTERS = string.ascii_lowercase\n\ndef replace_string_in_numpy_array(array, old_s, new_s):\n    \"\"\"\n    Given a numpy array, replace all occurrences of a specific string 'old_s' \n    in the array with a new string 'new_s'. The function return a new array \n    with the replaced values.\n\n    Parameters:\n    array (numpy array): The numpy array.\n    old_s (str): The old string to be replaced.\n    new_s (str): The new string to replace the old string with.\n\n    Returns:\n    numpy array: The numpy array with the replaced values.\n\n    Requirements:\n    - numpy\n    - re\n    - string\n    - random\n\n    Example:\n    >>> array = np.array(['abc', 'def', 'ghi', 'jkl'])\n    >>> replace_string_in_numpy_array(array, 'a', 'z')\n    \"\"\"\n    vect_replace = np.vectorize(lambda x: re.sub(old_s, new_s, x) if isinstance(x, str) else x)\n    new_array = vect_replace(array)\n    \n    return new_array\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, re, glob, shutil\n```python\nimport os\nimport re\nimport glob\nimport shutil\n\ndef replace_string_in_file_names(directory, old_s, new_s):\n    \"\"\"\n    Given a directory, replace all occurrences of a specific string 'old_s' in the \n    file names within the directory with a new string 'new_s'. The function return \n    a list of the new file names.\n\n    Parameters:\n    directory (str): The directory path.\n    old_s (str): The old string to be replaced.\n    new_s (str): The new string to replace the old string with.\n\n    Returns:\n    list: The list of the new file names.\n\n    Requirements:\n    - os\n    - re\n    - glob\n    - shutil\n\n    Example:\n    >>> replace_string_in_file_names('/path/to/directory', 'old', 'new')\n    \"\"\"\n    new_names = []\n    for file_path in glob.glob(directory + '/*'):\n        file_name = os.path.basename(file_path)\n        new_file_name = re.sub(old_s, new_s, file_name)\n        new_file_path = os.path.join(directory, new_file_name)\n        shutil.move(file_path, new_file_path)\n        new_names.append(new_file_name)\n        \n    return new_names\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, pandas, re, string\n```python\nimport numpy as np\nimport pandas as pd\nimport re\nimport string\n\n# Constants\nLETTERS = string.ascii_lowercase\n\ndef replace_string_in_series(series, old_s, new_s):\n    \"\"\"\n    Given a pandas Series, replace all occurrences of a specific string 'old_s' \n    in the series with a new string 'new_s'. The function return a new series \n    with the replaced values.\n\n    Parameters:\n    series (Series): The pandas Series.\n    old_s (str): The old string to be replaced.\n    new_s (str): The new string to replace the old string with.\n\n    Returns:\n    Series: The pandas Series with the replaced values.\n\n    Requirements:\n    - numpy\n    - pandas\n    - re\n    - string\n\n    Example:\n    >>> series = pd.Series(['abc', 'def', 'ghi', 'jkl'])\n    >>> replace_string_in_series(series, 'a', 'z')\n    \"\"\"\n    new_series = series.apply(lambda x: re.sub(old_s, new_s, x) if isinstance(x, str) else x)\n    \n    return new_series\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, re, random, string\n```python\nimport numpy as np\nimport re\nimport random\nimport string\n\n# Constants\nLETTERS = string.ascii_lowercase\n\ndef replace_string_in_2d_numpy_array(array, old_s, new_s):\n    \"\"\"\n    Given a 2D numpy array, replace all occurrences of a specific string 'old_s' \n    in the array with a new string 'new_s'. The function return a new array \n    with the replaced values.\n\n    Parameters:\n    array (numpy array): The 2D numpy array.\n    old_s (str): The old string to be replaced.\n    new_s (str): The new string to replace the old string with.\n\n    Returns:\n    numpy array: The 2D numpy array with the replaced values.\n\n    Requirements:\n    - numpy\n    - re\n    - random\n    - string\n\n    Example:\n    >>> array = np.array([['abc', 'def'], ['ghi', 'jkl']])\n    >>> replace_string_in_2d_numpy_array(array, 'a', 'z')\n    \"\"\"\n    vect_replace = np.vectorize(lambda x: re.sub(old_s, new_s, x) if isinstance(x, str) else x)\n    new_array = vect_replace(array)\n    \n    return new_array\n```"}
{"seed": "{\"task_id\": 37327, \"prompt\": \"def f_37327():\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"\\n\\twhile True:\\n\\t\\ttry:\\n\\t\\t\\tline = input()\\n\\t\\t\\tif line == '':\\n\\t\\t\\t\\tbreak\\n\\t\\t\\telse:\\n\\t\\t\\t\\tyield line\\n\\t\\texcept EOFError:\\n\\t\\t\\tbreak\\n\", \"test_start\": \"\\nimport sys\\n\\ndef check(candidate):\", \"test\": [\"\\n    with open('p.txt', 'w') as f:\\n        f.write('1\\\\n\\\\n')\\n    f = open('p.txt')\\n    sys.stdin = f\\n    d = candidate()\\n    print(type(d))\\n    assert 'generator' in str(type(d))\\n    sys.stdin = sys.__stdin__\\n\"], \"entry_point\": \"f_37327\", \"intent\": \"\\u7a7a\\u884c\\u304c\\u5165\\u529b\\u3055\\u308c\\u308b\\u307e\\u3067\\u6a19\\u6e96\\u5165\\u529b\\u3092\\u53d7\\u3051\\u4ed8\\u3051\\u308b\", \"library\": [\"sys\"]}", "generation": "Scenario 1: \nos, pandas, sys, json\n```python\nimport os\nimport pandas as pd\nimport sys\nimport json\n\ndef read_input_and_write_to_csv():\n    \"\"\"\n    Reads JSON input from the terminal until an empty line is received, and writes \n    the data into a csv file in the current directory.\n\n    Returns:\n    str: The path to the csv file.\n\n    Requirements:\n    - os\n    - pandas\n    - sys\n    - json\n\n    Example:\n    >>> path = read_input_and_write_to_csv()\n    >>> pd.read_csv(path)\n    \"\"\"\n    data = []\n    while True:\n        try:\n            line = input()\n            if line == '':\n                break\n            else:\n                data.append(json.loads(line))\n        except EOFError:\n            break\n    \n    df = pd.DataFrame(data)\n    path = os.path.join(os.getcwd(), 'data.csv')\n    df.to_csv(path, index=False)\n    \n    return path\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nos, sys, csv, pathlib\n```python\nimport os\nimport sys\nimport csv\nfrom pathlib import Path\n\ndef generate_directory_structure():\n    \"\"\"\n    Reads input from the terminal until an empty line is received, and creates \n    directories and files according to the input.\n\n    The input should be in the following format:\n    - 'd <directory_name>' to create a directory\n    - 'f <file_name>' to create a file\n\n    The directories and files are created in the current directory.\n\n    Returns:\n    str: The path to the current directory.\n\n    Requirements:\n    - os\n    - sys\n    - csv\n    - pathlib.Path\n\n    Example:\n    >>> path = generate_directory_structure()\n    >>> print(os.listdir(path)) # should print the created directories and files\n    \"\"\"\n    while True:\n        try:\n            line = input()\n            if line == '':\n                break\n            else:\n                command, name = line.split(' ')\n                if command == 'd':\n                    Path(name).mkdir(parents=True, exist_ok=True)\n                elif command == 'f':\n                    Path(name).touch()\n                else:\n                    print(f'Unrecognized command: {command}')\n        except EOFError:\n            break\n\n    return os.getcwd()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nsys, random, hashlib, json\n```python\nimport sys\nimport random\nimport hashlib\nimport json\n\ndef generate_passwords():\n    \"\"\"\n    Reads input from the terminal until an empty line is received, and generates \n    passwords for the given usernames. The passwords are then hashed and returned as a JSON string.\n\n    The input should be a list of usernames.\n\n    Returns:\n    str: The JSON string containing the usernames and their hashed passwords.\n\n    Requirements:\n    - sys\n    - random\n    - hashlib\n    - json\n\n    Example:\n    >>> passwords = generate_passwords()\n    >>> print(passwords) # should print a JSON string\n    \"\"\"\n    usernames = []\n    while True:\n        try:\n            line = input()\n            if line == '':\n                break\n            else:\n                usernames.append(line)\n        except EOFError:\n            break\n\n    passwords = {}\n    for username in usernames:\n        password = ''.join(random.choices('abcdefghijklmnopqrstuvwxyz0123456789', k=10))\n        hashed_password = hashlib.sha256(password.encode()).hexdigest()\n        passwords[username] = hashed_password\n\n    return json.dumps(passwords)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nsys, time, datetime\n```python\nimport sys\nimport time\nfrom datetime import datetime, timedelta\n\ndef print_time_until_new_year():\n    \"\"\"\n    Reads the current date and time from the terminal until an empty line is received, \n    and prints the remaining time until the next New Year.\n\n    The input should be in the format 'yyyy-mm-dd hh:mm:ss'.\n\n    Returns:\n    None.\n\n    Requirements:\n    - sys\n    - time\n    - datetime.datetime\n    - datetime.timedelta\n\n    Example:\n    >>> print_time_until_new_year()\n    \"\"\"\n    while True:\n        try:\n            line = input()\n            if line == '':\n                break\n            else:\n                current_datetime = datetime.strptime(line, '%Y-%m-%d %H:%M:%S')\n                new_year_datetime = datetime(current_datetime.year + 1, 1, 1)\n                remaining_time = new_year_datetime - current_datetime\n                print(f'Time remaining until the next New Year: {remaining_time}')\n        except EOFError:\n            break\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nsys, csv, collections\n```python\nimport sys\nimport csv\nfrom collections import Counter\n\ndef count_word_occurrences():\n    \"\"\"\n    Reads lines from the terminal until an empty line is received, \n    and counts the occurrences of each word. The result is written to a csv file.\n\n    Returns:\n    str: The path to the csv file.\n\n    Requirements:\n    - sys\n    - csv\n    - collections.Counter\n\n    Example:\n    >>> path = count_word_occurrences()\n    >>> pd.read_csv(path)\n    \"\"\"\n    words = []\n    while True:\n        try:\n            line = input()\n            if line == '':\n                break\n            else:\n                words.extend(line.split())\n        except EOFError:\n            break\n\n    word_counts = Counter(words)\n\n    with open('word_counts.csv', 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(['Word', 'Count'])\n        for word, count in word_counts.items():\n            writer.writerow([word, count])\n\n    return 'word_counts.csv'\n```\n"}
{"seed": "{\"task_id\": 37831, \"prompt\": \"def f_37831(model, X, Y):\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"model.fit(X, Y, epochs=200, batch_size=1, verbose=0)\", \"test_start\": \"\\nimport numpy as np\\nfrom tensorflow.keras import *\\nfrom tensorflow.keras.layers import *\\nfrom tensorflow.keras.optimizers import *\\n\\ndef check(candidate):\", \"test\": [\"\\n    model = Sequential([\\n        Dense(input_dim=2, units=1), Activation('sigmoid')\\n    ])\\n    model.compile(loss='binary_crossentropy', optimizer=SGD(lr=0.1))\\n    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\\n    Y = np.array([[0], [1], [1], [1]])\\n\\n    try:\\n        candidate(model, X, Y)\\n    except:\\n        assert False\\n\"], \"entry_point\": \"f_37831\", \"intent\": \"\\u30ed\\u30b0\\u3092\\u51fa\\u529b\\u305b\\u305a\\u306b\\u30e2\\u30c7\\u30eb\\u306e\\u5b66\\u7fd2\\u3092\\u884c\\u3046\", \"library\": [\"numpy\", \"tensorflow\"]}", "generation": "Scenario 1:\nnumpy, tensorflow, matplotlib, sklearn\n```python\nimport numpy as np\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Dense, Activation\nfrom tensorflow.keras.optimizers import SGD\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\ndef build_train_plot_model(X, Y):\n    \"\"\"\n    Split the input data into training and test sets (75% training, 25% test),\n    build a sequential model with one hidden layer and sigmoid activation function,\n    compile the model with binary cross entropy loss and SGD optimizer,\n    fit the model on the training data without verbose output,\n    and plot the model's loss on the training and test sets over the epochs.\n\n    Parameters:\n    X (numpy array): The input data.\n    Y (numpy array): The target data.\n\n    Returns:\n    tensorflow.python.keras.engine.sequential.Sequential: The trained model.\n\n    Requirements:\n    - numpy\n    - tensorflow\n    - sklearn.model_selection\n    - matplotlib.pyplot\n\n    Example:\n    >>> X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n    >>> Y = np.array([[0], [1], [1], [1]])\n    >>> model = build_train_plot_model(X, Y)\n    \"\"\"\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25)\n\n    model = keras.Sequential([Dense(input_dim=2, units=1), Activation('sigmoid')])\n    model.compile(loss='binary_crossentropy', optimizer=SGD(lr=0.1))\n\n    history = model.fit(X_train, Y_train, epochs=200, batch_size=1, verbose=0, validation_data=(X_test, Y_test))\n\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Test'], loc='upper left')\n    plt.show()\n\n    return model\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, tensorflow, pandas, sklearn, seaborn\n```python\nimport numpy as np\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Dense, Activation, Dropout\nfrom tensorflow.keras.optimizers import SGD\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport seaborn as sns\n\ndef build_train_analyze_model(dataframe):\n    \"\"\"\n    Preprocess a pandas DataFrame by standardizing the features,\n    split the data into training and test sets (80% training, 20% test),\n    build a sequential model with one hidden layer, dropout and sigmoid activation function,\n    compile the model with binary cross entropy loss and SGD optimizer,\n    fit the model on the training data without verbose output,\n    and analyze the model's performance by plotting a confusion matrix.\n\n    Parameters:\n    dataframe (pandas DataFrame): The DataFrame containing the input and target data.\n\n    Returns:\n    tensorflow.python.keras.engine.sequential.Sequential: The trained model.\n\n    Requirements:\n    - numpy\n    - tensorflow\n    - pandas\n    - sklearn.preprocessing\n    - sklearn.model_selection\n    - seaborn\n\n    Example:\n    >>> data = pd.DataFrame({'feature1': [0, 0, 1, 1], 'feature2': [0, 1, 0, 1], 'target': [0, 1, 1, 1]})\n    >>> model = build_train_analyze_model(data)\n    \"\"\"\n    X = dataframe.drop('target', axis=1).values\n    Y = dataframe['target'].values.reshape(-1, 1)\n\n    scaler = StandardScaler()\n    X = scaler.fit_transform(X)\n\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n\n    model = keras.Sequential([Dense(input_dim=2, units=1), Dropout(0.2), Activation('sigmoid')])\n    model.compile(loss='binary_crossentropy', optimizer=SGD(lr=0.1))\n\n    model.fit(X_train, Y_train, epochs=200, batch_size=1, verbose=0)\n\n    Y_pred = model.predict_classes(X_test)\n    confusion_matrix = pd.crosstab(Y_test.flatten(), Y_pred.flatten(), rownames=['Actual'], colnames=['Predicted'])\n\n    sns.heatmap(confusion_matrix, annot=True)\n\n    return model\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, tensorflow, sklearn, matplotlib\n```python\nimport numpy as np\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Dense, Activation\nfrom tensorflow.keras.optimizers import SGD\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\n\ndef build_train_roc_curve(X, Y):\n    \"\"\"\n    Split the input data into training and test sets (70% training, 30% test),\n    build a sequential model with one hidden layer and sigmoid activation function,\n    compile the model with binary cross entropy loss and SGD optimizer,\n    fit the model on the training data without verbose output,\n    and plot the ROC curve for the model on the test set.\n\n    Parameters:\n    X (numpy array): The input data.\n    Y (numpy array): The target data.\n\n    Returns:\n    tensorflow.python.keras.engine.sequential.Sequential: The trained model.\n\n    Requirements:\n    - numpy\n    - tensorflow\n    - sklearn.model_selection\n    - sklearn.metrics\n    - matplotlib.pyplot\n\n    Example:\n    >>> X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n    >>> Y = np.array([[0], [1], [1], [1]])\n    >>> model = build_train_roc_curve(X, Y)\n    \"\"\"\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)\n\n    model = keras.Sequential([Dense(input_dim=2, units=1), Activation('sigmoid')])\n    model.compile(loss='binary_crossentropy', optimizer=SGD(lr=0.1))\n\n    model.fit(X_train, Y_train, epochs=200, batch_size=1, verbose=0)\n\n    Y_pred = model.predict(X_test).ravel()\n    fpr, tpr, thresholds = roc_curve(Y_test, Y_pred)\n    auc_score = auc(fpr, tpr)\n\n    plt.figure()\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.plot(fpr, tpr, label='AUC = {:.3f}'.format(auc_score))\n    plt.xlabel('False positive rate')\n    plt.ylabel('True positive rate')\n    plt.title('ROC curve')\n    plt.legend(loc='best')\n    plt.show()\n\n    return model\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, tensorflow, sklearn, matplotlib\n```python\nimport numpy as np\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Dense, Activation\nfrom tensorflow.keras.optimizers import SGD\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_recall_curve\nimport matplotlib.pyplot as plt\n\ndef build_train_pr_curve(X, Y):\n    \"\"\"\n    Split the input data into training and test sets (70% training, 30% test),\n    build a sequential model with one hidden layer and sigmoid activation function,\n    compile the model with binary cross entropy loss and SGD optimizer,\n    fit the model on the training data without verbose output,\n    and plot the Precision-Recall curve for the model on the test set.\n\n    Parameters:\n    X (numpy array): The input data.\n    Y (numpy array): The target data.\n\n    Returns:\n    tensorflow.python.keras.engine.sequential.Sequential: The trained model.\n\n    Requirements:\n    - numpy\n    - tensorflow\n    - sklearn.model_selection\n    - sklearn.metrics\n    - matplotlib.pyplot\n\n    Example:\n    >>> X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n    >>> Y = np.array([[0], [1], [1], [1]])\n    >>> model = build_train_pr_curve(X, Y)\n    \"\"\"\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)\n\n    model = keras.Sequential([Dense(input_dim=2, units=1), Activation('sigmoid')])\n    model.compile(loss='binary_crossentropy', optimizer=SGD(lr=0.1))\n\n    model.fit(X_train, Y_train, epochs=200, batch_size=1, verbose=0)\n\n    Y_pred = model.predict(X_test).ravel()\n    precision, recall, thresholds = precision_recall_curve(Y_test, Y_pred)\n\n    plt.figure()\n    plt.plot(recall, precision, label='Precision-Recall curve')\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title('Precision-Recall curve')\n    plt.legend(loc='best')\n    plt.show()\n\n    return model\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, tensorflow, sklearn, pandas, seaborn\n```python\nimport numpy as np\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Dense, Activation\nfrom tensorflow.keras.optimizers import SGD\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport seaborn as sns\n\ndef build_train_correlation(X, Y):\n    \"\"\"\n    Split the input data into training and test sets (80% training, 20% test),\n    build a sequential model with one hidden layer and sigmoid activation function,\n    compile the model with binary cross entropy loss and SGD optimizer,\n    fit the model on the training data without verbose output,\n    and plot a correlation matrix of the input features and predicted output.\n\n    Parameters:\n    X (numpy array): The input data.\n    Y (numpy array): The target data.\n\n    Returns:\n    tensorflow.python.keras.engine.sequential.Sequential: The trained model.\n\n    Requirements:\n    - numpy\n    - tensorflow\n    - sklearn.model_selection\n    - pandas\n    - seaborn\n\n    Example:\n    >>> X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n    >>> Y = np.array([[0], [1], [1], [1]])\n    >>> model = build_train_correlation(X, Y)\n    \"\"\"\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n\n    model = keras.Sequential([Dense(input_dim=2, units=1), Activation('sigmoid')])\n    model.compile(loss='binary_crossentropy', optimizer=SGD(lr=0.1))\n\n    model.fit(X_train, Y_train, epochs=200, batch_size=1, verbose=0)\n\n    Y_pred = model.predict(X_test)\n    data = np.concatenate((X_test, Y_pred), axis=1)\n    dataframe = pd.DataFrame(data, columns=['feature1', 'feature2', 'prediction'])\n    \n    correlation_matrix = dataframe.corr()\n    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n\n    return model\n```\n"}
{"seed": "{\"task_id\": 53340, \"prompt\": \"def f_53340(str):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"str.encode('utf-8')\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate('mystr') == b'mystr'\\n\"], \"entry_point\": \"f_53340\", \"intent\": \"\\u6587\\u5b57\\u5217`str`\\u3092\\u30d0\\u30a4\\u30c8\\u5217\", \"library\": []}", "generation": "Scenario 1: \nbase64, json, hashlib\n```python\nimport base64\nimport json\nimport hashlib\n\n# Constants\nSECRET_KEY = \"MY_SECRET_KEY\"\n\ndef create_secure_message(data):\n    \"\"\"\n    Create a secure message by encoding the data into a JSON string, encoding \n    it into bytes, and then creating a hash.\n\n    Parameters:\n    data (dict): The data to be encoded.\n\n    Returns:\n    bytes: The encoded and hashed data.\n\n    Requirements:\n    - base64\n    - json\n    - hashlib\n\n    Example:\n    >>> data = {'name': 'John Doe', 'email': 'john.doe@example.com'}\n    >>> secure_message = create_secure_message(data)\n    >>> print(secure_message)\n    \"\"\"\n    json_str = json.dumps(data)\n    bytes_str = json_str.encode('utf-8')\n    hash_object = hashlib.sha256(bytes_str + SECRET_KEY.encode())\n    secure_message = base64.b64encode(hash_object.digest())\n\n    return secure_message\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \nos, sys, hashlib\n```python\nimport os\nimport sys\nimport hashlib\n\n# Constants\nBUFFER_SIZE = 65536\n\ndef calculate_file_hash(file_path):\n    \"\"\"\n    Calculate the SHA256 hash of a file.\n\n    Parameters:\n    file_path (str): The path to the file.\n\n    Returns:\n    str: The SHA256 hash of the file.\n\n    Requirements:\n    - os\n    - sys\n    - hashlib\n\n    Example:\n    >>> file_hash = calculate_file_hash('/path/to/file')\n    >>> print(file_hash)\n    \"\"\"\n    sha256_hash = hashlib.sha256()\n\n    with open(file_path, 'rb') as f:\n        while True:\n            data = f.read(BUFFER_SIZE)\n            if not data:\n                break\n            sha256_hash.update(data)\n\n    return sha256_hash.hexdigest()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nos, glob, time\n```python\nimport os\nimport glob\nimport time\n\n# Constants\nFILE_PATTERN = '*.txt'\n\ndef get_latest_modified_file(directory):\n    \"\"\"\n    Get the latest modified file in a directory that matches a given pattern.\n\n    Parameters:\n    directory (str): The directory to search.\n\n    Returns:\n    str: The path to the latest modified file.\n\n    Requirements:\n    - os\n    - glob\n    - time\n\n    Example:\n    >>> latest_file = get_latest_modified_file('/path/to/directory')\n    >>> print(latest_file)\n    \"\"\"\n    files = glob.glob(os.path.join(directory, FILE_PATTERN))\n    latest_file = max(files, key=os.path.getmtime)\n\n    return latest_file\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \ncsv, random, pandas\n```python\nimport csv\nimport random\nimport pandas as pd\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef create_random_csv(file_path, rows):\n    \"\"\"\n    Create a CSV file with random numeric data.\n\n    Parameters:\n    file_path (str): The path to the file to be created.\n    rows (int): The number of rows of data to generate.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the generated data.\n\n    Requirements:\n    - csv\n    - random\n    - pandas\n\n    Example:\n    >>> df = create_random_csv('/path/to/file.csv', 10)\n    >>> print(df)\n    \"\"\"\n    with open(file_path, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(COLUMNS)\n\n        for i in range(rows):\n            row = [random.random() for _ in range(len(COLUMNS))]\n            writer.writerow(row)\n\n    df = pd.read_csv(file_path)\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nos, shutil, glob\n```python\nimport os\nimport shutil\nimport glob\n\n# Constants\nFILE_PATTERN = '*.txt'\n\ndef move_files(source_directory, target_directory):\n    \"\"\"\n    Move all files that match a given pattern from one directory to another.\n\n    Parameters:\n    source_directory (str): The source directory.\n    target_directory (str): The target directory.\n\n    Returns:\n    list: A list of the paths to the moved files.\n\n    Requirements:\n    - os\n    - shutil\n    - glob\n\n    Example:\n    >>> moved_files = move_files('/path/to/source', '/path/to/target')\n    >>> print(moved_files)\n    \"\"\"\n    files = glob.glob(os.path.join(source_directory, FILE_PATTERN))\n    moved_files = []\n\n    for file in files:\n        shutil.move(file, target_directory)\n        moved_files.append(os.path.join(target_directory, os.path.basename(file)))\n\n    return moved_files\n```"}
{"seed": "{\"task_id\": 45120, \"prompt\": \"def f_45120(n):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"[pd.DataFrame() for i in range(n)]\", \"test_start\": \"\\nimport pandas as pd \\n\\ndef check(candidate):\", \"test\": [\"\\n    df_list = candidate(3)\\n    assert len(df_list) == 3\\n    assert all([type(a)==pd.DataFrame for a in df_list])\\n\"], \"entry_point\": \"f_45120\", \"intent\": \"\\u8981\\u7d20\\u304c\\u30c7\\u30fc\\u30bf\\u30d5\\u30ec\\u30fc\\u30e0\\u3067\\u3001\\u8981\\u7d20\\u6570`n`\\u306e\\u30ea\\u30b9\\u30c8\\u3092\\u4f5c\\u308b\", \"library\": [\"pandas\"]}", "generation": "Scenario 1: \npandas, random, numpy, matplotlib\n```python\nimport pandas as pd\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCITIES = ['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']\nPOPULATION = [8600000, 8900000, 21500000, 9300000, 5100000]\n\ndef generate_population_dataframes(n):\n    \"\"\"\n    Generate a list of n pandas DataFrames. Each DataFrame contains a \n    random selection of cities along with their corresponding population.\n    \n    Parameters:\n    n (int): The number of DataFrames to generate.\n    \n    Returns:\n    list: A list of pandas DataFrames.\n    \n    Requirements:\n    - pandas\n    - random\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df_list = generate_population_dataframes(3)\n    >>> assert len(df_list) == 3\n    >>> assert all([type(a)==pd.DataFrame for a in df_list])\n    \"\"\"\n    df_list = []\n    \n    for i in range(n):\n        random_selection = random.choices(CITIES, k=n)\n        population = [POPULATION[CITIES.index(city)] for city in random_selection]\n        df = pd.DataFrame({'City': random_selection, 'Population': population})\n        df_list.append(df)\n    \n    return df_list\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, scipy.stats, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\n# Constants\nMEAN = 0\nSTD_DEV = 1\n\ndef create_normal_distributions(n):\n    \"\"\"\n    Generate a list of n pandas DataFrames. Each DataFrame contains a \n    normal distribution with a specified mean and standard deviation.\n    \n    Parameters:\n    n (int): The number of DataFrames to generate.\n    \n    Returns:\n    list: A list of pandas DataFrames.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df_list = create_normal_distributions(3)\n    >>> assert len(df_list) == 3\n    >>> assert all([type(a)==pd.DataFrame for a in df_list])\n    \"\"\"\n    df_list = []\n    \n    for i in range(n):\n        data = np.random.normal(MEAN, STD_DEV, 1000)\n        df = pd.DataFrame(data, columns=['Data'])\n        df_list.append(df)\n    \n    return df_list\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, sklearn.preprocessing, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\n\n# Constants\nDATA_RANGE = (0, 100)\n\ndef create_scaled_dataframes(n):\n    \"\"\"\n    Generate a list of n pandas DataFrames. Each DataFrame contains a \n    set of randomly generated data that is then scaled using MinMaxScaler.\n    \n    Parameters:\n    n (int): The number of DataFrames to generate.\n    \n    Returns:\n    list: A list of pandas DataFrames.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.preprocessing\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df_list = create_scaled_dataframes(3)\n    >>> assert len(df_list) == 3\n    >>> assert all([type(a)==pd.DataFrame for a in df_list])\n    \"\"\"\n    scaler = MinMaxScaler()\n    df_list = []\n    \n    for i in range(n):\n        data = np.random.randint(*DATA_RANGE, size=(100, 1))\n        scaled_data = scaler.fit_transform(data)\n        df = pd.DataFrame(scaled_data, columns=['Scaled Data'])\n        df_list.append(df)\n    \n    return df_list\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, seaborn, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nMEAN = 0\nSTD_DEV = 1\n\ndef create_normal_distribution_dataframes(n):\n    \"\"\"\n    Generate a list of n pandas DataFrames. Each DataFrame contains a \n    normal distribution with a mean and standard deviation.\n    \n    Parameters:\n    n (int): The number of DataFrames to generate.\n    \n    Returns:\n    list: A list of pandas DataFrames.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - seaborn\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df_list = create_normal_distribution_dataframes(3)\n    >>> assert len(df_list) == 3\n    >>> assert all([type(a)==pd.DataFrame for a in df_list])\n    \"\"\"\n    df_list = []\n    \n    for i in range(n):\n        data = np.random.normal(MEAN, STD_DEV, 1000)\n        df = pd.DataFrame(data, columns=['Data'])\n        df_list.append(df)\n    \n    return df_list\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, sklearn.datasets, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.datasets import make_classification\nimport matplotlib.pyplot as plt\n\n# Constants\nSAMPLES = 100\nFEATURES = 20\nCLASSES = 2\n\ndef create_classification_dataframes(n):\n    \"\"\"\n    Generate a list of n pandas DataFrames. Each DataFrame contains a \n    classification dataset with a specified number of samples, features, and classes.\n    \n    Parameters:\n    n (int): The number of DataFrames to generate.\n    \n    Returns:\n    list: A list of pandas DataFrames.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.datasets\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df_list = create_classification_dataframes(3)\n    >>> assert len(df_list) == 3\n    >>> assert all([type(a)==pd.DataFrame for a in df_list])\n    \"\"\"\n    df_list = []\n    \n    for i in range(n):\n        data, target = make_classification(n_samples=SAMPLES, n_features=FEATURES, n_classes=CLASSES)\n        df = pd.DataFrame(data)\n        df['Target'] = target\n        df_list.append(df)\n    \n    return df_list\n```\n"}
{"seed": "{\"task_id\": 24987, \"prompt\": \"def f_24987(str):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"re.sub('[\\\\u3000]{2,}', '\\\\u3000', str)\", \"test_start\": \"\\nimport re\\n\\ndef check(candidate):\", \"test\": [\"\\n    text = u\\\"\\u540d\\u524d\\uff11\\u3000\\u3000\\u3000\\u540d\\u524d\\uff12\\u3000\\u3000\\u3000\\u540d\\u524d\\uff13\\u3000\\u540d\\u524d\\uff14\\u3000\\u3000\\u3000\\\"\\n    assert candidate(text) == '\\u540d\\u524d\\uff11\\u3000\\u540d\\u524d\\uff12\\u3000\\u540d\\u524d\\uff13\\u3000\\u540d\\u524d\\uff14\\u3000'\\n\"], \"entry_point\": \"f_24987\", \"intent\": \"\\u6587\\u5b57\\u5217`str`\\u306e\\u4e2d\\u306e\\u9023\\u7d9a\\u3057\\u305f\\u8907\\u6570\\u306e\\u5168\\u89d2\\u7a7a\\u767d\\u30921\\u3064\\u306e\\u5168\\u89d2\\u7a7a\\u767d\\u3067\\u7f6e\\u63db\\u3059\\u308b\", \"library\": [\"re\"]}", "generation": "Scenario 1: \nre, nltk, matplotlib, collections\n```python\nimport re\nimport nltk\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n\n# Constants\nPUNCTUATIONS = \"[.,;:!?]\"\n\ndef plot_word_frequency(text):\n    \"\"\"\n    Remove multiple whitespaces from a text string, tokenize the words, and plot a bar chart of \n    word frequencies (top 10 words) excluding punctuations and stopwords.\n\n    Parameters:\n    text (str): The input text string.\n\n    Returns:\n    None\n\n    Requirements:\n    - re\n    - nltk\n    - matplotlib.pyplot\n    - collections.Counter\n\n    Example:\n    >>> text = \"Hello, world! This   is   a   test message. Hello again!\"\n    >>> plot_word_frequency(text)\n    \"\"\"\n    text = re.sub('\\s{2,}', ' ', text)\n    words = nltk.word_tokenize(text)\n    words = [word for word in words if word not in nltk.corpus.stopwords.words('english') and not re.match(PUNCTUATIONS, word)]\n    word_frequency = Counter(words)\n    top_words = word_frequency.most_common(10)\n    words, frequencies = zip(*top_words)\n    plt.bar(words, frequencies)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nre, os, shutil, mimetypes\n```python\nimport re\nimport os\nimport shutil\nimport mimetypes\n\n# Constants\nPDF_PATTERN = \".*\\.pdf$\"\n\ndef move_pdfs(source_directory, destination_directory):\n    \"\"\"\n    Remove multiple whitespaces from directory names, find all PDF files within a directory \n    (and its subdirectories), and move them to a destination directory.\n\n    Parameters:\n    source_directory (str): The source directory containing the PDF files.\n    destination_directory (str): The destination directory where the PDFs should be moved.\n\n    Returns:\n    list: The list of moved PDF files.\n\n    Requirements:\n    - os\n    - re\n    - shutil\n    - mimetypes\n\n    Example:\n    >>> source_directory = \"./source\"\n    >>> destination_directory = \"./destination\"\n    >>> moved_files = move_pdfs(source_directory, destination_directory)\n    >>> print(moved_files)\n    \"\"\"\n    moved_files = []\n    \n    for root, dirs, files in os.walk(source_directory):\n        for file in files:\n            if re.match(PDF_PATTERN, file) and mimetypes.guess_type(file)[0] == 'application/pdf':\n                source_file = os.path.join(root, file)\n                destination_file = os.path.join(destination_directory, file)\n                shutil.move(source_file, destination_file)\n                moved_files.append(destination_file)\n\n    return moved_files\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nre, requests, bs4, csv\n```python\nimport re\nimport requests\nfrom bs4 import BeautifulSoup\nimport csv\n\n# Constants\nURL_PATTERN = \"https?://[^\\s]+\"\n\ndef scrape_links_write_to_csv(url, csv_file_path):\n    \"\"\"\n    Remove multiple whitespaces from a webpage's HTML, scrape all URLs from that webpage, \n    and write them to a CSV file.\n\n    Parameters:\n    url (str): The webpage URL to scrape.\n    csv_file_path (str): The path to the CSV file to write the URLs.\n\n    Returns:\n    None\n\n    Requirements:\n    - re\n    - requests\n    - bs4.BeautifulSoup\n    - csv\n\n    Example:\n    >>> url = \"https://example.com\"\n    >>> csv_file_path = \"./urls.csv\"\n    >>> scrape_links_write_to_csv(url, csv_file_path)\n    \"\"\"\n    response = requests.get(url)\n    html = response.text\n    html = re.sub('\\s{2,}', ' ', html)\n    soup = BeautifulSoup(html, 'html.parser')\n    links = soup.find_all('a')\n    urls = [link.get('href') for link in links if re.match(URL_PATTERN, link.get('href'))]\n\n    with open(csv_file_path, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerows(urls)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nre, pandas, matplotlib\n```python\nimport re\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nCSV_FILE_PATH = \"./data.csv\"\n\ndef plot_data_from_csv():\n    \"\"\"\n    Remove multiple whitespaces from CSV data, load the data into a pandas DataFrame, \n    and plot a bar chart of the data.\n\n    Returns:\n    None\n\n    Requirements:\n    - re\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_data_from_csv()\n    \"\"\"\n    with open(CSV_FILE_PATH, 'r') as file:\n        data = file.read()\n        data = re.sub('\\s{2,}', ' ', data)\n    \n    with open(CSV_FILE_PATH, 'w') as file:\n        file.write(data)\n    \n    df = pd.read_csv(CSV_FILE_PATH)\n    df.plot(kind='bar')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nre, numpy, matplotlib, sklearn\n```python\nimport re\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n# Constants\nDATA = np.array([\n    [1, 2],\n    [2, 3],\n    [3, 5],\n    [4, 7],\n    [5, 9]\n])\n\ndef linear_regression_plot():\n    \"\"\"\n    Remove multiple whitespaces from a numpy array, perform a linear regression on the data, \n    and plot the data points and the regression line.\n\n    Returns:\n    None\n\n    Requirements:\n    - re\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.linear_model.LinearRegression\n\n    Example:\n    >>> linear_regression_plot()\n    \"\"\"\n    data_str = np.array2string(DATA)\n    data_str = re.sub('\\s{2,}', ' ', data_str)\n    data = np.fromstring(data_str, sep=' ')\n\n    X = data[:, 0].reshape(-1, 1)\n    y = data[:, 1]\n\n    model = LinearRegression()\n    model.fit(X, y)\n\n    plt.scatter(X, y, color='blue')\n    plt.plot(X, model.predict(X), color='red')\n    plt.show()\n```"}
{"seed": "{\"task_id\": 50500, \"prompt\": \"def f_50500(img):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"cv2.imwrite('file.jpg', img)\", \"test_start\": \"\\nimport cv2\\nfrom os import path\\nfrom PIL import Image\\n\\ndef check(candidate):\", \"test\": [\"\\n    im = Image.new('RGBA', (200, 200), (255, 255, 255, 255))\\n    im.save('v.png')\\n    img = cv2.imread('v.png')\\n    candidate(img)\\n    assert path.exists('file.jpg')\\n\"], \"entry_point\": \"f_50500\", \"intent\": \"\\u753b\\u50cf`img`\\u3092\\u30d5\\u30a1\\u30a4\\u30eb\\u540d`file.jpg`\\u3068\\u3057\\u3066\\u4fdd\\u5b58\\u3059\\u308b\", \"library\": [\"PIL\", \"cv2\", \"os\"]}", "generation": "Scenario 1:\nnumpy, cv2, PIL, os, matplotlib\n```python\nimport numpy as np\nimport cv2\nimport os\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\n# Constants\nIMAGE_PATH = 'image.jpg'\nTHRESHOLD = 128\n\ndef binarize_and_save_image(image_path=IMAGE_PATH):\n    \"\"\"\n    Read an image, convert it to grayscale, binarize it using a threshold, and save it as 'binary_image.jpg'.\n    The function also displays the original and binarized images.\n\n    Parameters:\n    image_path (str): The path to the image file.\n\n    Returns:\n    None\n\n    Requirements:\n    - PIL\n    - cv2\n    - numpy\n    - os\n    - matplotlib.pyplot\n\n    Example:\n    >>> binarize_and_save_image('image.jpg')\n    \"\"\"\n    # Read the image\n    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n\n    # Binarize the image\n    binary_img = np.where(img > THRESHOLD, 255, 0).astype('uint8')\n\n    # Save the binary image\n    cv2.imwrite('binary_image.jpg', binary_img)\n\n    # Display the original and binary images\n    plt.subplot(121), plt.imshow(img, cmap='gray'), plt.title('Original')\n    plt.subplot(122), plt.imshow(binary_img, cmap='gray'), plt.title('Binarized')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncv2, numpy, os, matplotlib, sklearn.cluster\n```python\nimport cv2\nimport numpy as np\nimport os\nfrom matplotlib import pyplot as plt\nfrom sklearn.cluster import KMeans\n\n# Constants\nIMAGE_PATH = 'image.jpg'\nN_CLUSTERS = 3\n\ndef segment_image_using_kmeans(image_path=IMAGE_PATH, n_clusters=N_CLUSTERS):\n    \"\"\"\n    Read an image, apply K-means clustering to segment the image into 'n_clusters' regions,\n    and save each region as a separate image.\n\n    Parameters:\n    image_path (str): The path to the image file.\n    n_clusters (int): The number of clusters for K-means clustering.\n\n    Returns:\n    None\n\n    Requirements:\n    - cv2\n    - numpy\n    - os\n    - matplotlib.pyplot\n    - sklearn.cluster\n\n    Example:\n    >>> segment_image_using_kmeans('image.jpg', 3)\n    \"\"\"\n    # Read the image\n    img = cv2.imread(image_path)\n    \n    # Reshape the image to a 2D array of pixels\n    pixels = img.reshape(-1, 3)\n\n    # Apply K-means clustering\n    kmeans = KMeans(n_clusters=n_clusters)\n    kmeans.fit(pixels)\n\n    # Replace each pixel with the color of its closest centroid\n    segmented_image = kmeans.cluster_centers_[kmeans.labels_]\n    segmented_image = segmented_image.reshape(img.shape).astype('uint8')\n\n    # Save each cluster as a separate image\n    for i in range(n_clusters):\n        cluster_img = np.where(kmeans.labels_.reshape(img.shape[:2]) == i, segmented_image, 255).astype('uint8')\n        cv2.imwrite(f'cluster_{i+1}.jpg', cluster_img)\n\n    # Display the original and segmented images\n    plt.subplot(121), plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)), plt.title('Original')\n    plt.subplot(122), plt.imshow(cv2.cvtColor(segmented_image, cv2.COLOR_BGR2RGB)), plt.title('Segmented')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, cv2, os, matplotlib, sklearn.decomposition\n```python\nimport numpy as np\nimport cv2\nimport os\nfrom matplotlib import pyplot as plt\nfrom sklearn.decomposition import PCA\n\n# Constants\nIMAGE_PATH = 'image.jpg'\nN_COMPONENTS = 50\n\ndef apply_pca_to_image(image_path=IMAGE_PATH, n_components=N_COMPONENTS):\n    \"\"\"\n    Read an image, apply PCA to reduce the image dimensionality to 'n_components', and save the result as 'pca_image.jpg'.\n    The function also displays the original and PCA-transformed images.\n\n    Parameters:\n    image_path (str): The path to the image file.\n    n_components (int): The number of components for PCA.\n\n    Returns:\n    None\n\n    Requirements:\n    - cv2\n    - numpy\n    - os\n    - matplotlib.pyplot\n    - sklearn.decomposition\n\n    Example:\n    >>> apply_pca_to_image('image.jpg', 50)\n    \"\"\"\n    # Read the image\n    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n\n    # Apply PCA\n    pca = PCA(n_components=n_components)\n    pca_img = pca.fit_transform(img)\n\n    # Save the PCA image\n    cv2.imwrite('pca_image.jpg', pca_img)\n\n    # Display the original and PCA images\n    plt.subplot(121), plt.imshow(img, cmap='gray'), plt.title('Original')\n    plt.subplot(122), plt.imshow(pca_img, cmap='gray'), plt.title('PCA')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ncv2, numpy, os, matplotlib, PIL\n```python\nimport cv2\nimport numpy as np\nimport os\nfrom matplotlib import pyplot as plt\nfrom PIL import Image\n\n# Constants\nIMAGE_PATH = 'image.jpg'\n\ndef create_histogram_of_image(image_path=IMAGE_PATH):\n    \"\"\"\n    Read an image, create a histogram of the image pixel intensities, and save the histogram as 'histogram.png'.\n    The function also displays the original image and its histogram.\n\n    Parameters:\n    image_path (str): The path to the image file.\n\n    Returns:\n    None\n\n    Requirements:\n    - cv2\n    - numpy\n    - os\n    - matplotlib.pyplot\n    - PIL\n\n    Example:\n    >>> create_histogram_of_image('image.jpg')\n    \"\"\"\n    # Read the image\n    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n\n    # Create a histogram\n    hist = cv2.calcHist([img], [0], None, [256], [0, 256])\n\n    # Save the histogram\n    plt.figure()\n    plt.title(\"Grayscale Histogram\")\n    plt.xlabel(\"Bins\")\n    plt.ylabel(\"# of Pixels\")\n    plt.plot(hist)\n    plt.xlim([0, 256])\n    plt.savefig('histogram.png')\n\n    # Display the original image and its histogram\n    plt.figure()\n    plt.subplot(121), plt.imshow(img, cmap='gray'), plt.title('Original')\n    plt.subplot(122), plt.plot(hist), plt.title('Histogram')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\ncv2, numpy, os, matplotlib, skimage.filters\n```python\nimport cv2\nimport numpy as np\nimport os\nfrom matplotlib import pyplot as plt\nfrom skimage import filters\n\n# Constants\nIMAGE_PATH = 'image.jpg'\n\ndef apply_sobel_filter_to_image(image_path=IMAGE_PATH):\n    \"\"\"\n    Read an image, apply the Sobel filter to detect edges in the image, and save the result as 'edges.jpg'.\n    The function also displays the original image and the edge-detected image.\n\n    Parameters:\n    image_path (str): The path to the image file.\n\n    Returns:\n    None\n\n    Requirements:\n    - cv2\n    - numpy\n    - os\n    - matplotlib.pyplot\n    - skimage.filters\n\n    Example:\n    >>> apply_sobel_filter_to_image('image.jpg')\n    \"\"\"\n    # Read the image\n    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n\n    # Apply the Sobel filter\n    edges = filters.sobel(img)\n\n    # Save the edge-detected image\n    cv2.imwrite('edges.jpg', edges)\n\n    # Display the original image and the edge-detected image\n    plt.subplot(121), plt.imshow(img, cmap='gray'), plt.title('Original')\n    plt.subplot(122), plt.imshow(edges, cmap='gray'), plt.title('Edges')\n    plt.show()\n```\n"}
{"seed": "{\"task_id\": 41336, \"prompt\": \"def f_41336(a, b):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"np.dot(a, b)\", \"test_start\": \"\\nimport numpy as np\\n\\ndef check(candidate):\", \"test\": [\"\\n    arr = np.array([[1,3,-5]])\\n    arr_1 = np.array([[4],[-2],[-1]])\\n    assert candidate(np.mat(arr),np.mat(arr_1)) == 3\\n\", \"\\n    assert np.array_equal(candidate(np.zeros(shape=(5,2)), np.zeros(shape=(2,5))), np.zeros(shape=(5,5)))\\n\", \"\\n    assert np.array_equal(candidate(np.array([[4]]), np.array([[0]])), np.array([[0]]))\\n\", \"\\n    assert candidate(3,4) == 12\\n\"], \"entry_point\": \"f_41336\", \"intent\": \"\\u884c\\u5217`a`\\u3068`b`\\u306e\\u7a4d\\u3092\\u8a08\\u7b97\\u3059\\u308b\", \"library\": [\"numpy\"]}", "generation": "Scenario 1:\nnumpy, matplotlib, scipy.signal\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.signal import welch\n\ndef plot_spectral_density(signal, fs):\n    \"\"\"\n    Given a time series signal, compute and plot its Power Spectral Density (PSD)\n    using the Welch's method.\n    \n    Parameters:\n    signal (numpy.array): The input time series signal.\n    fs (int): The sampling frequency of the signal.\n\n    Returns: \n    None. This function plots the PSD of the given signal.\n\n    Requirements:\n    - numpy\n    - matplotlib\n    - scipy.signal\n\n    Example:\n    >>> signal = np.random.normal(size=1000)\n    >>> plot_spectral_density(signal, fs=100)\n    \"\"\"\n    freqs, psd = welch(signal, fs)\n    plt.semilogx(freqs, psd)\n    plt.title('Power spectral density')\n    plt.xlabel('Frequency')\n    plt.ylabel('Power')\n    plt.grid(True)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, scipy.sparse.linalg, matplotlib\n```python\nimport numpy as np\nfrom scipy.sparse.linalg import svds\nimport matplotlib.pyplot as plt\n\ndef plot_singular_values(matrix, k):\n    \"\"\"\n    Compute the singular values of a given matrix using the singular value decomposition (SVD)\n    and plot the 'k' largest singular values.\n\n    Parameters:\n    matrix (numpy.array): The input matrix.\n    k (int): The number of largest singular values to compute and plot.\n\n    Returns: \n    None. This function plots the 'k' largest singular values of the given matrix.\n\n    Requirements:\n    - numpy\n    - scipy.sparse.linalg\n    - matplotlib\n\n    Example:\n    >>> matrix = np.random.normal(size=(100, 50))\n    >>> plot_singular_values(matrix, k=10)\n    \"\"\"\n    singular_values = svds(matrix, k=k)[1]\n    plt.plot(np.sort(singular_values)[::-1])\n    plt.title('Largest singular values')\n    plt.xlabel('Index')\n    plt.ylabel('Singular value')\n    plt.grid(True)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, scipy.ndimage, matplotlib\n```python\nimport numpy as np\nfrom scipy.ndimage import gaussian_filter\nimport matplotlib.pyplot as plt\n\ndef visualize_smoothed_image(image, sigma):\n    \"\"\"\n    Apply a Gaussian filter to smooth an image and visualize the result.\n\n    Parameters:\n    image (numpy.array): The input image.\n    sigma (float): The standard deviation for the Gaussian kernel.\n\n    Returns: \n    None. This function visualizes the smoothed image.\n\n    Requirements:\n    - numpy\n    - scipy.ndimage\n    - matplotlib\n\n    Example:\n    >>> image = np.random.normal(size=(100, 100))\n    >>> visualize_smoothed_image(image, sigma=1.0)\n    \"\"\"\n    smoothed_image = gaussian_filter(image, sigma=sigma)\n    plt.imshow(smoothed_image, cmap='gray')\n    plt.title('Smoothed image')\n    plt.xlabel('X')\n    plt.ylabel('Y')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, scipy.stats, seaborn\n```python\nimport numpy as np\nfrom scipy.stats import norm\nimport seaborn as sns\n\ndef plot_normal_distribution(mean, std_dev):\n    \"\"\"\n    Plot a normal distribution with a given mean and standard deviation.\n\n    Parameters:\n    mean (float): The mean of the normal distribution.\n    std_dev (float): The standard deviation of the normal distribution.\n\n    Returns: \n    None. This function plots the normal distribution.\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - seaborn\n\n    Example:\n    >>> plot_normal_distribution(mean=0, std_dev=1)\n    \"\"\"\n    x = np.linspace(norm.ppf(0.01, loc=mean, scale=std_dev), norm.ppf(0.99, loc=mean, scale=std_dev), 100)\n    y = norm.pdf(x, loc=mean, scale=std_dev)\n\n    sns.lineplot(x, y)\n    plt.title('Normal distribution')\n    plt.xlabel('Value')\n    plt.ylabel('Probability density')\n    plt.grid(True)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, scipy.optimize, matplotlib\n```python\nimport numpy as np\nfrom scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\n\ndef fit_and_plot_exponential(data):\n    \"\"\"\n    Fit an exponential function to the given data and plot the original data along with the fitted curve.\n\n    Parameters:\n    data (numpy.array): The input data.\n\n    Returns: \n    None. This function plots the original data and the fitted curve.\n\n    Requirements:\n    - numpy\n    - scipy.optimize\n    - matplotlib\n\n    Example:\n    >>> data = np.random.exponential(scale=1.0, size=100)\n    >>> fit_and_plot_exponential(data)\n    \"\"\"\n    def exponential_func(x, a, b, c):\n        return a * np.exp(-b * x) + c\n\n    x = np.arange(len(data))\n    popt, pcov = curve_fit(exponential_func, x, data, p0=(1, 0.01, 1))\n\n    plt.plot(x, data, 'ko', label=\"Original data\")\n    plt.plot(x, exponential_func(x, *popt), 'r-', label=\"Fitted curve\")\n    plt.title('Exponential fit')\n    plt.xlabel('X')\n    plt.ylabel('Y')\n    plt.legend()\n    plt.show()\n```"}
{"seed": "{\"task_id\": 48742, \"prompt\": \"def f_48742():\\n\\t\", \"suffix\": \"\\n\\treturn \", \"canonical_solution\": \"df = pd.read_table('file.txt', header=None, delim_whitespace=True)\\n\\tdf.to_csv('new_file.csv', index=False, header=False)\", \"test_start\": \"\\nimport csv\\nimport pandas as pd\\n\\ndef check(candidate):\", \"test\": [\"\\n    with open(\\\"file.txt\\\", \\\"w\\\") as text_file:\\n        text_file.write('''col1 col2 col3\\n''')\\n        text_file.write('''1 2 3\\n''')\\n    \\n    candidate()\\n    import csv\\n    file = open('new_file.csv')\\n    csvreader = csv.reader(file)\\n    rows = []\\n    for row in csvreader:\\n            rows.append(row)\\n    \\n    assert rows[0] == ['col1', 'col2', 'col3'] and rows[1] == ['1', '2', '3']\\n\", \"\\n    with open(\\\"file.txt\\\", \\\"w\\\") as text_file:\\n        text_file.write('''col1\\n''')\\n        text_file.write('''1\\n''')\\n        text_file.write('''2\\n''')\\n    \\n    candidate()\\n    import csv\\n    file = open('new_file.csv')\\n    csvreader = csv.reader(file)\\n    rows = []\\n    for row in csvreader:\\n            rows.append(row)\\n    \\n    assert rows[0] == ['col1'] and rows[1] == ['1'] and rows[2] == ['2']\\n\", \"\\n    with open(\\\"file.txt\\\", \\\"w\\\") as text_file:\\n        text_file.write('''col1    col2   col3  \\n''')\\n        text_file.write('''  1   2  3 \\n''')\\n    \\n    candidate()\\n    import csv\\n    file = open('new_file.csv')\\n    csvreader = csv.reader(file)\\n    rows = []\\n    for row in csvreader:\\n            rows.append(row)\\n    \\n    assert rows[0] == ['col1', 'col2', 'col3'] and rows[1] == ['1', '2', '3']\\n\"], \"entry_point\": \"f_48742\", \"intent\": \"\\u7a7a\\u767d\\u533a\\u5207\\u308a\\u306e\\u30c6\\u30ad\\u30b9\\u30c8\\u30d5\\u30a1\\u30a4\\u30eb`file.txt`\\u3092\\u30b3\\u30f3\\u30de\\u533a\\u5207\\u308a\\u306ecsv\\u30d5\\u30a1\\u30a4\\u30eb`new_file.csv`\\u306b\\u5909\\u63db\\u3059\\u308b\", \"library\": [\"csv\", \"pandas\"]}", "generation": "Scenario 1:\npandas, csv, os, glob\n```python\nimport pandas as pd\nimport csv\nimport os\nimport glob\n\ndef convert_all_txt_to_csv(directory):\n    \"\"\"\n    Convert all text files in a directory to CSV files. \n    The text files are assumed to be tab-delimited. \n\n    Parameters:\n    directory (str): The directory containing the text files.\n\n    Requirements:\n    - pandas\n    - csv\n    - os\n    - glob\n\n    Example:\n    >>> convert_all_txt_to_csv('text_files_directory')\n    \"\"\"\n    text_files = glob.glob(os.path.join(directory, '*.txt'))\n\n    for text_file in text_files:\n        df = pd.read_table(text_file, header=None, delim_whitespace=True)\n        csv_file = os.path.splitext(text_file)[0] + '.csv'\n        df.to_csv(csv_file, index=False, header=False)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, csv, random, numpy\n```python\nimport pandas as pd\nimport csv\nimport random\nimport numpy as np\n\n# Constants\nNUM_ROWS = 100\nNUM_COLS = 50\nMIN_VAL = 1\nMAX_VAL = 100\n\ndef create_random_csv(filename):\n    \"\"\"\n    Create a CSV file with random integer values.\n\n    Parameters:\n    filename (str): The name of the CSV file to be created.\n\n    Requirements:\n    - pandas\n    - csv\n    - random\n    - numpy\n\n    Example:\n    >>> create_random_csv('random_data.csv')\n    \"\"\"\n    data = np.random.randint(MIN_VAL, MAX_VAL, size=(NUM_ROWS, NUM_COLS))\n    df = pd.DataFrame(data)\n    df.to_csv(filename, index=False, header=False)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, csv, os, sys\n```python\nimport pandas as pd\nimport csv\nimport os\nimport sys\n\ndef convert_csv_to_txt(filename):\n    \"\"\"\n    Convert a CSV file to a tab-delimited text file.\n\n    Parameters:\n    filename (str): The name of the CSV file.\n\n    Requirements:\n    - pandas\n    - csv\n    - os\n    - sys\n\n    Example:\n    >>> convert_csv_to_txt('data.csv')\n    \"\"\"\n    if not filename.endswith('.csv'):\n        print(f\"{filename} is not a CSV file.\", file=sys.stderr)\n        return\n\n    df = pd.read_csv(filename, header=None)\n    txt_filename = os.path.splitext(filename)[0] + '.txt'\n    df.to_csv(txt_filename, index=False, header=False, sep='\\t')\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, csv, matplotlib, seaborn\n```python\nimport pandas as pd\nimport csv\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef plot_csv_data(filename):\n    \"\"\"\n    Plot the data from a CSV file. Assumes the CSV file has a header row.\n\n    Parameters:\n    filename (str): The name of the CSV file.\n\n    Requirements:\n    - pandas\n    - csv\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> plot_csv_data('data.csv')\n    \"\"\"\n    df = pd.read_csv(filename)\n    sns.pairplot(df)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, csv, numpy, sklearn.preprocessing\n```python\nimport pandas as pd\nimport csv\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\n\ndef standardize_csv(filename):\n    \"\"\"\n    Standardize the data in a CSV file and save it to a new CSV file.\n\n    Parameters:\n    filename (str): The name of the CSV file.\n\n    Requirements:\n    - pandas\n    - csv\n    - numpy\n    - sklearn.preprocessing\n\n    Example:\n    >>> standardize_csv('data.csv')\n    \"\"\"\n    df = pd.read_csv(filename, header=None)\n    scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(df.values)\n    new_filename = \"standardized_\" + filename\n    pd.DataFrame(standardized_data).to_csv(new_filename, index=False, header=False)\n```\n`GPT_ODEX_BREAK`"}
{"seed": "{\"task_id\": 35271, \"prompt\": \"def f_35271(n):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"str(n)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(0) == '0'\\n\", \"\\n    assert candidate(34.12) == '34.12'\\n\", \"\\n    assert candidate(-1) == '-1'\\n\", \"\\n    assert candidate(float('inf')) == 'inf'\\n\", \"\\n    assert candidate(123412) == '123412'\\n\"], \"entry_point\": \"f_35271\", \"intent\": \"\\u6570\\u5024`n`\\u3092\\u6587\\u5b57\\u5217\\u306b\\u5909\\u63db\\u3059\\u308b\", \"library\": []}", "generation": "Scenario 1:\nrandom, string, hashlib, binascii\n```python\nimport random\nimport string\nimport hashlib\nimport binascii\n\n# Constants\nSALT_LENGTH = 64\nHASH_ALGORITHM = 'sha256'\n\ndef hash_password(password):\n    \"\"\"\n    Generate a salt and hash a password using a specified hash algorithm with the salt.\n    \n    Parameters:\n    password (str): The password to be hashed.\n    \n    Returns:\n    tuple: A tuple containing the salt and the hashed password.\n    \n    Requirements:\n    - random\n    - string\n    - hashlib\n    - binascii\n    \n    Example:\n    >>> salt, hashed_password = hash_password('my_secure_password')\n    >>> print(salt, hashed_password)\n    \"\"\"\n    salt = ''.join(random.choice(string.ascii_letters + string.digits) for _ in range(SALT_LENGTH))\n    hashed_password = hashlib.pbkdf2_hmac(HASH_ALGORITHM, password.encode(), salt.encode(), 100000)\n    \n    return salt, binascii.hexlify(hashed_password).decode()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nrandom, matplotlib, math\n```python\nimport random\nimport matplotlib.pyplot as plt\nimport math\n\n# Constants\nSAMPLE_SIZE = 1000\nBIN_SIZE = 50\n\ndef plot_normal_distribution(mu, sigma):\n    \"\"\"\n    Generate a sample of data from a normal distribution with given mean \n    and standard deviation, then plot the histogram of the data.\n    \n    Parameters:\n    mu (float): The mean of the normal distribution.\n    sigma (float): The standard deviation of the normal distribution.\n    \n    Requirements:\n    - random\n    - matplotlib.pyplot\n    - math\n    \n    Example:\n    >>> plot_normal_distribution(0, 1)\n    \"\"\"\n    sample = [random.gauss(mu, sigma) for _ in range(SAMPLE_SIZE)]\n    plt.hist(sample, BIN_SIZE, density=True)\n\n    x = plt.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n    plt.plot(x, plt.mlab.normpdf(x, mu, sigma))\n    plt.show()\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 3:\nos, glob, shutil\n```python\nimport os\nimport glob\nimport shutil\n\n# Constants\nFILE_EXTENSIONS = ['.txt', '.csv', '.json']\n\ndef move_files(src_dir, dest_dir):\n    \"\"\"\n    Move all files with certain extensions from source directory to destination directory.\n    \n    Parameters:\n    src_dir (str): The source directory.\n    dest_dir (str): The destination directory.\n    \n    Requirements:\n    - os\n    - glob\n    - shutil\n    \n    Example:\n    >>> move_files('/path/to/src', '/path/to/dest')\n    \"\"\"\n    files_moved = 0\n    for ext in FILE_EXTENSIONS:\n        files = glob.glob(os.path.join(src_dir, '*' + ext))\n        for file in files:\n            shutil.move(file, dest_dir)\n            files_moved += 1\n\n    return files_moved\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, pandas, sklearn\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\n# Constants\nFEATURES = ['feature1', 'feature2', 'feature3']\nTARGET = 'target'\n\ndef predict_value(df, new_data):\n    \"\"\"\n    Fit a linear regression model on a dataframe using certain columns as features \n    and a column as target, then predict the target value for new data.\n    \n    Parameters:\n    df (DataFrame): The dataframe containing the training data.\n    new_data (dict): The new data for which to predict the target value.\n    \n    Returns:\n    float: The predicted target value.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.linear_model.LinearRegression\n    \n    Example:\n    >>> df = pd.DataFrame({'feature1': [1,2,3], 'feature2': [2,3,4], 'feature3': [3,4,5], 'target': [4,5,6]})\n    >>> new_data = {'feature1': 4, 'feature2': 5, 'feature3': 6}\n    >>> predict_value(df, new_data)\n    \"\"\"\n    model = LinearRegression()\n    model.fit(df[FEATURES], df[TARGET])\n    \n    return model.predict(np.array([new_data[feat] for feat in FEATURES]).reshape(1, -1))[0]\n```\n\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, scipy.stats, matplotlib\n```python\nimport numpy as np\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\n\n# Constants\nN_POINTS = 100\n\ndef plot_beta_distribution(alpha, beta):\n    \"\"\"\n    Plot the PDF of a Beta distribution with given alpha and beta.\n    \n    Parameters:\n    alpha (float): The alpha parameter of the Beta distribution.\n    beta (float): The beta parameter of the Beta distribution.\n    \n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n    \n    Example:\n    >>> plot_beta_distribution(2, 5)\n    \"\"\"\n    x = np.linspace(0, 1, N_POINTS)\n    y = stats.beta.pdf(x, alpha, beta)\n    \n    plt.plot(x, y)\n    plt.show()\n```"}
{"seed": "{\"task_id\": 20048, \"prompt\": \"def f_20048(li):\\n\\t\", \"suffix\": \"\\n\\treturn result\", \"canonical_solution\": \"\\n\\tc = Counter(tuple(x) for x in li)\\n\\tresult = [list(k) for k,v in c.items() if v >=2]\\n\", \"test_start\": \"\\nfrom collections import Counter\\n\\ndef check(candidate):\", \"test\": [\"\\n    li=[[1,2,3],[5,6,7],[2,3,4,5],[1,2,3],[7,8,9],[2,3,4,5],[1,2,3],[5,6,7]]\\n    assert candidate(li) == [[1, 2, 3], [5, 6, 7], [2, 3, 4, 5]]\\n\"], \"entry_point\": \"f_20048\", \"intent\": \"2\\u6b21\\u5143\\u30ea\\u30b9\\u30c8`li`\\u5185\\u306e\\u91cd\\u8907\\u3057\\u3066\\u3044\\u308b\\u8981\\u7d20\\u3092\\u53d6\\u308a\\u51fa\\u3059\", \"library\": [\"collections\"]}", "generation": "Scenario 1:\npandas, collections, random, matplotlib\n```python\nimport pandas as pd\nfrom collections import Counter\nfrom random import sample\nimport matplotlib.pyplot as plt\n\n# Constants\nELEMENTS = list(range(1, 101))\n\ndef find_repeat_elements(num_lists, list_size):\n    \"\"\"\n    Generate a certain number of lists with random elements from a given range\n    and find the elements that repeat in these lists. Display a histogram of these\n    repeating elements.\n    \n    Parameters:\n    num_lists (int): The number of lists to generate.\n    list_size (int): The size of each list.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with repeating elements and their counts.\n    \n    Requirements:\n    - pandas\n    - collections.Counter\n    - random.sample\n    - matplotlib.pyplot\n    \n    Example:\n    >>> report = find_repeat_elements(5, 10)\n    >>> print(report)\n    >>> report['Count'].plot(kind='bar')\n    \"\"\"\n    lists = [sample(ELEMENTS, list_size) for _ in range(num_lists)]\n    flat_list = [item for sublist in lists for item in sublist]\n    counter = Counter(flat_list)\n    \n    repeat_elements = {k: v for k, v in counter.items() if v > 1}\n    \n    report_df = pd.DataFrame.from_records(list(repeat_elements.items()), columns=['Element', 'Count'])\n\n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, collections, itertools, matplotlib\n```python\nimport numpy as np\nfrom collections import Counter\nimport itertools\nimport matplotlib.pyplot as plt\n\n# Constants\nELEMENTS = np.array([1, 2, 3, 4, 5])\n\ndef generate_permutations_and_find_duplicates(n):\n    \"\"\"\n    Generate all possible permutations of n elements from a predefined list,\n    count the occurrences of each permutation and find the duplicate ones.\n    Plot a histogram of the counts of duplicate permutations.\n    \n    Parameters:\n    n (int): The number of elements to use in each permutation.\n    \n    Returns:\n    dict: A dictionary with duplicate permutations and their counts.\n    \n    Requirements:\n    - numpy\n    - collections.Counter\n    - itertools.permutations\n    - matplotlib.pyplot\n    \n    Example:\n    >>> duplicates = generate_permutations_and_find_duplicates(3)\n    >>> print(duplicates)\n    >>> plt.bar(range(len(duplicates)), list(duplicates.values()), align='center')\n    \"\"\"\n    permutations = list(itertools.permutations(ELEMENTS, n))\n    counter = Counter(permutations)\n    \n    duplicates = {k: v for k, v in counter.items() if v > 1}\n    \n    return duplicates\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, collections, random, matplotlib\n```python\nimport numpy as np\nfrom collections import Counter\nfrom random import choice\nimport matplotlib.pyplot as plt\n\n# Constants\nELEMENTS = np.array([1, 2, 3, 4, 5])\n\ndef generate_sequences_and_find_duplicates(n, seq_length):\n    \"\"\"\n    Generate n sequences of a certain length with random elements from a predefined list,\n    count the occurrences of each sequence and find the duplicate ones.\n    Plot a histogram of the counts of duplicate sequences.\n    \n    Parameters:\n    n (int): The number of sequences to generate.\n    seq_length (int): The length of each sequence.\n    \n    Returns:\n    dict: A dictionary with duplicate sequences and their counts.\n    \n    Requirements:\n    - numpy\n    - collections.Counter\n    - random.choice\n    - matplotlib.pyplot\n    \n    Example:\n    >>> duplicates = generate_sequences_and_find_duplicates(1000, 4)\n    >>> print(duplicates)\n    >>> plt.bar(range(len(duplicates)), list(duplicates.values()), align='center')\n    \"\"\"\n    sequences = [tuple(choice(ELEMENTS) for _ in range(seq_length)) for _ in range(n)]\n    counter = Counter(sequences)\n    \n    duplicates = {k: v for k, v in counter.items() if v > 1}\n    \n    return duplicates\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, pandas, collections, random, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nfrom collections import Counter\nfrom random import choice\nimport matplotlib.pyplot as plt\n\n# Constants\nELEMENTS = np.array([1, 2, 3, 4, 5])\n\ndef generate_data_and_find_duplicates(n, column_names):\n    \"\"\"\n    Generate a pandas DataFrame with n rows and random elements from a predefined list\n    for each column. Find the duplicate rows and display a bar chart of their counts.\n    \n    Parameters:\n    n (int): The number of rows in the DataFrame.\n    column_names (list): The names of the columns in the DataFrame.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with duplicate rows and their counts.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - collections.Counter\n    - random.choice\n    - matplotlib.pyplot\n    \n    Example:\n    >>> duplicates = generate_data_and_find_duplicates(1000, ['A', 'B', 'C'])\n    >>> print(duplicates)\n    >>> duplicates['Count'].plot(kind='bar')\n    \"\"\"\n    data = pd.DataFrame({col: [choice(ELEMENTS) for _ in range(n)] for col in column_names})\n    \n    counter = Counter([tuple(x) for x in data.values])\n    duplicates = {k: v for k, v in counter.items() if v > 1}\n    \n    duplicates_df = pd.DataFrame.from_records(list(duplicates.items()), columns=column_names + ['Count'])\n\n    return duplicates_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, collections, itertools, matplotlib\n```python\nimport numpy as np\nfrom collections import Counter\nimport itertools\nimport matplotlib.pyplot as plt\n\n# Constants\nELEMENTS = np.array([1, 2, 3, 4, 5])\n\ndef generate_combinations_and_find_duplicates(n):\n    \"\"\"\n    Generate all possible combinations of n elements from a predefined list,\n    count the occurrences of each combination and find the duplicate ones.\n    Plot a histogram of the counts of duplicate combinations.\n    \n    Parameters:\n    n (int): The number of elements to use in each combination.\n    \n    Returns:\n    dict: A dictionary with duplicate combinations and their counts.\n    \n    Requirements:\n    - numpy\n    - collections.Counter\n    - itertools.combinations\n    - matplotlib.pyplot\n    \n    Example:\n    >>> duplicates = generate_combinations_and_find_duplicates(3)\n    >>> print(duplicates)\n    >>> plt.bar(range(len(duplicates)), list(duplicates.values()), align='center')\n    \"\"\"\n    combinations = list(itertools.combinations(ELEMENTS, n))\n    counter = Counter(combinations)\n    \n    duplicates = {k: v for k, v in counter.items() if v > 1}\n    \n    return duplicates\n```\nEnd of the scenarios."}
{"seed": "{\"task_id\": 21070, \"prompt\": \"def f_21070(li, li2):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"list(filter(lambda x:x not in li2, li))\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([1, 2, 3], [1, 3, 4]) == [2]\\n\", \"\\n    assert candidate(['abc', 'def'], ['abc']) == ['def']\\n\"], \"entry_point\": \"f_21070\", \"intent\": \"2\\u3064\\u306e\\u30ea\\u30b9\\u30c8`li`\\u3068`li2`\\u3092\\u6bd4\\u8f03\\u3057\\u3001\\u91cd\\u8907\\u3057\\u3066\\u3044\\u308b\\u8981\\u7d20\\u3092\\u524a\\u9664\\u3059\\u308b\", \"library\": []}", "generation": "Scenario 1:\nnumpy, pandas, random, collections\n```python\nimport numpy as np\nimport pandas as pd\nfrom random import choice\nfrom collections import Counter\n\n# Constants\nLETTERS = ['a', 'b', 'c', 'd', 'e', 'f', 'g']\n\ndef generate_random_df(list1, list2):\n    \"\"\"\n    Generate a random DataFrame from two lists, count the frequencies of items\n    in each column, and return a DataFrame of these frequencies.\n\n    Parameters:\n    list1 (list): List of items.\n    list2 (list): List of items.\n\n    Returns:\n    DataFrame: A pandas DataFrame with frequencies of items in each column.\n\n    Requirements:\n    - numpy\n    - pandas\n    - random\n    - collections.Counter\n\n    Example:\n    >>> list1 = ['a', 'b', 'c']\n    >>> list2 = ['a', 'b', 'c', 'd']\n    >>> df = generate_random_df(list1, list2)\n    >>> print(df)\n    \"\"\"\n    df = pd.DataFrame({\n        'Column1': [choice(list1) for _ in range(1000)],\n        'Column2': [choice(list2) for _ in range(1000)]\n    })\n\n    freq_df = pd.DataFrame({\n        'Column1': pd.Series(dict(Counter(df['Column1']))),\n        'Column2': pd.Series(dict(Counter(df['Column2'])))\n    })\n\n    return freq_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, pandas, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef plot_diff_between_lists(list1, list2):\n    \"\"\"\n    Generate a bar plot showing the difference in count of elements in two lists.\n\n    Parameters:\n    list1 (list): List of items.\n    list2 (list): List of items.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> list1 = ['a', 'b', 'c', 'a', 'b', 'c', 'a']\n    >>> list2 = ['a', 'b', 'c', 'a', 'b', 'c']\n    >>> plot_diff_between_lists(list1, list2)\n    \"\"\"\n    df1 = pd.DataFrame(list1, columns=['Item'])\n    df2 = pd.DataFrame(list2, columns=['Item'])\n\n    diff_df = df1['Item'].value_counts() - df2['Item'].value_counts()\n    diff_df.dropna(inplace=True)\n\n    diff_df.plot(kind='bar')\n\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, matplotlib, random\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import choice\n\n# Constants\nCOLORS = ['r', 'g', 'b', 'c', 'm', 'y', 'k']\n\ndef scatter_plot_difference(list1, list2):\n    \"\"\"\n    Generate a scatter plot showing the difference in values between two lists.\n\n    Parameters:\n    list1 (list): List of numeric values.\n    list2 (list): List of numeric values.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - random.choice\n\n    Example:\n    >>> list1 = [1, 2, 3, 4, 5]\n    >>> list2 = [2, 3, 4, 5, 6]\n    >>> scatter_plot_difference(list1, list2)\n    \"\"\"\n    diff_list = np.array(list1) - np.array(list2)\n    plt.scatter(range(len(diff_list)), diff_list, c=choice(COLORS))\n\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, itertools, matplotlib\n```python\nimport numpy as np\nfrom itertools import product\nimport matplotlib.pyplot as plt\n\ndef plot_2d_func(list1, list2):\n    \"\"\"\n    Generate a 2D plot of a function with inputs from two lists.\n\n    Parameters:\n    list1 (list): List of x-values.\n    list2 (list): List of y-values.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - itertools.product\n    - matplotlib.pyplot\n\n    Example:\n    >>> list1 = [1, 2, 3]\n    >>> list2 = [4, 5, 6]\n    >>> plot_2d_func(list1, list2)\n    \"\"\"\n    x = np.array(list1)\n    y = np.array(list2)\n\n    for (i, j) in product(range(len(x)), range(len(y))):\n        plt.plot(x[i], y[j], 'o')\n\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, pandas, seaborn\n```python\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\ndef plot_density_difference(list1, list2):\n    \"\"\"\n    Generate a density plot showing the difference in distribution between two lists.\n\n    Parameters:\n    list1 (list): List of numeric values.\n    list2 (list): List of numeric values.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - pandas\n    - seaborn\n\n    Example:\n    >>> list1 = [1, 2, 3, 4, 5]\n    >>> list2 = [2, 3, 4, 5, 6]\n    >>> plot_density_difference(list1, list2)\n    \"\"\"\n    df = pd.DataFrame({\n        'List1': list1,\n        'List2': list2\n    })\n\n    sns.kdeplot(data=df)\n\n    plt.show()\n```"}
{"seed": "{\"task_id\": 19098, \"prompt\": \"def f_19098(li):\\n\\t\", \"suffix\": \"\\n\\treturn result\", \"canonical_solution\": \"\\n\\tresult=[]\\n\\tli.sort()\\n\\tM = len(li) - 1\\n\\tfor i, e in enumerate(li):\\n\\t\\tj = i + 1\\n\\t\\tk = M\\n\\t\\twhile k > j:\\n\\t\\t\\ts = li[i] + li[j] + li[k]\\n\\t\\t\\tif s == 0:\\n\\t\\t\\t\\tresult.append([li[i], li[j], li[k]])\\n\\t\\t\\t\\tk -= 1\\n\\t\\t\\telif s > 0:\\n\\t\\t\\t\\tk -= 1\\n\\t\\t\\telse:\\n\\t\\t\\t\\tj += 1\\n\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    li = [1, -1, 0, 2]\\n    assert len(candidate(li)) == 1\\n\"], \"entry_point\": \"f_19098\", \"intent\": \"\\u6574\\u6570\\u3092\\u8981\\u7d20\\u306b\\u6301\\u3064\\u30ea\\u30b9\\u30c8`li'\\u304b\\u3089\\u3001\\u5408\\u8a08\\u3059\\u308b\\u30680\\u306b\\u306a\\u308b3\\u3064\\u306e\\u6574\\u6570\\u3092\\u6c42\\u3081\\u308b\", \"library\": []}", "generation": "Scenario 1:\npandas, itertools, numpy, matplotlib\n```python\nimport pandas as pd\nfrom itertools import combinations\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nN_NUMBERS = 3\nSUM_TARGET = 0\n\ndef find_triplets(data):\n    \"\"\"\n    Find triplets in the given pandas DataFrame column whose sum equals to zero and plot a \n    histogram of the sum of all combinations.\n\n    Parameters:\n    data (DataFrame): The pandas DataFrame containing the column 'Numbers'.\n\n    Returns:\n    list: The list of triplets.\n\n    Requirements:\n    - pandas\n    - itertools\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame({'Numbers': [1, -1, 0, 2, 3, -3, 4, -4]})\n    >>> find_triplets(df)\n    \"\"\"\n    numbers = data['Numbers'].values\n    all_combinations = list(combinations(numbers, N_NUMBERS))\n    sum_combinations = np.array([sum(comb) for comb in all_combinations])\n    triplets = [comb for comb in all_combinations if sum(comb) == SUM_TARGET]\n\n    plt.hist(sum_combinations, bins=20, edgecolor='black')\n    plt.xlabel('Sum')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of the Sum of all Combinations')\n    plt.show()\n\n    return triplets\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, itertools, random, matplotlib\n```python\nimport numpy as np\nfrom itertools import combinations\nimport random\nimport matplotlib.pyplot as plt\n\n# Constants\nN_NUMBERS = 3\nSUM_TARGET = 0\nDATA_SIZE = 100\nDATA_MIN = -50\nDATA_MAX = 50\n\ndef find_triplets_and_plot():\n    \"\"\"\n    Generate a list of random integers, find triplets whose sum equals to zero, and plot a \n    histogram of the sum of all combinations.\n\n    Returns:\n    list: The list of triplets.\n\n    Requirements:\n    - numpy\n    - itertools\n    - random\n    - matplotlib.pyplot\n\n    Example:\n    >>> find_triplets_and_plot()\n    \"\"\"\n    numbers = np.random.randint(DATA_MIN, DATA_MAX, DATA_SIZE)\n    all_combinations = list(combinations(numbers, N_NUMBERS))\n    sum_combinations = np.array([sum(comb) for comb in all_combinations])\n    triplets = [comb for comb in all_combinations if sum(comb) == SUM_TARGET]\n\n    plt.hist(sum_combinations, bins=20, edgecolor='black')\n    plt.xlabel('Sum')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of the Sum of all Combinations')\n    plt.show()\n\n    return triplets\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, itertools, random, seaborn\n```python\nimport numpy as np\nfrom itertools import combinations\nimport random\nimport seaborn as sns\n\n# Constants\nN_NUMBERS = 3\nSUM_TARGET = 0\nDATA_SIZE = 100\nDATA_MIN = -50\nDATA_MAX = 50\n\ndef find_triplets_and_plot():\n    \"\"\"\n    Generate a list of random integers, find triplets whose sum equals to zero, and plot a \n    histogram of the sum of all combinations using seaborn.\n\n    Returns:\n    list: The list of triplets.\n\n    Requirements:\n    - numpy\n    - itertools\n    - random\n    - seaborn\n\n    Example:\n    >>> find_triplets_and_plot()\n    \"\"\"\n    numbers = np.random.randint(DATA_MIN, DATA_MAX, DATA_SIZE)\n    all_combinations = list(combinations(numbers, N_NUMBERS))\n    sum_combinations = np.array([sum(comb) for comb in all_combinations])\n    triplets = [comb for comb in all_combinations if sum(comb) == SUM_TARGET]\n\n    sns.histplot(sum_combinations, bins=20, kde=True)\n    plt.xlabel('Sum')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of the Sum of all Combinations')\n    plt.show()\n\n    return triplets\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, itertools, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom itertools import combinations\nimport matplotlib.pyplot as plt\n\n# Constants\nN_NUMBERS = 3\nSUM_TARGET = 0\nDATA_SIZE = 100\nDATA_MIN = -50\nDATA_MAX = 50\n\ndef find_triplets_and_plot():\n    \"\"\"\n    Generate a pandas DataFrame of random integers, find triplets whose sum equals to zero, \n    and plot a histogram of the sum of all combinations.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the triplets.\n\n    Requirements:\n    - pandas\n    - numpy\n    - itertools\n    - matplotlib.pyplot\n\n    Example:\n    >>> find_triplets_and_plot()\n    \"\"\"\n    df = pd.DataFrame({'Numbers': np.random.randint(DATA_MIN, DATA_MAX, DATA_SIZE)})\n    all_combinations = list(combinations(df['Numbers'], N_NUMBERS))\n    sum_combinations = np.array([sum(comb) for comb in all_combinations])\n    triplets = [comb for comb in all_combinations if sum(comb) == SUM_TARGET]\n\n    plt.hist(sum_combinations, bins=20, edgecolor='black')\n    plt.xlabel('Sum')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of the Sum of all Combinations')\n    plt.show()\n\n    triplets_df = pd.DataFrame(triplets, columns=['Number 1', 'Number 2', 'Number 3'])\n\n    return triplets_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, itertools, random, seaborn, pandas\n```python\nimport numpy as np\nfrom itertools import combinations\nimport random\nimport seaborn as sns\nimport pandas as pd\n\n# Constants\nN_NUMBERS = 3\nSUM_TARGET = 0\nDATA_SIZE = 100\nDATA_MIN = -50\nDATA_MAX = 50\n\ndef find_triplets_and_plot():\n    \"\"\"\n    Generate a list of random integers, find triplets whose sum equals to zero, and plot a \n    histogram of the sum of all combinations using seaborn.\n\n    Returns:\n    DataFrame: A pandas DataFrame with the triplets.\n\n    Requirements:\n    - numpy\n    - itertools\n    - random\n    - seaborn\n    - pandas\n\n    Example:\n    >>> find_triplets_and_plot()\n    \"\"\"\n    numbers = np.random.randint(DATA_MIN, DATA_MAX, DATA_SIZE)\n    all_combinations = list(combinations(numbers, N_NUMBERS))\n    sum_combinations = np.array([sum(comb) for comb in all_combinations])\n    triplets = [comb for comb in all_combinations if sum(comb) == SUM_TARGET]\n\n    sns.histplot(sum_combinations, bins=20, kde=True)\n    plt.xlabel('Sum')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of the Sum of all Combinations')\n    plt.show()\n\n    triplets_df = pd.DataFrame(triplets, columns=['Number 1', 'Number 2', 'Number 3'])\n\n    return triplets_df\n```"}
{"seed": "{\"task_id\": 36377, \"prompt\": \"def f_36377(ary):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"np.array2string(ary, separator=', ', formatter={'float_kind': lambda x: '{: .4f}'.format(x)})\", \"test_start\": \"\\nimport numpy as np\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(np.array([1, 2, 3])) == \\\"[1, 2, 3]\\\"\\n\", \"\\n    assert candidate(np.array([])) == \\\"[]\\\"\\n\", \"\\n    assert candidate(np.array([1, 2, 3, 4, 4])) == \\\"[1, 2, 3, 4, 4]\\\"\\n\", \"\\n    assert candidate(np.array([1, 2, 3, 4, 5])) != \\\"[1 2 3 4 5]\\\"\\n\"], \"entry_point\": \"f_36377\", \"intent\": \"\\u914d\\u5217`ary`\\u306e\\u5404\\u8981\\u7d20\\u306b\\u30b3\\u30f3\\u30de\\u3092\\u4ed8\\u3051\\u3066\\u5c0f\\u6570\\u70b9\\u56db\\u6841\\u307e\\u3067\\u8868\\u793a\\u3059\\u308b\", \"library\": [\"numpy\"]}", "generation": "Scenario 1: \nnumpy, pandas, matplotlib, scipy.stats\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n# Constants\nARR_FORMAT = {'float_kind': lambda x: '{: .4f}'.format(x)}\n\ndef draw_distribution_and_calc_pvalue(ary):\n    \"\"\"\n    Draw a histogram of the data in the array, calculate and plot its normal distribution,\n    and calculate the p-value for the hypothesis that the data in the array comes from a normal distribution.\n    \n    Parameters:\n    ary (numpy array): The data array.\n    \n    Returns:\n    float: The p-value.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib\n    - scipy.stats\n    \n    Example:\n    >>> ary = np.random.normal(size=1000)\n    >>> draw_distribution_and_calc_pvalue(ary)\n    \"\"\"\n    # Draw histogram\n    pd.Series(ary).plot(kind='hist', bins=50, density=True, alpha=0.6, color='g')\n    \n    # Calculate and draw normal distribution\n    mu, std = stats.norm.fit(ary)\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, mu, std)\n    plt.plot(x, p, 'k', linewidth=2)\n    plt.show()\n\n    # Perform the Shapiro-Wilk test for normality.\n    p_value = stats.shapiro(ary)[1]\n\n    return p_value\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, pandas, sklearn, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\n\n# Constants\nARR_FORMAT = {'float_kind': lambda x: '{: .4f}'.format(x)}\n\ndef visualize_and_rescale_array(ary):\n    \"\"\"\n    Visualize the array as a bar plot and then rescale it using MinMaxScaler to a range between 0 and 1.\n    \n    Parameters:\n    ary (numpy array): The data array.\n    \n    Returns:\n    numpy array: The rescaled array.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - sklearn\n    - matplotlib\n    \n    Example:\n    >>> ary = np.array([10, 20, 30, 40, 50])\n    >>> visualize_and_rescale_array(ary)\n    \"\"\"\n    # Visualize the array\n    pd.Series(ary).plot(kind='bar')\n    plt.show()\n\n    # Rescale the array\n    scaler = MinMaxScaler()\n    rescaled_ary = scaler.fit_transform(ary.reshape(-1, 1)).flatten()\n\n    return np.array2string(rescaled_ary, separator=', ', formatter=ARR_FORMAT)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \nnumpy, pandas, seaborn\n```python\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n# Constants\nARR_FORMAT = {'float_kind': lambda x: '{: .4f}'.format(x)}\n\ndef plot_dist_and_calc_kurtosis(ary):\n    \"\"\"\n    Plot a distribution of the data in the array and calculate its kurtosis.\n    \n    Parameters:\n    ary (numpy array): The data array.\n    \n    Returns:\n    float: The kurtosis of the array.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - seaborn\n    \n    Example:\n    >>> ary = np.random.normal(size=1000)\n    >>> plot_dist_and_calc_kurtosis(ary)\n    \"\"\"\n    # Plot distribution\n    sns.distplot(ary)\n    \n    # Calculate kurtosis\n    kurtosis = pd.Series(ary).kurtosis()\n\n    return kurtosis\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, pandas, matplotlib, sklearn\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\n# Constants\nARR_FORMAT = {'float_kind': lambda x: '{: .4f}'.format(x)}\n\ndef standardize_and_plot_array(ary):\n    \"\"\"\n    Standardize the array (mean=0, std=1) and plot its histogram.\n    \n    Parameters:\n    ary (numpy array): The data array.\n    \n    Returns:\n    numpy array: The standardized array.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib\n    - sklearn\n    \n    Example:\n    >>> ary = np.array([10, 20, 30, 40, 50])\n    >>> standardize_and_plot_array(ary)\n    \"\"\"\n    # Standardize the array\n    scaler = StandardScaler()\n    standardized_ary = scaler.fit_transform(ary.reshape(-1, 1)).flatten()\n\n    # Plot histogram\n    pd.Series(standardized_ary).plot(kind='hist', bins=50)\n    plt.show()\n\n    return np.array2string(standardized_ary, separator=', ', formatter=ARR_FORMAT)\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nnumpy, pandas, scipy.stats, seaborn\n```python\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import zscore\nimport seaborn as sns\n\n# Constants\nARR_FORMAT = {'float_kind': lambda x: '{: .4f}'.format(x)}\n\ndef calc_zscore_and_plot_dist(ary):\n    \"\"\"\n    Calculate the z-score of the array and plot its distribution.\n    \n    Parameters:\n    ary (numpy array): The data array.\n    \n    Returns:\n    numpy array: The array of z-scores.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - scipy.stats\n    - seaborn\n    \n    Example:\n    >>> ary = np.array([10, 20, 30, 40, 50])\n    >>> calc_zscore_and_plot_dist(ary)\n    \"\"\"\n    # Calculate z-score\n    z_scores = zscore(ary)\n\n    # Plot distribution\n    sns.distplot(z_scores)\n\n    return np.array2string(z_scores, separator=', ', formatter=ARR_FORMAT)\n```\n"}
{"seed": "{\"task_id\": 23839, \"prompt\": \"def f_23839():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"subprocess.check_output('cat file', shell=True)\", \"test_start\": \"\\nimport subprocess\\nfrom unittest.mock import Mock\\n\\ndef check(candidate):\", \"test\": [\"\\n    subprocess.check_output = Mock(return_value = \\\"Success\\\")\\n    assert candidate() == \\\"Success\\\"\\n\"], \"entry_point\": \"f_23839\", \"intent\": \"\\u5916\\u90e8\\u30d7\\u30ed\\u30bb\\u30b9`cat`\\u3092\\u547c\\u3073\\u51fa\\u3057\\u3001\\u30d5\\u30a1\\u30a4\\u30eb`file`\\u306e\\u4e2d\\u8eab\\u3092\\u8aad\\u307f\\u8fbc\\u3080\", \"library\": [\"subprocess\"]}", "generation": "Scenario 1:\nsubprocess, random, os, tempfile\n```python\nimport subprocess\nimport random\nimport os\nfrom tempfile import TemporaryDirectory\n\n# Constants\nFILES = ['file1.txt', 'file2.txt', 'file3.txt', 'file4.txt', 'file5.txt']\n\ndef copy_random_file_to_temp_dir():\n    \"\"\"\n    Copy a random file from a list of files to a temporary directory.\n\n    Returns:\n    str: The path of the copied file in the temporary directory.\n\n    Requirements:\n    - subprocess\n    - random\n    - os\n    - tempfile\n\n    Example:\n    >>> copy_random_file_to_temp_dir()\n    \"\"\"\n    file_to_copy = random.choice(FILES)\n    with TemporaryDirectory() as temp_dir:\n        copied_file_path = os.path.join(temp_dir, file_to_copy)\n        subprocess.check_output(f'cp {file_to_copy} {copied_file_path}', shell=True)\n\n    return copied_file_path\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nsubprocess, time, datetime\n```python\nimport subprocess\nimport time\nfrom datetime import datetime\n\ndef show_system_uptime():\n    \"\"\"\n    Display the system uptime.\n\n    Returns:\n    str: The system uptime.\n\n    Requirements:\n    - subprocess\n    - time\n    - datetime\n    \n    Example:\n    >>> show_system_uptime()\n    \"\"\"\n    uptime_output = subprocess.check_output('uptime -s', shell=True).decode('utf-8').strip()\n    uptime_start = datetime.strptime(uptime_output, '%Y-%m-%d %H:%M:%S')\n\n    current_time = datetime.now()\n    uptime_duration = current_time - uptime_start\n\n    return str(uptime_duration)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nsubprocess, os, sys\n```python\nimport subprocess\nimport os\nimport sys\n\ndef create_directory_and_file(directory_path, filename):\n    \"\"\"\n    Create a directory and a file inside it.\n\n    Parameters:\n    directory_path (str): The path of the directory to be created.\n    filename (str): The name of the file to be created.\n\n    Returns:\n    bool: True if the directory and file were created successfully, False otherwise.\n\n    Requirements:\n    - subprocess\n    - os\n    - sys\n\n    Example:\n    >>> create_directory_and_file('/path/to/directory', 'file.txt')\n    \"\"\"\n    try:\n        subprocess.check_output(f'mkdir -p {directory_path}', shell=True)\n        subprocess.check_output(f'touch {os.path.join(directory_path, filename)}', shell=True)\n\n        return True\n    except subprocess.CalledProcessError as e:\n        sys.stderr.write(f\"An error occurred while creating the directory and file: {str(e)}\")\n\n        return False\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nsubprocess, os, shutil\n```python\nimport subprocess\nimport os\nfrom shutil import which\n\ndef check_and_install_package(package_name):\n    \"\"\"\n    Check if a package is installed. If it's not, install it using apt-get.\n\n    Parameters:\n    package_name (str): The name of the package to check and install.\n\n    Returns:\n    bool: True if the package is installed, False otherwise.\n\n    Requirements:\n    - subprocess\n    - os\n    - shutil\n\n    Example:\n    >>> check_and_install_package('python3')\n    \"\"\"\n    if which(package_name) is not None:\n        return True\n\n    try:\n        subprocess.check_output(f'sudo apt-get install -y {package_name}', shell=True)\n\n        return True\n    except subprocess.CalledProcessError as e:\n        os.stderr.write(f\"An error occurred while installing the package: {str(e)}\")\n\n        return False\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nsubprocess, os, sys\n```python\nimport subprocess\nimport os\nimport sys\n\ndef execute_python_script(script_path):\n    \"\"\"\n    Execute a Python script using the Python interpreter.\n\n    Parameters:\n    script_path (str): The path to the Python script.\n\n    Returns:\n    int: The exit code of the script. If an error occurs during execution, returns -1.\n\n    Requirements:\n    - subprocess\n    - os\n    - sys\n\n    Example:\n    >>> execute_python_script('/path/to/python_script.py')\n    \"\"\"\n    try:\n        return subprocess.check_call(['python3', script_path])\n    except subprocess.CalledProcessError as e:\n        sys.stderr.write(f\"An error occurred while executing the script: {str(e)}\")\n\n        return -1\n```"}
{"seed": "{\"task_id\": 34981, \"prompt\": \"def f_34981():\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"cv2.imread('file.png', 0)\", \"test_start\": \"\\nimport cv2\\nfrom os import path\\nfrom PIL import Image\\n\\ndef check(candidate):\", \"test\": [\"\\n    im = Image.new('RGBA', (200, 200), (255, 255, 255, 255))\\n    im.save('file.png')\\n    img = candidate()\\n    cv2.imwrite('g.png', img)\\n    assert path.exists('g.png')\\n\"], \"entry_point\": \"f_34981\", \"intent\": \"\\u753b\\u50cf'file.png'\\u3092\\u30b0\\u30ec\\u30fc\\u30b9\\u30b1\\u30fc\\u30eb\\u3067\\u8aad\\u307f\\u8fbc\\u3080\", \"library\": [\"PIL\", \"cv2\", \"os\"]}", "generation": "Scenario 1:\nPIL, cv2, os, glob, numpy\n```python\nimport cv2\nimport os\nfrom PIL import Image\nimport glob\nimport numpy as np\n\n# Constants\nIMAGE_DIR = 'images/'\n\ndef calculate_average_brightness(image_dir=IMAGE_DIR):\n    \"\"\"\n    Calculate the average brightness of all grayscale images in a given directory.\n\n    Parameters:\n    image_dir (str): The directory containing the images.\n\n    Returns:\n    float: The average brightness.\n\n    Requirements:\n    - PIL\n    - cv2\n    - os\n    - glob\n    - numpy\n\n    Example:\n    >>> calculate_average_brightness('images/')\n    \"\"\"\n    image_files = glob.glob(os.path.join(image_dir, '*.png'))\n    \n    brightnesses = []\n    for image_file in image_files:\n        img = cv2.imread(image_file, 0)\n        brightness = np.mean(img)\n        brightnesses.append(brightness)\n\n    return np.mean(brightnesses)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nPIL, cv2, os, matplotlib, numpy\n```python\nimport cv2\nimport os\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nIMAGE_FILE = 'file.png'\n\ndef histogram_of_image(image_file=IMAGE_FILE):\n    \"\"\"\n    Create a histogram of the pixel values of a grayscale image.\n\n    Parameters:\n    image_file (str): The image file.\n\n    Requirements:\n    - PIL\n    - cv2\n    - os\n    - matplotlib.pyplot\n    - numpy\n\n    Example:\n    >>> histogram_of_image('file.png')\n    \"\"\"\n    img = cv2.imread(image_file, 0)\n    plt.hist(img.ravel(), bins=256, color='gray')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nPIL, cv2, os, random, numpy\n```python\nimport cv2\nimport os\nfrom PIL import Image\nimport random\nimport numpy as np\n\n# Constants\nIMAGE_FILE = 'file.png'\n\ndef add_noise_to_image(image_file=IMAGE_FILE):\n    \"\"\"\n    Add random noise to a grayscale image and save the noisy image.\n\n    Parameters:\n    image_file (str): The image file.\n\n    Returns:\n    str: The file name of the noisy image.\n\n    Requirements:\n    - PIL\n    - cv2\n    - os\n    - random\n    - numpy\n\n    Example:\n    >>> add_noise_to_image('file.png')\n    \"\"\"\n    img = cv2.imread(image_file, 0)\n\n    noised_img = img + np.random.randint(0, 50, img.shape, dtype='uint8')\n\n    noisy_image_file = 'noisy_{}'.format(image_file)\n    cv2.imwrite(noisy_image_file, noised_img)\n\n    return noisy_image_file\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nPIL, cv2, os, skimage, numpy\n```python\nimport cv2\nimport os\nfrom PIL import Image\nfrom skimage.transform import resize\nimport numpy as np\n\n# Constants\nIMAGE_FILE = 'file.png'\nTARGET_SIZE = (100, 100)\n\ndef resize_image(image_file=IMAGE_FILE, target_size=TARGET_SIZE):\n    \"\"\"\n    Resize a grayscale image to a target size and save the resized image.\n\n    Parameters:\n    image_file (str): The image file.\n    target_size (tuple): The target size.\n\n    Returns:\n    str: The file name of the resized image.\n\n    Requirements:\n    - PIL\n    - cv2\n    - os\n    - skimage.transform\n    - numpy\n\n    Example:\n    >>> resize_image('file.png', (100, 100))\n    \"\"\"\n    img = cv2.imread(image_file, 0)\n\n    resized_img = resize(img, target_size, mode='reflect', preserve_range=True).astype('uint8')\n\n    resized_image_file = 'resized_{}'.format(image_file)\n    cv2.imwrite(resized_image_file, resized_img)\n\n    return resized_image_file\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nPIL, cv2, os, numpy, scipy\n```python\nimport cv2\nimport os\nfrom PIL import Image\nimport numpy as np\nfrom scipy import ndimage\n\n# Constants\nIMAGE_FILE = 'file.png'\n\ndef blur_image(image_file=IMAGE_FILE):\n    \"\"\"\n    Blur a grayscale image using a Gaussian filter and save the blurred image.\n\n    Parameters:\n    image_file (str): The image file.\n\n    Returns:\n    str: The file name of the blurred image.\n\n    Requirements:\n    - PIL\n    - cv2\n    - os\n    - numpy\n    - scipy\n\n    Example:\n    >>> blur_image('file.png')\n    \"\"\"\n    img = cv2.imread(image_file, 0)\n\n    blurred_img = ndimage.gaussian_filter(img, sigma=3)\n\n    blurred_image_file = 'blurred_{}'.format(image_file)\n    cv2.imwrite(blurred_image_file, blurred_img)\n\n    return blurred_image_file\n```"}
{"seed": "{\"task_id\": 33506, \"prompt\": \"def f_33506(s):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"base64.b64decode(s).decode()\", \"test_start\": \"\\nimport base64\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(b'R2Vla3NGb3JHZWVrcw==') == 'GeeksForGeeks'\\n\"], \"entry_point\": \"f_33506\", \"intent\": \"base64\\u3067\\u7b26\\u53f7\\u5316\\u3055\\u308c\\u305f\\u6587\\u5b57\\u5217`s`\\u3092\\u3092\\u30c7\\u30b3\\u30fc\\u30c9\\u3059\\u308b\", \"library\": [\"base64\"]}", "generation": "Scenario 3:\nbase64, hashlib, hmac, binascii\n```python\nimport base64\nimport hashlib\nimport hmac\nimport binascii\n\nSECRET_KEY = 'my_secret_key'\n\ndef verify_message_signature(s, signature):\n    \"\"\"\n    Verify the signature of a base64 encoded message.\n\n    Parameters:\n    s (str): The base64 encoded message.\n    signature (str): The provided signature.\n\n    Returns:\n    bool: True if the signature matches, False otherwise.\n\n    Requirements:\n    - base64\n    - hashlib\n    - hmac\n    - binascii\n\n    Example:\n    >>> verify_message_signature('R2Vla3NGb3JHZWVrcw==', '6dcd4ce23d88e2ee95838f7b014b6284f0b7415f')\n    \"\"\"\n    decoded_msg = base64.b64decode(s).decode()\n    computed_signature = hmac.new(SECRET_KEY.encode(), decoded_msg.encode(), hashlib.sha1)\n    return binascii.hexlify(computed_signature.digest()).decode() == signature\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nbase64, os, io, PIL\n```python\nimport base64\nimport os\nfrom io import BytesIO\nfrom PIL import Image\n\ndef save_base64_image(s, filename):\n    \"\"\"\n    Save a base64 encoded image to a file.\n\n    Parameters:\n    s (str): The base64 encoded image.\n    filename (str): The name of the file to save the image.\n\n    Returns:\n    str: The path of the saved file.\n\n    Requirements:\n    - base64\n    - os\n    - io\n    - PIL\n\n    Example:\n    >>> save_base64_image('iVBORw0KG...', 'my_image.png')\n    \"\"\"\n    img_data = base64.b64decode(s)\n    img = Image.open(BytesIO(img_data))\n    file_path = os.path.join(os.getcwd(), filename)\n    img.save(file_path)\n    return file_path\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nbase64, numpy, matplotlib\n```python\nimport base64\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_base64_encoded_data(s):\n    \"\"\"\n    Plot a base64 encoded numpy array.\n\n    Parameters:\n    s (str): The base64 encoded numpy array.\n\n    Returns:\n    None.\n\n    Requirements:\n    - base64\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_base64_encoded_data('gANjbnVtcHkuY29yZS5tdWx0aWFyc...'\n    \"\"\"\n    data = base64.b64decode(s)\n    array = np.frombuffer(data, dtype=np.float64)\n    plt.plot(array)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 6:\nbase64, urllib, socket\n```python\nimport base64\nimport urllib.request\nimport socket\n\ndef get_base64_encoded_website_ip(s):\n    \"\"\"\n    Get the IP address of a base64 encoded website URL.\n\n    Parameters:\n    s (str): The base64 encoded website URL.\n\n    Returns:\n    str: The IP address of the website.\n\n    Requirements:\n    - base64\n    - urllib.request\n    - socket\n\n    Example:\n    >>> get_base64_encoded_website_ip('aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbQ==')\n    \"\"\"\n    url = base64.b64decode(s).decode()\n    hostname = urllib.request.urlparse(url).hostname\n    ip_address = socket.gethostbyname(hostname)\n    return ip_address\n```\n`GPT_ODEX_BREAK`\n\nScenario 7:\nbase64, cryptography, zlib\n```python\nimport base64\nfrom cryptography.fernet import Fernet\nimport zlib\n\nKEY = Fernet.generate_key()\n\ndef decrypt_and_decompress_data(s):\n    \"\"\"\n    Decrypt and decompress a base64 encoded string.\n\n    Parameters:\n    s (str): The base64 encoded string.\n\n    Returns:\n    str: The decrypted and decompressed string.\n\n    Requirements:\n    - base64\n    - cryptography.fernet\n    - zlib\n\n    Example:\n    >>> decrypt_and_decompress_data('gAAAAABhE6D0YX6EF9O8ZXB...'\n    \"\"\"\n    cipher_suite = Fernet(KEY)\n    decoded_data = base64.b64decode(s)\n    decrypted_data = cipher_suite.decrypt(decoded_data)\n    decompressed_data = zlib.decompress(decrypted_data)\n    return decompressed_data.decode()\n```"}
{"seed": "{\"task_id\": 24190, \"prompt\": \"def f_24190(x, y):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"plt.scatter(x, y)\", \"test_start\": \"\\nimport matplotlib.pyplot as plt\\n\\ndef extract_data_from_plot(plot):\\n    x_plot, y_plot = plot.get_offsets().data.T\\n    return x_plot, y_plot\\n    \\ndef check(candidate):\", \"test\": [\"\\n    x, y = [0, 1, 2], [0, 1, 2]\\n    plot = candidate(x, y)\\n    x_plot, y_plot = extract_data_from_plot(plot)\\n    assert y_plot.tolist(), float(y).tolist()\\n    assert x_plot.tolist(), float(x).tolist()\\n\", \"\\n    x, y = [10.3, 11.12, 133.44], [4.9, 2.48, 3.67]\\n    plot = candidate(x, y)\\n    x_plot, y_plot = extract_data_from_plot(plot)\\n    assert y_plot.tolist(), float(y).tolist()\\n    assert x_plot.tolist(), float(x).tolist()\\n\"], \"entry_point\": \"f_24190\", \"intent\": \"\\u914d\\u5217\\u30c7\\u30fc\\u30bf`x`,`y`\\u306e\\u6563\\u5e03\\u56f3\\u3092\\u8868\\u793a\\u3059\\u308b\", \"library\": [\"matplotlib\"]}", "generation": "Scenario 1:\npandas, numpy, matplotlib, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nTITLES = ['Scatter plot', 'Histogram', 'Box plot', 'Violin plot']\n\ndef plot_data(df, x, y):\n    \"\"\"\n    Plot four different types of plots (scatter plot, histogram, box plot, and violin plot) \n    for the provided dataframe and x, y columns.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n    x (str): The column name for x-axis.\n    y (str): The column name for y-axis.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> df = pd.DataFrame(np.random.rand(50, 2), columns=['A', 'B'])\n    >>> plot_data(df, 'A', 'B')\n    \"\"\"\n    plt.figure(figsize=(20, 5))\n\n    for i, title in enumerate(TITLES, 1):\n        plt.subplot(1, 4, i)\n        if title == 'Scatter plot':\n            sns.scatterplot(data=df, x=x, y=y)\n        elif title == 'Histogram':\n            sns.histplot(data=df, x=x, y=y, bins=10)\n        elif title == 'Box plot':\n            sns.boxplot(data=df, x=x, y=y)\n        elif title == 'Violin plot':\n            sns.violinplot(data=df, x=x, y=y)\n        plt.title(title)\n    \n    plt.tight_layout()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nmatplotlib, pandas, numpy, seaborn\n```python\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n\n# Constants\nCOLORS = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n\ndef multiple_scatter_plots(df, x, y, hue):\n    \"\"\"\n    Plot multiple scatter plots on the same figure for the provided dataframe, x, y columns and hue.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n    x (str): The column name for x-axis.\n    y (str): The column name for y-axis.\n    hue (str): The column name for hue.\n\n    Returns:\n    None\n\n    Requirements:\n    - matplotlib.pyplot\n    - pandas\n    - numpy\n    - seaborn\n\n    Example:\n    >>> df = pd.DataFrame(np.random.rand(100, 4), columns=['A', 'B', 'C', 'D'])\n    >>> multiple_scatter_plots(df, 'A', 'B', 'C')\n    \"\"\"\n    plt.figure(figsize=(10, 10))\n\n    categories = df[hue].unique()\n\n    for i, category in enumerate(categories):\n        subset_df = df[df[hue] == category]\n        plt.scatter(subset_df[x], subset_df[y], color=COLORS[i % len(COLORS)], label=category)\n\n    plt.xlabel(x)\n    plt.ylabel(y)\n    plt.legend(title=hue)\n    plt.title(f\"Scatter plot of {x} vs {y} grouped by {hue}\")\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nmatplotlib, pandas, numpy, sklearn\n```python\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\n# Constants\nCOLORS = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n\ndef scatter_plot_with_regression(df, x, y):\n    \"\"\"\n    Plot the scatter plot of x and y along with the regression line.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n    x (str): The column name for x-axis.\n    y (str): The column name for y-axis.\n\n    Returns:\n    None\n\n    Requirements:\n    - matplotlib.pyplot\n    - pandas\n    - numpy\n    - sklearn.linear_model.LinearRegression\n\n    Example:\n    >>> df = pd.DataFrame(np.random.rand(100, 2), columns=['A', 'B'])\n    >>> scatter_plot_with_regression(df, 'A', 'B')\n    \"\"\"\n    X = df[x].values.reshape(-1, 1)\n    Y = df[y].values.reshape(-1, 1)\n\n    model = LinearRegression()\n    model.fit(X, Y)\n\n    plt.scatter(X, Y, color='b', label='Data')\n    plt.plot(X, model.predict(X), color='r', label='Fitted line')\n    plt.xlabel(x)\n    plt.ylabel(y)\n    plt.title(f\"Scatter plot of {x} vs {y} with regression line\")\n    plt.legend()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nmatplotlib, pandas, numpy, sklearn\n```python\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cluster import KMeans\n\n# Constants\nCOLORS = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n\ndef scatter_plot_with_clusters(df, x, y, n_clusters):\n    \"\"\"\n    Plot the scatter plot of x and y along with the clusters.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n    x (str): The column name for x-axis.\n    y (str): The column name for y-axis.\n    n_clusters (int): The number of clusters.\n\n    Returns:\n    None\n\n    Requirements:\n    - matplotlib.pyplot\n    - pandas\n    - numpy\n    - sklearn.cluster.KMeans\n\n    Example:\n    >>> df = pd.DataFrame(np.random.rand(100, 2), columns=['A', 'B'])\n    >>> scatter_plot_with_clusters(df, 'A', 'B', 3)\n    \"\"\"\n    X = df[[x, y]].values\n\n    kmeans = KMeans(n_clusters=n_clusters)\n    kmeans.fit(X)\n    labels = kmeans.labels_\n\n    for i in range(n_clusters):\n        subset_X = X[labels == i]\n        plt.scatter(subset_X[:, 0], subset_X[:, 1], color=COLORS[i % len(COLORS)], label=f\"Cluster {i+1}\")\n\n    plt.xlabel(x)\n    plt.ylabel(y)\n    plt.title(f\"Scatter plot of {x} vs {y} with {n_clusters} clusters\")\n    plt.legend()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, matplotlib, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nTITLES = ['Scatter plot', 'Line plot']\n\ndef plot_timeseries_data(df, x, y):\n    \"\"\"\n    Plot scatter plot and line plot for the provided dataframe and x, y columns.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n    x (str): The column name for x-axis.\n    y (str): The column name for y-axis.\n\n    Returns:\n    None\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - seaborn\n\n    Example:\n    >>> df = pd.DataFrame({'A': pd.date_range(start='1/1/2021', periods=50), 'B': np.random.rand(50)})\n    >>> plot_timeseries_data(df, 'A', 'B')\n    \"\"\"\n    plt.figure(figsize=(20, 5))\n\n    for i, title in enumerate(TITLES, 1):\n        plt.subplot(1, 2, i)\n        if title == 'Scatter plot':\n            sns.scatterplot(data=df, x=x, y=y)\n        elif title == 'Line plot':\n            sns.lineplot(data=df, x=x, y=y)\n        plt.title(title)\n    \n    plt.tight_layout()\n    plt.show()\n```"}
{"seed": "{\"task_id\": 40646, \"prompt\": \"def f_40646(html):\\n\\t\", \"suffix\": \"\\n\\treturn new_list\", \"canonical_solution\": \"\\n\\tsoup = bs4.BeautifulSoup(html, 'lxml')\\n\\tunorder_list = soup.find_all('ul')\\n\\tnew_list = []\\n\\tfor ul_tag in unorder_list:\\n\\t    for li in ul_tag.find_all('li'):\\n\\t        new_list.append(li.text)\\n\", \"test_start\": \"\\nimport bs4\\nimport urllib\\nimport ssl\\n    \\ndef check(candidate):\", \"test\": [\"\\n    ctx = ssl.create_default_context()\\n    ctx.check_hostname = False\\n    ctx.verify_mode = ssl.CERT_NONE\\n    url = 'https://en.wikipedia.org/wiki/Blue_Moon_of_Josephine'\\n    html = urllib.request.urlopen(url, context=ctx).read()\\n    assert 'List of diamonds' in candidate(html)\\n\"], \"entry_point\": \"f_40646\", \"intent\": \"HTML\\u30d5\\u30a1\\u30a4\\u30eb`html`\\u5185\\u306e\\u9806\\u5e8f\\u306a\\u3057\\u30ea\\u30b9\\u30c8\\u3092python\\u306e\\u30ea\\u30b9\\u30c8\\u3068\\u3057\\u3066\\u53d6\\u308a\\u8fbc\\u3080\", \"library\": [\"bs4\", \"ssl\", \"urllib\"]}", "generation": "Scenario 1:\nbs4, requests, re, pandas\n```python\nimport requests\nfrom bs4 import BeautifulSoup\nimport re\nimport pandas as pd\n\ndef get_job_postings(url):\n    \"\"\"\n    Scrapes a job listing webpage and returns a pandas DataFrame containing the job titles and \n    their corresponding URLs.\n\n    Parameters:\n    url (str): The URL of the job listing webpage.\n\n    Returns:\n    DataFrame: A pandas DataFrame containing the job titles and their corresponding URLs.\n\n    Requirements:\n    - requests\n    - bs4.BeautifulSoup\n    - re\n    - pandas\n\n    Example:\n    >>> url = 'https://www.example.com/jobs'\n    >>> job_postings = get_job_postings(url)\n    >>> print(job_postings)\n    \"\"\"\n    response = requests.get(url)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    jobs = soup.find_all('a', href=re.compile('/jobs/'))\n\n    job_postings = []\n    for job in jobs:\n        title = job.text\n        link = job['href']\n        job_postings.append([title, link])\n\n    df = pd.DataFrame(job_postings, columns=['Job Title', 'URL'])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nbs4, urllib, ssl, json\n```python\nimport urllib.request\nimport ssl\nfrom bs4 import BeautifulSoup\nimport json\n\ndef get_metadata(url):\n    \"\"\"\n    Extracts metadata from a webpage and returns it in a dictionary format.\n\n    Parameters:\n    url (str): The URL of the webpage.\n\n    Returns:\n    dict: A dictionary containing the metadata of the webpage.\n\n    Requirements:\n    - urllib.request\n    - ssl\n    - bs4.BeautifulSoup\n    - json\n\n    Example:\n    >>> url = 'https://www.example.com'\n    >>> metadata = get_metadata(url)\n    >>> print(json.dumps(metadata, indent=4))\n    \"\"\"\n    ctx = ssl.create_default_context()\n    ctx.check_hostname = False\n    ctx.verify_mode = ssl.CERT_NONE\n    html = urllib.request.urlopen(url, context=ctx).read()\n    soup = BeautifulSoup(html, 'html.parser')\n    meta = soup.find_all('meta')\n\n    metadata = {}\n    for tag in meta:\n        if 'name' in tag.attrs.keys() and tag.attrs['content']:\n            metadata[tag.attrs['name'].lower()] = tag.attrs['content']\n\n    return metadata\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nbs4, requests, os, csv\n```python\nimport requests\nfrom bs4 import BeautifulSoup\nimport os\nimport csv\n\ndef download_images(url, folder):\n    \"\"\"\n    Downloads all images from a webpage and saves them to a specified folder.\n\n    Parameters:\n    url (str): The URL of the webpage.\n    folder (str): The path to the folder where the images will be saved.\n\n    Returns:\n    None\n\n    Requirements:\n    - requests\n    - bs4.BeautifulSoup\n    - os\n    - csv\n\n    Example:\n    >>> url = 'https://www.example.com'\n    >>> folder = './images'\n    >>> download_images(url, folder)\n    \"\"\"\n    response = requests.get(url)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    images = soup.find_all('img')\n\n    for i, img in enumerate(images):\n        img_url = img['src']\n        img_data = requests.get(img_url).content\n        with open(os.path.join(folder, f'image_{i}.jpg'), 'wb') as handler:\n            handler.write(img_data)\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nbs4, requests, pandas, seaborn, matplotlib\n```python\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef visualize_ratings(url):\n    \"\"\"\n    Scrapes a movie listing webpage, extracts the movie ratings, and creates a histogram \n    of the distribution of ratings.\n\n    Parameters:\n    url (str): The URL of the movie listing webpage.\n\n    Returns:\n    None\n\n    Requirements:\n    - requests\n    - bs4.BeautifulSoup\n    - pandas\n    - seaborn\n    - matplotlib.pyplot\n\n    Example:\n    >>> url = 'https://www.example.com/movies'\n    >>> visualize_ratings(url)\n    \"\"\"\n    response = requests.get(url)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    ratings = soup.find_all('div', class_='rating')\n\n    ratings_list = []\n    for rating in ratings:\n        ratings_list.append(float(rating.text))\n\n    df = pd.DataFrame(ratings_list, columns=['Rating'])\n    \n    sns.histplot(data=df, x='Rating', kde=True)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nbs4, requests, lxml, pandas\n```python\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\ndef extract_table(url, table_id):\n    \"\"\"\n    Extracts a specific table from a webpage and returns it as a pandas DataFrame.\n\n    Parameters:\n    url (str): The URL of the webpage.\n    table_id (str): The id of the table to be extracted.\n\n    Returns:\n    DataFrame: A pandas DataFrame containing the data from the table.\n\n    Requirements:\n    - requests\n    - bs4.BeautifulSoup\n    - pandas\n\n    Example:\n    >>> url = 'https://www.example.com'\n    >>> table_id = 'data'\n    >>> df = extract_table(url, table_id)\n    >>> print(df)\n    \"\"\"\n    response = requests.get(url)\n    soup = BeautifulSoup(response.text, 'lxml')\n    table = soup.find('table', {'id': table_id})\n\n    data = []\n    headers = []\n    for header in table.find_all('th'):\n        headers.append(header.text)\n\n    for row in table.find_all('tr'):\n        cols = row.find_all('td')\n        cols = [ele.text.strip() for ele in cols]\n        data.append([ele for ele in cols if ele])\n\n    df = pd.DataFrame(data, columns=headers)\n\n    return df\n```\n"}
{"seed": "{\"task_id\": 41775, \"prompt\": \"def f_41775(df, col_label):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"df[df.duplicated(subset=col_label)]\", \"test_start\": \"\\nimport pandas as pd\\nimport numpy as np\\n\\ndef check(candidate):\", \"test\": [\"\\n    df1 = pd.DataFrame(data={'CRcode':['Gk125', 'GK126'], 'client name & address':['Jhone', 'Mike']})\\n    assert candidate(df1, \\\"CRcode\\\").to_dict() == {'CRcode': {}, 'client name & address': {}}\\n\", \"\\n    df2 = pd.DataFrame(data={'CRcode':['598', '2598', '341', '796'], 'client name & address':['random', 'random2', 'random3', 'random4']})\\n    assert candidate(df2, \\\"client name & address\\\").to_dict() == {'CRcode': {}, 'client name & address': {}}\\n\"], \"entry_point\": \"f_41775\", \"intent\": \"\\u30c7\\u30fc\\u30bf\\u30d5\\u30ec\\u30fc\\u30e0`df`\\u5185\\u306e\\u5217`col_label`\\u304c\\u91cd\\u8907\\u3057\\u3066\\u3044\\u308b\\u884c\\u3092\\u62bd\\u51fa\\u3059\\u308b\\uff08pandas)\", \"library\": [\"numpy\", \"pandas\"]}", "generation": "Scenario 1: \npandas, numpy, matplotlib, random\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import choice\n\n# Constants\nCANDY_BRANDS = ['Snickers', 'Twix', 'Milky Way', '3 Musketeers', 'Starburst']\nCANDY_COLORS = ['Red', 'Blue', 'Green', 'Yellow', 'Orange', 'Purple']\n\ndef plot_duplicated_candy_colors(df):\n    \"\"\"\n    Given a dataframe `df` with columns 'Candy Brand' and 'Candy Color', \n    find duplicated rows based on 'Candy Color' and plot the counts of \n    each duplicated 'Candy Color' in a bar plot.\n\n    Parameters:\n    df (DataFrame): The dataframe with 'Candy Brand' and 'Candy Color' columns.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> df = pd.DataFrame({'Candy Brand': [choice(CANDY_BRANDS) for _ in range(1000)],\n    >>>                    'Candy Color': [choice(CANDY_COLORS) for _ in range(1000)]})\n    >>> plot_duplicated_candy_colors(df)\n    \"\"\"\n    duplicated_rows = df[df.duplicated(subset='Candy Color')]\n\n    plt.bar(duplicated_rows['Candy Color'].value_counts().index, \n            duplicated_rows['Candy Color'].value_counts().values)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \npandas, numpy, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n\n# Constants\nBIKE_BRANDS = ['Trek', 'Specialized', 'Cannondale', 'Giant', 'Scott']\n\ndef plot_heatmap_duplicated_bike_brands(df):\n    \"\"\"\n    Given a dataframe `df` with columns 'Bike Brand', 'Price' and 'Weight', \n    find duplicated rows based on 'Bike Brand' and plot a heatmap of the correlation \n    matrix of 'Price' and 'Weight' for the duplicated rows.\n\n    Parameters:\n    df (DataFrame): The dataframe with 'Bike Brand', 'Price' and 'Weight' columns.\n\n    Requirements:\n    - pandas\n    - numpy\n    - seaborn\n\n    Example:\n    >>> df = pd.DataFrame({'Bike Brand': np.random.choice(BIKE_BRANDS, size=1000),\n    >>>                    'Price': np.random.randint(500, 2000, size=1000),\n    >>>                    'Weight': np.random.randint(10, 20, size=1000)})\n    >>> plot_heatmap_duplicated_bike_brands(df)\n    \"\"\"\n    duplicated_rows = df[df.duplicated(subset='Bike Brand')]\n    correlation_matrix = duplicated_rows[['Price', 'Weight']].corr()\n\n    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \npandas, numpy, sklearn, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nimport matplotlib.pyplot as plt\n\n# Constants\nCAR_MODELS = ['Model S', 'Model 3', 'Model X', 'Model Y', 'Roadster']\n\ndef plot_encoded_duplicated_car_models(df):\n    \"\"\"\n    Given a dataframe `df` with columns 'Car Model' and 'Price', \n    find duplicated rows based on 'Car Model', encode 'Car Model' with \n    LabelEncoder and plot a scatter plot of the encoded 'Car Model' and 'Price'.\n\n    Parameters:\n    df (DataFrame): The dataframe with 'Car Model' and 'Price' columns.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.preprocessing\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame({'Car Model': np.random.choice(CAR_MODELS, size=1000),\n    >>>                    'Price': np.random.randint(35000, 120000, size=1000)})\n    >>> plot_encoded_duplicated_car_models(df)\n    \"\"\"\n    duplicated_rows = df[df.duplicated(subset='Car Model')]\n    le = LabelEncoder()\n    duplicated_rows['Car Model'] = le.fit_transform(duplicated_rows['Car Model'])\n    plt.scatter(duplicated_rows['Car Model'], duplicated_rows['Price'])\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \npandas, numpy, seaborn, scipy\n```python\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom scipy import stats\n\n# Constants\nFRUIT_TYPES = ['Apple', 'Banana', 'Orange', 'Pear', 'Pineapple']\n\ndef plot_dist_duplicated_fruit_types(df):\n    \"\"\"\n    Given a dataframe `df` with columns 'Fruit Type' and 'Weight', \n    find duplicated rows based on 'Fruit Type' and plot a distribution plot \n    of the 'Weight' for the duplicated rows.\n\n    Parameters:\n    df (DataFrame): The dataframe with 'Fruit Type' and 'Weight' columns.\n\n    Requirements:\n    - pandas\n    - numpy\n    - seaborn\n    - scipy\n\n    Example:\n    >>> df = pd.DataFrame({'Fruit Type': np.random.choice(FRUIT_TYPES, size=1000),\n    >>>                    'Weight': np.random.normal(1, 0.2, size=1000)})\n    >>> plot_dist_duplicated_fruit_types(df)\n    \"\"\"\n    duplicated_rows = df[df.duplicated(subset='Fruit Type')]\n    sns.distplot(duplicated_rows['Weight'], fit=stats.norm)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \npandas, numpy, sklearn, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import OneHotEncoder\nimport matplotlib.pyplot as plt\n\n# Constants\nDOG_BREEDS = ['Labrador', 'German Shepherd', 'Golden Retriever', 'Bulldog', 'Beagle']\n\ndef plot_onehot_duplicated_dog_breeds(df):\n    \"\"\"\n    Given a dataframe `df` with columns 'Dog Breed' and 'Height', \n    find duplicated rows based on 'Dog Breed', one-hot encode 'Dog Breed' \n    and plot a boxplot of the one-hot encoded 'Dog Breed' and 'Height'.\n\n    Parameters:\n    df (DataFrame): The dataframe with 'Dog Breed' and 'Height' columns.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.preprocessing\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame({'Dog Breed': np.random.choice(DOG_BREEDS, size=1000),\n    >>>                    'Height': np.random.uniform(0.5, 1.5, size=1000)})\n    >>> plot_onehot_duplicated_dog_breeds(df)\n    \"\"\"\n    duplicated_rows = df[df.duplicated(subset='Dog Breed')]\n    ohe = OneHotEncoder(sparse=False)\n    one_hot_encoded = ohe.fit_transform(duplicated_rows[['Dog Breed']])\n    one_hot_encoded_df = pd.DataFrame(one_hot_encoded, columns=ohe.get_feature_names(['Dog Breed']))\n    one_hot_encoded_df['Height'] = duplicated_rows.reset_index()['Height']\n    one_hot_encoded_df.boxplot()\n    plt.show()\n```\n"}
{"seed": "{\"task_id\": 33034, \"prompt\": \"def f_33034(df, col_1, col_2):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"pandas.crosstab(df[col_1], df[col_2]).plot(kind='bar',stacked=True)\", \"test_start\": \"\\nimport pandas\\n\\ndef check(candidate):\", \"test\": [\"\\n    d = {'col_1':[1, 2], 'col_2':[3, 5]}\\n    df = pandas.DataFrame(data = d)\\n    x = candidate(df, 'col_1', 'col_2')\\n    assert str(type(x)).split(\\\"'\\\")[1] == 'matplotlib.axes._subplots.AxesSubplot'\\n\"], \"entry_point\": \"f_33034\", \"intent\": \"\\u30c7\\u30fc\\u30bf\\u30d5\\u30ec\\u30fc\\u30e0`df`\\u306e\\u5217`col_1`\\u3068`col_2`\\u306b\\u3064\\u3044\\u3066\\u30af\\u30ed\\u30b9\\u96c6\\u8a08\\u3092\\u884c\\u3063\\u305f\\u7d50\\u679c\\u3092\\u7a4d\\u307f\\u4e0a\\u3052\\u30b0\\u30e9\\u30d5\\u306b\\u3059\\u308b\", \"library\": [\"pandas\"]}", "generation": "Scenario 1:\npandas, seaborn, matplotlib, numpy, datetime\n```python\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom datetime import datetime, timedelta\n\n# Constants\nDAYS = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n\ndef visualize_weekly_trends(df, date_col, metric_col):\n    \"\"\"\n    Visualize the weekly trends of a certain metric from a DataFrame.\n\n    Parameters:\n    df (DataFrame): The DataFrame containing the data.\n    date_col (str): The column name for the date.\n    metric_col (str): The column name for the metric to visualize.\n\n    Returns:\n    AxesSubplot: The seaborn lineplot.\n\n    Requirements:\n    - pandas\n    - seaborn\n    - matplotlib.pyplot\n    - numpy\n    - datetime\n\n    Example:\n    >>> df = pd.DataFrame({'date': pd.date_range(start='1/1/2021', end='1/31/2021'), 'metric': np.random.rand(31)})\n    >>> visualize_weekly_trends(df, 'date', 'metric')\n    \"\"\"\n    df[date_col] = pd.to_datetime(df[date_col])\n    df['day_of_week'] = df[date_col].dt.day_name()\n    df['day_of_week'] = pd.Categorical(df['day_of_week'], categories=DAYS, ordered=True)\n\n    plt.figure(figsize=(10, 6))\n    sns.lineplot(x='day_of_week', y=metric_col, data=df)\n    plt.title(f'Weekly Trends of {metric_col}')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, seaborn, matplotlib, numpy, sklearn.preprocessing\n```python\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Constants\nCOLORS = ['blue', 'orange', 'green', 'red', 'purple', 'brown', 'pink', 'gray', 'olive', 'cyan']\n\ndef visualize_metric_distribution(df, metrics):\n    \"\"\"\n    Visualize the distribution of multiple metrics from a DataFrame.\n\n    Parameters:\n    df (DataFrame): The DataFrame containing the data.\n    metrics (list): The list of metric column names to visualize.\n\n    Returns:\n    AxesSubplot: The seaborn distplot.\n\n    Requirements:\n    - pandas\n    - seaborn\n    - matplotlib.pyplot\n    - numpy\n    - sklearn.preprocessing.MinMaxScaler\n\n    Example:\n    >>> df = pd.DataFrame(np.random.rand(100, 5), columns=['metric1', 'metric2', 'metric3', 'metric4', 'metric5'])\n    >>> visualize_metric_distribution(df, ['metric1', 'metric2', 'metric3'])\n    \"\"\"\n    scaler = MinMaxScaler()\n\n    plt.figure(figsize=(10, 6))\n\n    for i, metric in enumerate(metrics):\n        data = df[[metric]].values\n        data_scaled = scaler.fit_transform(data)\n        sns.distplot(data_scaled, hist=False, rug=True, color=COLORS[i], label=metric)\n\n    plt.title('Metric Distributions')\n    plt.legend()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, matplotlib, numpy, scipy.stats\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import pearsonr\n\ndef visualize_correlation(df, col_1, col_2):\n    \"\"\"\n    Visualize the correlation between two columns from a DataFrame.\n\n    Parameters:\n    df (DataFrame): The DataFrame containing the data.\n    col_1 (str): The first column name.\n    col_2 (str): The second column name.\n\n    Returns:\n    float: The Pearson correlation coefficient.\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - numpy\n    - scipy.stats.pearsonr\n\n    Example:\n    >>> df = pd.DataFrame(np.random.rand(100, 2), columns=['col1', 'col2'])\n    >>> visualize_correlation(df, 'col1', 'col2')\n    \"\"\"\n    plt.scatter(df[col_1], df[col_2])\n    plt.title(f'Correlation between {col_1} and {col_2}')\n    plt.xlabel(col_1)\n    plt.ylabel(col_2)\n    plt.show()\n\n    corr, _ = pearsonr(df[col_1], df[col_2])\n\n    return corr\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, seaborn, matplotlib, numpy, sklearn.decomposition\n```python\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.decomposition import PCA\n\ndef visualize_pca(df, n_components, target_col):\n    \"\"\"\n    Visualize the first two principal components of a DataFrame.\n\n    Parameters:\n    df (DataFrame): The DataFrame containing the data.\n    n_components (int): The number of principal components.\n    target_col (str): The target column name.\n\n    Returns:\n    DataFrame: The DataFrame with principal components.\n\n    Requirements:\n    - pandas\n    - seaborn\n    - matplotlib.pyplot\n    - numpy\n    - sklearn.decomposition.PCA\n\n    Example:\n    >>> df = pd.DataFrame(np.random.rand(100, 5), columns=['feature1', 'feature2', 'feature3', 'feature4', 'target'])\n    >>> visualize_pca(df.drop('target', axis=1), 2, df['target'])\n    \"\"\"\n    pca = PCA(n_components)\n    principalComponents = pca.fit_transform(df)\n    principalDf = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2'])\n\n    finalDf = pd.concat([principalDf, target_col], axis = 1)\n\n    fig = plt.figure(figsize = (8,8))\n    ax = fig.add_subplot(1,1,1) \n    ax.set_xlabel('Principal Component 1', fontsize = 15)\n    ax.set_ylabel('Principal Component 2', fontsize = 15)\n    ax.set_title('2 Component PCA', fontsize = 20)\n    targets = [0, 1]\n    colors = ['r', 'g']\n    for target, color in zip(targets,colors):\n        indicesToKeep = finalDf['target'] == target\n        ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n                   , finalDf.loc[indicesToKeep, 'principal component 2']\n                   , c = color\n                   , s = 50)\n    ax.legend(targets)\n    ax.grid()\n\n    return finalDf\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, seaborn, matplotlib, numpy, sklearn.cluster\n```python\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.cluster import KMeans\n\ndef visualize_kmeans_clustering(df, n_clusters):\n    \"\"\"\n    Visualize the k-means clustering results of a DataFrame.\n\n    Parameters:\n    df (DataFrame): The DataFrame containing the data.\n    n_clusters (int): The number of clusters.\n\n    Returns:\n    DataFrame: The DataFrame with cluster labels.\n\n    Requirements:\n    - pandas\n    - seaborn\n    - matplotlib.pyplot\n    - numpy\n    - sklearn.cluster.KMeans\n\n    Example:\n    >>> df = pd.DataFrame(np.random.rand(100, 2), columns=['feature1', 'feature2'])\n    >>> visualize_kmeans_clustering(df, 3)\n    \"\"\"\n    kmeans = KMeans(n_clusters=n_clusters)\n    kmeans.fit(df)\n\n    df['cluster'] = kmeans.labels_\n\n    plt.figure(figsize=(10, 6))\n    sns.scatterplot(x='feature1', y='feature2', hue='cluster', data=df, palette='viridis')\n    plt.title('K-means Clustering')\n    plt.show()\n\n    return df\n```\n"}
{"seed": "{\"task_id\": 23246, \"prompt\": \"def f_23246(li, v):\\n\\t\", \"suffix\": \"\\n\\treturn ans\", \"canonical_solution\": \"ans = []\\n\\tfor index, value in enumerate(li):\\n\\t    if value == v:\\n\\t        ans.append(index)\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([1,2,3,3,3,4,5], 3) == [2, 3, 4]\\n\"], \"entry_point\": \"f_23246\", \"intent\": \"\\u30ea\\u30b9\\u30c8`li`\\u304b\\u3089\\u691c\\u7d22\\u3059\\u308b\\u5024`v`\\u306b\\u4e00\\u81f4\\u3059\\u308b\\u8981\\u7d20\\u306e\\u30a4\\u30f3\\u30c7\\u30c3\\u30af\\u30b9\\u3092\\u3059\\u3079\\u3066\\u53d6\\u5f97\\u3057\\u3066\\u8868\\u793a\\u3059\\u308b\", \"library\": []}", "generation": "Scenario 1:\nnumpy, random, matplotlib, pandas\n```python\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants\nVALUES = [1, 2, 3, 4, 5]\nCOUNTS = [10, 20, 30, 40, 50]\n\ndef plot_distribution_of_values(li, v):\n    \"\"\"\n    Generate random data based on the count of a particular value in a list. \n    Plot a histogram of the generated data.\n\n    Parameters:\n    li (list): The list of integers.\n    v (int): The value to search for in the list.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - random\n    - matplotlib.pyplot\n    - pandas\n\n    Example:\n    >>> plot_distribution_of_values([1,2,3,3,3,4,5], 3)\n    \"\"\"\n    count = li.count(v)\n    data = np.random.choice(VALUES, p=[c/sum(COUNTS) for c in COUNTS], size=count*100)\n    plt.hist(data, bins=5)\n    plt.xlabel('Values')\n    plt.ylabel('Frequency')\n    plt.title(f'Distribution of Values (Count of {v} in list: {count})')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, datetime, dateutil\n```python\nimport numpy as np\nfrom datetime import datetime\nfrom dateutil.parser import parse\n\n# Constants\nLEAP_YEARS = np.array([1972, 1976, 1980, 1984, 1988, 1992, 1996, 2000, 2004, 2008, 2012, 2016, 2020])\n\ndef calculate_days_since_date(li, v):\n    \"\"\"\n    Calculate the number of days that have passed since the dates in a list where the year \n    is equal to a given value, considering leap years.\n\n    Parameters:\n    li (list): The list of date strings in \"yyyy-mm-dd\" format.\n    v (int): The year to search for in the dates.\n\n    Returns:\n    list: The list of days passed for each date.\n\n    Requirements:\n    - numpy\n    - datetime\n    - dateutil.parser\n\n    Example:\n    >>> calculate_days_since_date(['1972-01-01', '1980-01-01', '1984-01-01'], 1980)\n    \"\"\"\n    days_passed = []\n    for date_str in li:\n        date = parse(date_str)\n        if date.year == v:\n            current_date = datetime.now()\n            days = (current_date - date).days\n            if date.year in LEAP_YEARS:\n                days += 1\n            days_passed.append(days)\n    return days_passed\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ncollections, operator, matplotlib\n```python\nfrom collections import Counter\nimport operator\nimport matplotlib.pyplot as plt\n\ndef plot_most_common_values(li, v):\n    \"\"\"\n    Find the most common values in a list and plot them.\n\n    Parameters:\n    li (list): The list of integers.\n    v (int): The number of most common values to plot.\n\n    Returns:\n    None\n\n    Requirements:\n    - collections\n    - operator\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_most_common_values([1,2,3,3,3,4,5], 3)\n    \"\"\"\n    counter = Counter(li)\n    most_common = dict(sorted(counter.items(), key=operator.itemgetter(1), reverse=True)[:v])\n    plt.bar(most_common.keys(), most_common.values())\n    plt.xlabel('Values')\n    plt.ylabel('Frequency')\n    plt.title('Most Common Values')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, matplotlib, random\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\n\ndef plot_random_walk(li, v):\n    \"\"\"\n    Simulate and plot a random walk based on the indices of a value in a list.\n\n    Parameters:\n    li (list): The list of integers.\n    v (int): The value to search for in the list.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> plot_random_walk([1,2,3,3,3,4,5], 3)\n    \"\"\"\n    indices = [i for i, x in enumerate(li) if x == v]\n    random_walk = np.random.choice(indices, size=100)\n    plt.plot(np.cumsum(random_walk))\n    plt.xlabel('Steps')\n    plt.ylabel('Position')\n    plt.title('Random Walk')\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, scipy.stats, matplotlib\n```python\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef plot_distribution_fit(li, v):\n    \"\"\"\n    Fit a normal distribution to the indices of a value in a list and plot the distribution.\n\n    Parameters:\n    li (list): The list of integers.\n    v (int): The value to search for in the list.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_distribution_fit([1,2,3,3,3,4,5], 3)\n    \"\"\"\n    indices = np.array([i for i, x in enumerate(li) if x == v])\n    mu, std = stats.norm.fit(indices)\n    plt.hist(indices, bins=5, density=True, alpha=0.6, color='g')\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, mu, std)\n    plt.plot(x, p, 'k', linewidth=2)\n    plt.title(\"Fit results: mu = %.2f,  std = %.2f\" % (mu, std))\n    plt.show()\n```\n"}
{"seed": "{\"task_id\": 23246, \"prompt\": \"def f_23246(li, v):\\n\\t\", \"suffix\": \"\\n\\treturn ans\", \"canonical_solution\": \"ans = [ i for i, value in enumerate(li) if value == v]\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate([1,2,3,3,3,4,5], 3) == [2, 3, 4]\\n\"], \"entry_point\": \"f_23246\", \"intent\": \"\\u30ea\\u30b9\\u30c8`li`\\u304b\\u3089\\u691c\\u7d22\\u3059\\u308b\\u5024`v`\\u306b\\u4e00\\u81f4\\u3059\\u308b\\u8981\\u7d20\\u306e\\u30a4\\u30f3\\u30c7\\u30c3\\u30af\\u30b9\\u3092\\u3059\\u3079\\u3066\\u53d6\\u5f97\\u3057\\u3066\\u8868\\u793a\\u3059\\u308b\", \"library\": []}", "generation": "Scenario 1: \nnumpy, matplotlib, scipy, random\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.interpolate import interp1d\nfrom random import randint\n\n# Constants\nX = np.linspace(0, 1, num=100, endpoint=True)\nY = np.cos(-X**2)\n\ndef find_interpolation_indices(Y, v, kind='linear'):\n    \"\"\"\n    For a given value 'v', find the interpolated indices in the cosine function and plot the function and the interpolated points.\n\n    Parameters:\n    Y (numpy array): The Y values of the cosine function.\n    v (float): The value to find the interpolated indices.\n\n    Returns:\n    list: The indices of the interpolated points.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - scipy.interpolate\n    - random\n\n    Example:\n    >>> find_interpolation_indices(Y, 0.5)\n    \"\"\"\n    f = interp1d(Y, X, kind)\n    interpolated_indices = f(v)\n    plt.plot(X, Y)\n    plt.plot(interpolated_indices, [v]*len(interpolated_indices), 'ro') \n    plt.show()\n\n    return list(interpolated_indices)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2: \npandas, numpy, matplotlib, random\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef generate_df_plot_random_column(n, v):\n    \"\"\"\n    Generate a pandas DataFrame of size 'n' with random integer values, find the column with maximum sum, \n    and plot the values greater than 'v' in that column.\n\n    Parameters:\n    n (int): The size of the DataFrame.\n    v (int): The value to filter the column.\n\n    Returns:\n    str: The name of the column with maximum sum.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> generate_df_plot_random_column(100, 50)\n    \"\"\"\n    df = pd.DataFrame(np.random.randint(0,100,size=(n, 5)), columns=COLUMNS)\n    max_sum_column = df.sum().idxmax()\n    df[df[max_sum_column] > v][max_sum_column].plot(kind='bar')\n    plt.show()\n\n    return max_sum_column\n```\n`GPT_ODEX_BREAK`\n\nScenario 3: \ncollections, itertools, random, datetime\n```python\nimport collections\nfrom itertools import combinations\nfrom random import randint\nfrom datetime import datetime\n\n# Constants\nITEMS = ['apple', 'banana', 'cherry', 'date', 'elderberry']\n\ndef find_combinations(li, v):\n    \"\"\"\n    Generate a list of 'v' random items, find all combinations of the items, then find and return the most common combinations.\n\n    Parameters:\n    li (list): The list of items.\n    v (int): The number of items to select.\n\n    Returns:\n    list: The most common combinations.\n\n    Requirements:\n    - collections\n    - itertools\n    - random\n    - datetime\n\n    Example:\n    >>> find_combinations(ITEMS, 3)\n    \"\"\"\n    selected_items = [li[randint(0, len(li)-1)] for _ in range(v)]\n    combos = list(combinations(selected_items, 2))\n    counter = collections.Counter(combos)\n    most_common_combos = counter.most_common(1)\n    \n    return most_common_combos\n```\n`GPT_ODEX_BREAK`\n\nScenario 4: \nnumpy, matplotlib, random, math\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\nimport math\n\n# Constants\nX = np.linspace(0, 2*math.pi, 100)\n\ndef plot_y_transformed_function(X, v):\n    \"\"\"\n    Generate a sine function, add a random noise of amplitude 'v', and plot both the original and the noisy function.\n\n    Parameters:\n    X (numpy array): The X values of the function.\n    v (float): The amplitude of the noise.\n\n    Returns:\n    None\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - random\n    - math\n\n    Example:\n    >>> plot_y_transformed_function(X, 0.05)\n    \"\"\"\n    Y = np.sin(X)\n    noisy_Y = Y + v * np.random.randn(len(Y))\n\n    plt.figure()\n    plt.plot(X, Y)\n    plt.plot(X, noisy_Y)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5: \nitertools, random, math, datetime\n```python\nimport itertools\nfrom random import randint\nimport math\nfrom datetime import datetime\n\n# Constants\nITEMS = ['apple', 'banana', 'cherry', 'date', 'elderberry']\n\ndef generate_permutations(li, v):\n    \"\"\"\n    Select 'v' random items from the list, generate all permutations of the items, and return the permutations with length equal to 'v'.\n\n    Parameters:\n    li (list): The list of items.\n    v (int): The number of items to select.\n\n    Returns:\n    list: The permutations of length 'v'.\n\n    Requirements:\n    - itertools\n    - random\n    - math\n    - datetime\n\n    Example:\n    >>> generate_permutations(ITEMS, 3)\n    \"\"\"\n    selected_items = [li[randint(0, len(li)-1)] for _ in range(v)]\n    perms = list(itertools.permutations(selected_items, v))\n\n    return perms\n```"}
{"seed": "{\"task_id\": 30088, \"prompt\": \"def f_30088(d, k):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"d.pop(k, None) \", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    d = {\\\"a\\\": 1, \\\"b\\\": 2, \\\"c\\\": 3}\\n    assert candidate(d, \\\"a\\\") == 1\\n\", \"\\n    d = {\\\"a\\\": 1, \\\"b\\\": 2, \\\"c\\\": 3}\\n    assert candidate(d, \\\"b\\\") == 2\\n\", \"\\n    d = {\\\"a\\\": 1, \\\"b\\\": 2, \\\"c\\\": 3}\\n    assert candidate(d, \\\"c\\\") == 3\\n\", \"\\n    d = {\\\"a\\\": 1, \\\"b\\\": 2, \\\"c\\\": 3}\\n    assert candidate(d, \\\"d\\\") == None\\n\"], \"entry_point\": \"f_30088\", \"intent\": \"\\u8f9e\\u66f8\\u578b\\u30aa\\u30d6\\u30b8\\u30a7\\u30af\\u30c8`d`\\u5185\\u306e\\u5b58\\u5728\\u3057\\u306a\\u3044\\u53ef\\u80fd\\u6027\\u304c\\u3042\\u308b\\u30ad\\u30fc`k`\\u3092\\u524a\\u9664\\u3059\\u308b\", \"library\": []}", "generation": "Scenario 1: \njson, os, collections, time\n```python\nimport json\nimport os\nimport collections\nimport time\n\n# Constants\nDEFAULT_VALUE = 0\n\ndef clean_and_analyze_json_data(json_file_path, key):\n    \"\"\"\n    Loads a JSON file, removes a potentially non-existent key from each item in the JSON data,\n    analyzes the data to find the most common value of a key, and the average value of a key over time.\n    \n    Parameters:\n    json_file_path (str): The path to the JSON file.\n    key (str): The key to remove and analyze.\n\n    Returns:\n    tuple: The most common value of the key, and the average value of the key over time.\n\n    Requirements:\n    - json\n    - os\n    - collections\n    - time\n\n    Example:\n    >>> clean_and_analyze_json_data('data.json', 'temperature')\n    \"\"\"\n    if not os.path.exists(json_file_path):\n        raise FileNotFoundError(f\"No such file: '{json_file_path}'\")\n\n    with open(json_file_path) as f:\n        data = json.load(f)\n\n    values = []\n    timestamps = []\n    for item in data:\n        value = item.pop(key, DEFAULT_VALUE)\n        values.append(value)\n        timestamps.append(time.mktime(time.strptime(item['timestamp'], '%Y-%m-%d %H:%M:%S')))\n\n    common_value = collections.Counter(values).most_common(1)[0][0]\n    average_value = sum(values) / len(values)\n\n    return common_value, average_value\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nrandom, itertools, math\n```python\nimport random\nimport itertools\nimport math\n\n# Constants\nCHAR_SET = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890'\nKEY_LENGTH = 5\n\ndef generate_and_search_keys(search_key):\n    \"\"\"\n    Generate a set of random keys, then find and return the index of a search key if it exists.\n\n    Parameters:\n    search_key (str): The key to search for.\n\n    Returns:\n    int: The index of the search key if it exists, else -1.\n\n    Requirements:\n    - random\n    - itertools\n    - math\n\n    Example:\n    >>> generate_and_search_keys('abcde')\n    \"\"\"\n    keys = [''.join(random.choices(CHAR_SET, k=KEY_LENGTH)) for _ in range(10000)]\n\n    try:\n        index = keys.index(search_key)\n    except ValueError:\n        index = -1\n\n    return index\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nDEFAULT_VALUE = 0\n\ndef analyze_data_frame(df, column):\n    \"\"\"\n    Analyze a pandas DataFrame for missing values in a column, fill them with a default value,\n    and plot the distribution of values in the column.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame to analyze.\n    column (str): The column to analyze.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib\n\n    Example:\n    >>> df = pd.DataFrame({'a': [1, 2, np.nan, 4, 5]})\n    >>> analyze_data_frame(df, 'a')\n    \"\"\"\n    df[column].fillna(DEFAULT_VALUE, inplace=True)\n    df[column].plot(kind='hist')\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ncsv, collections, matplotlib\n```python\nimport csv\nimport collections\nimport matplotlib.pyplot as plt\n\n# Constants\nDEFAULT_VALUE = 0\n\ndef analyze_csv_data(csv_file_path, column):\n    \"\"\"\n    Load a CSV file, analyze the data to find the most common value in a column,\n    and plot the distribution of values in the column.\n\n    Parameters:\n    csv_file_path (str): The path to the CSV file.\n    column (str): The column to analyze.\n\n    Returns:\n    str: The most common value in the column.\n\n    Requirements:\n    - csv\n    - collections\n    - matplotlib\n\n    Example:\n    >>> analyze_csv_data('data.csv', 'age')\n    \"\"\"\n    with open(csv_file_path) as f:\n        reader = csv.DictReader(f)\n        data = [row[column] for row in reader]\n\n    common_value = collections.Counter(data).most_common(1)[0][0]\n\n    plt.hist(data, bins=20)\n    plt.show()\n\n    return common_value\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nrandom, string, hashlib\n```python\nimport random\nimport string\nimport hashlib\n\n# Constants\nPASSWORD_LENGTH = 10\nSALT = 'salt'\n\ndef generate_and_hash_password():\n    \"\"\"\n    Generate a random password, hash it with a salt, and return the password and hash.\n\n    Returns:\n    tuple: The generated password and its hash.\n\n    Requirements:\n    - random\n    - string\n    - hashlib\n\n    Example:\n    >>> generate_and_hash_password()\n    \"\"\"\n    password = ''.join(random.choices(string.ascii_letters + string.digits, k=PASSWORD_LENGTH))\n    password_hash = hashlib.sha256((password + SALT).encode()).hexdigest()\n\n    return password, password_hash\n```\n"}
{"seed": "{\"task_id\": 41700, \"prompt\": \"def f_41700(df, col_label):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"df.drop_duplicates(subset=col_label)\", \"test_start\": \"\\nimport pandas as pd\\n\\ndef check(candidate):\", \"test\": [\"\\n    d1 = {'A': [1, 1, 1, 2], 'B': [2, 2, 2, 3], 'C': [3, 3, 4, 5], 'D' : [1, 2, 3, 3]}\\n    source_df = pd.DataFrame(d1)\\n\\n    d2 = {'A': [1, 1, 1], 'B': [2, 2, 2], 'C': [3, 3, 4], 'D' : [1, 2, 3]}\\n    res = pd.DataFrame(d2)\\n    \\n    assert candidate(source_df, ['D']).equals(res)\\n\"], \"entry_point\": \"f_41700\", \"intent\": \"\\u30c7\\u30fc\\u30bf\\u30d5\\u30ec\\u30fc\\u30e0`df`\\u5185\\u306e\\u5217`col_label`\\u304c\\u91cd\\u8907\\u3057\\u3066\\u3044\\u308b\\u884c\\u3092\\u524a\\u9664\\u3059\\u308b\\uff08pandas)\", \"library\": [\"pandas\"]}", "generation": "Scenario 1:\npandas, numpy, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nAGE_RANGES = ['10-20', '21-30', '31-40', '41-50', '51-60', '61-70', '71-80']\nGENDERS = ['Male', 'Female']\n\ndef analyze_demographics(df, age_col, gender_col):\n    \"\"\"\n    Generate a bar chart showing the number of males and females in different age ranges \n    based on the given DataFrame.\n    \n    Parameters:\n    df (DataFrame): The pandas DataFrame containing the data.\n    age_col (str): The name of the column containing age data.\n    gender_col (str): The name of the column containing gender data.\n    \n    Returns:\n    None: This function does not return a value. It generates a plot.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    \n    Example:\n    >>> df = pd.DataFrame({\n            'Age': np.random.randint(10, 81, 1000),\n            'Gender': np.random.choice(['Male', 'Female'], 1000)\n            })\n    >>> analyze_demographics(df, 'Age', 'Gender')\n    \"\"\"\n    df['Age Range'] = pd.cut(df[age_col], bins=np.arange(10, 81, 10), labels=AGE_RANGES)\n\n    gender_counts = df.groupby([gender_col, 'Age Range']).size().unstack(gender_col)\n\n    gender_counts.plot(kind='bar', stacked=True)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n\ndef analyze_sales_per_region(df, region_col, sales_col):\n    \"\"\"\n    Generate a boxplot showing the distribution of sales in different regions based \n    on the given DataFrame.\n    \n    Parameters:\n    df (DataFrame): The pandas DataFrame containing the data.\n    region_col (str): The name of the column containing region data.\n    sales_col (str): The name of the column containing sales data.\n    \n    Returns:\n    None: This function does not return a value. It generates a plot.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - seaborn\n    \n    Example:\n    >>> df = pd.DataFrame({\n            'Region': np.random.choice(['North', 'South', 'East', 'West'], 1000),\n            'Sales': np.random.randint(1000, 5000, 1000)\n            })\n    >>> analyze_sales_per_region(df, 'Region', 'Sales')\n    \"\"\"\n    sns.boxplot(x=region_col, y=sales_col, data=df)\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, sklearn, matplotlib\n```python\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom matplotlib import pyplot as plt\n\ndef standardize_and_plot(df, numerical_cols):\n    \"\"\"\n    Standardize the numerical columns of the DataFrame and plot the \n    distribution of each column before and after standardization.\n    \n    Parameters:\n    df (DataFrame): The pandas DataFrame containing the data.\n    numerical_cols (list): The list of numerical columns to be standardized.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with standardized columns.\n    \n    Requirements:\n    - pandas\n    - sklearn.preprocessing.StandardScaler\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame({\n            'Age': np.random.randint(10, 81, 1000),\n            'Sales': np.random.randint(1000, 5000, 1000)\n            })\n    >>> df_standardized = standardize_and_plot(df, ['Age', 'Sales'])\n    \"\"\"\n    scaler = StandardScaler()\n\n    for col in numerical_cols:\n        df[col].plot(kind='hist', title=f'Before Standardization - {col}')\n        plt.show()\n\n        df[col] = scaler.fit_transform(df[[col]])\n\n        df[col].plot(kind='hist', title=f'After Standardization - {col}')\n        plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, sklearn\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\n\ndef encode_categorical_columns(df, categorical_cols):\n    \"\"\"\n    Encode the categorical columns of the DataFrame using LabelEncoder.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame containing the data.\n    categorical_cols (list): The list of categorical columns to be encoded.\n\n    Returns:\n    DataFrame: A pandas DataFrame with encoded columns.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.preprocessing.LabelEncoder\n\n    Example:\n    >>> df = pd.DataFrame({\n            'Color': np.random.choice(['Red', 'Green', 'Blue'], 1000),\n            'Size': np.random.choice(['Small', 'Medium', 'Large'], 1000)\n            })\n    >>> df_encoded = encode_categorical_columns(df, ['Color', 'Size'])\n    \"\"\"\n    encoder = LabelEncoder()\n\n    for col in categorical_cols:\n        df[col] = encoder.fit_transform(df[col])\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, datetime\n```python\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\n\ndef calculate_age(df, dob_col):\n    \"\"\"\n    Calculate the age of individuals based on their date of birth.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame containing the data.\n    dob_col (str): The name of the column containing date of birth data.\n\n    Returns:\n    DataFrame: A pandas DataFrame with an additional column for age.\n\n    Requirements:\n    - pandas\n    - numpy\n    - datetime\n\n    Example:\n    >>> df = pd.DataFrame({\n            'DOB': pd.date_range(start='1950-01-01', end='2000-12-31', periods=1000)\n            })\n    >>> df = calculate_age(df, 'DOB')\n    \"\"\"\n    current_year = datetime.now().year\n\n    df['Age'] = current_year - df[dob_col].dt.year\n\n    return df\n```"}
{"seed": "{\"task_id\": 30824, \"prompt\": \"def f_30824():\\n\\t\", \"suffix\": \"\\n\\treturn previous_month\", \"canonical_solution\": \"today = datetime.date.today()\\n\\tprevious_month = today - dateutil.relativedelta.relativedelta(months=1)\", \"test_start\": \"\\nimport datetime\\nimport dateutil\\n\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate() == datetime.date.today() - dateutil.relativedelta.relativedelta(months=1)\\n\"], \"entry_point\": \"f_30824\", \"intent\": \"\\u4eca\\u65e5\\u304b\\u3089\\u4e00\\u30f6\\u6708\\u524d\\u306e\\u65e5\\u4ed8\\u3092\\u53d6\\u5f97\\u3059\\u308b\", \"library\": [\"datetime\", \"dateutil\"]}", "generation": "Scenario 1: \npandas, datetime, dateutil, matplotlib, random\n```python\nimport pandas as pd\nfrom datetime import datetime\nfrom dateutil.relativedelta import relativedelta\nimport matplotlib.pyplot as plt\nfrom random import randint\n\n# Constants\nCITIES = ['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']\nWEATHER_CONDITIONS = ['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy']\n\ndef generate_weather_report_past_month():\n    \"\"\"\n    Generate a report of weather conditions for a list of cities for the past month \n    with random weather conditions.\n    \n    Returns:\n    DataFrame: A pandas DataFrame with weather conditions for the cities.\n    \n    Requirements:\n    - pandas\n    - datetime\n    - dateutil.relativedelta\n    - matplotlib.pyplot\n    - random\n    \n    Example:\n    >>> report = generate_weather_report_past_month()\n    >>> print(report)\n    >>> report['Weather Condition'].value_counts().plot(kind='bar')\n    \"\"\"\n    report_data = []\n\n    for day in range(30):\n        date = datetime.today() - relativedelta(days=day)\n        for city in CITIES:\n            weather = WEATHER_CONDITIONS[randint(0, len(WEATHER_CONDITIONS)-1)]\n            report_data.append([city, date, weather])\n\n    report_df = pd.DataFrame(report_data, columns=['City', 'Date', 'Weather Condition'])\n\n    return report_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ndatetime, dateutil, numpy, math\n```python\nfrom datetime import datetime\nfrom dateutil.relativedelta import relativedelta\nimport numpy as np\nimport math\n\n# Constants\nPI = np.pi\n\ndef calculate_sin_cos_past_month():\n    \"\"\"\n    Calculate the sin and cos of the number of days passed since a month ago to \n    today, and return their sum.\n    \n    Returns:\n    float: The sum of sin and cos of the number of days passed since a month ago.\n    \n    Requirements:\n    - datetime\n    - dateutil.relativedelta\n    - numpy\n    - math\n    \n    Example:\n    >>> calculate_sin_cos_past_month()\n    \"\"\"\n    today = datetime.today()\n    month_ago = today - relativedelta(months=1)\n    days_passed = (today - month_ago).days\n\n    sin_days = math.sin(days_passed)\n    cos_days = math.cos(days_passed)\n\n    return sin_days + cos_days\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ndatetime, dateutil, pytz, requests\n```python\nfrom datetime import datetime\nfrom dateutil.relativedelta import relativedelta\nimport pytz\nimport requests\n\n# Constants\nAPI = \"http://worldtimeapi.org/api/timezone/\"\n\ndef fetch_time_difference(city):\n    \"\"\"\n    Fetch the current time of a city from an API and calculate the difference \n    between the current time in that city and a month ago in local time.\n    \n    Parameters:\n    city (str): The city for which to fetch the time.\n    \n    Returns:\n    int: The difference in seconds.\n    \n    Requirements:\n    - datetime\n    - dateutil.relativedelta\n    - pytz\n    - requests\n    \n    Example:\n    >>> fetch_time_difference('Europe/London')\n    \"\"\"\n    response = requests.get(API + city)\n    city_time = datetime.fromtimestamp(response.json()['unixtime'])\n\n    month_ago = datetime.now(pytz.timezone(city)) - relativedelta(months=1)\n    diff = city_time - month_ago\n\n    return diff.total_seconds()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ndatetime, dateutil, os, hashlib\n```python\nfrom datetime import datetime\nfrom dateutil.relativedelta import relativedelta\nimport os\nimport hashlib\n\n# Constants\nFOLDER_PATH = \"/path/to/folder\"\n\ndef hash_files_modified_past_month():\n    \"\"\"\n    Hash the content of all files in a directory that were modified within \n    the past month.\n    \n    Returns:\n    dict: A dictionary with file names as keys and their hashes as values.\n    \n    Requirements:\n    - datetime\n    - dateutil.relativedelta\n    - os\n    - hashlib\n    \n    Example:\n    >>> hash_files_modified_past_month()\n    \"\"\"\n    files_hash = {}\n    month_ago = datetime.now() - relativedelta(months=1)\n\n    for filename in os.listdir(FOLDER_PATH):\n        filepath = os.path.join(FOLDER_PATH, filename)\n        if datetime.fromtimestamp(os.path.getmtime(filepath)) > month_ago:\n            with open(filepath, 'rb') as f:\n                files_hash[filename] = hashlib.sha256(f.read()).hexdigest()\n\n    return files_hash\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\ndatetime, dateutil, pandas, numpy\n```python\nfrom datetime import datetime\nfrom dateutil.relativedelta import relativedelta\nimport pandas as pd\nimport numpy as np\n\n# Constants\nDATA = {\n    'date': pd.date_range(start='1/1/2020', end='1/1/2023'),\n    'value': np.random.randint(0, high=100, size=1096),\n}\nDF = pd.DataFrame(DATA)\n\ndef calculate_mean_past_month():\n    \"\"\"\n    Calculate the mean of a column in a DataFrame for the past month.\n    \n    Returns:\n    float: The mean value.\n    \n    Requirements:\n    - datetime\n    - dateutil.relativedelta\n    - pandas\n    - numpy\n    \n    Example:\n    >>> calculate_mean_past_month()\n    \"\"\"\n    month_ago = datetime.now() - relativedelta(months=1)\n    past_month_data = DF.loc[(DF['date'] > month_ago) & (DF['date'] <= datetime.now())]\n\n    return past_month_data['value'].mean()\n```"}
{"seed": "{\"task_id\": 42011, \"prompt\": \"def f_42011(str):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"str.strip()\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    assert candidate(\\\"  hello  \\\") == \\\"hello\\\"\\n\", \"\\n    assert candidate(\\\"  hello world !  \\\") == \\\"hello world !\\\"\\n\", \"\\n    assert candidate(\\\"hello\\\") == \\\"hello\\\"\\n\", \"\\n    assert candidate(\\\"\\\") == \\\"\\\"\\n\"], \"entry_point\": \"f_42011\", \"intent\": \"\\u6587\\u5b57\\u5217`str`\\u304b\\u3089\\u7a7a\\u767d\\u3068\\u6539\\u884c\\u3092\\u53d6\\u308a\\u9664\\u304f\", \"library\": []}", "generation": "Scenario 1: \npandas, os, re, urllib\n```python\nimport pandas as pd\nimport os\nimport re\nimport urllib.request\n\ndef download_and_clean_csv_data(url, output_dir):\n    \"\"\"\n    Download a CSV file from a given URL, clean the data by removing leading and trailing spaces \n    in all the columns, and save the cleaned data to a specified directory.\n\n    Parameters:\n    url (str): The URL of the CSV file.\n    output_dir (str): The directory where the cleaned CSV file should be saved.\n\n    Returns:\n    str: The full path of the cleaned CSV file.\n\n    Requirements:\n    - pandas\n    - os\n    - re\n    - urllib\n    \n    Example:\n    >>> download_and_clean_csv_data('http://example.com/data.csv', '/path/to/output')\n    \"\"\"\n    filename = re.search(r'/([^/]*\\.csv)$', url).group(1)\n    output_file = os.path.join(output_dir, filename)\n\n    urllib.request.urlretrieve(url, output_file)\n\n    df = pd.read_csv(output_file)\n    df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n    df.to_csv(output_file, index=False)\n\n    return output_file\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\nnumpy, pandas, sklearn, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\n# Constants\nFEATURES = ['feature1', 'feature2', 'feature3']\n\ndef standardize_and_plot(df):\n    \"\"\"\n    Standardize the numerical features in a given pandas DataFrame and plot the \n    distributions of the original and standardized features.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame containing the numerical features.\n\n    Returns:\n    DataFrame: The standardized DataFrame.\n\n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.preprocessing.StandardScaler\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame({\n    ...     'feature1': np.random.normal(0, 1, 1000),\n    ...     'feature2': np.random.normal(0, 2, 1000),\n    ...     'feature3': np.random.normal(0, 3, 1000)\n    ... })\n    >>> standardized_df = standardize_and_plot(df)\n    \"\"\"\n    scaler = StandardScaler()\n\n    df_standardized = df.copy()\n    df_standardized[FEATURES] = scaler.fit_transform(df[FEATURES])\n\n    for feature in FEATURES:\n        plt.figure(figsize=(10, 4))\n\n        plt.subplot(121)\n        plt.hist(df[feature], bins=20, alpha=0.5, color='g', label='original')\n        plt.title('Original ' + feature)\n        \n        plt.subplot(122)\n        plt.hist(df_standardized[feature], bins=20, alpha=0.5, color='b', label='standardized')\n        plt.title('Standardized ' + feature)\n\n        plt.show()\n\n    return df_standardized\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\nnumpy, pandas, sklearn, seaborn\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.decomposition import PCA\nimport seaborn as sns\n\ndef apply_pca_and_visualize(df, n_components):\n    \"\"\"\n    Apply Principal Component Analysis (PCA) to a given pandas DataFrame and \n    visualize the data in the transformed space using a scatter plot.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n    n_components (int): The number of principal components.\n\n    Returns:\n    DataFrame: The DataFrame in the transformed space.\n\n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.decomposition.PCA\n    - seaborn\n\n    Example:\n    >>> df = pd.DataFrame(np.random.rand(100, 5), columns=list('ABCDE'))\n    >>> transformed_df = apply_pca_and_visualize(df, 2)\n    \"\"\"\n    pca = PCA(n_components=n_components)\n    transformed = pca.fit_transform(df)\n    transformed_df = pd.DataFrame(transformed, columns=[f'PC{i+1}' for i in range(n_components)])\n\n    sns.scatterplot(data=transformed_df, x='PC1', y='PC2')\n\n    return transformed_df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nre, nltk, pandas, collections\n```python\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nimport pandas as pd\nfrom collections import Counter\n\n# Constants\nSTOPWORDS = set(stopwords.words('english'))\n\ndef count_non_stop_words(text):\n    \"\"\"\n    Count the occurrence of non-stop words in a given text.\n\n    Parameters:\n    text (str): The text.\n\n    Returns:\n    dict: A dictionary with non-stop words as keys and counts as values.\n\n    Requirements:\n    - re\n    - nltk\n    - pandas\n    - collections.Counter\n\n    Example:\n    >>> count_non_stop_words(\"This is a simple text for testing.\")\n    \"\"\"\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    non_stop_words = [word for word in words if word not in STOPWORDS]\n    word_counts = dict(Counter(non_stop_words))\n\n    return word_counts\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, pandas, matplotlib, sklearn\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n# Constants\nFEATURES = ['feature1', 'feature2', 'feature3']\nTARGET = 'target'\n\ndef fit_linear_regression(df):\n    \"\"\"\n    Fit a linear regression model to a given pandas DataFrame and plot the \n    target variable against the predicted values.\n\n    Parameters:\n    df (DataFrame): The pandas DataFrame.\n\n    Returns:\n    LinearRegression: The fitted model.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - sklearn.linear_model.LinearRegression\n\n    Example:\n    >>> df = pd.DataFrame(np.random.rand(100, 4), columns=FEATURES + [TARGET])\n    >>> model = fit_linear_regression(df)\n    \"\"\"\n    model = LinearRegression()\n    model.fit(df[FEATURES], df[TARGET])\n\n    predictions = model.predict(df[FEATURES])\n\n    plt.scatter(df[TARGET], predictions)\n    plt.xlabel('True Values')\n    plt.ylabel('Predictions')\n\n    return model\n```"}
{"seed": "{\"task_id\": 23218, \"prompt\": \"def f_23218(data, fs):\\n\\t\", \"suffix\": \"\\n\\treturn plot\", \"canonical_solution\": \"\\n\\tf, t, Sxx = signal.spectrogram(data, fs)\\n\\tplot = plt.pcolormesh(t, f, Sxx)\\n\", \"test_start\": \"\\nfrom scipy import signal\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\ndef check(candidate):\", \"test\": [\"\\n    data = np.array([1, 4, 6])\\n    assert str(type(candidate(data, 1.0))).split(' ')[1] == \\\"'matplotlib.collections.QuadMesh'>\\\"\\n\"], \"entry_point\": \"f_23218\", \"intent\": \"\\u4fe1\\u53f7\\u30c7\\u30fc\\u30bf`data`\\u3092\\u30b5\\u30f3\\u30d7\\u30ea\\u30f3\\u30b0\\u5468\\u6ce2\\u6570`fs`\\u3067\\u5468\\u6ce2\\u6570\\u89e3\\u6790\\u3057\\u30b9\\u30da\\u30af\\u30c8\\u30ed\\u30b0\\u30e9\\u30e0\\u3092\\u8868\\u793a\\u3059\\u308b\", \"library\": [\"matplotlib\", \"numpy\", \"scipy\"]}", "generation": "Scenario 1:\nscipy, numpy, matplotlib, pandas\n```python\nfrom scipy import signal\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants\nSAMPLE_RATE = 1.0\n\ndef plot_spectrogram_and_save(dataframe, column_name, filename):\n    \"\"\"\n    Perform spectral analysis on a given column of a pandas DataFrame and save the spectrogram to a file.\n    \n    Parameters:\n    dataframe (DataFrame): The pandas DataFrame containing the data.\n    column_name (str): The name of the column to analyze.\n    filename (str): The name of the file to save the spectrogram to.\n\n    Returns:\n    None\n\n    Requirements:\n    - scipy\n    - numpy\n    - matplotlib.pyplot\n    - pandas\n\n    Example:\n    >>> df = pd.DataFrame({'data': [1, 4, 6, 7, 8, 9]})\n    >>> plot_spectrogram_and_save(df, 'data', 'spectrogram.png')\n    \"\"\"\n    data = dataframe[column_name].to_numpy()\n    f, t, Sxx = signal.spectrogram(data, SAMPLE_RATE)\n    plt.pcolormesh(t, f, Sxx)\n    plt.ylabel('Frequency [Hz]')\n    plt.xlabel('Time [sec]')\n    plt.savefig(filename)\n\n    return None\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, sklearn, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\n\ndef plot_linear_regression(df, feature_col, target_col):\n    \"\"\"\n    Performs a linear regression on the given feature and target columns of a DataFrame and \n    plots the resulting model.\n    \n    Parameters:\n    df (pandas.DataFrame): The DataFrame containing the data.\n    feature_col (str): The name of the feature column.\n    target_col (str): The name of the target column.\n\n    Returns:\n    LinearRegression: The trained linear regression model.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.linear_model.LinearRegression\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame({'feature': [1, 2, 3, 4, 5], 'target': [2, 4, 6, 8, 10]})\n    >>> model = plot_linear_regression(df, 'feature', 'target')\n    \"\"\"\n    X = df[[feature_col]]\n    y = df[target_col]\n\n    model = LinearRegression()\n    model.fit(X, y)\n\n    plt.scatter(X, y, color='b')\n    plt.plot(X, model.predict(X), color='r')\n\n    plt.xlabel(feature_col)\n    plt.ylabel(target_col)\n    plt.title('Linear Regression')\n\n    return model\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, scipy, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef plot_and_test_normality(df, column):\n    \"\"\"\n    Plots a histogram of the given column of a DataFrame and performs a normality test.\n    \n    Parameters:\n    df (pandas.DataFrame): The DataFrame containing the data.\n    column (str): The column to test.\n\n    Returns:\n    tuple: A tuple containing the p-value of the normality test and the plotted histogram.\n\n    Requirements:\n    - pandas\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame({'column': np.random.normal(size=100)})\n    >>> p_value, hist = plot_and_test_normality(df, 'column')\n    \"\"\"\n    data = df[column]\n    _, p_value = stats.normaltest(data)\n\n    plt.hist(data, bins='auto', color='b', alpha=0.7)\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.title('Histogram')\n\n    return p_value, plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\nnumpy, scipy, matplotlib, math\n```python\nimport numpy as np\nfrom scipy.integrate import quad\nimport matplotlib.pyplot as plt\nimport math\n\ndef plot_integral_of_sine(freq):\n    \"\"\"\n    Calculate the integral of a sine function with a given frequency from 0 to 2pi, and plot the sine function.\n    \n    Parameters:\n    freq (float): The frequency of the sine function.\n\n    Returns:\n    tuple: A tuple containing the result of the integral and the plotted sine function.\n\n    Requirements:\n    - numpy\n    - scipy.integrate\n    - matplotlib.pyplot\n    - math\n\n    Example:\n    >>> integral, plot = plot_integral_of_sine(1.0)\n    \"\"\"\n    sine_func = lambda x: np.sin(freq * x)\n    integral, _ = quad(sine_func, 0, 2 * math.pi)\n\n    x = np.linspace(0, 2 * math.pi, 1000)\n    y = sine_func(x)\n\n    plt.plot(x, y, label='sin({}x)'.format(freq))\n    plt.legend()\n\n    return integral, plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\nnumpy, pandas, sklearn, matplotlib\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\n# Constants\nN_CLUSTERS = 3\n\ndef perform_kmeans_and_plot(df, feature_cols):\n    \"\"\"\n    Performs KMeans clustering on the given feature columns of a DataFrame and plots the resulting clusters.\n    \n    Parameters:\n    df (pandas.DataFrame): The DataFrame containing the data.\n    feature_cols (list[str]): The list of feature column names.\n\n    Returns:\n    KMeans: The trained KMeans model.\n\n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.cluster.KMeans\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame({'feature1': np.random.normal(size=100), 'feature2': np.random.normal(size=100)})\n    >>> model = perform_kmeans_and_plot(df, ['feature1', 'feature2'])\n    \"\"\"\n    X = df[feature_cols]\n    model = KMeans(n_clusters=N_CLUSTERS)\n    model.fit(X)\n\n    plt.scatter(df[feature_cols[0]], df[feature_cols[1]], c=model.labels_)\n\n    return model\n```\n"}
{"seed": "{\"task_id\": 39255, \"prompt\": \"def f_39255(sjis_str):\\n\\treturn \", \"suffix\": \"\", \"canonical_solution\": \"sjis_str.decode('shift-jis')\", \"test_start\": \"\\ndef check(candidate):\", \"test\": [\"\\n    sjis_str = b'Wall'\\n    assert candidate(sjis_str) == 'Wall'\\n\"], \"entry_point\": \"f_39255\", \"intent\": \"Shift_JIS\\u3067\\u7b26\\u53f7\\u5316\\u3055\\u308c\\u305f\\u6587\\u5b57\\u5217`sjis_str`\\u3092\\u30c7\\u30b3\\u30fc\\u30c9\\u3059\\u308b\", \"library\": []}", "generation": "Scenario 1:\nos, codecs, shutil, zipfile, glob\n```python\nimport os\nimport codecs\nimport shutil\nimport zipfile\nimport glob\n\ndef extract_decode_shiftjis_zip(zip_path, extract_path):\n    \"\"\"\n    Extracts a zip file containing Shift-JIS encoded text files, decodes them and stores them in UTF-8 format.\n\n    Parameters:\n    zip_path (str): The path of the zip file.\n    extract_path (str): The path where the extracted and decoded files should be stored.\n\n    Requirements:\n    - os\n    - codecs\n    - shutil\n    - zipfile\n    - glob\n\n    Example:\n    >>> extract_decode_shiftjis_zip('shiftjis_files.zip', 'decoded_files')\n    \"\"\"\n    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n        zip_ref.extractall(extract_path)\n\n    for filename in glob.glob(os.path.join(extract_path, \"*.txt\")):\n        with codecs.open(filename, 'r', encoding='shift_jis') as f_in:\n            content = f_in.read()\n\n        with codecs.open(filename, 'w', encoding='utf-8') as f_out:\n            f_out.write(content)\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\ncodecs, os, smtplib, email.mime.text, email.mime.multipart\n```python\nimport codecs\nimport os\nimport smtplib\nfrom email.mime.text import MIMEText\nfrom email.mime.multipart import MIMEMultipart\n\nSMTP_SERVER = 'smtp.gmail.com'\nSMTP_PORT = 587\n\ndef send_shiftjis_email(sjis_file, subject, recipient, sender, password):\n    \"\"\"\n    Sends an email with the decoded content of a Shift-JIS encoded file as the body.\n\n    Parameters:\n    sjis_file (str): The path of the Shift-JIS encoded file.\n    subject (str): The subject of the email.\n    recipient (str): The recipient of the email.\n    sender (str): The sender of the email.\n    password (str): The password of the sender's email account.\n\n    Requirements:\n    - codecs\n    - os\n    - smtplib\n    - email.mime.text\n    - email.mime.multipart\n\n    Example:\n    >>> send_shiftjis_email('shiftjis_file.txt', 'Test Email', 'recipient@gmail.com', 'sender@gmail.com', 'password')\n    \"\"\"\n    with codecs.open(sjis_file, 'r', encoding='shift_jis') as f:\n        content = f.read()\n\n    msg = MIMEMultipart()\n    msg['From'] = sender\n    msg['To'] = recipient\n    msg['Subject'] = subject\n    msg.attach(MIMEText(content, 'plain', 'utf-8'))\n\n    with smtplib.SMTP(SMTP_SERVER, SMTP_PORT) as server:\n        server.starttls()\n        server.login(sender, password)\n        server.send_message(msg)\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\ncodecs, os, pandas, matplotlib.pyplot\n```python\nimport codecs\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef analyze_shiftjis_csv(sjis_file):\n    \"\"\"\n    Reads a Shift-JIS encoded CSV file into a pandas DataFrame and plots a histogram of the values in the first column.\n\n    Parameters:\n    sjis_file (str): The path of the Shift-JIS encoded CSV file.\n\n    Requirements:\n    - codecs\n    - os\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> analyze_shiftjis_csv('shiftjis_file.csv')\n    \"\"\"\n    with codecs.open(sjis_file, 'r', encoding='shift_jis') as f:\n        df = pd.read_csv(f)\n    df.iloc[:,0].hist()\n    plt.show()\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\ncodecs, os, sqlite3, pandas\n```python\nimport codecs\nimport os\nimport sqlite3\nimport pandas as pd\n\ndef load_shiftjis_csv_to_sqlite(sjis_file, db_path, table_name):\n    \"\"\"\n    Reads a Shift-JIS encoded CSV file into a pandas DataFrame and stores it in a SQLite database.\n\n    Parameters:\n    sjis_file (str): The path of the Shift-JIS encoded CSV file.\n    db_path (str): The path of the SQLite database.\n    table_name (str): The name of the table to store the data in.\n\n    Requirements:\n    - codecs\n    - os\n    - sqlite3\n    - pandas\n\n    Example:\n    >>> load_shiftjis_csv_to_sqlite('shiftjis_file.csv', 'database.db', 'table')\n    \"\"\"\n    with codecs.open(sjis_file, 'r', encoding='shift_jis') as f:\n        df = pd.read_csv(f)\n    conn = sqlite3.connect(db_path)\n    df.to_sql(table_name, conn, if_exists='replace', index=False)\n    conn.close()\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\ncodecs, os, numpy, matplotlib.pyplot\n```python\nimport codecs\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_shiftjis_txt(sjis_file):\n    \"\"\"\n    Reads a Shift-JIS encoded text file containing numeric data separated by newlines and plots the data.\n\n    Parameters:\n    sjis_file (str): The path of the Shift-JIS encoded text file.\n\n    Requirements:\n    - codecs\n    - os\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> plot_shiftjis_txt('shiftjis_file.txt')\n    \"\"\"\n    with codecs.open(sjis_file, 'r', encoding='shift_jis') as f:\n        data = np.array([float(line.rstrip()) for line in f.readlines()])\n    plt.plot(data)\n    plt.show()\n```"}
{"seed": "{\"task_id\": 41519, \"prompt\": \"def f_41519(df, col):\\n\\t\", \"suffix\": \"\\n\\treturn df\", \"canonical_solution\": \"df[col] = df[col].astype(str)\", \"test_start\": \"\\nimport pandas as pd\\n\\ndef check(candidate):\", \"test\": [\"\\n    data = [1, 4, 5]\\n    df = pd.DataFrame(data, columns=['Vals'])\\n    candidate(df, 'Vals')\\n    assert str(type(df['Vals'][0])) == \\\"<class 'str'>\\\"\\n\"], \"entry_point\": \"f_41519\", \"intent\": \"\\u30c7\\u30fc\\u30bf\\u30d5\\u30ec\\u30fc\\u30e0`df`\\u306e\\u5217`col`\\u3092\\u6587\\u5b57\\u5217\\u578b\\u306b\\u5909\\u66f4\\u3059\\u308b\", \"library\": [\"pandas\"]}", "generation": "Scenario 1:\npandas, numpy, datetime, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\n\n# Constants\nSTART_DATE = datetime(2020, 1, 1)\nEND_DATE = datetime(2022, 12, 31)\n\ndef generate_and_plot_random_data(df, col):\n    \"\"\"\n    Generate a random time series data within a given date range, add it as a new column \n    in the given DataFrame, and plot the time series.\n\n    Parameters:\n    df (DataFrame): The DataFrame to add the new column to.\n    col (str): The name of the new column.\n\n    Returns:\n    DataFrame: The DataFrame with the new column.\n\n    Requirements:\n    - pandas\n    - numpy\n    - datetime\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame()\n    >>> df = generate_and_plot_random_data(df, 'Random')\n    >>> print(df.head())\n    \"\"\"\n    dates = pd.date_range(START_DATE, END_DATE)\n    data = np.random.randn(len(dates))\n    df[col] = pd.Series(data, index=dates)\n\n    df[col].plot(figsize=(10,5))\n    plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 2:\npandas, numpy, seaborn, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nNUM_SAMPLES = 1000\nCATEGORIES = ['Category 1', 'Category 2', 'Category 3']\n\ndef generate_and_plot_categorical_data(df, col):\n    \"\"\"\n    Generate a random categorical data, add it as a new column in the given DataFrame, \n    and plot the distribution of categories.\n\n    Parameters:\n    df (DataFrame): The DataFrame to add the new column to.\n    col (str): The name of the new column.\n\n    Returns:\n    DataFrame: The DataFrame with the new column.\n\n    Requirements:\n    - pandas\n    - numpy\n    - seaborn\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame()\n    >>> df = generate_and_plot_categorical_data(df, 'Category')\n    >>> print(df.head())\n    \"\"\"\n    data = np.random.choice(CATEGORIES, NUM_SAMPLES)\n    df[col] = pd.Series(data)\n\n    sns.countplot(x=col, data=df)\n    plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 3:\npandas, numpy, sklearn.preprocessing, matplotlib\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\n\n# Constants\nNUM_SAMPLES = 1000\n\ndef generate_and_normalize_data(df, col):\n    \"\"\"\n    Generate a random numerical data, add it as a new column in the given DataFrame, \n    normalize the data using MinMaxScaler and plot the data before and after normalization.\n\n    Parameters:\n    df (DataFrame): The DataFrame to add the new column to.\n    col (str): The name of the new column.\n\n    Returns:\n    DataFrame: The DataFrame with the new column.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn.preprocessing\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame()\n    >>> df = generate_and_normalize_data(df, 'Data')\n    >>> print(df.head())\n    \"\"\"\n    data = np.random.randn(NUM_SAMPLES)\n    df[col] = pd.Series(data)\n\n    plt.hist(df[col], bins=30)\n    plt.title('Before Normalization')\n    plt.show()\n\n    scaler = MinMaxScaler()\n    df[col] = scaler.fit_transform(df[[col]])\n\n    plt.hist(df[col], bins=30)\n    plt.title('After Normalization')\n    plt.show()\n\n    return df\n```\n`GPT_ODEX_BREAK`\n\nScenario 4:\npandas, numpy, datetime, sklearn.linear_model\n```python\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nfrom sklearn.linear_model import LinearRegression\n\n# Constants\nSTART_DATE = datetime(2020, 1, 1)\nEND_DATE = datetime(2022, 12, 31)\n\ndef generate_and_predict_data(df, col):\n    \"\"\"\n    Generate a random time series data within a given date range, add it as a new column \n    in the DataFrame. Train a linear regression model on this data and predict the value \n    for the next day.\n\n    Parameters:\n    df (DataFrame): The DataFrame to add the new column to.\n    col (str): The name of the new column.\n\n    Returns:\n    float: The predicted value.\n\n    Requirements:\n    - pandas\n    - numpy\n    - datetime\n    - sklearn.linear_model\n\n    Example:\n    >>> df = pd.DataFrame()\n    >>> prediction = generate_and_predict_data(df, 'Data')\n    >>> print(prediction)\n    \"\"\"\n    dates = pd.date_range(START_DATE, END_DATE)\n    data = np.random.randn(len(dates))\n    df[col] = pd.Series(data, index=dates)\n\n    model = LinearRegression()\n    X = np.array(range(len(df))).reshape(-1, 1)\n    y = df[col].values\n    model.fit(X, y)\n\n    next_day = len(df)\n    prediction = model.predict([[next_day]])\n\n    return prediction[0]\n```\n`GPT_ODEX_BREAK`\n\nScenario 5:\npandas, numpy, datetime, sklearn.preprocessing, seaborn\n```python\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nfrom sklearn.preprocessing import LabelEncoder\nimport seaborn as sns\n\n# Constants\nSTART_DATE = datetime(2020, 1, 1)\nEND_DATE = datetime(2022, 12, 31)\nCATEGORIES = ['Category 1', 'Category 2', 'Category 3']\n\ndef generate_and_encode_categorical_data(df, col):\n    \"\"\"\n    Generate a random categorical data within a given date range, add it as a new column \n    in the DataFrame. Encode the categorical data using LabelEncoder and plot the distribution \n    of categories before and after encoding.\n\n    Parameters:\n    df (DataFrame): The DataFrame to add the new column to.\n    col (str): The name of the new column.\n\n    Returns:\n    DataFrame: The DataFrame with the new column.\n\n    Requirements:\n    - pandas\n    - numpy\n    - datetime\n    - sklearn.preprocessing\n    - seaborn\n\n    Example:\n    >>> df = pd.DataFrame()\n    >>> df = generate_and_encode_categorical_data(df, 'Category')\n    >>> print(df.head())\n    \"\"\"\n    dates = pd.date_range(START_DATE, END_DATE)\n    data = np.random.choice(CATEGORIES, len(dates))\n    df[col] = pd.Series(data, index=dates)\n\n    sns.countplot(x=col, data=df)\n    plt.title('Before Encoding')\n    plt.show()\n\n    encoder = LabelEncoder()\n    df[col] = encoder.fit_transform(df[col])\n\n    sns.countplot(x=col, data=df)\n    plt.title('After Encoding')\n    plt.show()\n\n    return df\n```"}
